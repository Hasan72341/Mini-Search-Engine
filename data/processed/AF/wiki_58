{"id": "66279", "revid": "1827553", "url": "https://en.wikipedia.org/wiki?curid=66279", "title": "Magnus the Good", "text": "King of Norway (1035\u20131047) and Denmark (1042\u20131047)\nMagnus Olafsson (; Norwegian and Danish: \"Magnus Olavsson\"; c.\u20091024 \u2013 25 October 1047), better known as Magnus the Good (; Norwegian and Danish: \"Magnus den gode\"), was King of Norway from 1035 and King of Denmark from 1042 until his death in 1047.\nMagnus was an illegitimate son of Saint Olaf, and fled with his mother Alfhild when his father was dethroned in 1028. He returned to Norway in 1035 and was crowned king at the age of 11. In 1042, he was also crowned king of Denmark. Magnus ruled the two countries until 1047, when he died under unclear circumstances. After his death, his kingdom was split between Harald Hardrada in Norway and Sweyn Estridsson in Denmark.\nEarly life.\nMagnus was an illegitimate son of King Olaf Haraldsson (later Saint Olaf) by his English concubine Alfhild, originally a slave (thrall) of Olaf's queen Astrid Olofsdotter. Born prematurely, the child was weak and unable to breathe for the first few minutes, and he was probably not expected to survive. Olaf was not present at the child's birth, and his Icelandic skald Sigvatr \u00de\u00f3r\u00f0arson became his godfather. In a hasty baptism, Sigvatr named Magnus after the greatest king he knew of, also Olaf's greatest role model, \"Karla Magnus\", or Charlemagne. Against the odds, Magnus went on to grow strong and healthy, and he became of vital importance to Olaf as his only son.\nOlaf was dethroned by the Danish king Cnut the Great in 1028, and he went into exile with his family and court, including the young Magnus. They travelled over the mountains and through Eidskog during the winter, entered V\u00e4rmland, and were given shelter by a chieftain called Sigtrygg in N\u00e4rke. After a few months, they departed N\u00e4rke, and by March went eastwards towards Sigtuna, where the Swedish king Anund Jacob had left them a ship. The party thereafter sailed through the Baltic Sea and into the Gulf of Finland, eventually landing in Kievan Rus' (\"Gar\u00f0ar\u00edki\"). They made their first stop at Staraya Ladoga (\"Aldeigjuborg\") to organise the further journey. From there they travelled southwards to Novgorod (\"Holmgard\"), where Olaf sought assistance from Grand Prince Yaroslav the Wise. Yaroslav, however, did not want to become directly involved in the Scandinavian power-struggles, and declined to help. After some time, in early 1030, Olaf learned that the Earl of Lade H\u00e5kon Eiriksson, Cnut's regent in Norway, had disappeared at sea, and gathered his men to make a swift return to Norway. Magnus was left to be fostered by Yaroslav and his wife Ingegerd.\nIn early 1031, a party including Magnus's uncle Harald Sigurdsson (later also to be king and then known as Harald Hardrada) arrived to report the news of his father's death at the Battle of Stiklestad. For the next few years, Magnus was educated in Old Russian and some Greek and was trained as a warrior. In 1030, Cnut appointed his first wife \u00c6lfgifu and their son Svein as regents, but the Norwegians found their rule oppressive and, by the time of Cnut's death in 1035, they had been driven out and Magnus was established as king. Einar Thambarskelfir and Kalf Arnesson, who had both sought to be appointed regents under Cnut after Olaf's death in 1030, had gone together to Kievan Rus' to bring the boy back to rule as the King of Norway. After receiving the approval of Ingegerd, they returned with Magnus to Sigtuna in early 1035, and received backing from the Swedish king, brother of Magnus's stepmother Astrid. Astrid immediately became an important supporter of Magnus, and an army was gathered in Sweden, headed by Einar and Kalf, to place Magnus on the Norwegian throne.\nKing of Norway and Denmark.\nMagnus was proclaimed king in 1035 at 11 years of age. At first, Magnus sought revenge against his father's enemies, but on Sigvatr's advice, he stopped doing so, which is why he became known as \"good\" or \"noble\".\nAnother son of Cnut, Harthacnut, was on the throne of Denmark and wanted his country to reunite with Norway, while Magnus initiated a campaign against Denmark around 1040. However, the noblemen of both countries brought the two kings together at the G\u00f6ta \u00e4lv. They made peace and agreed that the first of them to die would be succeeded by the other. In 1042, Harthacnut died while in England, and Magnus also became King of Denmark, in spite of a claim by Cnut's nephew Sweyn Estridsen, whom Harthacnut had left in control of Denmark when he went to England, and who had some support.\nAs part of consolidating his control, Magnus destroyed the Jomsborg, headquarters of the Jomsvikings. Sweyn fled east and returned as one of the leaders of an invasion by the Wends in 1043, which Magnus decisively defeated at the Battle of Lyrskov Heath, near Hedeby. In the battle, Magnus wielded Saint Olaf's battle-axe, named Hel after the goddess of death. He had dreamt of his father the night before, and the Norwegians swore that before the battle they could hear the bell that Saint Olaf had given to the Church of St. Clement in Kaupang, in Nidaros\u2014a sign that the saint was watching over his son and the army. It was the greatest victory ever over the Wends, with up to 15,000 killed.\nSweyn continued to oppose Magnus in Denmark, although according to \"Heimskringla\", they reached a settlement by which Sweyn became Earl of Denmark under Magnus.\nMagnus wanted to reunite Cnut the Great's entire North Sea Empire by also becoming king of England. When Harthacnut died, the English nobles had chosen as their king \u00c6thelred the Unready's son Edward (later known as Edward the Confessor); Magnus wrote to him that he intended to attack England with combined Norwegian and Danish forces and \"he will then govern it who wins the victory.\" The English were mostly hostile to Magnus; Sweyn was made welcome there, although Edward's mother, Emma, curiously favored Magnus and in 1043 the king confiscated her property, with which by one report she had promised to assist Magnus.\nMeanwhile, Magnus' uncle Harald Sigurdsson had returned to Norway from the east and contested his rule there, while Sweyn was still a threat in Denmark; Harald allied himself with Sweyn. Magnus chose to appease Harald, and made him his co-king in Norway in 1046.\nDeath.\nSweyn increased the pressure on Magnus from his base in Scania, but by late 1046, Magnus had driven Sweyn out of Denmark. However, on 25 October 1047, Magnus suddenly died while in Denmark, either in Zealand or in Jutland, either in an accident or of a disease; accounts vary. Reports include falling overboard from one of the ships he was mustering to invade England and drowning, falling off a horse, and falling ill while on board a ship. He is said to have made Sweyn his heir in Denmark, and Harald in Norway; some say in a deathbed statement. Magnus was buried with his father in the cathedral at Nidaros, modern Trondheim.\nPhysical appearance.\nHeimskringla describes Magnus as \"of middle height, with regular features and light complexion. He had light blond hair, was well spoken and quick to make up his mind, was of noble character, most generous, a great warrior, and most valorous.\"\nDescendants.\nThe line of Olaf II ended with Magnus' death. However, in 1280, Eric II of Norway, who was descended through his mother from Magnus' legitimate sister, Wulfhild, was crowned king of Norway.\nMagnus was not married, but had a daughter out of wedlock, Ragnhild Magnusdatter, who married Haakon Ivarsson, a Norwegian nobleman. Ragnhild and Haakon had daughters Sunniva and Ragnhild. Sunniva had a son Hakon Sunnivasson, whose son became King Eric III of Denmark. Ragnhild married Paul Thorfinnsson, Earl of Orkney, and together they were the parents of Haakon Paulsson, who also became an earl of Orkney.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66280", "revid": "116101", "url": "https://en.wikipedia.org/wiki?curid=66280", "title": "Modulating differential scanning calorimeter", "text": ""}
{"id": "66281", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=66281", "title": "Senator Eastland of Mississippi", "text": ""}
{"id": "66283", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=66283", "title": "Charles Ives", "text": "American modernist composer (1874\u20131954)\nCharles Edward Ives (; October 20, 1874\u00a0\u2013 May 19, 1954) was an American modernist composer, actuary and businessman. Ives was among the earliest renowned American composers to achieve recognition on a global scale. His music was largely ignored during his early career, and many of his works went unperformed for many years. Later in life, the quality of his music was publicly recognized through the efforts of contemporaries like Henry Cowell and Lou Harrison, and he came to be regarded as an \"American original\". He was also among the first composers to engage in a systematic program of experimental music, with musical techniques including polytonality, polyrhythm, tone clusters, aleatory elements, and quarter tones. His experimentation foreshadowed many musical innovations that were later more widely adopted during the 20th century. Hence, he is often regarded as the leading American composer of art music of the 20th century.\nSources of Ives's tonal imagery included hymn tunes and traditional songs; he also incorporated melodies of the town band at holiday parade, the fiddlers at Saturday night dances, patriotic songs, sentimental parlor ballads, and the melodies of Stephen Foster.\nLife and career.\nIves was born in Danbury, Connecticut, on October 20, 1874, the son of George (Edward) Ives, a US Army bandleader in the American Civil War, and his wife, Mary Elizabeth Ives. The Iveses, descended from founding colonists of Connecticut, were one of Danbury's leading families, and they were prominent in business and civic improvement. They were similarly active in progressive social movements of the 19th century, including the abolition of slavery.\nGeorge Ives directed bands, choirs, and orchestras, and taught music theory and a number of instruments. Charles got his influences by sitting in the Danbury town square and listening to his father's marching band and other bands on other sides of the square simultaneously. His father taught him and his brother (Joseph) Moss Ives (February 5, 1876 \u2013 April 7, 1939) music, teaching harmony and counterpoint and guided his first compositions; George took an open-minded approach to theory, encouraging him to experiment in bitonal and polytonal harmonizations. It was from him that Ives also learned the music of Stephen Foster. He became a church organist at the age of 14 and wrote various hymns and songs for church services, including his \"Variations on \"America\"\", which he wrote for a Fourth of July concert in Brewster, New York. It is considered challenging even by modern concert organists, but he famously spoke of it as being \"as much fun as playing baseball\", a commentary on his own organ technique at that age.\nIves moved to New Haven, Connecticut, in 1893, enrolling in the Hopkins School, where he captained the baseball team. In September 1894, Ives entered Yale University, studying under Horatio Parker. Here he composed in a choral style similar to his mentor, writing church music and even an 1896 campaign song for William McKinley. On November 4, 1894, his father died, a crushing blow to him, but to a large degree, he continued the musical experimentation he had begun with him. His brother Moss later became a lawyer.\nAt Yale, Ives was a prominent figure; he was a member of HeBoule, Delta Kappa Epsilon (Phi chapter) and Wolf's Head Society, and sat as chairman of the Ivy Committee. He enjoyed sports at Yale and played on the varsity American football team. Michael C. Murphy, his coach, once remarked that it was a \"crying shame\" that he spent so much time at music as otherwise he could have been a champion sprinter. His works \"Calcium Light Night\" and \"Yale-Princeton Football Game\" show the influence of college and sports on Ives's composition. He wrote his Symphony No. 1 as his senior thesis under Parker's supervision. Ives continued his work as a church organist until May 1902.\nSoon after he graduated from Yale in 1898, he started work in the actuarial department of the Mutual Life Insurance Company of New York. In 1899, Ives moved to employment with the insurance agency Charles H. Raymond &amp; Co., where he stayed until 1906. In 1907, upon the failure of Raymond &amp; Co., he and his friend Julian Myrick formed their own insurance agency Ives &amp; Co., which later became Ives &amp; Myrick, where he remained until he retired.\nDuring his career as an insurance executive and actuary, Ives devised creative ways to structure life-insurance packages for people of means, which laid the foundation of the modern practice of estate planning. His \"Life Insurance with Relation to Inheritance Tax\", published in 1918, was well received. As a result of this he achieved considerable fame in the insurance industry of his time, with many of his business peers surprised to learn that he was also a composer. In his spare time, he composed music and, until his marriage, worked as an organist in Danbury and New Haven as well as Bloomfield, New Jersey, and New York City.\nIn 1907, Ives suffered the first of several \"heart attacks\" (as he and his family called them) that he had throughout his life. These attacks may have been psychological in origin rather than physical. Stuart Feder questions the legitimacy of these heart attacks, as he could not find any medical confirmation of them in previous reports. According to Feder, \"For the only reliable information tells us that he suffered from palpitations, not pain, the cardinal symptom of heart attack.\" Following his recovery from the 1907 attack, Ives entered into one of the most creative periods of his life as a composer.\nIn 1908, he married Harmony Twichell, daughter of Congregational minister Joseph Twichell and his wife, Julia Harmony Cushman. The young couple moved into their own apartment in New York.\nIves had a successful career in insurance. He also continued to be a prolific composer until he suffered another of several heart attacks in 1918, after which he composed very little. He wrote his last piece, the song \"Sunrise\", in August 1926. In 1922, Ives published his \"114 Songs\", which represents the breadth of his work as a composer\u2014it includes art songs, songs he wrote as a teenager and young man, and highly dissonant songs such as \"The Majority\".\nAccording to his wife, one day in early 1927, Ives came downstairs with tears in his eyes. He could compose no more, he said; \"nothing sounds right\". There have been numerous theories advanced to explain the silence of his late years. It seems as mysterious as the last several decades of the life of Jean Sibelius, who stopped composing at almost the same time. While Ives had stopped composing, and was increasingly plagued by health problems, he continued to revise and refine his earlier work, as well as oversee premieres of his music.\nAfter continuing health problems, including diabetes, in 1930 he retired from his insurance business. Although he had more time to devote to music, he was unable to write any new music. During the 1940s, he revised his \"Concord Sonata\", publishing it in 1947 (an earlier version of the sonata and the accompanying prose volume, \"Essays Before a Sonata\" were privately printed in 1920).\nIves died of a stroke in 1954 in New York City. His widow, who died in 1969 at age 92, bequeathed the royalties from his music to the American Academy of Arts and Letters for the Charles Ives Prize.\nMusical career.\nIves's career and dedication to music began when he started playing drums in his father's band at a young age. Ives published a large collection of songs, many of which had piano parts. He composed two string quartets and other works of chamber music, though he is now best known for his orchestral music. His work as an organist led him to write \"Variations on \"America\"\" in 1891, which he premiered at a recital celebrating the Fourth of July.\nHe composed four numbered symphonies as well as a number of works with the word 'Symphony' in their titles, as well as \"The Unanswered Question\" (1908), written for the unusual combination of trumpet, four flutes, and string quartet. \"The Unanswered Question\" was influenced by the New England writers Ralph Waldo Emerson and Henry David Thoreau.\nAround 1910, Ives began composing his most accomplished works, including the \"Holiday Symphony\" and \"Three Places in New England\". \"The Piano Sonata No. 2, Concord, Mass\" (known as the \"Concord Sonata\"), was one of his most notable pieces. He started work on this in 1911 and completed most of it in 1915. However, it was not until 1920 that the piece was published. His revised version was not released until 1947. This piece contains one of the most striking examples of his experimentation. In the second movement, he instructed the pianist to use a piece of wood to create a massive cluster chord. The piece also amply demonstrates Ives's fondness for musical quotation: the opening bars of Beethoven's Fifth Symphony are quoted in each movement. Sinclair's catalogue also notes less obvious quotations of Beethoven's \"Hammerklavier\" Sonata and various other works.\nAnother notable piece of orchestral music Ives completed was his Symphony No. 4, which he worked on from 1910 to 1916, with further revisions in the 1920s. This four-movement symphony is notable for its complexity and vast orchestra. A complete performance of the work was not given until 1965, half a century after it was completed and over a decade after Ives's death.\nIves left behind material for an unfinished \"Universe Symphony\", which he was unable to complete despite two decades of work. This was due to his health problems as well as his shifting ideas of the work.\nReception.\nIves's music was largely ignored during his life, particularly during the years in which he actively composed. Many of his published works went unperformed even many years after his death in 1954. However, his reputation in more recent years has greatly increased. The Juilliard School commemorated the fiftieth anniversary of his death by performing his music over six days in 2004. His musical experiments, including his increasing use of dissonance, were not well received by his contemporaries. The difficulties in performing the rhythmic complexities in his major orchestral works made them daunting challenges even decades after they were composed.\nEarly supporters of Ives's music included Henry Cowell, Elliott Carter, and Aaron Copland. Cowell's periodical \"New Music\" published a substantial number of Ives's scores (with his approval). But for nearly 40 years, Ives had few performances of his music that he did not personally arrange or financially back. He generally used Nicolas Slonimsky as the conductor. After seeing a copy of Ives's self-published \"114 Songs\" during the 1930s, Copland published a newspaper article praising the collection.\nIves began to acquire some public recognition during the 1930s, with performances of a chamber orchestra version of his \"Three Places in New England\", both in the US and on tour in Europe by conductor Nicolas Slonimsky. The Town Hall (New York City) premiered his \"Concord Sonata\" in 1939, featuring pianist John Kirkpatrick. This received favorable commentary in the major New York newspapers. Later, around the time of Ives's death in 1954, Kirkpatrick teamed with soprano Helen Boatwright for the first extended recorded recital of Ives's songs for the obscure Overtone label (Overtone Records catalog number 7). They recorded a new selection of songs for the Ives Centennial Collection that Columbia Records published in 1974.\nIn the 1940s, Ives met Lou Harrison, a fan of his music who began to edit and promote it. Most notably, Harrison conducted the premiere of the Symphony No. 3, \"The Camp Meeting\" (1904) in 1946. The next year, it won Ives the Pulitzer Prize for Music. He gave the prize money away (half of it to Harrison), saying \"prizes are for boys, and I'm all grown up\".\nIves was a generous financial supporter of twentieth-century music, often financing works that were written by other composers. This he did in secret, telling his beneficiaries that his wife wanted him to do so. Nicolas Slonimsky said in 1971, \"He financed my entire career\".\nAt this time, Ives was also promoted by Bernard Herrmann, who worked as a conductor at CBS and in 1940 became principal conductor of the CBS Symphony Orchestra. While there, he championed Ives's music. When they met, Herrmann confessed that he had tried his hand at performing the \"Concord Sonata\". Ives, who avoided the radio and the phonograph, agreed to make a series of piano recordings from 1933 to 1943. One of the more unusual recordings, made in New York City in 1943, features Ives playing the piano and singing the words to his popular World War I song \"They Are There!\", which he composed in 1917. He revised it in 1942\u201343 for World War II.\nIves's piano recordings were later issued in 1974 by Columbia Records on a special LP set for his centenary. New World Records issued 42 tracks of his recordings on CD on April 1, 2006, as \"Ives Plays Ives\".\nIn Canada in the 1950s, the expatriate English pianist Lloyd Powell played a series of concerts including all of Ives's piano works, at the University of British Columbia.\nRecognition of Ives's music steadily increased. He received praise from Arnold Schoenberg, who regarded him as a monument to artistic integrity, and from the New York School of William Schuman. Shortly after Schoenberg's death (three years before Ives died), his widow found a note written by her husband. The note had originally been written in 1944 when Schoenberg was living in Los Angeles and teaching at UCLA. It said: \"There is a great Man living in this Country \u2013 a composer. He has solved the problem how to preserve one's self-esteem and to learn [\"sic\"]. He responds to negligence by contempt. He is not forced to accept praise or blame. His name is Ives.\"\nIves reportedly also won the admiration of Gustav Mahler, who said that he was a true musical revolutionary. Mahler was said to have talked of premiering Ives's third symphony with the New York Philharmonic, but he died in 1911 before conducting this premiere. The source of this account was Ives; since Mahler died, there was no way to verify whether he had seen the score of the symphony or decided to perform it in the 1911\u201312 season. Ives regularly attended New York Philharmonic concerts and probably heard Mahler conduct the Philharmonic at Carnegie Hall.\nIn 1951, Leonard Bernstein conducted the world premiere of Ives's Symphony No. 2 in a broadcast concert by the New York Philharmonic. The Iveses heard the performance on their cook's radio and were amazed at the audience's warm reception to the music. Bernstein continued to conduct Ives's music and made a number of recordings with the Philharmonic for Columbia Records. He honored Ives on one of his televised youth concerts and in a special disc included with the reissue of the 1960 recording of the second symphony and the \"Fourth of July\" movement from Ives's .\nAnother pioneering Ives recording, undertaken during the 1950s, was the first complete set of the four violin sonatas, performed by Minneapolis Symphony concertmaster Rafael Druian and John Simms. Leopold Stokowski took on Symphony No. 4 in 1965, regarding the work as \"the heart of the Ives problem\". The Carnegie Hall world premiere by the American Symphony Orchestra led to the first recording of the music. Another promoter of his was choral conductor Gregg Smith, who made a series of recordings of his shorter works during the 1960s. These included the first stereo recordings of the psalm settings and arrangements of many short pieces for theater orchestra. The Juilliard String Quartet recorded the two string quartets during the 1960s.\nIn the early 21st century, conductor Michael Tilson Thomas is an enthusiastic exponent of Ives's symphonies, as is composer and biographer Jan Swafford. Ives's work is regularly programmed in Europe. He has also inspired pictorial artists, most notably Eduardo Paolozzi, who entitled one of his 1970s sets of prints \"Calcium Light Night\", each print being named for an Ives piece (including \"Central Park in the Dark\"). In 1991, Connecticut's legislature designated Ives as that state's official composer.\nThe Scottish baritone Henry Herford began a survey of Ives's songs in 1990, but this remains incomplete. The record company involved (Unicorn-Kanchana) collapsed. Pianist-composer and Wesleyan University professor Neely Bruce has made a life's study of Ives. To date, he has staged seven parts of a concert series devoted to the complete songs of Ives. Musicologist David Gray Porter reconstructed a piano concerto, the \"Emerson\" Concerto, from Ives's sketches. A recording of the work was released by Naxos Records.\nAmerican singer and composer Frank Zappa included Charles Ives in a list of influences that he presented in the liner notes of his debut album \"Freak Out!\" (1966). Ives continues to influence contemporary composers, arrangers and musicians. Planet Arts Records released \".\" Ives befriended and encouraged a young Elliott Carter. In addition, Phil Lesh, bassist of the Grateful Dead, described Ives as one of his two musical heroes. Jazz musician Albert Ayler also named Charles Ives as an influence in a 1970 interview with \"Swing Journal\".\nAmerican microtonal musician and composer Johnny Reinhard reconstructed and performed \"Universe symphony\" in 1996.\n\"The Unanswered Ives\" is an hour-long film documentary directed by Anne-Kathrin Peitz and produced by Accentus Music (Leipzig, Germany). This was released in 2018 and shown on Swedish and German television stations; it features interviews with Jan Swafford, John Adams, James Sinclair and Jack Cooper.\nIn 1965, Ives won a Grammy Award for his composition Symphony No. 4 and the American Symphony Orchestra won for their recording of the work. Ives had previously been nominated in 1964 for \"New England Holidays\" and in 1960 for Symphony No. 2.\nIgor Stravinsky praised Ives. In 1966 he said: \"[Ives] was exploring the 1960s during the heyday of Strauss and Debussy. Polytonality; atonality; tone clusters; perspectivistic effects; chance; statistical composition; permutation; add-a-part, practical-joke, and improvisatory music: these were Ives\u2019s discoveries a half-century ago as he quietly set about devouring the contemporary cake before the rest of us even found a seat at the same table.\"\nJohn Cage expressed his admiration for Ives in \"Two Statements on Ives\", writing \"I think that Ives's relevance increases as time goes on\" and stating that \"his contribution to American music was in every sense 'not only spiritual, by also concretely musical.' Nowadays everything I hear by Ives delights me.\" Cage recalled that during the 1930s, he was \"not interested in Ives because of the inclusion in his music of aspects of American folk and popular material\". but that once he began to focus on indeterminacy, he \"was able to approach Ives in an entirely different... spirit.\" Cage noted that Ives \"knew that if sound sources came from different points in space that that fact was in itself interesting. Nobody before him had thought about this...\" and stated that \"the freedom that he gave to a performer saying Do this or do that according to your choice is directly in line with present indeterminate music.\" Cage also expressed his interest in what he called the \"mud of Ives\", by which he meant \"the part that is not referential...\" from which arises a \"complex superimposition [of] lines that makes a web in which we cannot clearly perceive anything...\" leading to \"the possibility of not knowing what's happening...\" Cage wrote that \"more and more... I think this experience of non-knowledge is more useful and more important to us than the Renaissance notion of knowing A B C D E F...\" Cage also praised Ives's \"understanding... of inactivity and of silence...\" and recalled having read an essay in which:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[Ives] sees someone sitting on a porch in a rocking chair smoking a pipe looking out over the landscape which goes into the distance and imagines that as that person who is anyone is sitting there doing nothing that he is hearing his own symphony. This I think is for all intents and purposes the goal of music. I doubt whether we can find a higher goal namely that art and our involvement in it will somehow introduce us to the very life that we are living and that we will be able without scores without performers and so forth simply to sit still to listen to the sounds which surround us and hear them as music.\" (Cage refers to the essay as the one \"which [Ives] wrote that follows his One Hundred and Thirteen Songs\", probably referring to the \"Postface to 114 Songs\".)\nConductor Gustavo Dudamel and the Los Angeles Philharmonic won a Grammy Award for Best Orchestral Performance for \"Ives's Complete Symphonies\" (Deutsche Grammophon, recorded in 2020).\nThere is evidence that Ives backdated his scores to sound more modern than he really was. This was first proposed by Maynard Solomon, an advocate of Ives's music. This has, in turn, generated some controversy and puzzlement.\nCompositions.\n\"Note: Because Ives often made several different versions of the same piece, and because his work was generally ignored during his life, it is often difficult to put exact dates on his compositions. The dates given here are sometimes best guesses. There have also been controversial speculations that he purposefully misdated his own pieces earlier or later than actually written.\"\nPolitics.\nIves proposed in 1920 that there be a 20th amendment to the U.S. Constitution which would authorize citizens to submit legislative proposals to Congress. Members of Congress would then cull the proposals, selecting 10 each year as referendums for popular vote by the nation's electorate. He even had printed at his own expense several thousand copies of a pamphlet on behalf of his proposed amendment. The pamphlet proclaimed the need to curtail \"THE EFFECTS OF TOO MUCH POLITICS IN OUR representative DEMOCRACY\". He planned to distribute the pamphlets at the 1920 Republican National Convention, but they arrived from the printer after the convention had ended.\nIt is stated in the biographical film \"A Good Dissonance Like a Man\" that the first of Ives's crippling heart attacks occurred as a result of a World War I-era argument with a young Franklin D. Roosevelt over his idea of issuing of war bonds in amounts as low as $50 each. Roosevelt was chairman of a war bonds committee on which Ives served, and he \"scorned the idea of anything so useless as a $50 bond\". Roosevelt changed his mind about small contributions as seen many years later when he endorsed the March of Dimes to combat poliomyelitis.\nIn popular culture.\nCharles Ives and his wife Harmony (n\u00e9e Twichell) Ives were the subjects of the opera \"Harmony\" (2021) by Robert Carl and Russell Banks, which was premiered by the Seagle Festival in August 2021. Charles Ives was played by baritone Joel Clemens and Harmony Twitchell was played by soprano Victoria Erickson.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "66284", "revid": "42195518", "url": "https://en.wikipedia.org/wiki?curid=66284", "title": "Formic acid", "text": "Simplest carboxylic acid (HCOOH)\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nFormic acid (from la \" formica\"\u00a0'ant'), systematically named methanoic acid, is the simplest carboxylic acid. It has the chemical formula HCOOH and structure . This acid is an important intermediate in chemical synthesis and occurs naturally, most notably in some ants. Esters, salts, and the anion derived from formic acid are called formates. Industrially, formic acid is produced from methanol.\nNatural occurrence.\nFormic acid, which has a pungent, penetrating odor, is found naturally in insects, weeds, fruits and vegetables, and forest emissions. It appears in most ants and in stingless bees of the genus \"Oxytrigona\". Wood ants from the genus \"Formica\" can spray formic acid on their prey or to defend the nest. The puss moth caterpillar (\"Cerura vinula\") will spray it as well when threatened by predators. It is also found in the trichomes of stinging nettle (\"Urtica dioica\"). Apart from that, this acid is incorporated in many fruits such as pineapple (0.21 mg per 100 g), apple (2 mg per 100 g) and kiwi (1 mg per 100 g), as well as in many vegetables, namely onion (45 mg per 100 g), eggplant (1.34 mg per 100 g) and, in extremely low concentrations, cucumber (0.11 mg per 100 g). Formic acid is a naturally occurring component of the atmosphere primarily due to forest emissions.\nHistory.\nAs early as the 15th century, some alchemists and naturalists were aware that ant hills give off an acidic vapor. The first person to describe the isolation of this substance (by the distillation of large numbers of ants) was the English naturalist John Ray, in 1671. Ants secrete the formic acid for attack and defense purposes. Formic acid was first synthesized from hydrocyanic acid by the French chemist Joseph Gay-Lussac. In 1855, another French chemist, Marcellin Berthelot, developed a synthesis from carbon monoxide similar to the process used today.\nFormic acid was long considered a chemical compound of only minor interest in the chemical industry. In the late 1960s, significant quantities became available as a byproduct of acetic acid production. It now finds increasing use as a preservative and antibacterial in livestock feed.\nProperties.\nFormic acid is a colorless liquid having a pungent, penetrating odor at room temperature, comparable to the related acetic acid. Formic acid is about ten times stronger of an acid than acetic acid; its (logarithmic) dissociation constant (pKa) is 3.745, compared to the pKa of 4.756 for acetic acid.\nIt is miscible with water and most polar organic solvents, and is somewhat soluble in hydrocarbons. In hydrocarbons and in the vapor phase, it consists of hydrogen-bonded dimers rather than individual molecules. Owing to its tendency to hydrogen-bond, gaseous formic acid does not obey the ideal gas law. Solid formic acid, which can exist in either of two polymorphs, consists of an effectively endless network of hydrogen-bonded formic acid molecules. Formic acid forms a high-boiling azeotrope with water (107.3\u00a0\u00b0C; 77.5% formic acid). Liquid formic acid tends to supercool.\nChemical reactions.\nDecomposition.\nFormic acid readily decomposes by dehydration in the presence of concentrated sulfuric acid to form carbon monoxide and water:\nHCO2H \u2192 H2O + CO\nTreatment of formic acid with sulfuric acid is a convenient laboratory source of CO.\nIn the presence of platinum, it decomposes with a release of hydrogen and carbon dioxide.\nHCO2H \u2192 H2 + CO2\nSoluble ruthenium catalysts are also effective for producing carbon monoxide-free hydrogen.\nReactant.\nFormic acid shares most of the chemical properties of other carboxylic acids. Because of its high acidity, solutions in alcohols form esters spontaneously; in Fischer esterifications of formic acid, it self-catalyzes the reaction and no additional acid catalyst is needed. Formic acid shares some of the reducing properties of aldehydes, reducing solutions of metal oxides to their respective metal.\nFormic acid is a source for a formyl group for example in the formylation of \"N\"-methylaniline to \"N\"-methylformanilide in toluene.\nIn synthetic organic chemistry, formic acid is often used as a source of hydride ion, as in the Eschweiler\u2013Clarke reaction:\nIt is used as a source of hydrogen in transfer hydrogenation, as in the Leuckart reaction to make amines, and (in aqueous solution or in its azeotrope with triethylamine) for hydrogenation of ketones.\nAddition to alkenes.\nFormic acid is unique among the alkenes in its ability to participate in addition reactions with alkenes. Formic acids and alkenes readily react to form formate esters. In the presence of certain acids, including sulfuric and hydrofluoric acids, however, a variant of the Koch reaction occurs instead, and formic acid adds to the alkene to produce a larger carboxylic acid.\nFormic acid anhydride.\nAn unstable formic anhydride, H(C=O)\u2212O\u2212(C=O)H, can be obtained by dehydration of formic acid with \"N\",\"N\u2032\"-dicyclohexylcarbodiimide in ether at low temperature.\nProduction.\nIn 2009, the worldwide capacity for producing formic acid was per year, roughly equally divided between Europe (, mainly in Germany) and Asia (, mainly in China) while production was below per year in all other continents. It is commercially available in solutions of various concentrations between 85 and 99 w/w %. As of 2009[ [update]], the largest producers are BASF, Eastman Chemical Company, LC Industrial, and Feicheng Acid Chemicals, with the largest production facilities in Ludwigshafen ( per year, BASF, Germany), Oulu (, Eastman, Finland), Nakhon Pathom (n/a, LC Industrial), and Feicheng (, Feicheng, China). 2010 prices ranged from around \u20ac650/tonne (equivalent to around $800/tonne) in Western Europe to $1250/tonne in the United States.\nRegenerating CO2 to make useful products, that displace incumbent fossil fuel based pathways is a more impactful process than CO2 sequestration.\nBoth formic acid and CO (carbon monoxide) are C1 (one carbon molecules). \u00a0Formic is a hydrogen-rich liquid which can be transported and easily donates its hydrogen to enable a variety of condensation and esterification reactions to make a wide variety of derivative molecules. \u00a0CO, while more difficult to transport as a gas, is also one of the primary constituents of syngas useful in synthesizing a wide variety of molecules.\nCO2 electrolysis is distinct from photosynthesis and offers a promising alternative to accelerate decarbonization. By converting CO2 into products using clean electricity, we reduce CO2 emissions in two ways: first and most simply by the amount of CO2 that is regenerated, but the second way is less obvious but even more consequential by avoiding the CO2 emissions otherwise generated by making these same products from fossil fuels. This is known as carbon displacement or abatement.\nCO2 electrolysis holds promise for reducing atmospheric CO2 levels and providing a sustainable method for producing chemicals, materials, and fuels. Its efficiency and scalability are active areas of research, but now also commercialization, aiming to make it a viable commercial technology for both carbon management and molecule production.\nFrom methyl formate and formamide.\nWhen methanol and carbon monoxide are combined in the presence of a strong base, the result is methyl formate, according to the chemical equation:\nCH3OH + CO \u2192 HCO2CH3\nIn industry, this reaction is performed in the liquid phase at elevated pressure. Typical reaction conditions are 80\u00a0\u00b0C and 40 atm. The most widely used base is sodium methoxide. Hydrolysis of the methyl formate produces formic acid:\nHCO2CH3 + H2O \u2192 HCOOH + CH3OH\nEfficient hydrolysis of methyl formate requires a large excess of water. Some routes proceed indirectly by first treating the methyl formate with ammonia to give formamide, which is then hydrolyzed with sulfuric acid:\nHCO2CH3 + NH3 \u2192 HC(O)NH2 + CH3OH\n2 HC(O)NH2 + 2H2O + H2SO4 \u2192 2HCO2H + (NH4)2SO4\nA disadvantage of this approach is the need to dispose of the ammonium sulfate byproduct. This problem has led some manufacturers to develop energy-efficient methods of separating formic acid from the excess water used in direct hydrolysis. In one of these processes, used by BASF, the formic acid is removed from the water by liquid-liquid extraction with an organic base.\nNiche and obsolete chemical routes.\nBy-product of acetic acid production.\nA significant amount of formic acid is produced as a byproduct in the manufacture of other chemicals. At one time, acetic acid was produced on a large scale by oxidation of alkanes, by a process that cogenerates significant formic acid. This oxidative route to acetic acid has declined in importance so that the aforementioned dedicated routes to formic acid have become more important.\nHydrogenation of carbon dioxide.\nThe catalytic hydrogenation of CO2 to formic acid has long been studied. This reaction can be conducted homogeneously.\nOxidation of biomass.\nFormic acid can also be obtained by aqueous catalytic partial oxidation of wet biomass by the OxFA process. A Keggin-type polyoxometalate (H5PV2Mo10O40) is used as the homogeneous catalyst to convert sugars, wood, waste paper, or cyanobacteria to formic acid and CO2 as the sole byproduct. Yields of up to 53% formic acid can be achieved.\nLaboratory methods.\nIn the laboratory, formic acid can be obtained by heating oxalic acid in glycerol followed by steam distillation. Glycerol acts as a catalyst, as the reaction proceeds through a glyceryl oxalate intermediate. If the reaction mixture is heated to higher temperatures, allyl alcohol results. The net reaction is thus:\nC2O4H2 \u2192 HCO2H + CO2\nAnother illustrative method involves the reaction between lead formate and hydrogen sulfide, driven by the formation of lead sulfide.\nPb(HCOO)2 + H2S \u2192 2HCOOH + PbS\nElectrochemical production.\nFormate is formed by the electrochemical reduction of CO2 (in the form of bicarbonate) at a lead cathode at pH 8.6:\nHCO3- + H2O + 2e\u2212 \u2192 HCO2- + 2OH-\nor\nCO2 + H2O + 2e\u2212 \u2192 HCO2- + OH-\nIf the feed is CO2 and oxygen is evolved at the anode, the total reaction is:\nCO2 + OH- \u2192 HCO2- + 1/2 \nBiosynthesis.\nFormic acid is named after ants which have high concentrations of the compound in their venom, derived from serine through a 5,10-methenyltetrahydrofolate intermediate. The conjugate base of formic acid, formate, also occurs widely in nature. An assay for formic acid in body fluids, designed for determination of formate after methanol poisoning, is based on the reaction of formate with bacterial formate dehydrogenase.\nUses.\nAgriculture.\nA major use of formic acid is as a preservative and antibacterial agent in livestock feed. It arrests certain decay processes and causes the feed to retain its nutritive value longer.\nIn Europe, it is applied on silage, including fresh hay, to promote the fermentation of lactic acid and to suppress the formation of butyric acid; it also allows fermentation to occur quickly, and at a lower temperature, reducing the loss of nutritional value. It is widely used to preserve winter feed for cattle, and is sometimes added to poultry feed to kill \"E. coli\" bacteria. Use as a preservative for silage and other animal feed constituted 30% of the global consumption in 2009.\nBeekeepers use formic acid as a miticide against the tracheal mite (\"Acarapis woodi\") and the \"Varroa destructor\" mite and \"Varroa jacobsoni\" mite.\nEnergy.\nFormic acid can be used directly in formic acid fuel cells or indirectly in hydrogen fuel cells.\nElectrolytic conversion of electrical energy to chemical fuel has been proposed as a large-scale source of formate by various groups. The formate could be used as feed to modified \"E. coli\" bacteria for producing biomass. Natural methylotroph microbes can feed on formic acid or formate.\nFormic acid has been considered as a means of hydrogen storage. The co-product of this decomposition, carbon dioxide, can be rehydrogenated back to formic acid in a second step. Formic acid contains 53 g/L hydrogen at room temperature and atmospheric pressure, which is three and a half times as much as compressed hydrogen gas can attain at 350 bar pressure (14.7 g/L). Pure formic acid is a liquid with a flash point of 69\u00a0\u00b0C, much higher than that of gasoline (\u221240\u00a0\u00b0C) or ethanol (13\u00a0\u00b0C).\nIt is possible to use formic acid as an intermediary to produce isobutanol from CO2 using microbes.\nSoldering.\nFormic acid has a potential application in soldering. Due to its capacity to reduce oxide layers, formic acid gas can be blasted at an oxide surface to increase solder wettability.\nChromatography.\nFormic acid is used as a volatile pH modifier in HPLC and capillary electrophoresis. Formic acid is often used as a component of mobile phase in reversed-phase high-performance liquid chromatography (RP-HPLC) analysis and separation techniques for the separation of hydrophobic macromolecules, such as peptides, proteins and more complex structures including intact viruses. Especially when paired with mass spectrometry detection, formic acid offers several advantages over the more traditionally used phosphoric acid.\nOther uses.\nFormic acid is also significantly used in the production of leather, including tanning (23% of the global consumption in 2009), and in dyeing and finishing textiles (9% of the global consumption in 2009) because of its acidic nature. Use as a coagulant in the production of rubber consumed 6% of the global production in 2009.\nFormic acid is also used in place of mineral acids for various cleaning products, such as limescale remover and toilet bowl cleaner. Some formate esters are artificial flavorings and perfumes.\nFormic acid application has been reported to be an effective treatment for warts.\nSafety.\nFormic acid has low toxicity (hence its use as a food additive), with an LD50 of 1.8g/kg (tested orally on mice). The concentrated acid is corrosive to the skin.\nFormic acid is readily metabolized and eliminated by the body. Nonetheless, it has specific toxic effects; the formic acid and formaldehyde produced as metabolites of methanol are responsible for the optic nerve damage, causing blindness, seen in methanol poisoning. Some chronic effects of formic acid exposure have been documented. Some experiments on bacterial species have demonstrated it to be a mutagen. Chronic exposure in humans may cause kidney damage. Another possible effect of chronic exposure is development of a skin allergy that manifests upon re-exposure to the chemical.\nConcentrated formic acid slowly decomposes to carbon monoxide and water, leading to pressure buildup in the containing vessel. For this reason, 98% formic acid is shipped in plastic bottles with self-venting caps.\nThe hazards of solutions of formic acid depend on the concentration. The following table lists the Globally Harmonized System of Classification and Labelling of Chemicals for formic acid solutions:\nFormic acid in 85% concentration is flammable, and diluted formic acid is on the U.S. Food and Drug Administration list of food additives. The principal danger from formic acid is from skin or eye contact with the concentrated liquid or vapors. The U.S. OSHA Permissible Exposure Level (PEL) of formic acid vapor in the work environment is 5 parts per million (ppm) of air.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66285", "revid": "36979282", "url": "https://en.wikipedia.org/wiki?curid=66285", "title": "Chlorides", "text": ""}
{"id": "66286", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=66286", "title": "Organic acid", "text": "Organic compound with acidic properties\nAn organic acid is an organic compound with acidic properties. The most common organic acids are the carboxylic acids, whose acidity is associated with their carboxyl group\u00a0\u2013COOH. Sulfonic acids, containing the group\u00a0\u2013SO2OH, are relatively stronger acids. Alcohols, with \u2013OH, can act as acids but they are usually very weak. The relative stability of the conjugate base of the acid determines its acidity. Other groups can also confer acidity, usually weakly: the thiol group\u00a0\u2013SH, the enol group, and the phenol group. In biological systems, organic compounds containing these groups are generally referred to as organic acids.\nA few common examples include:\nCharacteristics.\nIn general, organic acids are weak acids and do not dissociate completely in water, whereas the strong mineral acids do. Lower molecular mass organic acids such as formic and lactic acids are miscible in water, but higher molecular mass organic acids, such as benzoic acid, are insoluble in molecular (neutral) form.\nOn the other hand, most organic acids are very soluble in organic solvents. \"p\"-Toluenesulfonic acid is a comparatively strong acid used in organic chemistry often because it is able to dissolve in the organic reaction solvent.\nExceptions to these solubility characteristics exist in the presence of other substituents that affect the polarity of the compound.\nApplications.\nSimple organic acids like formic or acetic acids are used for oil and gas well stimulation treatments. These organic acids are much less reactive with metals than are strong mineral acids like hydrochloric acid (HCl) or mixtures of HCl and hydrofluoric acid (HF). For this reason, organic acids are used at high temperatures or when long contact times between acid and pipe are needed.\nThe conjugate bases of organic acids such as citrate and lactate are often used in biologically compatible buffer solutions.\nCitric and oxalic acids are used as rust removal. As acids, they can dissolve the iron oxides, but without damaging the base metal as do stronger mineral acids. In the dissociated form, they may be able to chelate the metal ions, helping to speed removal.\nBiological systems create many more complex organic acids such as -lactic, citric, and -glucuronic acids that contain hydroxyl or carboxyl groups. Human blood and urine contain these plus organic acid degradation products of amino acids, neurotransmitters, and intestinal bacterial action on food components. Examples of these categories are alpha-ketoisocaproic, vanilmandelic, and -lactic acids, derived from catabolism of -leucine and epinephrine (adrenaline) by human tissues and catabolism of dietary carbohydrate by intestinal bacteria, respectively. Organic acids (C1\u2013C7) are widely distributed in nature as normal constituents of plants or animal tissues. They are also formed through microbial fermentation of carbohydrates mainly in the large intestine. They are sometimes found in their sodium, potassium, or calcium salts, or even stronger double salts.\nIn food.\nOrganic acids are used in food preservation because of their effects on bacteria. The key basic principle on the mode of action of organic acids on bacteria is that non-dissociated (non-ionized) organic acids can penetrate the bacteria cell wall and disrupt the normal physiology of certain types of bacteria that we call \"pH-sensitive\", meaning that they cannot tolerate a wide internal and external pH gradient. Among those bacteria are \"Escherichia coli\", \"Salmonella\" spp., \"C. perfringens\", \"Listeria monocytogenes\", and \"Campylobacter\" species.\nUpon passive diffusion of organic acids into the bacteria, where the pH is near or above neutrality, the acids will dissociate and raise the bacteria internal pH, leading to situations that will not impair nor stop the growth of bacteria. On the other hand, the anionic part of the organic acids that can escape the bacteria in its dissociated form will accumulate within the bacteria and disrupt few metabolic functions, leading to osmotic pressure increase, incompatible with the survival of the bacteria.\nIt has been well demonstrated that the state of the organic acids (undissociated or dissociated) is not important to define their capacity to inhibit the growth of bacteria, compared to undissociated acids.\nLactic acid and its salts sodium lactate and potassium lactate are widely used as antimicrobials in food products, in particular, dairy and poultry such as ham and sausages.\nIn nutrition and animal feeds.\nOrganic acids have been used successfully in pig production for more than 25 years. Although less research has been done in poultry, organic acids have also been found to be effective in poultry production.\nOrganic acids added to feeds should be protected to avoid their dissociation in the crop and in the intestine (high pH segments) and reach far into the gastrointestinal tract, where the bulk of the bacteria population is located.\nFrom the use of organic acids in poultry and pigs, one can expect an improvement in performance similar to or better than that of antibiotic growth promoters, without the public health concern, a preventive effect on the intestinal problems like necrotic enteritis in chickens and \"Escherichia coli\" infection in young pigs. Also one can expect a reduction of the carrier state for \"Salmonella\" species and \"Campylobacter\" species.\nOngoing research.\nIn addition to the end uses previously seen, organic acids have been tested for the following applications:\nBarbero-L\u00f3pez and colleagues tested at the University of Eastern Finland the potential use of three organic acids, acetic, formic and propionic acids, in wood preservation. They showed a high antifungal potential against the decaying fungi tested (brown rotting fungi \"Coniophora puteana\", \"Rhodonia placenta\" and \"Gloeophyllum trabeum;\" White rotting fungus \"Trametes versicolor)\" in Petri dish. However, when they treated wood with organic acids, the acids leached out from wood and did not prevent degradation. Additionally, the organic acids' acidity may have caused chemical degradation on wood. Additionally, in a more recent study, the ecotoxicity of several natural wood preservatives was compared, and the results indicated a very low toxicity of propionic acid.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66287", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=66287", "title": "American Communists", "text": ""}
{"id": "66288", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=66288", "title": "Electrical field", "text": ""}
{"id": "66291", "revid": "44613172", "url": "https://en.wikipedia.org/wiki?curid=66291", "title": "Wadden Sea", "text": "Intertidal zone in the southeastern part of the North Sea\nThe Wadden Sea ( ; ; or ; ; ; ) is an intertidal zone in the southeastern part of the North Sea. It lies between the coast of northwestern continental Europe and the range of low-lying Frisian Islands, forming a shallow body of water with tidal flats and wetlands. It has high biological diversity and is an important area for both breeding and migrating birds. In 2009, the Dutch and German parts of the Wadden Sea were inscribed on UNESCO's World Heritage List and the Danish part was added in June 2014.\nThe Wadden Sea stretches from Den Helder, in the northwest of the Netherlands, past the great river estuaries of Germany to its northern boundary at Skallingen in Denmark along a total coastline of some and a total area of about . Within the Netherlands, it is bounded from the IJsselmeer by the Afsluitdijk. Historically, the coastal regions were often subjected to large floods, resulting in thousands of deaths, including the Saint Marcellus' floods of 1219 and 1362, Burchardi flood of 1634 and Christmas Flood of 1717. Some of these also significantly changed the coastline. Numerous dikes and several causeways have been built, and as a result recent floods have resulted in few or no fatalities (even if some dikes rarely and locally have been overrun in recent history). This makes it among the most human-altered habitats on the planet.\nEnvironment.\nThe word \"wad\" is Frisian and Dutch for \"mud flat\" (Low German and , ). The area is typified by extensive tidal mud flats, deeper tidal trenches (tidal creeks) and the islands that are contained within this, a region continually contested by land and sea.\nThe landscape has been formed for a great part by storm tides in the 10th to 14th centuries, overflowing and carrying away former peat land behind the coastal dunes. The present islands are a remnant of the former coastal dunes.\nTowards the North Sea the islands are marked by dunes and wide sandy beaches, and towards the Wadden Sea a low, tidal coast. The impact of waves and currents carrying away sediments is slowly changing both land masses and coastlines. For example, the islands of Vlieland and Ameland have moved eastwards through the centuries, having lost land on one side and gained it on the other.\nSea level rise poses a significant threat to areas with low-lying coastal areas with small gradients, such as the tidal flats of the Wadden Sea. However, recent studies indicate that the current sea level rise (3.7 mm/yr) is being exceeded by sediment accretion rates across most of these tidal flats, particularly along the German coastline. While tidal flats in the Dutch Wadden Sea are also accreting, they are doing so at a slower pace.\nFauna.\nThe Wadden Sea is famous for its rich flora and fauna, especially birds. Hundreds of thousands of waders, ducks, and geese use the area as a migration stopover or wintering site. It is also a rich habitat for gulls and terns, as well as a few species of herons, Eurasian spoonbills and birds-of-prey, including a small and increasing breeding population of white-tailed eagles. However, the biodiversity of Wadden Sea is smaller today than it once was; for birds, greater flamingos and Dalmatian pelicans used to be common as well, at least during the Holocene climatic optimum when the climate was warmer. Due to human activity and a changing environment, species have gone extinct, while others are expected to migrate in.\nLarger fish including rays, Atlantic salmon and brown trout are still present in several sections of the Wadden Sea, but others like European sea sturgeon only survive in the region through a reintroduction project. The world's only remaining natural population of houting survives in the Danish part of the Wadden Sea and it has been used as a basis for reintroductions further south, but considerable taxonomic confusion remains over its status (whether it is the same as the houting that once lived further south in the Wadden Sea). European oyster once formed large beds in the region and was still present until a few decades ago, when extirpated due to a combination of disease and the continued spread of the invasive Pacific oyster, which now forms large beds in the Wadden Sea. Especially the southwestern part of the Wadden Sea has been greatly reduced. Historically, the Rhine was by far the most important river flowing into this section, but it has been greatly reduced due to dams. As a result, about 90% of all the species that historically inhabited that part of the Wadden Sea are at risk.\nThe Wadden Sea is an important habitat for both harbour and grey seals. Harbour porpoises and white-beaked dolphins are the sea's only resident cetaceans. They were once extinct in the southern part of the sea but have also re-colonized that area again. Many other cetaceans only visit seasonally, or occasionally. In early history, North Atlantic right whales and gray whales (now extinct in the North Atlantic) were present in the region, perhaps using the shallow, calm waters for feeding and breeding. It has been theorized that they were hunted to extinction in this region by shore-based whalers in medieval times. They are generally considered long-extinct in the region, but in the Netherlands, a possible right whale was observed close to beaches on Texel in the West Frisian Islands and off Steenbanken, Schouwen-Duiveland in July 2005. Recent increases in the number of North Atlantic humpback whales and minke whales might have resulted in more visits and possible re-colonization by the species to the areas especially around Marsdiep. Future recovery of once-extinct local bottlenose dolphins is also expected.\nConservation.\nA number of human-introduced invasive species, including algae, plants, and smaller organisms, are causing negative effects on native species.\nEach of the three countries has designated Ramsar sites in the region (see Wadden Sea National Parks).\nAlthough the Wadden Sea is not yet listed as a transboundary Ramsar site, a great part of the Wadden Sea is protected in cooperation of all three countries. The governments of the Netherlands, Denmark and Germany have been working together since 1978 on the protection and conservation of the Wadden Sea. Co-operation covers management, monitoring and research, as well as political matters. Furthermore, in 1982, a Joint Declaration on the Protection of the Wadden Sea was agreed upon to co-ordinate activities and measures for the protection of the Wadden Sea. In 1997, a Trilateral Wadden Sea Plan was adopted.\nIn 1986, the Wadden Sea Area was declared a biosphere reserve by UNESCO.\nIn June 2009, the Wadden Sea (comprising the Dutch Wadden Sea Conservation Area and the German Wadden Sea National Parks of Lower Saxony and Schleswig-Holstein) was placed on the World Heritage list by UNESCO. A minor boundary modification in 2011 added the Hamburg Wadden Sea National Park to the site, and the Danish part was added to in 2014. The state of Bremen, covering part of the Weser River estuary, is not participating. Conservation efforts are coordinated by the Common Wadden Sea Secretariat, seated in Wilhelmshaven.\nRecreation.\nMany of the islands have been popular seaside resorts since the 19th century.\nMudflat hiking, i.e., walking on the sandy flats at low tide, has become popular in the Wadden Sea.\nIt is also a popular region for pleasure boating.\nLiterature.\nThe German part of the Wadden Sea was the setting for the 1903 Erskine Childers novel \"The Riddle of the Sands\" and Else Ury's 1915 novel \"Nesth\u00e4kchen in the Children's Sanitorium\".\nWadden Sea Region.\nThe area bordering the Wadden Sea, including the Frisian islands and the mainland coastal marshes, is also called the Wadden Coast. In Germany the area is referred to as North Sea Coast (\"Nordseek\u00fcste\"). The embanked polderlands and saltmarshes in the Wadden Sea area \u2013 including the Elbe Marshes \u2013 are referred to in Germany as North Sea coastal marshes (\"Nordseemarschen\").\nMore recent are terms such as Waddenland, Wadden Sea area and Wadden Sea Region. The latter term is generally understood to include all coastal regions around the Wadden Sea that participate in the trilateral cooperation between Denmark, Germany and the Netherlands. The entire area is known for its rich cultural heritage, dating back to the Roman Iron Age and the Middle Ages, and largely coincides with the area internationally referred to as Frisia. Between 2002 and 2023 stakeholder organizations and NGOs from the Wadden Sea Region cooperated in a platform or association called the Wadden Sea Forum (WSF).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66292", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=66292", "title": "Peoples Republic of China/Art", "text": ""}
{"id": "66294", "revid": "50757683", "url": "https://en.wikipedia.org/wiki?curid=66294", "title": "Reinforcement learning", "text": "Field of machine learning\nIn machine learning and optimal control, reinforcement learning (RL) is concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nWhile supervised learning and unsupervised learning algorithms respectively attempt to discover patterns in labeled and unlabeled data, reinforcement learning involves training an agent through interactions with its environment. To learn to maximize rewards from these interactions, the agent makes decisions between trying new actions to learn more about the environment (exploration), or using current knowledge of the environment to take the best action (exploitation). The search for the optimal balance between these two strategies is known as the exploration\u2013exploitation dilemma.\nThe environment is typically stated in the form of a Markov decision process, as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large Markov decision processes where exact methods become infeasible. &lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nPrinciples.\nDue to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called \"approximate dynamic programming\", or \"neuro-dynamic programming.\" The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).\nBasic reinforcement learning is modeled as a Markov decision process:\nThe purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.\nA basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state formula_12 and reward formula_13. It then chooses an action formula_14 from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state formula_15 and the reward formula_16 associated with the \"transition\" formula_17 is determined. The goal of a reinforcement learning agent is to learn a \"policy\":\nformula_18\nthat maximizes the expected cumulative reward.\nFormulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have \"full observability\". If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have \"partial observability\", and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\nWhen the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.\nThus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage, robot control, photovoltaic generators, backgammon, checkers, Go (AlphaGo), and autonomous driving systems.\nTwo elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:\nThe first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\nExploration.\nThe trade-off between exploration and exploitation has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).\nReinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\nOne such method is formula_19-greedy, where formula_20 is a parameter controlling the amount of exploration vs. exploitation. With probability formula_21, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability formula_19, exploration is chosen, and the action is chosen uniformly at random. formula_19 is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.\nAlgorithms for control learning.\nEven if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\nCriterion of optimality.\nPolicy.\nThe agent's action selection is modeled as a map called \"policy\":\nformula_24\nThe policy map gives the probability of taking action formula_7 when in state formula_5.61 There are also deterministic policies formula_27 for which formula_28 denotes the action that should be played at state formula_5.\nState-value function.\nThe state-value function formula_30 is defined as, \"expected discounted return\" starting with state formula_5, i.e. formula_32, and successively following policy formula_27. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.60\nformula_34\nwhere the random variable formula_35 denotes the discounted return, and is defined as the sum of future discounted rewards:\nformula_36\nwhere formula_16 is the reward for transitioning from state formula_12 to formula_15, formula_40 is the discount rate. formula_41 is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.\nThe algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called \"stationary\" policies. A policy is \"stationary\" if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to \"deterministic\" stationary policies. A \"deterministic stationary\" policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\nBrute force.\nThe brute force approach entails two steps:\nOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.\nThese problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\nValue function.\nValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns formula_42 for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\nThese methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from \"any\" initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.\nTo define optimality in a formal manner, define the state-value of a policy formula_27 by\nformula_44\nwhere formula_35 stands for the discounted return associated with following formula_27 from the initial state formula_5. Defining formula_48 as the maximum possible state-value of formula_49, where formula_27 is allowed to change,\nformula_51\nA policy that achieves these optimal state-values in each state is called \"optimal\". Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since formula_52, where formula_5 is a state randomly sampled from the distribution formula_54 of initial states (so \nAlthough state-values suffice to define optimality, it is useful to define action-values. Given a state formula_5, an action formula_7 and a policy formula_27, the action-value of the pair formula_58 under formula_27 is defined by\nformula_60\nwhere formula_35 now stands for the random discounted return associated with first taking action formula_7 in state formula_5 and following formula_27, thereafter.\nThe theory of Markov decision processes states that if formula_65 is an optimal policy, we act optimally (take the optimal action) by choosing the action from formula_66 with the highest action-value at each state, formula_5. The \"action-value function\" of such an optimal policy (formula_68) is called the \"optimal action-value function\" and is commonly denoted by formula_69. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\nAssuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions formula_70 (formula_71) that converge to formula_69. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\nMonte Carlo methods.\nMonte Carlo methods are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment's dynamics, Monte Carlo methods rely solely on actual or simulated experience\u2014sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.\nMonte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term \"Monte Carlo\" generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from \"complete\" returns, rather than \"partial\" returns.\nThese methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process, Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.\nTemporal difference methods.\nThe first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of \"generalized policy iteration\" algorithms. Many \"actor-critic\" methods belong to this category.\nThe second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation. The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method, may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\nAnother problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called formula_73 parameter formula_74 that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\nFunction approximation methods.\nIn order to address the fifth issue, \"function approximation methods\" are used. \"Linear function approximation\" starts with a mapping formula_75 that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair formula_58 are obtained by linearly combining the components of formula_77 with some \"weights\" formula_78:\nformula_79\nThe algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\nValue iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants. Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.\nThe problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\nDirect policy search.\nAn alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\nGradient-based methods (\"policy gradient methods\") start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector formula_78, let formula_81 denote the policy associated to formula_78. Defining the performance function by formula_83 under mild conditions this function will be differentiable as a function of the parameter vector formula_78. If the gradient of formula_85 was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams's REINFORCE method (which is known as the likelihood ratio method in the simulation-based optimization literature).\nA large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\nPolicy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, \"actor\u2013critic methods\" have been proposed and performed well on various problems.\nPolicy search methods have been used in the robotics context. Many policy search methods may get stuck in local optima (as they are based on local search).\nModel-based algorithms.\nFinally, all of the above methods can be combined with algorithms that first learn a model of the Markov decision process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions. Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and \"replayed\" to the learning algorithm.\nModel-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov decision process can be learnt.\nThere are other ways to use models than to update a value function. For instance, in model predictive control the model is used to update the behavior directly.\nTheory.\nBoth the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\nEfficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\nFor incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\nResearch.\nResearch topics include:\nComparison of key algorithms.\nThe following table lists the key algorithms for learning a policy depending on several criteria:\nAssociative reinforcement learning.\nAssociative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.\nDeep reinforcement learning.\nThis approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.\nAdversarial deep reinforcement learning.\nAdversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.\nFuzzy reinforcement learning.\nBy introducing fuzzy inference in reinforcement learning, approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\nInverse reinforcement learning.\nIn inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal. One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.\nMulti-objective reinforcement learning.\nMulti-objective reinforcement learning (MORL) is a form of reinforcement learning concerned with conflicting alternatives. It is distinct from multi-objective optimization in that it is concerned with agents acting in environments.\nSafe reinforcement learning.\nSafe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. An alternative approach is risk-averse reinforcement learning, where instead of the \"expected\" return, a \"risk-measure\" of the return is optimized, such as the conditional value at risk (CVaR). In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties. However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias and blindness to success.\nSelf-reinforcement learning.\nSelf-reinforcement learning (or self-learning), is a learning paradigm which does not use the concept of immediate reward formula_8 after transition from formula_5 to formula_6 with action formula_7. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation.\nThe self-reinforcement algorithm updates a memory matrix formula_90 such that in each iteration executes the following machine learning routine:\nInitial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior).\nSelf-reinforcement (self-learning) was introduced in 1982 along with a neural network capable of self-reinforcement learning, named Crossbar Adaptive Array (CAA). The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence states. The system is driven by the interaction between cognition and emotion.\nReinforcement Learning in Natural Language Processing.\nIn recent years, reinforcement learning has become a significant concept in natural language processing (NLP), where tasks are often sequential decision-making rather than static classification. Reinforcement learning is where an agent take actions in an environment to maximize the accumulation of rewards. This framework is best fit for many NLP tasks, including dialogue generation, text summarization, and machine translation, where the quality of the output depends on optimizing long-term or human-centered goals rather than the prediction of single correct label.\nEarly application of RL in NLP emerged in dialogue systems, where conversation was determined as a series of actions optimized for fluency and coherence. These early attempts, including policy gradient and sequence-level training techniques, laid a foundation for the broader application of reinforcement learning to other areas of NLP.\nA major breakthrough happened with the introduction of reinforcement learning from human feedback (RLHF), a method in which human feedback ratings are used to train a reward model that guides the RL agent. Unlike traditional rule-based or supervised systems, RLHF allows models to align their behavior with human judgments on complex and subjective tasks. This technique was initially used in the development of InstructGPT, an effective language model trained to follow human instructions and later in ChatGPT which incorporates RLHF for improving output responses and ensuring safety.\nMore recently, researchers have explored the use of offline RL in NLP to improve dialogue systems without the need of live human interaction. These methods optimize for user engagement, coherence, and diversity based on past conversation logs and pre-trained reward models.\nOne example is DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. This model was trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step.\nStatistical comparison of reinforcement learning algorithms.\nEfficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other. After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test. This requires to accumulate all the rewards within an episode into a single number\u2014the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.\nChallenges and Limitations.\nDespite significant advancements, reinforcement learning (RL) continues to face several challenges and limitations that hinder its widespread application in real-world scenarios.\nSample Inefficiency.\nRL algorithms often require a large number of interactions with the environment to learn effective policies, leading to high computational costs and time-intensive to train the agent. For instance, OpenAI's Dota-playing bot utilized thousands of years of simulated gameplay to achieve human-level performance. Techniques like experience replay and curriculum learning have been proposed to deprive sample inefficiency, but these techniques add more complexity and are not always sufficient for real-world applications.\nStability and Convergence Issues.\nTraining RL models, particularly for deep neural network-based models, can be unstable and prone to divergence. A small change in the policy or environment can lead to extreme fluctuations in performance, making it difficult to achieve consistent results. This instability is further enhanced in the case of the continuous or high-dimensional action space, where the learning step becomes more complex and less predictable.\nGeneralization and Transferability.\nThe RL agents trained in specific environments often struggle to generalize their learned policies to new, unseen scenarios. This is the major setback preventing the application of RL to dynamic real-world environments where adaptability is crucial. The challenge is to develop such algorithms that can transfer knowledge across tasks and environments without extensive retraining.\nBias and Reward Function Issues.\nDesigning appropriate reward functions is critical in RL because poorly designed reward functions can lead to unintended behaviors. In addition, RL systems trained on biased data may perpetuate existing biases and lead to discriminatory or unfair outcomes. Both of these issues requires careful consideration of reward structures and data sources to ensure fairness and desired behaviors.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66297", "revid": "40863318", "url": "https://en.wikipedia.org/wiki?curid=66297", "title": "Chinese art", "text": "Chinese art is visual art that originated in or is practiced in China, Greater China or by Chinese artists. Art created by Chinese residing outside of China can also be considered a part of Chinese art when it is based on or draws on Chinese culture, heritage, and history. Early \"Stone Age art\" dates back to 10,000 BC, mostly consisting of simple pottery and sculptures. After that period, Chinese art, like Chinese history, was typically classified by the succession of ruling dynasties of Chinese emperors, most of which lasted several hundred years. The Palace Museum in Beijing and the National Palace Museum in Taipei contains extensive collections of Chinese art.\nChinese art is marked by an unusual degree of continuity within, and consciousness of, tradition, lacking an equivalent to the Western collapse and gradual recovery of Western classical styles of art. Decorative arts are extremely important in Chinese art, and much of the finest work was produced in large workshops or factories by essentially unknown artists, especially in Chinese ceramics.\nMuch of the best work in ceramics, textiles, carved lacquer were produced over a long period by the various Imperial factories or workshops, which as well as being used by the court was distributed internally and abroad on a huge scale to demonstrate the wealth and power of the Emperors. In contrast, the tradition of ink wash painting, practiced mainly by scholar-officials and court painters especially of landscapes, flowers, and birds, developed aesthetic values depending on the individual imagination of and objective observation by the artist that are similar to those of the West, but long pre-dated their development there. After contacts with Western art became increasingly important from the 19th century onwards, in recent decades China has participated with increasing success in worldwide contemporary art.\nHistory and development.\nNeolithic pottery.\nEarly forms of art in China are found in the Neolithic Yangshao culture, which dates back to the 6th millennium BC. Archeological findings such as those at the Banpo have revealed that the Yangshao made pottery; early ceramics were unpainted and most often cord-marked. The first decorations were fish and human faces, but these eventually evolved into symmetrical-geometric abstract designs, some painted.\nThe most distinctive feature of Yangshao culture was the extensive use of painted pottery, especially human facial, animal, and geometric designs. Unlike the later Longshan culture, the Yangshao culture did not use pottery wheels in pottery making. Excavations have found that children were buried in painted pottery jars.\nJade culture.\nThe Liangzhu culture was the last Neolithic Jade culture in the Yangtze River Delta and was spaced over a period of about 1,300 years. The Jade from this culture is characterized by finely worked, large ritual jades such as Cong cylinders, Bi discs, Yue axes and also pendants and decorations in the form of chiseled open-work plaques, plates and representations of small birds, turtles and fish. The Liangzhu Jade has a white, milky bone-like aspect due to its tremolite rock origin and influence of water-based fluids at the burial sites.\nBronze casting.\nThe Bronze Age in China began with the Xia dynasty. Examples from this period have been recovered from ruins of the Erlitou culture, in Shanxi, and include complex but unadorned utilitarian objects. In the following Shang dynasty more elaborate objects, including many ritual vessels, were crafted. The Shang are remembered for their bronze casting, noted for its clarity of detail. Shang bronzesmiths usually worked in foundries outside the cities to make ritual vessels, and sometimes weapons and chariot fittings as well. The bronze vessels were receptacles for storing or serving various solids and liquids used in the performance of sacred ceremonies. Some forms such as the \"ku\" and \"jue\" can be very graceful, but the most powerful pieces are the \"ding\", sometimes described as having an \"air of ferocious majesty\".\nIt is typical of the developed Shang style that all available space is decorated, most often with stylized forms of real and imaginary animals. The most common motif is the \"taotie\", which shows a mythological being presented frontally as though squashed onto a horizontal plane to form a symmetrical design. The early significance of \"taotie\" is not clear, but myths about it existed around the late Zhou dynasty. It was considered to be variously a covetous man banished to guard a corner of heaven against evil monsters; or a monster equipped with only a head which tries to devour men but hurts only itself.\nThe function and appearance of bronzes changed gradually from the Shang to the Zhou. They shifted from being used in religious rites to more practical purposes. By the Warring States period, bronze vessels had become objects of aesthetic enjoyment. Some were decorated with social scenes, such as from a banquet or hunt; whilst others displayed abstract patterns inlaid with gold, silver, or precious and semiprecious stones.\nBronze artifacts also have significant meaning and roles in the Han dynasty as well. People used them for funerary purposes which reflect the aesthetic and artistic qualities of the Han dynasty. Many bronze vessels excavated from tombs in Jiangsu Province, China, have various shapes like Ding, Hu, and Xun which represent traditional Chinese aesthete. These vessels are classical representations of Chinese celestial art forms which play a great role in ancient Chinese's communication with spirits of their ancestors. Other than the vessels, bronze weapons, daily items, and musical instruments are also found in royal Han families' tomb in Jiangsu. Being able to put a full set of Bianzhong in ones tomb signifies his or her status and class in the Han dynasty since this particular type of instrument is only acquired and owned by royal and wealth families. Apparently, Bianzhong and music are also used as a path for the Han rulers to communication with their Gods. The excavation of Bianzhong, a typical and royal instrument found in ancient China, emphasizes the development of complex music systems in the Han dynasty. The set of Bianzhong can vary in many cases; for example, a specific excavation of Bianzhong from Jiangsu Province include different sets of bells, like Niuzhong and Yongzhong bells, and many of them appear in animal forms like the dragon, a traditional Chinese spiritual animal.\nShang bronzes became appreciated as works of art from the Song dynasty, when they were collected and prized not only for their shape and design but also for the various green, blue green, and even reddish patinas created by chemical action as they lay buried in the ground. The study of early Chinese bronze casting is a specialized field of art history.\nZhou dynasty (c.\u20091046 \u2013 256 BCE).\nDuring the Zhou period, few sculptures, especially sculptures of human or animal form, are recorded in the extant archaeology, and there does not appear to have been much of a sculptural tradition. Among the very few such depictions known in China before that date: four wooden figurines from Liangdaicun (\u6881\u5e36\u6751) in Hancheng (\u97d3\u57ce), Shaanxi, possibly dating to the 9th century BCE; two wooden human figurines of foreigners possibly representing sedan chair bearers from a Qin state tomb in Longxian (\u96b4\u7e23), Shaanxi, from about 700 BCE; and more numerous statuettes from around 5th century bronze musicians in a miniature house from Shaoxing (\u7d39\u8208) in Zhejiang; a 4th-century human-shaped lamp stand from Pingshan (\u5e73\u5c71) county royal tomb, Hebei. The \"Taerpo horserider\" is a Zhou-era Warrior-State Qin terracotta figurine from a tomb in the Taerpo cemetery (\u5854\u5152\u5761\u5893) near Xianyang in Shaanxi, dated to the 4th\u20133rd century BCE. Another nearly-identical statuette is known, from the same tomb. Small holes in his hands suggest that he was originally holding reins in one hand, and a weapon in the other. This is the earliest known representation of a cavalryman in China.\nChu and Southern culture (c.\u20091030 BC \u2013 223 BC).\nA rich source of art in early China was the state of Chu, which developed in the Yangtze River valley. Excavations of Chu tombs have found painted wooden sculptures, jade disks, glass beads, musical instruments, and an assortment of lacquerware. Many of the lacquer objects are finely painted, red on black or black on red. A site in Changsha, Hunan province, has revealed some of the oldest paintings on silk discovered to date.\nEarly imperial China (221 BCE \u2013 220 CE).\nQin art.\nThe Terracotta Army, inside the Mausoleum of the First Qin Emperor, consists of more than 7,000 life-size tomb terracotta figures of warriors and horses buried with the self-proclaimed first Emperor of China Qin Shi Huang in 210\u2013209 BC. The figures were painted before being placed into the vault. The original colors were visible when the pieces were first unearthed. However, exposure to air caused the pigments to fade, so today the unearthed figures appear terracotta in color. The figures are in several poses including standing infantry and kneeling archers, as well as charioteers with horses. Each figure's head appears to be unique, showing a variety of facial features and expressions as well as hair styles. The spectacular realism displayed by the sculptures is an evidence of the advancement of art during the Qin dynasty. It is without precedent in the historical record of art in East Asia.\nA music instrument called Qin zither was developed during the Qin dynasty.\nThe aesthetic components have always been as important as the functional parts on a musical instrument in Chinese history. The Qin zither has seven strings. Although Qin zither can sometimes remind people of corruptive history times, it is often considered as a delivery of peace and harmony.\nHan art.\nDuring the Qin dynasty, Chinese font, measurement systems, currency were all standardized in order to bring further unification. The Great Wall of China was expanded as a defensive construction against the northern intruders. \nThe Han dynasty was known for jade burial suits. One of the earliest known depictions of a landscape in Chinese art comes from a pair of hollow-tile door panels from a Western Han dynasty tomb near Zhengzhou, dated 60 BCE. A scene of continuous depth recession is conveyed by the zigzag of lines representing roads and garden walls, giving the impression that one is looking down from the top of a hill. This artistic landscape scene was made by the repeated impression of standard stamps on the clay while it was still soft and not yet fired. However, the oldest known landscape art scene tradition in the classical sense of painting is a work by Zhan Ziqian of the Sui dynasty (581\u2013618).\nOther than jade artifacts, bronze is another favorite medium for artists since it is hard and durable. Bronze mirrors have been mass-produced in the Han dynasty (206 BCE\u00a0\u2013 220 CE), and almost every tomb excavated that has been dated as Han dynasty has mirror in the burial. The reflective side is usually made by a composition of bronze, copper, tin, and lead. The word \"mirror\" means \"to reflect\" or \"to look into\" in Chinese, so bronze mirrors have been used as a trope for reflecting the reality. The ancient Chinese believe that mirror can act as a representation of the reality, which could make them more aware of the current situation; also, mirrors are used as a media to convey or present a reflection of the past events. The bronze mirrors made in the Han dynasty always have complex decorations on their non-reflective side; some of them consist narratives that tell stories. The narratives themselves always reflect the common but essential theories to the Han people's lives.\nFirst monumental stone sculptures (117 BCE).\nTerracotta statuettes had been known for a long time in China, but there are no known examples of monumental stone statuary before the stone sculptures at the Mausoleum of Huo Qubing (140\u2013117 BCE), a general of Emperor Han Wudi who went to the western regions to fight the Xiongnu. In literary sources, there is only a single 3rd\u20134th century CE record of a possible earlier example: two alleged monumental stone statues of \"qilin\" (Chinese unicorns) said have been set up on top of the tomb of the First Emperor Qin Shihuang. The most famous of Huo Qubing's statues is that of .\nThe Mausoleum of Huo Qubing (located in Maoling, the Mausoleum of Han Wudi) has 15 more stone sculptures. These are less naturalistic than the \"Horse trampling a Xiongnu\", and tend to follow the natural shape of the stone, with details of the figures only emerging in high-relief. Following these early attempts, the usage of monumental stone statues would only develop from the end of the Western Han to the Eastern Han.\nMonumental stone statuary would become a major art form from the 4\u20136th centuries CE with the onset of monumental Buddhist sculpture in China.\nPeriod of division (220\u2013581).\nInfluence of Buddhism.\nBuddhism arrived in China around the 1st century CE (although there are some traditions about a monk visiting China during Asoka's reign), and through to the 8th century it became very active and creative in the development of Buddhist art, particularly in the area of statuary. Receiving this distant religion, China soon incorporated strong Chinese traits in its artistic expression.\nIn the fifth to sixth century the Northern dynasties, rather removed from the original sources of inspiration, tended to develop rather symbolic and abstract modes of representation, with schematic lines. Their style is also said to be solemn and majestic. The lack of corporeality of this art, and its distance from the original Buddhist objective of expressing the pure ideal of enlightenment in an accessible, realistic manner, progressively led to a research towards more naturalism and realism, leading to the expression of Tang Buddhist art.\nCalligraphy.\nIn ancient China, painting and calligraphy were the most highly appreciated arts in court circles and were produced almost exclusively by amateurs, aristocrats and scholar-officials who alone had the leisure to perfect the technique and sensibility necessary for great brushwork. Calligraphy was thought to be the highest and purest form of painting. The implements were the brush, made of animal hair, and black ink made from pine soot and animal glue. Writing as well as painting was done on silk. But after the invention of paper in the 1st century, silk was gradually replaced by the new and cheaper material. Original writings by famous calligraphers have been greatly valued throughout China's history and are mounted on scrolls and hung on walls in the same way that paintings are.\nWang Xizhi was a famous Chinese calligrapher who lived in the 4th century AD. His most famous work is the \"Lanting Xu\", the preface to a collection of poems. The script was often celebrated as the high point of the semi-cursive \"Running Style\" in the history of Chinese calligraphy.\nWei Shuo was a well-known calligrapher of the Eastern Jin dynasty who established consequential rules about the regular script. Her well-known works include \"Famous Concubine Inscription\" (\u540d\u59ec\u5e16 Ming Ji Tie) and \"The Inscription of Wei-shi He'nan\" (\u885b\u6c0f\u548c\u5357\u5e16 Wei-shi He'nan Tie).\nPainting.\nGu Kaizhi is a celebrated painter of ancient China born in Wuxi. He wrote three books about painting theory: \"On Painting\" (\u756b\u8ad6), \"Introduction of Famous Paintings of Wei and Jin Dynasties\" (\u9b4f\u6649\u540d\u756b\u8a18) and \"Painting Yuntai Mountain\" (\u756b\u96f2\u81fa\u5c71\u8a18). He wrote, \"In figure paintings the clothes and the appearances were not very important. The eyes were the spirit and the decisive factor.\"\nThree of Gu's paintings still survive today: \"Admonitions of the Instructress to the Court Ladies\", \"Nymph of the Luo River\" (\u6d1b\u795e\u8ce6), and \"Wise and Benevolent Women\".\nThere are other examples of Jin dynasty painting from tombs. This includes the \"Seven Sages of the Bamboo Grove,\" painted on a brick wall of a tomb located near modern Nanjing and now found in the Shaanxi Provincial Museum. Each of the figures are labeled and shown either drinking, writing, or playing a musical instrument. Other tomb paintings also depict scenes of daily life, such as men plowing fields with teams of oxen.\nThe Sui and Tang dynasties (581\u2013960).\nBuddhist architecture and sculpture.\nFollowing a transition under the Sui dynasty, Buddhist sculpture of the Tang evolved towards a markedly lifelike expression. As a consequence of the dynasty's openness to foreign trade and influences through the Silk Road, Tang dynasty Buddhist sculpture assumed a rather classical form, inspired by the Greco-Buddhist art of Central Asia.\nHowever, foreign influences came to be negatively perceived towards the end of the Tang dynasty. In the year 845, the Tang emperor Wuzong outlawed all \"foreign\" religions (including Nestorian Christianity, Zoroastrianism and Buddhism) in order to support indigenous Taoism. He confiscated Buddhist possessions and forced the faith to go underground, therefore affecting the ulterior development of the religion and its arts in China.\nGlazed or painted earthenware Tang dynasty tomb figures are famous, and well-represented in museums around the world. Most wooden Tang sculptures have not survived, though representations of the Tang international style can still be seen in Nara, Japan. The longevity of stone sculpture has proved much greater. Some of the finest examples can be seen at Longmen, near Luoyang, Yungang near Datong, and Bingling Temple in Gansu.\nOne of the most famous Buddhist Chinese pagodas is the Giant Wild Goose Pagoda, built in 652 AD.\nPainting.\nBeginning in the Tang dynasty (618\u2013907), the primary subject matter of painting was the landscape, known as shanshui (mountain water) painting. In these landscapes, usually monochromatic and sparse, the purpose was not to reproduce exactly the appearance of nature but rather to grasp an emotion or atmosphere so as to catch the \"rhythm\" of nature.\nPainting in the traditional style involved essentially the same techniques as calligraphy and was done with a brush dipped in black or colored ink; oils were not used. As with calligraphy, the most popular materials on which paintings were made were paper and silk. The finished works were then mounted on scrolls, which could be hung or rolled up. Traditional painting was also done in albums, on walls, lacquer work, and in other media.\nDong Yuan was an active painter in the Southern Tang Kingdom. He was known for both figure and landscape paintings, and exemplified the elegant style which would become the standard for brush painting in China over the next 900 years. As with many artists in China, his profession was as an official where he studied the existing styles of Li Sixun and Wang Wei. However, he added to the number of techniques, including more sophisticated perspective, use of pointillism and crosshatching to build up vivid effect.\nZhan Ziqian was a painter during the Sui dynasty. His only painting in existence is \"Strolling About In Spring\" arranged mountains perspectively. Because pure landscape paintings are hardly seen in Europe until the 17th century, \"Strolling About In Spring\" may well be the world's first landscape painting.\nThe Song and Yuan dynasties (960\u20131368).\nSong painting.\nDuring the Song dynasty (960\u20131279), landscapes of more subtle expression appeared; immeasurable distances were conveyed through the use of blurred outlines, mountain contours disappearing into the mist, and impressionistic treatment of natural phenomena. Emphasis was placed on the spiritual qualities of the painting and on the ability of the artist to reveal the inner harmony of man and nature, as perceived according to Taoist and Buddhist concepts.\nLiang Kai was a Chinese painter who lived in the 13th century (Song dynasty). He called himself \"Madman Liang\", and he spent his life drinking and painting. Eventually, he retired and became a Zen monk. Liang is credited with inventing the Zen school of Chinese art. Wen Tong was a painter who lived in the 11th century. He was famous for ink paintings of bamboo. He could hold two brushes in one hand and paint two different distanced bamboos simultaneously. He did not need to see the bamboo while he painted them because he had seen a lot of them.\nZhang Zeduan was a notable painter for his horizontal \"Along the River During the Qingming Festival\" landscape and cityscape painting. It is considered one of China's most renowned paintings and has had many well-known remakes throughout Chinese history. Other famous paintings include \"The Night Revels of Han Xizai\", originally painted by the Southern Tang artist Gu Hongzhong in the 10th century, while the well-known version of his painting is a 12th-century remake of the Song dynasty. This is a large horizontal handscroll of a domestic scene showing men of the gentry class being entertained by musicians and dancers while enjoying food, beverage, and wash basins provided by maidservants. In 2000, the modern artist Wang Qingsong created a parody of this painting with a long, horizontal photograph of people in modern clothing making similar facial expressions, poses, and hand gestures as the original painting.\nYuan painting.\nWith the fall of the Song dynasty in 1279, and the subsequent dislocation caused by the establishment of the Yuan dynasty by the Mongol conquerors, many court and literary artists retreated from social life, and returned to nature, through landscape paintings, and by renewing the \"blue and green\" style of the Tang era.\nWang Meng was one such painter, and one of his most famous works is the \"Forest Grotto\". Zhao Mengfu was a Chinese scholar, painter and calligrapher during the Yuan dynasty. His rejection of the refined, gentle brushwork of his era in favor of the cruder style of the 8th century is considered to have brought about a revolution that created the modern Chinese landscape painting. There was also the vivid and detailed works of art by Qian Xuan (1235\u20131305), who had served the Song court, and out of patriotism refused to serve the Mongols, instead turning to painting. He was also famous for reviving and reproducing a more Tang dynasty style of painting.\nThe later Yuan dynasty is characterized by the work of the so-called \"Four Great Masters\". The most notable of these was Huang Gongwang (1269\u20131354) whose cool and restrained landscapes were admired by contemporaries, and by the Chinese literati painters of later centuries. Another of great influence was Ni Zan (1301\u20131374), who frequently arranged his compositions with a strong and distinct foreground and background, but left the middle-ground as an empty expanse. This scheme was frequently to be adopted by later Ming and Qing dynasty painters.\nPottery.\nChinese porcelain is made from a hard paste made of the clay kaolin and a feldspar called petuntse, which cements the vessel and seals any pores. \"China\" has become synonymous with high-quality porcelain. Most china pots comes from the city of Jingdezhen in Jiangxi province. Jingdezhen porcelain, under a variety of names, has been central to porcelain production in China since at least the Yuan dynasty.\nLate imperial China (1368\u20131912).\nMing painting.\nUnder the Ming dynasty, Chinese culture bloomed. Narrative painting, with a wider color range and a much busier composition than the Song paintings, was immensely popular during the time.\nWen Zhengming (1470\u20131559) developed the style of the Wu school in Suzhou, which dominated Chinese painting during the 16th century.\nDong Qichang (1555\u20131636) further influenced East Asian art history by absorbing Chan Buddhism ideas and putting forward the \"Southern and Northern Schools\" theory.\nEuropean culture began to make an impact on Chinese art during this period. The Jesuit priest Matteo Ricci visited Nanjing with many Western artworks, which were influential in showing different techniques of perspective and shading.\nEarly Qing painting.\nThe early Qing dynasty developed in two main strands: the Orthodox school, and the Individualist painters, both of which followed the theories of Dong Qichang, but emphasizing very different aspects. Court painting of the Qing dynasty was also greatly influenced by Western artists such as Jean Denis Attiret (1702\u20131768) and Giuseppe Castiglione (1688\u20131766).\nThe Four Wangs, including Wang Jian (1598\u20131677) and Wang Shimin (1592\u20131680), were particularly renowned in the Orthodox school, and sought inspiration in recreating the past styles, especially the technical skills in brushstrokes and calligraphy of ancient masters. The younger Wang Yuanqi (1642\u20131715) ritualized the approach of engaging with and drawing inspiration from a work of an ancient master. His own works were often annotated with his theories of how his painting relates to the master's model.\nThe Individualist painters included Bada Shanren (1626\u20131705) and Shitao (1641\u20131707). They drew more from the revolutionary ideas of transcending the tradition to achieve an original individualistic styles; in this way they were more faithfully following the way of Dong Qichang than the Orthodox school (who were his official direct followers.)\nPainters outside of the literati-scholar and aristocratic traditions also gained renown, with some artists creating paintings to sell for money. These included Ma Quan (late 17th\u201318th century), who depicted common flowers, birds, and insects that were not typical subject matter among scholars. Such painters were, however, not separated from formal schools of painting, but were usually well-versed in artistic styles and techniques. Ma Quan, for example, modelled her brushwork on Song dynasty examples. Simultaneously, the boneless technique (), thought to have originated as a preparatory step when painting gold-line images during the Tang, was continued by painters like Yun Shouping (1633\u20131690) and his descendant Yun Bing.\nAs the techniques of color printing were perfected, illustrated manuals on the art of painting began to be published. \"Jieziyuan Huazhuan\" (Manual of the Mustard Seed Garden), a five-volume work first published in 1679, has been in use as a technical textbook for artists and students ever since.\nLate Qing art.\nNianhua were a form of colored woodblock prints in China, depicting images for decoration during the Chinese New Year. In the 19th century Nianhua were used as news mediums.\nShanghai School.\nThe Shanghai School is a very important Chinese school of traditional arts during the Qing dynasty and the 20th century. Under efforts of masters from this school, traditional Chinese art reached another climax and continued to the present in forms of Chinese painting (\u4e2d\u570b\u756b), or \"guohua\" (\u570b\u756b) for short. The Shanghai School challenged and broke the literati tradition of Chinese art, while also paying technical homage to the ancient masters and improving on existing traditional techniques. Members of this school were themselves educated literati who had come to question their very status and the purpose of art, and had anticipated the impending modernization of Chinese society. In an era of rapid social change, works from the Shanghai School were widely innovative and diverse, and often contained thoughtful yet subtle social commentary. The best known figures from this school are Ren Xiong, Ren Bonian, Zhao Zhiqian, Wu Changshuo, Sha Menghai, Pan Tianshou, Fu Baoshi, He Tianjian, and Xie Zhiliu. Other well-known painters include Wang Zhen, Xugu, Zhang Xiong, Hu Yuan, and Yang Borun.\nPainting.\nTraditional Chinese painting, like Chinese calligraphy, is done with a\nbrush dipped in black or colored ink; oils are not used. As with calligraphy, the most popular materials on which paintings are made of are paper and silk. The finished work can be mounted on scrolls, such as hanging scrolls or handscrolls. Traditional painting can also be done on album sheets, walls, lacquerware, folding screens, and other media.\nThe two main techniques in Chinese painting are:\nArtists from the Han to the Tang dynasties mainly painted the human figure. Much of what is known of early Chinese figure painting comes from burial sites, where paintings were preserved on silk banners, lacquered objects, and tomb walls. Many early tomb paintings were meant to protect the dead or help their souls get to paradise. Others illustrated the teachings of the Chinese philosopher Confucius, or showed scenes of daily life. Most Chinese portraits showed a formal full-length frontal view, and were used in the family in ancestor veneration. Imperial portraits were more flexible, but were generally not seen outside the court, and portraiture formed no part of Imperial propaganda, as in other cultures.\nMany critics consider landscape to be the highest form of Chinese painting. The time from the Five Dynasties period to the Northern Song period (907\u20131127) is known as the \"Great age of Chinese landscape\". In the north, artists such as Jing Hao, Li Cheng, Fan Kuan, and Guo Xi painted pictures of towering mountains, using strong black lines, ink wash, and sharp, dotted brushstrokes to suggest rough rocks. In the south, Dong Yuan, Juran, and other artists painted the rolling hills and rivers of their native countryside in peaceful scenes done with softer, rubbed brushwork. These two kinds of scenes and techniques became the classical styles of Chinese landscape painting.\nSculpture.\nChinese ritual bronzes from the Shang and Western Zhou dynasties come from a period of over a thousand years from c. 1500 BC, and have exerted a continuing influence over Chinese art. They are cast with complex patterned and zoomorphic decoration, but avoid the human figure, unlike the huge figures only recently discovered at Sanxingdui. The spectacular Terracotta Army was assembled for the tomb of Qin Shi Huang, the first emperor of a unified China from 221 to 210 BC, as a grand imperial version of the figures long placed in tombs to enable the deceased to enjoy the same lifestyle in the afterlife as when alive, replacing actual sacrifices of very early periods. Smaller figures in pottery or wood were placed in tombs for many centuries afterwards, reaching a peak of quality in the Tang dynasty tomb figures.\nNative Chinese religions do not usually use cult images of deities, or even represent them, and large religious sculpture is nearly all Buddhist, dating mostly from the 4th to the 14th century, and initially using Greco-Buddhist models arriving via the Silk Road. Buddhism is also the context of all large portrait sculpture; in total contrast to some other areas in medieval China even painted images of the emperor were regarded as private. Imperial tombs have spectacular avenues of approach lined with real and mythological animals on a scale matching Egypt, and smaller versions decorate temples and palaces. Small Buddhist figures and groups were produced to a very high quality in a range of media, as was relief decoration of all sorts of objects, especially in metalwork and jade. Sculptors of all sorts were regarded as artisans and very few names are recorded.\nCeramics.\nChinese ceramic ware shows a continuous development since the pre-dynastic periods, and is one of the most significant forms of Chinese art. China is richly endowed with the raw materials needed for making ceramics. The first types of ceramics were made during the Palaeolithic era, and in later periods range from construction materials such as bricks and tiles, to hand-built pottery vessels fired in bonfires or kilns, to the sophisticated Chinese porcelain wares made for the imperial court. Most later Chinese ceramics, even of the finest quality, were made on an industrial scale, thus very few individual potters or painters are known. Many of the most renowned workshops were owned by or reserved for the Emperor, and large quantities of ceramics were exported as diplomatic gifts or for trade from an early date.\nDecorative arts.\nAs well as porcelain, a wide range of materials that were more valuable were worked and decorated with great skill for a range of uses or just for display. Chinese jade was attributed with magical powers, and was used in the Stone and Bronze Ages for large and impractical versions of everyday weapons and tools, as well as the \"bi\" disks and \"cong\" vessels. Later a range of objects and small sculptures were carved in jade, a difficult and time-consuming technique. Bronze, gold and silver, rhinoceros horn, Chinese silk, ivory, lacquer and carved lacquer, cloisonne enamel and many other materials had specialist artists working in them. Cloisonne underwent an interesting process of artistic hybridization in China, particularly in the pieces promoted by missionaries and Chinese Christian communities.\nFolding screens () are often decorated with beautiful art; major themes include mythology, scenes of palace life, and nature. Materials such as wood panel, paper and silk are used in making folding screens. They were considered ideal ornaments for many painters to display their paintings and calligraphy. Many artists painted on paper or silk and applied it onto the folding screen. There were two distinct artistic folding screens mentioned in historical literature of the era.\nArchitecture.\nChinese architecture refers to a style of architecture that has taken shape in East Asia over many centuries. Especially Japan, Korea, Vietnam and Ryukyu. The structural principles of Chinese architecture have remained largely unchanged, the main changes being only the decorative details. Since the Tang dynasty, Chinese architecture has had a major influence on the architectural styles of Korea, Vietnam, and Japan.\nFrom the Neolithic era Longshan culture and Bronze Age era Erlitou culture, the earliest rammed earth fortifications exist, with evidence of timber architecture. The subterranean ruins of the palace at Yinxu dates back to the Shang. In historic China, architectural emphasis was laid upon the horizontal axis, in particular the construction of a heavy platform and a large roof that floats over this base, with the vertical walls not as well emphasized. This contrasts Western architecture, which tends to grow in height and depth. Chinese architecture stresses the visual impact of the width of the buildings. The deviation from this standard is the tower architecture of the Chinese tradition, which began as a native tradition and was eventually influenced by the Buddhist building for housing religious sutras\u2014the stupa\u2014which came from Nepal. Ancient Chinese tomb model representations of multiple story residential towers and watchtowers date to the Han. However, the earliest extant Buddhist Chinese pagoda is the Songyue Pagoda, a tall circular-based brick tower built in Henan in the year 523 CE From the 6th century onwards, stone-based structures become more common, while the earliest are from stone and brick arches found in Han dynasty tombs. The Zhaozhou Bridge built from 595 to 605 CEis China's oldest extant stone bridge, as well as the world's oldest fully stone open-spandrel segmental arch bridge.\nThe vocational trade of architect, craftsman, and engineer was not as highly respected in premodern Chinese society as the scholar-bureaucrats who were drafted into the government by the civil service examination system. Much of the knowledge about early Chinese architecture was passed on from one tradesman to his son or associative apprentice. However, there were several early treatises on architecture in China, with encyclopedic information on architecture dating back to the Han dynasty. The height of the classical Chinese architectural tradition in writing and illustration can be found in the \"Yingzao Fashi\", a building manual written by 1100 and published by Li Jie (1065\u20131110) in 1103. In it there are numerous and meticulous illustrations and diagrams showing the assembly of halls and building components, as well as classifying structure types and building components.\nThere were certain architectural features that were reserved solely for buildings built for the Emperor of China. One example is the use of yellow roof tiles; yellow having been the Imperial color, yellow roof tiles still adorn most of the buildings within the Forbidden City. The Temple of Heaven, however, uses blue roof tiles to symbolize the sky. The roofs are almost invariably supported by brackets, a feature shared only with the largest of religious buildings. The wooden columns of the buildings, as well as the surface of the walls, tend to be red in colour.\nMany current Chinese architectural designs follow post-modern and western styles.\nChinoiserie.\nChinoiserie is the European interpretation and imitation of Chinese and East Asian artistic traditions, especially in the decorative arts, garden design, architecture, literature, theatre, and music. The aesthetic of Chinoiserie has been expressed in different ways depending on the region. It is related to the broader current of Orientalism, which studied Far East cultures from a historical, philological, anthropological, philosophical, and religious point of view. First appearing in the 17th century, this trend was popularized in the 18th century due to the rise in trade with China and East Asia.\nAs a style, chinoiserie is related to the rococo style. Both styles are characterized by exuberant decoration, asymmetry, a focus on materials, and stylized nature and subject matter that focuses on leisure and pleasure. Chinoiserie focuses on subjects that were thought by colonial-era Europeans to be typical of Chinese culture.\nModern Chinese art.\nNew China art (1912\u20131949).\nModern art movement.\nThe movement to modernize Chinese art started toward the end of the Qing dynasty. The traditional art form started to lose its appeal as the feudalistic structure of the society was dissolving. The modern view of the world had to be expressed in a different form. The explorations went on two main paths: one was to draw from the past to enrich the present ()*, the other was to \"learn the new methods\" ().*\nDraw from the past.\nThe literati art for the social elite was not appealing to the bourgeois patrons. Wu Changshuo (1844\u20131927) was among the Shanghai-based artists responsible for flowers and plants as the subject matter. His paintings used bold colors and energetic brush strokes, making them more accessible to the general public. Qi Baishi (1864\u20131957) painted images like crabs and shrimps that were even more approachable to the common people. Huang Binhong (1865\u20131955) denounced the literati paintings of the Qing dynasty and created his own style of landscape paintings by extensive investigations in Chinese art history. Zhang Daqian (1899\u20131983) used wall paintings in the Dunhuang () caves to help him move beyond the literati tradition.\nLearn new methods.\nThe Lingnan School () made some borrowings from the language of Western art in their ink paintings. Gao Jianfu (1879\u20131951), one of the founders of Lingnan School, was an active participant in the revolutionary movement of Sun Yat-sen (1866\u20131925). He was innovative in that he intended to use his paintings to highlight national issues, a medium for positive change in society.\nA more radical style change started with Kang Youwei (1858\u20131927), a reformer who admired the more reality-based art of the Song dynasty. He believed that Chinese art could be rejuvenated by employing the reality-oriented art techniques of Europe. Xu Beihong (1895\u20131953) took this idea to heart and went to Paris to acquire the necessary skills. Liu Haisu (1896\u20131994), on the other hand, went to Japan to learn western techniques. Both Xu, and Liu became presidents of prestigious art schools, instilling new concepts and skills in the next generation of artists. Cai Yuanpei (1868\u20131940) was one of the leaders in the New Culture Movement (). Those involved believed that intellectual activities should benefit all, not just the elites. Cai's belief that art could play a public, socially reformist role was adopted by Lin Fengmian (1900\u20131991).\nTogether with Yan Wenliang (1893\u20131988), Xu, Liu, and Lin were considered the \"Four Great Academy Presidents\" (), who spearheaded the national modern art movement. However, the subsequent upheaval caused by the Sino\u2013Japanese war and the civil war did not allow this movement to grow. The Chinese modern art movement after the war developed differently in the four the regions: the Mainland, Taiwan, Hong Kong, and overseas.\nPostwar Chinese art (1949\u20131976).\nThe postwar era is roughly from 1949, the end of Chinese civil war, to 1976, the opening of mainland China to the outside world.\nMainland.\nThe postwar era in mainland China could be divided into two periods: 1949 to 1966 is generally called \"The 17 Years\"; 1966 to 1976 is the period of the \"Cultural Revolution\".\nThe 17 Years.\nChinese artists adopted social realism as a form of expression; it was a combination of revolutionary realism and revolutionary romanticism. Artwork was not valued on its own terms but was subservient to a political purpose. According to Mao Zedong, art should be a \"powerful weapon for uniting and educating the people, fighting and destroying the enemy\". Praising political leaders and celebrating the achievements of socialism became the theme of all artwork. Western art forms, including Cubism, Fauvism, Abstraction, and Expressionism were deemed superficial and were categorized as formalism.\nThe biggest blow to art was the Anti-Rightist Campaign of 1957. Artists who were labeled as rightists were stripped of their right to create and even their jobs, and worse, the social standing of the artists and their families was placed at the lowest level, causing great mental suffering.\nSome influential paintings from this period are:\nCultural Revolution.\nThese ten years could also be called the \"Ten Years of Calamity\" (). In order to destroy everything that supported the old social order, countless temples, historic sites, artworks, and books were ravaged and burnt. During this period the portrait of Mao and propaganda posters of revolution were everywhere. Anything that was remotely suspected of being out of line was destroyed, and the person behind it was prosecuted. For example, \"Owl\" by Huang Yongyu had one eye open and one eye closed; it was deemed an expression of dissatisfaction with current events. Zong Qixiang's painting, which shows three tigers, was deemed critical of the leader Lin Biao, whose name contained a character that had three tigers in it. \"Residual Lotus\" by Li Kuchan had eight lotus flowers; it was deemed to be critical of the eight communist approved movies (). Many prominent artists were persecuted during this time. For example, Yan, Xu, Liu, and Lin, the \"Four Great Academy Presidents\" (except for Xu who died before the Cultural Revolution), were all prosecuted and jailed, and all their work was destroyed during this time.\nHowever, despite the difficult environment, some noteworthy paintings were created. The following are some examples:\nHong Kong.\nHong Kong was a British colony from 1842 to 1997. The local art organizations were mostly run by Westerners who outnumbered Chinese painters until a large migration of Chinese from Southern China during Sino\u2013Japanese War. Innovative art colleges were established after the war. The shows organized by local artists started in the early 1960s. After a reaction against the traditional Western artistic practices of the 1940s and the 1950s, some experimental works that combined both western and eastern techniques were made. Then came the call for a return to Chinese traditional art and the creation of forms of art that Hong Kong could call its own. The trend was led by Lui Shou Kwan. Some western concepts were incorporated into his Chinese ink paintings.\nOverseas.\nParis.\nMany Chinese artists went to study western art in Paris in the early 1900s, for example: Fang Ganmin (), Wu Dayu (), Ong Schan Tchow (), Lin Fengmian (), Yan Wenliang (), Wu Guanzhong (), Zao Wou-Ki (). All except Zao completed their education before 1949 and returned to become leaders in the modern art movement. (Zao happened to be in Paris in 1949 and did not return.) Some Chinese artists went to stay there because of the rich international art environment, for example: Sanyu, Pan Yuliang (), Chu Teh-Chun (). Zao, Sanyu, Pang, and Chu all had shows in Paris and the Republic. All their paintings had varying degrees of Chinese elements in them. These artists not only had a profound influence in Chinese modern art, but they also continued to engage Parisians with modern art from the East.\nUnited States.\nLi Tiefu (1869\u20131952) was an accomplished oil painter educated in Canada and the United States. He was an active participant in the revolutionary movement of Sun Yat-sen (1866\u20131925).\nZeng Youhe (1925\u20132017) was born in Beijing. She started receiving international recognition in 1946, when Michael Sullivan began praising and writing about her work. Zeng moved to Honolulu in 1949 and visited Hong Kong and Taiwan in 1960. Like those of the Fifth Moon Group (), her paintings were abstract; but the flavor of traditional Chinese ink paintings were not as pronounced.\nTaiwan.\nBecause of its history, traditional Chinese art does not have strong roots in Taiwan. The art forms in Taiwan were generally decorative, until youths growing up under the Japanese occupation received formal art education in Japan. Not burdened with traditional art form, their exploration generally followed the path of \"learning the new methods\" (). When the Nationalists arrived in Taiwan, a group of ambitious youths who came with the Nationalists continued the modern art movement. The most notable were the Fifth Moon Group () and the Ton-Fan Art Group ().\nFifth Moon Group.\nThe original members of the group were alumni with art majors from the Academic Teachers College (), the only university with an art major at the time. Their first intention was to show that the effort to create new art was worthwhile in itself, even if it did not directly enhance art pedagogy. Later, it became a movement to modernize Chinese art.\nThe members of the Fifth Moon Group studied western art movements, and concluded that the abstract art form was the best medium for modern Chinese art. They felt the best the Chinese paintings were ones that de-emphasized realistic representation, and emphasized atmosphere and \"vividness\", which comes from the brush strokes and the natural interaction between ink and paper. To further that idea, one does not need representation of objects in painting, or strictly use ink and paper. The beauty of a painting can be appreciated directly from the forms, textures, and colors on the canvas without their relation to real objects.\nThe group was active from 1957 to 1972. The main members are Liu Guosong, Chuang Che (), Hu Chi-Chung (), Fong Chung-ray (), and Han Hsiang-ning.\nTon-Fan Art Group ().\nThe members of this group were students who attended private art classes offered by Li Zhongsheng (), a mainland-born artist who had been one of the active participants in the modern art movement. He and a number of mainland artists who painted in a western style continued the modern art movement by publishing magazines and writing articles to introduce modern art to Taiwan. His teaching style was unconventional and socratic in nature.\nThe original intention of the group was to introduce modern art to the public. They believed there should be no restriction on the form or style of a modern Chinese painting, as long as the painting expressed meaning that was Chinese in nature. The group was active from 1957 to 1971. The main members were: Ho Kan (), Li Yuan-chia, Wu Hao (), Oyan Wen-Yuen (), Hsia Yan (), Hsiao Chin (), Tommy Chen (), and Hsiao Ming-Hsien (). The following are a sample of their paintings from that period:\nRedevelopment (mid-1980s \u2013 1990s).\nContemporary art.\nContemporary Chinese art () often referred to as Chinese avant-garde art, continued to develop since the 1980s as an outgrowth of modern art developments post-Cultural Revolution.\nContemporary Chinese art fully incorporates painting, film, video, photography, and performance. Until recently, art exhibitions deemed controversial have been routinely shut down by police, and performance artists in particular faced the threat of arrest in the early 1990s. More recently there has been greater tolerance by the Chinese government, though many internationally acclaimed artists are still restricted from media exposure at home or have exhibitions ordered closed. Leading contemporary visual artists include Ai Weiwei, Cai Guoqiang, Cai Jin, Chan Shengyao, Concept 21, Ding Yi, Fang Lijun, Fu Wenjun, He Xiangyu, Huang Yan, Huang Yong Ping, Han Yajuan, Kong Bai Ji, Li Hongbo, Li Hui, Liu Bolin, Lu Shengzhong, Ma Liuming, Qiu Deshu, Qiu Shihua, Shen Fan, Shen Shaomin, Shi Jinsong, Song Dong, Li Wei, Wang Guangyi, Wenda Gu, Xu Bing, Yang Zhichao, Zhan Wang, Zheng Lianjie, Zhang Dali, Zhang Xiaogang, Zhang Huan, Zhu Yu, Wu Shaoxiang, Ma Kelu, Ding Fang, Shang Yang, Gao Minglu and Guo Jian.\nVisual art.\nBeginning in the late 1980s, there was unprecedented exposure for younger Chinese visual artists in the west to some degree through the agency of curators based outside the country such as Hou Hanru. Local curators within the country such as Gao Minglu and critics such as Li Xianting reinforced this promotion of particular brands of painting that had recently emerged, while also spreading the idea of art as a strong social force within Chinese culture. There was some controversy as critics identified these imprecise representations of contemporary Chinese art as having been constructed out of personal preferences, a kind of programmatized artist-curator relationship that only further alienated the majority of the avant-garde from Chinese officialdom and western art market patronage.\nArt market.\nToday, the market for Chinese art, both antique and contemporary, is widely reported to be among the hottest and fastest-growing in the world, attracting buyers all over the world. The \"Voice of America\" reported in 2006 that modern Chinese art is raking in record prices both internationally and in domestic markets, some experts even fearing the market might be overheating. \"The Economist\" reported that Chinese art has become the latest darling in the world market according to the record sales from Sotheby's and Christie's, the biggest fine-art auction houses.\nContemporary Chinese art saw record sales throughout the 2000s. In 2007, it was estimated that 5 of the world's 10 best selling living artists at art auction were from China, with artists such as Zhang Xiaogang whose works were sold for a total of $56.8 million at auction in 2007. In terms of buying-market, China overtook France in the late 2000s as the world's third-largest art market, after the United States and the United Kingdom, due to the growing middle-class in the country. Sotheby's noted that contemporary Chinese art has rapidly changed the contemporary Asian art world into one of the most dynamic sectors on the international art market. During the global economic crisis, the contemporary Asian art market and the contemporary Chinese art market experienced a slow down in late 2008. The market for Contemporary Chinese and Asian art saw a major revival in late 2009 with record level sales at Christie's.\nFor centuries largely made-up of European and American buyers, the international buying market for Chinese art has also begun to be dominated by Chinese dealers and collectors in recent years. It was reported in 2011, China has become the world's second biggest market for art and antiques, accounting for 23 percent of the world's total art market, behind the United States (which accounts for 34 percent of the world's art market). Another transformation driving the growth of the Chinese art market is the rise of a clientele no longer mostly European or American. New fortunes from countries once thought of as poor often prefer non-Western art; a large gallerist in the field has offices in both New York and Beijing, but clients mainly hailing from Latin America, Asia and the Middle East.\nOne of the areas that has revived art concentration and also commercialized the industry is the 798 Art District in Dashanzi of Beijing. The artist Zhang Xiaogang sold a 1993 painting for US$2.3 million in 2006, which included blank faced Chinese families from the Cultural Revolution era, while Yue Minjun's work \"Execution\" in 2007 was sold for a then record of nearly $6 million at Sotheby's. Collectors including Stanley Ho, the owner of the Macau Casinos, investment manager Christopher Tsai, and casino developer Steve Wynn, would capitalize on the art trends. Items such as Ming dynasty vases and assorted Imperial pieces were auctioned off.\nOther art works were sold in places such as Christie's including a Chinese porcelain piece with the mark of the Qianlong Emperor sold for HKD $ $151.3 million. Sotheby's and Christie's act as major market platforms for classical Chinese porcelain art pieces to be sold, including Ming dynasty, Xuande mark and period (1426\u201335) Blue and White jar (Five-Clawed Dragon Print), which was auctioned for Approx. USD 19,224,491.2, through Christie's in Spring 2016 The \"International Herald Tribune\" reported that Chinese porcelains were fought over in the art market as \"if there was no tomorrow\".\nA 1964 painting by Li Keran \"All the Mountains Blanketed in Red\" was sold for HKD $35 million. Auctions were also held at Sotheby's where Xu Beihong's 1939 masterpiece \"Put Down Your Whip\" sold for HKD $72 million. The industry is not limited to fine arts, as many other types of contemporary pieces were also sold. In 2000, a number of Chinese artists were included in Documenta and the Venice Biennale of 2003. China now has its own major contemporary art showcase with the Venice Biennale. Fuck Off was a notorious art exhibition which ran alongside the Shanghai Biennial Festival in 2000 and was curated by independent curator Feng Boyi and contemporary artist Ai Weiwei.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66299", "revid": "46051904", "url": "https://en.wikipedia.org/wiki?curid=66299", "title": "Lockheed Martin F-22 Raptor", "text": "American stealth air superiority fighter\nThe Lockheed Martin/Boeing F-22 Raptor is an American twin-engine, jet-powered, all-weather, supersonic stealth fighter aircraft. As a product of the United States Air Force's Advanced Tactical Fighter (ATF) program, the aircraft was designed as an air superiority fighter, but also incorporates ground attack, electronic warfare, and signals intelligence capabilities. The prime contractor, Lockheed Martin, built most of the F-22 airframe and weapons systems and conducted final assembly, while program partner Boeing provided the wings, aft fuselage, avionics integration, and training systems.\nFirst flown in 1997, the F-22 descended from the Lockheed YF-22 and was variously designated F-22 and F/A-22 before it formally entered service in December 2005 as the F-22A. It replaced the F-15 Eagle in most active duty U.S. Air Force (USAF) squadrons. Although the service had originally planned to buy a total of 750 ATFs to replace its entire F-15 fleet, it later scaled down to 381 and the program was ultimately cut to 195 aircraft \u2013 187 of them operational models \u2013 in 2009 due to political opposition from high costs, a perceived lack of air-to-air threats at the time of production, and the development of the more affordable and versatile F-35 Lightning II. The last aircraft was delivered in 2012.\nThe F-22 is a critical component of the USAF's tactical airpower as its most advanced air superiority fighter. While it had a protracted development and initial operational difficulties, the aircraft became the service's leading counter-air platform. Its deployments include Okinawa, the Middle East, and NATO's eastern flank. Although designed for air superiority operations, the F-22 has also carried out and supported airstrikes in Syria, Afghanistan, and Iran. The F-22 is expected to remain a cornerstone of the USAF's fighter fleet until its succession by the Boeing F-47 expected around 2030.\nDevelopment.\nOrigins.\nThe F-22 originated from the Advanced Tactical Fighter (ATF) program that the U.S. Air Force (USAF) initiated in 1981 to replace the F-15 Eagle and F-16 Fighting Falcon. Intelligence reports indicated that their effectiveness would be eroded by emerging worldwide threats emanating from the Soviet Union, including new developments in surface-to-air missile systems for integrated air defense networks, the introduction of the Beriev A-50 \"Mainstay\" airborne warning and control system (AWACS), and the proliferation of the Sukhoi Su-27 \"Flanker\" and Mikoyan MiG-29 \"Fulcrum\" class of fighter aircraft. Code-named \"Senior Sky\", the ATF would become an air superiority fighter program influenced by these threats; in the potential scenario of a Soviet and Warsaw Pact invasion in Central Europe, the ATF was envisaged to support the air-land battle by spearheading offensive and defensive counter-air operations (OCA/DCA) in this highly contested environment that would then enable following echelons of NATO strike and attack aircraft to perform air interdiction against ground formations; to do so, the ATF would make an ambitious leap in capability and survivability by taking advantage of the new technologies in fighter design on the horizon, including composite materials, lightweight alloys, advanced flight control systems and avionics, more powerful propulsion systems for supersonic cruise (or supercruise) around Mach 1.5, and stealth technology for low observability.\nThe USAF published an ATF request for information (RFI) to the aerospace industry in May 1981, and following a period of concept and specification development, the ATF System Program Office (SPO) issued the demonstration and validation (Dem/Val) request for proposals (RFP) in September 1985, with requirements placing strong emphasis on stealth, supersonic cruise and maneuver. The RFP saw some alterations after its initial release, including more stringent signature reduction requirements in December 1985 and the addition of the requirement for flying technology demonstrator prototypes in May 1986. Owing to the immense investments required to develop the advanced technologies, teaming among companies was encouraged. Of the seven bidding companies, Lockheed and Northrop were selected on 31 October 1986 as Dem/Val finalists. Lockheed, through its Skunk Works division at Burbank, California, teamed with Boeing and General Dynamics while Northrop teamed with McDonnell Douglas. These two contractor teams undertook a 50-month Dem/Val phase to compete for ATF full-scale development, culminating in the flight test of two technology demonstrator prototypes, the Lockheed YF-22 and Northrop YF-23; while they represented competing designs, the prototypes were meant for demonstrating concept viability and risk mitigation rather than a competitive flyoff. Concurrently, Pratt &amp; Whitney and General Electric competed for the ATF engines.\nDem/Val was focused on system engineering, technology development plans, and risk reduction over point aircraft designs; in fact, after down-select, the Lockheed team completely redesigned the airframe configuration in summer 1987 due to weight analysis, with notable changes including the wing planform from swept trapezoidal to diamond-like delta and a reduction in forebody planform area. The team extensively used analytical and empirical methods including computational fluid dynamics and computer-aided design, wind tunnel testing (18,000 hours for Dem/Val), and radar cross-section (RCS) calculations and pole testing. Avionics were tested in ground prototypes and flying laboratories. During Dem/Val, the SPO used trade studies from both teams to review the ATF system specifications and adjust or delete requirements that were significant weight and cost drivers while having marginal value. The short takeoff and landing (STOL) requirement was relaxed to delete thrust-reversers, saving substantial weight. Side looking radars and the dedicated infrared search and track (IRST) system were eventually removed as well, although space and cooling provisions were retained to allow for their later addition. The ejection seat was downgraded from a fresh design to the existing ACES II. Despite efforts by both teams to rein in weight, the takeoff gross weight estimates grew from , resulting in engine thrust requirement increasing from class.\nEach team built two prototype air vehicles for Dem/Val, one for each engine option. The YF-22 had its maiden flight on 29 September 1990 and, in testing, successfully demonstrated supercruise, high angle-of-attack maneuvers, and the firing of air-to-air missiles from internal weapons bays. After the flight test of the demonstrator prototypes at Edwards Air Force Base, the teams submitted the results and their full-scale development design proposals \u2013 or Preferred System Concept \u2013 in December 1990; on 23 April 1991, the Secretary of the USAF, Donald Rice, announced the Lockheed team and Pratt &amp; Whitney as the winners of the ATF and engine competitions. Both designs met or exceeded all performance requirements; the YF-23 was considered stealthier and faster, but the YF-22, with its thrust vectoring nozzles, was more maneuverable as well as less expensive and risky, having flown considerably more test sorties and hours than its counterpart. The press also speculated that the Lockheed team's design was more adaptable to the Navy Advanced Tactical Fighter (NATF) for replacing the F-14 Tomcat, but by fiscal year (FY) 1992, the U.S. Navy had abandoned NATF due to cost.\nFull-scale development.\nThe program formally moved to full-scale development, or Engineering &amp; Manufacturing Development (EMD), in August 1991. The production F-22 design (internally designated Configuration 645) had also evolved to have notable differences from the YF-22, which was immature due to being frozen relatively soon after the complete redesign in the summer of 1987. While the overall layout was similar, the external geometry saw significant alterations; the wing's leading edge sweep angle was decreased from 48\u00b0 to 42\u00b0, while the vertical stabilizers were shifted rearward and decreased in area by 20%. The radome shape was changed for better radar performance, the wingtips were clipped for antennas, and the dedicated airbrake was eliminated. To improve pilot visibility and aerodynamics, the canopy was moved forward and the engine inlets moved rearward . The shapes of the fuselage, wing, and stabilator trailing edges were refined to improve aerodynamics, strength, and stealth characteristics. The internal structural design was refined and reinforced, with the production airframe designed for a service life of 8,000 hours. The revised shaping was validated with over 17,000 additional hours of wind tunnel testing and further RCS testing at Helendale, California and the USAF RATSCAT range before the first flight. Increasing weight during EMD due to demanding ballistic survivability requirements and added capabilities caused slight reductions in projected range and maneuver performance.\nAside from advances in air vehicle and propulsion technology, the F-22's avionics were unprecedented in complexity and scale for a combat aircraft, with the integration of multiple sensors systems and antennas, including electronic warfare, communication/navigation/identification (CNI), and software of 1.7\u00a0million lines of code written in Ada. Avionics often became the pacing factor of the whole program. In light of rapidly advancing computing and semiconductor technology, the avionics was to employ the Department of Defense's (DoD) \"PAVE PILLAR\" systems architecture and Very High Speed Integrated Circuit (VHSIC) program technology; the computing and processing requirements were equivalent to multiple contemporary supercomputers to achieve sensor fusion. To enable early looks and troubleshooting for mission software development, the software was ground-tested in Boeing's Avionics Integration Laboratory (AIL) and flight-tested on a Boeing 757 modified with F-22 avionics and sensors, called Flying Test Bed (FTB). Because much of the F-22's avionics design occurred in the 1990s as the electronics industry was shifting from military to commercial applications as the predominant market, avionics upgrade efforts was initially difficult and protracted due to changing industry standards; for instance, C/C++ rather than Ada became predominant programming languages.\nThe roughly equal division of work amongst the team largely carried through from Dem/Val to EMD, with prime contractor Lockheed responsible for the forward fuselage and control surfaces, General Dynamics for the center fuselage, and Boeing for aft fuselage and wings. Lockheed acquired General Dynamics' fighter portfolio at Fort Worth, Texas in 1993 and thus had the majority of the airframe manufacturing, and merged with Martin Marietta in 1995 to form Lockheed Martin. While Lockheed primarily performed Dem/Val work at its Skunk Works sites in Burbank and Palmdale, California, it shifted its program office and EMD work from Burbank to Marietta, Georgia, where it performed final assembly; Boeing manufactured airframe components, performed avionics integration and developed the training systems in Seattle, Washington. The EMD contract originally ordered seven single-seat F-22As and two twin-seat F-22Bs, although the latter was canceled in 1996 to reduce development costs and the orders were converted to single seaters. The first F-22A, an EMD aircraft with tail number 91-4001, was unveiled at Air Force Plant 6 in Dobbins Air Reserve Base in Marietta on 9 April 1997 where it was officially named \"Raptor\". The aircraft first flew on 7 September 1997, piloted by chief test pilot Alfred \"Paul\" Metz. The Raptor's designation was briefly changed to F/A-22 starting in September 2002, mimicking the Navy's F/A-18 Hornet and intended to highlight a planned ground-attack capability amid debate over the aircraft's role and relevance. The F-22 designation was reinstated in December 2005, when the aircraft entered service.\nThe F-22 flight test program consisted of flight sciences, developmental test (DT), and initial operational test and evaluation (IOT&amp;E) by the 411th Flight Test Squadron (FLTS) at Edwards AFB, California, as well as follow-on OT&amp;E and development of tactics and operational employment by the 422nd Test and Evaluation Squadron (TES) at Nellis AFB, Nevada. Nine EMD jets assigned to the 411th FLTS would participate in the test program under the Combined Test Force (CTF) at Edwards. The first two aircraft conducted envelope expansion testing, such as flying qualities, air vehicle performance, propulsion, and stores separation. The third aircraft, the first to have production-level internal structure, tested flight loads, flutter, and stores separation, while two non-flying F-22s were built for testing static loads and fatigue. Subsequent EMD aircraft and the Boeing 757 FTB tested avionics, environmental qualifications, and observables, with the first combat-capable Block 3.0 software flying in 2001. Air vehicle testing resulted in several structural design modifications and retrofits for earlier lots, including tail fin strengthening to resolve buffeting in certain conditions. Raptor 4001 was retired from flight testing in 2000 and subsequently sent to Wright-Patterson AFB for survivability testing, including live fire testing and battle damage repair training. Other retired EMD F-22s were repurposed as maintenance trainers.\nBecause the F-22 had been designed to defeat contemporary and projected Soviet fighters, the end of the Cold War and the dissolution of the Soviet Union in 1991 had major impacts on program funding; the DoD reduced its urgency for new weapon systems and the following years would see successive reductions in its budget. This resulted in the F-22's EMD being rescheduled and lengthened multiple times. Furthermore, the aircraft's sophistication and numerous technological innovations required extensive testing, which exacerbated the cost overruns and delays, especially from mission avionics. Some capabilities were also deferred to post-service upgrades, reducing the upfront cost but increasing total program cost. The program transitioned to full-rate production in March 2005 and completed EMD that December, after which the test force had flown 3,496 sorties for over 7,600 flight hours. As the F-22 was designed for upgrades throughout its lifecycle, the 411th FLTS and 422nd TES continued the DT/OT&amp;E and tactics development of these upgrades. Derivatives such as the X-44 thrust vectoring research aircraft and the FB-22 medium-range regional bomber were proposed in the late 1990s and early 2000s, although these were eventually abandoned. In 2006, the F-22 development team won the Collier Trophy, American aviation's most prestigious award. Due to the aircraft's sophisticated capabilities, contractors have been targeted by cyberattacks and technology theft.\nProduction and procurement.\nThe USAF originally envisioned ordering 750 ATFs at a total program cost of $44.3\u00a0billion and procurement cost of $26.2\u00a0billion in FY 1985 dollars, with production beginning in 1994 and service entry in the mid-to-late 1990s. The 1990 Major Aircraft Review (MAR) led by Secretary of Defense Dick Cheney reduced this to 648 aircraft beginning in 1996 and service entry in the early-to-mid 2000s. After the end of the Cold War, this was further curtailed to 442 in the 1993 Bottom-Up Review while the USAF eventually set its requirement to 381 to support its Air Expeditionary Force structure with the last deliveries in 2013. Throughout development and production, the program was continually scrutinized for its costs and less expensive alternatives such as modernized F-15 or F-16 variants were being proposed, even though the USAF considered the F-22 to provide the greatest capability increase against peer adversaries for the investment. However, funding instability had reduced the total to 339 by 1997 and production was nearly halted by Congress in 1999. Although funds were eventually restored, the planned number continued to decline due to delays and cost overruns during EMD, slipping to 277 by 2003. In 2004, with its focus on asymmetric counterinsurgency warfare in Iraq and Afghanistan, the DoD under Secretary Donald Rumsfeld further cut procurement to 183 production aircraft, despite the USAF's requirement for 381; funding for this number was reached by a multi-year procurement contract awarded in 2006, with aircraft distributed to seven combat squadrons; total program cost was projected to be $62\u00a0billion (equivalent to approximately $ in 2024). In 2008, the Congressional defense spending bill raised the number to 187.\nF-22 production would support over 1,000 subcontractors and suppliers from 46 states and up to 95,000 jobs, and spanned 15 years at a peak rate of roughly two airplanes per month, about half of the initially planned rate from the 1990 MAR; after EMD aircraft contracts, the first production lot was awarded in September 2000. As production wound down in 2011, the total program cost was estimated to be about $67.3\u00a0billion (about $360\u00a0million for each production aircraft delivered), with $32.4\u00a0billion spent on Research, Development, Test, and Evaluation (RDT&amp;E) and $34.9\u00a0billion on procurement and military construction in then year dollars. The incremental cost for an additional F-22 was estimated at $138\u00a0million (equivalent to approximately $ in 2024) in 2009.\nIn total, 195 F-22s were built. The first two were EMD aircraft in the Block 1.0 configuration for initial flight testing and envelope expansion, while the third was a Block 2.0 aircraft built to represent the internal structure of production airframes and enabled it to test full flight loads. Six more EMD aircraft were built in the Block 10 configuration for development and upgrade testing, with the last two considered essentially production-quality jets. Production for operational squadrons consisted of 74 Block 10/20 training aircraft and 112 Block 30/35 combat aircraft for a total of 186 (or 187 when accounting for Production Representative Test Vehicles and certain EMD jets); one of the Block 30 aircraft is dedicated to flight sciences at Edwards AFB. By 2020, Block 20 aircraft from Lot 3 onward were upgraded to Block 30 standards under the Common Configuration Plan, increasing the Block 30/35 fleet to 149 aircraft while 37 remained in the Block 20 configuration for training.\nBan on exports.\nIn order to prevent the inadvertent disclosure of the aircraft's stealth technology and classified capabilities to U.S. adversaries, annual DoD appropriations acts since FY1998 have included a provision prohibiting the use of funds made available in each act to approve or license the sale of the F-22 to any foreign government. Customers for U.S. fighters are acquiring earlier designs such as the F-15 Eagle and F-16 Fighting Falcon or the newer F-35 Lightning II, which contains technology from the F-22 but was designed to be cheaper, more flexible, and available for export. In September 2006, Congress upheld the ban on foreign F-22 sales. Despite the ban, the 2010 defense authorization bill included provisions requiring the DoD to report on the costs and feasibility for an F-22 export variant, and another report on the effect of export sales on the U.S. aerospace industry.\nSome Australian defense officials and politicians have expressed interest in procuring the F-22; in 2008, the Chief of the Defence Force, Air Chief Marshal Angus Houston, stated that the aircraft was being considered by the Royal Australian Air Force (RAAF) as a potential supplement to the F-35. Some defense commentators have even advocated for the purchase in lieu of the planned F-35s, citing the F-22's known capabilities and F-35's delays and developmental uncertainties. However, considerations for the F-22 were later dropped and the F/A-18E/F Super Hornet would serve as the RAAF's interim aircraft prior to the F-35's service entry.\nThe Japanese government also showed interest in the F-22. The Japan Air Self-Defense Force (JASDF) would reportedly require fewer fighters for its mission if it obtained the F-22, thus reducing engineering and staffing costs. With the end of F-22 production, Japan chose the F-35 in December 2011. At one point the Israeli Air Force had hoped to purchase up to 50 F-22s. In November 2003, however, Israeli\nrepresentatives announced that after years of analysis and discussions with Lockheed Martin and the DoD, they had concluded that Israel could not afford the aircraft. Israel eventually purchased the F-35.\nProduction termination.\nThroughout the 2000s when the U.S. was primarily fighting counterinsurgency wars in Iraq and Afghanistan, the USAF's requirement for 381 F-22s was questioned over rising costs, initial reliability and availability problems, limited multirole versatility, and a lack of relevant adversaries for air combat missions. In 2006, Comptroller General of the United States David Walker found that \"the DoD has not demonstrated the need\" for more investment in the F-22, and further opposition was expressed by Bush Administration Secretary of Defense Rumsfeld and his successor Robert Gates, Deputy Secretary of Defense Gordon R. England, and Chairman of U.S. Senate Armed Services Committee (SASC) Senators John Warner and John McCain. Under Rumsfeld, procurement was severely cut to 183 aircraft. The F-22 lost influential supporters in 2008 after the forced resignations of Secretary of the Air Force Michael Wynne and the Chief of Staff of the Air Force General T. Michael Moseley. In November 2008, Gates stated that the F-22 lacked relevance in asymmetric post-Cold War conflicts, and in April 2009, under the Obama Administration, he called for production to end in FY 2011 after completing 187 F-22s.\nThe loss of staunch F-22 advocates in the upper DoD echelons resulted in the erosion of its political support. In July 2008, General James Cartwright, Vice Chairman of the Joint Chiefs of Staff, stated to the SASC his reasons for supporting the termination of F-22 production, including shifting resources to the multi-service F-35 and the electronic warfare EA-18G Growler. Although Russian and Chinese fighter developments fueled concern for the USAF, Gates dismissed this and in 2010, he set the F-22 requirement to 187 aircraft by lowering the number of major regional conflict preparations from two to one, despite an effort by Wynne's and Moseley's successors Michael Donley and General Norton Schwartz to raise the number to 243; according to Schwartz, he and Donley finally relented in order to convince Gates to preserve the Long Range Strike Bomber program. After President Barack Obama threatened to veto further production at Gates' urging, both the Senate and House agreed to abide by the 187 cap in July 2009. Gates highlighted the F-35's role in the decision, and believed that the U.S. would maintain its stealth fighter numbers advantage by 2025 even with F-35 delays. In December 2011, the 195th and final F-22 was completed out of 8 test and 187 production aircraft built; the jet was delivered on 2 May 2012.\nAfter production ended, F-22 tooling and associated documentation were retained and mothballed at the Sierra Army Depot to support repairs and maintenance throughout the fleet life cycle, as well as the possibility of a production restart or a Service Life Extension Program (SLEP). The Marietta plant space was repurposed to support the C-130J and F-35, while engineering work for sustainment and upgrades continued at Fort Worth, Texas and Palmdale, California. The curtailed F-22 production forced the USAF to extend the service of 179 F-15C/Ds to 2026\u2014well beyond its planned retirement\u2014to maintain adequate air superiority fighter numbers.\nIn April 2016, Congress directed the USAF to conduct a cost study and assessment associated with resuming production of the F-22, citing advancing threats from Russia and China. On 9 June 2017, the USAF submitted their report stating they had no plans to restart the F-22 production line due to cost-prohibitive economic and logistical challenges; it estimated it would cost approximately $50\u00a0billion to procure 194 additional F-22s at a cost of $206\u2013216\u00a0million per aircraft, including approximately $9.9\u00a0billion for non-recurring start-up costs and $40.4\u00a0billion for acquisition with the first delivery in the mid-to-late 2020s. The long gap since the end of production meant hiring new workers, sourcing replacement vendors, and finding new plant space, contributing to the high start-up costs and lead times. The USAF believed that the funding would be better invested in its next-generation Air Superiority 2030 effort, which evolved into the Next Generation Air Dominance (NGAD). Starting in 2019, to replace its aging F-15C/D still remaining in inventory, the USAF procured a number of new-build F-15EX, which minimized start-up costs by using an active export production line.\nModernization and upgrades.\nThe F-22 and its subsystems were designed to be upgraded over its life cycle via numbered Increments and Operational Flight Program (OFP) updates in anticipation for technological advances and evolving threats, although this initially proved difficult and costly due to the highly integrated avionics systems architecture. Amid debates over the airplane's relevance in asymmetric counterinsurgency warfare, the first upgrades primarily focused on ground attack, or strike capabilities. Joint Direct Attack Munitions (JDAM) employment was added with Increment 2 in 2005 and Small Diameter Bomb (SDB) was integrated with 3.1 in 2011; the improved AN/APG-77(V)1 radar, which incorporates air-to-ground modes, was certified in March 2007 and fitted on airframes from Lot 5 onward. To address oxygen deprivation issues, F-22s were fitted with an automatic backup oxygen system (ABOS) and modified life support system starting in 2012.\nIn contrast to prior upgrades, Increment 3.2 emphasized air combat capabilities with updates to electronic warfare, CNI (including Link 16 receive), and geolocation as well as AIM-9X and AIM-120D integration. Fleet releases of the two-part process began in 2013 and 2019 respectively. Concurrently, OFP updates added Automatic Ground Collision Avoidance System, cryptographic enhancements, and improved avionics stability, among others. A MIDS-JTRS terminal, which includes Mode 5 IFF and Link 16 transmit/receive capability, was installed starting in 2021. To address obsolescence and modernization difficulties, the F-22's mission computers were upgraded in 2021 with military-hardened commercial off-the-shelf (COTS) open mission system (OMS) processor modules with a modular open systems architecture (MOSA). Agile software development process in conjunction with an orchestration system was implemented to enable faster upgrades from additional vendors, and software updates shifted away from Increments developed using the waterfall model to numbered annual releases.\nAdditional upgrades being tested include new sensors and antennas, integration of new weapons including the AIM-260 JATM, and reliability improvements such as more durable stealth coatings; the dedicated infrared search and track (IRST), originally deleted during Dem/Val, is one of the sensors added. Other developments include all-aspect IRST functionality for the Missile Launch Detector (MLD), manned-unmanned teaming (MUM-T) capability with uncrewed collaborative combat aircraft (CCA) or \"loyal wingmen\", and integration of the Gentex/Raytheon (later Thales USA) Scorpion helmet-mounted display (HMD). To preserve the aircraft's stealth while enabling additional payload and fuel capacity, stealthy external carriage has been investigated since the early-2000s, with a low drag, low-observable external tank and pylon under development to increase stealthy combat radius. The F-22 has also been used a platform to test and apply technologies from the NGAD program.\nNot all proposed upgrades have been implemented. The planned Multifunction Advanced Data Link (MADL) integration was cut due to development delays and lack of proliferation. While Block 20 aircraft from Lot 3 onwards have been upgraded to Block 30/35 under the Common Configuration Plan, Lockheed Martin in 2017 had also proposed upgrading all remaining Block 20 training aircraft to Block 30/35 as well to increase numbers available for combat; this was not pursued due to other budget priorities.\nAside from modernizations, the F-22's structural design and construction was improved over the course of the production run; for instance, aircraft from Lot 3 onwards had improved stabilators built by Vought. The fleet underwent a $350\u00a0million \"structures repair/retrofit program\" (SRP) to resolve problems identified during testing as well as address improper titanium heat treatment in the parts of early batches. By January 2021, all aircraft had gone through the SRP to ensure full service lives for the entire fleet. The F-22 has also been used to test and qualify alternative fuels, including a synthetic jet fuel consisting of 50/50 mix of JP-8 and a Fischer\u2013Tropsch process-produced, natural gas-based fuel in August 2008, and a 50% mixture of biofuel derived from camelina in March 2011.\nDesign.\nOverview.\nThe F-22 Raptor is a fifth-generation air superiority fighter that is considered fourth generation in stealth aircraft technology by the USAF. It is the first operational aircraft to combine supercruise, supermaneuverability, stealth, and integrated avionics (or sensor fusion) in a single weapons platform to enable it to survive and conduct missions, primarily offensive and defensive counter-air operations, in highly contested environments.\nThe F-22's shape combines stealth and aerodynamic performance. Planform and panel edges are aligned at common angular aspects and the surfaces, also aligned accordingly, have continuous curvature to minimize the aircraft's radar cross-section. Its clipped diamond-like delta wings have the leading edge swept 42\u00b0, trailing edge swept \u221217\u00b0, a slight anhedral and a conical camber to reduce supersonic wave drag. The shoulder-mounted wings are smoothly blended into the fuselage with four empennage surfaces and leading edge root extensions running to the caret inlets' upper edges, where the forebody chines also meet. Flight control surfaces include leading-edge flaps, flaperons, ailerons, rudders on the canted vertical stabilizers, and all-moving horizontal tails (stabilators); for air braking, the ailerons deflect up, flaperons down, and rudders outwards to increase drag. Owing to the focus on supersonic performance, area rule is applied extensively to the airplane's shape and nearly all of the fuselage volume lies ahead of the wing's trailing edge to reduce drag at supersonic speeds, with the stabilators pivoting from tail booms extending aft of the engine nozzles. Weapons are carried internally in the fuselage for stealth. The jet has a retractable tricycle landing gear and an emergency tailhook. Fire suppression system and fuel tank inerting system are installed for survivability.\nThe aircraft's dual Pratt &amp; Whitney F119 augmented turbofan engines are closely spaced and incorporate rectangular two-dimensional thrust vectoring nozzles with a range of \u00b120 degrees in the pitch-axis; the nozzles are fully integrated into the F-22's flight controls and vehicle management system. Each engine has dual-redundant Hamilton Standard full-authority digital engine control (FADEC) and maximum thrust in the 35,000\u00a0lbf (156\u00a0kN) class. The F-22's thrust-to-weight ratio at typical combat weight is nearly at unity in maximum military power and 1.25 in full afterburner. The fixed shoulder-mounted caret inlets are offset from the forward fuselage to bypass the turbulent boundary layer and generate oblique shocks with the upper inboard corners to ensure good total pressure recovery and efficient supersonic flow compression. Maximum speed without external stores is approximately Mach 1.8 in supercruise at military/intermediate power and greater than Mach 2 with afterburners. With of internal fuel and an additional in two 600-gallon external tanks, the jet has a ferry range of over . The aircraft has a refueling boom receptacle centered on its spine and an auxiliary power unit embedded in the left wing root.\nThe F-22's high cruise speed and operating altitude over prior fighters improve the effectiveness of its sensors and weapon systems, and increase survivability against ground defenses such as surface-to-air missiles. Its ability to supercruise, or sustain supersonic flight without using afterburners, allows it to intercept targets that afterburner-dependent aircraft would lack the fuel to reach. The use of internal weapons bays permits the aircraft to maintain comparatively higher performance over most other combat-configured fighters due to a lack of parasitic drag from external stores. The F-22's thrust and aerodynamics enable regular combat speeds of Mach 1.5 at , thus providing 50% greater employment range for air-to-air missiles and twice the effective range for JDAMs than with prior platforms. Its structure contains a significant amount of high-strength materials to withstand stress and heat of sustained supersonic flight. Respectively, titanium alloys and bismaleimide/epoxy composites comprise 42% and 24% of the structural weight; the materials and multiple load path structural design also enable good ballistic survivability.\nThe airplane's aerodynamics, relaxed stability, and powerful thrust-vectoring engines give it excellent maneuverability and energy potential across its flight envelope, capable of 9-\"g\" maneuvers at takeoff gross weight with full internal fuel. Its large control surfaces, vortex-generating chines and LERX, and vectoring nozzles provide excellent high alpha (angle of attack) characteristics, and is capable of flying at trimmed alpha of over 60\u00b0 while maintaining roll control and performing maneuvers such as the Herbst maneuver (J-turn) and Pugachev's Cobra; vortex impingement on the vertical tail fins did cause more buffeting than initially anticipated, resulting in the strengthening of the fin structure by changing the rear spar from composite to titanium. The computerized triplex-redundant fly-by-wire control system and FADEC make the aircraft highly departure resistant and controllable, thus giving the pilot carefree handling.\nStealth.\nThe F-22 was designed to be highly difficult to detect and track by radar, with radio waves reflected, scattered, or diffracted away from the emitter source towards specific sectors, or absorbed and attenuated. Measures to reduce RCS include airframe shaping such as alignment of edges and continuous curvature of surfaces, internal carriage of weapons, fixed-geometry serpentine inlet ducts and curved vanes that prevent line-of-sight of the engine fan faces and turbines from any exterior view, use of radar-absorbent material (RAM), and attention to detail such as hinges and pilot helmets that could provide a radar return. The F-22 was also designed to have decreased radio frequency emissions, infrared signature and acoustic signature as well as reduced visibility to the naked eye. The aircraft's rectangular thrust-vectoring nozzles flatten the exhaust plume and facilitate its mixing with ambient air through shed vortices, which reduces infrared emissions to mitigate the threat of infrared homing (\"heat seeking\") surface-to-air or air-to-air missiles. Additional measures to reduce the infrared signature include special topcoat and active cooling to manage the heat buildup from supersonic flight.\nCompared to previous stealth designs, the F-22 is less reliant on RAM, which are maintenance-intensive and susceptible to adverse weather conditions, and can undergo repairs on the flight line or in a normal hangar without climate control. The F-22 incorporates a \"Signature Assessment System\" which delivers warnings when the radar signature is degraded and necessitates repair. While the F-22's exact RCS is classified, in 2009 Lockheed Martin released information indicating that from certain angles the airplane has an RCS of 0.0001 m2 or \u221240 dBsm \u2013 equivalent to the radar reflection of a \"steel marble\"; the aircraft can mount a Luneburg lens reflector to mask its RCS. For missions where stealth is required, the mission capable rate is 62\u201370%. Beginning in 2021, the F-22 has been seen testing a new chrome-like surface coating, speculated to help reduce the F-22's detectability by infrared tracking systems.\nThe effectiveness of the stealth characteristics is difficult to gauge. The RCS value is a restrictive measurement of the aircraft's frontal or side area from the perspective of a static radar. When an aircraft maneuvers it exposes a completely different set of angles and surface area, potentially increasing radar observability. Furthermore, the F-22's stealth contouring and radar-absorbent materials are chiefly effective against high-frequency radars, usually found on other aircraft. The effects of Rayleigh scattering and resonance mean that low-frequency radars such as weather radars and early-warning radars are more likely to detect the F-22 due to its physical size. These are also conspicuous, susceptible to clutter, and have low precision. Additionally, while faint or fleeting radar contacts make defenders aware that a stealth aircraft is present, reliably vectoring interception to attack the aircraft is much more challenging.\nAvionics.\nThe aircraft has an integrated avionics system where through sensor fusion, data from all onboard sensor systems as well as off-board inputs are filtered and processed into a combined tactical picture, thus enhancing the pilot's situational awareness and reducing workload. Key mission systems include Sanders/General Electric AN/ALR-94 electronic warfare system, Martin Marietta AN/AAR-56 infrared and ultraviolet Missile Launch Detector (MLD), Westinghouse/Texas Instruments AN/APG-77 active electronically scanned array (AESA) radar, TRW AN/ASQ-220 Communication/Navigation/Identification (CNI) suite, and Raytheon advanced infrared search and track (IRST) being tested.\nThe APG-77 radar has a low-observable, active-aperture, electronically scanned antenna with multiple target track-while-scan in all weather conditions; the antenna is tilted back for stealth. Its emissions can be focused to overload enemy sensors as an electronic attack capability. The radar changes frequencies more than 1,000 times per second to lower interception probability and has an estimated range of against an target and or more in narrow beams. The upgraded APG-77(V)1 provides air-to-ground functionality through synthetic aperture radar (SAR) mapping, ground moving target indication/track (GMTI/GMTT), and strike modes.\nThe ALR-94 electronic warfare system, among the most technically complex equipment on the F-22, integrates more than 30 antennas blended into the wings and fuselage for all-round radar warning receiver (RWR) coverage and threat geolocation. It can be used as a passive detector capable of searching targets at ranges (250+ nmi) exceeding the radar's, and can provide enough information for a target lock through narrowband interleaved search-and-rack (NBILST) and cue radar emissions to a narrow beam (down to 2\u00b0 by 2\u00b0 in azimuth and elevation). Depending on the detected threat, the defensive systems can prompt the pilot to release countermeasures such as flares or chaff. To ensure stealth in the radio frequency spectrum, CNI emissions are strictly controlled and confined to specific sectors, with tactical communication between F-22s performed using the directional Inter/Intra-Flight Data Link (IFDL); the ASQ-220 CNI system, which later incorporated a MIDS-JTRS terminal, also manages TACAN, IFF (including Mode 5), and communication through various methods such as HAVE QUICK/SATURN and SINCGARS. Additional passive detection methods include the AAR-56 Missile Launch Detector, which uses six sensors to provide full spherical infrared coverage; the sensors are being updated to the TacIRST for improved detection and survivability measures. Separately, the advanced IRST, housed in a stealthy wing pod, is a narrow field-of-view sensor for long-range passive identification and targeting. The aircraft was also upgraded with an automatic ground collision avoidance system (GCAS).\nInformation from radar, CNI, and other sensors are processed by two Hughes Common Integrated Processor (CIP) mission computers, each capable of processing up to 10.5\u00a0billion instructions per second. The F-22's baseline software has some 1.7\u00a0million lines of code, the majority involving the mission systems such as processing radar data. The highly integrated nature of the avionics architecture system, as well as the use of the programming language Ada, has made the development and testing of upgrades challenging. To enable more rapid upgrades, the CIPs were upgraded with Curtiss-Wright open mission systems (OMS) processor modules as well as a modular open systems architecture called the Open Systems Enclave (OSE) orchestration platform to allow the avionics suite to interface with containerized software from third-party vendors.\nThe F-22's ability to operate close to the battlefield gives the aircraft threat detection and identification capability comparative with the RC-135 Rivet Joint, and the ability to function as a \"mini-AWACS\", though its radar is less powerful than those of dedicated platforms. This allows the F-22 to rapidly designate targets for allies and coordinate friendly aircraft. Although communication with other aircraft types was initially limited to voice, upgrades have enabled data to be transferred through a Battlefield Airborne Communications Node (BACN) or via JTIDS/Link 16 traffic through MIDS-JTRS. The IEEE 1394B bus developed for the F-22 was derived from the commercial IEEE 1394 \"FireWire\" bus system. In 2007, the F-22's radar was tested as a wireless data transceiver, transmitting data at 548 megabits per second and receiving at gigabit speed, far faster than the Link 16 system. The aircraft's radio frequency receivers provide electronic support measures (ESM) and the ability to perform intelligence, surveillance, and reconnaissance (ISR) tasks.\nCockpit.\nThe F-22 has a glass cockpit with all-digital flight instruments. The monochrome head-up display offers a wide field of view and serves as a primary flight instrument; information is also displayed upon six color liquid-crystal display (LCD) panels. The primary flight controls are a force-sensitive side-stick controller and a pair of throttles. The USAF initially wanted to implement direct voice input (DVI) controls, but this was judged to be too technically risky and was abandoned. The canopy's dimensions are approximately 140\u00a0inches long, 45\u00a0inches wide, and 27\u00a0inches tall (355\u00a0cm \u00d7 115\u00a0cm \u00d7 69\u00a0cm) and weighs 360 pounds. The canopy was redesigned after the original design lasted an average of 331 hours instead of the required 800 hours. Although the F-22 was originally intended to have a helmet mounted display (HMD), this was deferred during development to save costs; the aircraft is currently integrating the Scorpion HMD.\nThe F-22 has integrated radio functionality, the signal processing systems are virtualized rather than as a separate hardware module. The integrated control panel (ICP) is a keypad system for entering communications, navigation, and autopilot data. Two up-front displays located around the ICP are used to display integrated caution advisory/warning (ICAW) data, CNI data and also serve as the stand-by flight instrumentation group and fuel quantity indicator for redundancy. The stand-by flight group displays an artificial horizon, for basic instrument meteorological conditions. The primary multi-function display (PMFD) is located under the ICP, and is used for navigation and situation assessment. Three secondary multi-function displays are located around the PMFD for tactical information and stores management.\nThe ejection seat is a version of the ACES II commonly used in USAF aircraft, with a center-mounted ejection control. The F-22 has a complex life support system, which includes the onboard oxygen generation system (OBOGS), protective pilot garments, and a breathing regulator/anti-g (BRAG) valve controlling flow and pressure to the pilot's mask and garments. The pilot garments were developed under the Advanced Technology Anti-G Suit (ATAGS) project and protect against chemical/biological hazards and cold-water immersion, counter g-forces and low pressure at high altitudes, and provide thermal relief. Following a series of hypoxia-related issues, the life support system was consequently revised to include an automatic backup oxygen system and a new flight vest valve. In combat environments, the ejection seat includes a modified M4 carbine designated the GAU-5/A.\nArmament.\nThe F-22 has three internal weapons bays: a large main bay on the bottom of the fuselage, and two smaller bays on the sides of the fuselage, aft of the engine inlets; a small bay for countermeasures such as flares is located behind each side bay. The main bay is split along the centerline and can accommodate six LAU-142/A launchers for beyond-visual-range (BVR) missiles and each side bay has an LAU-141/A launcher for short-range missiles. The primary air-to-air missiles are the AIM-120 AMRAAM and the AIM-9 Sidewinder, with planned integration of the AIM-260 JATM. Missile launches require the bay doors to be open for less than a second, during which pneumatic or hydraulic arms push missiles clear of the aircraft; this is to reduce vulnerability to detection and to deploy missiles during high-speed flight. An internally mounted M61A2 Vulcan 20\u00a0mm rotary cannon is embedded in the airplane's right wing root with the muzzle covered by a retractable door, which remains closed when the cannon is not firing in order to minimize the negative effect the exposed muzzle on the aircraft's radar signature The radar projection of the cannon fire's path is displayed on the pilot's head-up display.\nAlthough designed for air-to-air missiles, the main bay can replace four launchers with two bomb racks that can each carry one 1,000\u00a0lb (450\u00a0kg) or four 250\u00a0lb (110\u00a0kg) bombs for a total of of air-to-surface ordnance. In 2024, Lockheed Martin disclosed its proposed Mako hypersonic missile, a 1,300\u00a0lb (590\u00a0kg) weapon that can be carried internally in the F-22. While capable of carrying weapons with GPS guidance such as JDAMs and SDBs, the F-22 cannot self-designate laser-guided weapons.\nWhile the F-22 typically carries weapons internally, the wings include four hardpoints, each rated to handle . Each hardpoint can accommodate a pylon that can carry a detachable 600-gallon (2,270 L) external fuel tank or a launcher holding two air-to-air missiles; the two inboard hardpoints are \"plumbed\" for external fuel tanks. The two outboard hardpoints have since been dedicated to a pair of stealthy pods housing the IRST and mission systems. The aircraft can jettison external tanks and their pylon attachments to restore its low observable characteristics and kinematic performance.\nMaintenance.\nEach F-22 requires a three-week packaged maintenance plan (PMP) every 300 flight hours. Its stealth coatings were designed to be more robust and weather-resistant than those of earlier stealth aircraft, yet early coatings failed against rain and moisture when F-22s were initially posted to Guam in 2009. Stealth measures account for almost one third of maintenance, with coatings being particularly demanding. F-22 depot maintenance is performed at Ogden Air Logistics Complex at Hill AFB, Utah; considerable care is taken during maintenance due to the small fleet size and limited attrition reserve.\nF-22s were available for missions 63% of the time on average in 2015, up from 40% when it was introduced in 2005. Maintenance hours per flight hour was also improved from 30 early on to 10.5 by 2009, lower than the requirement of 12; man-hours per flight hour was 43 in 2014. When introduced, the F-22 had a Mean Time Between Maintenance (MTBM) of 1.7 hours, short of the required 3.0; this rose to 3.2 hours in 2012. By fiscal year 2015, the cost per flight hour was $59,116, while the user reimbursement rate was approximately US$35,000 (~$ in 2024) per flight hour in 2019.\nOperational history.\nIntroduction into service.\nThe F-22 underwent extensive testing before its service introduction. While the first production aircraft was delivered to Edwards AFB in October 2002 for IOT&amp;E and the first jet for the 422nd TES at Nellis AFB arrived in January 2003, IOT&amp;E was continually pushed back from its planned start in mid-2003, with mission avionics stability being particularly challenging. Following a preliminary assessment, called OT&amp;E Phase 1, formal IOT&amp;E began in April 2004 and was completed in December of that year. This milestone marked the successful demonstration of the jet's air-to-air mission capability, although the jet was more maintenance intensive than expected. A Follow-On OT&amp;E (FOT&amp;E) in 2005 cleared the F-22's air-to-ground mission capability.\nThe first combat ready F-22 of the 1st Fighter Wing arrived at Langley AFB, Virginia in January 2005 and that December, the USAF announced that the aircraft had achieved Initial Operational Capability (IOC) with the 94th Fighter Squadron. The unit subsequently participated in Exercise Northern Edge 06 in Alaska in June 2006 and Exercise Red Flag 07\u20132 at Nellis AFB in February 2007, where it demonstrated the F-22's greatly increased air combat capabilities when flying against Red Force Aggressor F-15s and F-16s with a simulated kill ratio of 108\u20130. These large force exercises also further refined the F-22's operational tactics and employment.\nThe F-22 achieved Full Operational Capability (FOC) in December 2007, when General John Corley of Air Combat Command (ACC) officially declared the F-22s of the integrated active duty 1st Fighter Wing and Virginia Air National Guard 192nd Fighter Wing fully operational. This was followed by an Operational Readiness Inspection (ORI) of the integrated wing in April 2008, in which it was rated \"excellent\" in all categories, with a simulated kill-ratio of 221\u20130. The fielding of the F-22 with its precision strike capability also contributed to the retirement of the F-117 from operational service in 2008, with the 49th Fighter Wing operating the F-22 for a brief period prior to a series of fleet consolidations to reduce long term operational costs; further consolidations to improve availability and pilot training were recommended by the Government Accountability Office in 2018.\nTraining.\nThe 43rd Fighter Squadron was reactivated in 2002 as the F-22 Formal Training Unit (FTU) for the type's basic course at Tyndall AFB and the first aircraft for pilot training was delivered in September 2003. Following severe damage to the installation in the wake of Hurricane Michael in 2018, the squadron and its aircraft were relocated to nearby Eglin AFB; although it was initially feared that several jets were lost due to storm damage, all were later repaired and flown out. The FTU and its aircraft were reassigned to the 71st Fighter Squadron at Langley AFB in 2023.\nAs of 2014, B-Course students require 38 sorties to graduate (previously 43 sorties). Track 1 course pilots, pilots retraining from other aircraft, also saw a reduction in the number of sorties needed to graduate, from 19 to 12 sorties. F-22 students are first trained on the T-38 Talon trainer aircraft. Additional pilot training takes place on the F-16 because the aging T-38 is not rated to sustain higher G-forces and lacks modern avionics. Due to a lack of a modern trainer stand-in that can accurately emulate the F-22, the Air Force often uses F-22s to supplement training, which is costly as the F-22 costs almost 10 times more than the T-38 per flight hour. The upcoming T-7 Red Hawk features modern avionics that better approximate those of the F-22 and F-35. This is scheduled to enter initial operating capability in 2027, several years behind schedule. In 2014 the Air Force stood up the 2nd Fighter Training Squadron at Tyndall AFB which was equipped with T-38s to serve as adversary aircraft to reduce adversary training flights on the F-22s. To reduce operating costs and prolong the F-22's service life, some pilot training sorties are performed using flight simulators. The advanced F-22 weapons instructor course at USAF Weapons School is conducted by the 433rd Weapons Squadron at Nellis AFB.\nInitial operational problems.\nDuring the initial years of service, F-22 pilots experienced symptoms as a result of oxygen system issues that include loss of consciousness, memory loss, emotional lability and neurological changes as well as lingering respiratory problems and a chronic cough; the issues resulted in a fatal mishap in 2010 and four-month grounding in 2011 and subsequent altitude and distance flight restrictions. In August 2012, the DoD found that the BRAG valve, which inflated the pilot's vest at high-\"g\", was defective and restricted breathing and the OBOGS (onboard oxygen generation system) unexpectedly fluctuated oxygen supply at high \"g\". In 2005, the Raptor Aeromedical Working Group had recommended oxygen system changes that were unfunded, but received reconsideration in 2012. The F-22 CTF and 412th Aerospace Medicine Squadron eventually determined breathing restrictions as the root cause; coughing symptoms were attributed to acceleration atelectasis from high \"g\" exposure and OBOGS delivering excessive oxygen concentration. The presence of toxins and particles in some ground crew was deemed unrelated. Modifications to the life support and oxygen systems, including the installation of an automatic backup, allowed altitude and distance restrictions to be lifted in April 2013.\nOperational service.\nFollowing IOC and large-scale exercises, the F-22 flew its first homeland defense mission in January 2007 under Operation Noble Eagle. In November 2007, F-22s of 90th Fighter Squadron at Elmendorf AFB, Alaska, performed their first North American Aerospace Defense Command (NORAD) interception of two Russian Tu-95MS bombers. Since then, F-22s have also escorted probing Tu-160 bombers.\nThe F-22 was first deployed overseas in February 2007 with the 27th Fighter Squadron to Kadena Air Base in Okinawa, Japan. This first overseas deployment was initially marred by problems when six F-22s flying from Hickam AFB, Hawaii, experienced multiple software-related system failures while crossing the International Date Line (180th meridian of longitude). The aircraft returned to Hawaii by following tanker aircraft. Within 48 hours, the error was resolved and the journey resumed. Kadena would be a frequent rotation for F-22 units; they have also been involved in training exercises in South Korea, Malaysia, and the Philippines.\nDefense Secretary Gates initially refused to deploy F-22s to the Middle East in 2007; the type made its first deployment in the region at Al Dhafra Air Base in the UAE in 2009. In April 2012, F-22s have been rotating into Al Dhafra, less than 200 miles from Iran. In March 2013, the USAF announced that an F-22 had intercepted an Iranian F-4 Phantom II that approached within 16 miles of an MQ-1 Predator flying off the Iranian coastline.\nOn 22 September 2014, F-22s performed the type's first combat sorties by conducting some of the opening strikes of Operation Inherent Resolve, the American-led intervention in Syria; aircraft dropped 1,000-pound GPS-guided bombs on Islamic State targets near Tishrin Dam. Between September 2014 and July 2015, F-22s flew 204 sorties over Syria, dropping 270 bombs at some 60 locations. Throughout their deployment, F-22s conducted close air support (CAS) and also deterred Syrian, Iranian, and Russian aircraft from attacking U.S.-backed Kurdish forces and disrupting U.S. operations in the region. F-22s also participated in the U.S. strikes that defeated pro-Assad and Russian Wagner Group paramilitary forces near Khasham in eastern Syria on 7 February 2018. These strikes notwithstanding, the F-22's main role in the operation was conducting intelligence, surveillance and reconnaissance. The aircraft also performed missions in other regions of the Middle East; in November 2017, F-22s operating alongside B-52s bombed opium production and storage facilities in Taliban-controlled regions of Afghanistan.\nTo increase deployment responsiveness and reduce logistical footprint in a peer or near-peer conflict, the USAF developed a deployment concept called Rapid Raptor which involves two to four F-22s and one C-17 for logistical support, first proposed in 2008 by two F-22 pilots. The goal was for the type to be able to set up and engage in combat within 24 hours in smaller and more austere environments that would enable more dispersed and survivable disposition of forces. This concept was tested at Wake Island in 2013 and Guam in late 2014. Four F-22s were deployed to Spangdahlem Air Base in Germany, \u0141ask Air Base in Poland, and \u00c4mari Air Base in Estonia in August and September 2015 to further test the concept and train with NATO allies in response to the Russian annexation of Crimea in 2014. The USAF would build on the principles of Rapid Raptor and eventually integrate it into its new operational concept called Agile Combat Employment, which shifts towards distributed operations during peer conflicts; for instance, detachments of F-22s have operated from austere airfields on Tinian and Iwo Jima during exercises.\nOn 4 February 2023, an F-22 of the 1st Fighter Wing shot down a suspected Chinese spy balloon within visual range off the coast of South Carolina at an altitude of 60,000 to , marking the F-22's first air-to-air kill. The wreckage landed approximately 6 miles offshore and was subsequently secured by ships of the U.S. Navy and U.S. Coast Guard. F-22s shot down additional high-altitude objects near the coast of Alaska on 10 February and over Yukon on 11 February.\nThe USAF expects to begin retiring the F-22 in the 2030s as it gets replaced by the Next Generation Air Dominance (NGAD) sixth-generation crewed fighter, the Boeing F-47. In May 2021, Air Force Chief of Staff Charles Q. Brown Jr. said that he envisioned a reduction in the future number of fighter fleets to \"four plus one\": the F-22 followed by NGAD, the F-35A, the F-15E followed by F-15EX, the F-16 followed by \"MR-X\", and the A-10; the A-10 was later dropped from the plans due to that aircraft's accelerated retirement. In 2022 the Air Force requested that it be allowed to divest all but three of its Block 20 F-22s at Tyndall AFB. Congress denied the request to divest its 33 non-combat-coded Block 20 aircraft and passed language prohibiting the divestment through FY2026. While the Block 30/35 F-22 remains one of the USAF's top priorities and will be continually updated, the service believes the Block 20 aircraft is obsolescent and unsuitable even for training F-22 pilots and that upgrading them to Block 30/35 standards would be cost-prohibitive at $3.5 billion. In September of 2025, Lockheed Martin revealed plans to modify 35 Block 20 F-22s, used for training purposes, to combat ready Block 30/35 F-22s.\nAccording to US officials, F-22s and F-35As launched from land bases in the Middle East, were used to attempt to draw surface-to-air missile fire ahead of B-2 bombing runs during the United States strikes on Iranian nuclear sites on 22 June 2025.\nSingle-seat version, was designated \"F/A-22A\" in early 2000s before reverting back to \"F-22A\" in 2005; 195 built, consisting of 8 test and 187 operational aircraft.\nPlanned two-seat version with the same combat capabilities as the single-seat version, cancelled in 1996 to save development costs with test aircraft orders converted to F-22A.\nSometimes referred to as \"NATF-22\" or \"F-22N\" (it was never formally designated), planned carrier-borne variant/derivative for the U.S. Navy's Navy Advanced Tactical Fighter (NATF) program. Because the NATF needed lower landing speeds than the F-22 for aircraft carrier operations while still attaining Mach 2-class speeds, the design would have incorporated variable-sweep wings; it would also have had expanded weapons carriage, including the AIM-152 AAAM, AGM-88 HARM, and AGM-84 Harpoon. Program was cancelled in 1991 due to tightening budgets.\nVariants.\nProposed derivatives.\nThe X-44 MANTA, or \"multi-axis, no-tail aircraft\", was a planned experimental aircraft based on the F-22 with enhanced thrust vectoring controls and no aerodynamic surface backup. The aircraft was to be solely controlled by thrust vectoring, without featuring any rudders, ailerons, or elevators. Funding for this program was halted in 2000.\nThe FB-22 was proposed in the early 2000s as a supersonic stealth regional bomber for the USAF. The design went through several iterations and the later ones would combine an F-22 fuselage with greatly enlarged delta wings and was projected to carry up to 30 Small Diameter Bombs to over , about twice the combat range of the F-22A. The FB-22 proposals were cancelled with the 2006 Quadrennial Defense Review and subsequent developments, in lieu of a larger subsonic strategic bomber with a much greater range; this became the Next-Generation Bomber, although it would be rescoped in 2009 as the Long Range Strike Bomber resulting in the B-21 Raider.\nIn August 2018, Lockheed Martin proposed an F-22 derivative to the Japan Air Self-Defense Force (JASDF) for its 5th/6th generation F-X program. The design, which was later also proposed to the USAF, would combine a modified F-22 airframe with enlarged wings to increase fuel capacity and combat radius to as well as the avionics and improved stealth coatings of the F-35. The proposal was ultimately not considered by the USAF or JASDF due to cost as well as existing export restrictions and industrial workshare concerns.\nOperators.\nThe United States Air Force is the only operator of the F-22. As of August 2022, it has 178 active aircraft in its inventory. Current units are bolded.\nAir Combat Command.\n&lt;templatestyles src=\"Tree list/styles.css\" /&gt;\nPacific Air Forces.\n&lt;templatestyles src=\"Tree list/styles.css\" /&gt;\nAir National Guard.\n&lt;templatestyles src=\"Tree list/styles.css\" /&gt;\nAir Force Reserve Command.\n&lt;templatestyles src=\"Tree list/styles.css\" /&gt;\nAir Force Materiel Command.\n&lt;templatestyles src=\"Tree list/styles.css\" /&gt;\nAccidents.\nThe first F-22 crash occurred during takeoff at Nellis AFB on 20 December 2004, in which the pilot ejected safely before impact. The investigation revealed that a brief interruption in power during an engine shutdown prior to flight caused a flight-control system malfunction; consequently the aircraft design was corrected to avoid the problem. Following a brief grounding, F-22 operations resumed after a review.\nOn 25 March 2009, an EMD F-22 crashed northeast of Edwards AFB during a test flight, resulting in the death of Lockheed Martin test pilot David P. Cooley. An Air Force Materiel Command investigation found that Cooley momentarily lost consciousness during a high-G maneuver, or g-LOC, then ejected when he found himself too low to recover. Cooley was killed during ejection by blunt-force trauma from windblast due to the aircraft's speed. The investigation found no design issues.\nOn 16 November 2010, an F-22 from Elmendorf AFB crashed, killing the pilot, Captain Jeffrey Haney. F-22s were restricted to flying below 25,000 feet, then grounded during the investigation. The crash was attributed to a bleed air system malfunction after an engine overheat condition was detected, shutting down the Environmental Control System (ECS) and OBOGS. The accident review board ruled Haney was to blame, as he did not react properly to engage the emergency oxygen system. Haney's widow sued Lockheed Martin, claiming equipment defects, and later reached a settlement. After the ruling, the emergency oxygen system engagement handle was redesigned and the entire system was eventually replaced by an automatic backup. On 11 February 2013, the DoD's Inspector General released a report stating that the USAF had erred in blaming Haney, and that facts did not sufficiently support conclusions; the USAF stated that it stood by the ruling.\nOn 15 November 2012, an F-22 crashed to the east of Tyndall AFB during a training mission. The pilot ejected safely and no injuries were reported on the ground. The investigation determined that a \"chafed\" electrical wire ignited the fluid in a hydraulic line, causing a fire that damaged the flight controls.\nOn 15 May 2020, an F-22 from Eglin Air Force Base crashed during a routine training mission shortly after takeoff; the pilot ejected safely. The cause of the crash was attributed to a maintenance error after an aircraft wash resulting in faulty air data sensor readings.\nSpecifications (F-22A).\n\"Data from\" USAF, manufacturers' data, \"Aerofax\", \"Aviation Week\", \"Air Forces Monthly\", and \"Journal of Electronic Defense\"\nGeneral characteristics* Crew: 1* Aspect ratio: 2.36* Airfoil: NACA 6 series airfoil* Fuel capacity: internally, or with 2\u00d7 600 U.S. gal tanks\nPerformance* Maximum speed: Mach 2.25, at altitude\nArmament\n *Guns: 1\u00d7 20 mm M61A2 Vulcan rotary cannon, 480 rounds\nAvionics\n *AN/APG-77 or AN/APG-77(V)1 AESA radar: against targets (estimated range), more than in narrow beams\nSee also.\nRelated development\nAircraft of comparable role, configuration, and era\nRelated lists\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "66304", "revid": "36504275", "url": "https://en.wikipedia.org/wiki?curid=66304", "title": "Yoni", "text": "Aniconic representation of the Hindu goddess Shakti, consort of Shiva\nYoni (Sanskrit: \u092f\u094b\u0928\u093f, IAST: ), sometimes called pindika, is an abstract or aniconic representation of the Hindu goddess Shakti. It is usually shown with \"linga\" \u2013 its masculine counterpart. Together, they symbolize the merging of microcosmos and macrocosmos, the divine eternal process of creation and regeneration, and the union of the feminine and the masculine that recreates all of existence. The \"yoni\" is conceptualized as nature's gateway of all births, particularly in the esoteric Kaula and Tantra practices, as well as the Shaktism and Shaivism traditions of Hinduism.\n\"Yoni\" is a Sanskrit word that has been interpreted to literally mean the \"womb\", the \"source\", and the female organs of generation. It also connotes the female sexual organs such as \"vagina\", \"vulva\", and \"uterus\", or alternatively to \"origin, abode, or source\" of anything in other contexts. For example, the Vedanta text \"Brahma Sutras\" metaphorically refers to the metaphysical concept \"Brahman\" as the \"yoni of the universe\". The \"yoni\" with \"linga\" iconography is found in Shiva temples and archaeological sites of the Indian subcontinent and southeast Asia, as well in sculptures such as the \"Lajja Gauri\".\nEtymology and significance.\nYoni appears in the \"Rigveda\" and other Vedic literature in the sense of feminine life-creating regenerative and reproductive organs, as well as in the sense of \"source, origin, fountain, place of birth, womb, nest, abode, fire pit of incubation\". Other contextual meanings of the term include \"race, caste, family, fertility symbol, grain or seed\". It is a spiritual metaphor and icon in Hinduism for the origin and the feminine regenerative powers in the nature of existence. The \"Brahma Sutras\" metaphorically calls the metaphysical concept \"Brahman\" as the \"yoni of the universe\", which Adi Shankara states in his commentaries means the material cause and \"source of the universe\".\nAccording to Indologists Constance Jones and James D. Ryan, the yoni symbolizes the female principle in all life forms as well as the \"earth's seasonal and vegetative cycles\", and thus is an emblem of cosmological significance. The yoni is a metaphor for nature's gateway of all births, particularly in the Shaktism and Shaivism traditions of Hinduism, as well as the esoteric Kaula and Tantra sects. \"Yoni\" together with the \"lingam\" is a symbol for \"prakriti\", its cyclic creation and dissolution. According to Corinne Dempsey \u2013 a professor of Religious Studies, yoni is an \"aniconic form of the goddess\" in Hinduism, the feminine principle \"Shakti\".\nThe \"yoni\" is sometimes referred to as \"pindika\". The base on which the linga-yoni sit is called the \"pitha\", but in some texts such as the \"Nisvasa tattva samhita\" and \"Mohacudottara\", the term \"pitha\" generically refers to the base and the yoni.\nHistory.\nThe reverence for yoni, state Jones and Ryan, is probably pre-Vedic. Figurines recovered from Zhob valley and dated to the 4th millennium BCE show pronounced breasts and yoni, and these may have been fertility symbols used in prehistoric times that ultimately evolved into spiritual symbols. According to David Lemming, the yoni worship tradition dates to the pre-Vedic period, over the 4000 BCE to 1000 BCE period.\nThe yoni has served as a divine symbol from ancient times, and it may well be the oldest spiritual icon not only in India but across many ancient cultures. Some in the orthodox Western cultures, states the Indologist Laura Amazzone, have treated the feminine sexual organs and sexuality in general as a taboo subject, but in Indic religions and other ancient cultures the yoni has long been accepted as profound cosmological and philosophical truth, of the feminine potential and power, one mysteriously interconnected with the natural periodic cycles of moon, earth and existence.\nThe yoni is considered to be an abstract representation of Shakti and Devi, the creative force that moves through the entire universe. In tantra, yoni is the origin of life.\nArchaeology.\nThe colonial era archaeologists John Marshall and Ernest Mackay proposed that certain polished stones with holes found at Harappan sites may be evidence of yoni-linga worship in Indus Valley civilisation. Scholars such as Arthur Llewellyn Basham dispute whether such artifacts discovered at the archaeological sites of Indus Valley sites are yoni. For example, Jones and Ryan state that lingam/yoni shapes have been recovered from the archaeological sites at Harappa and Mohenjo-daro, part of the Indus Valley civilisation. In contrast, Jane McIntosh states that truncated ring stones with holes were once considered as possibly yonis. Later discoveries at the Dholavira site, and further studies, have proven that these were pillar components because the \"truncated ring stones with holes\" are integral architectural components of the pillars. However, states McIntosh, the use of these structures in architecture does not rule out their simultaneous religious significance as yoni.\nAccording to the Indologist Asko Parpola, \"it is true that Marshall's and Mackay's hypotheses of linga and yoni worship by the Harappans has rested on rather slender grounds, and that for instance the interpretation of the so-called ring-stones as yonis seems untenable\". He quotes Dales 1984 paper, which states \"with the single exception of the unidentified photography of a realistic phallic object in Marshall's report, there is no archaeological evidence to support claims of special sexually-oriented aspects of Harappan religion\". However, adds Parpola, a re-examination at Indus Valley sites suggest that the Mackay's hypothesis cannot be ruled out because erotic and sexual scenes such as ithyphallic males, naked females, a human couple having intercourse and trefoil imprints have now been identified at the Harappan sites. The \"finely polished circular stand\" found by Mackay may be yoni although it was found without the linga. The absence of linga, states Parpola, may be because it was made from wood which did not survive.\nSanskrit literature.\nThe term \"yoni\" and its derivatives appear in ancient medicine and surgery-related Sanskrit texts such as the \"Sushruta Samhita\" and \"Charaka Samhita\". In this context, \"yoni\" broadly refers to \"female sexual and procreative organs\". According to Indologists Rahul Das and Gerrit Meulenbeld known for their translations and reviews of ancient Sanskrit medical and other literature, \"yoni\" \"usually denotes the vagina or the vulva, in a technical sense it also includes the uterus along with these; moreover, yoni- can at times mean simply 'womb, uterus' too, though it [Cakrapanidata's commentary on \"Sushruta Samhita\"] does so relatively seldom\". According to Amit Rupapara et al., \"yoni-roga\" means \"gynecological disorders\" and \"yoni-varti\" means \"vaginal suppository\". The \"Charaka Samhita\" dedicates its 30th chapter in Chikitsa Sthana to \"yoni-vyapath\" or \"gynecological disorders\".\nIn sexuality-related Sanskrit literature, as well as Tantric literature, yoni connotes many layers of meanings. Its literal meaning is \"female genitalia\", but it also encompasses other meanings such as \"womb, origin, and source\". In some Indic literature, yoni means vagina, and other organs regarded as \"divine symbol of sexual pleasure, the matrix of generation and the visible form of Shakti\".\nOrientalist literature.\nThe colonial era Orientalists and Christian missionaries, raised in the Victorian mold where sex and sexual imagery were a taboo subject, were shocked by and were hostile to the yoni iconography and reverence they witnessed. The 19th and early 20th-century colonial and missionary literature described yoni, lingam-yoni, and related theology as obscene, corrupt, licentious, hyper-sexualized, puerile, impure, demonic and a culture that had become too feminine and dissolute. To the Hindus, particularly the Shaivites, these icons and ideas were the abstract, a symbol of the entirety of creation and spirituality. The colonial disparagement in part triggered the opposite reaction from Bengali nationalists, who more explicitly valorised the feminine. Swami Vivekananda called for the revival of the Mother Goddess as a feminine force, inviting his countrymen to \"proclaim her to all the world with the voice of peace and benediction\".\nAccording to Wendy Doniger, the terms lingam and yoni became explicitly associated with human sexual organs in the western imagination after the widely popular first \"Kama Sutra\" translation by Sir Richard Burton in 1883. In his translation, even though the original Sanskrit text does not use the words lingam or yoni for sexual organs, Burton adroitly sidestepped being viewed as obscene to the Victorian mindset by using them throughout in place of words such as penis, vulva, and vagina to discuss sex, sexual relationships and human sexual positions. This conscious and incorrect word substitution, states Doniger, thus served as an Orientalist means to \"anthropologize sex, distance it, make it safe for English readers by assuring them, or pretending to assure them, that the text was not about real sexual organs, their sexual organs, but merely about the appendages of weird, dark people far away.\" Similar Orientalist literature of the Christian missionaries and the British era, states Doniger, stripped all spiritual meanings and insisted on the Victorian vulgar interpretation only, which had \"a negative effect on the self-perception that Hindus had of their own bodies\" and they became \"ashamed of the more sensual aspects of their own religious literature\". Some contemporary Hindus, states Doniger, in their passion to spiritualize Hinduism and for their Hindutva campaign have sought to sanitize the historic earthly sexual meanings, and insist on the abstract spiritual meaning only.\nIconography and temples.\nWithin Shaivism, the sect dedicated to the god Shiva, the Shakti is his consort and both have aniconic representations: lingam for Shiva, yoni for Shakti. The yoni iconography is typically represented in the form of a horizontally placed round or square base with a lipped edge and an opening in the center usually with a cylindrical lingam. Often, one side of this base extends laterally, and this projection is called the \"yoni-mukha\". An alternate symbol for yoni that is commonly found in Indic arts is the lotus, an icon found in temples.\nThe yoni is one of the sacred icons of the Hindu Shaktism tradition, with historic arts and temples dedicated to it. Some significant artworks related to yoni include the Lajja Gauri found in many parts of India and the Kamakhya Temple in Assam. Both of these have been dated to the late 1st millennium CE, with the major expansion of the Kamakhya temple that added a new sanctum above the natural rock yoni attached to an older temple being dated to the 16th-century Koch dynasty period.\nLajja Gauri.\nThe Lajja Gauri is an ancient icon that is found in many Devi-related temples across India and one that has been unearthed at several archaeological sites in South Asia. The icon represents yoni but with more context and complexity. According to the Art Historian Carol Bolon, the Lajja Gauri icon evolved over time with increasing complexity and richness. It is a fertility icon and symbolizes the procreative and regenerative powers of mother earth, \"the elemental source of all life, animal and plant\", the vivifier and \"the support of all life\". The earliest representations were variants of aniconic pot, the second stage represented it as the three-dimensional artwork with no face or hands but a lotus-head that included yoni, chronologically followed by the third stage that added breasts and arms to the lotus-headed figure. The last stage was an anthropomorphic figure of a squatting naked goddess holding lotus and motifs of agricultural abundance spread out showing her yoni as if she is giving birth or sexually ready to procreate. According to Bolon, the different aniconic and anthropomorphic representations of Lajja Gauri are symbols for the \"yoni of Prithvi (Earth)\", she as womb.\nThe Lajja Gauri iconography \u2013 sometimes referred to by other names such as Yellamma or Ellamma \u2013 has been discovered in many South Indian sites such as the Aihole (4th to 12th-century), Nagarjunakonda (4th century Lajja Gauri inscription and artwork), Balligavi, Elephanta Caves, Ellora Caves, many sites in Gujarat (6th century), central India such as Nagpur, northern parts of the subcontinent such as Bhaktapur (Nepal), Kausambi and many other sites.\nKamakhya Temple.\nThe Kamakhya temple is one of the oldest \"shakta pithas\" in South Asia or sacred pilgrimage sites of the Shaktism tradition. Textual, inscriptional and archaeological evidence suggests that the temple has been revered in the Shaktism tradition continuously since at least the 8th-century CE, as well as the related esoteric tantric worship traditions. The Shakta tradition believes, states Hugh Urban \u2013 a professor of Religious Studies primarily focusing on South Asia, that this temple site is the \"locus of goddess' own yoni\".\nThe regional tantric tradition considers this yoni site as the \"birthplace\" or \"principal center\" of tantra. While the temple premises, walls and mandapas have numerous depictions of goddess Kamakhya in her various roles, include those relating to her procreative powers, as a martial warrior, and as a nurturing motherly figure (one image near the western gate shows her nursing a baby with her breast, dated to 10th-12th century). The temple sanctum, however, has no idols. The sanctum features a yoni-shaped natural rock with a fissure and a natural water spring flowing over it. The Kamakhya yoni is linked to the Shiva-Sati legend, both mentioned in the early puranic literature related to Shaktism such as the \"Kalika Purana\".\nEvery year, about the start of monsoons, the natural spring turns red because of iron oxide and \"sindoor\" (red pigment) anointed by the devotees and temple priests. This is celebrated as a symbol of the menstruating goddess, and as the Ambubachi Mela (also known as \"Ambuvaci\" or \"ameti\"), an annual fertility festival held in June. During Ambubachi, a symbolic annual menstruation course of the goddess Kamakhya is worshipped in the Kamakhya Temple. The temple stays closed for three days and then reopens to receive pilgrims and worshippers. The sanctum with the yoni of the goddess is one of the most important pilgrimage sites for the Shakti tradition, attracting between 70,000 and 200,000 pilgrims during the \"Ambubachi Mela\" alone from the northeastern and eastern states of India such as West Bengal, Bihar and Uttar Pradesh. It also attracts yogis, tantrikas, sadhus, aghoris as well as other monks and nuns from all over India.\nYantra.\nIn esoteric traditions such as tantra, particularly the Sri Chakra tradition, the main icon (yantra) has nine interlocking triangles. Five of these point downwards and these are consider symbols of yoni, while four point upwards and these are symbols of linga. The interlocking represents the interdependent union of the feminine and masculine energies for the creation and destruction of existence.\nSoutheast Asia.\nYoni typically with linga is found in historic stone temples and panel reliefs of Indonesia, Vietnam, Cambodia, and Thailand. In Cham literature, yoni is sometimes referred to as \"Awar\", while the linga is referred to as \"Ahier\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66305", "revid": "40399160", "url": "https://en.wikipedia.org/wiki?curid=66305", "title": "Seven Days (TV series)", "text": "Science fiction television series based on the premise of limited time travel\nSeven Days, stylized as SEVEN 7 DAYS, is an American science fiction television series based on the premise of time travel. It was created by Christopher and Zachary Crowe, and aired on UPN from October 7, 1998 to May 29, 2001.\nSynopsis.\nThe plot follows Frank B. Parker, a former Navy SEAL and CIA operative who was drafted as a member of \"Project Backstep\", a secret black-ops branch of the US National Security Agency stationed in a base located somewhere in the Nevada desert called \"Never Never Land\" (a play on Area 51, or Groom Lake Flight Test Facilities, also known as \"Dreamland\") responding specifically to national security issues that would otherwise endanger the safety of America and the world at large, utilizing the \"Chronosphere\"\u2014an experimental time machine reverse-engineered from alien technology found at Roswell years ago\u2014to avert disasters before they begin.\nThe Chronosphere.\nThe \"Chronosphere\", otherwise classified as the \"Backstep Sphere\" or simply \"the Sphere\", is a blue-colored 16-sided chamfered dodecahedron time machine with a detachable vacuum-sealed entry hatch. As each episode's introduction implies, the Chronosphere is designed to send \"one human being back in time seven days\" to avert disasters, referred to as a \"backstep\". The show's title refers to the chief limitation of the technology, namely that a \"chrononaut\" can only \"backstep\" seven days due to limitations imposed by the device's fuel source\u2014a transuranic alien substance salvaged from the Roswell crash site known as \"Element-115\"\u2014and its external reactor outside of its hangar. As the fuel source is limited, there is a strict mandate that the backstep is confined to events relating directly to national security, though it can replenish itself to a sufficient amount seven days after its usage.\nWhen sufficiently charged to 100%, the Chronosphere's reactor, the gravitational field generators located outside the Chronosphere and Element-115 itself create a time-displacement field around the device before seemingly vanishes from existence in a bright flash of light as it slingshots into a wormhole in space where the time-bending properties of space itself works in tandem with the time-displacement field to send the Chronosphere and its contents backward in time and into Earth as it crashes down for landing. In the process, past iterations of the Chronosphere and its contents fades from existence to prevent further paradoxes as if it was never there, stating that \"two instances of the same object cannot occupy the same space\".\nWhile it is accurate in traveling through time, navigating the Chronosphere to its destination seven days into the past requires having to use the navigation joystick to maintain and center the Sphere's six gravitational axes (referred to as \"flying the needles\") as \"backstepping\" has often proven to be agonizingly painful on a physical and psychological level during its transit, sometimes leading to fatal worst-case scenarios should the chrononaut prematurely let go of the joystick before the transit is complete such as being stuck in space or phased into the ground.\nHowever, being a reverse-engineered experimental tech based on alien technology, the Chronosphere tends to suffer from a variety of malfunctions, either due to the unpredictable properties of Element-115 or the untested nature of the device itself, as the recurring element of the show has Parker and/or Project Backstep having to prevent any given crisis under the limitations of the Sphere's unpredictable effects, ranging from causing time loops (one of which is in the vein of \"Run Lola Run\"), intercepting a soul on its way to the afterlife that results in the Chronosphere creating a black hole in its hull, reverting one's mind to a child-like state, being stuck in the body of a Pope, separating one's soul from its body, transporting into a parallel universe, and splitting the chrononaut into two opposing halves, among many others.\nProduction.\nThree seasons of \"Seven Days\" were produced. All three seasons have been shown in North America, and by the BBC in the United Kingdom.\n\"Seven Days\" was based on an idea from Kerry McCluggage, then-president of Paramount Television. He pitched the idea to Christopher Crowe, who mixed it with his own research on Area 51 to create the series. The show was not well-received by reviewers, who criticized the show's \"flimsy\" premise and violence.\nOriginal cast member Sam Whipple, who played Dr. John Ballard, left the series four episodes into the third season, due to a diagnosis of cancer that was eventually fatal. He was replaced by Kevin Christy as Andrew \"Hooter\" Owsley for the rest of the season.\nJustina Vail, who played Dr. Olga Vukavitch, quit the series before the end of the third season, though she agreed to film a few extra scenes to wrap up her character's arc. Her departure and the tensions within the cast, as well as the show's low ratings, played a role in UPN's decision not to renew the series for a fourth season.\nVail's final episode was the series' penultimate episode, \"Born in the USSR\" although she was seen as usual in the title sequence from the final episode, \"Live: From Death Row\".\nDVD release.\nOn November 26, 2018, Visual Entertainment released the complete series on DVD in Region 1 for the first time.\nAwards.\n\"Seven Days\" was nominated for six awards, winning one. Actress Justina Vail won a Saturn Award in 2000 for her performance on the show.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66306", "revid": "45900750", "url": "https://en.wikipedia.org/wiki?curid=66306", "title": "Osmium tetroxide", "text": "&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nOsmium tetroxide (also osmium(VIII) oxide) is the chemical compound with the formula OsO4. The compound is noteworthy for its many uses, despite its toxicity and the rarity of osmium. It also has a number of unusual properties, one being that the solid is volatile. The compound is colourless, but most samples appear yellow. This is most likely due to the presence of the impurity osmium dioxide (OsO2), which is yellow-brown in colour. In biology, its property of binding to lipids has made it a widely used stain in electron microscopy.\nPhysical properties.\nOsmium(VIII) oxide forms monoclinic crystals. It has a characteristic acrid chlorine-like odor. The element name osmium is derived from \"osme\", Greek for \"odor\". OsO4 is volatile: it sublimes at room temperature. It is soluble in a wide range of organic solvents. It is moderately soluble in water, with which it reacts reversibly to form osmic acid (see below). \"Pure\" osmium(VIII) oxide is probably colourless; it has been suggested that its yellow hue is attributable due to osmium dioxide (OsO2) impurities. The osmium tetroxide molecule is tetrahedral and therefore nonpolar. This nonpolarity helps OsO4 penetrate charged cell membranes.\nStructure and electron configuration.\nThe osmium of OsO4 has an oxidation number of VIII; however, the metal does not possess a corresponding 8+ charge as the bonding in the compound is largely covalent in character (the ionization energy required to produce a formal 8+ charge also far exceeds the energies available in normal chemical reactions). The osmium atom exhibits double bonds to the four oxide ligands, resulting in a 16 electron complex. OsO4 is isoelectronic with permanganate and chromate ions.\nSynthesis.\nOsO4 is formed slowly when osmium powder reacts with O2 at ambient temperature. Reaction of bulk solid requires heating to 400\u00a0\u00b0C.\nReactions.\nOxidation of alkenes.\nAlkenes add to OsO4 to give diolate species that hydrolyze to \"cis\"-diols. The net process is called dihydroxylation. This proceeds via a [3 + 2] cycloaddition reaction between the OsO4 and alkene to form an intermediate osmate ester that rapidly hydrolyses to yield the vicinal diol. As the oxygen atoms are added in a concerted step, the resulting stereochemistry is \"cis\".\nOsO4 is expensive and highly toxic, making it an unappealing reagent to use in stoichiometric amounts. However, its reactions are made catalytic by adding reoxidants to reoxidise the Os(VI) by-product back to Os(VIII). Typical reagents include H2O2 (Milas hydroxylation), N-methylmorpholine N-oxide (Upjohn dihydroxylation) and K3Fe(CN)6/water. These reoxidants do not react with the alkenes on their own. Other osmium compounds can be used as catalysts, including osmate(VI) salts ([OsO2(OH)4)]2\u2212, and osmium trichloride hydrate (OsCl3\u00b7\"x\"H2O). These species oxidise to osmium(VIII) in the presence of such oxidants.\nLewis bases such as tertiary amines and pyridines increase the rate of dihydroxylation. This \"ligand-acceleration\" arises via the formation of adduct OsO4L, which adds more rapidly to the alkene. If the amine is chiral, then the dihydroxylation can proceed with enantioselectivity (see Sharpless asymmetric dihydroxylation). OsO4 does not react with most carbohydrates.\nThe process can be extended to give two aldehydes in the Lemieux\u2013Johnson oxidation, which uses periodate to achieve diol cleavage and to regenerate the catalytic loading of OsO4. This process is equivalent to that of ozonolysis.\nCoordination chemistry.\nOsO4 is a Lewis acid and a mild oxidant. It reacts with alkaline aqueous solution to give the perosmate anion OsO4(OH)22\u2212. This species is easily reduced to osmate anion, OsO2(OH)42-.\nWhen the Lewis base is an amine, adducts are also formed. Thus OsO4 can be stored in the form of osmeth, in which OsO4 is complexed with hexamine. Osmeth can be dissolved in tetrahydrofuran (THF) and diluted in an aqueous buffer solution to make a dilute (0.25%) working solution of OsO4.\nWith tert-BuNH2, the imido derivative is produced:\nOsO4 + Me3CNH2 \u2192 OsO3(NCMe3) + H2O\nSimilarly, with NH3 one obtains the nitrido complex:\nOsO4 + NH3 + KOH \u2192 + 2 H2O\nThe \u2212 anion is isoelectronic and isostructural with OsO4.\nOsO4 is very soluble in tert-butyl alcohol. In solution, it is readily reduced by hydrogen to osmium metal. The suspended osmium metal can be used to catalytically hydrogenate a wide variety of organic chemicals containing double or triple bonds.\nOsO4 + 4 H2 \u2192 Os + 4 H2O\nOsO4 undergoes \"reductive carbonylation\" with carbon monoxide in methanol at 400\u00a0K and 200\u00a0bar to produce the triangular cluster Os3(CO)12:\n3 OsO4 + 24 CO \u2192 Os3(CO)12 + 12 CO2\nOxofluorides.\nOsmium forms several oxofluorides, all of which are very sensitive to moisture.\nPurple \"cis\"-OsO2F4 forms at 77\u00a0K in an anhydrous HF solution:\n OsO4 + 2 KrF2 \u2192 \"cis\"-OsO2F4 + 2 Kr + O2\nOsO4 also reacts with F2 to form yellow OsO3F2:\n 2 OsO4 + 2 F2 \u2192 2 OsO3F2 + O2\nOsO4 reacts with one equivalent of [Me4N]F at 298\u00a0K and 2 equivalents at 253\u00a0K:\n OsO4 + [Me4N]F \u2192 [Me4N][OsO4F]\n OsO4 + 2 [Me4N]F \u2192 [Me4N]2[\"cis\"-OsO4F2]\nUses.\nOrganic synthesis.\nIn organic synthesis OsO4 is widely used to oxidize alkenes to the vicinal diols, adding two hydroxyl groups at the same side (syn addition). See reaction and mechanism above. This reaction has been made both catalytic (Upjohn dihydroxylation) and asymmetric (Sharpless asymmetric dihydroxylation).\nOsmium(VIII) oxide is also used in catalytic amounts in the Sharpless oxyamination to give vicinal amino-alcohols.\nIn combination with sodium periodate, OsO4 is used for the oxidative cleavage of alkenes (Lemieux-Johnson oxidation) when the periodate serves both to cleave the diol formed by dihydroxylation, and to regenerate OsO4. The net transformation is identical to that produced by ozonolysis. Below an example from the total synthesis of Isosteviol.\nBiological staining.\nOsO4 is a widely used staining agent used in transmission electron microscopy (TEM) to provide contrast to the image. This staining method may also be known in the literature as the OTO (osmium-thiocarbohydrazide-osmium) method, or osmium impregnation technique or simply as osmium staining. As a lipid stain, it is also useful in scanning electron microscopy (SEM) as an alternative to sputter coating. It embeds a heavy metal directly into cell membranes, creating a high electron scattering rate without the need for coating the membrane with a layer of metal, which can obscure details of the cell membrane. In the staining of the plasma membrane, osmium(VIII) oxide binds phospholipid head regions, thus creating contrast with the neighbouring protoplasm (cytoplasm). Additionally, osmium(VIII) oxide is also used for fixing biological samples in conjunction with HgCl2. Its rapid killing abilities are used to quickly kill live specimens such as protozoa. OsO4 stabilizes many proteins by transforming them into gels without destroying structural features. Tissue proteins that are stabilized by OsO4 are not coagulated by alcohols during dehydration. Osmium(VIII) oxide is also used as a stain for lipids in optical microscopy. OsO4 also stains the human cornea (see safety considerations).\nPolymer staining.\nIt is also used to stain copolymers preferentially, the best known example being block copolymers where one phase can be stained so as to show the microstructure of the material. For example, styrene-butadiene block copolymers have a central polybutadiene chain with polystyrene end caps. When treated with OsO4, the butadiene matrix reacts preferentially and so absorbs the oxide. The presence of a heavy metal is sufficient to block the electron beam, so the polystyrene domains are seen clearly in thin films in TEM.\nOsmium ore refining.\nOsO4 is an intermediate in the extraction of osmium from its ores. Osmium-containing residues are treated with sodium peroxide (Na2O2) forming Na2[OsO4(OH)2], which is soluble. When exposed to chlorine, this salt gives OsO4. In the final stages of refining, crude OsO4 is dissolved in alcoholic NaOH forming Na2[OsO2(OH)4], which, when treated with NH4Cl, to give . This salt is reduced under hydrogen to give osmium.\nBuckminsterfullerene adduct.\nOsO4 allowed for the confirmation of the soccer ball model of buckminsterfullerene, a 60-atom carbon allotrope. The adduct, formed from a derivative of OsO4, was C60(OsO4)(4-\"tert\"-butylpyridine)2. The adduct broke the fullerene's symmetry, allowing for crystallization and confirmation of the structure of C60 by X-ray crystallography.\nMedicine.\nThe only known clinical use of osmium tetroxide is for the treatment of arthritis. The lack of reports of long-term side effects from the local administration of osmium tetroxide (OsO4) suggest that osmium itself can be biocompatible, though this depends on the osmium compound administered.\nSafety considerations.\nOsO4 will irreversibly stain the human cornea, which can lead to blindness. The permissible exposure limit for osmium(VIII) oxide (8 hour time-weighted average) is 2\u00a0\u03bcg/m3. Osmium(VIII) oxide can penetrate plastics and food packaging, and therefore must be stored in glass under refrigeration.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66313", "revid": "50479220", "url": "https://en.wikipedia.org/wiki?curid=66313", "title": "Redox", "text": "Chemical reaction with oxidation state changes\nRedox ( , , reduction\u2013oxidation or oxidation\u2013reduction) is a type of chemical reaction in which the oxidation states of the reactants change. Oxidation is the loss of electrons or an increase in the oxidation state, while reduction is the gain of electrons or a decrease in the oxidation state. The oxidation and reduction processes occur simultaneously in the chemical reaction.\nThere are two classes of redox reactions:\nTerminology.\n\"Redox\" is a portmanteau of \"reduction\" and \"oxidation.\" The term was first used in a 1928 article by Leonor Michaelis and Louis B. Flexner.\nOxidation is a process in which a substance loses electrons. Reduction is a process in which a substance gains electrons.\nThe processes of oxidation and reduction occur simultaneously and cannot occur independently. In redox processes, the reductant transfers electrons to the oxidant. Thus, in the reaction, the reductant or reducing agent loses electrons and is oxidized, and the oxidant or oxidizing agent gains electrons and is reduced. The pair of an oxidizing and reducing agent that is involved in a particular reaction is called a redox pair. A redox couple is a reducing species and its corresponding oxidizing form, e.g., Fe2+/ Fe3+.The oxidation alone and the reduction alone are each called a \"half-reaction\" because two half-reactions always occur together to form a whole reaction.\nIn electrochemical reactions the oxidation and reduction processes do occur simultaneously but are separated in space.\nOxidants.\nOxidation originally implied a reaction with oxygen to form an oxide. Later, the term was expanded to encompass substances that accomplished chemical reactions similar to those of oxygen. Ultimately, the meaning was generalized to include all processes involving the loss of electrons or the increase in the oxidation state of a chemical species. Substances that have the ability to oxidize other substances (cause them to lose electrons) are said to be oxidative or oxidizing, and are known as oxidizing agents, oxidants, or oxidizers. The oxidant removes electrons from another substance, and is thus itself reduced. Because it \"accepts\" electrons, the oxidizing agent is also called an electron acceptor. Oxidants are usually chemical substances with elements in high oxidation states (e.g., N2O4, MnO4-, CrO3, Cr2O72-, OsO4), or else highly electronegative elements (e.g. O2, F2, Cl2, Br2, I2) that can gain extra electrons by oxidizing another substance.\nOxidizers are oxidants, but the term is mainly reserved for sources of oxygen, particularly in the context of explosions. Nitric acid is a strong oxidizer.\nReductants.\nSubstances that have the ability to reduce other substances (cause them to gain electrons) are said to be reductive or reducing and are known as reducing agents, reductants, or reducers. The reductant transfers electrons to another substance and is thus itself oxidized. Because it donates electrons, the reducing agent is also called an electron donor. Electron donors can also form charge transfer complexes with electron acceptors. The word reduction originally referred to the loss in weight upon heating a metallic ore such as a metal oxide to extract the metal. In other words, ore was \"reduced\" to metal. Antoine Lavoisier demonstrated that this loss of weight was due to the loss of oxygen as a gas. Later, scientists realized that the metal atom gains electrons in this process. The meaning of reduction then became generalized to include all processes involving a gain of electrons. Reducing equivalent refers to chemical species which transfer the equivalent of one electron in redox reactions. The term is common in biochemistry. A reducing equivalent can be an electron or a hydrogen atom as a hydride ion.\nReductants in chemistry are very diverse. Electropositive elemental metals, such as lithium, sodium, magnesium, iron, zinc, and aluminium, are good reducing agents. These metals donate electrons relatively readily.\nHydride transfer reagents, such as NaBH4 and LiAlH4, reduce by atom transfer: they transfer the equivalent of hydride or H\u2212. These reagents are widely used in the reduction of carbonyl compounds to alcohols. A related method of reduction involves the use of hydrogen gas (H2) as sources of H atoms.\nElectronation and de-electronation.\nThe electrochemist John Bockris proposed the words electronation and de-electronation to describe reduction and oxidation processes, respectively, when they occur at electrodes. These words are analogous to protonation and deprotonation. IUPAC has recognized the terms electronation and de-electronation.\nRates, mechanisms, and energies.\nRedox reactions can occur slowly, as in the formation of rust, or rapidly, as in the case of burning fuel. Electron transfer reactions are generally fast, occurring within the time of mixing.\nThe mechanisms of atom-transfer reactions are highly variable because many kinds of atoms can be transferred. Such reactions can also be quite complex, involving many steps. The mechanisms of electron-transfer reactions occur by two distinct pathways, inner sphere electron transfer and outer sphere electron transfer.\nAnalysis of bond energies and ionization energies in water allows calculation of the thermodynamic aspects of redox reactions.\nStandard electrode potentials (reduction potentials).\nEach half-reaction has a standard electrode potential (\"E\"), which is equal to the potential difference or voltage at equilibrium under standard conditions of an electrochemical cell in which the cathode reaction is the half-reaction considered, and the anode is a standard hydrogen electrode where hydrogen is oxidized:\n&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 H2 \u2192 H+ + e\u2212\nThe electrode potential of each half-reaction is also known as its reduction potential (\"E\"), or potential when the half-reaction takes place at a cathode. The reduction potential is a measure of the tendency of the oxidizing agent to be reduced. Its value is zero for H+ + e\u2212 \u2192 &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442H2 by definition, positive for oxidizing agents stronger than H+ (e.g., +2.866 V for F2) and negative for oxidizing agents that are weaker than H+ (e.g., \u22120.763V for Zn2+).\nFor a redox reaction that takes place in a cell, the potential difference is:\n\"E\" = \"E\" \u2212 \"E\"\nHowever, the potential of the reaction at the anode is sometimes expressed as an \"oxidation potential\":\n\"E\" = \u2212\"E\"\nThe oxidation potential is a measure of the tendency of the reducing agent to be oxidized but does not represent the physical potential at an electrode. With this notation, the cell voltage equation is written with a plus sign\n\"E\" = \"E\" + \"E\"\nExamples of redox reactions.\nIn the reaction between hydrogen and fluorine, hydrogen is being oxidized and fluorine is being reduced:\nThis spontaneous reaction releases a large amount of energy (542 kJ per 2 g of hydrogen) because two H-F bonds are much stronger than one H-H bond and one F-F bond. This reaction can be analyzed as two half-reactions. The oxidation reaction converts hydrogen to protons:\nThe reduction reaction converts fluorine to the fluoride anion:\nThe half-reactions are combined so that the electrons cancel:\nThe protons and fluoride combine to form hydrogen fluoride in a non-redox reaction:\n2 H+ + 2 F\u2212 \u2192 2 HF\nThe overall reaction is:\nMetal displacement.\nIn this type of reaction, a metal atom in a compound or solution is replaced by an atom of another metal. For example, copper is deposited when zinc metal is placed in a copper(II) sulfate solution:\nIn the above reaction, zinc metal displaces the copper(II) ion from the copper sulfate solution, thus liberating free copper metal. The reaction is spontaneous and releases 213 kJ per 65 g of zinc.\nThe ionic equation for this reaction is:\nAs two half-reactions, it is seen that the zinc is oxidized:\nAnd the copper is reduced:\n\nHere the overall equation involves adding the reduction equation to twice the oxidation equation, so that the electrons cancel:\nDisproportionation.\nA disproportionation reaction is one in which a single substance is both oxidized and reduced. For example, thiosulfate ion with sulfur in oxidation state +2 can react in the presence of acid to form elemental sulfur (oxidation state 0) and sulfur dioxide (oxidation state +4).\nThus one sulfur atom is reduced from +2 to 0, while the other is oxidized from +2 to +4.\nRedox reactions in industry.\nCathodic protection is a technique used to control the corrosion of a metal surface by making it the cathode of an electrochemical cell. A simple method of protection connects protected metal to a more easily corroded \"sacrificial anode\" to act as the anode. The sacrificial metal, instead of the protected metal, then corrodes. \nOxidation is used in a wide variety of industries, such as in the production of and oxidizing ammonia to produce nitric acid.\nRedox reactions are the foundation of electrochemical cells, which can generate electrical energy or support electrosynthesis. Metal ores often contain metals in oxidized states, such as oxides or sulfides, from which the pure metals are extracted by smelting at high temperatures in the presence of a reducing agent. The process of electroplating uses redox reactions to coat objects with a thin layer of a material, as in chrome-plated automotive parts, silver plating cutlery, galvanization and gold-plated jewelry.\nRedox reactions in biology.\nMany essential biological processes involve redox reactions. Before some of these processes can begin, iron must be assimilated from the environment.\nAerobic cellular respiration, for instance, is the oxidation of substrates [in this case: glucose (C6H12O6)] and the reduction of oxygen to water. The summary equation for aerobic respiration is:\nThe process of cellular respiration also depends heavily on the reduction of NAD+ to NADH and the reverse reaction (the oxidation of NADH to NAD+). Photosynthesis and cellular respiration are complementary, but photosynthesis is not the reverse of the redox reaction in cellular respiration:\nBiological energy is frequently stored and released using redox reactions. Photosynthesis involves the reduction of carbon dioxide into sugars and the oxidation of water into molecular oxygen. The reverse reaction, respiration, oxidizes sugars to produce carbon dioxide and water. As intermediate steps, the reduced carbon compounds are used to reduce nicotinamide adenine dinucleotide (NAD+) to NADH, which then contributes to the creation of a proton gradient, which drives the synthesis of adenosine triphosphate (ATP) and is maintained by the reduction of oxygen. In animal cells, mitochondria perform similar functions.\nThe term \"redox state\" is often used to describe the balance of GSH/GSSG, NAD+/NADH and NADP+/NADPH in a biological system such as a cell or organ. The redox state is reflected in the balance of several sets of metabolites (e.g., lactate and pyruvate, beta-hydroxybutyrate and acetoacetate), whose interconversion is dependent on these ratios. Redox mechanisms also control some cellular processes. Redox proteins and their genes must be co-located for redox regulation according to the CoRR hypothesis for the function of DNA in mitochondria and chloroplasts.\nRedox cycling.\nWide varieties of aromatic compounds are enzymatically reduced to form free radicals that contain one more electron than their parent compounds. In general, the electron donor is any of a wide variety of flavoenzymes and their coenzymes. Once formed, these anion free radicals reduce molecular oxygen to superoxide and regenerate the unchanged parent compound. The net reaction is the oxidation of the flavoenzyme's coenzymes and the reduction of molecular oxygen to form superoxide. This catalytic behavior has been described as a futile cycle or redox cycling.\nRedox reactions in geology.\nMinerals are generally oxidized derivatives of metals. Iron is mined as ores such as magnetite (Fe3O4) and hematite (Fe2O3). Titanium is mined as its dioxide, usually in the form of rutile (TiO2). These oxides must be reduced to obtain the corresponding metals, often achieved by heating these oxides with carbon or carbon monoxide as reducing agents. Blast furnaces are the reactors where iron oxides and coke (a form of carbon) are combined to produce molten iron. The main chemical reaction producing the molten iron is:\nRedox reactions in soils.\nElectron transfer reactions are central to myriad processes and properties in soils, and redox potential, quantified as Eh (platinum electrode potential (voltage) relative to the standard hydrogen electrode) or pe (analogous to pH as \u2212log electron activity), is a master variable, along with pH, that controls and is governed by chemical reactions and biological processes. Early theoretical research with applications to flooded soils and paddy rice production was seminal for subsequent work on thermodynamic aspects of redox and plant root growth in soils. Later work built on this foundation, and expanded it for understanding redox reactions related to heavy metal oxidation state changes, pedogenesis and morphology, organic compound degradation and formation, free radical chemistry, wetland delineation, soil remediation, and various methodological approaches for characterizing the redox status of soils.\nMnemonics.\nThe key terms involved in redox can be confusing. For example, a reagent that is oxidized loses electrons; however, that reagent is referred to as the reducing agent. Likewise, a reagent that is reduced gains electrons and is referred to as the oxidizing agent. These mnemonics are commonly used by students to help memorise the terminology:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66315", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66315", "title": "Solid-state chemistry", "text": "Study of solid materials' properties and composition\nSolid-state chemistry, also sometimes referred as materials chemistry, is the study of the synthesis, structure, and properties of solid phase materials. It therefore has a strong overlap with solid-state physics, mineralogy, crystallography, ceramics, metallurgy, thermodynamics, materials science and electronics with a focus on the synthesis of novel materials and their characterization. A diverse range of synthetic techniques, such as the ceramic method and chemical vapour depostion, make solid-state materials. Solids can be classified as crystalline or amorphous on basis of the nature of order present in the arrangement of their constituent particles. Their elemental compositions, microstructures, and physical properties can be characterized through a variety of analytical methods.\nHistory.\nBecause of its direct relevance to products of commerce, solid state inorganic chemistry has been strongly driven by technology. Progress in the field has often been fueled by the demands of industry, sometimes in collaboration with academia. Applications discovered in the 20th century include zeolite and platinum-based catalysts for petroleum processing in the 1950s, high-purity silicon as a core component of microelectronic devices in the 1960s, and \u201chigh temperature\u201d superconductivity in the 1980s. The invention of X-ray crystallography in the early 1900s by William Lawrence Bragg was an enabling innovation. Our understanding of how reactions proceed at the atomic level in the solid state was advanced considerably by Carl Wagner's work on oxidation rate theory, counter diffusion of ions, and defect chemistry. Because of his contributions, he has sometimes been referred to as the \"father of solid state chemistry\".\nSynthetic methods.\nGiven the diversity of solid-state compounds, an equally diverse array of methods are used for their preparation. Synthesis can range from high-temperature methods, like the ceramic method, to gas methods, like chemical vapour deposition. Often, the methods prevent defect formation or produce high-purity products.\nHigh-temperature methods.\nCeramic method.\nThe ceramic method is one of the most common synthesis techniques. The synthesis occurs entirely in the solid state. \u00a0The reactants are ground together, formed into a pellet using a pellet press and hydraulic press, and heated at high temperatures. When the temperature of the reactants are sufficient, the ions at the grain boundaries react to form desired phases. Generally ceramic methods give polycrystalline powders, but not single crystals. \nUsing a mortar and pestle, ResonantAcoustic mixer, or ball mill, the reactants are ground together, which decreases size and increases surface area of the reactants. If the mixing is not sufficient, we can use techniques such as co-precipitation and sol-gel. A chemist forms pellets from the ground reactants and places the pellets into containers for heating. The choice of container depends on the precursors, the reaction temperature and the expected product. For example, metal oxides are typically synthesized in silica or alumina containers. A tube furnace heats the pellet. Tube furnaces are available up to maximum temperatures of 2800\u00b0C. \nMolten flux synthesis.\nMolten flux synthesis can be an efficient method for obtaining single crystals. In this method, the starting reagents are combined with flux, an inert material with a melting point lower than that of the starting materials. The flux serves as a solvent. After the reaction, the excess flux can be washed away using an appropriate solvent or it can be heat again to remove the flux by sublimation if it is a volatile compound. \nCrucible materials have a great role to play in molten flux synthesis. The crucible should not react with the flux or the starting reagent. If any of the material is volatile, it is recommended to conduct the reaction in a sealed ampule. If the target phase is sensitive to oxygen, a carbon- coated fused silica tube or a carbon crucible inside a fused silica tube is often used which prevents the direct contact between the tube wall and reagents.\nChemical vapour transport.\nChemical vapour transport results in very pure materials. The reaction typically occurs in a sealed ampoule. A transporting agent, added to the sealed ampoule, produces a volatile intermediate species from the solid reactant. For metal oxides, the transporting agent is usually Cl2 or HCl. The ampoule has a temperature gradient, and, as the gaseous reactant travels along the gradient, it eventually deposits as a crystal. An example of an industrially-used chemical vapor transport reaction is the Mond process. The Mond process involves heating impure nickel in a stream of carbon monoxide to produce pure nickel.\nLow-temperature methods.\nIntercalation method.\nIntercalation synthesis is the insertion of molecules or ions between layers of a solid. The layered solid has weak intermolecular bonds holding its layers together. The process occurs via diffusion. Intercalation is further driven by ion exchange, acid-base reactions or electrochemical reactions. The intercalation method was first used in China with the discovery of porcelain. Also, graphene is produced by the intercalation method, and this method is the principle behind lithium-ion batteries.\nSolution methods.\nIt is possible to use solvents to prepare solids by precipitation or by evaporation. At times, the solvent is a hydrothermal that is under pressure at temperatures higher than the normal boiling point. A variation on this theme is the use of flux methods, which use a salt with a relatively low melting point as the solvent.\nGas methods.\nMany solids react vigorously with gas species like chlorine, iodine, and oxygen. Other solids form adducts, such as CO or ethylene. Such reactions are conducted in open-ended tubes, which the gasses are passed through. Also, these reactions can take place inside a measuring device such as a TGA. In that case, stoichiometric information can be obtained during the reaction, which helps identify the products.\nChemical vapour deposition.\nChemical vapour deposition is a method widely used for the preparation of coatings and semiconductors from molecular precursors. A carrier gas transports the gaseous precursors to the material for coating.\nCharacterization.\nThis is the process in which a material\u2019s chemical composition, structure, and physical properties are determined using a variety of analytical techniques.\nNew phases.\nSynthetic methodology and characterization often go hand in hand in the sense that not one but a series of reaction mixtures are prepared and subjected to heat treatment. Stoichiometry, a numerical relationship between the quantities of reactant and product, is typically varied systematically. It is important to find which stoichiometries will lead to new solid compounds or solid solutions between known ones. A prime method to characterize the reaction products is powder diffraction because many solid-state reactions will produce polycrystalline molds or powders. Powder diffraction aids in the identification of known phases in the mixture. If a pattern is found that is not known in the diffraction data libraries, an attempt can be made to index the pattern. The characterization of a material's properties is typically easier for a product with crystalline structures.\nCompositions and structures.\nOnce the unit cell of a new phase is known, the next step is to establish the stoichiometry of the phase. This can be done in several ways. Sometimes the composition of the original mixture will give a clue, under the circumstances that only a product with a single powder pattern is found or a phase of a certain composition is made by analogy to known material, but this is rare.\nOften, considerable effort in refining the synthetic procedures is required to obtain a pure sample of the new material. If it is possible to separate the product from the rest of the reaction mixture, elemental analysis methods such as scanning electron microscopy (SEM) and transmission electron microscopy (TEM) can be used. The detection of scattered and transmitted electrons from the surface of the sample provides information about the surface topography and composition of the material. Energy dispersive X-ray spectroscopy (EDX) is a technique that uses electron beam excitation. Exciting the inner shell of an atom with incident electrons emits characteristic X-rays with specific energy to each element. The peak energy can identify the chemical composition of a sample, including the distribution and concentration.Similar to EDX, X-ray diffraction analysis (XRD) involves the generation of characteristic X-rays upon interaction with the sample. The intensity of diffracted rays scattered at different angles is used to analyze the physical properties of a material such as phase composition and crystallographic structure. These techniques can also be coupled to achieve a better effect. For example, SEM is a useful complement to EDX due to its focused electron beam, it produces a high-magnification image that provides information on the surface topography. Once the area of interest has been identified, EDX can be used to determine the elements present in that specific spot. Selected area electron diffraction can be coupled with TEM or SEM to investigate the level of crystallinity and the lattice parameters of a sample.\nMore information.\nX-ray diffraction is also used due to its imaging capabilities and speed of data generation. The latter often requires \"revisiting\" and refining the preparative procedures and that are linked to the question of which phases are stable at what composition and what stoichiometry. In other words, what the phase diagram looks like. An important tool in establishing this are thermal analysis techniques like DSC or DTA and increasingly also, due to the advent of synchrotrons, temperature-dependent powder diffraction. Increased knowledge of the phase relations often leads to further refinement in synthetic procedures in an iterative way. New phases are thus characterized by their melting points and their stoichiometric domains. The latter is important for the many solids that are non-stoichiometric compounds. The cell parameters obtained from XRD are particularly helpful to characterize the homogeneity ranges of the latter.\nLocal structure.\nIn contrast to the large structures of crystals, the local structure describes the interaction of the nearest neighbouring atoms. Methods of nuclear spectroscopy use specific nuclei to probe the electric and magnetic fields around the nucleus. E.g. electric field gradients are very sensitive to small changes caused by lattice expansion/compression (thermal or pressure), phase changes, or local defects. Common methods are M\u00f6ssbauer spectroscopy and perturbed angular correlation.\nOptical properties.\nFor metallic materials, their optical properties arise from the collective excitation of conduction electrons. The coherent oscillations of electrons under electromagnetic radiation along with associated oscillations of the electromagnetic field are called surface plasmon resonances. The excitation wavelength and frequency of the plasmon resonances provide information on the particle's size, shape, composition, and local optical environment.\nFor non-metallic materials or semiconductors, they can be characterized by their band structure. It contains a band gap that represents the minimum energy difference between the top of the valence band and the bottom of the conduction band. The band gap can be determined using Ultraviolet-visible spectroscopy to predict the photochemical properties of the semiconductors.\nFurther characterization.\nIn many cases, new solid compounds are further characterized by a variety of techniques that straddle the fine line that separates solid-state chemistry from solid-state physics. See Characterisation in material science for additional information."}
{"id": "66316", "revid": "1295089202", "url": "https://en.wikipedia.org/wiki?curid=66316", "title": "Burette", "text": "Graduated glass tube with a tap at one end\nA burette (also spelled buret) is a graduated glass tube with a tap at one end, for delivering known volumes of a liquid, especially in titrations. It is a long, graduated glass tube, with a stopcock at its lower end and a tapered capillary tube at the stopcock's outlet. The flow of liquid from the tube to the burette tip is controlled by the stopcock valve. \nThere are two main types of burette; the volumetric burette and the piston burette. A volumetric burette delivers measured volumes of liquid. Piston burettes are similar to syringes, but with a precision bore and a plunger. Piston burettes may be manually operated or may be motorized. A weight burette delivers measured weights of a liquid.\nOverview.\nA burette is a volumetric measuring glassware which is used in analytical chemistry for the accurate dispensing of a liquid, especially of one of the reagents in a titration. The burette tube carries graduated marks from which the dispensed volume of the liquid can be determined. Compared to a volumetric pipette, a burette has similar precision if used to its full capacity, but as it is usually used to deliver less than its full capacity, a burette is slightly less precise than a pipette.\nThe burette is used to measure the volume of a dispensed substance, but is different from a graduated cylinder as its graduations measure from top to bottom. Therefore, the difference between the starting and the final volume is equal to the amount dispensed. The precision and control of the burette over other means of adding solution is beneficial for use in titration.\nVolumetric burette.\nA volumetric burette can be made of glass or plastic, and is a straight tube with a\u00a0graduation\u00a0scale. At the tip of burette, there are a\u00a0stopcock and valve to control the flow of the chemical solution. The barrel of the stopcock can be made of\u00a0glass\u00a0or the plastic\u00a0PTFE. Stopcocks with glass barrels need to be lubricated with vaseline or a specialized grease. Burettes are manufactured for specific tolerances, designated as class A or B and this also is etched on the glass.\nBurette reading.\nIn order to measure the amount of solution added in or drained out, the burette must be observed at eye level straight to the bottom of the meniscus. The liquid in the burette should be completely free of bubbles to ensure accurate measurements. The difference in volume can be calculated by taking the difference of the final and initial recorded volume. Using the burette with a colorless solution may make it difficult to observe the bottom of the meniscus, so the black strip technique can make it easier to accurately observe and record measurements.\nSpecification.\nThe specification of a volumetric burette indicates its properties, such as the nominal volume, volume unit, error limit, and accuracy class, plus other related details from the manufacturer. The nominal volume and error limit is usually given in units of mL or cm3. Another specification commonly found on burettes is the calibration mark \"TD\" or \"Ex\". This stands for \"calibrated to deliver\", indicating that the printed volume is accurate when the burette is used to deliver (rather than contain) a solution. Another commonly indicated specification is the accuracy class, including class A and class B. Class A is preferred to Class B when volumetric accuracy is important, as it has a narrower range of error with accuracy up to 0.1 percent compared to 0.2 percent in Class B burette. \nDigital burette.\nDigital burettes are based on a syringe design. The barrel and plunger may be made of glass. With liquids that corrode glass, including solutions of alkali, the barrel and plunger may be made of polyethylene or another resistant plastic material. The barrel is held in a fixed position and the plunger is moved incrementally either by turning wheel by hand, or by means of a step motor. The volume is shown on a digital display. A high-precision syringe may be used to deliver very precise aliquots. Motorized digital burettes may be controlled by a computer; for example, a titration may be recorded digitally and then subject to numerical processing to find the titer at an end-point.\nHistory.\nThe first burette was invented in 1845 by the French chemist \u00c9tienne-Ossian Henry (1798\u20131873). In 1855, the German chemist Karl Friedrich Mohr (1806\u20131879) presented an improved version of Henry's burette, having graduations inscribed on the tube of the burette.\nThe word \"burette\" was coined in 1824 by the French chemist Joseph Louis Gay-Lussac (1778\u20131850).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66317", "revid": "42907258", "url": "https://en.wikipedia.org/wiki?curid=66317", "title": "Valence", "text": "Valence or valency may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "66320", "revid": "45796270", "url": "https://en.wikipedia.org/wiki?curid=66320", "title": "Romanian", "text": "Romanian may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "66321", "revid": "98543", "url": "https://en.wikipedia.org/wiki?curid=66321", "title": "Group 6 element", "text": "Group of chemical elements\nGroup 6, numbered by IUPAC style, is a group of elements in the periodic table. Its members are chromium (Cr), molybdenum (Mo), tungsten (W), and seaborgium (Sg). These are all transition metals and chromium, molybdenum and tungsten are refractory metals.\nThe electron configuration of these elements do not follow a unified trend, though the outermost shells do correlate with trends in chemical behavior:\n\"Group 6\" is the new IUPAC name for this group; the old style name was \"\"group VIB\" in the old US system (CAS) or \"group VIA\"\" in the European system (old IUPAC). Group 6 must not be confused with the group with the old-style group crossed names of either \"VIA\" (US system, CAS) or \"VIB\" (European system, old IUPAC). \"That\" group is now called group 16.\nHistory.\nDiscoveries.\nChromium was first reported on July 26, 1761, when Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains of Russia, which he named \"Siberian red lead,\" which was found out in less than 10 years to be a bright yellow pigment. Though misidentified as a lead compound with selenium and iron components, the mineral was crocoite with a formula of PbCrO4. Studying the mineral in 1797, Louis Nicolas Vauquelin produced chromium trioxide by mixing crocoite with hydrochloric acid, and metallic chromium by heating the oxide in a charcoal oven a year later. He was also able to detect traces of chromium in precious gemstones, such as ruby or emerald.\nMolybdenite\u2014the principal ore from which molybdenum is now extracted\u2014was previously known as molybdena, which was confused with and often implemented as though it were graphite. Like graphite, molybdenite can be used to blacken a surface or as a solid lubricant. Even when molybdena was distinguishable from graphite, it was still confused with a galena (a common lead ore), which took its name from Ancient Greek \"\", meaning \"lead\". It was not until 1778 that Swedish chemist Carl Wilhelm Scheele realized that molybdena was neither graphite nor lead. He and other chemists then correctly assumed that it was the ore of a distinct new element, named \"molybdenum\" for the mineral in which it was discovered. Peter Jacob Hjelm successfully isolated molybdenum by using carbon and linseed oil in 1781.\nRegarding tungsten, in 1781 Carl Wilhelm Scheele discovered that a new acid, tungstic acid, could be made from scheelite (at the time named tungsten). Scheele and Torbern Bergman suggested that it might be possible to obtain a new metal by reducing this acid. In 1783, Jos\u00e9 and Fausto Elhuyar found an acid made from wolframite that was identical to tungstic acid. Later that year, in Spain, the brothers succeeded in isolating tungsten by reduction of this acid with charcoal, and they are credited with the discovery of the element.\nSeaborgium was first produced by a team of scientists led by Albert Ghiorso who worked at the Lawrence Berkeley Laboratory in Berkeley, California, in 1974. They created seaborgium by bombarding atoms of californium-249 with ions of oxygen-18 until seaborgium-263 was produced.\nHistorical development and uses.\nDuring the 1800s, chromium was primarily used as a component of paints and in tanning salts. At first, crocoite from Russia was the main source, but in 1827, a larger chromite deposit was discovered near Baltimore, United States. This made the United States the largest producer of chromium products until 1848 when large deposits of chromite where found near Bursa, Turkey. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.\nFor about a century after its isolation, molybdenum had no industrial use, owing to its relative scarcity, difficulty extracting the pure metal, and the immaturity of the metallurgical subfield. Early molybdenum steel alloys showed great promise in their increased hardness, but efforts were hampered by inconsistent results and a tendency toward brittleness and recrystallization. In 1906, William D. Coolidge filed a patent for rendering molybdenum ductile, leading to its use as a heating element for high-temperature furnaces and as a support for tungsten-filament light bulbs; oxide formation and degradation require that moly be physically sealed or held in an inert gas. In 1913, Frank E. Elmore developed a flotation process to recover molybdenite from ores; flotation remains the primary isolation process. During the first World War, demand for molybdenum spiked; it was used both in armor plating and as a substitute for tungsten in high-speed steels. Some British tanks were protected by 75\u00a0mm (3\u00a0in) manganese steel plating, but this proved to be ineffective. The manganese steel plates were replaced with 25\u00a0mm (1\u00a0in) molybdenum-steel plating allowing for higher speed, greater maneuverability, and better protection. After the war, demand plummeted until metallurgical advances allowed extensive development of peacetime applications. In World War II, molybdenum again saw strategic importance as a substitute for tungsten in steel alloys.\nIn World War II, tungsten played a significant role in background political dealings. Portugal, as the main European source of the element, was put under pressure from both sides, because of its deposits of wolframite ore at Panasqueira. Tungsten's resistance to high temperatures and its strengthening of alloys made it an important raw material for the arms industry.\nChemistry.\nUnlike other groups, the members of this family do not show patterns in its electron configuration, as two lighter members of the group are exceptions from the Aufbau principle:\nMost of the chemistry has been observed only for the first three members of the group. The chemistry of seaborgium is not very established and therefore the rest of the section deals only with its upper neighbors in the periodic table. The elements in the group, like those of groups 7\u201311, have high melting points, and form volatile compounds in higher oxidation states. All the elements of the group are relatively nonreactive metals with a high melting points (1907\u00a0\u00b0C, 2477\u00a0\u00b0C, 3422\u00a0\u00b0C); that of tungsten is the highest of all metals. The metals form compounds in different oxidation states: chromium forms compounds in all states from \u22122 to +6: disodium pentacarbonylchromate, disodium decacarbonyldichromate, bis(benzene)chromium, tripotassium pentanitrocyanochromate, chromium(II) chloride, chromium(III) oxide, chromium(IV) chloride, potassium tetraperoxochromate(V), and chromium(VI) dichloride dioxide; the same is also true for molybdenum and tungsten, but the stability of the +6 state grows down the group. Depending on oxidation states, the compounds are basic, amphoteric, or acidic; the acidity grows with the oxidation state of the metal.\nOccurrence and production.\nChromium is a very common naturally occurring element. It is the 21st most abundant element in the Earth's crust with an average concentration of 100 ppm. The most common oxidation states for chromium are zero, trivalent, and hexavalent states. Most naturally occurring chromium is in the hexavalent state. About two-fifths of the worlds chromium are produced in South Africa, with Kazakhstan, India, Russia, and Turkey following. Chromium is mined as chromite ore.\nMolybdenum is refined mainly from molybdenite. It is mainly mined in the United States, China, Chile, and Peru, with the total amount produced being 200,000 tonnes per year.\nTungsten is not a common element on Earth, having an average concentration of 1.5 ppm in Earth's crust. Tungsten is mainly found in the minerals wolframite and scheelite, and it usually never occurs as a free element in nature. The largest producers of tungsten in the world are China, Russia, and Portugal.\nSeaborgium is a transuranium element that is made artificially by bombarding californium-249 with oxygen-18 nuclei. It is artificial, therefore it does not occur in nature.\nPrecautions.\nHexavalent chromium compounds are genotoxic carcinogens. Seaborgium is a radioactive synthetic element that is not found in nature; the most stable known isotope has a half-life of approximately 14 minutes.\nBiological occurrences.\nGroup 6 is notable in that it contains some of the only elements in periods 5 and 6 with a known role in the biological chemistry of living organisms: molybdenum is common in enzymes of many organisms including humans, and tungsten has been identified in an analogous role in enzymes from some archaea, such as \"Pyrococcus furiosus\". In contrast, and unusually for a first-row d-block transition metal, chromium appears to have few biological roles, although it is thought to form part of the glucose metabolism enzyme in some mammals.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66322", "revid": "37664675", "url": "https://en.wikipedia.org/wiki?curid=66322", "title": "M (1931 film)", "text": "1931 film by Fritz Lang\nM is a 1931 German mystery thriller film directed by Fritz Lang and starring Peter Lorre as Hans Beckert, a serial killer who targets children, in his third screen role. Both Lang's first sound film and an early example of a procedural drama, \"M\" centres on the efforts of both a city's police force and its criminal syndicates to apprehend a serial child-murderer.\nThe film's screenplay was written by Lang and his wife Thea von Harbou. It features many cinematic innovations, including the use of long tracking shots and a musical \"leitmotif\" in the form of \"In the Hall of the Mountain King\", which is repeatedly whistled by Lorre's character. Lang regarded the film as his magnum opus, and it is widely considered one of the greatest films of all time and an indispensable influence on modern crime and thriller fiction.\nAn American remake under the same title, directed by Joseph Losey, was released in 1951.\nPlot.\nIn Berlin, a group of children are playing an elimination game in the courtyard of an apartment building using a macabre chant about a child-killer. Frau Beckmann sets the table for lunch, waiting for her daughter Elsie to come home from school. A wanted poster warns of a serial killer preying on children, as anxious parents wait outside a school.\nElsie leaves school, bouncing a ball on her way home. She is approached by Hans Beckert, who is whistling \"In the Hall of the Mountain King\" by Edvard Grieg. He offers to buy her a balloon from a blind street vendor, and walks and talks with her. Elsie's place at the table remains empty, her ball rolls away through a patch of grass, and her balloon gets briefly caught in the telephone lines overhead before blowing away in the wind.\nIn the wake of Elsie's disappearance, anxiety runs high among the public. Beckert sends an anonymous letter to the newspapers taking credit for the child murders and promising that he will commit others. The police extract clues from the letter using the new techniques of fingerprinting and handwriting analysis. Under mounting pressure from the government, the police work around the clock. Inspector Karl Lohmann, head of the homicide squad, instructs his men to intensify their search and to check the records of recently released psychiatric patients, focusing on any with a history of violence against children.\nThey stage frequent raids in seedier parts of the city to question known criminals, disrupting organized crime so badly that (\"The Safecracker\") summons the bosses of Berlin's \"Ringvereine\" to a conference to address the situation. They decide to organize their own manhunt, assigning beggars to watch the children. The police search Beckert's rented room, find evidence there connecting him to both the letter and a past crime scene, and lie in wait to arrest him.\nBeckert sees a young girl in the reflection of a shop window and begins to follow her, but stops when the girl meets her mother. He encounters another girl and befriends her, but the blind balloon vendor recognizes his whistling. The vendor alerts one of his friends, who follows Beckert and sees him inside a shop with the girl. As the two exit onto the street, the man chalks the letter \"M\" (for , \"murderer\") onto his palm, pretends to trip, and bumps into Beckert, marking the back of his overcoat with the letter. The girl eventually notices the chalk and offers to clean it for him, but before she finishes, Beckert realizes he is being watched and flees without her.\nAttempting to evade the beggars, Beckert hides inside a large office building just before the workers leave for the evening. The beggars call , who arrives at the building with a team of other criminals. They capture and torture one of the watchmen for information and, after capturing the other two, search the building and catch Beckert in the attic. When one of the watchmen trips the silent alarm, the criminals narrowly escape with their prisoner before the police arrive. Franz, one of the criminals, is left behind in the confusion and captured by the police. By falsely claiming that one of the watchmen was killed during the break-in, Lohmann tricks Franz into admitting that the gang's only motive was to find Beckert and revealing their plans for him.\nThe criminals take Beckert to an abandoned distillery to face a kangaroo court. He finds a large, silent crowd awaiting him. Beckert is given a \"lawyer\" who gamely argues in his defense but fails to win any sympathy from the improvised jury. Beckert delivers an impassioned monologue, saying that he cannot control his homicidal urges, while the other criminals present break the law by choice. He questions why they believe they have any right to judge him:\nWhat right have you to speak? Criminals! Perhaps you are even proud of yourselves! Proud of being able to crack into safes, or climb into buildings or cheat at cards. All of which, it seems to me, you could just as easily give up, if you had learned something useful, or if you had jobs, or if you were not such lazy pigs. I can not help myself! I have no control over this evil thing that is inside me\u2014the fire, the voices, the torment!\nBeckert pleads to be handed over to the police. His \"lawyer\" points out that , presiding over the proceedings, is wanted on three counts of manslaughter, and that it is unjust to execute an insane man. Just as the mob is about to kill Beckert, the police arrive to arrest both him and the criminals.\nAs a panel of judges prepares to deliver their verdict at Beckert's trial, the mothers of three of his victims weep in the gallery. Frau Beckmann says that \"no sentence will bring the dead children back\" and \"one has to keep closer watch over the children\". The screen fades to black as she adds, \"All of you\".\nCast.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nProduction.\nLang placed an advertisement in a newspaper in 1930 stating that his next film would be (\"Murderer Among Us\") and that it was about a child murderer. He immediately began receiving threatening letters in the mail and was also denied a studio space to shoot the film at the Staaken Studios. When Lang confronted the head of Staaken Studio to find out why he was being denied access, the studio head informed Lang that he was a member of the Nazi party and that the party suspected that the film was meant to depict the Nazis. This assumption was based entirely on the film's original title and the Nazi party relented when told the plot.\n\"M\" was eventually shot in six weeks at a \"Staaken Zeppelinhalle\" studio, just outside Berlin. Lang made the film for Nero-Film, rather than with UFA or his own production company. It was produced by Nero studio head Seymour Nebenzal who later produced Lang's \"The Testament of Dr. Mabuse\". Working titles for the film included (\"A City Searches for a Murderer\") and (\"Your Murderer Looks at You\").\nWhile researching for the film, Lang spent eight days inside a mental institution in Germany and met several child murderers, including Peter K\u00fcrten. He used several real criminals as extras in the film and eventually 25 cast members were arrested during the film's shooting. Peter Lorre was cast in the lead role of Hans Beckert, acting for the film during the day and appearing on stage in Valentine Katayev's \"Squaring the Circle\" at night.\nLang did not show any acts of violence or deaths of children on screen and later said that by only suggesting violence, he forced \"each individual member of the audience to create the gruesome details of the murder according to their personal imagination\".\n\"M\" has been said, by various critics and reviewers, to be based on serial killer Peter K\u00fcrten\u2014the \"Vampire of D\u00fcsseldorf\"\u2014whose crimes took place in the 1920s. Lang denied that he drew from this case in an interview in 1963 with film historian Gero Gandert: \"At the time I decided to use the subject matter of \"M\", there were many serial killers terrorizing Germany\u2014Haarmann, Grossmann, K\u00fcrten, Denke, [...]\". Inspector Karl Lohmann is based on Ernst Gennat, then director of the Berlin criminal police.\nLang's depiction of the Berlin underworld in the film was inspired by the real \"Ringvereine\". The film's portrayal of the \"Ringvereine\" as organized with a board of directors that were dominated by a charismatic master criminal was based on reality. Likewise, the practice of the \"Ringvereine\" shown in the film of providing financial support for the families of imprisoned members was also based on reality. The break-in of an office building depicted in the film was inspired by the real life 1929 break-in of the Disconto Bank in Berlin by the Sass brothers gang, though unlike in the film the objective was larceny, not to capture a serial killer.\nThe \"Ringvereine\", which were officially wrestling associations that existed for the physical betterment of German men, always sought to promote a very 'respectable', almost middle-class image of themselves. Like the Mafia, the \"Ringvereine\" paradoxically portrayed themselves as the guardians of society's values, who upheld a certain social order. The image the \"Ringvereine\" sought to project was as \"professionals\" whose crimes did not harm ordinary people.\nThough the \"Ringvereine\" were known to be gangsters, their hierarchal structure and strict discipline led to a certain popular admiration for them as a force for social order unlike the psychopathic serial killers who murdered random strangers for reasons that often seemed unfathomable, sparking widespread fear and dread. In an article originally published in \"Die Filmwoche\", Lang wrote that the crime scene in Germany was \"such compelling cinematic material that I lived in constant fear that someone else would exploit this idea before me\".\nThe Weimar Republic was marked by intense debates about the morality and efficiency of capital punishment, with the political left arguing that the death penalty was barbaric while the right-wing argued that the death penalty was needed to maintain law and order. Adding to the debate was popular interest in the new science of psychiatry, with many psychiatrists arguing that crime was caused by damaged minds and emotions which could be cured. In the background was a popular obsessive fear of crime and social breakdown, which was fed by sensationalist newspaper coverage of crime.\nIn addition, for many conservative Germans, the Weimar republic was itself born of crime, namely the November Revolution of 1918 which began with the High Seas Fleet mutiny. According to this viewpoint, its origins in mutiny and revolution made the Weimar Republic an illegitimate state that could not maintain social order. Lang followed these debates closely and incorporated them into several of his Weimar-era films. The debate at Beckert's \"trial\" about whether he deserved to be killed or not paralleled the contemporary debates about capital punishment in Germany.\nThe fact that \"Der Schr\u00e4nker\", a career criminal, serves as both the prosecutor and judge at the kangaroo court, egging on the mob of criminals to kill Beckert, seems to suggest that Lang's sympathy was with the abolitionists. The arguments that \"Der Schr\u00e4nker\" makes at the kangaroo court, namely that certain people are so evil that they deserved to be killed for the good of society, was precisely the same argument made by supporters of the death penalty.\nThe incorporation of social issues in the film can be seen through the lens of Jeffrey Jerome Cohen's \"Monster Culture (Seven Theses)\". The first of these theses states that \u201cThe monster is born only at this metaphoric crossroads, as an embodiment of a certain cultural moment\u2014of a time, a feeling, and a place.\u201d Beckert, as the \"monster\" in this film, embodies the cultural moment, reflecting Weimar society's interest in morality and criminality.\nUse of sound.\n\"M\" was Lang's first sound film, and he experimented with the new technology. It has a dense and complex soundtrack, as opposed to the more theatrical \"talkies\" being released at the time. The soundtrack includes sounds occurring off-camera, sounds motivating action and suspenseful moments of silence before sudden noise. Lang was also able to make fewer cuts in the film's editing, since sound effects could now be used to inform the narrative.\nThe film was one of the first to use a \"leitmotif\", a technique borrowed from opera. It associates a melody with Lorre's character, who whistles \"In the Hall of the Mountain King\" from Edvard Grieg's \"Peer Gynt\". Later in the film, the mere sound of the song lets the audience know that he is nearby. This association of a musical theme with a particular character or situation is now a film staple. As Lorre could not whistle, Lang dubbed Beckert's whistling.\nRelease.\n\"M\" premiered on 11 May 1931 at the Ufa-Palast am Zoo in Berlin, in a version lasting 117 minutes. The original negative is preserved at the Federal Film Archive in a 96-minute version. In 1960, an edited 98-minute version was released. The film was restored in 2000 by the Netherlands Film Museum in collaboration with the Federal Film Archive, the Cinemateque Suisse, Kirsch Media and ZDF/ARTE., with Janus Films releasing the 109-minute version as part of its Criterion Collection using prints from the Cinemateque Suisse and the Netherlands Film Museum. A complete print of the English version and selected scenes from the French version were included in the 2010 Criterion Collection release of the film.\nThe film was released in the United States in April 1933 by Foremco Pictures. After playing in German with English subtitles for two weeks, it was pulled from theaters and replaced by an English-language version. The re-dubbing was directed by Eric Hakim, and Lorre was one of the few cast members to reprise his role in the film.\nAs with many other early talkies from the years 1930\u20131931, \"M\" was partially reshot with actors (including Lorre) performing dialogue in other languages for foreign markets after the German original was completed, apparently without Lang's involvement. An English-language version was filmed and released in 1932 from an edited script with Lorre speaking his own words, his first English part. An edited French version was also released but despite the fact that Lorre spoke French his speaking parts were dubbed.\nIn 2013, a DCP version was released by Kino Lorber and played theatrically in North America in the original aspect ratio of 1.19:1. Critic Kenneth Turan of the \"Los Angeles Times\" called this the \"most-complete-ever version\" at 111 minutes. The film was restored by TLEFilms Film Restoration &amp; Preservation Services (Berlin) in association with Archives fran\u00e7aises du film \u2013 CNC (Paris) and PostFactory GmbH (Berlin).\nCritical reception.\nInitial response.\nA \"Variety\" review said that the film was \"a little too long. Without spoiling the effect\u2014even bettering it\u2014cutting could be done. There are a few repetitions and a few slow scenes.\" Graham Greene compared the film to \"looking through the eye-piece of a microscope, through which the tangled mind is exposed, laid flat on the slide: love and lust; nobility and perversity, hatred of itself and despair jumping at you from the jelly\".\nReassessment.\nIn later years, the film received widespread critical praise and holds an approval rating of 100% on Rotten Tomatoes based on 65 reviews, with an average rating of 9.30/10. The site's critics consensus reads: \"A landmark psychological thriller with arresting images, deep thoughts on modern society, and Peter Lorre in his finest performance.\"\nMarc Savlov of \"Austin Chronicle\" awarded the film five out of five stars, calling it, \"One of the greatest of all German Expressionistic films\". Savlov praised the film's cinematography, the use of sound and Lorre's performance. In 1997, critic Roger Ebert added \"M\" to his \"Great Movies\" list. He proposed Lang's limited use of dialogue was a critical factor in the film's success, in contrast with many early sound films which \"felt they had to talk all the time\". Ebert also argued the film's characters, nearly all grotesques, embodied Lang's distaste for his adopted homeland: \"What I sense is that Lang hated the people around him, hated Nazism, and hated Germany for permitting it.\"\nIn 2015, Taste of Cinema ranked the film 18th among the \"30 Great Psychopath Movies That Are Worth Your Time\", and in 2024 \"Paste\" ranked the character 1st among \"the best portrayals of cinematic sociopaths\".\nLegacy.\nLang considered \"M\" to be his favorite of his own films because of the social criticism in the film. In 1937, he told a reporter that he made the film \"to warn mothers about neglecting children\". The film has appeared on multiple lists as one of the greatest films ever made. It was voted the best German film of all time with 306 votes in a 1994 poll of 324 film journalists, film critics, filmmakers, and cineastes organized by the Association of German Cin\u00e9math\u00e8ques. It is included in \"Empire\"'s 100 Best Films of World Cinema in 2010.\nIt is listed in the film reference book \"1001 Movies You Must See Before You Die\", which says: \"Establishing conventions still being used by serial killer movies, Lang and scenarist Thea von Harbou intercut the pathetic life of the murderer with the frenzy of the police investigation into the outrageous crimes, and pay attention to issues of press coverage of the killings, vigilante action, and the political pressure that comes down from the politicians and hinders as much as encourages the police.\"\nIn 2018, it was voted the thirteenth greatest foreign-language film of all time in BBC's poll of 209 critics in 43 countries. The film is also referenced in the song \"In Germany Before the War\" by American songwriter Randy Newman in his 1977 album \"Little Criminals\".\nA scene from the movie was used in the 1940 Nazi propaganda movie \"The Eternal Jew\".\nRemakes and adaptations.\nA Hollywood remake of the same title was released in 1951, shifting the action from Berlin to Los Angeles. Nero Films head Seymour Nebenzal and his son Harold produced the film for Columbia Pictures. Lang had once told a reporter: \"People ask me why I do not remake \"M\" in English. I have no reason to do that. I said all I had to say about that subject in the picture. Now, I have other things to say.\" The remake was directed by Joseph Losey and starred David Wayne in Lorre's role. Losey stated that he had seen \"M\" in the early 1930s and watched it again shortly before shooting the remake, but that he \"never referred to it. I only consciously repeated one shot. There may have been unconscious repetitions in terms of the atmosphere, of certain sequences.\" Lang later said that when the remake was released, he \"had the best reviews of [his] life\".\nArgentine noir classic \"The Black Vampire\", released in 1953 and directed by Rom\u00e1n Vi\u00f1oly Barreto, is based on Lang's original script.\nIn 2003, \"M\" was adapted for radio by Peter Straughan and broadcast on BBC Radio 3 on 2 February, later re-broadcast on BBC Radio 4 Extra on 8 October 2016. Directed by Toby Swift, this drama won the Prix Italia for Adapted Drama in 2004.\nWriter Jon J. Muth adapted the screenplay into a four-part comic book series in 1990, which was reissued as a graphic novel in 2008.\nIn 2015, Joseph D. Kucan adapted the screenplay into a theatrical stageplay entitled \"A Summons from the Tinker to Assemble the Membership in Secret at the Usual Place\" for production by the Las Vegas-based theatre company A Public Fit. The play is environmental in nature, transforming its audience into the members of the criminal underground who have captured - and will judge - the elusive serial child murderer. The play is primarily a courtroom drama, presented with no fourth wall, and uses flashback sequences to tell the story of the man's detection, capture and confession. A brief segment of the play is dedicated to improvised audience debate and deliberation.\nIn 2019, a six-episode Austrian-German miniseries adaptation of the film was released, entitled \"M \u2014 A City Hunts a Murderer\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66323", "revid": "1315456292", "url": "https://en.wikipedia.org/wiki?curid=66323", "title": "Crime Traveller", "text": "Crime Traveller is a 1997 British television science fiction detective series produced by Carnival Films for the BBC. It was based on the premise of using time travel for the purpose of solving crimes.\nAnthony Horowitz created the series and wrote every episode. He had the idea while writing an episode of \"Poirot\". Despite having over eight million viewers on a regular basis, \"Crime Traveller\" was not renewed after its first series. According to Horowitz, \"The show wasn't exactly cut. There was a chasm at the BBC, created by the arrival of a new Head of Drama and our run ended at that time. There was no-one around to commission a new series...and so it just didn't happen.\"\nPlot.\nJeff Slade is a detective with the Criminal Investigation Department of the local police force led by Kate Grisham; although unusually for such a position he is an armed officer, carrying a handgun as routine. Slade is a good detective who gets results although his approach is somewhat maverick and his methods do leave a lot to be desired and have more than once landed him in trouble. Amongst Slade's colleagues at the department is science officer Holly Turner who has a secret that Slade manages to uncover. Holly owns a working time machine that was built by her late father. The machine is able to take Slade and Holly back far enough in time to witness a crime as it happens and discover who committed it. As a result, Slade's track record with crime solving goes through the roof with case after case being solved in record time.\nHome media.\nRevelation Films released the entire series on DVD on 21 June 2004. The DVD release includes an exclusive interview with writer and creator Anthony Horowitz.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66326", "revid": "1398", "url": "https://en.wikipedia.org/wiki?curid=66326", "title": "Bra\u0219ov", "text": "City in Bra\u0219ov County, Romania\nBra\u0219ov (, , ; , also \"Brasau\"; ; ; Transylvanian Saxon: \"Kruhnen\") is a city in Transylvania, Romania and the county seat (i.e. administrative centre) of Bra\u0219ov County.\nAccording to the 2021 census, with 237,589 inhabitants, Bra\u0219ov is the 6th most populous city in Romania. The metropolitan area was home to 371,802 residents.\nBra\u0219ov is located in the central part of the country, about north of Bucharest and from the Black Sea. It is surrounded by the Southern Carpathians and is part of the historical region of Transylvania.\nHistorically, the city was the centre of the Burzenland (), once dominated by the Transylvanian Saxons (), and a significant commercial hub on the trade roads between Austria (then Archduchy of Austria, within the Habsburg monarchy, and subsequently Austrian Empire) and Turkey (then Ottoman Empire). It is also where the national anthem of Romania was first sung.\nNames.\nBrassovia, Brass\u00f3, Bra\u0219ov, etc..\nAccording to Drago\u0219 Moldovanu, the name of Bra\u0219ov came from the name of local river named B\u00e2rsa (also pronounced as \"B\u0103rsa\") that was adopted by Slavs and transformed to Barsa, and later to Barsov, finally to Brasov. According to P\u00e1l Binder, the current Romanian and the Hungarian name () are derived from the Turkic word \"barasu\", meaning \"white water\" with a Slavic suffix \"-ov\". Other linguists proposed various etymologies including an Old Slavic anthroponym Brasa. The first attested mention of this name is \"Terra Saxonum de Barasu\" (\"Saxon Land of Baras\") in a 1252 document issued by B\u00e9la IV of Hungary. According to some historians, \"Corona\" was name of the city-fortress while \"Brass\u00f3\" was referring to the county, while others consider both names may refer to the city and the county as well.\nCorona, Kronstadt.\nAccording to Bal\u00e1zs Orb\u00e1n, the name \"Corona\"\u00a0\u2013 a Latin word meaning \"crown\"\u00a0\u2013 is first mentioned in the Catalogus Ninivensis in 1235 AD, stating a monastic quarter existed in the territory of the Roman Catholic Diocese of Cumania (\"In Hungaria assignata est paternitas Dyocesis Cumanie: Corona\"). P\u00e1l Binder supposed it is a reference to the St. Catherine's Monastery. Others suggest the name derives from the old coat of arms of the city, as it is symbolized by the German name \"Kronstadt\" meaning \"Crown City\". The two names of the city, \"Kronstadt\" and \"Corona\", were used simultaneously in the Middle Ages, along with the Medieval Latin \"Brassovia\".\nStephanopolis, Ora\u0219ul Stalin.\nAnother historical name used for Bra\u0219ov is \"Stephanopolis\", from \"Stephanos\", crown, and \"polis\", city.\nOn 22 August 1950, Bra\u0219ov was renamed \"Ora\u0219ul Stalin\" (\"lit.\" Stalin City) after Joseph Stalin. Constantin Ion Parhon, the nominal Head of State at the time, decreed the city be renamed \"in honour of the great genius of working humanity, the leader of the Soviet people, the liberator and beloved friend of our people, Joseph Vissarionovich Stalin\". The city's name reverted to Bra\u0219ov in 1960.\nHistory.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nHistorical affiliations\n Kingdom of Hungary 1235\u20131526 Eastern Hungarian Kingdom 1526\u20131570 Principality of Transylvania 1570\u20131711 Grand Principality of Transylvania 1711\u20131804 Austrian Empire 1804\u20131867 Austria-Hungary 1867\u20131918 Kingdom of Romania 1920\u20131947 Romanian People's Republic 1947\u20131965 Socialist Republic of Romania 1965\u20131989 Romania 1989\u2013present\nThe oldest traces of human activity and settlements in Bra\u0219ov date back to the Neolithic age (about 9500 BCE). Archaeologists working from the last half of the 19th century discovered continuous traces of human settlements in areas situated in Bra\u0219ov: Valea Cet\u0103\u021bii, Pietrele lui Solomon, \u0218prenghi, T\u00e2mpa, Dealul Melcilor, and Noua. The first three locations show traces of Dacian citadels; \u0218prenghi Hill housed a Roman-style construction. The last two locations had their names applied to Bronze Age cultures\u2014Schneckenberg (\"Hill of the Snails\"; Early Bronze Age) and \"Noua\" (\"The New\"; Late Bronze Age).\nTransylvanian Saxons played a decisive role in Bra\u0219ov's development and were invited by Hungarian kings to develop towns, build mines, and cultivate the land of Transylvania at different stages between 1141 and 1300. The settlers came primarily from the Rhineland, Flanders, and the Moselle region, with others from Thuringia, Bavaria, Wallonia, and even France.\nIn 1211, by order of King Andrew II of Hungary, the Teutonic Knights fortified the Burzenland to defend the border of the Kingdom of Hungary. On the site of the village of Bra\u0219ov, the Teutonic Knights built Kronstadt \u2013 'the City of the Crown'. Although the crusaders were evicted by 1225, the colonists they brought in long ago remained, along with local population in three distinct settlements they founded on the site of Bra\u0219ov:\nGermans living in Bra\u0219ov were mainly involved in trade and crafts. The location of the city at the intersection of trade routes linking the Ottoman Empire and Western Europe, together with certain tax exemptions, allowed Saxon merchants to obtain considerable wealth and exert a strong political influence. They contributed a great deal to the architectural flavour of the city. Fortifications around the city were erected and continually expanded, with several towers maintained by different craftsmen's guilds, according to the medieval custom. Part of the fortification ensemble was recently restored using UNESCO funds, and other projects are ongoing. At least two entrances to the city, \"Poarta Ecaterinei\" (or \"Katharinentor\") and \"Poarta \u0218chei\" (or \"Waisenhausg\u00e4ssertor\"), are still in existence. The city centre is marked by the mayor's former office building () and the surrounding square (\"pia\u021ba\"), which includes one of the oldest buildings in Bra\u0219ov, the Hirscher Haus. Nearby is the \"Black Church\" (\"Biserica Neagr\u0103\"), which some claim to be the largest Gothic style church in Southeastern Europe.\nIn 1689, a great fire destroyed the walled city almost entirely, and its rebuilding lasted several decades.\nBesides the German (Saxon) population living in the walled city and in the northern suburbs, Bra\u0219ov had also a significant Romanian and Bulgarian population (living in the \u0218chei district), and also some Hungarian population (living in the Blum\u0103na district). The cultural and religious importance of the Romanian church and school in \u0218chei is underlined by the generous donations received from more than thirty hospodars of Moldavia and Wallachia, as well as that from Elizabeth of Russia. In the 17th and 19th centuries, the Romanians in \u0218chei campaigned for national, political, and cultural rights, and were supported in their efforts by Romanians from all other provinces, as well as by the local Greek merchant community. In 1838, they established the first Romanian language newspaper \"Gazeta Transilvaniei\" and the first Romanian institutions of higher education: \"\u0218colile Centrale Greco-Ortodoxe\" (\"The Greek-Orthodox Central Schools\", today named after Andrei \u0218aguna). The Holy Roman Emperor and sovereign of Transylvania Joseph II awarded Romanians citizenship rights for a brief period during the latter decades of the 18th century.\nIn 1850, the town had 21,782 inhabitants: 8,874 (40.7%) Germans, 8,727 (40%) Romanians, 2,939 (13.4%) Hungarians. In 1910 there were 41,056 inhabitants: 17,831 (43.4%) Hungarians, 11,786 (28.7%) Romanians, 10,841 (26.4%) Germans.\nOn 29 August 1916, during the First World War, the Romanian Army occupied Bra\u0219ov. Romanian troops entered the city at around five o'clock p.m. and paraded towards the city square. Romanian rule over the city lasted until early October, when the area was retaken by the Central Powers in the Battle of Brass\u00f3 (7\u20139 October 1916). The Romanian mayor installed during the brief Romanian occupation was Gheorghe Baiulescu. His term lasted from 29 August, when the city was occupied by the Romanian Army, until 8 October \u2013 the height of the Battle of Bra\u0219ov. On 9 October, at the end of the battle, the previous mayor Karl Ernst Schnell was reinstated.\nFollowing the collapse of Austria-Hungary, the 1 December 1918 Proclamation of the Union of Alba Iulia, adopted by deputies of the Romanians from Transylvania, Banat, Cri\u0219ana and Maramure\u0219 during the Great National Assembly of Alba Iulia declared the union of Transylvania into the Romanian state. Bra\u0219ov was permanently occupied by Romanian forces on 7 December, as Hungarians gradually withdrew northwards. The King and some Transylvanians suggested that, because of Bra\u0219ov's central geographical location in the new Romania, it should be considered as the new national capital. Though this did not happen, the inter-war period was a time of flourishing economy and cultural life in general, including the Saxons in Bra\u0219ov. However, at the end of World War II many ethnic Germans were forcibly deported to the Soviet Union. A majority of them emigrated to West Germany after Romania had become a communist country.\nThe first Jewish community in Bra\u0219ov was established in 1828, joining the Neolog association in 1868. Orthodox Jews founded their religious organization in 1877. The Neolog synagogue, seating 800, was built between 1899 and 1905. During the interwar period, the communities had separate institutions, but opened a jointly managed school in 1940. Zionist organizations appeared already in 1920. By 1930, Jews numbered 2594 individuals, or 4% of the total population. In autumn 1940, during the National Legionary State, the antisemitic Iron Guard nationalized all Jewish institutions and seized most shops owned by Jews. In 1941, Jews were drafted for service in forced labour battalions. Those from throughout southern Transylvania were concentrated in Bra\u0219ov; a further 200 refugees came from Ploie\u0219ti. In August 1942, 850 Jews between the ages of 18 and 50 were drafted into labour battalions and ordered to work in Bra\u0219ov, while others were sent to Predeal and Bran. In spring 1943, 250 youths were sent to Suraia camp to build fortifications. By August 1944, the labour battalions were reduced to 250\u2013300 while most of the Jews managed to obtain their freedom. In 1945\u20131946, the Jewish population increased to 3500.\nLike many other cities in Transylvania, Bra\u0219ov is also home to a significant ethnic Hungarian minority.\nDuring the communist period, industrial development was vastly accelerated. Under Nicolae Ceau\u0219escu's rule, the city was the site of the 1987 Bra\u0219ov strike. This was brutally repressed by the authorities and resulted in numerous workers being imprisoned.\nEconomy.\nIndustrial development in Bra\u0219ov started in the inter-war period, with one of the largest factories being the aeroplane manufacturing plant (IAR Bra\u0219ov), which produced the first Romanian fighter planes used during World War II. After signing the armistice with USSR on September 12, 1944, the factory started repairing trucks, and in October 1945 it began manufacturing agricultural tractors. IAR 22 was the first Romanian-made wheeled tractor. In 1948 the company was renamed Uzina Tractorul Bra\u0219ov known internationally as Universal Tractor Bra\u0219ov.\nA big part of the factory was demolished during 2013 and 2014 giving way to buildings, shopping mall and recreation parks. Aircraft manufacturing resumed in 1968 at first under the name ICA and then under its old name of IAR at a new location in nearby Ghimbav.\nIndustrialization was accelerated in the Communist era, with special emphasis being placed on heavy industry, attracting many workers from other parts of the country. Heavy industry is still abundant, including Roman, which manufactures MAN AG trucks as well as native-designed trucks and coaches. Although the industrial base has been in decline in recent years, Bra\u0219ov is still a site for manufacturing hydraulic transmissions, auto parts, ball-bearings, construction materials, hand tools, furniture, textiles and shoe-wear. There is also a large brewery. \nGeography.\nBra\u015fov is situated in central Romania, northwest of Ploie\u0219ti. It is at the foot of the Transylvanian Alps.\nClimate.\nBra\u0219ov has a humid continental climate (K\u00f6ppen climate classification: \"Dfb\").\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nBra\u0219ov has a total population of 237,589 (2021 census).\nIts ethnic composition includes (as of 2011):\nIn 2005, the Bra\u0219ov metropolitan area was created. With its surrounding localities, Bra\u0219ov had 371,802 inhabitants as of 2021[ [update]].\nAdministration.\nBra\u0219ov is administered by a mayor and a local council. The current mayor of Bra\u0219ov (starting October 21, 2024) is George Scripcaru from the National Liberal Party (PNL).\nThe Bra\u0219ov Local Council, elected at the 2024 Romanian local elections, is made up of 27 counselors, with the following party composition:\nTransportation.\nThe Bra\u0219ov local transport network has 44 urban bus and trolleybus lines and 19 metropolitan bus lines. There are also regular bus lines serving Poiana Bra\u0219ov, a nearby winter resort and part of the city of Bra\u0219ov. All are operated by RAT Bra\u0219ov. Because of its central location, the Bra\u0219ov railway station is one of the busiest stations in Romania with trains to/from most destinations in the country served by rail.\nThe Bra\u0219ov-Ghimbav International Airport is an international airport located in nearby Ghimbav, right by the future A3 motorway. It is the first airport to be developed in post-communist Romania, and the 17th commercial airport in the country. The contract for the construction of the main terminal building, with a total area of 11,780 m2 (126,799 sq ft), was awarded to the Romanian contractor Bog'Art Bucharest and was signed on 21 August 2019. Construction works for the passenger terminal started on 17 March 2020 and the first commercial flight took place on June 15, 2023.\nCFR announced a feasibility study for the construction of a rail line (8\u00a0km) which would connect the airport to the Bra\u0219ov railway station.\nTourism.\nWith its central location, Bra\u0219ov is a suitable location from which to explore Romania, and the distances to several tourist destinations (including the Black Sea resorts, the monasteries in northern Moldavia, and the wooden churches of Maramure\u0219) are similar. It is also the largest city in a mountain resorts area. The old city is very well preserved and is best seen by taking the cable-car to the top of T\u00e2mpa Mountain.\nTemperatures from May to September fluctuate around . Bra\u0219ov benefits from a winter tourism season centred on winter sports and other activities. Poiana Bra\u0219ov is the most popular Romanian ski resort and an important tourist centre preferred by many tourists from other European states.\nThe city ranks on the second place in terms of tourism arrivals countrywide, after the capital Bucharest.\nSport.\nThe city has a long tradition in sports, the first sport associations being established at the end of the 19th century (Target shooting Association, Gymnastics School). The Transylvanian Sports Museum is among the oldest in the country and presents the evolution of consecrated sports in the city. During the communist period, universiades and Daciads were held, where local sportsmen were obliged to participate. Nowadays, the infrastructure of the city allows other sports to be practiced, such as football, rugby, tennis, cycling, handball, gliding, skiing, skating, mountain climbing, paintball, bowling, swimming, target shooting, basketball, martial arts, equestrian, volleyball or gymnastics. Annually, at \"Olimpia\" sports ground, the \"Bra\u0219ov Challenge Cup\" tennis competition is held.\nCol\u021bea Bra\u0219ov was the football champion in 1928, managing a second place in 1927, in its only 10 years of existence (1921\u20131931). It was succeeded by Bra\u0219ovia Bra\u0219ov.\nBetween 17 and 22 February 2013, the city hosted the 2013 European Youth Winter Olympic Festival.\nAs of 2012[ [update]], Bra\u0219ov is hosting two trail semi-marathons: the Semimaraton Intersport Bra\u0219ov (held in April) and the Bra\u0219ov International Marathon (held in April or May).\nIn November 2013, Bra\u0219ov submitted their bid for the 2020 Winter Youth Olympics. They were up against Lausanne, Switzerland to be awarded the event. In December that year, the city was signed the Youth Olympic Game Candidature Procedure. The host city was to be announced in July 2015, in which Lausanne was selected.\nSports venues.\n\u2022 Under construction\n\u2022 Sala Polivalent\u0103 (10,059 capacity) \u2013 under construction multi-purpose 10,059-seat indoor arena\nMedia.\nThe city of Bra\u0219ov is home to several local media publications such as Transilvania Express, Monitorul Express, Bun\u0103 Ziua Bra\u0219ov or Bra\u0219ovul T\u0103u. Also, several local television stations exist, such as RTT, MIX TV and Nova TV.\nTwin towns \u2013 sister cities.\nBra\u0219ov is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66327", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=66327", "title": "Serbocroatian language", "text": ""}
{"id": "66328", "revid": "94900", "url": "https://en.wikipedia.org/wiki?curid=66328", "title": "Joseph Michael Straczynski", "text": ""}
{"id": "66331", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=66331", "title": "Battle of Dien Bien Phu", "text": "1954 battle in the First Indochina War\nThe Battle of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 was a decisive defeat of the French Union by the Viet Minh in the First Indochina War. It took place between 13 March and 7 May 1954.\nThe French began an operation to insert, and support, their soldiers at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, deep in the autonomous Tai Federation in northwest Tonkin. The operation's purpose was to cut off enemy supply lines into the neighboring Kingdom of Laos (a French ally) and draw the Viet Minh into a major confrontation in order to cripple them. The French based their forces in an isolated but well-fortified camp that would be resupplied by air, a strategy adopted based on the belief that the Viet Minh had no anti-aircraft capability.\nThe communist Viet Minh, however, under General V\u00f5 Nguy\u00ean Gi\u00e1p, surrounded and besieged the French. They brought in vast amounts of heavy artillery (including anti-aircraft guns) and managed to move these bulky weapons through difficult terrain up the rear slopes of the mountains. They dug tunnels and arranged the guns to target the French positions. The tunnels featured a front terrace, onto which the Viet Minh would pull their cannons from out of the tunnels, fire a few shots, to then pull them back into protective cover.\nIn March, the Viet Minh began a massive artillery bombardment of the French defenses. The strategic positioning of their artillery made it nearly impervious to French counter-battery fire. Tenacious fighting on the ground ensued, reminiscent of the trench warfare of World War I. At times, the French repulsed Viet Minh assaults on their positions while supplies and reinforcements were delivered by air. As key positions were overrun, the perimeter contracted, and the air resupply on which the French had placed their hopes became impossible as aircraft were shot down and runways were destroyed.\nThe garrison was overrun in May after a two-month siege, and most of the French forces surrendered. A few men escaped to Laos. Among the 11,721 French troops captured, 858 of the most seriously wounded were evacuated via the Red Cross mediation in May 1954. Only 3,290 were returned four months later, although it is believed that a small fraction of the outstanding missing troops were Vietnamese who had not yet been returned by the French, and did not necessarily die in captivity; adjusting for this, the death rate of French troops in captivity of the Viet Minh is estimated to be approximately 60%. The French government in Paris resigned. The new prime minister, the left-of-centre Pierre Mend\u00e8s France, supported French withdrawal from Indochina.\nThe Battle of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 was decisive. The war ended shortly afterward and the 1954 Geneva Accords were signed. France agreed to withdraw its forces from all its colonies in French Indochina, while stipulating that Vietnam would be temporarily divided at the 17th parallel, with control of the north given to the Viet Minh as the Democratic Republic of Vietnam under Ho Chi Minh. With huge support by the US, the south became the State of Vietnam, nominally under Emperor B\u1ea3o \u0110\u1ea1i, preventing Ho Chi Minh from gaining control of the entire country.\nBackground.\nMilitary situation.\nBy 1953, the First Indochina War was not going well for France. A succession of commanders\u00a0\u2013 Philippe Leclerc de Hauteclocque, Jean \u00c9tienne Valluy, Roger Blaizot, Marcel Carpentier, Jean de Lattre de Tassigny, and Raoul Salan\u00a0\u2013 had proven incapable of suppressing the insurrection of the Viet Minh, who were fighting for independence. During their 1952\u20131953 campaign, the Viet Minh had overrun vast swathes of Laos, Vietnam's western neighbor, advancing as far as Luang Prabang and the Plain of Jars. The French were unable to slow the advance of the Viet Minh, who fell back only after outrunning their always-tenuous supply lines. In 1953, the French had begun to strengthen their defenses in the Hanoi delta region to prepare for a series of offensives against Viet Minh staging areas in northwest Vietnam. They set up fortified towns and outposts in the area, including Lai Ch\u00e2u near the Chinese border to the north, N\u00e0 S\u1ea3n to the west of Hanoi, and the Plain of Jars in northern Laos.\nIn May 1953, French Premier Ren\u00e9 Mayer named Henri Navarre as Salan's successor to command French Union forces in Indochina. Mayer had given Navarre a single order\u2014to create military conditions that would lead to an \"honorable political solution\". According to military scholar Phillip Davidson:\n On arrival, Navarre was shocked by what he found. There had been no long-range plan since de Lattre's departure. Everything was conducted on a day-to-day, reactive basis. Combat operations were undertaken only in response to enemy moves or threats. There was no comprehensive plan to develop the organization and build up the equipment of the Expeditionary force. Finally, Navarre, the intellectual, the cold and professional soldier, was shocked by the \"school's out\" attitude of Salan and his senior commanders and staff officers. They were going home, not as victors or heroes, but then, not as clear losers either. To them the important thing was that they were getting out of Indochina with their reputations frayed, but intact. They spared little thought or concern for the problems of their successors.\nN\u00e0 S\u1ea3n and the hedgehog concept.\nNavarre began searching for a way to address the Viet Minh threat to Laos. Colonel Louis Berteil, commander of Mobile Group 7 and Navarre's main strategist, formulated the \"h\u00e9risson\" ('hedgehog') concept. The French army would establish a fortified airhead by airlifting soldiers to positions adjacent to key Viet Minh supply lines to Laos. They would cut off Viet Minh soldiers fighting in Laos and force them to withdraw. \"It was an attempt to interdict the enemy's rear area, to stop the flow of supplies and reinforcements, to establish a redoubt in the enemy's rear and disrupt his lines\".\nThe \"h\u00e9risson\" was based on French experiences at the Battle of N\u00e0 S\u1ea3n. In late November and early December 1952, Gi\u00e1p had attacked the French outpost at N\u00e0 S\u1ea3n, which was essentially an \"air-land base\", a fortified camp supplied only by air. There, the French beat back Gi\u00e1p's forces repeatedly, inflicting very heavy losses on them. The French hoped that by repeating the strategy on a much larger scale, they would be able to lure Gi\u00e1p into committing the bulk of his forces to a massed assault. This would enable superior French artillery, armor, and air support to decimate the exposed Viet Minh forces. The success at N\u00e0 S\u1ea3n convinced Navarre of the viability of the fortified airhead concept.\nHowever, French staff officers failed to treat seriously several crucial differences between \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 and N\u00e0 S\u1ea3n:\nFirst, at N\u00e0 S\u1ea3n, the French commanded most of the high ground with overwhelming artillery support. At \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, however, the Viet Minh controlled much of the high ground around the valley, their artillery far exceeded French expectations, and they outnumbered the French troops four to one. Gi\u00e1p compared \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 to a \"rice bowl\", where his troops occupied the edge and the French the bottom. Second, Gi\u00e1p made a mistake at N\u00e0 S\u1ea3n by committing his forces to reckless frontal attacks before being fully prepared. He learned his lesson: at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, Gi\u00e1p spent months meticulously stockpiling ammunition and emplacing heavy artillery and anti-aircraft guns before making his move. He obtained crucial intelligence on French artillery positions from Viet Minh spies posing as camp laborers. Artillery pieces were sited within well-constructed and camouflaged casemates. As a result, when the battle finally began, the Viet Minh knew exactly where the French artillery pieces were, while the French did not even know how many guns Gi\u00e1p possessed. Third, the aerial resupply lines at N\u00e0 S\u1ea3n were never severed, despite Viet Minh anti-aircraft fire. At \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, Gi\u00e1p made it a priority for his gunners to focus on the French runways and aircraft, crippling supply runs and making it impossible for fresh soldiers to be sent in.\nPrelude.\nLead up to Castor.\nIn June 1953, Major General Ren\u00e9 Cogny, the French commander in the Tonkin Delta, proposed \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, which had an old airstrip built by the Japanese during World War II, as a \"mooring point\". In another misunderstanding, Cogny envisioned a lightly defended point from which to launch raids; Navarre, however, believed that he intended to build a heavily fortified base capable of withstanding a siege. Navarre selected \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 for Berteil's \"hedgehog\" operation. When presented with the plan, every major subordinate officer \u2013 Colonel Jean-Louis Nicot (commander of the French Air transport fleet), Cogny, and Generals Jean Gilles and Jean Dechaux (the ground and air commanders for \"Operation Castor\", the initial airborne assault on \u0110i\u1ec7n Bi\u00ean Ph\u1ee7) \u2013 protested. Cogny pointed out, presciently, that \"we are running the risk of a new N\u00e0 S\u1ea3n under worse conditions\". Navarre rejected the criticisms of his proposal and concluded a 17 November conference by declaring that the operation would begin three days later, on 20 November 1953.\nNavarre decided to go ahead with the plan despite serious operational difficulties. These later became painfully obvious, but at the time may have been less apparent. He had been repeatedly assured by his intelligence officers that the operation carried very little risk of involvement by a strong enemy force. Navarre had previously considered three other approaches to defending Laos: mobile warfare, which was impossible given the terrain in Vietnam; a static defense line stretching to Laos, which was not feasible given the number of troops at Navarre's disposal; or placing troops in the Laotian provincial capitals and supplying them by air, which was unworkable due to the distance from Hanoi to Luang Prabang and Vientiane. Navarre believed that this left only the hedgehog option, which he characterized as \"a mediocre solution\". The French National Defense Committee ultimately agreed that Navarre's responsibility did not include defending Laos. However, its decision, which was drawn up on 13 November, was not delivered to him until 4 December, two weeks after the \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 operation began.\nEstablishment of air operations.\nOperations at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 began at 10:35 on 20 November 1953. In \"Operation Castor\", the French dropped or flew 9,000 troops into the area over three days, as well as a bulldozer to prepare the airstrip. They were landed at three drop zones: \"Natasha\" (northwest of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7), \"Octavie\" (to the southwest), and \"Simone\" (to the southeast). The Viet Minh elite 148th Independent Infantry Regiment, headquartered at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, reacted \"instantly and effectively\". Three of its four battalions, however, were absent. Initial operations proceeded well for the French. By the end of November, six parachute battalions had been landed, and the French Army consolidated its positions.\nIt was at this time that Gi\u00e1p began his countermoves. He had expected an attack but had not foreseen when or where it would occur. Gi\u00e1p realized that, if pressed, the French would abandon Lai Ch\u00e2u Province and fight a pitched battle at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7. On 24 November, Gi\u00e1p ordered the 148th Infantry Regiment and the 316th Division to attack Lai Chau, while the 308th, 312th, and 351st divisions assaulted \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 from \"Vi\u1ec7t B\u1eafc\".\nStarting in December, the French, under the command of Colonel Christian de Castries, began transforming their anchoring point into a fortress by setting up seven satellite positions. (Each was said to be named after a former mistress of de Castries, although the allegation is probably unfounded, as the eight names begin with letters from the first nine of the alphabet.) The fortified headquarters was centrally located, with positions \"Huguette\" to the west, \"Claudine\" to the south, and \"Dominique\" to the northeast. The other positions were \"Anne-Marie\" to the northwest, \"Beatrice\" to the northeast, \"Gabrielle\" to the north, and \"Isabelle\" to the south, covering the reserve airstrip.\nThe arrival of the 316th Viet Minh Division prompted Cogny to order the evacuation of the Lai Chau garrison to \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, exactly as Gi\u00e1p had anticipated. En route, they were virtually annihilated by the Viet Minh. \"Of the 2,100 men who left Lai Chau on 9 December, only 185 made it to \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 on 22 December. The rest had been killed, captured, or \"deserted\".\nFrench military forces had committed 10,800 troops, together with yet more reinforcements, totalling nearly 16,000 men, to the defense of a monsoon-affected valley surrounded by heavily-wooded hills and high ground that had not been secured. Artillery as well as ten US M24 Chaffee light tanks (each broken down into 180 individual parts, flown into the base, and then re-assembled) and numerous aircraft (attack and supply types) were committed to the garrison. A number of quadruple 0.50 calibre machine guns were present and used in the ground role. This included France's regular troops (notably elite paratrooper units, plus those of the artillery), French Foreign Legionnaires, Algerian and Moroccan tirailleurs (colonial troops from North Africa) and locally-recruited Indochinese (Laotian, Vietnamese and Cambodian) infantry.\nIn comparison, altogether the Viet Minh had moved up to 50,000 regular troops into the hills surrounding the French-held valley, totalling five divisions, including the 351st Heavy Division, which was an artillery formation equipped with medium artillery, such as the US M101 105mm howitzer, supplied by the neighbouring People's Republic of China (PRC) from captured stocks obtained from defeated Nationalist China as well as US forces in Korea, together with some heavier field-guns as well as anti-aircraft artillery. Various types of artillery and anti-aircraft guns (mainly of Soviet origin), which outnumbered their French counterparts by about four to one, were moved into strategic positions overlooking the valley and the French forces based there. The French garrison came under sporadic direct artillery fire from the Viet Minh for the first time on 31 January 1954 and patrols encountered the Viet Minh troops in all directions around them. The French were completely surrounded.\nGi\u00e1p's change of strategy.\nOriginally, the planned Viet Minh attack was based on the Chinese \"Fast Strike, Fast Victory\" model, which aimed to use all available power to thrust into the command center of the base to secure victory, but this was changed to the \"Steady Fight, Steady Advance\" model of siege tactics.\nThe battle plan designed on the fast strike model was due to open at 17:00 on 25 January and to finish three nights and two days later. Nevertheless this start date was delayed to 26 January, because on 21 January Viet Minh's intelligence indicated that the French had grasped this plan.\nAfter much debate, due to the French knowledge of the battle plan and along with other complications, the assault was canceled on 26 January, and Gi\u00e1p went away and designed a new plan with a new start time. He said that this change of plan was the hardest decision of his military career.\nBattle.\nB\u00e9atrice.\nThe Viet Minh assault began in earnest on 13 March 1954 with an attack on the northeastern outpost, \"B\u00e9atrice\", which was held by the 3rd Battalion, 13th Foreign Legion Demi-Brigade. Viet Minh artillery opened a fierce bombardment with two batteries each of 105mm howitzers, 120mm mortars, and 75mm mountain guns (plus seventeen 57mm recoilless rifles and numerous 60mm and 81/82mm mortars). French command was disrupted at 18:30 when a shell hit the French command post, killing the battalion commander, Major Paul P\u00e9got, and most of his staff. A few minutes later, Lieutenant Colonel Jules Gaucher, commander of the entire central subsector, was also killed by artillery fire. The Viet Minh 312th Division then launched an assault with its 141st and 209th Infantry Regiments, using sappers to breach the French obstacles.\n\"B\u00e9atrice\" comprised three separate strong points forming a triangle with the point facing north. In the southeast, strong point \"Beatrice\"-3, its defenses smashed by 75mm mountain guns firing at point-blank range, was quickly overrun by the 209th Regiment's 130th Battalion. In the north, most of \"Beatrice\"-1 was swiftly conquered by the 141st Regiment's 428th Battalion, but the defenders held out in corner of the position for a time because the attackers thought they had captured the entire strong point when they encountered an internal barbed wire barrier in the dark. In the southwest, the assault on \"Beatrice\"-2 by the 141st Regiment's 11th Battalion did not fare well because its assault trenches were too shallow and portions of them had been flattened by French artillery. Its efforts to breach \"Beatrice\"-2's barbed wire were stalled for hours by flanking fire from \"Beatrice\"-1 and several previously-undetected bunkers on \"Beatrice\"-2 that had been spared by the bombardment. The holdouts on \"Beatrice\"-1 were eliminated by 22:30, and the 141st Regiment's 11th and 16th Battalions finally broke into \"Beatrice\"-2 an hour later, though the strong point was not entirely taken until after 01:00 on 14 March. Roughly 350 French legionnaires were killed, wounded, or captured. About 100 managed to escape and rejoin the French lines. The French estimated that Viet Minh losses totaled 600 dead and 1,200 wounded. According to the Viet Minh, they lost 193 killed and 137 wounded The victory at \"Beatrice\" \"galvanized the morale\" of the Viet Minh troops.\nMuch to French disbelief, the Viet Minh had employed direct artillery fire, in which each gun crew does its own artillery spotting (as opposed to indirect fire, in which guns are massed further away from the target, out of direct line of sight, and rely on a forward artillery spotter). Indirect artillery, generally held as being far superior to direct fire, requires experienced, well-trained crews and good communications, which the Viet Minh lacked. Navarre wrote that, \"Under the influence of Chinese advisers, the Viet Minh commanders had used processes quite different from the classic methods. The artillery had been dug in by single pieces...They were installed in shellproof dugouts, and fire point-blank from portholes... This way of using artillery and AA guns was possible only with the expansive ant holes at the disposal of the Viet Minh and was to make shambles of all the estimates of our own artillerymen.\" Two days later, the French artillery commander, Colonel Charles Piroth, distraught at his inability to silence the well-camouflaged Viet Minh batteries, went into his dugout and committed suicide with a grenade. He was buried there in secret to prevent loss of morale among the French troops.\n\"Gabrielle\".\nFollowing a five-hour ceasefire on the morning of 14 March, Viet Minh artillery resumed pounding French positions. The airstrip, already closed since 16:00 the day before due to a light bombardment, was now put permanently out of commission. Any further French supplies would have to be delivered by parachute. That night, the Viet Minh launched an attack on the northern outpost \"Gabrielle\", held by an elite Algerian battalion. The attack began with a concentrated artillery barrage at 17:00. This was very effective and stunned the defenders. Two regiments from the crack 308th Division attacked starting at 20:00. At 04:00 the following morning, an artillery shell hit the battalion headquarters, severely wounding the battalion commander and most of his staff.\nDe Castries ordered a counterattack to relieve \"Gabrielle\". However, Colonel Pierre Langlais, in forming the counterattack, chose to rely on the 5th Vietnamese Parachute Battalion, which had jumped in the day before and was exhausted. Although some elements of the counterattack reached \"Gabrielle\", most were paralyzed by Viet Minh artillery and took heavy losses. At 08:00 the next day, the Algerian battalion fell back, abandoning \"Gabrielle\" to the Viet Minh. The French lost around 1,000 men defending \"Gabrielle\", and the Viet Minh between 1,000 and 2,000 attacking the strongpoint.\n\"Anne-Marie\".\nThe northwestern outpost \"Anne-Marie\" was defended by Tai troops, members of an ethnic minority loyal to the French. For weeks, Gi\u00e1p had distributed subversive propaganda leaflets, telling the Tais that this was not their fight. The fall of \"Beatrice\" and \"Gabrielle\" had demoralized them. On the morning of 17 March, under the cover of fog, the bulk of the Tais left or defected. The French and the few remaining Tais on \"Anne-Marie\" were then forced to withdraw.\nLull.\nA lull in fighting occurred from March 17 to March 30. The Viet Minh further encircled the French central area (formed by the strong points \"Huguette\", \"Dominique\", \"Claudine\", and \"Eliane\"), effectively cutting off \"Isabelle\" and its 1,809 personnel to the south. During this lull, the French suffered from a serious crisis of command. Senior officers with the garrison and Cogny in Hanoi began to feel that de Castries was incompetent in defending Dien Bien Phu. After the loss of the northern outposts, he isolated himself in his bunker, \"de facto\" shirking his command of the situation. On 17 March, Cogny attempted to fly into \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 to take command, but his plane was driven off by anti-aircraft fire. He considered parachuting into the encircled garrison, but his staff talked him out of it.\nDe Castries' seclusion in his bunker, combined with his superiors' inability to replace him, created a leadership vacuum in the French command. On 24 March, an event took place which later became a matter of historical debate. The historian Bernard Fall records, based on Langlais' memoirs, that Langlais and his fellow paratroop commanders, all fully armed, confronted de Castries in his bunker on 24 March. They told him he would retain the appearance of command, but that Langlais would exercise it. De Castries is said by Fall to have accepted the arrangement without protest, although he did exercise some command functions thereafter. Phillip Davidson stated that the \"truth would seem to be that Langlais did take over effective command of Dien Bien Phu, and that Castries became 'commander emeritus' who transmitted messages to Hanoi and offered advice about matters in Dien Bien Phu\". Jules Roy, however, makes no mention of this event, and Martin Windrow argues that the \"paratrooper putsch\" is unlikely to have ever happened. Both historians record that Langlais and Marcel Bigeard were known to be on good terms with their commanding officer.\nFrench aerial resupply took heavy losses from Viet Minh machine guns near the landing strip. On 27 March, the Hanoi air transport commander, Nicot, ordered that all supply deliveries be made from or higher; losses were expected to remain heavy. The following day, De Castries ordered an attack against the Viet Minh AA machine guns west of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7. Remarkably, the attack was a complete success, with 350 Viet Minh's casualties and seventeen 12.7mm AA machine guns destroyed (French estimate), while the French lost 20 killed and 97 wounded.\n30 March \u2013 5 April assaults.\nThe next phase of the battle saw more massed Viet Minh assaults against French positions in central \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 \u2013 particularly at \"Eliane\" and \"Dominique\", the two remaining outposts east of the Nam Yum River. Those two areas were held by five understrength battalions, composed of Frenchmen, Legionnaires, Vietnamese, North Africans, and Tais. Gi\u00e1p planned to use the tactics from the \"Beatrice\" and \"Gabrielle\" skirmishes.\nAt 19:00 on 30 March, the Viet Minh 312th Division captured \"Dominique\" 1 and 2, making \"Dominique\" 3 the final outpost between the Viet Minh and the French general headquarters, as well as outflanking all positions east of the river. At this point, the French 4th Colonial Artillery Regiment entered the fight, setting its 105\u00a0mm howitzers to zero elevation and firing directly on the Viet Minh attackers, blasting huge holes in their ranks. Another group of French soldiers, near the airfield, opened fire on the Viet Minh with anti-aircraft machine guns, forcing the Viet Minh to retreat.\nThe Viet Minh's simultaneous attacks elsewhere were more successful. The 316th Division captured \"Eliane\" 1 from its Moroccan defenders, and half of \"Eliane\" 2 by midnight. On the west side of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, the 308th attacked \"Huguette\" 7, and nearly succeeded in breaking through, but a French sergeant took charge of the defenders and sealed the breach.\nJust after midnight on 31 March, the French launched a counterattack against \"Eliane\" 2, and recaptured it. Langlais ordered another counterattack the following afternoon against \"Dominique\" 2 and \"Eliane\" 1, using virtually \"everybody left in the garrison who could be trusted to fight\". The counterattacks allowed the French to retake \"Dominique\" 2 and \"Eliane\" 1, but the Viet Minh launched their own renewed assault. The French, who were exhausted and without reserves, fell back from both positions late in the afternoon. Reinforcements were sent north from \"Isabelle\", but were attacked en route and fell back to \"Isabelle\".\nShortly after dark on 31 March, Langlais told Bigeard, who was leading the defense at \"Eliane\" 2, to fall back from \"Eliane\" 4. Bigeard refused, saying \"As long as I have one man alive I won't let go of \"Eliane\" 4. Otherwise, Dien Bien Phu is done for.\" The night of 31 March, the 316th Division attacked \"Eliane\" 2. Just as it appeared the French were about to be overrun, a few French tanks arrived from the central garrison, and helped push the Viet Minh back. Smaller attacks on \"Eliane\" 4 were also pushed back. The Viet Minh briefly captured \"Huguette\" 7, only to be pushed back by a French counterattack at dawn on 1 April.\nFighting continued in this manner over the next several nights. The Viet Minh repeatedly attacked \"Eliane\" 2, only to be beaten back. Repeated attempts to reinforce the French garrison by parachute drops were made, but had to be carried out by lone planes at irregular times to avoid excessive casualties from Viet Minh anti-aircraft fire. Some reinforcements did arrive, but not enough to replace French casualties.\nTrench warfare.\nOn 5 April, after a long night of battle, French fighter-bombers and artillery inflicted particularly devastating losses on one Viet Minh regiment, which was caught on open ground. At that point, Gi\u00e1p decided to change tactics. Although Gi\u00e1p still had the same objective \u2013 to overrun French defenses east of the river \u2013 he decided to employ entrenchment and sapping to achieve it.\nOn 10 April, the French attempted to retake \"Eliane\" 1, which had been lost eleven days earlier. The loss posed a significant threat to \"Eliane\" 4, and the French wanted to eliminate that threat. The dawn attack, which Bigeard devised, began with a short, massive artillery barrage, followed by small unit infiltration attacks, then mopping-up operations. \"Eliane\" 1 changed hands several times that day, but by the next morning the French had control of the strong point. The Viet Minh attempted to retake it on the evening of 12 April, but were pushed back.\nAt this point, the morale of the Viet Minh soldiers was greatly lowered due to the massive casualties they had received from heavy French gunfire. During a period of stalemate from 15 April to 1 May, the French intercepted enemy radio messages which told of whole units refusing orders to attack, and Viet Minh prisoners in French hands said that they were told to advance or be shot by the officers and non-commissioned officers behind them. Worse still, the Viet Minh lacked advanced medical treatment and care, leading a US general commenting on the war to observe that \"Nothing strikes at combat-morale like the knowledge that if wounded, the soldier will go uncared for\".\nDuring the fighting at \"Eliane\" 1, on the other side of camp, the Viet Minh entrenchments had almost entirely surrounded \"Huguette\" 1 and 6. On 11 April the garrison of \"Huguette\" 1, supported by artillery from \"Claudine\", launched an attack with the goal of resupplying \"Huguette\" 6 with water and ammunition. The attacks were repeated on the nights of the 14\u201315 and 16\u201317 April. While they did succeed in getting some supplies through, the French suffered heavy casualties, which convinced Langlais to abandon \"Huguette\" 6. Following a failed attempt to link up, on 18 April, the defenders at \"Huguette\" 6 made a daring break out, but only a few managed to make it to French lines. The Viet Minh repeated the isolation and probing attacks against \"Huguette\" 1, and overran the fort on the morning of 22 April. After this key advance, the Viet Minh took control of more than 90 percent of the airfield, making accurate French parachute drops impossible. This caused the landing zone to become perilously small, and effectively choked off much needed supplies. A French attack against \"Huguette\" 1 later that day was repulsed.\nIsabelle.\n\"Isabelle\" saw only light action until 30 March, when the Viet Minh isolated it and beat back the attempt to send reinforcements north. Following a massive artillery barrage on 30 March, the Viet Minh began employing the same trench warfare tactics that they were using against the central camp. By the end of April, \"Isabelle\" had exhausted its water supply and was nearly out of ammunition.\nFinal attacks.\nThe Viet Minh launched a massed assault against the exhausted defenders on the night of 1 May, overrunning \"Eliane\" 1, \"Dominique\" 3, and \"Huguette\" 5, although the French managed to beat back attacks on \"Eliane\" 2. On 6 May, the Viet Minh launched another massed attack against \"Eliane\" 2, using, for the first time, Katyusha rockets. The French artillery fired a \"TOT\" (time on target) mission, so that artillery rounds fired from different positions would strike on target at the same time. This barrage defeated the first assault wave, but later that night the Viet Minh detonated a mine under \"Eliane\" 2, with devastating effect. The Viet Minh attacked again, and within a few hours the defenders were overrun.\nOn 7 May, Gi\u00e1p ordered an all-out attack against the remaining French units with over 25,000 Viet Minh against fewer than 3,000 garrison troops. At 17:00, de Castries radioed French headquarters in Hanoi and talked with Cogny: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe last radio transmission from the French headquarters reported that enemy troops were directly outside the headquarters bunker and that all the positions had been overrun. The radio operator in his last words stated: \"The enemy has overrun us. We are blowing up everything. \"Vive la France\"!\" That night the garrison made a breakout attempt, in the Camar\u00f3n tradition. While some of the main body managed to break out, none succeeded in escaping the valley. At \"Isabelle\", a similar attempt later the same night saw about 70 troops, out of 1,700 men in the garrison, escape to Laos. By about 18:20, only one French position, strong point Lily, manned by Moroccan soldiers commanded by a French officer, Major Jean Nicolas, had not been overrun. The position surrendered that night when Nicolas personally waved a small white flag (probably a handkerchief) from his rifle.\nAftermath.\nDien Bien Phu was a serious defeat for the French and was the decisive battle of the Indochina war. The garrison constituted roughly one-tenth of the total French Union manpower in Indochina, and the defeat seriously weakened the position and prestige of the French; it produced psychological repercussions both in the armed forces and in the political structure in France. This was apparent with the previously planned negotiations over the future of Indochina, which had just begun. Militarily there was no point in France fighting on, as the Viet Minh could repeat the strategy and tactics of the Dien Bien Phu campaign elsewhere, to which the French had no effective response.\nNews of Dien Bien Phu's fall was announced in France several hours after the surrender, around 16:45, by Prime Minister Joseph Laniel. The Archbishop of Paris ordered a mass, while radio performances were cancelled and replaced by solemn music, notably Berlioz' \"Requiem\". Theatres and restaurants closed and many social engagements were cancelled as a mark of respect. Public opinion in France registered shock that a guerrilla army had defeated a major European power.\nDisaster in the Central Highlands.\nFollowing the fall of Dien Bien Phu the French high command gave the order for any isolated positions to be abandoned, to avoid another disaster. The area of concern was the isolated defensive positions in the Central Highlands at An Khe. \"Groupement Mobile No. 100\" (\"Group Mobile 100\" or G.M. 100) which included the elite veteran UN \"Bataillon de Cor\u00e9e\" was ordered to abandon the area and fall back to Pleiku, some 50 miles away over Route Coloniale 19. Code named \"op\u00e9ration \u00c9glantine\", GM.100, however were hit in a series of deadly ambushes at the Battle of Mang Yang Pass, suffering heavy losses in men and vehicles by 29 June and the remnants struggled back to Pleiku.\nThe French then needed to keep \"Route Coloniale 14\" between Pleiku and Ban M\u00ea Thu\u1ed9t open and withdraw the units posted there. Operation \"Myosotis\" \"(Forget-Me-Not)\" began - 'Groupement Mobile No. 42' (GM.42) in an armoured convoy was sent to relieve the units, but were ambushed at the Battle of Chu Dreh Pass on July 17, just four days before the armistice suffering further heavy losses and the almost complete destruction of Bataillon de Cor\u00e9e.\nPrisoners.\nOn 8 May, the Viet Minh counted 11,721 prisoners, of whom 4,436 were wounded. This was the greatest number the Viet Minh had ever captured, amounting to one-third of the total captured during the entire war. The prisoners were divided into groups. Able-bodied soldiers were force-marched over to prison camps to the north and east, where they were intermingled with Viet Minh soldiers to discourage French bombing runs. Hundreds died of disease along the way. The wounded were given basic first aid until the Red Cross arrived, extracted 858 prisoners, and provided better aid to the remainder. Those wounded who were not evacuated by the Red Cross were sent into detention.\nOnly around 200 French soldiers managed to escape, going through the Viet Minh line towards Luang Prabang in Laos on the famous \"Pavie Trail\". Of 10,863 prisoners (including Vietnamese fighting for the French), only 3,290 were repatriated four months later; however, the losses figure may include the 3,013 prisoners of Vietnamese origin whose fate is unknown.\nCasualties.\nThe Vietnamese government reported its casualties in the battle as 4,020 dead, 9,118 wounded, and 792 missing. The French estimated Viet Minh casualties at 8,000 dead and 15,000 wounded. Max Hastings stated that \"In 2018 Hanoi has still not credibly enumerated its Dien Bien Phu losses, surely a reflection of their immensity.\" Mark Moyar's book \"Triumph Forsaken\" lists Viet Minh casualties as 22,900 out of an original force of 50,000.\nPolitical ramifications.\nThe Geneva Conference opened on 8 May 1954, the day after the surrender of the garrison. The resulting agreement in July partitioned Vietnam into two zones: communist North Vietnam and the State of Vietnam, which gained full independence from France on 4 June and opposed the agreement, to the south. The partition was supposed to be temporary, and the two zones were meant to be reunited through national elections in 1956, which were never held. The last French forces withdrew from Vietnam in 1956.\nGeneral Georges Catroux presided over a commission of inquiry into the defeat. The commission's final report (\"Rapport concernant la conduite des op\u00e9rations en Indochine sous la direction du g\u00e9n\u00e9ral Navarre\") concluded:\nThe fall of Dien Bien Phu, in a strictly military perspective, represented a very serious failure but one that in the immediate, that is to say, spring of 1954, did not upset the balance of forces present in Indochina. It only assumed the aspect of a definitive defeat of our forces by reason of its profound psychological effects on French public opinion, which, tired of a war that was unpopular and seemingly without end, demanded in a way that it be ended.\nThe event itself was in fact, both in terms of public opinion and of the military conduct of the war and operations, merely the end result of a long process of degradation of a faraway enterprise which, not having the assent of the nation, could not receive from the authorities the energetic impulse, and the size and continuity of efforts required for success.\nIf, therefore, one wishes to establish objectively the responsibilities incurred in the final phase of the Indochina war one would have to examine its origins and evoke the acts and decisions of the various governments in power, that is to say their war policies, as well as the ways in which these policies were translated by the military commanders into operations.Outside of Indochina, the political significance of the battle was far-reaching, as news of the French defeat rapidly spread throughout the remainder of its colonies. The Algerian National Liberation Front viewed it as an epoch-changing moment, with Ferhat Abbas, post-colonial Algeria's first president, declaring:Dien Bien Phu was more than just a military victory. This battle is a symbol. It's the \"Valmy\" of the colonized peoples. It's the affirmation of the Asian and the African vis-\u00e0-vis the European. It is the confirmation of the universality of human rights. At Dien Bien Phu, the French lost the only source of \"legitimation\" on which their presence turned, that is the right of the strongest [to rule the weakest].\nWomen.\nMany of the flights operated by the French Air force to evacuate casualties had female flight nurses on board. A total of 15 women served on flights to \u0110i\u1ec7n Bi\u00ean Ph\u1ee7. One, Genevi\u00e8ve de Galard, was stranded there when her plane was destroyed by shellfire while it was being repaired on the airfield. She remained on the ground providing medical services in the field hospital until the surrender. She was referred to as the \"Angel of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7\". Historians disagree regarding the moniker with Martin Windrow maintaining that de Galard was referred to by name only by the garrison; Michael Kenney and Bernard Fall also maintained that the nickname was added by outside press agencies.\nThe French forces came to \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 accompanied by two \"bordels mobiles de campagne\", (mobile field brothels), served by Algerian and Vietnamese women. When the siege ended, the Viet Minh sent the surviving Vietnamese women for \"re-education\".\nUS participation.\nBefore the battle started both British and American missions visited Di\u00ean Bi\u00ean Phu to complete an assessment, and then left.\nThe fall of Di\u00ean Bi\u00ean Phu was a disaster not just for France but also for the United States who, by 1954, was underwriting 80% of French expenditures in Indochina. According to the Mutual Defense Assistance Act, the United States provided the French with material aid during the battle\u00a0\u2013 aircraft (supplied by the ), weapons, mechanics, 24 CIA/CAT pilots, and US Air Force maintenance crews.\nThe United States nevertheless intentionally avoided overt direct intervention. In February 1954, following the French occupation of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, Democratic senator Michael Mansfield asked the United States Defense Secretary, Charles Erwin Wilson, whether the United States would send naval or air units if the French were subjected to greater pressure there, but Wilson replied that \"for the moment there is no justification for raising United States aid above its present level\". On 31 March, following the fall of \"Beatrice\", \"Gabrielle\", and \"Anne-Marie\", a panel of US Senators and Representatives questioned the US Chairman of the Joint Chiefs of Staff, Admiral Arthur W. Radford, about the possibility of US involvement. Radford concluded it was too late for the US Air Force which had the potential to use its Philippines-based B-29s against the Viet Minh heavy artillery. A proposal for direct intervention was unanimously voted down by the committee three days later, which \"concluded that intervention was a positive \"casus belli\" (act of war)\".\nBoth Eisenhower and the Secretary of State John Foster Dulles then pressed the British and other allies in a joint military operation. Prime Minister Winston Churchill and Foreign Secretary Anthony Eden refused, but agreed on a collective security arrangement for the region which could be agreed at the Geneva conference. For the Americans, in particular Dulles, this was not enough. Britain, already for some years involved in the Malayan Emergency, was concerned at the American alarmism in the region, but was unaware of the scale of US financial aid and covert involvement in the Indochina war.\nThere were already suggestions at the time, notably from French author Jules Roy, that Admiral Radford had discussed with the French the possibility of using tactical nuclear weapons in support of the French garrison. Moreover, Dulles reportedly mentioned the possibility of lending atomic bombs to the French for use at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 in April, Dulles tried to put more pressure on the British, and asked Eden for British support for American air action to save Di\u00ean Bi\u00ean Phu. Eden refused, which enraged Dulles; however, Eisenhower relented. The President felt that, along with the political risks, airstrikes alone would not decide the battle, and did not want to escalate US involvement by using American pilots. \"Nobody is more opposed to intervention than I am\".\nThe United States did covertly participate in the battle. Following a request for help from Navarre, Radford provided two squadrons of B-26 Invader bomber aircraft and crew personnel to support the French. However not the Pentagon but the CIA under the leadership of Secretary Dulles' brother Allen Dulles managed the operation. Following this, 39 American transport pilots, officially employed by CAT, a CIA owned company, flew 682 sorties over the course of the battle. Earlier, in order to succeed the pre-\u0110i\u1ec7n Bi\u00ean Ph\u1ee7 Operation Castor of November 1953, General Chester McCarty made available twelve additional C-119 Flying Boxcars flown by French crews.\nTwo of the American pilots, James B. McGovern Jr. and Wallace Buford, were killed in action during the siege of \u0110i\u1ec7n Bi\u00ean Ph\u1ee7. On 25 February 2005, the seven still-living American pilots were awarded the French Legion of Honor by Jean-David Levitte, the French Ambassador to the United States. The role of CIA controlled airlines in Indochina and their role at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 was little known until they were published by Robbins after the end of the Vietnam War, and not officially acknowledged until the 21st century. A generation later the U.S. historian Erik Kirsinger researched the case for more than a year.\nDulles, on hearing of the news of the fall of the garrison, was furious, placing heavy blame on Eden for his \"inaction\". Eden, however, doubted that intervention could have saved Di\u00ean Bi\u00ean Phu, and felt \"it might have far reaching consequences\". Colonel William F. Long stated twelve years after the defeat:\nDien Bien Phu\u2014or DBP\u2014has become an acronym or shorthand symbol for defeat of the West by the East, for the triumph of primitive new doctrines and techniques of peoples' war over the sophisticated principles and maxims of the heritage of Napoleon Bonaparte. Dien Bien Phu resulted in severe political consequences.\nLegacy.\nComparison with Khe Sanh.\nIn January 1968, during the Vietnam War, the North Vietnamese Army under V\u00f5 Nguy\u00ean Gi\u00e1p's command initiated a siege and artillery bombardment on the US Marine Corps base at Khe Sanh in South Vietnam, as they did at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7. A number of factors were significantly different between Khe Sanh and \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, however. Khe Sanh was much closer to a US supply base () compared to \u0110i\u1ec7n Bi\u00ean Ph\u1ee7's proximity to the nearest French base (). At Khe Sanh, the US Marines held the high ground, and their artillery forced the North Vietnamese to use their own artillery from a much greater distance. By contrast, at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7, the French artillery (six 105\u00a0mm batteries and one battery of four 155\u00a0mm howitzers and mortars) was only sporadically effective. Furthermore, by 1968, the US military presence in Vietnam dwarfed that of the French in 1954, and included numerous technological advances such as effective helicopters.\nKhe Sanh received 18,000 tons of aerial resupplies during the 77-day battle, whereas during the 167 days that the French forces at \u0110i\u1ec7n Bi\u00ean Ph\u1ee7 held out, they received only 4,000 tons. Also, the US Air Force dropped 114,810 tons of bombs on the North Vietnamese at Khe Sanh, roughly as many as dropped on all of Japan in 1945 during World War II.\nIt is also possible that Gi\u00e1p never intended to capture Khe Sanh in the first place, and that Khe Sanh was used as a diversion for the upcoming Tet Offensive.\nBattlefield today.\n\u0110i\u1ec7n Bi\u00ean Ph\u1ee7 today is a popular Vietnam historical tourist attraction. It has a modern museum and much of the battlefield is preserved, including several of the fortified French positions, the bunkered French headquarters, the Viet Minh headquarters complex and a number of memorials.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66334", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=66334", "title": "Word bubble", "text": ""}
{"id": "66337", "revid": "44424", "url": "https://en.wikipedia.org/wiki?curid=66337", "title": "Robert A. Caro", "text": ""}
{"id": "66338", "revid": "44880119", "url": "https://en.wikipedia.org/wiki?curid=66338", "title": "Holography", "text": "Recording to reproduce a three-dimensional light field\nHolography is a technique that allows a wavefront to be recorded and later reconstructed. It is best known as a method of generating three-dimensional images, and has a wide range of other uses, including data storage, microscopy, and interferometry. In principle, it is possible to make a hologram for any type of wave.\nA hologram is a recording of an interference pattern that can reproduce a 3D light field using diffraction. In general usage, a hologram is a recording of any type of wavefront in the form of an interference pattern. It can be created by capturing light from a real scene, or it can be generated by a computer, in which case it is known as a computer-generated hologram, which can show virtual objects or scenes. Optical holography needs a laser light to record the light field. The reproduced light field can generate an image that has the depth and parallax of the original scene. A hologram is usually unintelligible when viewed under diffuse ambient light. When suitably lit, the interference pattern diffracts the light into an accurate reproduction of the original light field, and the objects that were in it exhibit visual depth cues such as parallax and perspective that change realistically with the different angles of viewing. That is, the view of the image from different angles shows the subject viewed from similar angles.\nA hologram is traditionally generated by overlaying a second wavefront, known as the reference beam, onto a wavefront of interest. This generates an interference pattern, which is then captured on a physical medium. When the recorded interference pattern is later illuminated by the second wavefront, it is diffracted to recreate the original wavefront. The 3D image from a hologram can often be viewed with non-laser light. However, in common practice, major image quality compromises are made to remove the need for laser illumination to view the hologram.\nA computer-generated hologram is created by digitally modeling and combining two wavefronts to generate an interference pattern image. This image can then be printed onto a mask or film and illuminated with an appropriate light source to reconstruct the desired wavefront. Alternatively, the interference pattern image can be directly displayed on a dynamic holographic display.\nHolographic portraiture often resorts to a non-holographic intermediate imaging procedure, to avoid the dangerous high-powered pulsed lasers which would be needed to optically \"freeze\" moving subjects as perfectly as the extremely motion-intolerant holographic recording process requires. Early holography required high-power and expensive lasers. Currently, mass-produced low-cost laser diodes, such as those found on DVD recorders and used in other common applications, can be used to make holograms. They have made holography much more accessible to low-budget researchers, artists, and dedicated hobbyists.\nMost holograms produced are of static objects, but systems for displaying changing scenes on dynamic holographic displays are now being developed.\nThe word \"holography\" comes from the Greek words (\"holos\"; \"whole\") and (\"graph\u0113\"; \"writing\" or \"drawing\").\nHistory.\nThe Hungarian-British physicist Dennis Gabor invented holography in 1948 while he was looking for a way to improve image resolution in electron microscopes. Gabor's work was built on pioneering work in the field of X-ray microscopy by other scientists including Mieczys\u0142aw Wolfke in 1920 and William Lawrence Bragg in 1939. The formulation of holography was an unexpected result of Gabor's research into improving electron microscopes at the British Thomson-Houston Company (BTH) in Rugby, England, and the company filed a patent in December 1947 (patent GB685286). The technique as originally invented is still used in electron microscopy, where it is known as electron holography. Gabor was awarded the Nobel Prize in Physics in 1971 \"for his invention and development of the holographic method\".\nOptical holography did not really advance until the development of the laser in 1960. The development of the laser enabled the first practical optical holograms that recorded 3D objects to be made in 1962 by Yuri Denisyuk in the Soviet Union and by Emmett Leith and Juris Upatnieks at the University of Michigan, US.\nEarly optical holograms used silver halide photographic emulsions as the recording medium. They were not very efficient as the produced diffraction grating absorbed much of the incident light. Various methods of converting the variation in transmission to a variation in refractive index (known as \"bleaching\") were developed which enabled much more efficient holograms to be produced.\nA major advance in the field of holography was made by Stephen Benton, who invented a way to create holograms that can be viewed with natural light instead of lasers. These are called rainbow holograms.\nBasics of holography.\nHolography is a technique for recording and reconstructing light fields. \nA light field is generally the result of a light source scattered off objects. Holography can be thought of as somewhat similar to sound recording, whereby a sound field created by vibrating matter like musical instruments or vocal cords, is encoded in such a way that it can be reproduced later, without the presence of the original vibrating matter. However, it is even more similar to Ambisonic sound recording in which any listening angle of a sound field can be reproduced in the reproduction.\nLaser.\nIn laser holography, the hologram is recorded using a source of laser light, which is in phase. Various setups may be used, and several types of holograms can be made, but all involve the interaction of light coming from different directions and producing a microscopic interference pattern which a plate, film, or other medium photographically records.\nIn one common arrangement, the laser beam is split into two, one known as the object beam and the other as the reference beam. The object beam is expanded by passing it through a lens and used to illuminate the subject. The recording medium is located where this light, after being reflected or scattered by the subject, will strike it. The edges of the medium will ultimately serve as a window through which the subject is seen, so its location is chosen with that in mind. The reference beam is expanded and made to shine directly on the medium, where it interacts with the light coming from the subject to create the desired interference pattern.\nLike conventional photography, holography requires an appropriate exposure time to correctly affect the recording medium. Unlike conventional photography, during the exposure the light source, the optical elements, the recording medium, and the subject must all remain motionless relative to each other, to within about a quarter of the wavelength of the light, or the interference pattern will be blurred and the hologram spoiled. With living subjects and some unstable materials, that is only possible if a very intense and extremely brief pulse of laser light is used, a hazardous procedure which is rarely done outside of scientific and industrial laboratory settings. Exposures lasting several seconds to several minutes, using a much lower-powered continuously operating laser, are typical.\nApparatus.\nA hologram can be made by shining part of the light beam directly into the recording medium, and the other part onto the object in such a way that some of the scattered light falls onto the recording medium. A more flexible arrangement for recording a hologram requires the laser beam to be aimed through a series of elements that change it in different ways. The first element is a beam splitter that divides the beam into two identical beams, each aimed in different directions:\nSeveral different materials can be used as the recording medium. One of the most common is a film very similar to photographic film (silver halide photographic emulsion), but with much smaller light-reactive grains (preferably with diameters less than 20\u00a0nm), making it capable of the much higher resolution that holograms require. A layer of this recording medium (e.g., silver halide) is attached to a transparent substrate, which is commonly glass, but may also be plastic.\nProcess.\nWhen the two laser beams reach the recording medium, their light waves intersect and interfere with each other. It is this interference pattern that is imprinted on the recording medium. The pattern itself is seemingly random, as it represents the way in which the scene's light \"interfered\" with the original light source \u2013 but not the original light source itself. The interference pattern can be considered an encoded version of the scene, requiring a particular key \u2013 the original light source \u2013 in order to view its contents.\nThis missing key is provided later by shining a laser, identical to the one used to record the hologram, onto the developed film. When this beam illuminates the hologram, it is diffracted by the hologram's surface pattern. This produces a light field identical to the one originally produced by the scene and scattered onto the hologram.\nComparison with photography.\nHolography may be better understood via an examination of its differences from ordinary photography:\nPhysics of holography.\nFor a better understanding of the process, it is necessary to understand interference and diffraction. Interference occurs when one or more wavefronts are superimposed. Diffraction occurs when a wavefront encounters an object. The process of producing a holographic reconstruction is explained below purely in terms of interference and diffraction. It is somewhat simplified but is accurate enough to give an understanding of how the holographic process works.\nFor those unfamiliar with these concepts, it is worthwhile to read those articles before reading further in this article.\nPlane wavefronts.\nA diffraction grating is a structure with a repeating pattern. A simple example is a metal plate with slits cut at regular intervals. A light wave that is incident on a grating is split into several waves; the direction of these diffracted waves is determined by the grating spacing and the wavelength of the light.\nA simple hologram can be made by superimposing two plane waves from the same light source on a holographic recording medium. The two waves interfere, giving a straight-line fringe pattern whose intensity varies sinusoidally across the medium. The spacing of the fringe pattern is determined by the angle between the two waves, and by the wavelength of the light.\nThe recorded light pattern is a diffraction grating. When it is illuminated by only one of the waves used to create it, it can be shown that one of the diffracted waves emerges at the same angle at which the second wave was originally incident, so that the second wave has been 'reconstructed'. Thus, the recorded light pattern is a holographic recording as defined above.\nPoint sources.\nIf the recording medium is illuminated with a point source and a normally incident plane wave, the resulting pattern is a sinusoidal zone plate, which acts as a negative Fresnel lens whose focal length is equal to the separation of the point source and the recording plane.\nWhen a plane wave-front illuminates a negative lens, it is expanded into a wave that appears to diverge from the focal point of the lens. Thus, when the recorded pattern is illuminated with the original plane wave, some of the light is diffracted into a diverging beam equivalent to the original spherical wave; a holographic recording of the point source has been created.\nWhen the plane wave is incident at a non-normal angle at the time of recording, the pattern formed is more complex, but still acts as a negative lens if it is illuminated at the original angle.\nComplex objects.\nTo record a hologram of a complex object, a laser beam is first split into two beams of light. One beam illuminates the object, which then scatters light onto the recording medium. According to diffraction theory, each point in the object acts as a point source of light so the recording medium can be considered to be illuminated by a set of point sources located at varying distances from the medium.\nThe second (reference) beam illuminates the recording medium directly. Each point source wave interferes with the reference beam, giving rise to its own sinusoidal zone plate in the recording medium. The resulting pattern is the sum of all these 'zone plates', which combine to produce a random (speckle) pattern as in the photograph above.\nWhen the hologram is illuminated by the original reference beam, each of the individual zone plates reconstructs the object wave that produced it, and these individual wavefronts are combined to reconstruct the whole of the object beam. The viewer perceives a wavefront that is identical with the wavefront scattered from the object onto the recording medium, so that it appears that the object is still in place even if it has been removed.\nApplications.\nArt.\nEarly on, artists saw the potential of holography as a medium and gained access to science laboratories to create their work. Holographic art is often the result of collaborations between scientists and artists, although some holographers would regard themselves as both an artist and a scientist.\nSalvador Dal\u00ed claimed to have been the first to employ holography artistically. He was certainly the first and best-known surrealist to do so, but the 1972 New York exhibit of Dal\u00ed holograms had been preceded by the holographic art exhibition that was held at the Cranbrook Academy of Art in Michigan in 1968 and by the one at the Finch College gallery in New York in 1970, which attracted national media attention. In Great Britain, Margaret Benyon began using holography as an artistic medium in the late 1960s and had a solo exhibition at the University of Nottingham art gallery in 1969. This was followed in 1970 by a solo show at the Lisson Gallery in London, which was billed as the \"first London expo of holograms and stereoscopic paintings\".\nDuring the 1970s, a number of art studios and schools were established, each with their particular approach to holography. Notably, there was the San Francisco School of Holography established by Lloyd Cross, The Museum of Holography in New York founded by Rosemary (Posy) H. Jackson, the Royal College of Art in London and the Lake Forest College Symposiums organised by Tung Jeong. None of these studios still exist; however, there is the Center for the Holographic Arts in New York and the HOLOcenter in Seoul, which offers artists a place to create and exhibit work.\nDuring the 1980s, many artists who worked with holography helped the diffusion of this so-called \"new medium\" in the art world, such as Harriet Casdin-Silver of the United States, Dieter Jung of Germany, and Moys\u00e9s Baumstein of Brazil, each one searching for a proper \"language\" to use with the three-dimensional work, avoiding the simple holographic reproduction of a sculpture or object. For instance, in Brazil, many concrete poets (Augusto de Campos, D\u00e9cio Pignatari, Julio Plaza and Jos\u00e9 Wagner Garcia, associated with Moys\u00e9s Baumstein) found in holography a way to express themselves and to renew concrete poetry.\nA small but active group of artists still integrate holographic elements into their work. Some are associated with novel holographic techniques; for example, artist Matt Brand employed computational mirror design to eliminate image distortion from specular holography.\nThe MIT Museum and Jonathan Ross both have extensive collections of holography and on-line catalogues of art holograms.\nData storage.\nHolographic data storage is a technique that can store information at high density inside crystals or photopolymers. The ability to store large amounts of information in some kind of medium is of great importance, as many electronic products incorporate storage devices. As current storage techniques such as Blu-ray Disc reach the limit of possible data density (due to the diffraction-limited size of the writing beams), holographic storage has the potential to become the next generation of popular storage media. The advantage of this type of data storage is that the volume of the recording media is used instead of just the surface.\nCurrently available SLMs can produce about 1000 different images a second at 1024\u00d71024-bit resolution which would result in about one-gigabit-per-second writing speed.\nIn 2005, companies such as Optware and Maxell produced a 120\u00a0mm disc that uses a holographic layer to store data to a potential 3.9\u00a0TB, a format called Holographic Versatile Disc. As of September 2014, no commercial product has been released.\nAnother company, InPhase Technologies, was developing a competing format, but went bankrupt in 2011 and all its assets were sold to Akonia Holographics, LLC.\nWhile many holographic data storage models have used \"page-based\" storage, where each recorded hologram holds a large amount of data, more recent research into using submicrometre-sized \"microholograms\" has resulted in several potential 3D optical data storage solutions. While this approach to data storage can not attain the high data rates of page-based storage, the tolerances, technological hurdles, and cost of producing a commercial product are significantly lower.\nDynamic holography.\nIn static holography, recording, developing and reconstructing occur sequentially, and a permanent hologram is produced.\nThere also exist holographic materials that do not need the developing process and can record a hologram in a very short time. This allows one to use holography to perform some simple operations in an all-optical way. Examples of applications of such real-time holograms include phase-conjugate mirrors (\"time-reversal\" of light), optical cache memories, image processing (pattern recognition of time-varying images), and optical computing.\nThe amount of processed information can be very high (terabits/s), since the operation is performed in parallel on a whole image. This compensates for the fact that the recording time, which is in the order of a microsecond, is still very long compared to the processing time of an electronic computer. The optical processing performed by a dynamic hologram is also much less flexible than electronic processing. On one side, one has to perform the operation always on the whole image, and on the other side, the operation a hologram can perform is basically either a multiplication or a phase conjugation. In optics, addition and Fourier transform are already easily performed in linear materials, the latter simply by a lens. This enables some applications, such as a device that compares images in an optical way.\nThe search for novel for dynamic holography is an active area of research. The most common materials are photorefractive crystals, but in semiconductors or semiconductor heterostructures (such as quantum wells), atomic vapors and gases, plasmas and even liquids, it was possible to generate holograms.\nA particularly promising application is optical phase conjugation. It allows the removal of the wavefront distortions a light beam receives when passing through an aberrating medium, by sending it back through the same aberrating medium with a conjugated phase. This is useful, for example, in free-space optical communications to compensate for atmospheric turbulence (the phenomenon that gives rise to the twinkling of starlight).\nHobbyist use.\nSince the beginning of holography, many holographers have explored its uses and displayed them to the public.\nIn 1971, Lloyd Cross opened the San Francisco School of Holography and taught amateurs how to make holograms using only a small (typically 5\u00a0mW) helium-neon laser and inexpensive home-made equipment. Holography had been supposed to require a very expensive metal optical table set-up to lock all the involved elements down in place and damp any vibrations that could blur the interference fringes and ruin the hologram. Cross's home-brew alternative was a sandbox made of a cinder block retaining wall on a plywood base, supported on stacks of old tires to isolate it from ground vibrations, and filled with sand that had been washed to remove dust. The laser was securely mounted atop the cinder block wall. The mirrors and simple lenses needed for directing, splitting and expanding the laser beam were affixed to short lengths of PVC pipe, which were stuck into the sand at the desired locations. The subject and the photographic plate holder were similarly supported within the sandbox. The holographer turned off the room light, blocked the laser beam near its source using a small relay-controlled shutter, loaded a plate into the holder in the dark, left the room, waited a few minutes to let everything settle, then made the exposure by remotely operating the laser shutter.\nIn 1979, Jason Sapan opened the Holographic Studios in New York City. Since then, they have been involved in the production of many holographs for many artists as well as companies. Sapan has been described as the \"last professional holographer of New York\".\nMany of these holographers would go on to produce art holograms. In 1983, Fred Unterseher, a co-founder of the San Francisco School of Holography and a well-known holographic artist, published the \"Holography Handbook\", an easy-to-read guide to making holograms at home. This brought in a new wave of holographers and provided simple methods for using the then-available AGFA silver halide recording materials.\nIn 2000, Frank DeFreitas published the \"Shoebox Holography Book\" and introduced the use of inexpensive laser pointers to countless hobbyists. For many years, it had been assumed that certain characteristics of semiconductor laser diodes made them virtually useless for creating holograms, but when they were eventually put to the test of practical experiment, it was found that not only was this untrue, but that some actually provided a coherence length much greater than that of traditional helium-neon gas lasers. This was a very important development for amateurs, as the price of red laser diodes had dropped from hundreds of dollars in the early 1980s to about $5 after they entered the mass market as a component pulled from CD, or later, DVD players from the mid-1980s onwards. Now, there are thousands of amateur holographers worldwide.\nBy late 2000, holography kits with inexpensive laser pointer diodes entered the mainstream consumer market. These kits enabled students, teachers, and hobbyists to make several kinds of holograms without specialized equipment, and became popular gift items by 2005. The introduction of holography kits with self-developing plates in 2003 made it possible for hobbyists to create holograms without the bother of wet chemical processing.\nIn 2006, a large number of surplus holography-quality green lasers (Coherent C315) became available and put dichromated gelatin (DCG) holography within the reach of the amateur holographer. The holography community was surprised at the amazing sensitivity of DCG to green light. It had been assumed that this sensitivity would be uselessly slight or non-existent. Jeff Blyth responded with the G307 formulation of DCG to increase the speed and sensitivity to these new lasers.\nKodak and Agfa, the former major suppliers of holography-quality silver halide plates and films, are no longer in the market. While other manufacturers have helped fill the void, many amateurs are now making their own materials. The favorite formulations are dichromated gelatin, Methylene-Blue-sensitised dichromated gelatin, and diffusion method silver halide preparations. Jeff Blyth has published very accurate methods for making these in a small lab or garage.\nA small group of amateurs are even constructing their own pulsed lasers to make holograms of living subjects and other unsteady or moving objects.\nHolographic interferometry.\nHolographic interferometry (HI) is a technique that enables static and dynamic displacements of objects with optically rough surfaces to be measured to optical interferometric precision (i.e. to fractions of a wavelength of light). It can also be used to detect optical-path-length variations in transparent media, which enables, for example, fluid flow to be visualized and analyzed. It can also be used to generate contours representing the form of the surface or the isodose regions in radiation dosimetry.\nIt has been widely used to measure stress, strain, and vibration in engineering structures.\nInterferometric microscopy.\nThe hologram keeps the information on the amplitude and phase of the field. Several holograms may keep information about the same distribution of light, emitted to various directions. The numerical analysis of such holograms allows one to emulate large numerical aperture, which, in turn, enables enhancement of the resolution of optical microscopy. The corresponding technique is called interferometric microscopy. Recent achievements of interferometric microscopy allow one to approach the quarter-wavelength limit of resolution.\nSensors or biosensors.\nThe hologram is made with a modified material that interacts with certain molecules generating a change in the fringe periodicity or refractive index, therefore, the color of the holographic reflection.\nSecurity.\nHolograms are commonly used for security, as they are replicated from a master hologram that requires expensive, specialized and technologically advanced equipment, and are thus difficult to forge. They are used widely in many currencies, such as the Brazilian 20, 50, and 100-reais notes; British 5, 10, 20 and 50-pound notes; South Korean 5000, 10,000, and 50,000-won notes; Japanese 5000 and 10,000 yen notes, Indian 50, 100, 500 rupee notes; and all the currently-circulating banknotes of the Canadian dollar, Croatian kuna, Danish krone, and Euro. They can also be found in credit and bank cards as well as passports, ID cards, books, food packaging, DVDs, and sports equipment. Such holograms come in a variety of forms, from adhesive strips that are laminated on packaging for fast-moving consumer goods to holographic tags on electronic products. They often contain textual or pictorial elements to protect identities and separate genuine articles from counterfeits.\nHolographic scanners are in use in post offices, larger shipping firms, and automated conveyor systems to determine the three-dimensional size of a package. They are often used in tandem with checkweighers to allow automated pre-packing of given volumes, such as a truck or pallet for bulk shipment of goods.\nHolograms produced in elastomers can be used as stress-strain reporters due to its elasticity and compressibility, the pressure and force applied are correlated to the reflected wavelength, therefore its color. Holography technique can also be effectively used for radiation dosimetry.\nHigh-security registration plates.\nHigh-security holograms can be used on license plates for vehicles such as cars and motorcycles. As of April 2019, holographic license plates are required on vehicles in parts of India to aid in identification and security, especially in cases of car theft. Such number plates hold electronic data of vehicles, and they have a unique ID number and a sticker to indicate authenticity.\nHolography using other types of waves.\nIn principle, it is possible to make a hologram for any wave.\nElectron holography is the application of holography techniques to electron waves rather than light waves. Electron holography was invented by Dennis Gabor to improve the resolution and avoid the aberrations of the transmission electron microscope. Today it is commonly used to study electric and magnetic fields in thin films, as magnetic and electric fields can shift the phase of the interfering wave passing through the sample. The principle of electron holography can also be applied to interference lithography.\nAcoustic holography enables sound maps of an object to be generated. Measurements of the acoustic field are made at many points close to the object. These measurements are digitally processed to produce the \"images\" of the object.\nAtomic holography has evolved out of the development of the basic elements of atom optics. With the Fresnel diffraction lens and atomic mirrors atomic holography follows a natural step in the development of the physics (and applications) of atomic beams. Recent developments including atomic mirrors and especially ridged mirrors have provided the tools necessary for the creation of atomic holograms, although such holograms have not yet been commercialized.\nNeutron beam holography has been used to see the inside of solid objects.\nHolograms with x-rays are generated by using synchrotrons or x-ray free-electron lasers as radiation sources and pixelated detectors such as CCDs as recording medium. The reconstruction is then retrieved via computation. Due to the shorter wavelength of x-rays compared to visible light, this approach allows imaging objects with higher spatial resolution. As free-electron lasers can provide ultrashort and x-ray pulses in the range of femtoseconds which are intense and coherent, x-ray holography has been used to capture ultrafast dynamic processes.\nFalse holograms.\nThere are many optical effects that are falsely confused with holography, such as the effects produced by lenticular printing, the Pepper's ghost illusion (or modern variants such as the Musion Eyeliner), tomography and volumetric displays. Such illusions have been called \"fauxlography\".\nThe Pepper's ghost technique, being the easiest to implement of these methods, is most prevalent in 3D displays that claim to be (or are referred to as) \"holographic\". While the original illusion, used in theater, involved actual physical objects and persons, located offstage, modern variants replace the source object with a digital screen, which displays imagery generated with 3D computer graphics to provide the necessary depth cues. The reflection, which seems to float mid-air, is still flat, however, and thus it is less realistic than if an actual 3D object was being reflected.\nExamples of this digital version of Pepper's ghost illusion include the Gorillaz performances in the 2005 MTV Europe Music Awards and the 48th Grammy Awards; and Tupac Shakur's virtual performance at Coachella Valley Music and Arts Festival in 2012, rapping alongside Snoop Dogg during his set with Dr. Dre. Digital avatars of the Swedish supergroup ABBA were displayed on stage in May 2022. The ABBA performance used technology that was an updated version of Pepper's Ghost created by Industrial Light &amp; Magic. American rock group KISS unveiled similar digital avatars in December 2023 to tour in their place at the conclusion of the End of the Road World Tour using the same Pepper's Ghost technology as the ABBA avatars.\nAn even simpler illusion can be created by rear-projecting realistic images into semi-transparent screens. The rear projection is necessary because otherwise the semi-transparency of the screen would allow the background to be illuminated by the projection, which would break the illusion.\nCrypton Future Media, a music software company that produced Hatsune Miku, one of many Vocaloid singing synthesizer applications, has produced concerts that have Miku, along with other Crypton Vocaloids, performing on stage as \"holographic\" characters. These concerts use rear projection onto a semi-transparent DILAD or Perspex screen to achieve its \"holographic\" effect.\nIn 2011, in Beijing, apparel company Burberry produced the \"Burberry Prorsum Autumn/Winter 2011 Hologram Runway Show\", which included life size 2-D projections of models. The company's own video shows several centered and off-center shots of the main 2-dimensional projection screen, the latter revealing the flatness of the virtual models. The claim that holography was used was reported as fact in the trade media.\nIn Madrid, on 10 April 2015, a public visual presentation called \"Hologramas por la Libertad\" (Holograms for Liberty), featuring a ghostly virtual crowd of demonstrators, was used to protest a new Spanish law that prohibits citizens from demonstrating in public places. Although widely called a \"hologram protest\" in news reports, no actual holography was involved \u2013 it was yet another technologically updated variant of the Pepper's ghost illusion.\nHolography is distinct from specular holography, which is a technique for making three-dimensional images by controlling the motion of specularities on a two-dimensional surface. It works by reflectively or refractively manipulating bundles of light rays, not by using interference and diffraction.\nIn fiction.\nHolography has been widely referred to in movies, novels, and TV, usually in science fiction, starting in the late 1970s. Science fiction writers absorbed the urban legends surrounding holography that had been spread by overly-enthusiastic scientists and entrepreneurs trying to market the idea. This had the effect of giving the public overly high expectations of the capability of holography, due to the unrealistic depictions of it in most fiction, where they are fully three-dimensional computer projections that are sometimes tactile through the use of force fields. Examples of this type of depiction include the hologram of Princess Leia in \"Star Wars\", Arnold Rimmer from \"Red Dwarf\", who was later converted to \"hard light\" to make him solid, and the Holodeck and from \"Star Trek\".\nHolography has served as an inspiration for many video games with science fiction elements. In many titles, fictional holographic technology has been used to reflect real-life misrepresentations of potential military use of holograms, such as the \"mirage tanks\" in ' that can disguise themselves as trees. Player characters are able to use holographic decoys in games such as ' and \"Crysis 2\" to confuse and distract the enemy. \"Starcraft\" ghost agent Nova has access to \"holo decoy\" as one of her three primary abilities in \"Heroes of the Storm.\"\nFictional depictions of holograms have, however, inspired technological advances in other fields, such as augmented reality, that promise to fulfill the fictional depictions of holograms by other means.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "66340", "revid": "36170109", "url": "https://en.wikipedia.org/wiki?curid=66340", "title": "Goldberg Variations", "text": "Keyboard work by Johann Sebastian Bach\nThe Goldberg Variations (), BWV\u00a0988, is a musical composition for keyboard by Johann Sebastian Bach, consisting of an aria and a set of thirty variations. First published in 1741, it is named after Johann Gottlieb Goldberg, who may also have been the first performer of the work.\nComposition.\nThe story of how the variations came to be composed comes from an early biography of Bach by Johann Nikolaus Forkel:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[For this work] we have to thank the instigation of the former Russian ambassador to the electoral court of Saxony, Count Kaiserling, who often stopped in Leipzig and brought there with him the aforementioned Goldberg, in order to have him given musical instruction by Bach. The Count was often ill and had sleepless nights. At such times, Goldberg, who lived in his house, had to spend the night in an antechamber, so as to play for him during his insomnia. ... Once the Count mentioned in Bach's presence that he would like to have some clavier pieces for Goldberg, which should be of such a smooth and somewhat lively character that he might be a little cheered up by them in his sleepless nights. Bach thought himself best able to fulfill this wish by means of Variations, the writing of which he had until then considered an ungrateful task on account of the repeatedly similar harmonic foundation. But since at this time all his works were already models of art, such also these variations became under his hand. Yet he produced only a single work of this kind. Thereafter the Count always called them \"his\" variations. He never tired of them, and for a long time sleepless nights meant: \"Dear Goldberg, do play me one of my variations.\" Bach was perhaps never so rewarded for one of his works as for this. The Count presented him with a golden goblet filled with 100 \"Louis d'or\". Nevertheless, even had the gift been a thousand times larger, their artistic value would not yet have been paid for.\nForkel wrote his biography in 1802, more than 60 years after the events related, and its accuracy has been questioned. The lack of dedication on the title page also makes the tale of the commission unlikely. Goldberg's age at the time of publication (14 years) has also been cited as grounds for doubting Forkel's tale, although it must be said that he was known to be an accomplished keyboardist and sight-reader. contends that the Forkel story is entirely spurious.\nArnold Schering has suggested that the aria on which the variations are based was not written by Bach. More recent scholarly literature (such as the edition by Christoph Wolff) suggests that there is no basis for such doubts.\nPublication.\nRather unusually for Bach's works, the \"Goldberg Variations\" were published in his own lifetime, in 1741. The publisher was Bach's friend Balthasar Schmid of Nuremberg. Schmid printed the work by making engraved copper plates (rather than using movable type); thus the notes of the first edition are in Schmid's own handwriting.\nThe title page, shown in the figure above, reads in German:\nThe term \"\"Clavier Ubung\" (nowadays spelled \"Klavier\u00fcbung\"\") had been assigned by Bach to some of his previous keyboard works. Klavier\u00fcbung part 1 was the six partitas, part 2 the \"Italian Concerto\" and \"French Overture\", and part 3 a series of chorale preludes for organ framed by a prelude and fugue in E\u266d major. Although Bach also called his variations \"\"Klavier\u00fcbung\", he did not specifically designate them as the fourth in this series.\nNineteen copies of the first edition survive today. Of these, the most valuable is the (Bach's personal copy of the published score), discovered in 1974 in Strasbourg by the French musicologist Olivier Alain and now kept in the Biblioth\u00e8que nationale de France, Paris. This copy includes printing corrections made by the composer and additional music in the form of fourteen canons on the Goldberg ground (see below). The nineteen printed copies provide virtually the only information available to modern editors trying to reconstruct Bach's intent, as the autograph (handwritten) score has not survived. A handwritten copy of just the aria is found in the 1725 Notebook for Anna Magdalena Bach. Christoph Wolff suggests on the basis of handwriting evidence that Anna Magdalena copied the aria from the autograph score around 1740; it appears on two pages previously left blank.\nInstrumentation.\nOn the title page, Bach specified that the work was intended for harpsichord. It is widely performed on this instrument today, though there are also a great number of performances on the piano (see Discography below). The piano was rare in Bach's day and there is no indication that Bach would have either approved or disapproved of performing the variations on this instrument.\nBach's specification is, more precisely, a two-manual harpsichord, and he indicated in the score which variations ought to be played using one hand on each manual: Variations 8, 11, 13, 14, 17, 20, 23, 25, 26, 27 and 28 are specified for two manuals, while variations 5, 7 and 29 are specified as playable with either one or two. With greater difficulty, the work can nevertheless be played on a single-manual harpsichord or piano.\nForm.\nAfter a statement of the aria at the beginning of the piece, there are thirty variations. The variations do not follow the melody of the aria, but rather use its bass line () and chord progression. The bass line is notated by harpsichordist and musicologist Ralph Kirkpatrick in his performing edition as follows.\n&lt;score&gt;\n\\relative\n \\key g \\major\n \\clef bass\n \\omit Staff.TimeSignature\n \\repeat volta 2 {\n g1 fis^\"6\" e^\"3,5,6\u266f\" d b^\"6\" c^\"3,5,6\" d g, \\break\n g' fis^\"6\" e a^\"\u266f\" fis^\"6\" g^\"6\" a^\"\u266f\" d,\n } \\break\n \\repeat volta 2 {\n d' b^\"6\" c^\"6\" b^\"\u266f\" g^\"6\" a^\"3,5,6\" b^\"\u266f\" e, \\break\n c^\"6\" b^\"6\" a d g, c^\"3,5,6\" d g,\n&lt;/score&gt;\nThe digits above the notes indicate the specified chord in the system of figured bass; where digits are separated by comma (stacked vertically in a proper figured bass), they indicate seventh chords in first inversion.\nEvery third variation in the series of 30 is a canon, following an ascending pattern. Thus, variation 3 is a canon at the unison, variation 6 is a canon at the second (the second entry begins the interval of a second above the first), variation 9 is a canon at the third, and so on until variation 27, which is a canon at the ninth. The final variation, instead of being the expected canon in the tenth, is a quodlibet, discussed below.\nAs Kirkpatrick has pointed out, the variations that intervene between the canons are also arranged in a pattern. If we leave aside the initial and final material of the work (specifically, the Aria, the first two variations, the Quodlibet, and the aria da capo), the remaining material is arranged as follows. The variations found just \"after\" each canon are genre pieces of various types, among them three Baroque dances (4, 7, 19); a fughetta (10); a French overture (16); two ornate arias for the right hand (13, 25); and others (22, 28). The variations located \"two\" after each canon (5, 8, 11, 14, 17, 20, 23, 26, and 29) are what Kirkpatrick calls \"arabesques\"; they are variations in lively tempo with a great deal of hand-crossing. This ternary pattern\u2014\"canon\", \"genre piece\", \"arabesque\"\u2014is repeated a total of nine times, until the Quodlibet breaks the cycle.\nAll the variations are in G major, apart from variations 15, 21, and 25, which are in G minor.\nAt the end of the thirty variations, Bach writes \"Aria da Capo e fine\", meaning that the performer is to return to the beginning (\"da capo\") and play the aria again before concluding.\nAria.\nThe aria is a sarabande in time, and features a heavily ornamented melody:\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \\relative c {\n \\key g \\major \\time 3/4\n g4 g( a8.)\\mordent b16 |\n a8 \\appoggiatura g16 fis8 \\appoggiatura e16 d2 |\n g,4\\mordent g4.\\downprall fis16 g |\n a32( g fis16) g32( fis e16) \\appoggiatura fis8 d2 |\n d'4 d( e8.)\\mordent f16 |\n e8 \\appoggiatura d16 c8 \\appoggiatura b16 a4. fis'!8\\turn |\n g32( fis16.) a32( g16.) fis32( e16.) d32( c16.) \\appoggiatura c a'8. c,16 |\n b32( g16.) fis8 \\appoggiatura fis g2\\mordent |\n \\new Staff \u00ab\n \\new Voice \\relative c' {\n \\voiceOne \\clef bass \\key g \\major\n s4 f\\rest d |\n s4 e\\rest d |\n s4 d\\rest cis |\n s4 d\\rest a |\n s4 d\\rest g, |\n s4 d'\\rest a |\n r8 c~ c b16 a g fis e fis |\n g8 a b2 |\n \\new Voice \\relative c' {\n \\voiceThree\n r4 b2 |\n r4 a2 |\n r4 g2 |\n r4 fis2 |\n r4 d2 |\n r4 e4. s8 |\n \\new Voice \\relative c' {\n \\voiceTwo\n g2. |\n fis2. |\n e2. |\n d2~ d8 c |\n b2. |\n c2~ c8 d |\n e8 c d2 |\n g,4. d'8[ e8.^\\mordent fis16] |\n \u00bb\n&lt;/score&gt;\nThe French style of ornamentation suggests that the ornaments are supposed to be parts of the melody; however, some performers (for example Wilhelm Kempff on piano) omit some or all ornaments and present the aria unadorned.\nWilliams opines that this is not the theme at all, but actually the first variation (a view emphasising the idea of the work as a chaconne rather than a piece in true variation form).\nVariatio 1. a 1 Clav..\nThis sprightly variation contrasts markedly with the slow, contemplative mood of the aria. The rhythm in the right hand forces the emphasis on the second beat, giving rise to syncopation from bars 1 to 7. Hands cross at bar 13 from the upper register to the lower, bringing back this syncopation for another two bars. In the first two bars of the B part, the rhythm mirrors that of the beginning of the A part, but after this a different idea is introduced.\nWilliams sees this as a sort of polonaise. The characteristic rhythm in the left hand is also found in Bach's Partita No. 3 for solo violin, in the A\u266d major prelude from the first book of \"The Well-Tempered Clavier\", and in the D minor prelude of the second book. Heinz Niem\u00fcller also mentions the polonaise character of this variation.\nVariatio 2. a 1 Clav..\nThis is a simple three-part contrapuntal piece in time, two voices engage in constant motivic interplay over an incessant bass line. Each section has an alternate ending to be played on the first and second repeat.\nVariatio 3. Canone all'Unisono. a 1 Clav..\nThe first of the regular canons, this is a canon at the unison: the follower begins on the same note as the leader, a bar later. As with all canons of the \"Goldberg Variations\" (except the 27th variation, canon at the ninth), there is a supporting bass line. The time signature of and the many sets of triplets suggest a kind of a simple dance.\nVariatio 4. a 1 Clav..\nLike the passepied, a Baroque dance movement, this variation is in time with a preponderance of quaver rhythms. Bach uses close but not exact imitation: the musical pattern in one part reappears a bar later in another (sometimes inverted).\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \u00ab\n \\new Voice \\relative c\" {\n \\key g \\major \\time 3/8\n r8 b g |\n \\voiceOne d'8 a d |\n g4.~ |\n g8 fis r |\n \\oneVoice R1*3/8 |\n \\voiceOne R1*3/8 |\n r8 a, c~ |\n c16 a b8 r |\n \\new Voice \\relative c' {\n \\voiceTwo\n s4. |\n r8 d fis |\n b,8 g' c |\n b16 g a8 r |\n s4. |\n r8 c, e |\n a,8 d4~ |\n d8 g r |\n \u00bb\n \\new Staff \u00ab\n \\new Voice \\relative c' {\n \\clef bass \\key g \\major \\voiceOne\n s4. |\n s4. |\n r8 b g |\n d'4 r8 |\n r8 g, b |\n e,8 a g~ |\n g8 fis16 e fis8 |\n g8 d r |\n \\new Voice \\relative c' {\n \\voiceTwo\n g4. |\n fis4. |\n e4. |\n r8 d fis |\n b,4. |\n c4. |\n d4. |\n r8 b d |\n \u00bb\n&lt;/score&gt;\nEach repeated section has alternate endings for the first or second time.\nVariatio 5. a 1 \u00f4 vero 2 Clav..\nThis is the first of the hand-crossing, two-part variations; the title means \"for one or two manuals\". The movement is written in time. A rapid melodic line predominantly in sixteenth notes is accompanied by another melody with longer note values, which features very wide leaps:\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \\relative c' {\n \\key g \\major \\time 3/4\n r16 d e fis g fis g a b a g b |\n a16 g fis e d e fis g a fis e d |\n g16 fis e d cis d e fis g b a g |\n fis16 e d cis d a d e fis d fis a |\n \\new Staff \\relative c' {\n \\key g \\major \\clef bass\n g8 r \\clef treble b' r \\clef bass g,4 |\n fis8 r \\clef treble a' r \\clef bass fis,4 |\n e8 r \\clef treble g' r \\clef bass e,4 |\n d8 r \\clef treble fis' r \\clef bass d, c! |\n\u00bb\n&lt;/score&gt;\nThe Italian type of hand-crossing such as is frequently found in the sonatas of Scarlatti is employed here, with one hand constantly moving back and forth between high and low registers while the other hand stays in the middle of the keyboard, playing the fast passages.\nVariatio 6. Canone alla Seconda. a 1 Clav..\nThe sixth variation is a canon at the second: the follower starts a major second higher than the leader. The piece is based on a descending scale and is in time. Kirkpatrick describes this piece as having \"an almost nostalgic tenderness\". Each section has an alternate ending to be played on the first and second repeat.\nVariatio 7. a 1 \u00f4 vero 2 Clav. al tempo di Giga.\nThe variation is in meter, suggesting several possible Baroque dances. In 1974, when scholars discovered Bach's own copy of the first printing of the \"Goldberg Variations\", they noted that over this variation Bach had added the heading \"al tempo di Giga\". But the implications of this discovery for modern performance have turned out to be less clear than was at first assumed. In his book \"The Keyboard Music of J. S. Bach\" the scholar and keyboardist David Schulenberg notes that the discovery \"surprised twentieth-century commentators who supposed gigues were always fast and fleeting.\" However, \"despite the Italian terminology [\"giga\"], this is a [less fleet] French gigue.\" Indeed, he notes, the dotted rhythmic pattern of this variation (pictured) is very similar to that of the gigue from Bach's second French suite and the gigue of the \"French Overture\". This kind of gigue is known as a \"Canary\", based on the rhythm of a dance which originated from the Canary islands.\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \\relative c\" {\n \\key g \\major \\time 6/8\n b8. a16 b8 d,8. g16 b8 |\n a8.\\prall g16 a8 d4. |\n g8.\\mordent fis16 g8 a,8. e'16 g8 |\n fis8.\\prall e16 fis8 d4. |\n \\new Staff \\relative c' {\n \\key g \\major \\clef bass\n g4\\mordent g,8 g'4.~\\mordent |\n g4. fis8.\\prall e16 d8 |\n e4 d'8 cis4 a8 |\n d4.~ d8. e16 c8 |\n\u00bb\n&lt;/score&gt;\nHe concludes, \"It need not go quickly.\" Moreover, Schulenberg adds that the \"numerous short trills and appoggiaturas\" preclude too fast a tempo.\nThe pianist Angela Hewitt, in the liner notes to her 1999 Hyperion recording, argues that by adding the \"al tempo di giga\" notation, Bach was trying to caution against taking too slow a tempo, and thus turning the dance into a forlane or siciliano. She does however argue, like Schulenberg, that it is a French \"gigue\", not an Italian \"giga\" and does play it at an unhurried tempo.\nVariatio 8. a 2 Clav..\nThis is another two-part hand-crossing variation, in time. The French style of hand-crossing such as is found in the clavier works of Fran\u00e7ois Couperin is employed, with both hands playing at the same part of the keyboard, one above the other. This is relatively easy to perform on a two-manual harpsichord, but quite difficult to do on a piano.\nMost bars feature either a distinctive pattern of eleven sixteenth notes and a sixteenth rest, or ten sixteenth notes and a single eighth note. Large leaps in the melody occur. Both sections end with descending passages in thirty-second notes.\nVariatio 9. Canone alla Terza. a 1 Clav..\nThis is a canon at the third, in time. The supporting bass line is slightly more active than in the previous canons.\nVariatio 10. Fughetta. a 1 Clav..\nVariation 10 is a four-voice fughetta, with a four-bar subject heavily decorated with ornaments and somewhat reminiscent of the opening aria's melody.\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \u00ab\n \\new Voice \\relative c {\n \\key g \\major \\time 2/2 \\voiceOne\n R1*8 |\n g2\\mordent g4.\\prallprall fis16 g |\n a4 fis d fis |\n b,4 e e, d' |\n cis4\\prall b8 c a b c a |\n d4 d, d'2~ |\n d4 d, d'2~ |\n d2 cis |\n d1 | \\bar \":|.\"\n \\new Voice \\relative c\" {\n \\voiceTwo\n R1*10 |\n \\once \\override MultiMeasureRest.staff-position = -8 R1 |\n R1 |\n a2\\mordent a4.\\prallprall g16 a |\n b4 g e g |\n e4 a a, g' |\n fis4\\prall e8 fis d2 |\n \u00bb\n \\new Staff \u00ab\n \\new Voice \\relative c' {\n \\key g \\major \\clef bass \\voiceOne\n \\override MultiMeasureRest.staff-position = 4 R1*2 |\n \\override MultiMeasureRest.staff-position = 6 R1 |\n \\override MultiMeasureRest.staff-position = 4 R1 |\n d2\\mordent d4.\\prallprall c16 d |\n e4 c a c |\n a4 d d, c' |\n b4\\prall a8 b g b a c |\n b8 a b4 e2~ |\n e4 a, d2~ |\n d4 b e2~ |\n e4 d cis e |\n a,8 b a g fis2 |\n g2 r |\n r4 e a2~ |\n a8 a g a fis |\n \\new Voice \\relative c' {\n \\voiceTwo\n g2\\mordent g4.\\prallprall fis16 g |\n a4 fis d fis |\n e4 a a, g' |\n fis4\\prall e8 fis d e c d |\n b8 d e fis g4 b, |\n c2 r4 a |\n fis2 r4 d |\n g4 d' b d |\n g2 r4 e |\n fis2 r4 d |\n g2 r4 e |\n a2 r4 g |\n fis2 r4 d |\n g,4 b g e |\n a2 r4 a |\n d1 |\n \u00bb\n&lt;/score&gt;\nThe exposition takes up the whole first section of this variation (pictured). First the subject is stated in the bass, starting on the G below middle C. The answer (in the tenor) enters in bar 5, but it's a tonal answer, so some of the intervals are altered. The soprano voice enters in bar 9, but only keeps the first two bars of the subject intact, changing the rest. The final entry occurs in the alto in bar 13. There is no regular counter-subject in this fugue.\nThe second section develops using the same thematic material with slight changes. It resembles a counter-exposition: the voices enter one by one, all begin by stating the subject (sometimes a bit altered, like in the first section). The section begins with the subject heard once again, in the soprano voice, accompanied by an active bass line, making the bass part the only exception since it doesn't pronounce the subject until bar 25.\nVariatio 11. a 2 Clav..\nThis is a virtuosic two-part toccata in time. Specified for two manuals, it is largely made up of various scale passages, arpeggios and trills, and features much hand-crossing of different kinds.\nVariatio 12. a 1 Clav. Canone alla Quarta in moto contrario.\nThis is a canon at the fourth in time, of the inverted variety: the follower enters in the second bar in contrary motion to the leader.\nIn the first section, the left hand accompanies with a bass line written out in repeated quarter notes, in bars 1, 2, 3, 5, 6, and 7. This repeated note motif also appears in the first bar of the second section (bar 17, two Ds and a C), and, slightly altered, in bars 22 and 23. In the second section, Bach changes the mood slightly by introducing a few appoggiaturas (bars 19 and 20) and trills (bars 29\u201330).\nVariatio 13. a 2 Clav..\nThis variation is a slow, gentle and richly decorated sarabande in time. Most of the melody is written out using thirty-second notes, and ornamented with a few appoggiaturas (more frequent in the second section) and a few mordents. Throughout the piece, the melody is in one voice, and in bars 16 and 24 an interesting effect is produced by the use of an additional voice. Here are bars 15 and 16, the ending of the first section (bar 24 exhibits a similar pattern):\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \\relative c\" {\n \\key g \\major \\time 3/4\n cis32 g fis g a g fis g e' cis b cis d cis b cis g' e d e a g fis e |\n \u00ab { fis16 cis cis d d g, g fis fis4 } \\\\ { s4 r8 cis d4 } \u00bb | \\bar \":|.\"\n \\new Staff \u00ab\n \\new Voice \\relative c' {\n \\key g \\major \\clef bass \\voiceOne\n a8[ cis] g' e cis4 |\n d8[ e,] f bes a4 |\n \\new Voice \\relative c {\n \\voiceTwo\n a8 a'16. g32 a4. a8 |\n d,2. |\n \u00bb\n&lt;/score&gt;\nVariatio 14. a 2 Clav..\nThis is a rapid two-part hand-crossing toccata in time, with many trills and other ornamentation. It is specified for two manuals and features large jumps between registers. Both features (ornaments and leaps in the melody) are apparent from the first bar: the piece begins with a transition from the G two octaves below middle C, with a lower mordent, to the G two octaves above it with a trill with initial turn.\nBach uses a loose inversion motif between the first half and the second half of this variation, \"recycling\" rhythmic and melodic material, passing material that was in the right hand to the left hand, and loosely (selectively) inverting it.\nContrasting it with Variation 15, Glenn Gould described this variation as \"certainly one of the giddiest bits of neo-Scarlatti-ism imaginable.\"\nVariatio 15. Canone alla Quinta. a 1 Clav.: Andante.\nThis is a canon at the fifth in time. Like Variation 12, it is in contrary motion with the leader appearing inverted in the second bar. This is the first of the three variations in G minor, and its melancholic mood contrasts sharply with the playfulness of the previous variation. Pianist Angela Hewitt notes that there is \"a wonderful effect at the very end [of this variation]: the hands move away from each other, with the right suspended in mid-air on an open fifth. This gradual fade, leaving us in awe but ready for more, is a fitting end to the first half of the piece.\"\nGlenn Gould said of this variation, \"It's the most severe and rigorous and beautiful canon ... the most severe and beautiful that I know, the canon in inversion at the fifth. It's a piece so moving, so anguished\u2014and so uplifting at the same time\u2014that it would not be in any way out of place in the St. Matthew's Passion; matter of fact, I've always thought of Variation 15 as the perfect Good Friday spell.\"\nVariatio 16. Ouverture. a 1 Clav..\nThe entire set of variations can be seen as being divided into two halves, clearly marked by this grand French overture, commencing with particularly emphatic opening and closing chords. It consists of a slow prelude with dotted rhythms with a following fugue-like contrapuntal section. Here Bach follows his custom of beginning the second half of a major collection with a movement in French style, as with the earlier \"Clavier-\u00dcbung\" volumes, in both parts of the \"Well-Tempered Clavier\", in the \"Musical Offering\" (#4 of the numbered canons) and in the early version of the \"Art of Fugue\" (#7 of P 200).\nVariatio 17. a 2 Clav..\nThis variation is another two-part virtuosic toccata. Peter Williams sees echoes of Antonio Vivaldi and Domenico Scarlatti here. Specified for two manuals, the piece features hand-crossing. It is in time and usually played at a moderately fast tempo. Rosalyn Tureck is one of the very few performers who recorded slow interpretations of the piece. In making his 1981 re-recording of the \"Goldberg Variations\", Glenn Gould considered playing this variation at a slower tempo, in keeping with the tempo of the preceding variation (Variation 16), but ultimately decided not to because \"Variation 17 is one of those rather skittish, slightly empty-headed collections of scales and arpeggios which Bach indulged when he wasn't writing sober and proper things like fugues and canons, and it just seemed to me that there wasn't enough substance to it to warrant such a methodical, deliberate, Germanic tempo.\"\nVariatio 18. Canone alla Sesta. a 1 Clav..\nThis is a canon at the sixth in time. The canonic interplay in the upper voices features many suspensions. Commenting on the structure of the canons of the \"Goldberg Variations\", Glenn Gould cited this variation as the extreme example of \"deliberate duality of motivic emphasis ... the canonic voices are called upon to sustain the passacaille role which is capriciously abandoned by the bass.\" Nicholas Kenyon calls Variation 18 \"an imperious, totally confident movement which must be among the most supremely logical pieces of music ever written, with the strict imitation to the half-bar providing ideal impetus and a sense of climax.\"\nVariatio 19. a 1 Clav..\nThis is a dance-like three-part variation in time. The same sixteenth note figuration is continuously employed and variously exchanged between each of the three voices. This variation incorporates the rhythmic model of variation 13 (complementary exchange of quarter and sixteenth notes) with variations 1 and 2 (syncopations).\nVariatio 20. a 2 Clav..\nThis variation is a virtuosic two-part toccata in time. Specified for two manuals, it involves rapid hand-crossing. The piece consists mostly of variations on the texture introduced during its first eight bars, where one hand plays a string of eighth notes and the other accompanies by plucking sixteenth notes after each eighth note. To demonstrate this, here are the first two bars of the first section:\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \\relative c\" {\n \\key g \\major \\time 3/4\n r16 b r g r d r b r \\clef bass g fis e |\n fis8 a \\clef treble d fis a d |\n \\new Staff \\relative c' {\n \\key g \\major\n g8 b d g b cis |\n r16 d r a r fis r d r \\clef bass a g fis |\n\u00bb\n&lt;/score&gt;\nVariatio 21. Canone alla Settima.\nThe second of the three minor key variations, variation 21 has a tone that is somber or even tragic, which contrasts starkly with variation 20. The bass line here is one of the most eloquent found in the variations, to which Bach adds chromatic intervals that provide tonal shadings. This variation is a canon at the seventh in time; Kenneth Gilbert sees it as an allemande despite the lack of anacrusis. The bass line begins the piece with a low note, proceeds to a slow lament bass and only picks up the pace of the canonic voices in bar 3:\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \\relative c\" {\n \\key g \\minor\n r2 r16 a bes c d c bes a |\n g8 d' g, c~ c16 fis, g a bes a g fis |\n g8 r r g as g as a |\n \\new Staff \u00ab\n \\new Voice \\relative c' {\n \\key g \\minor \\clef bass \\voiceOne\n r16 bes c d es d c bes a8 es' a, d~ |\n d16 g, a bes c bes a g a8 r r a |\n bes8 a bes b c16 d es d c bes a g |\n \\new Voice \\relative c {\n \\voiceTwo\n g4 g' fis f |\n e4 es d16 a bes c d8 d, |\n g16 d' e fis g f e d c8 b c cis |\n \u00bb\n&lt;/score&gt;\nA similar pattern, only a bit more lively, occurs in the bass line in the beginning of the second section, which begins with the opening motif inverted.\nVariatio 22. a 1 Clav. alla breve.\nThis variation features four-part writing with many imitative passages and its development in all voices but the bass is much like that of a fugue. The only specified ornament is a trill which is performed on a whole note and which lasts for two bars (11 and 12).\nThe ground bass on which the entire set of variations is built is heard perhaps most explicitly in this variation (as well as in the Quodlibet) due to the simplicity of the bass voice.\nVariatio 23. a 2 Clav..\nAnother lively two-part virtuosic variation for two manuals, in time. It begins with the hands chasing one another, as it were: the melodic line, initiated in the left hand with a sharp striking of the G above middle C, and then sliding down from the B one octave above to the F, is offset by the right hand, imitating the left at the same pitch, but a quaver late, for the first three bars, ending with a small flourish in the fourth:\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \\relative c\" {\n \\key g \\major \\time 3/4\n r8 g b'16 a g fis e d cis b |\n a16 g fis8 a'16 g fis e d cis b a |\n g16 fis e8 g'16 fis e d cis b a g |\n fis16 e d8 r r16 e32 fis g a b cis d8 |\n \\new Staff \\relative c\" {\n \\key g \\major\n g8 b'16 a g fis e d cis b a g |\n fis8 a'16 g fis e d cis b a g fis |\n e8 g'16 fis e d cis b a g fis e |\n d8 \\clef bass r16 c!32 b a g fis e d8 r r16 c! |\n\u00bb\n&lt;/score&gt;\nThis pattern is repeated during bars 5\u20138, only with the left hand imitating the right one, and the scales are ascending, not descending. We then alternate between hands in short bursts written out in short note values until the last three bars of the first section. The second section starts with this similar alternation in short bursts again, then leads to a dramatic section of alternating thirds between hands. Williams, marvelling at the emotional range of the work, asks: \"Can this really be a variation of the same theme that lies behind the adagio no 25?\"\nVariatio 24. Canone all'Ottava. a 1 Clav..\nThis variation is a canon at the octave, in time. The leader is answered both an octave below and an octave above; it is the only canon of the variations in which the leader alternates between voices in the middle of a section. \nVariatio 25. a 2 Clav.: Adagio.\nVariation 25 is the third and last variation in G minor; it is marked adagio in Bach's own copy and is in time. The melody is written out predominantly in sixteenth and thirty-second notes, with many chromaticisms. This variation generally lasts longer than any other piece of the set.\nWanda Landowska famously described this variation as \"the black pearl\" of the \"Goldberg Variations\". Williams writes that \"the beauty and dark passion of this variation make it unquestionably the emotional high point of the work\", and Glenn Gould said that \"the appearance of this wistful, weary cantilena is a master-stroke of psychology.\" In an interview with Gould, Tim Page described this variation as having an \"extraordinary chromatic texture\"; Gould agreed: \"I don't think there's been a richer lode of enharmonic relationships any place between Gesualdo and Wagner.\"\nVariatio 26. a 2 Clav..\nIn sharp contrast with the introspective and passionate nature of the previous variation, this piece is another virtuosic two-part toccata, joyous and fast-paced. Underneath the rapid arabesques, this variation is basically a sarabande. Two time signatures are used, for the incessant melody written in sixteenth notes and for the accompaniment in quarter and eighth notes; during the last five bars, both hands play in .\nVariatio 27. Canone alla Nona. a 2 Clav..\nVariation 27 is the last canon of the piece, at the ninth and in time. This is the only canon where two manuals are specified not due to hand-crossing difficulties, and the only pure canon of the work, because it does not have a bass line.\nVariatio 28. a 2 Clav..\nThis variation is a two-part toccata in time that employs a great deal of hand crossing. Trills are written out using thirty-second notes and are present in most of the bars. The piece begins with a pattern in which each hand successively picks out a melodic line while also playing trills. Following this is a section with both hands playing in contrary motion in a melodic contour marked by sixteenth notes (bars 9\u201312). The end of the first section features trills again, in both hands now and mirroring one another:\n&lt;score&gt;\n\\new PianoStaff \u00ab\n \\new Staff \u00ab\n \\new Voice \\relative c' {\n \\key g \\major \\time 3/4 \\voiceOne\n \\set Score.currentBarNumber = 13\n fis16 r r8 g16 r r8 a16 r r8 |\n b16 r r8 a16 r r8 g16 r r8 |\n a16 r r8 b16 r r8 a16 r r8 |\n \\oneVoice fis16 d cis d \\clef bass a fis e fis d4 | \\bar \":|.\"\n \\new Voice \\relative c' {\n \\voiceTwo\n \\repeat unfold 8 { r32 d e d e d e d } r cis d cis d cis d cis |\n \u00bb\n \\new Staff \u00ab\n \\new Voice \\relative c {\n \\key g \\major \\clef bass \\voiceOne\n \\repeat unfold 8 { r32 fis g fis g fis g fis } r e fis e fis e fis e |\n \\new Voice \\relative c {\n \\voiceTwo\n d16 r r8 b16 r r8 a16 r r8 |\n g16 r r8 a16 r r8 b16 r r8 |\n a16 r r8 g16 r r8 a16 r r8 |\n \\oneVoice d,16 fis e fis a d cis d fis4 |\n \u00bb\n&lt;/score&gt;\nThe second section starts and closes with the contrary motion idea seen in bars 9\u201312. Most of the closing bars feature trills in one or both hands.\nVariatio 29. a 1 \u00f4 vero 2 Clav..\nThis variation consists mostly of heavy chords alternating with sections of brilliant arpeggios shared between the hands. It is in time. A rather grand variation, it adds an air of resolution after the lofty brilliance of the previous variation. Glenn Gould states that variations 28 and 29 present the only case of \"motivic collaboration or extension between successive variations.\"\nVariatio 30. a 1 Clav. Quodlibet.\nThe final variation is titled after the \"quodlibet\" tradition, in which multiple popular songs are played at once or in succession. According to Forkel, the many musicians of the Bach family practiced this tradition at gatherings:\nAs soon as they were assembled a chorale was first struck up. From this devout beginning they proceeded to jokes which were frequently in strong contrast. That is, they then sang popular songs partly of comic and also partly of indecent content, all mixed together on the spur of the moment. ... This kind of improvised harmonizing they called a Quodlibet, and not only could laugh over it quite whole-heartedly themselves, but also aroused just as hearty and irresistible laughter in all who heard them.\nThough Bach never noted the sources of Variation 30, Forkel's anecdote led to the belief that it is composed from German Volkslied melodies, as if to evoke the Bach gatherings.\nSince folk tunes commonly shared melodies, music alone does not identify the songs intended. For example, part of Variation 30 traces back to the melody of the Italian Bergamask dance, which not only gave rise to compositions by many musicians (such as Dieterich Buxtehude, under the title of \"La Capricciosa\", for his thirty-two partite in G major, BuxWV 250), but is even sung to various words in regions such as Iceland today.\nA handwritten note found in a collector's copy of the \"Clavier Ubung\" claims that Bach's student, Johann Christian Kittel, identified two folk tunes making up Variation 30 by their first lines. Siegfried Dehn of the Prussian royal library later appended purported full texts to this note:\nDehn's texts, though unsourced, stand as the only historical evidence for the provenance of Bach's Quodlibet and are commonly quoted. Today, the identity of \"Kraut und R\u00fcben...\" is uncontroversial, since multiple versions of the text, including some explicitly set to the Bergamask theme, are preserved. In contrast, the \"Ich bin solang...\" text is much more obscure, and these words have not been found in any Volkslied archives.\nOther bars of Variation 30 can be heard as incipits of yet more songs, though none have been identified.\nAria da Capo.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nA note-for-note repeat of the aria at the beginning. Williams writes that the work's \"elusive beauty ... is reinforced by this return to the Aria. ... no such return can have a neutral \"Affekt\". Its melody is made to stand out by what has gone on in the last five variations, and it is likely to appear wistful or nostalgic or subdued or resigned or sad, heard on its repeat as something coming to an end, the same notes but now final.\"\nCanons on the Goldberg ground, BWV 1087.\nWhen Bach's personal copy of the printed edition of the \"Goldberg Variations\" (see above) was discovered in 1974, it was found to include an appendix in the form of fourteen canons built on the first eight bass notes from the aria. It is speculated that the number 14 refers to the ordinal values of the letters in the composer's name: B(2) + A(1) + C(3) + H(8) = 14. Among those canons, the eleventh and the thirteenth are first versions of BWV 1077 and BWV 1076; the latter is included in the famous portrait of Bach painted by Elias Gottlob Haussmann in 1746.\nTranscribed and popularized versions.\nThe \"Goldberg Variations\" has been reworked freely by many performers, changing either the instrumentation, the notes, or both. The Italian composer Busoni prepared a greatly altered transcription for piano. According to the art critic Michael Kimmelman, \"Busoni shuffled the variations, skipping some, then added his own rather voluptuous coda to create a three-movement structure; each movement has a distinct, arcing shape, and the whole becomes a more tightly organized drama than the original.\" Other arrangements include:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources"}
{"id": "66341", "revid": "4637213", "url": "https://en.wikipedia.org/wiki?curid=66341", "title": "Kings", "text": "Kings or King's may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "66345", "revid": "46963284", "url": "https://en.wikipedia.org/wiki?curid=66345", "title": "Robin Givens", "text": "American stage, television, and film actress (born 1964)\nRobin Givens (born November 27, 1964) is an American actress and director. Givens played Darlene Merriman in the ABC sitcom \"Head of the Class\" in 1986, and remained on the series for its five year duration. Her troubled marriage to boxer Mike Tyson in 1988 drew considerable media attention, as did their acrimonious divorce. She later went on to become a spokesperson for the National Domestic Violence Hotline for several years.\nGivens continued her career with film and television roles such as \"The Women of Brewster Place\" (1989) and \"Boomerang\" (1992). In 1996, Givens co-starred on the sitcom \"Sparks\", which aired for two seasons on UPN. In January 2000, she took over hosting duties on the syndicated talk show \"Forgive or Forget\". In 2007, Givens released her autobiography, \"Grace Will Lead Me Home\". She has since had recurring roles on \"The Game\", \"Tyler Perry's House of Payne\", \"Chuck,\" \"Riverdale\" and its spin-off \"Katy Keene\", and has been part of the main cast of \"Batwoman\". In 2020, Givens began working as a television and film director.\nEarly life.\nGivens' mother raised Robin and her sister Stephanie in Mount Vernon and New Rochelle, New York. Givens was raised Catholic. She occasionally modeled and acted as a teen. As a model, she appeared in magazines such as \"Seventeen\" and \"Mademoiselle\". She made her film debut at age 14 in the film \"The Wiz\" (1978) as a guest at Aunt Emma's Party.\nGivens graduated from New Rochelle Academy (a private school which closed in June 1987). At the age of 15, she enrolled at Sarah Lawrence College as a pre-medical major, becoming one of the youngest to attend the school. While in school she acted in daytime dramas. She graduated at the age of 19 in 1984. Givens claimed to have dropped out of Harvard Medical School to focus on her acting career, but the registrar's office stated that she never applied.\nCareer.\n1980s\u20131990s.\nIn 1985, Givens auditioned for a guest spot on \"The Cosby Show\". She won the spot and Bill Cosby became her mentor. He persuaded her to drop out of school and promised that if she was not successful in two years, he would get her back into medical school and pay her tuition. Soon after Givens appeared in \"Diff'rent Strokes\" and the 1986 television film \"Beverly Hills Madam\", opposite Faye Dunaway. That same year, she landed her breakthrough role as rich girl Darlene Merriman on the ABC sitcom \"Head of the Class\". The series lasted five seasons, ending in 1991. In 1989, while starring in \"Head of the Class\", she appeared in \"The Women of Brewster Place\" with Oprah Winfrey. She later starred in the feature films \"A Rage in Harlem\" (1991) and \"Boomerang\" (1992).\nIn 1994, Givens posed nude for \"Playboy\" magazine. During that period Givens felt she had lost her voice, so one of the reasons why she posed for the magazine was so that she could write her own article. Givens was ranked No. 88 on \"Empire\" magazine's \"100 Sexiest Stars in Film History\" list in May 1995. In 1996, Givens portrayed Claudia in the television movie \"The Face\" (also known as \"A Face to Die For\") with Yasmine Bleeth. Later that year, she co-starred in the UPN sitcom \"Sparks\", which ended its run in 1998. She also played Denise in \"The Fresh Prince of Bel-Air\".\n2000s\u20132010s.\nIn January 2000, Givens appeared in a cameo in Toni Braxton's music video \"He Wasn't Man Enough\", as the wife of a cheating husband. She returned to the entertainment industry later that year as the host of the talk show \"Forgive or Forget\", replacing television personality Mother Love halfway through the show's second season. Ratings initially increased after Givens took over hosting duties, but soon fell. The series was canceled after this season.\nIn 2006, Givens attempted a return to television on MyNetworkTV's telenovela \"Saints and Sinners\", but the show garnered low ratings and was soon canceled. Givens continued acting in made-for-television films while also making appearances on Trinity Broadcasting Network's \"Praise the Lord\" program (July 12, 2007), and \"Larry King Live\". In June 2007, she released her autobiography \"Grace Will Lead Me Home\". Givens returned to feature films in Tyler Perry's Southern drama \"The Family That Preys\" (2008). She also had a recurring role portraying a fictionalized version of herself on the CW comedy-drama \"The Game\". Additionally, she has had a recurring role on the TBS show \"Tyler Perry's House of Payne\", and a guest role on USA Network's \"Burn Notice\". In addition to television and film roles, Givens has performed onstage. In 2001, she appeared in an off-Broadway production of \"The Vagina Monologues\". From February to April 16, 2006, she played the role of Roxie Hart in the Broadway play \"Chicago\". In 2007, she toured the country playing a part in the I'm Ready Productions play \"Men, Money &amp; Golddiggers\". Givens starred in the 2009 stage play \"A Mother's Prayer\", which also starred Johnny Gill, Shirley Murdock, and Jermaine Crawford.\nIn 2007, Givens published a memoir entitled \"Grace Will Lead Me Home\". In it, she reflects on the life of her praying grandmother, Grace, her experiences of domestic violence, her strong will to survive, feeling abandoned by her father, and her faith in God. In 2011, she guest-starred in three episodes of NBC's spy-comedy \"Chuck\": \"Chuck Versus the Masquerade\", \"Chuck Versus the A-Team\", and \"Chuck Versus the Muuurder\", as Jane Bentley. Later that year, she performed as Angel, a struggling blues singer, in the play \"Blues for An Alabama Sky\" at Pasadena Playhouse. In 2015, she starred alongside Clifton Powell, Mishon Ratliff, and Malachi Malik in the segment \"Mama's Boy\" of TV One's anthology romance horror film \"Fear Files\".\nGivens was the spokesperson for the National Domestic Violence Hotline for several years.\nIn 2017, Givens hosted the San Diego Black Film Festival as she had for the several previous years.\n2020s.\nIn 2020, Givens made her directorial debut with the Lifetime mystery thriller film, \"A Murder to Remember\". She later directed films \"Favorite Son\" (2021) and its sequel, \"Favorite Son Christmas\" (2023), \"A Jenkins Family Christmas\" (2021) and \"The Christmas Clapback\" (2022), all for BET+. In 2021, she directed the horror films \"Haunted Trail\" and \"Horror Noire\". In 2023 she directed the comedy film, \"The Nana Project\".\nIn 2021, Givens was cast in season three of \"Batwoman\" as Jada Jet, the CEO of Jeturian Industries and Ryan Wilder's biological mother who is based on Jezebel Jet.\nIn 2022, Givens appeared in the Lifetime film \"He's Not Worth Dying For\" as part of its \"Ripped from the Headlines\" feature films that was inspired by the feud of Rachel Wade and Sarah Ludemann. She portrayed Cher Heinemann, the mother of Grace Heinemann who was based on Ludemann.\nGivens directed the episodes of television series \"Riverdale\", \"Dynasty\", \"Nancy Drew\", \"So Help Me Todd\" and \"Elsbeth\".\nPersonal life.\nGivens began dating boxer Mike Tyson in 1987. According to Givens, Tyson was physically abusive before they wed on February 7, 1988. Tyson stated that he was \"severely traumatized by that relationship.\" Tyson was then estimated to be worth $50 million; he and Givens did not have a prenuptial agreement. During their marriage, Givens bought a $4.3 million mansion in the affluent suburb of Bernardsville, New Jersey with money withdrawn from Tyson's brokerage account. They appeared in a Diet Pepsi commercial together and on the cover of \"Life\" magazine. \nAfter her miscarriage in June 1988, their marriage began to fall apart. Tyson claims Givens' pregnancy and miscarriage were part of a ruse to rush him into marriage, claiming that in all the time she was supposedly pregnant, Givens never gained a pound. In a joint interview with Tyson on \"20/20\" in September 1988, Givens told Barbara Walters that life with him was \"torture, pure hell, worse than anything I could possibly imagine,\" and she went on to describe his volatile temper. In October 1988, Givens filed for divorce, citing spousal abuse and was granted a temporary restraining order. Her attorney Marvin Mitchelson said, \"She loves Michael Tyson, but there is continued violence, and she fears for her safety.\" Tyson sought an annulment, accusing her of stealing millions of dollars and manipulating the public. Givens responded by filing a $125 million libel suit for defamation. Their divorce was finalized on Valentine's Day in 1989.\nGivens received negative press following her split from Tyson, particularly within the sports and African-American communities. Headlines heralded her as \"the Most Hated Woman in America\" and she was described as a \"gold digger who married Tyson solely for his millions.\" Givens denied that she received a reported divorce settlement of over $10 million from Tyson, stating that she \"didn't receive one dime.\"\nAccording to the 1989 biography \"Fire and Fear: The Inside Story of Mike Tyson\", Tyson admitted he punched Givens, stating \"that was the best punch I've ever thrown in my entire life.\" Tyson later claimed the book was \"filled with inaccuracies.\" In 2009, Tyson joked about \"socking\" Givens on \"Oprah\", which caused laughter in the audience. Winfrey later issued an apology to Givens.\nIn 1993, Givens adopted her first son, Michael \"Buddy\" Givens. In 1997, she married her tennis instructor, Svetozar Marinkovi\u0107. Givens filed for divorce months later. In 1999, she had a biological son, William \"Billy\" Jensen, with ex-boyfriend, tennis player Murphy Jensen.\nIn January 2004, Givens struck a pedestrian while driving an SUV through a Miami, Florida, intersection. Givens was ticketed for failing to use due care with a pedestrian in a crosswalk, but the charges were later dismissed. In June 2004, the injured party filed a civil lawsuit against Givens for an unspecified amount.\nA May 7, 2009, article in \"Forbes\" magazine reported that the Internal Revenue Service was suing Givens for unpaid federal income taxes totaling $292,000 ($ in present-day USD when adjusted for inflation), an amount which included interest and penalties. The government had asked a federal court in Florida for a judgment against her on 39 assessments covering a span of eight years.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66346", "revid": "2003421", "url": "https://en.wikipedia.org/wiki?curid=66346", "title": "The Stardroppers", "text": "1963 novel by John Brunner\nThe Stardroppers is a science fiction novel by British writer John Brunner. It was originally published at novella-length in 1962 as Listen, the Stars, in \"Analog\" and then as part of an Ace Double in 1963; in 1972 the revised, novel-length form was published by DAW Books.\nPlot introduction.\n\"The Stardroppers\" is about an undercover United Nations agent investigating a new fad, \"stardropping\", whereby physics-violating equipment is used to listen to sounds believed to be alien or paranormal signals. Superficially a harmless but expensive hobby, stardropping attracts a fanaticism resembling addiction, where some users assemble in semi-social communes and spend all of their money on increasingly improved equipment. The fad gains an additional aspect of risk when users begin disappearing into thin air, in cases of increasing profile and witnessing.\nReception.\nLester del Rey characterized the novel as \"a good adventure story [but] not of one Brunner's stronger literary efforts.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66348", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=66348", "title": "Paul Pimsleur", "text": "American linguist (1927\u20131979)\nPaul Pimsleur (October 17, 1927 \u2013 June 22, 1976) was a French-American linguist and scholar in the field of applied linguistics. He developed the Pimsleur language learning system, which, along with his many publications, had a significant effect upon theories of language learning and teaching. Pimsleur Language Programs is an American language learning company that develops and publishes courses based on the Pimsleur Method.\nEarly life and education.\nPaul M. Pimsleur was born in New York City and grew up in The Bronx. His father, Solomon Pimsleur, was a Jewish immigrant from France and a composer of music; his American-born mother, Miera, was a librarian at Columbia University. Pimsleur earned a bachelor's degree at the City College of New York, and from Columbia University he earned a master's degree in psychological statistics and a PhD in French.\nCareer.\nHis first position involved teaching French phonetics and phonemics at the University of California, Los Angeles. After leaving UCLA, Pimsleur went on to faculty positions at the Ohio State University, where he taught French and foreign language education. At the time, the foreign language education program at OSU was the major doctoral program in that field in the U.S. While at Ohio State he created and directed the Listening Center, one of the largest language laboratories in the United States. The center was developed in conjunction with Ohio Bell Telephone and allowed self-paced language study using a series of automated tapes and prompts that were delivered over the telephone.\nLater, Pimsleur was a professor of education and Romance languages at The State University of New York at Albany, where he held dual professorships in education and French. He was a Fulbright lecturer at the Ruprecht Karls University of Heidelberg in 1968 and 1969 and a founding member of the \"American Council on the Teaching of Foreign Languages\" (ACTFL). He did research on the psychology of language learning and in 1969 was section head of psychology of second languages learning at the International Congress of Applied Linguistics.\nHis research focused on understanding the language acquisition process, especially the learning process of children, who speak a language without knowing its formal structure. The term \"organic learning\" was applied to that phenomenon. For this, he studied the learning process of groups made of children, adults, and multilingual adults. The result of this research was the Pimsleur Language Programs. His many books and articles affected theories of language learning and teaching.\nIn the period from 1958 to 1966, Pimsleur reviewed previously published studies regarding linguistic and psychological factors involved in language learning. He also conducted several studies with support from Ohio State or from the US Office of Education. This led to the publication in 1963 of a coauthored monograph, \"Underachievement in Foreign Language Learning\", which was published in the International Review of Applied Linguistics.\nThrough this research, he identified three factors that could be used to calculate language learning aptitude: verbal intelligence, auditory ability, and motivation. Pimsleur was the primary author of the Pimsleur Language Aptitude Battery (PLAB) based on these three factors to assess language aptitude. He concluded that low auditory ability was a major factor in underachievement. Pimsleur was one of the first foreign language educators to show an interest in students who have difficulty in learning a foreign language while doing well in other subjects. Today, the PLAB is used to determine foreign language-learning aptitude, or even a foreign language-learning disability, among secondary-school students.\nDeath.\nPimsleur died unexpectedly of a heart attack during a visit to France in 1976.\nLegacy.\nSince its creation in 1977, \"The ACTFL-MLJ Paul Pimsleur Award for Research in Foreign Language Education\", which is awarded annually, bears his name.\nPimsleur's business partner, Charles Heinle, continued to develop the Pimsleur courses until he sold the company to Simon &amp; Schuster Audio in 1997.\nIn 2006, Pimsleur's daughter, Julia Pimsleur, created the Entertainment Immersion Method\u00ae inspired by the Pimsleur Method, which is the foundation of the http:// language teaching program for young children, sold in the U.S. and 22 countries.\nIn 2013, Simon &amp; Schuster reissued Dr. Paul Pimsleur's out-of-print book \"How to Learn a Foreign Language\" in hardcover and eBook format to celebrate the 50th anniversary of Paul Pimsleur's first course.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66350", "revid": "1175300", "url": "https://en.wikipedia.org/wiki?curid=66350", "title": "Ami Dolenz", "text": "American actress\nAmi Bluebell Dolenz ( ; born January 8, 1969) is an American-Canadian retired actress.\nEarly life.\nDolenz was born in Burbank, California, into a show business family. She is the daughter of Micky Dolenz of the 1960s group The Monkees, and British television presenter Samantha Juste. Her paternal grandparents were the film actors George Dolenz and Janelle Johnson.\nCareer.\nAt age 15, Dolenz won a junior talent contest and decided to become an actress. She dropped out of high school and began appearing in roles on various television series. One of her first acting roles was in the television movie \"The Children of Times Square\", followed by a two-episode stint on \"Growing Pains\". In 1987, she had a small role in the comedy \"Can't Buy Me Love\". Later that year, she landed the role of Melissa McKee in the long-running soap opera \"General Hospital\"; the role garnered critical attention for Dolenz and she earned two nominations (in 1988 and 1989, respectively) for a Young Artist Award.\nAfter leaving \"General Hospital\" in 1989, Dolenz landed a co-starring role opposite Tony Danza in \"She's Out of Control\". The following year, she portrayed Sloane Peterson in \"Ferris Bueller\", a television sitcom based on \"Ferris Bueller's Day Off\", which lasted only thirteen episodes and was cancelled in 1991. After its demise, Dolenz starred in \"Children of the Night\" and then had the lead role in 1992's \"Miracle Beach\".\nThroughout the 1990s, Dolenz continued to appear in films and television, including '; '; \"Murder, She Wrote\"; \"Wake, Rattle and Roll\"; \"\"; \"Demolition University\"; \"Pacific Blue\"; and \"Teen Angel\". In 1998, she voiced a character for the children's show \"The Secret Files of the Spy Dogs\". After a four-year hiatus from acting, Dolenz returned in the independent film \"Mr. Id\", in a co-starring role with Steve Parlavecchio. In 2007, she appeared in the film \"Even If\", which she also produced.\nPersonal life.\nOn August 10, 2002, Dolenz married actor and martial artist Jerry Trimble.\nIn addition to acting, Dolenz and her husband manage KidPix Productions, a company that stages movie shoots as birthday parties for children. She also performs with the Write Act Repertory Theatre, and owns Bluebell Boutique, an online custom jewelry shop that she previously co-owned and operated with her late mother.\nBoth Dolenz and Trimble currently reside in Vancouver, British Columbia, Canada; they became Canadian citizens on September 14, 2021.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66359", "revid": "8570870", "url": "https://en.wikipedia.org/wiki?curid=66359", "title": "Coati Mundi", "text": "Coati Mundi may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "66363", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=66363", "title": "Jewelweed plant", "text": ""}
{"id": "66364", "revid": "20460657", "url": "https://en.wikipedia.org/wiki?curid=66364", "title": "Moggy", "text": "Informal name for a non-pedigree cat\nA moggy is any cat which has not been intentionally bred. Moggies lack a consistent appearance unlike purebred cats that are selectively bred for appearance conforming to a standard. In contexts where cats need to be registered\u2014such as in veterinary practices or shelters\u2014moggies may be called domestic short-haired (DSH) or domestic long-haired (DLH) cats, depending on coat length (and less common designations may include \"domestic medium-haired (DMH)\" or \"domestic semi-long-haired\").\nThe vast majority of cats worldwide lack any pedigree ancestry.\nHistory.\nCat fancying is relatively new and over 85% of cat breeds have come into existence since the 1930s.\nDemography.\nIn the United States, domestic short-haired cats make up 95% of the cat population. In the UK 89\u201392% of cats are of non-pedigree lineage.\nDomestic short-haired.\nIn the cat fancy, and among veterinarians and animal control agencies, domestic short-haired cats may be classified with organization-specific terminology (often capitalized), such as: \"Domestic Shorthair\" (DSH); \"House Cat, Shorthair\" (HCS); or \"Shorthair Household Pet\".\nSuch a pseudo-breed is used for registry as well as shelter/rescue classification purposes. While not bred as show cats, some domestic short-haired cats are actually pedigreed (have a recorded genealogy) and are entered into cat shows that have non-purebred \"Household Pet\" divisions. Show rules vary; the F\u00e9d\u00e9ration Internationale F\u00e9line (FIFe) permits \"any eye colour, all coat colours and patterns, any coat length or texture, and any length of tail\" (basically, any cat). Others may be more restrictive; an example from the World Cat Federation: \"All classic colours are permitted. Any amount of white is permitted. The colours chocolate and cinnamon, as well as their dilution (lilac and fawn) are not recognized in any combinations (bicolour, tricolour, tabby). The pointed pattern is also not recognized.\"\nDomestic short-haired cats are characterised by a wide range of colouring, and typically \"revert to type\" after a few generations, which means they express their coats as a natural tabby pattern. This can be any colour or combination of colours. They also exhibit a wide range of physical characteristics; domestic short-haired cats in different countries tend to look different in body shape and size, as they developed from differing gene pools. DSH cats in Asia tend to have a build similar to a \"classic\" Siamese or Tonkinese, while European and American varieties have a thicker, heavier build.\nDomestic long-haired.\nA domestic long-haired cat is a cat of mixed ancestry\u00a0\u2013 thus not belonging to any particular recognised cat breed\u00a0\u2013 possessing a coat of semi-long to long fur. Domestic long-haired cats should not be confused with the British Longhair, American Longhair, or other breeds with \"Longhair\" names, which are standardised breeds defined by various registries. Other generic terms are in British English, \"moggie\" and in American English \"alley cat\". Domestic long-haired cats are the third most common type of cat in the United States.\nIn the cat fancy, and among veterinarians and animal control agencies, domestic long-haired cats may be classified with organisation-specific terminology (often capitalised), such as \"Domestic Longhair\" (DLH); \"House Cat, Longhair\" (HCL); or \"Semi-Longhair Household Pet\". Such a pseudo-breed is used for registry and shelter/rescue classification purposes, and breeds such as the Persian cat. While not bred as show cats, some mixed-breed cats are actually pedigreed and entered into cat shows that have non-purebred \"Household Pet\" divisions. Show rules vary; the F\u00e9d\u00e9ration Internationale F\u00e9line permits \"any eye colour, all coat colours and patterns, any coat length or texture, and any length of tail\" (basically any healthy cat). Others may be more restrictive; an example from the World Cat Federation: \"The colours chocolate and cinnamon, as well as their dilution (lilac and fawn) are not recognized in any combinations ...[and] the pointed pattern is also not recognized\".\nDomestic long-haireds come in all genetically possible cat colours including tabby, tortoiseshell, bicolour cat, and smoke. Domestic long-haireds can have fur that is up to six inches long. They can also have a mane similar to a Maine Coon's, as well as toe tufts and ear tufts. Some long-haired cats are not able to maintain their own coat, which must be frequently groomed by a human or may be prone to matting. Because of their wide gene pool, domestic long-haireds are not predisposed to any genetically inherited problems.\nHistory.\nHaving apparently originated in Western Asia, domestic long-haired cats have been kept as pets around the world for several centuries. During the 16th century, the first long-haired cats were imported into Europe. In the mid-17th century, when the Great Plague of London decimated much of London's human population, the number of cats started to recover after centuries of persecution, as they were encouraged as protectors from flea-carrying rats.\nHow the variant developed is still a matter of speculation. The long coat may have been the result of a recessive mutant gene. When a long-haired cat is mated to one with a short coat, only short-haired kittens can result; however, their offspring, when mated, can produce a proportion of long-coated kittens. Successive litters of early European long-haired cats produced more and more long-coated offspring, which were more likely to survive in the cooler European climates. By the year 1521, around the time they were first documented in Italy, the variety had become fixed after only a few generations.\nIn the late 18th century, Peter Simon Pallas advanced the hypothesis that the manul (also known as Pallas's cat) might be the ancestor of the long-haired domestic cat. He had anecdotal evidence that established, even though the male offspring would be sterile hybrids, the female offspring could again reproduce with domestic cats and pass on a small proportion of the manul's genes. In 1907, zoologist Reginald Innes Pocock refuted this claim, citing his work on the skull differences between the manul and the Angoras or Persians of his time. This early hypothesis overlooked the potential for crossbreeding within the family Felidae. For example, the Savannah cat is a crossbreed between a domestic short-haired cat and a wild serval\u2014both of which have different skulls and evolutionary lineage. Furthermore, hybrid females in the related genus \"Panthera\", such as ligers and tigons, have successfully mated, producing tiligers and litigons.\nThe first modern, formal breeds of long-haired cats were the Persian and the Angora (named after Ankara, Turkey) and were said to have come from those two areas.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66365", "revid": "609725", "url": "https://en.wikipedia.org/wiki?curid=66365", "title": "Kitten", "text": "Juvenile cat\nA kitten is a juvenile cat. After being born, kittens display primary altriciality and are fully dependent on their mothers for survival. They normally do not open their eyes for seven to ten days. After about two weeks, kittens develop quickly and begin to explore the world beyond their nest. After a further three to four weeks, they begin to eat solid food and grow baby teeth. Domestic kittens are highly social animals and usually enjoy human companionship.\nEtymology.\nThe word \"kitten\" derives from the Middle English word , which in turn came from the Old French or . Juvenile big cats are called \"cubs\" rather than kittens. For the young of smaller wild felids, such as ocelots, caracals, and lynxes, either term may be used, though \"kitten\" is more common.\nDevelopment.\nA feline litter usually consists of two to five kittens, but litters with one to more than ten are known. Kittens are typically born after a gestation lasting between 64 and 67 days, with an average length of 66 days. When they are born, kittens emerge in a sac called the amnion, which is bitten off and eaten by the mother cat.\nFor the first several weeks, kittens cannot urinate or defecate without being stimulated by their mother. They also cannot regulate their body temperature for the first three weeks, so kittens born in temperatures less than can die from hypothermia if their mother does not keep them warm. The mother's milk is very important for the kittens' nutrition and proper growth. This milk transfers antibodies to the kittens, which helps protect them against infectious diseases. As mentioned above, they cannot urinate, so they have a very high requirement for fluids. Kittens open their eyes about seven to ten days after birth. At first, the retina is poorly developed and vision is poor. Kittens cannot see as well as adult cats until about ten weeks after birth.\nKittens develop very quickly from about two weeks of age until their seventh week. Their coordination and strength improve, and they play-fight with their littermates and begin to explore the world outside the nest or den. They learn to wash themselves and others as well as play hunting and stalking games, showing their inborn ability as predators; the kittens' mother or other adult cats help develop these innate skills by bringing live prey to the nest. Later, the mother demonstrates hunting techniques for the kittens to emulate. As they reach three to four weeks old, the kittens are gradually weaned and begin to eat solid food, with weaning usually complete by six to eight weeks. Kittens generally begin to lose their baby teeth around three months of age, and they have a complete set of adult teeth by nine months. Kittens live primarily on solid food after weaning, but they usually continue to suckle from time to time until separated from their mothers. Some mother cats will scatter their kittens as early as three months of age, while others continue to look after them until they approach sexual maturity.\nThe sex of kittens is usually easy to determine at birth. By six to eight weeks, this becomes harder because of the growth of fur in the genital region. The male's urethral opening is round, whereas the female's urethral opening is a slit. Another marked difference is the distance between anus and urethral opening, which is greater in males than in females.\nKittens are very social and spend most of their waking hours interacting with other animals and playing on their own. Play with other kittens peaks in the third or fourth month after birth, with more solitary hunting and stalking play peaking later, at about five months.\nKittens are vulnerable because they like to find dark places to hide, sometimes with fatal results if they are not watched carefully. Cats have a habit of seeking refuge under or inside cars or on top of car tires during stormy or cold weather; this often leads to broken bones, burns, heat stroke, damaged internal organs or death.\nDomestic kittens are commonly sent to new homes at six to eight weeks of age, but it has been suggested that being with their mother and littermates from six to twelve weeks is important for a kitten's social and behavioural development. Usually, breeders and foster/rescue homes will not sell or adopt out a kitten that is younger than twelve weeks. In many jurisdictions, it is illegal to give away kittens younger than eight weeks of age. Kittens generally reach sexual maturity at around seven months, and full \"adulthood\" around one year of age.\nHealth.\nDomestic kittens in developed societies are usually vaccinated against common illnesses from two to three months of age. The usual combination vaccination protects against feline viral rhinotracheitis (FVR), feline calicivirus (C), and feline panleukopenia (P). This FVRCP inoculation is usually given at eight, twelve, and sixteen weeks, and an inoculation against rabies may be given at sixteen weeks. Kittens are usually spayed or neutered at seven months of age, but kittens may be neutered as young as seven weeks (if large enough), especially in animal shelters. Such early neutering does not appear to have any long-term health risks for cats, and it may even be beneficial in male cats. Kittens are commonly given deworming treatments for roundworms from about four weeks.\nNutrition.\nFelines are carnivores and have adapted to animal-based diets and low carbohydrate inclusion. Kittens belong to the growth life stage and have high energy and protein requirements. When feeding a kitten, it is often recommended to use highly digestible ingredients and a variety of components to support development and ensure a healthy adult cat. In North America, diets certified by the Association of American Feed Control Officials (AAFCO) are accepted as adequate nutrition; therefore, kitten diets should be AAFCO-approved to guarantee complete supplementation. Key components of the diet are high fat content to meet caloric requirements of growth, high protein to promote muscle growth, and supplementation of certain nutrients such as docosahexaenoic acid to support brain development and the optimization of cognition.\nPre-weaning nutrition.\nEstablishing immunity.\nPart of the kitten's immune system is the mucosal immune system, which is within the gastrointestinal tract. The mucosal immune system is largely responsible for coordinating proper immune responses by tolerating innocuous antigens and attacking foreign pathogens. In order to optimize kitten health and increase chances of survival, it is important to optimize the link between the gut-associated lymphoid tissue and the microbiota of the gastrointestinal tract. Lasting health and longevity can be promoted partly through proper nutrition, as well as by establishing a healthy gut from birth through the use of colostrum.\nWithin the first two days after birth, kittens acquire passive immunity from their mother's milk. Milk within the first few days of parturition is called colostrum and contains high concentrations of immunoglobulins. These include immunoglobulin A and immunoglobulin G, which cross the neonatal intestinal barrier. The immunoglobulins and growth factors found in the colostrum begin to establish and strengthen the weak immune system of the offspring. Kittens are able to chew solid food around 5\u20136 weeks after birth, and it is recommended that 30% of their diet consist of solid food at this time. The kitten remains on the mother's milk until around eight weeks of age, when weaning is complete and solid food becomes the primary food source.\nPost-weaning nutrition.\nFat.\nUntil approximately one year of age, kittens are in a growth phase during which energy requirements can be up to 2.5 times higher than maintenance. Pet nutritionists often suggest offering a commercial cat food designed specifically for kittens starting at four weeks of age. Fat has a higher caloric value than carbohydrates and protein, supplying 9 kcal/g. The growing kitten requires arachidonic and linoleic acid, which can be provided in omega\u22123 fatty acids. Docosahexaenoic acid (DHA) is another vital nutrient that can be supplied through omega\u22123 fatty acids. Addition of DHA to the diet benefits the cognition, brain, and visual development of kittens.\nProtein.\nCats are natural carnivores and require high amounts of protein in the diet. Since kittens are in a growth phase, they need substantial levels of protein to supply essential amino acids for the development of tissues and muscles. It is recommended that kittens consume a diet containing approximately 30% protein, on a dry matter basis, for proper growth.\nTaurine is an essential amino acid found only in animal tissue; the mother cat cannot produce enough of it for her kittens. As it is an indispensable amino acid, it must be provided exogenously through the diet at 10\u00a0mg per kg of bodyweight, each day. Taurine deficiency can lead to poor growth in kittens, and it can cause retinal degeneration in cats.\nCarbohydrates.\nFelines are natural carnivores and do not intentionally consume large quantities of carbohydrates. The domestic cat's liver has adapted to the lack of carbohydrates in the diet by using amino acids to produce glucose, which fuels the brain and other tissues. Studies have shown that carbohydrate digestion in young kittens is much less effective than that of a mature feline with a developed gastrointestinal tract. Highly digestible carbohydrates found in commercial kitten food provide additional energy and fiber to stimulate the immature gut tissue. Soluble fiber such as beet pulp is commonly used as a stool hardener and has been shown to strengthen intestinal muscles and thicken the gut mucosal layer, helping to prevent diarrhea.\nDiet composition.\nAmino acids.\nThe lack of readily available glucose from the limited carbohydrates in the diet has resulted in the liver adapting to produce glucose from the breakdown components of protein \u2014 amino acids. The enzymes that break down amino acids are constantly active in cats. Thus, cats need a constant source of protein in their diet. Because they are constantly growing, kittens require an increased amount of protein to supply readily available amino acids for daily maintenance and for building new body components. There are many required amino acids for kittens. A dietary level of approximately 0.3% histidine is necessary for kittens since histidine-free diets cause weight loss. Tryptophan is required at about 0.15% as this level maximizes performance in kittens. Kittens also need the following amino acids supplemented in their diet: arginine to avoid an excess of ammonia in the blood \u2014 otherwise known as hyperammonemia, isoleucine, leucine, valine, lysine, methionine as a sulfur-containing amino acid, asparagine for maximal growth in the early post-weaning kitten, threonine, and taurine to prevent central retinal degeneration.\nVitamins.\n\"Fat-soluble vitamins\".\nVitamin A is required in kitten diets because cats cannot convert carotenes to retinol in the intestinal mucosa due to a lack of the necessary enzyme; therefore, this vitamin must be supplied in the diet. Vitamin E is another required vitamin in kitten diets; deficiency leads to steatitis, causing the depot fat to become firm and yellow-orange in colour, which is painful and leads to death. Also, vitamin D is essential because cats cannot convert it from precursors in the skin.\n\"Water-soluble vitamins\".\nCats can synthesize niacin, but its breakdown exceeds the rate at which it can be synthesized, so they have a higher requirement for it, which can be fulfilled through an animal-based diet. Pyridoxine (vitamin B6) is required in increased amounts because it is essential for amino acid metabolism. Vitamin B12 is an AAFCO-recommended vitamin that is essential for the metabolism of carbohydrates and protein, supports the nervous system and mucous membranes, contributes to muscle and heart function, and promotes normal growth and development. Choline is also an AAFCO-recommended ingredient for kittens; it is important for neurotransmission in the brain and serves as a component of membrane phospholipids. Biotin (vitamin B7) is another AAFCO-recommended vitamin that supports the thyroid and adrenal glands as well as the reproductive and nervous systems. Kittens also require riboflavin (vitamin B2) for heart health, pantothenic acid (vitamin B5), and folacin (vitamin B9).\nMetabolism aids.\nSince kitten diets are very high in calories, ingredients must be implemented to ensure adequate digestion and utilization of these calories. Choline chloride is an ingredient that maintains fat metabolism. Biotin and niacin are also active in the metabolism of fats, carbs, and protein. Riboflavin is also necessary for the digestion of fats and carbohydrates. These are the main metabolic aids incorporated into kitten diets to maximize nutrient utilization.\nGrowth and development.\nA combination of essential nutrients is used to satisfy the overall growth and development of the kitten's body. Nonetheless, many ingredients that kittens do not require are also included in diet formulations to promote healthy growth and development; these ingredients include: dried egg as a source of high-quality protein and fatty acids, flax seeds \u2014 which are rich in omega\u22123 fatty acids and aid in digestion, calcium carbonate as a source of calcium, and calcium pantothenate (vitamin B5) \u2014 which acts as a coenzyme in the conversion of amino acids and is important for healthy skin.\nImmunity boosters.\nAntioxidants help support the development of a healthy immune system by preventing the oxidation of essential molecules in a growing kitten. Antioxidants in kitten diets can be derived from plant ingredients such as carrots, sweet potatoes, and spinach, as well as from dietary supplements like vitamin E and zinc proteinate.\nOrphaned kittens.\nKittens require a high-calorie diet that contains more protein than the diet of adult cats. Young orphaned kittens require cat milk every two to four hours, and they need physical stimulation to defecate and urinate. Cat milk replacer is manufactured to feed young kittens because cow's milk does not provide all the necessary nutrients. Human-reared kittens tend to be very affectionate with humans as adults and sometimes more dependent on them than kittens reared by their mothers, but they can also show volatile mood swings and aggression.\nDepending on the age at which they were orphaned and how long they were without their mothers, these kittens may be severely underweight and can have health problems later in life, such as heart conditions. The compromised immune system of orphaned kittens \u2014 due to the absence of antibodies normally found in the mother's milk \u2014 can make them especially susceptible to infections, often necessitating antibiotics.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66366", "revid": "15450995", "url": "https://en.wikipedia.org/wiki?curid=66366", "title": "Taiwan Relations Act", "text": "United States legislation\nThe Taiwan Relations Act (TRA; Pub. L.\u00a0, , 93\u00a0Stat.\u00a0https://, enacted ) is an act of the United States Congress. Since the formal recognition of the People's Republic of China, the Act has defined the officially substantial but non-diplomatic relations between the United States of America and Taiwan.\nBackground.\nAt The Third Plenum in 1978, Deng Xiaoping became the paramount leader of the People's Republic of China (PRC), definitively ending Maoist rule and beginning the reform era of Chinese history. During his speech at the plenum, he outlined a new Chinese foreign policy, whereby the Soviet Union\u2014not the United States, as in the past\u2014was identified as the main national security threat to China. During this time, China regarded itself as in a \"united front\" with the U.S., Japan, and western Europe against the Soviets. and thus established relations with the United States, China also supported American Operation Cyclone actions in Communist Afghanistan and leveled a military expedition against Vietnam, America's main antagonist in Southeast Asia. In exchange, the United States abrogated its mutual defense treaty (SAMDT) with the Republic of China (ROC).\nThe ROC government mobilized the China Lobby in the United States to lobby Congress for the swift passage of an American security guarantee for the island. Taiwan could appeal to members of Congress on many fronts: anti-communist China sentiment, a shared wartime history with the ROC, Beijing's human rights violations (despite committing violations of its own) and its curtailment of religious freedoms.\nSenator Barry Goldwater and other members of the United States Congress challenged the right of President Jimmy Carter to cancel SAMDT unilaterally, which the US had signed with the ROC in December 1954 and was ratified by the U.S. Senate in February 1955. Goldwater and his co-filers of the US Supreme Court case \"Goldwater v. Carter\" argued that the President required Senate approval to take such an action of termination, under of the U.S. Constitution, and that by not doing so, President Carter had acted beyond the powers of his office. The case ultimately was dismissed as non-justiciable and left open the constitutional question regarding a president's authority to dismiss a treaty unilaterally.\nThe Act was passed by both chambers of Congress and signed by President Carter in 1979 after the breaking of relations between the US and the ROC. Congress rejected the U.S. State Department's proposed draft and replaced it with language that has remained in effect since 1979. The TRA is intended to maintain commercial, cultural, and other relations through the unofficial relations in the form of a nonprofit corporation incorporated under the laws of the District of Columbia, the American Institute in Taiwan (AIT), without official government representation or formal diplomatic relations. The Act entered retroactively into force, effective January 1, 1979.\nProvisions.\nDefinition of Taiwan.\nThe act does not recognize the terminology of \"Republic of China\" after 1 January 1979, but uses the terminology of \"governing authorities on Taiwan\". Geographically speaking and following the similar content in the earlier defense treaty from 1955, it defines the term \"Taiwan\" to include, as the context may require, the island of Taiwan (the main Island) and the Pescadores (Penghu). Of the other islands or archipelagos under the control of the Republic of China, Kinmen, the Matsus, etc., are left outside the definition of Taiwan.\n\"De facto\" diplomatic relations.\nThe act authorizes \"de facto\" diplomatic relations with the governing authorities by giving special powers to the AIT to the level that it is the \"de facto\" embassy, and states that any international agreements made between the ROC and U.S. before 1979 are still valid unless otherwise terminated. One agreement that was unilaterally terminated by President Jimmy Carter upon the establishment of relations with the PRC was the Sino-American Mutual Defense Treaty.\nThe TRA provides for Taiwan to be treated under U.S. laws the same as \"foreign countries, nations, states, governments, or similar entities\", thus treating Taiwan as a sub-sovereign foreign state equivalent. The act provides that for most practical purposes of the U.S. government, the absence of diplomatic relations and recognition will have no effect.\nMilitary provisions.\nThe TRA does not guarantee or relinquish the U.S. intervening militarily if the PRC attacks or invades Taiwan, as its primary purpose is to ensure that the Taiwan policy will not be changed unilaterally by the U.S. president and ensure any decision to defend Taiwan will be made with the consent of the Congress. The act states that \"the United States will make available to Taiwan such defense articles and defense services in such quantity as may be necessary to enable Taiwan to maintain a sufficient self-defense capability\" and \"shall maintain the capacity of the United States to resist any resort to force or other forms of coercion that would jeopardize the security, or social or economic system, of the people on Taiwan\". However, the decision about the nature and quantity of defense services that America will provide to Taiwan is to be determined by the President and Congress. America's policy has been called \"strategic ambiguity,\" and it is designed to dissuade Taiwan from a unilateral declaration of independence, and to dissuade the PRC from unilaterally unifying Taiwan with the PRC.\nThe TRA further stipulates that the United States will \"consider any effort to determine the future of Taiwan by other than peaceful means, including by boycotts or embargoes, a threat to the peace and security of the Western Pacific area and of grave concern to the United States\".\nThe TRA requires the United States to have a policy \"to provide Taiwan with arms of a defensive character\" and \"to maintain the capacity of the United States to resist any resort to force or other forms of coercion that would jeopardize the security, or the social or economic system, of the people on Taiwan\".\nSuccessive U.S. administrations have sold arms to Taiwan despite demands from the PRC that the U.S. follow Three Joint Communiqu\u00e9s and the U.S. government's proclaimed One-China policy.\nReaction and reaffirmation.\nThe TRA's passage caused Chinese leader Deng Xiaoping to begin viewing the United States as an insincere partner willing to abandon its previous commitments to China.\nReagan administration.\nThe PRC aligned itself with the Third World countries rather than with the United States or the Soviet Union, engaging itself in various movements such as nuclear non-proliferation that would allow it to critique the superpowers. In the August 17th communique of 1982, the United States agreed to reduce arms sales to Taiwan. However, it also declared that it would not formally recognize PRC's sovereignty over Taiwan, as part of the Reagan administration's Six Assurances offered to Taipei in 1982.\nClinton administration.\nIn the late 1990s, the United States Congress passed a non-binding resolution stating that relations between Taiwan and the United States will be honored through the TRA first. This resolution, which puts greater weight on the TRA's value over that of the three communiques, was signed by President Bill Clinton. Both chambers of Congress have repeatedly reaffirmed the importance of the TRA.\nSince 2000.\nA July 2007 Congressional Research Service Report confirmed that U.S. policy has not recognized the PRC's sovereignty over Taiwan. The PRC continues to view the TRA as \"an unwarranted intrusion by the United States into the internal affairs of China\". The United States has continued to supply Taiwan with armaments and China has continued to protest.\nBipartisan affirmation (2016).\nOn 19 May 2016, one day before Tsai Ing-wen assumed the democratically elected presidency of the Republic of China, former U.S. Senators Marco Rubio (R-FL), a member of the Senate Foreign Relations Committee and Senate Select Committee on Intelligence and Bob Menendez (D-NJ), former chair of the Senate Foreign Relations Committee and co-chair of the Senate Taiwan Caucus, introduced a concurrent resolution reaffirming the TRA and the \"Six Assurances\" as cornerstones of United States\u2013Taiwan relations.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66373", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=66373", "title": "Taiwanization", "text": "Term used in Taiwan to emphasize the importance of a Taiwanese identity\nTaiwanization (), also known as the Taiwanese localization movement, is a conceptual term used in Taiwan to emphasize the importance of a Taiwanese culture, society, economy, nationality, and identity rather than to regard Taiwan as solely an appendage of China. This involves the teaching of history, geography, and culture from a Taiwan-centric perspective, as well as promoting languages locally established in Taiwan, including Taiwanese Hokkien (Taiwanese), Hakka, and aboriginal languages.\nThe localization movement has been expressed in forms such as the use of language or dialect in the broadcast media and entire channels devoted to aboriginal and Hakka affairs. Textbooks have been rewritten by scholars to more prominently emphasize Taiwan. The political compromise that has been reached is to teach both the history of Taiwan and the history of mainland China.\nSome Taiwanese state-owned companies or organizations established in an earlier era have names containing the words \"China\" or \"Chinese\". They have been encouraged in recent years to change the word \"China\" in their names to \"Taiwan\" in a campaign known as the \"name rectification campaign\" () or \"Taiwan name rectification\". Many Taiwan-based companies in international sectors already identify themselves as \"Taiwan\"-based for clarity's sake. This keeps international customers from confusing them with an enterprise based in the People's Republic of China. Other Taiwan-based companies decline to change to a \"Taiwanese\" name because of expense or the political views held by important clients and company leaders.\nHistory and development.\nNo one can confirm when the concept of Taiwanization has started. Some say when the first large wave of Han people emigrated from mainland China to Taiwan in the mid-16th century, they must have wanted to maintain some independence from the control of the ruling class in their original hometown. Others say that only when the Kingdom of Tungning, with its capital at Tainan, was built by the Zheng family in 1662, did this concept appear.\nMost Chinese contemporary scholars of mainland China believe the roots of the localization movement began during the Japanese rule (1895 to 1945), when groups organized to lobby the Imperial Japanese government for greater Taiwanese autonomy and home rule. After the Kuomintang (KMT) took over Taiwan, the Taiwan home-rule groups were decimated in the wake of the February 28 Incident of 1947. The KMT viewed Taiwan primarily as a base to retake mainland China and quickly tried to subdue potential political opposition on the island. The KMT did little to promote a unique Taiwanese identity; often newly immigrated Chinese or \"mainlanders\" as they were called, working in administrative positions, lived in neighborhoods where they were segregated from the Taiwanese. Others, especially poorer refugees, were shunned by the Hoklo Taiwanese and lived among aborigines instead. The mainlanders often learned Hokkien. However, since Mandarin was enforced as the official language of the Republic of China and Taiwanese was not allowed to be spoken in schools, the mainlanders who learned Taiwanese found their new language skills to diminish. As Taiwanese, or any language other than Mandarin, was forbidden in the military posts, many mainlanders whose family lived in martial villages only spoke Mandarin and perhaps their home language (e.g. Cantonese, Shanghainese, etc.). The promotion of Chinese nationalism within Taiwan and the fact that the ruling group on Taiwan were considered outsiders by some were the reasons cited for both the Taiwan independence movement and Taiwanization.\nIn the 1970s and 1980s there was a shift in power away from the KMT to people native to Taiwan. This, combined with cultural liberalization and the increasing remoteness of the possibility of retaking mainland China, led to a cultural and political movement which emphasized a Taiwan-centered view of history and culture rather than one which was China-centered or even, as before 1946, Japan-centered. Taiwanization was strongly supported by President Lee Teng-hui.\nThe \"Bentuhua\" or \"localization/indigenization movement\" was sparked in the mid-1970s with the growing expression of ethnic discontent due to unequal distribution of political and cultural power between mainlanders and Taiwanese people. Beginning in the 1960s, Taiwan was enveloped by the problems of rapid industrial development, rural abandonment, labor disputes and the uneven distribution of access to wealth and social power. These changes, combined with the loss of several key allies, forced the KMT regime to institute limited reforms. The reforms permitted under Chiang Ching-kuo allowed indigenization to increase as leading dissidents generated a response to the government's failures. The dissident groups, united under the \"Tangwai\", or \u201coutside the party\u201d banner, called for the government to accept the reality that it was only the government of Taiwan and not China. The key demands of the Tangwai involved instituting democracy and seeking international recognition as a sovereign state. Taiwanese demanded full civil rights as guaranteed under the ROC constitution and equal political rights as those experienced by the Mainlander elite.\nThe Taiwanese cultural elite fully promoted the development of \"Xiang tu\" literature and cultural activities, including rediscovering Taiwanese nativist literature written under Japanese colonial rule. The tangwai movement revived symbols of Taiwanese resistance to Japanese rule in the effort to mobilize ethnic Taiwanese. The opposition to the KMT's China-centered cultural policies resulted in dissidents crafting new national-historical narratives that placed the island of Taiwan itself at the center of the island's history. The Taiwanese emerged as a frequently colonized and often oppressed people. The concept of \"bentuhua\" was finally expressed in the cultural domain in the premise of Taiwan as a place with a unique society, culture and history. This principle has been largely adopted for understanding Taiwan's cultural representation and expressed in a variety of cultural activities, including music, film and the literary and performing arts.\nThe pressures of indigenization and the growing acceptance of a unique Taiwanese cultural identity have met opposition from more conservative elements of Taiwan society. Critics argue that the new perspective creates a \u201cfalse\u201d identity rooted in ethnic nationalism as opposed to an \u201cauthentic\u201d Chinese identity, which is primordial and inherent. Many mainlanders living on Taiwan complain that their own culture is marginalized by \"bentuhua\", and initially expressed fear of facing growing alienation. In the past decade these complaints have subsided somewhat as Taiwan increasingly views itself as a pluralistic society that embraces many cultures and recognizes the rights of all citizens.\nIn the mid-to-late 1990s, gestures toward Taiwanization were increasingly adopted by pro-unification figures who, while supporting the Chinese nationalism of Chiang Kai-shek, saw it as appropriate, or at least advisable, to display more appreciation for cultures of Taiwan. Pro-unification politicians such as James Soong, the former head of the Government Information Office who once oversaw the limitation of Taiwanese dialects, began speaking in Hoklo on semi-formal occasions.\n2000s name rectification campaign.\nThe \"Name Rectification Campaign\" includes efforts by the Taiwanese government beginning in 2000 to distance itself from China and rollback earlier sinicization efforts by taking actions such as removing Chinese influence from items within Taiwan control. While the Taiwanese localization movement may view such efforts as emphasizing the importance of Taiwan's culture, this section addresses the perspective of those who likely support the Chinese unification of all of Greater China under a single political entity.\nAt the end of World War II, Chinese Kuomintang forces took over Taiwan and soon began an effort to sinicize the population. Taiwanese urban elites were wiped out in the February 28 Incident. Mandarin Chinese became the only language allowed in media and school to the exclusion of other languages of Taiwan, as well as Japanese. Public institutions and corporations were given names that included the words \"China\" or \"Chinese\". School history and geography lessons focused on China with little attention paid to Taiwan. Street names in Taipei were changed from their original names to Chinese names that reflected the geography of China and Kuomintang ideals.\nWith the end of martial law in 1987 and the introduction of democracy in the 1990s after the Wild Lily student movement, an effort began to re-assert Taiwanese identity and culture while trying to get rid of many Chinese influences imposed by the Kuomintang.\nEducation and language campaign.\nIn 2000, then-ROC president Lee Teng-hui began making statements such as \"Taiwan culture is not a branch of Chinese culture\" and \"Taiwan's Minnan dialect is not a branch of Fujian's Minnan, dialect but rather a 'Taiwan dialect' Taiwan radio and TV increased their Taiwanese Hokkien programming. These efforts were perceived in China as initial efforts towards breaking the ties between Taiwan culture and Chinese culture by downplaying the long-term Chinese cultural and historic identification in that region.\nIn April 2003, the Committee for Promoting Mandarin, which was part of Taiwan's Ministry of Education, released a legislation proposal entitled \"Language Equality Law.\" The proposed legislation sought to designate fourteen languages as the national languages of Taiwan. In mainland China, this was seen as an effort to diminish the use of standard Mandarin and its cultural influences in favor of revising the cultural and psychological foundations on the island of Taiwan by using other languages. The draft was not adopted.\nThe textbook issue was raised in November 2004, when a group of lawmakers, legislative candidates and supporters of the pro-independence Taiwan Solidarity Union (TSU) urged the ROC Ministry of Education to publish Taiwan-centric history and geography textbooks for school children as part of the Taiwanization campaign. Although the resulting draft outline of history course for regular senior middle schools was criticized by a variety of groups, President Chen Shui-bian responded that \"to seek the truth of Taiwan's history\" is not equal to desinicization nor an act of independence and indicated that he would not interfere with the history editing and compilation efforts.\nThe proposals to revise Taiwan's history textbooks were condemned in February 2007 by the People's Republic of China's Taiwan Affairs Office of the State Council as being part of the desinicization campaign. In July 2007, the Taiwan Ministry of Education released a study that found 5,000 textbook terms, some relating to Chinese culture, as being \"unsuitable\". The Kuomintang saw this as part of a textbook censorship desinicization campaign. The proposals have not been adopted.\nName change campaign.\nBetween 2002 and 2007, the ROC government under Chen Shui-bian took steps to revise the terms \"China\", \"Republic of China\", \"Taipei\", and others that impart an association with the Chinese culture.\nIn 2002, the \"Name Rectification Campaign\" made significant advances in replacing the terms \"China\", \"Republic of China\", or \"Taipei\" with the term \"Taiwan\" on official documents, in the names of Taiwan-registered organizations, companies, and public enterprises on the island, and in the names of businesses stationed abroad. In 2003, the ROC Foreign Ministry issued a new passport with the word \"Taiwan\" printed in English on its cover. Moreover, in January 2005, Taiwan adopted a Westernized writing format for government documents, denied that it was an attempt at desinicization, and promoted the actions as \"a concerted effort at globalizing Taiwan's ossified bureaucracies and upgrading the nation's competitive edge.\"\nCampaigning in this area continued in March 2006, where the Democratic Progressive Party sought to change the Republic of China year designation used in Taiwan to the Gregorian calendar. Instead of the year 2006 being referred to as the \"95th year of the ROC\"\u2014with the 1912 founding of the Republic of China being referred to as \"the first year of the ROC\"\u2014the year 2006 would be identified as 2006 in official usage such as on banknotes, IDs, national health insurance cards, driver's licenses, diplomas and wedding certificates. This was viewed as the government trying another angle for desinicization by removing any trace of China from Taiwan.\nIn February 2007, the term \"China\" was replaced by the term \"Taiwan\" on Taiwan postage stamps to coincide with the 60th anniversary of the February 28 Incident that began on 28 February 1947 that was violently suppressed by the Kuomintang (KMT). In that same month, the name of the official postal service of Taiwan was changed from the Chunghwa Post Co. to The Taiwan Post Co. The company's name was changed back on 1 August 2008, and the names on the postal stamps were reversed in late 2008, soon after the Kuomintang (KMT) candidate Ma Ying-Jeou won back presidency and ended 8 years of the Democratic Progressive Party (DPP) rule.\nIn March 2007, the name plate of the ROC Embassy in Panama was revised both to include the word \"Taiwan\" in parentheses between the words \"the Republic of China\" and \"Embassy\" in both of its Chinese and Spanish titles, and to omit the ROC national emblem.\nSupporters of the name-change movement argue that the Republic of China no longer exists, as it did not include Taiwan when it was founded in 1912 and mainland China is now controlled by the Chinese Communist Party as the People's Republic of China. Furthermore, the ambiguity surrounding the legal status of Taiwan as a result of the Treaty of peace with Japan and Treaty of San Francisco after World War II, means that the Republic of China was merely a military occupier of Taiwan. As Japan relinquished its sovereignty over Taiwan without passing it to a specific country, it is argued that Taiwan ought to be deemed a land belonging to no country, whose international status has yet to be defined.\nConstitutional and political campaign.\nIn October 2003, President Chen Shui-bian announced that Taiwan would seek a new constitution suitable for the Taiwan people that would turn Taiwan into a \"normal country.\" In explaining what a normal country was in the context of desinicization and the 1992 One-China policy, Chen Shui-bian stated,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nIn response, the Pan-Blue Coalition within Taiwan sought to portray President Chen Shui-bian and his Democratic Progressive Party as radicals intent on implementing revolutionary desinicization that would disenfranchise various ethnic groups within Taiwan who have an affinity for China and the Chinese culture.\nIn February 2007, the Democratic Progressive Party (DPP) adopted a resolution to identify those responsible for the 1947 February 28 Incident massacre of Taiwanese people in order to charge them with war crimes and crimes against humanity. The effort also sought to remove the \"remnants of dictatorship\" traced to that sixty-year-old incident. This was seen in mainland China as being in line with a series of desinicization actions by both the Taiwan government and the DPP to rid both Chiang and China from the Taiwan public scene. Some applauded this as a courageous act of seeking justice. Others criticized the request, seeing it as \"rubbing salt into wounds\" by playing up the historical issues for political gain.\nOther campaign.\nIn March 2007, it was noted that the destruction of the Western Line railway base found below the floor of the Taipei Main Station and built in 1893 by Qing Empire-appointed Governor of Taiwan Province Liu Mingchuan was part of the government's call for desinicization through removal of the Chinese site.\nIn July 2007, President Chen Shui-bian announced that he would allow mainland Chinese diplomas or students into Taiwan during the rest of his presidential term. This, however, was not achieved.\nImpact.\nOne phenomenon that has resulted from the Taiwanization movement is the advent of Taike subculture, in which young people consciously adopt the wardrobe, language and cuisine to emphasize the uniqueness of popular, grassroots Taiwanese culture, which in previous times had often been seen as provincial and backwards by the mainstream.\nIn April 2002, the Chinese Communist Party (CCP) noted both active efforts on the part of Taiwan to push ahead its Taiwanization policy and intensified United States-Taiwan military cooperation. In response, the CCP publicly reminded its military to be prepared to achieve its goal of \"Chinese reunification\" (intended to mean making Taiwan a part of the People's Republic of China) through military means. In addition, the CCP sought assistance from the United States to address the matter with Taiwan. As part of making the upcoming U.S. visit by then vice-president Hu Jintao go smoothly, the United States cautioned the Chen Shui-bian administration not to \"go too far\" in cross-Strait relations.\nIn April 2005, the CCP general secretary Hu Jintao and the former ROC Vice President and then chairman of the Kuomintang party (KMT) Lien Chan shook hands. Billed as a historic moment, this was the first handshake by the top leaders of the KMT and the CCP in 60 years. In remarking on the handshake, chairman Lien noted that it was a turning point where the KMT and the CCP would work together to bring about peaceful cross-strait relations and specifically distanced the KMT from Taiwan independence and desinicization efforts.\nSupport and opposition.\nSignificant outcries surfaced both within Taiwan and abroad opposing the concept of Taiwan localization in the early years after President Chiang Ching-kuo's death, denouncing it as the \"independent Taiwan movement\" (Chinese: \u53f0\u7368\u904b\u52d5). Vocal opponents are primarily the 1949-generation Mainlanders, or older generations of Mainlanders living in Taiwan that had spent their formative years and adulthood on the pre-1949 mainland Republic of China, and native Taiwanese who identify with a pan-Han Chinese cultural identity. They included people ranging from academics like Chien Mu, reputed to be the last prominent Chinese intellectual opposing the conventional wisdom take on the May Fourth Movement, politicians like Lien Chan, from a family with a long history of active pan-Chinese patriotism despite being native Taiwanese, to gang mobsters like Chang An-lo, a leader of the notorious United Bamboo Gang.\nThe opposing voices were subsequently confined to the fringe in the mid-2000s Taiwan itself. Issues persist, particularly supporters of the Pan-Blue coalition, which advocates retaining a strong link to mainland China, dispute over such issues as what histories to teach. Nonetheless, both of the two major political forces in Taiwan reached a consensus, and the movement has overwhelming support among the population. This is in part due to the 1949-generation Mainlanders have gradually passed on from the scene, and politicians supporting and opposing the Taiwanese independence movement both realize a majority of Taiwan's current residents, either because they are born in Taiwan to Mainlander parents with no collective memories of their ancestral homes, or they are native Taiwanese, thus feeling no historical connotations with the entire pre-1949 Republic of China on mainland China, support the movement as such.\nIn mainland China, the PRC government has on the surface adopted a neutral policy on Taiwanization and its highest-level leaders publicly proclaim it does not consider the Taiwanization movement to be either a violation of its One China Policy or equivalent to the independence movement. Nonetheless, the state-owned media and academics employed by organizations such as universities' Institutes of Taiwan Studies or the Chinese Academy of Social Sciences (CASS) periodically release study results, academic journal articles, or editorials strongly criticizing the movement as \"the cultural arm of Taiwanese independence movement\" (Chinese: \u6587\u5316\u53f0\u7368) with the government's tacit approval, showing the PRC government's opposition towards Taiwanization.\nNowadays, another source of significant opposition to the Taiwanization movement remains in the overseas Chinese communities in Southeast Asia and the Western world, who identify more with the historic pre-1949 mainland Republic of China or pre-Taiwanization movement ROC on Taiwan that oriented itself as the rump legitimate government of China. A great many are themselves refugees and dissidents who fled mainland China, either directly or through Hong Kong or Taiwan, during the founding of the People's Republic of China and the subsequent periods of destructive policies (such as the Land Reform, the Anti-Rightist Movement, Great Leap Forward, or the Cultural Revolution), Hong Kong anti-Communist immigrants who fled Hong Kong in light of the Handover to the PRC in 1997, or Mainlanders living in Taiwan who moved to the West in response to the Taiwanization movement. Conversely, the current population of Taiwan regard these overseas Chinese as foreigners akin to Singaporean Chinese, as opposed to the pre-Taiwanization era when they were labeled as fellow Chinese compatriots. The PRC has capitalized on this window of opportunity in making overtures to the traditionally anti-Communist overseas Chinese communities, including gestures in supporting traditional Chinese culture and dumping explicitly Communist tones in overseas communications. This results in a decline of active political opposition to the PRC from overseas Chinese when compared with the times before the Taiwanization movement in Taiwan.\nIn Hong Kong, Taiwanization movements have pushed localization or pro-Chinese Communist tilts among the traditionally pro-Republic of China individuals and organizations. A prominent example is Chu Hai College, whose academic degree programs were recognized officially by the Hong Kong SAR government in May 2004, and registered as an \"Approved Post-secondary College\" with the Hong Kong SAR government since July of the same year. It has since been renamed the Chu Hai College of Higher Education (\u73e0\u6d77\u5b78\u9662) and no longer registered with the Republic of China's Ministry of Education. New students from 2004 have been awarded degrees in the right of Hong Kong rather than Taiwan.\nRole in domestic politics.\nEven though it is a broad consensus currently regarding the overall ideology of Taiwanization, there are still deep disputes over practical policies between the three main political groups of Taiwan independence, Chinese unification, and supporters of Chinese culture. Pro-independence supporters argue that Taiwan is and should be enhancing an identity which is separate from the Chinese one, and in more extreme cases advocates the removal of Chinese \"imprints\". Meanwhile, some would argue that Taiwan should create a distinctive identity that either includes certain Chinese aspect or exists within a broader Chinese one. Those who support Chinese unification call for a policy of enhancing the Chinese identity. Groups that support Chinese unification and Chinese nationalism have emphasized the distinction between Taiwanization and what some perceive as desinicization and argued that they do not oppose the promotion of a Taiwanese identity, but rather oppose the use of that identity to separate itself from a broader Chinese one. On the other hand, a few apolitical groups have claimed that most of the political factions merely use these points to win support for elections.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66380", "revid": "30905782", "url": "https://en.wikipedia.org/wiki?curid=66380", "title": "TOPS-20", "text": "Operating system by Digital Equipment Corporation\nThe TOPS-20 operating system by Digital Equipment Corporation (DEC) is a proprietary OS used on some of DEC's 36-bit mainframe computers. The Hardware Reference Manual was described as for \"DECsystem-10/DECSYSTEM-20 Processor\" (meaning the DEC PDP-10 and the DECSYSTEM-20).\nTOPS-20 began in 1969 as the TENEX operating system of Bolt, Beranek and Newman (BBN) and shipped as a product by DEC starting in 1976. TOPS-20 is almost entirely unrelated to the similarly named TOPS-10, but it was shipped with the PA1050 TOPS-10 Monitor Calls emulation facility which allowed most, but not all, TOPS-10 executables to run unchanged. As a matter of policy, DEC did not update PA1050 to support later TOPS-10 additions except where required by DEC software.\nTOPS-20 competed with TOPS-10, ITS and WAITS\u2014all of which were notable time-sharing systems for the PDP-10 during this timeframe. TOPS-20 is informally known as TWENEX.\nTENEX.\nTOPS-20 was based upon the TENEX operating system, which had been created by Bolt Beranek and Newman for Digital's PDP-10 computer. After Digital started development of the KI-10 version of the PDP-10, an issue arose: by this point TENEX was the most popular customer-written PDP-10 operating systems, but it would not run on the new, faster KI-10s. To correct this problem, the DEC PDP-10 sales manager purchased the rights to TENEX from BBN and set up a project to port it to the new machine. In the end, very little of the original TENEX code remained, and Digital ultimately named the resulting operating system TOPS-20.\nPA1050.\nSome of what came with TOPS-20 was merely an emulation of the TOPS-10 Operating System's calls. These were known as UUO's, standing for Unimplemented User Operation, and were needed both for compilers, which were not 20-specific, to run, as well as user-programs written in these languages. The package that was mapped into a user's address space was named PA1050: PA as in PAT as in compatibility; 10 as in DEC or PDP 10; 50 as in a PDP 10 Model 50, 10/50, 1050.\nSometimes PA1050 was referred to as PAT, a name that was a good fit to the fact that PA1050, \"was simply unprivileged user-mode code\" that \"performed the requested action, using JSYS calls where necessary.\"\nTOPS-20 capabilities.\nThe major ways to get at TOPS-20 capabilities, and what made TOPS-20 important, were\nThe \"EXEC\" accomplished its work primarily using\nCommand processor.\nRather advanced for its day were some TOPS-20-specific features:\n*\"noise-words\" - typing DIR and then pressing the ESCape key resulted in\nDIRectory (of files)\ntyping and pressing the key resulted in\n Information (about)\nOne could then type to find out what operands were permitted/required. Pressing displays status information.\nAvailable programming languages.\nSome available programming languages were FORTRAN, COBOL, BASIC, ALGOL, CPL, and APL.\nCommands.\nThe following list of commands are supported by the TOPS-20 Command Processor.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nJSYS features.\nJSYS stands for Jump to SYStem. Operands were at times memory addresses. \"TOPS-20 allows you to use 18-bit or 30-bit addresses. Some monitor calls require one kind, some the other; some calls accept either kind. Some monitor calls use only 18 bits to hold an address. These calls interpret 18-bit addresses as locations in the current section.\"\nInternally, files were first identified, using a GTJFN (Get Job File Number) JSYS, and then that JFN number was used to open (OPENF) and manipulate the file's contents.\nPCL (Programmable Command Language).\nPCL (Programmable Command Language) is a programming language that runs under TOPS-20. PCL source programs are, by default, stored with Filetype .PCL, and enable extending the TOPS-20 EXEC via a verb named DECLARE. Newly compiled commands then become functionally part of the EXEC.\nPCL language features.\nPCL includes:\nTOPS-20 today.\nPaul Allen maintained several publicly accessible historic computer systems before his death, including an XKL TOAD-2 running TOPS-20.\nSee also SDF Public Access Unix System.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66381", "revid": "15959810", "url": "https://en.wikipedia.org/wiki?curid=66381", "title": "TWENEX", "text": ""}
{"id": "66382", "revid": "8390765", "url": "https://en.wikipedia.org/wiki?curid=66382", "title": "Beta Carinae", "text": "Star in the constellation Carina\n&lt;/td&gt;\n! style=\"text-align: center; background-color: #FFFFC0;\" colspan=\"2\" | Observation dataEpoch J2000\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Equinox J2000\n! style=\"text-align:left\" | Constellation\n! style=\"text-align:left\" | \n! style=\"text-align:left\" | Right ascension\n! style=\"text-align:left\" | Declination\n! style=\"text-align:left\" | Apparent\u00a0magnitude\u00a0(V)\n! style=\"background-color: #FFFFC0; text-align: center;\" colspan=\"2\"| Characteristics\n! style=\"text-align:left\" | Spectral\u00a0type\n! style=\"text-align:left\" | U\u2212B \n! style=\"text-align:left\" | B\u2212V \n&lt;/th&gt;&lt;/tr&gt;\n&lt;/th&gt;&lt;/tr&gt;\nBeta Carinae is the second-brightest star in the southern constellation of Carina. It has the official name Miaplacidus; \"Beta Carinae\" is the star's Bayer designation, which is Latinised from \u03b2 Carinae and abbreviated Beta Car or \u03b2 Car. With apparent magnitude of 1.69, it is one of the brightest stars in the night sky. It is the brightest star in the southern asterism known as the Diamond Cross, marking the southwestern end of the asterism. It lies near the planetary nebula IC 2448. Parallax measurements place it at a distance of from the Sun.\nNomenclature.\n\"\u03b2 Carinae\" (Latinised to \"Beta Carinae\") is the star's Bayer designation.\nThe star's historical name \"Miaplacidus\" made its debut on star maps in 1856 when the star atlas \"Geography of the Heavens\", composed by Elijah Hinsdale Burritt, was published. The meaning and linguistic origin of the name remained an enigma for many decades, until William Higgins, a great scholar and expert on star names, surmised that the name \"Miaplacidus\" is apparently a bilingual combination of Arabic \"\u0645\u064a\u0627\u0647\" \"miy\u0101h\" for 'waters' and Latin \"placidus\" for 'placid'. The IAU Working Group on Star Names first bulletin of July 2016 included a table of the first two batches of names approved by the WGSN; which included \"Miaplacidus\" for this star.\nIn Chinese, (), meaning \"Southern Boat\", refers to an asterism consisting of \u03b2 Carinae, V337 Carinae, PP Carinae, \u03b8 Carinae and \u03c9 Carinae. Consequently, \u03b2 Carinae itself is known as (, ).\nPhysical properties.\nThe stellar classification of A1\u00a0III mean that Miaplacidus is an evolved giant star that has exhausted the hydrogen at its core and has expanded. It has an estimated age of 260 million years. This star does not show an excess emission of infrared radiation that might otherwise suggest the presence of a debris disk. It has about 3.8 times the Sun's mass and has expanded to almost six times the radius of the Sun. Presently it is radiating 223 times as much luminosity as the Sun from its outer envelope at an effective temperature of . Despite its enlarged girth, this star still shows a rapid rotation rate, with a projected rotational velocity of 146 km/s.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66383", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=66383", "title": "Tops-20", "text": ""}
{"id": "66387", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=66387", "title": "Hypertension (disambiguation)", "text": "Hypertension may refer to the following:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "66388", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=66388", "title": "Bronchodilator", "text": "Drug which widens the lung airways\nA bronchodilator or broncholytic (although the latter occasionally includes secretory inhibition as well) is a substance that dilates the bronchi and bronchioles, decreasing resistance in the respiratory airway and increasing airflow to the lungs. Bronchodilators may be originating naturally within the body, or they may be medications administered for the treatment of breathing difficulties, usually in the form of inhalers. They are most useful in obstructive lung diseases, of which asthma and chronic obstructive pulmonary disease are the most common conditions. They may be useful in bronchiolitis and bronchiectasis, although this remains somewhat controversial. They are often prescribed but of unproven significance in restrictive lung diseases.\nBronchodilators are either short-acting or long-acting. Short-acting medications provide quick or \"rescue\" relief from acute bronchoconstriction. Long-acting bronchodilators help to control and prevent symptoms. The three types of prescription bronchodilating drugs are beta-2 adrenergic agonists (short- and long-acting), anticholinergics (short- and long-acting), and theophylline (long-acting).\nShort-acting \u03b22-adrenergic agonists.\nThese are quick-relief or \"rescue\" medications that provide quick, temporary relief from asthma symptoms or flare-ups. These medications usually take effect within 20 minutes or less, and can last from four to six hours. These inhaled medications are best for treating sudden and severe or new asthma symptoms. Taken 15 to 20 minutes ahead of time, these medications can also prevent asthma symptoms triggered by exercise or exposure to cold air. Some short-acting \u03b2-agonists, such as salbutamol, are specific to the lungs; they are called \u03b22-adrenergic agonists and can relieve bronchospasms without unwanted cardiac side effects of nonspecific \u03b2-agonists (for example, ephedrine or epinephrine). Patients who regularly or frequently need to take a short-acting \u03b22-adrenergic agonist should consult their doctor, as such usage indicates uncontrolled asthma, and their routine medications may need adjustment.\nLong-acting \u03b22-adrenergic agonists.\nThese are long-term medications taken routinely in order to control and prevent bronchoconstriction. They are not intended for fast relief. These medications may take longer to begin working, but relieve airway constriction for up to 12 hours.\nCommonly taken twice a day with an anti-inflammatory medication, they maintain open airways and prevent asthma symptoms, particularly at night.\nSalmeterol and formoterol are examples of these.\nAnticholinergics.\nSome examples of anticholinergics are tiotropium (Spiriva) and ipratropium bromide.\nTiotropium is a long-acting, 24-hour, anticholinergic bronchodilator used in the management of chronic obstructive pulmonary disease (COPD).\nOnly available as an inhalant, ipratropium bromide is used in the treatment of asthma and COPD. As a short-acting anticholinergic, it improves lung function and reduces the risk of exacerbation in people with symptomatic asthma. However, it will not stop an asthma attack already in progress. Because it has no effect on asthma symptoms when used alone, it is most often paired with a short-acting \u03b22-adrenergic agonist. While it is considered a relief or rescue medication, it can take a full hour to begin working. For this reason, it plays a secondary role in acute asthma treatment. Dry throat is the most common side effect. If the medication gets in contact with the eyes, it may cause blurred vision for a brief time.\nThe use of anticholinergics in combination with short-acting \u03b22-adrenergic agonists has been shown to reduce hospital admissions in children and adults with acute asthma exacerbations.\nOther.\nAvailable in oral and injectable form, theophylline is a long-acting bronchodilator that prevents asthma episodes. It belongs to the chemical class methylxanthines (along with caffeine). It is prescribed in severe cases of asthma or those that are difficult to control. It must be taken 1\u20134 times daily, and doses cannot be missed. Blood tests are required to monitor therapy and to indicate when dosage adjustment is necessary. Side effects can include nausea, vomiting, diarrhea, stomach or headache, rapid or irregular heart beat, muscle cramps, nervous or jittery feelings, and hyperactivity. These symptoms may signal the need for an adjustment in medication. It may promote acid reflux, also known as GERD, by relaxing the lower esophageal sphincter muscle. Some medications, such as seizure and ulcer medications and antibiotics containing erythromycin, can interfere with the way theophylline works. Coffee, tea, colas, cigarette-smoking, and viral illnesses can all affect the action of theophylline and change its effectiveness. A physician should monitor dosage levels to meet each patient's profile and needs.\nAdditionally, some psychostimulant drugs that have an amphetamine like mode of action, such as amphetamine, methamphetamine, and cocaine, have bronchodilating effects and were used often for asthma due to the lack of effective \u03b22-adrenergic agonists for use as bronchodilator, but are now rarely, if ever, used medically for their bronchodilatory effects.\nGaseous carbon dioxide also relaxes airway musculature: hypocapnia caused by deliberate hyperventilation increases respiratory resistance while hypercapnia induced by carbon dioxide inhalation reduces it; however, this bronchodilating effect of carbon dioxide inhalation only lasts 4 to 5 minutes. Nonetheless, this observation has inspired the development of S-1226, carbon dioxide-enriched air formulated with nebulized perflubron.\nCommon bronchodilators.\nThe bronchodilators are divided into short- and long-acting groups. Short-acting bronchodilators are used for relief of bronchoconstriction, while long-acting bronchodilators are predominantly used for prevention.\nShort-acting bronchodilators include:\nLong-acting bronchodilators include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66390", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=66390", "title": "Sarin", "text": "Chemical compound and chemical warfare nerve agent\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nSarin (NATO designation GB \"short for G-series, B\") is an extremely toxic organophosphorus compound that has been often used as a chemical weapon due to its extreme potency as a nerve agent. \nSarin is a volatile, colorless and odorless liquid. Exposure can be lethal even at very low concentrations, and death can occur within one to ten minutes after direct inhalation of a lethal dose due to suffocation from respiratory paralysis, unless antidotes are quickly administered. People who absorb a non-lethal dose and do not receive immediate medical treatment may suffer permanent neurological damage.\nSarin is widely considered a weapon of mass destruction. Production and stockpiling of sarin was outlawed as of April 1997 by the Chemical Weapons Convention of 1993, and it is classified as a Schedule 1 substance.\nHealth effects.\nLike some other nerve agents that affect the neurotransmitter acetylcholine, sarin attacks the nervous system by interfering with the degradation of the neurotransmitter acetylcholine at neuromuscular junctions. Death usually occurs as a result of asphyxia due to the inability to control the muscles involved in breathing.\nInitial symptoms following exposure to sarin are a runny nose, tightness in the chest, and constriction of the pupils (miotic action). Soon after, the person will have difficulty breathing and experience nausea and drooling. This progresses to losing control of bodily functions, which may cause the person to vomit, defecate, and urinate. This phase is followed by twitching and jerking. Ultimately, the person becomes comatose and suffocates in a series of convulsive spasms. Common mnemonics for the symptomatology of organophosphate poisoning, including sarin, are the \"killer Bs\" of bronchorrhea and bronchospasm because they are the leading cause of death, and SLUDGE \u2013 salivation, lacrimation, urination, defecation, gastrointestinal distress, and emesis (vomiting). Death may follow in one to ten minutes after direct inhalation, but may also occur after a delay ranging from hours to several weeks, in cases where exposure is limited but no antidote is applied.\nSarin has a high volatility (ease with which a liquid can turn into vapour) relative to similar nerve agents, making inhalation very easy, and may even absorb through the skin. A person's clothing can release sarin for about 30 minutes after it has come in contact with sarin gas, which can lead to exposure of other people.\nManagement.\nTreatment measures have been described. Treatment is typically with the antidotes atropine and pralidoxime. Atropine, an antagonist to muscarinic acetylcholine receptors, is given to treat the physiological symptoms of poisoning. Since muscular response to acetylcholine is mediated through nicotinic acetylcholine receptors, atropine does not counteract the muscular symptoms. Pralidoxime can regenerate cholinesterases if administered within approximately five hours. Biperiden, a synthetic acetylcholine antagonist, has been suggested as an alternative to atropine due to its better blood\u2013brain barrier penetration and higher efficacy.\nMechanism of action.\nSarin is a potent inhibitor of acetylcholinesterase, an enzyme that degrades the neurotransmitter acetylcholine after it is released into the synaptic cleft. In vertebrates, acetylcholine is the neurotransmitter used at the neuromuscular junction, where signals are transmitted between neurons from the peripheral nervous system to muscle fibres. Normally, acetylcholine is released from the neuron to stimulate the muscle, after which it is degraded by acetylcholinesterase, allowing the muscle to relax. A build-up of acetylcholine in the synaptic cleft, due to the inhibition of acetylcholinesterase, means the neurotransmitter continues to act on the muscle fibre, so that any nerve impulses are effectively continually transmitted.\nSarin acts on acetylcholinesterase by forming a covalent bond with the particular serine residue at the active site. Fluoride is the leaving group, and the resulting organo-phosphoester is robust and biologically inactive.\nIts mechanism of action resembles that of some commonly used insecticides, such as malathion. In terms of biological activity, it resembles carbamate insecticides, such as Sevin, and the medicines pyridostigmine, neostigmine, and physostigmine.\nDiagnostic tests.\nControlled studies in healthy men have shown that a nontoxic 0.43\u00a0mg oral dose administered in several portions over a 3-day interval caused average maximum depressions of 22 and 30%, respectively, in plasma and erythrocyte acetylcholinesterase levels. A single acute 0.5\u00a0mg dose caused mild symptoms of intoxication and an average reduction of 38% in both measures of acetylcholinesterase activity. Sarin in blood is rapidly degraded either \"in vivo\" or \"in vitro\". Its primary inactive metabolites have \"in vivo\" serum half-lives of approximately 24 hours. The serum level of unbound isopropyl methylphosphonic acid (IMPA), a sarin hydrolysis product, ranged from 2\u2013135\u00a0\u03bcg/L in survivors of a terrorist attack during the first four hours post-exposure. Sarin or its metabolites may be determined in blood or urine by gas or liquid chromatography, while acetylcholinesterase activity is usually measured by enzymatic methods.\nA newer method called \"fluoride regeneration\" or \"fluoride reactivation\" detects the presence of nerve agents for a longer period after exposure than the methods described above. Fluoride reactivation is a technique that has been explored since at least the early 2000s. This technique obviates some of the deficiencies of older procedures. Sarin not only reacts with the water in the blood plasma through hydrolysis (forming so-called 'free metabolites'), but also reacts with various proteins to form 'protein adducts'. These protein adducts are not so easily removed from the body, and remain for a longer period of time than the free metabolites. One clear advantage of this process is that the period, post-exposure, for determination of sarin exposure is much longer, possibly five to eight weeks according to at least one study.\nToxicity.\nAs a nerve gas, sarin in its purest form is estimated to be 26 times more deadly than cyanide. The LD50 of subcutaneously injected sarin in mice is 172 \u03bcg/kg.\nSarin is highly toxic, whether by contact with the skin or breathed in. The toxicity of sarin in humans is largely based on calculations from studies with animals. The lethal concentration of sarin in air is approximately 28\u201335\u00a0mg per cubic meter per minute for a two-minute exposure time by a healthy adult breathing normally (exchanging 15 liters of air per minute, lower 28 mg/m3 value is for general population). This number represents the estimated lethal concentration for 50% of exposed victims, the LCt50 value. The LCt95 or LCt100 value is estimated to be 40\u201383 mg per cubic meter for exposure time of two minutes. Calculating effects for different exposure times and concentrations requires following specific toxic load models. In general, brief exposures to higher concentrations are more lethal than comparable long time exposures to low concentrations. There are many ways to make relative comparisons between toxic substances. The list below compares sarin to some current and historic chemical warfare agents, with a direct comparison to the respiratory LCt50:\nProduction and structure.\nSarin is a chiral molecule because it has four chemically distinct substituents attached to the tetrahedral phosphorus center. The \"SP \"form (the (\u2013) optical isomer) is the more active enantiomer due to its greater binding affinity to acetylcholinesterase. \nIt is almost always manufactured as a racemic mixture (a 1:1 mixture of its enantiomeric forms) as this involves a much simpler synthesis while providing an adequate weapon.\nA number of production pathways can be used to create sarin. The final reaction typically involves attachment of the isopropoxy group to the phosphorus with an alcoholysis with isopropyl alcohol. Two variants of this final step are common. One is the reaction of methylphosphonyl difluoride with isopropyl alcohol, which produces a racemic mixture of sarin enantiomers with hydrofluoric acid as a byproduct:\nThe second process, known as the \"Di-Di\" process, uses equimolar quantities of methylphosphonyl difluoride (Difluoro) and methylphosphonyl dichloride (Dichloro). This reaction gives sarin, hydrochloric acid and other minor byproducts. The Di-Di process was used by the United States for the production of its unitary sarin stockpile.\nThe scheme below shows a generic example that employs the Di-Di method as the final esterification step; in reality, the selection of reagents and reaction conditions dictate both product structure and yield. The choice of enantiomer of the mixed chloro fluoro intermediate displayed in the diagram is arbitrary, but the final substitution is selective for chloro over fluoro as the leaving group. Inert atmosphere and anhydrous conditions (Schlenk techniques) are used for synthesis of sarin and other organophosphates.\nAs both reactions leave considerable acid in the product, sarin produced in bulk by these methods has a short half-life without further processing, and would be corrosive to containers and damaging to weapons systems. Various methods have been tried to resolve these problems. In addition to industrial refining techniques to purify the chemical itself, various additives have been tried to combat the effects of the acid, such as:\nAnother byproduct of these two chemical processes is diisopropyl methylphosphonate, formed when a second isopropyl alcohol reacts with the sarin itself and from disproportionation of sarin, when distilled incorrectly. The factor of its formation in esterification is that as the concentration of DF-DCl decreases, the concentration of sarin increases, the probability of DIMP formation is greater. DIMP is a natural impurity of sarin, that is almost impossible to be eliminated, mathematically, when the reaction is a 1 mol-1 mol \"one-stream\".\nDegradation and shelf life.\nDegradation of phosphoryl halides begins with hydrolysis of the bond between phosphorus and the fluorine atom. This P-F bond is easily broken by nucleophilic agents through a SN2 mechanism, such as water and hydroxide. At high pH, sarin decomposes rapidly to relatively nontoxic phosphonic acid derivatives. The initial breakdown of sarin is into isopropyl methylphosphonic acid (IMPA), a chemical that is not commonly found in nature except as a breakdown product of sarin (this is useful for detecting the recent deployment of sarin as a weapon). IMPA then degrades into methylphosphonic acid (MPA), which can also be produced by other organophosphates.\nSarin with residual acid degrades after a period of several weeks to several months. The shelf life can be shortened by impurities in precursor materials. According to the CIA, some Iraqi sarin had a shelf life of only a few weeks, owing mostly to impure precursors.\nAlong with nerve agents such as tabun and VX, sarin can have a short shelf life. Therefore, it is usually stored as two separate precursors that produce sarin when combined. Sarin's shelf life can be extended by increasing the purity of the precursor and intermediates and incorporating stabilizers such as tributylamine. In some formulations, tributylamine is replaced by diisopropylcarbodiimide (DIC), allowing sarin to be stored in aluminium casings. In binary chemical weapons, the two precursors are stored separately in the same shell and mixed to form the agent immediately before or when the shell is in flight. This approach has the dual benefit of solving the stability issue and increasing the safety of sarin munitions.\nHistory.\nSarin was discovered in 1938 in Wuppertal-Elberfeld in Germany by scientists at IG Farben who were attempting to create stronger pesticides; it is the most toxic of the four G-Series nerve agents made by Germany. The compound, which followed the discovery of the nerve agent tabun, was named in honor of its discoverers: chemist Gerhard Schrader, chemist Otto Ambros, chemist Gerhard Ritter, and from Heereswaffenamt Hans-J\u00fcrgen von der Linde.\nUse as a weapon.\nIn mid-1939, the formula for the agent was passed to the chemical warfare section of the German Army Weapons Office, which ordered that it be brought into mass production for wartime use. Pilot plants were built, and a production facility was under construction (but was not finished) by the end of World War II. Estimates for total sarin production by Nazi Germany range from 500\u00a0kg to 10\u00a0tons.\nThough sarin, tabun, and soman were incorporated into artillery shells, Germany did not use nerve agents against Allied targets. Adolf Hitler refused to initiate the use of gases such as sarin as weapons.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66391", "revid": "21678789", "url": "https://en.wikipedia.org/wiki?curid=66391", "title": "Stimulant", "text": "Drug that increases alertness\nStimulants (also known as central nervous system stimulants, or psychostimulants, or colloquially as uppers) are a class of psychoactive drugs that increase alertness. They are used for various purposes, such as enhancing attention, motivation, cognition, mood, and physical performance. Some stimulants occur naturally, while others are exclusively synthetic. Common stimulants include caffeine, nicotine, cocaine (including crack cocaine), amphetamine/methamphetamine, methylphenidate, and modafinil. Stimulants may be subject to varying forms of regulation, or outright prohibition, depending on jurisdiction. Most stimulants are highly addictive and damage health when addicted.\nStimulants increase activity in the sympathetic nervous system, either directly or indirectly. Prototypical stimulants increase synaptic concentrations of excitatory neurotransmitters, particularly norepinephrine and dopamine (e.g., methylphenidate). Other stimulants work by binding to the receptors of excitatory neurotransmitters (e.g., nicotine) or by blocking the activity of endogenous agents that promote sleep (e.g., caffeine). Stimulants can affect various functions, including arousal, attention, the reward system, learning, memory, and emotion. Effects range from mild stimulation to euphoria, depending on the specific drug, dose, route of administration, and inter-individual characteristics. \nStimulants have a long history of use, both for medical and non-medical purposes. Archeological evidence from Peru shows that cocaine use dates back as far as 8000 B.C.E. Stimulants have been used to treat various conditions, such as narcolepsy, attention deficit hyperactivity disorder (ADHD), obesity, depression, and fatigue. They have also been used as recreational drugs, performance-enhancing substances, and cognitive enhancers, by various groups of people, such as students, athletes, artists, and workers. They have also been used to promote aggression of combatants in wartime, both historically and in the present day.\nStimulants have potential risks and side effects, such as addiction, tolerance, withdrawal, psychosis, anxiety, insomnia, cardiovascular problems, and neurotoxicity. The misuse and abuse of stimulants can lead to serious health and social consequences, such as overdose, dependence, crime, and violence. Therefore, the use of stimulants is regulated by laws and policies in most countries, and requires medical supervision and prescription in some cases.\nDefinition.\n\"Stimulant\" is an overarching term that covers many drugs including those that increase the activity of the central nervous system and the body, drugs that are pleasurable and invigorating, or drugs that have sympathomimetic effects. Sympathomimetic effects are those effects that mimic or copy the actions of the sympathetic nervous system. The sympathetic nervous system is a part of the nervous system that prepares the body for action, such as increasing the heart rate, blood pressure, and breathing rate. Stimulants can activate the same receptors as the natural chemicals released by the sympathetic nervous system (namely epinephrine and norepinephrine) and cause similar effects.\nEffects.\nAcute.\nStimulants in therapeutic doses, such as those given to patients with attention deficit hyperactivity disorder (ADHD), increase ability to focus, vigor, sociability, libido and may elevate mood. However, in higher doses, stimulants may actually decrease the ability to focus, a principle of the Yerkes-Dodson law. In higher doses, stimulants may also produce euphoria, vigor, and a decreased need for sleep.\nMany, but not all, stimulants have ergogenic effects; that is, they enhance physical performance. Drugs such as ephedrine, pseudoephedrine, amphetamine and methylphenidate have well documented ergogenic effects, while cocaine has the opposite effect.\nNeurocognitive enhancing effects of stimulants, specifically modafinil, amphetamine and methylphenidate have been reported in healthy adolescents by some studies, and is a commonly cited reason among illicit drug users for use, particularly among college students in the context of studying. Still, results of these studies is inconclusive: assessing the potential overall neurocognitive benefits of stimulants among healthy youth is challenging due to the diversity within the population, the variability in cognitive task characteristics, and the absence of replication of studies. Research on the cognitive enhancement effects of modafinil in healthy non-sleep-deprived individuals has yielded mixed results, with some studies suggesting modest improvements in attention and executive functions while others show no significant benefits or even a decline in cognitive functions.\nIn some cases, psychiatric phenomena may emerge such as stimulant psychosis, paranoia, and suicidal ideation. Acute toxicity has been reportedly associated with hyperhydrosis, panic attacks, severe anxiety, mydriasis, paranoia, aggressive behavior, excessive motor activity, psychosis, rhabdomyolysis, and punding. The violent and aggressive behavior associated with acute stimulant toxicity may partially be driven by paranoia. Most drugs classified as stimulants are sympathomimetic, meaning that they stimulate the sympathetic branch of the autonomic nervous system. This leads to effects such as mydriasis (dilation of the pupils), increased heart rate, blood pressure, respiratory rate and body temperature. When these changes become pathological, they are called arrhythmia, hypertension, and hyperthermia, and may lead to rhabdomyolysis, stroke, cardiac arrest, or seizures. However, given the complexity of the mechanisms that underlie these potentially fatal outcomes of acute stimulant toxicity, it is impossible to determine what dose may be lethal.\nChronic.\nAssessment of the effects of stimulants is relevant given the large population currently taking stimulants. A systematic review of cardiovascular effects of prescription stimulants found no association in children, but found a correlation between prescription stimulant use and ischemic heart attacks. A review over a four-year period found that there were few negative effects of stimulant treatment, but stressed the need for longer-term studies. A review of a year long period of prescription stimulant use in those with ADHD found that cardiovascular side effects were limited to transient increases in blood pressure only. However, a 2024 systematic review of the evidence found that stimulants overall improve ADHD symptoms and broadband behavioral measures in children and adolescents, though they carry risks of side effects like appetite suppression and other adverse events. Initiation of stimulant treatment in those with ADHD in early childhood appears to carry benefits into adulthood with regard to social and cognitive functioning, and appears to be relatively safe.\nAbuse of prescription stimulants (not following physician instruction) or of illicit stimulants carries many negative health risks. Abuse of cocaine, depending upon route of administration, increases risk of cardiorespiratory disease, stroke, and sepsis. Some effects are dependent upon the route of administration, with intravenous use associated with the transmission of many disease such as Hepatitis C, HIV/AIDS and potential medical emergencies such as infection, thrombosis or pseudoaneurysm, while inhalation may be associated with increased lower respiratory tract infection, lung cancer, and pathological restricting of lung tissue. Cocaine may also increase risk for autoimmune disease and damage nasal cartilage. Abuse of methamphetamine produces similar effects as well as marked degeneration of dopaminergic neurons, resulting in an increased risk for Parkinson's disease.\nMedical uses.\nStimulants are widely used throughout the world as prescription medicines as well as without a prescription (either legally or illicitly) as performance-enhancing or recreational drugs. Among narcotics, stimulants produce a noticeable crash or \"comedown\" at the end of their effects. In the US, the most frequently prescribed stimulants as of 2013 were lisdexamfetamine (Vyvanse), methylphenidate (Ritalin), and amphetamine (Adderall). It was estimated in 2015 that the percentage of the world population that had used cocaine during a year was 0.4%. For the category \"amphetamines and prescription stimulants\" (with \"amphetamines\" including amphetamine and methamphetamine) the value was 0.7%, and for MDMA 0.4%.\nStimulants have been used in medicine for many conditions including obesity, sleep disorders, mood disorders, impulse control disorders, asthma, nasal congestion and, in case of cocaine, as local anesthetics. Drugs used to treat obesity are called anorectics and generally include drugs that follow the general definition of a stimulant, but other drugs such as cannabinoid receptor antagonists also belong to this group. Eugeroics are used in management of sleep disorders characterized by excessive daytime sleepiness, such as narcolepsy, and include stimulants such as modafinil and pitolisant. Stimulants are used in impulse control disorders such as ADHD and off-label in mood disorders such as major depressive disorder to increase energy, focus and elevate mood. Stimulants such as epinephrine, theophylline and salbutamol orally have been used to treat asthma, but inhaled adrenergic drugs are now preferred due to less systemic side effects. Pseudoephedrine is used to relieve nasal or sinus congestion caused by the common cold, sinusitis, hay fever and other respiratory allergies; it is also used to relieve ear congestion caused by ear inflammation or infection.\nDepression.\nStimulants were one of the first classes of drugs to be used in the treatment of depression, beginning after the introduction of the amphetamines in the 1930s. However, they were largely abandoned for treatment of depression following the introduction of conventional antidepressants in the 1950s. Subsequent to this, there has been a resurgence in interest in stimulants for depression in recent years.\nStimulants produce a fast-acting and pronounced but transient and short-lived mood lift. In relation to this, they are minimally effective in the treatment of depression when administered continuously. In addition, tolerance to the mood-lifting effects of amphetamine has led to dose escalation and dependence. Although the efficacy for depression with continuous administration is modest, it may still reach statistical significance over placebo and provide benefits similar in magnitude to those of conventional antidepressants. The reasons for the short-term mood-improving effects of stimulants are unclear, but may relate to rapid tolerance. Tolerance to the effects of stimulants has been studied and characterized both in animals and humans. Stimulant withdrawal is remarkably similar in its symptoms to those of major depressive disorder.\nChemistry.\nClassifying stimulants is difficult, because of the large number of classes the drugs occupy, and the fact that they may belong to multiple classes; for example, ecstasy can be classified as a substituted methylenedioxyphenethylamine, a substituted amphetamine and consequently, a substituted phenethylamine.\nMajor stimulant classes include phenethylamines and their daughter class substituted amphetamines.\nAmphetamines (class).\nSubstituted amphetamines are a class of compounds based upon the amphetamine structure; it includes all derivative compounds which are formed by replacing, or substituting, one or more hydrogen atoms in the amphetamine core structure with substituents. Examples of substituted amphetamines are amphetamine (itself), methamphetamine, ephedrine, cathinone, phentermine, mephentermine, bupropion, methoxyphenamine, selegiline, amfepramone, pyrovalerone, MDMA (ecstasy), and DOM (STP). Many drugs in this class work primarily by activating trace amine-associated receptor 1 (TAAR1); in turn, this causes reuptake inhibition and effluxion, or release, of dopamine, norepinephrine, and serotonin. An additional mechanism of some substituted amphetamines is the release of vesicular stores of monoamine neurotransmitters through VMAT2, thereby increasing the concentration of these neurotransmitters in the cytosol, or intracellular fluid, of the presynaptic neuron.\nAmphetamine-type stimulants are often used for their therapeutic effects. Physicians sometimes prescribe amphetamine to treat major depressive disorder, where subjects do not respond well to traditional selective serotonin reuptake inhibitor (SSRI) medications, but evidence supporting this use is mixed. Two large phase III studies of lisdexamfetamine (a prodrug to amphetamine) as an adjunct to an SSRI or serotonin\u2013norepinephrine reuptake inhibitor (SNRI) in the treatment of major depressive disorder showed no further benefit relative to placebo in effectiveness. Numerous studies have demonstrated the effectiveness of drugs like Adderall (a mixture of salts of amphetamine and dextroamphetamine) in controlling symptoms associated with ADHD. Non-stimulants such as atomoxetine have also found to be effective. Due to their availability and fast-acting effects, substituted amphetamines are prime candidates for abuse.\nCocaine analogs.\nHundreds of cocaine analogs have been created, all of them usually maintaining a benzyloxy connected to the 3 carbon of a tropane. Various modifications include substitutions on the benzene ring, as well as additions or substitutions in place of the normal carboxylate on the tropane 2 carbon. Various compound with similar structure activity relationships to cocaine that aren't technically analogs have been developed as well.\nMechanisms of action.\nMost stimulants exert their activating effects by enhancing catecholamine neurotransmission. Catecholamine neurotransmitters are employed in regulatory pathways implicated in attention, arousal, motivation, task salience and reward anticipation. Classical stimulants either block the reuptake or stimulate the efflux of these catecholamines, resulting in increased activity of their circuits. Some stimulants, specifically those with entactogenic and hallucinogenic effects, also affect serotonergic transmission. Some stimulants, such as some amphetamine derivatives and, notably, yohimbine, can decrease negative feedback by antagonizing regulatory autoreceptors. Adrenergic agonists, such as, in part, ephedrine, act by directly binding to and activating adrenergic receptors, producing sympathomimetic effects. \nThere are also more indirect mechanisms of action by which a drug can elicit activating effects. Caffeine is an adenosine receptor antagonist, and only indirectly increases catecholamine transmission in the brain. Pitolisant is an histamine 3 (H3)-receptor inverse agonist. As histamine 3 (H3) receptors mainly act as autoreceptors, pitolisant decreases negative feedback to histaminergic neurons, enhancing histaminergic transmission.\nThe precise mechanism of action of some stimulants, such as modafinil, for treating symptoms of narcolepsy and other sleep disorders, remains unknown.\nNotable stimulants.\nAmphetamine.\nAmphetamine is a potent central nervous system (CNS) stimulant of the phenethylamine class that is approved for the treatment of attention deficit hyperactivity disorder (ADHD) and narcolepsy. Amphetamine is also used off-label as a performance and cognitive enhancer, and recreationally as an aphrodisiac and euphoriant. Although it is a prescription medication in many countries, unauthorized possession and distribution of amphetamine is often tightly controlled due to the significant health risks associated with uncontrolled or heavy use. As a consequence, amphetamine is illegally manufactured in clandestine labs to be trafficked and sold to users. Based upon drug and drug precursor seizures worldwide, illicit amphetamine production and trafficking is much less prevalent than that of methamphetamine.\nThe first pharmaceutical amphetamine was Benzedrine, a brand of inhalers used to treat a variety of conditions. Because the dextrorotary isomer has greater stimulant properties, Benzedrine was gradually discontinued in favor of formulations containing all or mostly dextroamphetamine. Presently, it is typically prescribed as mixed amphetamine salts, dextroamphetamine, and lisdexamfetamine.\nAmphetamine is a norepinephrine-dopamine releasing agent (NDRA). It enters neurons through dopamine and norepinephrine transporters and facilitates neurotransmitter efflux by activating TAAR1 and inhibiting VMAT2. At therapeutic doses, this causes emotional and cognitive effects such as euphoria, change in libido, increased arousal, and improved cognitive control. Likewise, it induces physical effects such as decreased reaction time, fatigue resistance, and increased muscle strength. In contrast, supratherapeutic doses of amphetamine are likely to impair cognitive function and induce rapid muscle breakdown. Very high doses can result in psychosis (e.g., delusions and paranoia), which very rarely occurs at therapeutic doses even during long-term use. As recreational doses are generally much larger than prescribed therapeutic doses, recreational use carries a far greater risk of serious side effects, such as dependence, which only rarely arises with therapeutic amphetamine use.\nCaffeine.\nCaffeine is a stimulant compound belonging to the xanthine class of chemicals naturally found in coffee, tea, and (to a lesser degree) cocoa or chocolate. It is included in many soft drinks, as well as a larger amount in energy drinks. Caffeine is the world's most widely used psychoactive drug and by far the most common stimulant. In North America, 90% of adults consume caffeine daily.\nA few jurisdictions restrict the sale and use of caffeine. In the United States, the FDA has banned the sale of pure and highly concentrated caffeine products for personal consumption, due to the risk of overdose and death. The Australian Government has announced a ban on the sale of pure and highly concentrated caffeine food products for personal consumption, following the death of a young man from acute caffeine toxicity. In Canada, Health Canada has proposed to limit the amount of caffeine in energy drinks to 180\u00a0mg per serving, and to require warning labels and other safety measures on these products.\nCaffeine is also included in some medications, usually for the purpose of enhancing the effect of the primary ingredient, or reducing one of its side-effects (especially drowsiness). Tablets containing standardized doses of caffeine are also widely available.\nCaffeine's mechanism of action differs from many stimulants, as it produces stimulant effects by inhibiting adenosine receptors. Adenosine receptors are thought to be a large driver of drowsiness and sleep, and their action increases with extended wakefulness. Caffeine has been found to increase striatal dopamine in animal models, as well as inhibit the inhibitory effect of adenosine receptors on dopamine receptors, however the implications for humans are unknown. Unlike most stimulants, caffeine has no addictive potential. Caffeine does not appear to be a reinforcing stimulus, and some degree of aversion may actually occur, per a study on drug abuse liability published in an NIDA research monograph that described a group preferring placebo over caffeine. In large telephone surveys only 11% reported dependence symptoms. However, when people were tested in labs, only half of those who claim dependence actually experienced it, casting doubt on caffeine's ability to produce dependence and putting societal pressures in the spotlight.\nCoffee consumption is associated with a lower overall risk of cancer. This is primarily due to a decrease in the risks of hepatocellular and endometrial cancer, but it may also have a modest effect on colorectal cancer. There does not appear to be a significant protective effect against other types of cancers, and heavy coffee consumption may increase the risk of bladder cancer. A protective effect of caffeine against Alzheimer's disease is possible, but the evidence is inconclusive. Moderate coffee consumption may decrease the risk of cardiovascular disease, and it may somewhat reduce the risk of type 2 diabetes. Drinking 1\u20133 cups of coffee per day does not affect the risk of hypertension compared to drinking little or no coffee. However those who drink 2\u20134 cups per day may be at a slightly increased risk. Caffeine increases intraocular pressure in those with glaucoma but does not appear to affect normal individuals. It may protect people from liver cirrhosis. There is no evidence that coffee stunts a child's growth. Caffeine may increase the effectiveness of some medications including ones used to treat headaches. Caffeine may lessen the severity of acute mountain sickness if taken a few hours prior to attaining a high altitude.\nEphedrine.\nEphedrine is a sympathomimetic amine similar in molecular structure to the well-known drugs phenylpropanolamine and methamphetamine, as well as to the important neurotransmitter epinephrine (adrenaline). Ephedrine is commonly used as a stimulant, appetite suppressant, concentration aid, and decongestant, and to treat hypotension associated with anesthesia.\nIn chemical terms, it is an alkaloid with a phenethylamine skeleton found in various plants in the genus \"Ephedra\" (family Ephedraceae). It works mainly by increasing the activity of norepinephrine (noradrenaline) on adrenergic receptors. It is most usually marketed as the \"hydrochloride\" or \"sulfate\" salt.\nThe herb \"m\u00e1 hu\u00e1ng\" (\"Ephedra sinica\"), used in traditional Chinese medicine (TCM), contains ephedrine and pseudoephedrine as its principal active constituents. The same may be true of other herbal products containing extracts from other \"Ephedra\" species.\nMDMA.\n3,4-Methylenedioxymethamphetamine (MDMA, ecstasy, or molly) is an entactogen, euphoriant, and stimulant of the amphetamine class. Briefly used by some psychotherapists as an adjunct to therapy, the drug became popular recreationally and the DEA listed MDMA as a Schedule I controlled substance, prohibiting most medical studies and applications. MDMA is known for its entactogenic properties. The stimulant effects of MDMA include hypertension, anorexia (appetite loss), euphoria, social disinhibition, insomnia (enhanced wakefulness/inability to sleep), improved energy, increased arousal, and increased perspiration, among others. Relative to catecholaminergic transmission, MDMA enhances serotonergic transmission significantly more, when compared to classical stimulants like amphetamine. MDMA does not appear to be significantly addictive or dependence forming.\nDue to the relative safety of MDMA, some researchers such as David Nutt have criticized the scheduling level, writing a satirical article finding MDMA to be 28 times less dangerous than horseriding, a condition he termed \"equasy\" or \"Equine Addiction Syndrome\".\nMDPV.\nMethylenedioxypyrovalerone (MDPV) is a psychoactive drug with stimulant properties that acts as a norepinephrine-dopamine reuptake inhibitor (NDRI). It was first developed in the 1960s by a team at Boehringer Ingelheim. MDPV remained an obscure stimulant until around 2004, when it was reported to be sold as a designer drug. Products labeled as bath salts containing MDPV were previously sold as recreational drugs in gas stations and convenience stores in the United States, similar to the marketing for Spice and K2 as incense.\nIncidents of psychological and physical harm have been attributed to MDPV use.\nMephedrone.\nMephedrone is a synthetic stimulant drug of the amphetamine and cathinone classes. Slang names include drone and MCAT. It is reported to be manufactured in China and is chemically similar to the cathinone compounds found in the khat plant of eastern Africa. It comes in the form of tablets or a powder, which users can swallow, snort, or inject, producing similar effects to MDMA, amphetamines, and cocaine.\nMephedrone was first synthesized in 1929, but did not become widely known until it was rediscovered in 2003. By 2007, mephedrone was reported to be available for sale on the Internet; by 2008 law enforcement agencies had become aware of the compound; and, by 2010, it had been reported in most of Europe, becoming particularly prevalent in the United Kingdom. Mephedrone was first made illegal in Israel in 2008, followed by Sweden later that year. In 2010, it was made illegal in many European countries, and, in December 2010, the EU ruled it illegal. In Australia, New Zealand, and the US, it is considered an analog of other illegal drugs and can be controlled by laws similar to the Federal Analog Act. In September 2011, the USA temporarily classified mephedrone as illegal, in effect from October 2011.\nMephedrone is neurotoxic and has abuse potential, predominantly exerted on 5-hydroxytryptamine (5-HT) terminals, mimicking that of MDMA with which it shares the same subjective sensations on abusers.\nMethamphetamine.\nMethamphetamine (contracted from \"N\"-methyl-alpha-methylphenethylamine) is a potent psychostimulant of the phenethylamine and amphetamine classes that is used to treat attention deficit hyperactivity disorder (ADHD) and obesity. Methamphetamine exists as two enantiomers, dextrorotary and levorotary. Dextromethamphetamine is a stronger CNS stimulant than levomethamphetamine; however, both are addictive and produce the same toxicity symptoms at high doses. Although rarely prescribed due to the potential risks, methamphetamine hydrochloride is approved by the United States Food and Drug Administration (USFDA) under the trade name \"Desoxyn\". Recreationally, methamphetamine is used to increase sexual desire, lift the mood, and increase energy, allowing some users to engage in sexual activity continuously for several days straight.\nMethamphetamine may be sold illicitly, either as pure dextromethamphetamine or in an equal parts mixture of the right- and left-handed molecules (i.e., 50%\u00a0levomethamphetamine and 50%\u00a0dextromethamphetamine). Both dextromethamphetamine and racemic methamphetamine are schedule II controlled substances in the United States. Also, the production, distribution, sale, and possession of methamphetamine is restricted or illegal in many other countries due to its placement in schedule II of the United Nations Convention on Psychotropic Substances treaty. In contrast, levomethamphetamine is an over-the-counter drug in the United States.\nIn low doses, methamphetamine can cause an elevated mood and increase alertness, concentration, and energy in fatigued individuals. At higher doses, it can induce psychosis, rhabdomyolysis, and cerebral hemorrhage. Methamphetamine is known to have a high potential for abuse and addiction. Recreational use of methamphetamine may result in psychosis or lead to post-withdrawal syndrome, a withdrawal syndrome that can persist for months beyond the typical withdrawal period. Unlike amphetamine and cocaine, methamphetamine is neurotoxic to humans, damaging both dopamine and serotonin neurons in the central nervous system (CNS). Unlike the long-term use of amphetamine in prescription doses, which may improve certain brain regions in individuals with ADHD, there is evidence that methamphetamine causes brain damage from long-term use in humans; this damage includes adverse changes in brain structure and function, such as reductions in gray matter volume in several brain regions and adverse changes in markers of metabolic integrity. However, recreational amphetamine doses may also be neurotoxic.\nMethylphenidate.\nMethylphenidate is a stimulant drug that is often used in the treatment of ADHD and narcolepsy and occasionally to treat obesity in combination with diet restraints and exercise. Its effects at therapeutic doses include increased focus, increased alertness, decreased appetite, decreased need for sleep and decreased impulsivity. Methylphenidate is not usually used recreationally, but when it is used, its effects are very similar to those of amphetamines.\nMethylphenidate acts as a norepinephrine-dopamine reuptake inhibitor (NDRI), by blocking the norepinephrine transporter (NET) and the dopamine transporter (DAT). Methylphenidate has a higher affinity for the dopamine transporter than for the norepinephrine transporter, and so its effects are mainly due to elevated dopamine levels caused by the inhibited reuptake of dopamine, however increased norepinephrine levels also contribute to various of the effects caused by the drug.\nMethylphenidate is sold under a number of brand names including Ritalin. Other versions include the long lasting tablet Concerta and the long lasting transdermal patch Daytrana.\nCocaine.\nCocaine is an SNDRI. Cocaine is made from the leaves of the coca shrub, which grows in the mountain regions of South American countries such as Bolivia, Colombia, and Peru, regions in which it was cultivated and used for centuries mainly by the Aymara people. In Europe, North America, and some parts of Asia, the most common form of cocaine is a white crystalline powder. Cocaine is a stimulant but is not normally prescribed therapeutically for its stimulant properties, although it sees clinical use as a local anesthetic, in particular in ophthalmology. Most cocaine use is recreational and its abuse potential is high (higher than amphetamine), and so its sale and possession are strictly controlled in most jurisdictions. Other tropane derivative drugs related to cocaine are also known such as troparil and lometopane but have not been widely sold or used recreationally.\nNicotine.\nNicotine is the active chemical constituent in tobacco, which is available in many forms, including cigarettes, cigars, chewing tobacco, and smoking cessation aids such as nicotine patches, nicotine gum, and electronic cigarettes. Nicotine is used widely throughout the world for its stimulating and relaxing effects. Nicotine exerts its effects through the agonism of nicotinic acetylcholine receptors, resulting in multiple downstream effects such as increase in activity of dopaminergic neurons in the midbrain reward system, and acetaldehyde one of the tobacco constituent decreased the expression of monoamine oxidase in the brain. Nicotine is addictive and dependence forming. Tobacco, the most common source of nicotine, has an overall harm to user and self score 3 percent below cocaine, and 13 percent above amphetamines, ranking 6th most harmful of the 20 drugs assessed, as determined by a multi-criteria decision analysis.\nPhenylpropanolamine.\nPhenylpropanolamine (PPA; Accutrim; \u03b2-hydroxyamphetamine), also known as the stereoisomers norephedrine and norpseudoephedrine, is a psychoactive drug of the phenethylamine and amphetamine chemical classes that is used as a stimulant, decongestant, and anorectic agent. It is commonly used in prescription and over-the-counter cough and cold preparations. In veterinary medicine, it is used to control urinary incontinence in dogs under trade names Propalin and Proin.\nIn the United States, PPA is no longer sold without a prescription due to a possible increased risk of stroke in younger women. In a few countries in Europe, however, it is still available either by prescription or sometimes over-the-counter. In Canada, it was withdrawn from the market on 31 May 2001. In India, human use of PPA and its formulations were banned on 10 February 2011.\nLisdexamfetamine.\nLisdexamfetamine (Vyvanse, etc.) is an amphetamine-type medication, sold for use in treating ADHD. Its effects typically last around 14 hours. Lisdexamfetamine is inactive on its own and is metabolized into dextroamphetamine in the body. Consequently, it has a lower abuse potential.\nPseudoephedrine.\nPseudoephedrine is a sympathomimetic drug of the phenethylamine and amphetamine chemical classes. It may be used as a nasal/sinus decongestant, as a stimulant, or as a wakefulness-promoting agent.\nThe salts pseudoephedrine hydrochloride and pseudoephedrine sulfate are found in many over-the-counter preparations, either as a single ingredient or (more commonly) in combination with antihistamines, guaifenesin, dextromethorphan, and/or paracetamol (acetaminophen) or another NSAID (such as aspirin or ibuprofen). It is also used as a precursor chemical in the illegal production of methamphetamine.\n\"Catha edulis\" (Khat).\nKhat is a flowering plant native to the Horn of Africa and the Arabian Peninsula.\nKhat contains a monoamine alkaloid called cathinone, a \"keto-amphetamine\". This alkaloid causes excitement, loss of appetite, and euphoria. In 1980, the World Health Organization (WHO) classified it as a drug of abuse that can produce mild to moderate psychological dependence (less than tobacco or alcohol), although the WHO does not consider khat to be seriously addictive. It is banned in some countries, such as the United States, Canada, and Germany, while its production, sale, and consumption are legal in other countries, including Djibouti, Ethiopia, Somalia, Kenya and Yemen.\nModafinil.\nModafinil is an eugeroic medication, which means that it promotes wakefulness and alertness. Modafinil is sold under the brand name Provigil among others. Modafinil is used to treat excessive daytime sleepiness due to narcolepsy, shift work sleep disorder, or obstructive sleep apnea. While it has seen off-label use as a purported cognitive enhancer, the research on its effectiveness for this use is not conclusive. Despite being a CNS stimulant, the addiction and dependence liabilities of modafinil are considered very low. Although modafinil shares biochemical mechanisms with stimulant drugs, it is less likely to have mood-elevating properties. The similarities in effects with caffeine are not clearly established. Unlike other stimulants, modafinil does not induce a subjective feeling of pleasure or reward, which is commonly associated with euphoria, an intense feeling of well-being. Euphoria is a potential indicator of drug abuse, which is the compulsive and excessive use of a substance despite adverse consequences. In clinical trials, modafinil has shown no evidence of abuse potential, that is why modafinil is considered to have a low risk of addiction and dependence, however, caution is advised.\nPitolisant.\nPitolisant is an inverse agonist (antagonist) of the histamine 3 (H3) autoreceptor. As such, pitolisant is an antihistamine medication that also belongs to the class of CNS stimulants. Pitolisant is also considered a medication of eugeroic class, which means that it promotes wakefulness and alertness. Pitolisant is the first wakefulness-promoting agent that acts by blocking the H3 autoreceptor.\nPitolisant has been shown to be effective and well-tolerated for the treatment of narcolepsy with or without cataplexy.\nPitolisant is the only non-controlled anti-narcoleptic drug in the US. It has shown minimal abuse risk in studies.\nBlocking the histamine 3 (H3) autoreceptor increases the activity of histamine neurons in the brain. The H3 autoreceptors regulate histaminergic activity in the central nervous system (and to a lesser extent, the peripheral nervous system) by inhibiting histamine biosynthesis and release upon binding to endogenous histamine. By preventing the binding of endogenous histamine at the H3, as well as producing a response opposite to that of endogenous histamine at the receptor (inverse agonism), pitolisant enhances histaminergic activity in the brain.\nSerotonin 5-HT2A receptor agonists.\nCertain serotonergic psychedelics and related non-hallucinogenic drugs, acting as serotonin 5-HT2A receptor agonists, have been reported to have mild stimulant and/or \"psychic energizing\" (i.e., acute antidepressant) effects, both in animals and humans. These effects are often present at low or sub-hallucinogenic doses. Psychedelics are also known to promote wakefulness or cause insomnia.\nPsychedelic and related drugs that have been reported to produce stimulant effects include the phenethylamines 2,5-DMA (DOH), DOM, DOET, DOPR, DON, MTFEM, Ariadne (4C-DOM; BL-3912; Dimoxamine), 2C-B, 2C-D, 2C-G-N, ASR-2001 (2CB-5PrO), 5-MeO-DiPT, and 5-MeO-MiPT, among others. The lysergamide LSD has also been reported to have mild stimulant effects. Conversely, psilocybin does not seem to produce the same stimulant effects. The non-hallucinogenic Ariadne was under development as a potential pharmaceutical drug to take advantage of such effects in the treatment of conditions like depression in the 1970s, and reached phase 3 clinical trials for such indications, but was shelved reportedly for strategic economic reasons. ASR-2001, which is likewise non-hallucinogenic, is under development for use as a stimulant-like medication for treatment of psychiatric disorders.\nSerotonin 5-HT2A receptor agonists have been found to increase dopamine levels in brain areas like the frontal cortex, striatum, and nucleus accumbens in animal studies. Relatedly, serotonin 5-HT2A receptor agonists are known to produce stimulant-like effects in animals such as hyperlocomotion (increased locomotor activity) and pro-motivational effects. The serotonin 5-HT2C receptor, which most psychedelics additionally activate to varying degrees, is known to have opposing effects on dopamine release and stimulant-related behavior, which may contribute to inverted U-shaped dose\u2013response relationships as well as divergent stimulant-like effects between different psychedelics.\nRecreational use and issues of abuse.\nStimulants enhance the activity of the central and peripheral nervous systems. Common effects may include increased alertness, awareness, wakefulness, endurance, productivity, and motivation, arousal, locomotion, heart rate, and blood pressure, and a diminished desire for food and sleep. Use of stimulants may cause the body to reduce significantly its production of natural body chemicals that fulfill similar functions. Until the body reestablishes its normal state, once the effect of the ingested stimulant has worn off the user may feel depressed, lethargic, confused, and miserable. This is referred to as a \"crash\", and may provoke reuse of the stimulant.\nAbuse of central nervous system (CNS) stimulants is common. Addiction to some CNS stimulants can quickly lead to medical, psychiatric, and psychosocial deterioration. Drug tolerance, dependence, and sensitization as well as a withdrawal syndrome can occur. Stimulants may be screened for in animal discrimination and self-administration models which have high sensitivity albeit low specificity. Research on a progressive ratio self-administration protocol has found amphetamine, methylphenidate, modafinil, cocaine, and nicotine to all have a higher break point than placebo that scales with dose indicating reinforcing effects. A progressive ratio self-administration protocol is a way of testing how much an animal or a human wants a drug by making them do a certain action (like pressing a lever or poking a nose device) to get the drug. The number of actions needed to get the drug increases every time, so it becomes harder and harder to get the drug. The highest number of actions that the animal or human is willing to do to get the drug is called the break point. The higher the break point, the more the animal or human wants the drug. In contrast to the classical stimulants such as amphetamine, the effects of modafinil depend on what the animals or humans have to do after getting the drug. If they have to do a performance task, like solving a puzzle or remembering something, modafinil makes them work harder for it than placebo, and the subjects wanted to self-administer modafinil. But if they had to do a relaxation task, like listening to music or watching a video, the subjects did not want to self-administer modafinil. This suggests that modafinil is more rewarding when it helps the animals or humans do something better or faster, especially considering that modafinil is not commonly abused or depended on by people, unlike other stimulants.\nTreatment for misuse.\nPsychosocial treatments, such as contingency management, have demonstrated improved effectiveness when added to treatment as usual consisting of counseling and/or case-management. This is demonstrated with a decrease in dropout rates and a lengthening of periods of abstinence.\nTesting.\nThe presence of stimulants in the body may be tested by a variety of procedures. Serum and urine are the common sources of testing material although saliva is sometimes used. Commonly used tests include chromatography, immunologic assay, and mass spectrometry.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66392", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=66392", "title": "Cardiopulmonary resuscitation", "text": "Emergency procedure after sudden cardiac arrest\nCardiopulmonary resuscitation (CPR) is an emergency procedure used during cardiac or respiratory arrest that involves chest compressions, often combined with artificial ventilation, to preserve brain function and maintain circulation until spontaneous breathing and heartbeat can be restored. It is recommended for those who are unresponsive with no breathing or abnormal breathing, for example, agonal respirations.\nCPR involves chest compressions for adults between and deep and at a rate of at least 100 to 120 per minute. The rescuer may also provide artificial ventilation by either exhaling air into the subject's mouth or nose (mouth-to-mouth resuscitation) or using a device that pushes air into the subject's lungs (mechanical ventilation). Current recommendations emphasize early and high-quality chest compressions over artificial ventilation; a simplified CPR method involving only chest compressions is recommended for untrained rescuers. With children, however, 2015 American Heart Association guidelines indicate that doing only compressions may result in worse outcomes, because such problems in children normally arise from respiratory issues rather than from cardiac ones, given their young age. Chest compression to breathing ratios are set at 30 to 2 in adults.\nCPR alone is unlikely to restart the heart. Its main purpose is to restore the partial flow of oxygenated blood to the brain and heart. The objective is to delay tissue death and to extend the brief window of opportunity for a successful resuscitation without permanent brain damage. Administration of an electric shock to the subject's heart, termed defibrillation, is usually needed to restore a viable, or \"perfusing\", heart rhythm. Defibrillation is effective only for certain heart rhythms, namely ventricular fibrillation or pulseless ventricular tachycardia, rather than asystole or pulseless electrical activity, which usually requires the treatment of underlying conditions to restore cardiac function. Early shock, when appropriate, is recommended. CPR may succeed in inducing a heart rhythm that may be shockable. In general, CPR is continued until the person has a return of spontaneous circulation (ROSC) or is declared dead.\nMedical uses.\nCPR is indicated for any person unresponsive with no breathing or breathing only in occasional agonal gasps, as it is most likely that they are in cardiac arrest. If a person still has a pulse but is not breathing (respiratory arrest), artificial ventilations may be more appropriate, but due to the difficulty people have in accurately assessing the presence or absence of a pulse, CPR guidelines recommend that lay persons should not be instructed to check the pulse while giving healthcare professionals the option to check a pulse. In those with cardiac arrest due to trauma, CPR is considered futile but still recommended. Correcting the underlying cause such as a tension pneumothorax or pericardial tamponade may help.\nPathophysiology.\nCPR is used on people in cardiac arrest to oxygenate the blood and maintain a cardiac output to keep vital organs alive. Blood circulation and oxygenation are required to transport oxygen to the tissues. The physiology of CPR involves generating a pressure gradient between the arterial and venous vascular beds; CPR achieves this via multiple mechanisms.\nThe brain may sustain damage after blood flow has been stopped for about four minutes and irreversible damage after about seven minutes. Typically if blood flow ceases for one to two hours, then body cells die. Therefore, in general, CPR is effective only if performed within seven minutes of the stoppage of blood flow. The heart also rapidly loses the ability to maintain a normal rhythm. Low body temperatures, as sometimes seen in near-drownings, prolong the time the brain survives.\nFollowing cardiac arrest, effective CPR enables enough oxygen to reach the brain to delay brain stem death and allows the heart to remain responsive to defibrillation attempts. If an incorrect compression rate is used during CPR, going against standing American Heart Association (AHA) guidelines of 100\u2013120 compressions per minute, this can cause a net decrease in venous return of blood, for what is required, to fill the heart. For example, if a compression rate of above 120 compressions per minute is used consistently throughout the entire CPR process, this error could adversely affect survival rates and outcomes for the victim.\nOrder of CPR in a first aid sequence.\nThe best position for CPR maneuvers in the sequence of first aid reactions to a cardiac arrest is a question that has been long studied.\nAs a general reference, the recommended order (according to the guidelines of many related associations such as AHA and Red Cross) is:\nIf there are multiple rescuers, these tasks can be distributed and performed simultaneously to save time.\nException to the main sequence.\nThe reason is that CPR ventilation (rescue breaths) is considered the most important action for those victims. Cardiac arrest in drowning victims originates from a lack of oxygen, and a child would probably not suffer from cardiac diseases.\nMethods.\nIn 2010, the AHA and International Liaison Committee on Resuscitation updated their CPR guidelines. The importance of high quality CPR (sufficient rate and depth without excessively ventilating) was emphasized. The order of interventions was changed for all age groups except newborns from airway, breathing, chest compressions (ABC) to chest compressions, airway, breathing (CAB). An exception to this recommendation is for those believed to be in a respiratory arrest (airway obstruction, drug overdose, etc.).\nThe most important aspects of CPR are: few interruptions of chest compressions, sufficient speed and depth of compressions, completely relaxing pressure between compressions, and not ventilating too much. It is unclear if a few minutes of CPR before defibrillation results in different outcomes than immediate defibrillation.\nCompressions with rescue breaths.\nA normal CPR procedure uses chest compressions and ventilations (rescue breaths, usually mouth-to-mouth) for any victim of cardiac arrest, who would be unresponsive (usually unconscious or approximately unconscious), not breathing, or only gasping because of the lack of heartbeats. But the ventilations could be omitted for untrained rescuers aiding adults who suffer a cardiac arrest (if it is not an asphyxial cardiac arrest, as by drowning, which needs ventilations). There has been evidence of increased effectiveness of CPR when the time between bouts of 30 compressions is limited.The patient's head is commonly tilted back (a head-tilt and chin-lift position) for improving the airflow if ventilations can be used. However, when a patient seems to have a possible serious injury in the spinal cord (in the backbone, either at the neck part or the back part), the head must not be moved except if that is completely necessary, and always very carefully, which avoids further damages for the patient's mobility in the future. And, in the case of babies, the head is left straight, looking forward, which is necessary for the ventilations, because of the size of the baby's neck.In CPR, the chest compressions push on the lower half of the sternum \u2014the bone that is along the middle of the chest from the neck to the belly\u2014 and leave it to rise up until recovering its normal position. The rescue breaths are made by pinching the victim's nose and blowing air mouth-to-mouth. This fills the lungs, which makes the chest rise up, and increases the pressure into the thoracic cavity. If the victim is a baby, the rescuer would compress the chest with only 2 fingers and would make the ventilations using their own mouth to cover the baby's mouth and nose at the same time. The recommended compression-to-ventilation ratio, for all victims of any age, is 30:2 (a cycle that alternates continually 30 rhythmic chest compressions series and 2 rescue breaths series). Victims of drowning receive an initial series of 2 rescue breaths before that cycle begins.\nAs an exception for the normal compression-to-ventilation ratio of 30:2, if at least two trained rescuers are present and the victim is a child, the preferred ratio is 15:2. Equally, in newborns, the ratio is 30:2 if one rescuer is present, and 15:2 if two rescuers are present (according to the AHA 2015 Guidelines). In an advanced airway treatment, such as an endotracheal tube or laryngeal mask airway, the artificial ventilation should occur without pauses in compressions at a rate of 1 breath every 6 to 8 seconds (8\u201310 ventilations per minute).\nIn all victims, the compression speed is at least 100 compressions per minute. Recommended compression depth in adults and children is of 5\u00a0cm (2 inches), and in infants it is 4\u00a0cm (1.6 inches). In adults, rescuers should use two hands for the chest compressions (one on top of the other), while in children one hand could be enough (or two, adapting the compressions to the child's constitution), and with babies the rescuer must use only two fingers.\nThere exist some plastic shields and respirators that can be used in the rescue breaths between the mouths of the rescuer and the victim, with the purposes of sealing a better vacuum and avoiding infections.\nIn some cases, the problem is one of the failures in the rhythm of the heart (ventricular fibrillation and ventricular tachycardia) that can be corrected with the electric shock of a defibrillator. So, if a victim is suffering a cardiac arrest, it is important that someone asks for a defibrillator nearby, to try with it a defibrillation process when the victim is already unconscious. The common model of a defibrillator (the AED) is an automatic portable machine that guides the user with recorded voice instructions along the process, analyzes the victim, and applies the correct shocks if they are needed.\nThe time in which a cardiopulmonary resuscitation can still work is unclear, and it depends on many factors. Many official guides recommend continuing cardiopulmonary resuscitation until emergency medical services arrive (for trying to keep the patient alive, at least). The same guides also indicate asking for any emergency defibrillator (AED) near, to try an automatic defibrillation as soon as possible before considering that the patient has died.\nA normal cardiopulmonary resuscitation has a recommended order named 'CAB': first 'Chest' (chest compressions), followed by 'Airway' (attempt to open the airway by performing a head tilt and a chin lift), and 'Breathing' (rescue breaths). As of 2010, the Resuscitation Council (UK) was still recommending an 'ABC' order, with the 'C' standing for 'Circulation' (check for a pulse), if the victim is a child. It can be difficult to determine the presence or absence of a pulse, so the pulse check has been removed for common providers and should not be performed for more than 10 seconds by healthcare providers.\nCompression only.\nFor untrained rescuers helping adult victims of cardiac arrest, it is recommended to perform compression-only CPR (chest compressions hands-only or cardiocerebral resuscitation, without artificial ventilation), as it is easier to perform and instructions are easier to give over a phone. In adults with out-of-hospital cardiac arrest, compression-only CPR by the average person has an equal or higher success rate than standard CPR.\nThe CPR 'compressions only' procedure consists only of chest compressions that push on the lower half of the bone that is in the middle of the chest (the sternum).\nCompression-only CPR is not as good for children who are more likely to have cardiac arrest from respiratory causes. Two reviews have found that compression-only CPR had no more success than no CPR whatsoever. Rescue breaths for children and especially for babies should be relatively gentle. Either a ratio of compressions to breaths of 30:2 or 15:2 was found to have better results for children. Both children and adults should receive 100 chest compressions per minute. Other exceptions besides children include cases of drownings and drug overdose; in both these cases, compressions, and rescue breaths are recommended if the bystander is trained and is willing to do so.\nAs per the AHA, the beat of the Bee Gees song \"Stayin' Alive\" provides an ideal rhythm in terms of beats per minute to use for hands-only CPR, which is 104 beats-per-minute. One can also hum Queen's \"Another One Bites the Dust\", which is 110 beats-per-minute and contains a repeating drum pattern. For those in cardiac arrest due to non-heart related causes and in people less than 20 years of age, standard CPR is superior to compression-only CPR.\nProne CPR.\nStandard CPR is performed with the victim in supine position. Prone CPR, or reverse CPR, is performed on a victim in prone position, lying on the chest. This is achieved by turning the head to the side and compressing the back. Due to the head being turned, the risk of vomiting and complications caused by aspiration pneumonia may be reduced.\nThe American Heart Association's current guidelines recommend performing CPR in the supine position and limiting prone CPR to situations where the patient cannot be turned.\nPregnancy.\nDuring pregnancy when a woman is lying on her back, the uterus may compress the inferior vena cava and thus decrease venous return. It is therefore recommended that the uterus be pushed to the woman's left. This can be done by placing a pillow or towel under her right hip so that she is on an angle of 15\u201330 degrees, and making sure their shoulders are flat to the ground. If this is not effective, healthcare professionals should consider emergency resuscitative hysterotomy.\nFamily presence.\nEvidence generally supports family being present during CPR. This includes in CPR for children.\nOther.\nInterposed abdominal compressions may be beneficial in the hospital environment. There is no evidence of benefit pre-hospital or in children.\nCooling during CPR is being studied as currently, results are unclear whether or not it improves outcomes.\nInternal cardiac massage is the manual squeezing of the exposed heart itself carried out through a surgical incision into the chest cavity, usually when the chest is already open for cardiac surgery.\nActive compression-decompression methods using mechanical decompression of the chest have not been shown to improve outcomes in cardiac arrest.\nUse of devices.\nDefibrillators.\nA defibrillator is a machine that produces defibrillation: electric shocks that can restore the normal heart function of the victim. The common model of a defibrillator out of a hospital is the automated external defibrillator (AED), a portable device that is especially easy to use because it produces recorded voice instructions.\nDefibrillation is only indicated for some arrhythmias (abnormal heart beatings), specifically ventricular fibrillation (VF) and pulseless ventricular tachycardia (VT). Defibrillation is not indicated if the patient has a normal pulse or is still conscious. Also, it is not indicated in asystole or pulseless electrical activity (PEA), in those cases a normal CPR would be used to oxygenate the brain until the heart function can be restored. Improperly given electrical shocks can cause dangerous arrhythmias, such as the ventricular fibrillation (VF).\nWhen a patient does not have heart beatings (or they present a sort of arrhythmia that will stop the heart immediately), it is recommended that someone asks for a defibrillator (because they are quite common in the present time), for trying with it a defibrillation on the already unconscious victim, in case it is successful.\nOrder of defibrillation in a first aid sequence\nIt is recommended to call for emergency medical services before a defibrillation. Afterward, a nearby AED defibrillator should be used on the patient as soon as possible. As a general reference, defibrillation is preferred to performing CPR, but only if the AED can be retrieved in a short period of time. All these tasks (calling by phone, getting an AED, and the chest compressions and rescue breaths maneuvers of CPR) can be distributed between many rescuers who make them simultaneously. The defibrillator itself would indicate if more CPR maneuvers are required.\nAs a slight variation for that sequence, if the rescuer is completely alone with a victim of drowning, or with a child who was already unconscious when the rescuer arrived, the rescuer would do the CPR maneuvers during 2 minutes (approximately 5 cycles of ventilations and compressions); after that, the rescuer would call to emergency medical services, and then it could be tried a search for a defibrillator nearby (the CPR maneuvers are supposed to be the priority for the drowned and most of the already collapsed children).\nAs another possible variation, if a rescuer is completely alone and without a phone nearby, and is aiding any other victim (not a victim of drowning, nor an already unconscious child), the rescuer would go to call by phone first. After the call, the rescuer would get a nearby defibrillator and use it, or continue the CPR (the phone call and the defibrillator are considered urgent when the problem has a cardiac origin).\nDefibrillation\nThe standard defibrillation device, prepared for fast use out of the medical centers, is the automated external defibrillator (AED), a portable machine of small size (similar to a briefcase) that can be used by any user with no previous training. That machine produces recorded voice instructions that guide the user along the defibrillation process. It also checks the victim's condition to automatically apply electric shocks at the correct level, if they are needed. Other models are semi-automatic and require the user to push a button before an electric shock.\nA defibrillator may ask for applying CPR maneuvers, so the patient would be placed lying in a face-up position. Additionally, the patient's head would be tilted back, except in the case of babies.\nWater and metals transmit the electric current. This depends on the amount of water, but it is convenient to avoid starting the defibrillation on a floor with puddles and to dry the wet areas of the patient before (fast, even with any cloth, if that is enough). It is not necessary to remove the patient's jewels or piercings, but it should be avoided placing the patches of the defibrillator directly on top of them. The patches with electrodes are put on the positions that appear at the right. In very small bodies: children between 1 and 8 years, and, in general, similar bodies up to 25\u00a0kg approximately, it is recommended the use of children's size patches with reduced electric doses. If that is not possible, sizes and doses for adults would be used, and, if the patches were too big, one would be placed on the chest and the other on the back (no matter which of them).\nThere are several devices for improving CPR, but only defibrillators (as of 2010) have been found better than standard CPR for an out-of-hospital cardiac arrest.\nWhen a defibrillator has been used, it should remain attached to the patient until emergency services arrive.\nDevices for timing CPR.\nTiming devices can feature a metronome (an item carried by many ambulance crews) to assist the rescuer in achieving the correct rate. Some units can also give timing reminders for performing compressions, ventilating, and changing operators.\nDevices for assisting in manual CPR.\nMechanical chest compression devices are not better than standard manual compressions. Their use is reasonable in situations where manual compressions are not safe to perform, such as in a moving vehicle.\nAudible and visual prompting may improve the quality of CPR and prevent the decrease of compression rate and depth that naturally occurs with fatigue, and to address this potential improvement, a number of devices have been developed to help improve CPR technique.\nThese items can be devices to be placed on top of the chest, with the rescuer's hands going over the device, and a display or audio feedback giving information on depth, force or rate, or in a wearable format such as a glove. Several published evaluations show that these devices can improve the performance of chest compressions.\nAs well as its use during actual CPR on a cardiac arrest victim, which relies on the rescuer carrying the device with them, these devices can also be used as part of training programs to improve basic skills in performing correct chest compressions.\nDevices for providing automatic CPR.\nMechanical CPR has not seen as much use as mechanical ventilation; however, use in the prehospital setting is increasing. Devices on the market include the LUCAS device, developed at the University Hospital of Lund, and AutoPulse. Both use straps around the chest to secure the patient. The first generation of the LUCAS uses a gas-driven piston and motor-driven constricting band, while later versions are battery-operated.\nThere are several advantages to automated devices: they allow rescuers to focus on performing other interventions; they do not fatigue and begin to perform less effective compressions, as humans do; they can perform effective compressions in limited-space environments such as air ambulances, where manual compressions are difficult, and they allow ambulance workers to be strapped in safely rather than standing over a patient in a speeding vehicle. However the disadvantages are cost to purchase, time to train emergency personnel to use them, interruption to CPR to implement, potential for incorrect application and the need for multiple device sizes.\nSeveral studies have shown little or no improvement in survival rates but acknowledge the need for more study.\nMobile apps for providing CPR instructions.\nTo support training and incident management, mobile apps have been published in the largest app markets. An evaluation of 61 available apps has revealed that a large number do not follow international guidelines for basic life support and many apps are not designed in a user-friendly way. As a result, the Red Cross updated and endorsed its emergency preparedness application, which uses pictures, text, and videos to assist the user. The UK Resuscitation Council has an app, called Lifesaver, which shows how to perform CPR.\nEffectivity rate.\nCPR oxygenates the body and brain, which favors making a later defibrillation and the advanced life support. Even in the case of a \"non-shockable\" rhythm, such as pulseless electrical activity (PEA) where defibrillation is not indicated, effective CPR is no less important. Used alone, CPR will result in few complete recoveries, though the outcome without CPR is almost uniformly fatal.\nStudies have shown that immediate CPR followed by defibrillation within 3\u20135 minutes of sudden VF cardiac arrest dramatically improves survival. In cities such as Seattle where CPR training is widespread and defibrillation by EMS personnel follows quickly, the survival rate is about 20 percent for all causes and as high as 57 percent for a witnessed \"shockable\" arrest. In cities such as New York, without those advantages, the survival rate is only 5 percent for witnessed shockable arrest. Similarly, in-hospital CPR is more successful when arrests are witnessed, occur in the ICU, or occur in patients wearing heart monitors.\n\\* AED data here exclude health facilities and nursing homes, where patients are sicker than average.\nIn adults compression-only CPR by bystanders appears to be better than chest compressions with rescue breathing. Compression-only CPR may be less effective in children than in adults, as cardiac arrest in children is more likely to have a non-cardiac cause. In a 2010 prospective study of cardiac arrest in children (age 1\u201317) for arrests with a non-cardiac cause, provision by bystanders of conventional CPR with rescue breathing yielded a favorable neurological outcome at one month more often than did compression-only CPR (OR 5.54). For arrests with a cardiac cause in this cohort, there was no difference between the two techniques (OR 1.20). This is consistent with American Heart Association guidelines for parents.\nWhen done by trained responders, 30 compressions interrupted by two breaths appears to have a slightly better result than continuous chest compressions with breaths being delivered while compressions are ongoing.\nMeasurement of end-tidal carbon dioxide during CPR reflects cardiac output and can predict chances of ROSC.\nIn a study of in-hospital CPR from 2000 to 2008, 59% of CPR survivors lived over a year after hospital discharge and 44% lived over 3 years.\nConsequences.\nSurvival rates: In US hospitals in 2017, 26% of patients who received CPR survived to hospital discharge.\nIn 2017 in the US, outside hospitals, 16% of people whose cardiac arrest was witnessed survived to hospital discharge.\nSince 2003, widespread cooling of patients after CPR\nand other improvements have raised survival and reduced mental disabilities.\nOrgan donation.\nOrgan donation is usually made possible by CPR, even if CPR does not save the patient. If there is a return of spontaneous circulation (ROSC), all organs can be considered for donation. If the patient does not achieve ROSC, and CPR continues until an operating room is available, the kidneys and liver can still be considered for donation.\n1,000 organs per year in the US are transplanted from patients who had CPR.\nDonations can be taken from 40% of patients who have ROSC and later become brain-dead.\nUp to 8 organs can be taken from each donor,\nand an average of 3 organs are taken from each patient who donates organs.\nMental abilities.\nMental abilities are about the same for survivors before and after CPR for 89% of patients, based on before and after counts of 12,500 US patients' Cerebral-Performance Category (CPC) codes in a 2000\u20132009 study of CPR in hospitals. 1% more survivors were in comas than before CPR. 5% more needed help with daily activities. 5% more had moderate mental problems and could still be independent.\nFor CPR outside hospitals, a Copenhagen study of 2,504 patients in 2007-2011 found that 21% of survivors developed moderate mental problems but could still be independent, and 11% of survivors developed severe mental problems, so they needed daily help. Two patients out of 2,504 went into comas (0.1% of patients, or 2 out of 419 survivors, 0.5%), and the study did not track how long the comas lasted.\nMost people in comas start to recover in 2\u20133 weeks. 2018 guidelines on disorders of consciousness say it is no longer appropriate to use the term \"permanent vegetative state.\" Mental abilities can continue to improve in the six months after discharge, and in subsequent years. For long-term problems, brains form new paths to replace damaged areas.\nInjuries.\nInjuries from CPR vary. 87% of patients are not injured by CPR. Overall, injuries are caused in 13% (2009\u201312 data) of patients, including broken sternum or ribs (9%), lung injuries (3%), and internal bleeding (3%).\nThe internal injuries counted here can include heart contusion, hemopericardium, upper airway complications, damage to the abdominal viscera\u00a0\u2212 lacerations of the liver and spleen, fat emboli, pulmonary complications\u00a0\u2212 pneumothorax, hemothorax, lung contusions. Most injuries did not affect care; only 1% of those given CPR received life-threatening injuries from it.\nBroken ribs are present in 3%\nof those who survive to hospital discharge, and 15% of those who die in the hospital, for an average rate of 9% (2009-12 data)\nto 8% (1997\u201399).\nIn the 2009-12 study, 20% of survivors were older than 75. A study in the 1990s found 55% of CPR patients who died before discharge had broken ribs, and a study in the 1960s found 97% did; training and experience levels have improved. Lung injuries were caused in 3% of patients and other internal bleeding in 3% (2009\u201312).\nBones heal in 1\u20132 months.\nThe costal cartilage also breaks in an unknown number of additional cases, which can sound like breaking bones.\nThe type and frequency of injury can be affected by factors such as sex and age. A 1999 Austrian study of CPR on cadavers, using a machine that alternately compressed the chest and then pulled it outward, found a higher rate of sternal fractures in female cadavers (9 of 17) than males (2 of 20), and found the risk of rib fractures rose with age, though they did not say how much.\nChildren and infants have a low risk of rib fractures during CPR, with an incidence of less than 2%, although, when they do occur, they are usually anterior and multiple.\nWhere CPR is performed in error by a bystander, on a person not in cardiac arrest, around 2% have injury as a result (although 12% experienced discomfort).\nA 2004 overview said, \"Chest injury is a price worth paying to achieve optimal efficacy of chest compressions. Cautious or faint-hearted chest compression may save bones in the individual case but not the patient's life.\"\nOther side effects.\nThe most common side effect is vomiting, which necessitates clearing the mouth so patients do not breathe it in.\nIt happened in 16 of 35 CPR efforts in a 1989 study in King County, Washington.\nSurvival differences, based on prior illness, age or location.\nThe American Heart Association guidelines say that survival rates below 1% are \"futility,\" but all groups have better survival than that. Even among very sick patients, at least 10% survive: A study of CPR in a sample of US hospitals from 2001 to 2010, where overall survival was 19%, found 10% survival among cancer patients, 12% among dialysis patients, 14% over age 80, 15% among blacks, 17% for patients who lived in nursing homes, 19% for patients with heart failure, and 25% for patients with heart monitoring outside the ICU.\nAnother study, of advanced cancer patients, found the same 10% survival mentioned above.\nA study of Swedish patients in 2007\u20132015 with ECG monitors found 40% survived at least 30 days after CPR at ages 70\u201379, 29% at ages 80\u201389, and 27% above age 90.\nAn earlier study of Medicare patients in hospitals from 1992 to 2005, where overall survival was 18%, found 13% survival in the poorest neighborhoods, 12% survival over age 90, 15% survival among ages 85\u201389, and 17% survival among ages 80\u201384.\nSwedish patients 90 years or older had 15% survival to hospital discharge, 80\u201389 had 20%, and 70\u201379 had 28%.\nA study of King County WA patients who had CPR outside hospitals in 1999\u20132003, where 34% survived to hospital discharge overall, found that among patients with 4 or more major medical conditions, 18% survived; with 3 major conditions 24% survived, and 33% of those with 2 major medical conditions survived.\nNursing home residents' survival has been studied by several authors,\nand is measured annually by the Cardiac Arrest Registry to Enhance Survival (CARES). CARES reports CPR results from a catchment area of 115 million people, including 23 state-wide registries, and individual communities in 18 other states as of 2019. CARES data show that in health care facilities and nursing homes where AEDs are available and used, survival rates are double the average survival found in nursing homes overall.\nGeographically, there is wide variation from state to state in survival after CPR \"in US hospitals,\" from 40% in Wyoming to 20% in New York, so there is room for good practices to spread, raising the averages.\nFor CPR \"outside hospitals\", survival varies even more across the US, from 3% in Omaha to 45% in Seattle in 2001. This study only counted heart rhythms that can respond to defibrillator shocks (tachycardia).\nA major reason for the variation has been the delay in some areas between the call to emergency services and the departure of medics, and then arrival and treatment. Delays were caused by a lack of monitoring, and the mismatch between recruiting people as firefighters, though most emergency calls they are assigned to are medical, so staff resisted and delayed on the medical calls. Building codes have cut the number of fires, but staff still think of themselves as firefighters.\nDysthanasia.\nIn some instances, CPR can be considered a form of dysthanasia.\nPrevalence.\nChance of receiving CPR.\nVarious studies show that in out-of-home cardiac arrest, bystanders in the US attempt CPR in between 14% and 45% of the time, with a median of 32%. Globally, rates of bystander CPR are reported to be as low as 1% and as high as 44%. However, the effectiveness of this CPR is variable, and the studies suggest only around half of bystander CPR is performed correctly. One study found that members of the public having received CPR training in the past lack the skills and confidence needed to save lives. The report's authors suggested that better training is needed to improve the willingness to respond to cardiac arrest. Factors that influence bystander CPR in out-of-hospital cardiac arrest include:\nThere is a relation between age and the chance of CPR being commenced. Younger people are far more likely to have CPR attempted on them before the arrival of emergency medical services. Bystanders more commonly administer CPR when in public than when at the person's home, although healthcare professionals are responsible for more than half of out-of-hospital resuscitation attempts. People with no connection to the person are more likely to perform CPR than family members.\nThere is also a clear relation between the cause of arrest and the likelihood of a bystander initiating CPR. Laypersons are most likely to give CPR to younger people in cardiac arrest in a public place when it has a medical cause; those in arrest from trauma, exsanguination or intoxication are less likely to receive CPR.\nIt is believed that there is a higher chance that CPR will be performed if the bystander is told to perform only the chest compression element of the resuscitation.\nThe first formal study into gender bias in receiving CPR from the public versus professionals was conducted by the American Heart Association and the National Institutes of Health (NIH), and examined nearly 20,000 cases across the U.S. The study found that women are six percent less likely than men to receive bystander CPR when in cardiac arrest in a public place, citing the disparity as \"likely due to the fear of being falsely accused of sexual assault.\"\nChance of receiving CPR in time.\nCPR is likely to be effective only if commenced within 6 minutes after the blood flow stops because permanent brain cell damage occurs when fresh blood infuses the cells after that time, since the cells of the brain become dormant in as little as 4\u20136 minutes in an oxygen-deprived environment and, therefore, cannot survive the reintroduction of oxygen in a traditional resuscitation. Research using cardioplegic blood infusion resulted in a 79.4% survival rate with cardiac arrest intervals of 72\u00b143 minutes, traditional methods achieve a 15% survival rate in this scenario, by comparison. New research is currently needed to determine what role CPR, defibrillation, and new advanced gradual resuscitation techniques will have with this new knowledge.\nA notable exception is cardiac arrest which occurs in conjunction with exposure to very cold temperatures. Hypothermia seems to protect by slowing down metabolic and physiologic processes, greatly decreasing the tissues' need for oxygen. There are cases where CPR, defibrillation, and advanced warming techniques have revived victims after substantial periods of hypothermia.\nSociety and culture.\nPortrayed effectiveness.\nCPR is often severely misrepresented in movies and television as being highly effective in resuscitating a person who is not breathing and has no circulation.\nA 1996 study published in the New England Journal of Medicine showed that CPR success rates in television shows were 75% for immediate circulation, and 67% survival to discharge. This gives the general public an unrealistic expectation of a successful outcome. When educated on the actual survival rates, the proportion of patients over 60 years of age desiring CPR should they have a cardiac arrest drops from 41% to 22%.\nTraining and stage CPR.\nIt is dangerous to perform CPR on a person who is breathing normally. These chest compressions create significant local blunt trauma, risking bruising or fracture of the sternum or ribs. If a patient is not breathing, these risks still exist but are dwarfed by the immediate threat to life. For this reason, training is always done with a mannequin, such as the well-known Resusci Anne model.\nThe portrayal of the CPR technique on television and film often is purposely incorrect. Actors simulating the performance of CPR may bend their elbows while appearing to compress, to prevent force from reaching the chest of the actor portraying the patient.\nSelf-CPR hoax.\nA form of \"self-CPR\" termed \"cough CPR\" was the subject of a hoax chain e-mail entitled \"How to Survive a Heart Attack When Alone,\" which wrongly cited \"Via Health Rochester General Hospital\" as the source of the technique. Rochester General Hospital has denied any connection with the technique.\n\"Cough CPR\" in the sense of \"resuscitating\" oneself is impossible because a prominent symptom of cardiac arrest is unconsciousness, which makes coughing impossible.\nThe American Heart Association (AHA) and other resuscitation bodies do not endorse \"cough CPR\", which it terms a misnomer as it is not a form of \"resuscitation\". The AHA does recognize a limited legitimate use of the coughing technique: \"This coughing technique to maintain blood flow during brief arrhythmias has been useful in the hospital, particularly during cardiac catheterization. In such cases, the patient's ECG is monitored continuously, and a physician is present.\" When coughing is used on trained and monitored patients in hospitals, it is effective only for 90 seconds.\nLearning from film.\nIn at least one case, it has been alleged that CPR learned from a film was used to save a person's life. In April 2011, it was claimed that nine-year-old Tristin Saghin saved his sister's life by administering CPR on her after she fell into a swimming pool, using only the knowledge of CPR that he had gleaned from a motion picture, \"Black Hawk Down.\"\nHands-only CPR portrayal.\nLess than 1/3 of those people who experience a cardiac arrest at home, work, or in a public location have CPR performed on them. Most bystanders are worried that they might do something wrong. On October 28, 2009, the American Heart Association and the Ad Council launched a hands-only CPR public service announcement and website as a means to address this issue. In July 2011, new content was added to the website including a digital app that helps a user learn how to perform hands-only CPR.\nHistory.\nIn the 19th century, Doctor H. R. Silvester described a method (the Silvester method) of artificial ventilation in which the patient is laid on their back, and their arms are raised above their head to aid inhalation and then pressed against their chest to aid exhalation. The Holger Nielsen technique of artificial respiration, developed by Danish physician Holger Nielsen, revolutionized the field of emergency medical care. Introduced in the early 20th century, this technique involved positioning the patient in a supine position (lying flat on their back) and the performer of the technique kneeling beside or above the patient. The Holger Nielsen technique utilized a manual resuscitator, commonly referred to as the \"Holger Nielsen bag,\" to administer rescue breaths. The performer would place a mask or the bag's mouthpiece over the patient's mouth and nose while manually compressing the bag. This action would deliver a controlled flow of air into the patient's lungs, aiding in oxygenation and facilitating the exchange of gases.\nIt was not until the middle of the 20th century that the wider medical community started to recognize and promote artificial ventilation in the form of mouth-to-mouth resuscitation combined with chest compressions as a key part of resuscitation following cardiac arrest. The combination was first seen in a 1962 training video called \"The Pulse of Life\" created by James Jude, Guy Knickerbocker, and Peter Safar. Jude and Knickerbocker, along with William Kouwenhoven and Joseph S. Redding had recently discovered the method of external chest compressions, whereas Safar had worked with Redding and James Elam to prove the effectiveness of mouth-to-mouth resuscitation. The first effort at testing the technique was performed on a dog by Redding, Safar, and JW Pearson. Soon afterward, the technique was used to save the life of a child. Their combined findings were presented at the annual Maryland Medical Society meeting on September 16, 1960, in Ocean City, and gained widespread acceptance over the following decade, helped by the video and speaking tour they undertook. Peter Safar wrote the book \"ABC of Resuscitation\" in 1957. In the U.S., it was first promoted as a technique for the public to learn in the 1970s.\nMouth-to-mouth resuscitation was combined with chest compressions based on the assumption that active ventilation is necessary to keep circulating blood oxygenated, and the combination was accepted without comparing its effectiveness with chest compressions alone. However, research in the 2000s demonstrated that assumption to be in error, resulting in the American Heart Association's acknowledgment of the effectiveness of chest compressions alone (see \"Compression only\" in this article).\nCPR methods continued to advance, with developments in the 2010s including an emphasis on constant, rapid heart stimulation, and a de-emphasis on the respiration aspect. Studies have shown that people who had rapid, constant heart-only chest compression are 22% more likely to survive than those receiving conventional CPR that included breathing. Because people tend to be reluctant to do mouth-to-mouth resuscitation, chest-only CPR nearly doubles the chances of survival overall, by increasing the odds of receiving CPR in the first place.\nOn animals.\nIt is feasible to perform CPR on animals, including cats and dogs. The principles and practices are similar to CPR for humans, except that resuscitation is usually done through the animal's nose, not the mouth. CPR should only be performed on unconscious animals to avoid the risk of being bitten; a conscious animal would not require chest compressions. Animals, depending on species, may have a lower bone density than humans and so CPR can cause bones to become weakened after it is performed.\nResearch.\nCerebral performance category (CPC scores) are used as a research tool to describe \"good\" and \"poor\" outcomes. Level 1 is conscious and alert with normal function. Level 2 is only slight disability. Level 3 is a moderate disability. Level 4 is a severe disability. Level 5 is comatose or persistent vegetative state. Level 6 is brain dead or death from other causes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66393", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=66393", "title": "Clinical death", "text": "Medical term defining death\nClinical death is the medical term for cessation of blood circulation and breathing, the two criteria necessary to sustain the lives of human beings and of many other organisms. It occurs when the heart stops beating in a regular rhythm, a condition called cardiac arrest. The term is also sometimes used in resuscitation research.\nStopped blood circulation has historically proven irreversible in most cases. Prior to the invention of cardiopulmonary resuscitation (CPR), defibrillation, epinephrine injection, and other treatments in the 20th century, the absence of blood circulation (and vital functions related to blood circulation) was historically considered the official definition of death. With the advent of these strategies, cardiac arrest came to be called \"clinical death\" rather than simply \"death\", to reflect the possibility of post-arrest resuscitation.\nAt the onset of clinical death, consciousness is lost within several seconds, and in dogs, measurable brain activity has been measured to stop within 20 to 40 seconds. Irregular gasping may occur during this early time period, and is sometimes mistaken by rescuers as a sign that CPR is not necessary. During clinical death, all tissues and organs in the body steadily accumulate a type of injury called ischemic injury.\nLimits of reversal.\nMost tissues and organs of the body can survive clinical death for considerable periods. Blood circulation can be stopped in the entire body below the heart for at least 30 minutes, with injury to the spinal cord being a limiting factor. Detached limbs may be successfully reattached after 6 hours of no blood circulation at warm temperatures. Bone, tendon, and skin can survive as long as 8 to 12 hours.\nThe brain, however, appears to accumulate ischemic injury faster than any other organ. Without special treatment after circulation is restarted, full recovery of the brain after more than 3 minutes of clinical death at normal body temperature is rare. Usually brain damage or later brain death results after longer intervals of clinical death even if the heart is restarted and blood circulation is successfully restored. Brain injury is therefore the chief limiting factor for recovery from clinical death.\nAlthough loss of function is almost immediate, there is no specific duration of clinical death at which the non-functioning brain clearly dies. The most vulnerable cells in the brain, CA1 neurons of the hippocampus, are fatally injured by as little as 10 minutes without oxygen. However, the injured cells do not actually die until hours after resuscitation. This delayed death can be prevented \"in vitro\" by a simple drug treatment even after 20 minutes without oxygen. In other areas of the brain, viable human neurons have been recovered and grown in culture hours after clinical death. Brain failure after clinical death is now known to be due to a complex series of processes called reperfusion injury that occur \"after\" blood circulation has been restored, especially processes that interfere with blood circulation during the recovery period. Control of these processes is the subject of ongoing research.\nIn 1990, the laboratory of resuscitation pioneer Peter Safar discovered that reducing body temperature by three degrees Celsius after restarting blood circulation could double the time window of recovery from clinical death without brain damage from 5 minutes to 10 minutes. This induced hypothermia technique is beginning to be used in emergency medicine. The combination of mildly reducing body temperature, reducing blood cell concentration, and increasing blood pressure after resuscitation was found especially effective\u00a0\u2013 allowing for recovery of dogs after 12 minutes of clinical death at normal body temperature with practically no brain injury. The addition of a drug treatment protocol has been reported to allow recovery of dogs after 16 minutes of clinical death at normal body temperature with no lasting brain injury. Cooling treatment alone has permitted recovery after 17 minutes of clinical death at normal temperature, but with brain injury.\nUnder laboratory conditions at normal body temperature, the longest period of clinical death of a cat (after complete circulatory arrest) survived with eventual return of brain function is one hour.\nHypothermia.\nReduced body temperature, or therapeutic hypothermia, during clinical death slows the rate of injury accumulation, and extends the time period during which clinical death can be survived. The decrease in the rate of injury can be approximated by the Q10 rule, which states that the rate of biochemical reactions decreases by a factor of two for every 10\u00a0\u00b0C reduction in temperature. As a result, humans can sometimes survive periods of clinical death exceeding one hour at temperatures below 20\u00a0\u00b0C. The prognosis is improved if clinical death is caused by hypothermia rather than occurring prior to it; in 1999, 29-year-old Swedish woman Anna B\u00e5genholm spent 80 minutes trapped in ice and survived with a near full recovery from a 13.7\u00a0\u00b0C core body temperature. It is said in emergency medicine that \"nobody is dead until they are warm and dead.\" In animal studies, up to three hours of clinical death can be survived at temperatures near 0\u00a0\u00b0C.\nLife support.\nThe purpose of cardiopulmonary resuscitation (CPR) during cardiac arrest is ideally reversal of the clinically dead state by restoration of blood circulation and breathing. However, there is great variation in the effectiveness of CPR for this purpose. Blood pressure is very low during manual CPR, resulting in only a ten-minute average extension of survival. Yet there are cases of patients regaining consciousness during CPR while still in full cardiac arrest. In absence of cerebral function monitoring or frank return to consciousness, the neurological status of patients undergoing CPR is intrinsically uncertain. It is somewhere between the state of clinical death and a normal functioning state.\nPatients supported by methods that certainly maintain enough blood circulation and oxygenation for sustaining life during stopped heartbeat and breathing, such as cardiopulmonary bypass, are not customarily considered clinically dead. All parts of the body except the heart and lungs continue to function normally. Clinical death occurs only if machines providing sole circulatory support are turned off, leaving the patient in a state of stopped blood circulation.\nControlled.\nCertain surgeries for cerebral aneurysms or aortic arch defects require that blood circulation be stopped while repairs are performed. This deliberate temporary induction of clinical death is called circulatory arrest. It is typically performed by lowering body temperature to between 18\u00a0\u00b0C and 20\u00a0\u00b0C (64 and 68\u00a0\u00b0F) and stopping the heart and lungs. This state is called deep hypothermic circulatory arrest. At such low temperatures most patients can tolerate the clinically dead state for up to 30 minutes without incurring significant brain injury.\nLonger durations are possible at lower temperatures, but the usefulness of longer procedures has not been established yet.\nControlled clinical death has also been proposed as a treatment for exsanguinating trauma to create time for surgical repair.\nDetermination.\nDeath was historically believed to be an event that coincided with the onset of clinical death. It is now understood that death is a series of physical events, not a single one, and determination of permanent death is dependent on other factors beyond simple cessation of breathing and heartbeat.\nClinical death that occurs unexpectedly is treated as a medical emergency. CPR is initiated. In a United States hospital, a Code Blue is declared and Advanced Cardiac Life Support procedures used to attempt to restart a normal heartbeat. This effort continues until either the heart is restarted, or a physician determines that continued efforts are useless and recovery is impossible. If this determination is made, the physician pronounces legal death and resuscitation efforts stop.\nIf clinical death is expected due to terminal illness or withdrawal of supportive care, often a Do Not Resuscitate (DNR) or \"no code\" order is in place. This means that no resuscitation efforts are made, and a physician or nurse may pronounce legal death at the onset of clinical death.\nA patient with working heart and lungs who is determined to be brain dead can be pronounced legally dead without clinical death occurring. However, some courts have been reluctant to impose such a determination over the religious objections of family members, such as in the Jesse Koochin case. Similar issues were also raised by the case of Mordechai Dov Brody, but the child died before a court could resolve the matter.\nConversely, in the case of Marlise Mu\u00f1oz, a hospital refused to remove a brain dead woman from life support machines for nearly two months, despite her husband's requests, because she was pregnant.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66394", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=66394", "title": "Island of stability", "text": "Predicted set of isotopes of relatively more stable superheavy elements\nIn nuclear physics, the island of stability is a predicted set of isotopes of superheavy elements that may have considerably longer half-lives than known isotopes of these elements. It is predicted to appear as an \"island\" in the chart of nuclides, separated from known stable and long-lived primordial radionuclides. Its theoretical existence is attributed to stabilizing effects of predicted \"magic numbers\" of protons and neutrons in the superheavy mass region.\nSeveral predictions have been made regarding the exact location of the island of stability, though it is generally thought to center near copernicium and flerovium isotopes in the vicinity of the predicted closed neutron shell at \"N\"\u00a0=\u00a0184. These models strongly suggest that the closed shell will confer further stability towards fission and alpha decay. While these effects are expected to be greatest near atomic number \"Z\"\u00a0=\u00a0114 (flerovium) and \"N\"\u00a0=\u00a0184, the region of increased stability is expected to encompass several neighboring elements, and there may also be additional islands of stability around heavier nuclei that are doubly magic (having magic numbers of both protons and neutrons). Estimates of the stability of the nuclides within the island are usually around a half-life of minutes or days; some optimists propose half-lives on the order of millions of years.\nAlthough the nuclear shell model predicting magic numbers has existed since the 1940s, the existence of long-lived superheavy nuclides has not been definitively demonstrated. Like the rest of the superheavy elements, the nuclides within the island of stability have never been found in nature; thus, they must be created artificially in a nuclear reaction to be studied. Scientists have not found a way to carry out such a reaction, for it is likely that new types of reactions will be needed to populate nuclei near the center of the island. Nevertheless, the successful synthesis of superheavy elements up to \"Z\"\u00a0=\u00a0118 (oganesson) with up to 177 neutrons demonstrates a slight stabilizing effect around elements 110 to 114 that may continue in heavier isotopes, consistent with the existence of the island of stability.\nIntroduction.\nNuclide stability.\nThe composition of a nuclide (atomic nucleus) is defined by the number of protons \"Z\" and the number of neutrons \"N\", which sum to mass number \"A\". Proton number \"Z\", also named the atomic number, determines the position of an element in the periodic table. The approximately 3300 known nuclides are commonly represented in a chart with \"Z\" and \"N\" for its axes and the half-life for radioactive decay indicated for each unstable nuclide (see figure). As of 2019[ [update]], 251 nuclides are observed to be stable (having never been observed to decay); generally, as the number of protons increases, stable nuclei have a higher neutron\u2013proton ratio (more neutrons per proton). The last element in the periodic table that has a stable isotope is lead (\"Z\"\u00a0=\u00a082), with stability (i.e., half-lives of the longest-lived isotopes) generally decreasing in heavier elements, especially beyond curium (\"Z\"\u00a0=\u00a096). The half-lives of nuclei also decrease when there is a lopsided neutron\u2013proton ratio, such that the resulting nuclei have too few or too many neutrons to be stable.\nThe stability of a nucleus is determined by its binding energy, higher binding energy conferring greater stability. The binding energy per nucleon increases with atomic number to a broad plateau around \"A\"\u00a0=\u00a060, then declines. If a nucleus can be split into two parts that have a lower total energy (a consequence of the mass defect resulting from greater binding energy), it is unstable. The nucleus can hold together for a finite time because there is a potential barrier opposing the split, but this barrier can be crossed by quantum tunneling. The lower the barrier and the masses of the fragments, the greater the probability per unit time of a split.\nProtons in a nucleus are bound together by the strong force, which counterbalances the Coulomb repulsion between positively charged protons. In heavier nuclei, larger numbers of uncharged neutrons are needed to reduce repulsion and confer additional stability. Even so, as physicists started to synthesize elements that are not found in nature, they found the stability decreased as the nuclei became heavier. Thus, they speculated that the periodic table might come to an end. The discoverers of plutonium (element 94) considered naming it \"ultimium\", thinking it was the last. Following the discoveries of heavier elements, of which some decayed in microseconds, it then seemed that instability with respect to spontaneous fission would limit the existence of heavier elements. In 1939, an upper limit of potential element synthesis was estimated around element 104, and following the first discoveries of transactinide elements in the early 1960s, this upper limit prediction was extended to element 108.\nMagic numbers.\nAs early as 1914, the possible existence of superheavy elements with atomic numbers well beyond that of uranium\u2014then the heaviest known element\u2014was suggested, when German physicist Richard Swinne proposed that superheavy elements around \"Z\"\u00a0=\u00a0108 were a source of radiation in cosmic rays. Although he did not make any definitive observations, he hypothesized in 1931 that transuranium elements around \"Z\"\u00a0=\u00a0100 or \"Z\"\u00a0=\u00a0108 may be relatively long-lived and possibly exist in nature. In 1955, American physicist John Archibald Wheeler also proposed the existence of these elements; he is credited with the first usage of the term \"superheavy element\" in a 1958 paper published with Frederick Werner. This idea did not attract wide interest until a decade later, after improvements in the nuclear shell model. In this model, the atomic nucleus is built up in \"shells\", analogous to electron shells in atoms. Independently of each other, neutrons and protons have energy levels that are normally close together, but after a given shell is filled, it takes substantially more energy to start filling the next. Thus, the binding energy per nucleon reaches a local maximum and nuclei with filled shells are more stable than those without. This theory of a nuclear shell model originates in the 1930s, but it was not until 1949 that German physicists Maria Goeppert Mayer and Johannes Hans Daniel Jensen et al. independently devised the correct formulation.\nThe numbers of nucleons for which shells are filled are called magic numbers. Magic numbers of 2, 8, 20, 28, 50, 82 and 126 have been observed for neutrons, and the next number is predicted to be 184. Protons share the first six of these magic numbers, and 126 has been predicted as a magic proton number since the 1940s. Nuclides with a magic number of each\u2014such as 16O (\"Z\"\u00a0=\u00a08, \"N\"\u00a0=\u00a08), 132Sn (\"Z\"\u00a0=\u00a050, \"N\"\u00a0=\u00a082), and 208Pb (\"Z\"\u00a0=\u00a082, \"N\"\u00a0=\u00a0126)\u2014are referred to as \"doubly magic\" and are more stable than nearby nuclides as a result of greater binding energies.\nIn the late 1960s, more sophisticated shell models were formulated by American physicist William Myers and Polish physicist W\u0142adys\u0142aw \u015awi\u0105tecki, and independently by German physicist Heiner Meldner (1939\u20132019). With these models, taking into account Coulomb repulsion, Meldner predicted that the next proton magic number may be 114 instead of 126. Myers and \u015awi\u0105tecki appear to have coined the term \"island of stability\", and American chemist Glenn Seaborg, later a discoverer of many of the superheavy elements, quickly adopted the term and promoted it. Myers and \u015awi\u0105tecki also proposed that some superheavy nuclei would be longer-lived as a consequence of higher fission barriers. Further improvements in the nuclear shell model by Soviet physicist Vilen Strutinsky led to the emergence of the macroscopic\u2013microscopic method, a nuclear mass model that takes into consideration both smooth trends characteristic of the liquid drop model and local fluctuations such as shell effects. This approach enabled Swedish physicist Sven Nilsson et al., as well as other groups, to make the first detailed calculations of the stability of nuclei within the island. With the emergence of this model, Strutinsky, Nilsson, and other groups argued for the existence of the doubly magic nuclide 298Fl (\"Z\"\u00a0=\u00a0114, \"N\"\u00a0=\u00a0184), rather than 310Ubh (\"Z\"\u00a0=\u00a0126, \"N\"\u00a0=\u00a0184) which was predicted to be doubly magic as early as 1957. Subsequently, estimates of the proton magic number have ranged from 114 to 126, and there is still no consensus.\nDiscoveries.\nInterest in a possible island of stability grew throughout the 1960s, as some calculations suggested that it might contain nuclides with half-lives of billions of years. They were also predicted to be especially stable against spontaneous fission in spite of their high atomic mass. It was thought that if such elements exist and are sufficiently long-lived, there may be several novel applications as a consequence of their nuclear and chemical properties. These include use in particle accelerators as neutron sources, in nuclear weapons as a consequence of their predicted low critical masses and high number of neutrons emitted per fission, and as nuclear fuel to power space missions. These speculations led many researchers to conduct searches for superheavy elements in the 1960s and 1970s, both in nature and through nucleosynthesis in particle accelerators.\nDuring the 1970s, many searches for long-lived superheavy nuclei were conducted. Experiments aimed at synthesizing elements ranging in atomic number from 110 to 127 were conducted at laboratories around the world. These elements were sought in fusion-evaporation reactions, in which a heavy target made of one nuclide is irradiated by accelerated ions of another in a cyclotron, and new nuclides are produced after these nuclei fuse and the resulting excited system releases energy by evaporating several particles (usually protons, neutrons, or alpha particles). These reactions are divided into \"cold\" and \"hot\" fusion, which respectively create systems with lower and higher excitation energies; this affects the yield of the reaction. For example, the reaction between 248Cm and 40Ar was expected to yield isotopes of element 114, and that between 232Th and 84Kr was expected to yield isotopes of element 126. None of these attempts were successful, indicating that such experiments may have been insufficiently sensitive if reaction cross sections were low\u2014resulting in lower yields\u2014or that any nuclei reachable via such fusion-evaporation reactions might be too short-lived for detection. Subsequent successful experiments reveal that half-lives and cross sections indeed decrease with increasing atomic number, resulting in the synthesis of only a few short-lived atoms of the heaviest elements in each experiment; as of 2022[ [update]], the highest reported cross section for a superheavy nuclide near the island of stability is for 288Mc in the reaction between 243Am and 48Ca.\nSimilar searches in nature were also unsuccessful, suggesting that if superheavy elements do exist in nature, their abundance is less than 10\u221214 moles of superheavy elements per mole of ore. Despite these unsuccessful attempts to observe long-lived superheavy nuclei, new superheavy elements were synthesized every few years in laboratories through light-ion bombardment and cold fusion reactions; rutherfordium, the first transactinide, was discovered in 1969, and copernicium, eight protons closer to the island of stability predicted at \"Z\"\u00a0=\u00a0114, was reached by 1996. Even though the half-lives of these nuclei are very short (on the order of seconds), the very existence of elements heavier than rutherfordium is indicative of stabilizing effects thought to be caused by closed shells; a model not considering such effects would forbid the existence of these elements due to rapid spontaneous fission.\nFlerovium, with the expected magic 114 protons, was first synthesized in 1998 at the Joint Institute for Nuclear Research in Dubna, Russia, by a group of physicists led by Yuri Oganessian. A single atom of element 114 was detected, with a lifetime of 30.4 seconds, and its decay products had half-lives measurable in minutes. Because the produced nuclei underwent alpha decay rather than fission, and the half-lives were several orders of magnitude longer than those previously predicted or observed for superheavy elements, this event was seen as a \"textbook example\" of a decay chain characteristic of the island of stability, providing strong evidence for the existence of the island of stability in this region. Even though the original 1998 chain was not observed again, and its assignment remains uncertain, further successful experiments in the next two decades led to the discovery of all elements up to oganesson, whose half-lives were found to exceed initially predicted values; these decay properties further support the presence of the island of stability. However, a 2021 study on the decay chains of flerovium isotopes suggests that there is no strong stabilizing effect from \"Z\"\u00a0=\u00a0114 in the region of known nuclei (\"N\"\u00a0=\u00a0174), and that extra stability would be predominantly a consequence of the neutron shell closure. Although known nuclei still fall several neutrons short of \"N\"\u00a0=\u00a0184 where maximum stability is expected (the most neutron-rich confirmed nuclei, 293Lv and 294Ts, only reach \"N\"\u00a0=\u00a0177), and the exact location of the center of the island remains unknown, the trend of increasing stability closer to \"N\"\u00a0=\u00a0184 has been demonstrated. For example, the isotope 285Cn, with eight more neutrons than 277Cn, has a half-life almost five orders of magnitude longer. This trend is expected to continue into unknown heavier isotopes in the vicinity of the shell closure.\nDeformed nuclei.\nThough nuclei within the island of stability around \"N\"\u00a0=\u00a0184 are predicted to be spherical, studies from the early 1990s\u2014beginning with Polish physicists Zygmunt Patyk and Adam Sobiczewski in 1991\u2014suggest that some superheavy elements do not have perfectly spherical nuclei. A change in the shape of the nucleus changes the position of neutrons and protons in the shell. Research indicates that large nuclei farther from spherical magic numbers are deformed, causing magic numbers to shift or new magic numbers to appear. Current theoretical investigation indicates that in the region \"Z\"\u00a0=\u00a0106\u2013108 and \"N\"\u00a0\u2248\u00a0160\u2013164, nuclei may be more resistant to fission as a consequence of shell effects for deformed nuclei; thus, such superheavy nuclei would only undergo alpha decay. Hassium-270 is now believed to be a doubly magic deformed nucleus, with deformed magic numbers \"Z\"\u00a0=\u00a0108 and \"N\"\u00a0=\u00a0162. It has a half-life of 9 seconds. This is consistent with models that take into account the deformed nature of nuclei intermediate between the actinides and island of stability near \"N\"\u00a0=\u00a0184, in which a stability \"peninsula\" emerges at deformed magic numbers \"Z\"\u00a0=\u00a0108 and \"N\"\u00a0=\u00a0162. Determination of the decay properties of neighboring hassium and seaborgium isotopes near \"N\"\u00a0=\u00a0162 provides further strong evidence for this region of relative stability in deformed nuclei. This also strongly suggests that the island of stability (for spherical nuclei) is not completely isolated from the region of stable nuclei, but rather that both regions are instead linked through an isthmus of relatively stable deformed nuclei.\nPredicted decay properties.\nThe half-lives of nuclei in the island of stability itself are unknown since none of the nuclides that would be \"on the island\" have been observed. Many physicists believe that the half-lives of these nuclei are relatively short, on the order of minutes or days. Some theoretical calculations indicate that their half-lives may be long, on the order of 100 years, or possibly as long as 109 years.\nThe shell closure at \"N\"\u00a0=\u00a0184 is predicted to result in longer partial half-lives for alpha decay and spontaneous fission. It is believed that the shell closure will result in higher fission barriers for nuclei around 298Fl, strongly hindering fission and perhaps resulting in fission half-lives 30 orders of magnitude greater than those of nuclei unaffected by the shell closure. For example, the neutron-deficient isotope 284Fl (with \"N\"\u00a0=\u00a0170) undergoes fission with a half-life of 2.5 milliseconds, and is thought to be one of the most neutron-deficient nuclides with increased stability in the vicinity of the \"N\"\u00a0=\u00a0184 shell closure. Beyond this point, some undiscovered isotopes are predicted to undergo fission with still shorter half-lives, limiting the existence and possible observation of superheavy nuclei far from the island of stability (namely for \"N\"\u00a0&lt;\u00a0170 as well as for \"Z\"\u00a0&gt;\u00a0120 and \"N\"\u00a0&gt;\u00a0184). These nuclei may undergo alpha decay or spontaneous fission in microseconds or less, with some fission half-lives estimated on the order of 10\u221220 seconds in the absence of fission barriers. In contrast, 298Fl (predicted to lie within the region of maximum shell effects) may have a much longer spontaneous fission half-life, possibly on the order of 1019 years.\nIn the center of the island, there may be competition between alpha decay and spontaneous fission, though the exact ratio is model-dependent. The alpha decay half-lives of 1700 nuclei with 100\u00a0\u2264\u00a0\"Z\"\u00a0\u2264\u00a0130 have been calculated in a quantum tunneling model with both experimental and theoretical alpha decay Q-values, and are in agreement with observed half-lives for some of the heaviest isotopes.\nThe longest-lived nuclides are also predicted to lie on the beta-stability line, for beta decay is predicted to compete with the other decay modes near the predicted center of the island, especially for isotopes of elements 111\u2013115. Unlike other decay modes predicted for these nuclides, beta decay does not change the mass number. Instead, a neutron is converted into a proton or vice versa, producing an adjacent isobar closer to the center of stability (the isobar with the lowest mass excess). For example, significant beta decay branches may exist in nuclides such as 291Fl and 291Nh; these nuclides have only a few more neutrons than known nuclides, and might decay via a \"narrow pathway\" towards the center of the island of stability. The possible role of beta decay is highly uncertain, as some isotopes of these elements (such as 290Fl and 293Mc) are predicted to have shorter partial half-lives for alpha decay. Beta decay would reduce competition and would result in alpha decay remaining the dominant decay channel, unless additional stability towards alpha decay exists in superdeformed isomers of these nuclides.\nConsidering all decay modes, various models indicate a shift of the center of the island (i.e., the longest-living nuclide) from 298Fl to a lower atomic number, and competition between alpha decay and spontaneous fission in these nuclides; these include 100-year half-lives for 291Cn and 293Cn, a 1000-year half-life for 296Cn, a 300-year half-life for 294Ds, and a 3500-year half-life for 293Ds, with 294Ds and 296Cn exactly at the \"N\"\u00a0=\u00a0184 shell closure. It has also been posited that this region of enhanced stability for elements with 112\u00a0\u2264\u00a0\"Z\"\u00a0\u2264\u00a0118 may instead be a consequence of nuclear deformation, and that the true center of the island of stability for spherical superheavy nuclei lies around 306Ubb (\"Z\"\u00a0=\u00a0122, \"N\"\u00a0=\u00a0184). This model defines the island of stability as the region with the greatest resistance to fission rather than the longest total half-lives; the nuclide 306Ubb is still predicted to have a short half-life with respect to alpha decay. The island of stability for spherical nuclei may also be a \"coral reef\" (i.e., a broad region of increased stability without a clear \"peak\") around \"N\"\u00a0=\u00a0184 and 114\u00a0\u2264\u00a0\"Z\"\u00a0\u2264\u00a0120, with half-lives rapidly decreasing at higher atomic number, due to combined effects from proton and neutron shell closures.\nAnother potentially significant decay mode for the heaviest superheavy elements was proposed to be cluster decay by Romanian physicists Dorin N. Poenaru and Radu A. Gherghescu and German physicist Walter Greiner. Its branching ratio relative to alpha decay is expected to increase with atomic number such that it may compete with alpha decay around \"Z\"\u00a0=\u00a0120, and perhaps become the dominant decay mode for heavier nuclides around \"Z\"\u00a0=\u00a0124. As such, it is expected to play a larger role beyond the center of the island of stability (though still influenced by shell effects), unless the center of the island lies at a higher atomic number than predicted.\nPossible natural occurrence.\nEven though half-lives of hundreds or thousands of years would be relatively long for superheavy elements, they are far too short for any such nuclides to exist primordially on Earth. Additionally, instability of nuclei intermediate between primordial actinides (232Th, 235U, and 238U) and the island of stability may inhibit production of nuclei within the island in \"r\"-process nucleosynthesis. Various models suggest that spontaneous fission will be the dominant decay mode of nuclei with \"A\"\u00a0&gt;\u00a0280, and that neutron-induced or beta-delayed fission\u2014respectively neutron capture and beta decay immediately followed by fission\u2014will become the primary reaction channels. As a result, beta decay towards the island of stability may only occur within a very narrow path or may be entirely blocked by fission, thus precluding the synthesis of nuclides within the island. The non-observation of superheavy nuclides such as 292Hs and 298Fl in nature is thought to be a consequence of a low yield in the \"r\"-process resulting from this mechanism, as well as half-lives too short to allow measurable quantities to persist in nature. Various studies utilizing accelerator mass spectroscopy and crystal scintillators have reported upper limits of the natural abundance of such long-lived superheavy nuclei on the order of relative to their stable homologs.\nDespite these obstacles to their synthesis, a 2013 study published by a group of Russian physicists led by Valeriy Zagrebaev proposes that the longest-lived copernicium isotopes may occur at an abundance of 10\u221212 relative to lead, whereby they may be detectable in cosmic rays. Similarly, in a 2013 experiment, a group of Russian physicists led by Aleksandr Bagulya reported the possible observation of three cosmogenic superheavy nuclei in olivine crystals in meteorites. The atomic number of these nuclei was estimated to be between 105 and 130, with one nucleus likely constrained between 113 and 129, and their lifetimes were estimated to be at least 3,000 years. Although this observation has yet to be confirmed in independent studies, it strongly suggests the existence of the island of stability, and is consistent with theoretical calculations of half-lives of these nuclides.\nThe decay of heavy, long-lived elements in the island of stability is a proposed explanation for the unusual presence of the short-lived radioactive isotopes observed in Przybylski's Star.\nSynthesis and difficulties.\nThe manufacture of nuclei on the island of stability proves to be very difficult because the nuclei available as starting materials do not deliver the necessary sum of neutrons. Radioactive ion beams (such as 44S) in combination with actinide targets (such as 248Cm) may allow the production of more neutron rich nuclei nearer to the center of the island of stability, though such beams are not currently available in the required intensities to conduct such experiments. Several heavier isotopes such as 250Cm and 254Es may still be usable as targets, allowing the production of isotopes with one or two more neutrons than known isotopes, though the production of several milligrams of these rare isotopes to create a target is difficult. It may also be possible to probe alternative reaction channels in the same 48Ca-induced fusion-evaporation reactions that populate the most neutron-rich known isotopes, namely those at a lower excitation energy (resulting in fewer neutrons being emitted during de-excitation), or those involving evaporation of charged particles (\"pxn\", evaporating a proton and several neutrons, or \"\u03b1xn\", evaporating an alpha particle and several neutrons). This may allow the synthesis of neutron-enriched isotopes of elements 111\u2013117. Although the predicted cross sections are on the order of 1\u2013900\u00a0fb, smaller than when only neutrons are evaporated (\"xn\" channels), it may still be possible to generate otherwise unreachable isotopes of superheavy elements in these reactions. Some of these heavier isotopes (such as 291Mc, 291Fl, and 291Nh) may also undergo electron capture (converting a proton into a neutron) in addition to alpha decay with relatively long half-lives, decaying to nuclei such as 291Cn that are predicted to lie near the center of the island of stability. However, this remains largely hypothetical as no superheavy nuclei near the beta-stability line have yet been synthesized and predictions of their properties vary considerably across different models. In 2024, a team of researchers at the JINR observed one decay chain of the known isotope 289Mc as a product in the \"p2n\" channel of the reaction between 242Pu and 50Ti, an experiment targeting neutron-deficient livermorium isotopes. This was the first successful report of a charged-particle exit channel in a hot fusion reaction between an actinide target and a projectile with \"Z\"\u00a0\u2265\u00a020.\nThe process of slow neutron capture used to produce nuclides as heavy as 257Fm is blocked by short-lived isotopes of fermium that undergo spontaneous fission (for example, 258Fm has a half-life of 370\u00a0\u03bcs); this is known as the \"fermium gap\" and prevents the synthesis of heavier elements in such a reaction. It might be possible to bypass this gap, as well as another predicted region of instability around \"A\"\u00a0=\u00a0275 and \"Z\"\u00a0=\u00a0104\u2013108, in a series of controlled nuclear explosions with a higher neutron flux (about a thousand times greater than fluxes in existing reactors) that mimics the astrophysical \"r\"-process. First proposed in 1972 by Meldner, such a reaction might enable the production of macroscopic quantities of superheavy elements within the island of stability; the role of fission in intermediate superheavy nuclides is highly uncertain, and may strongly influence the yield of such a reaction.\nIt may also be possible to generate isotopes in the island of stability such as 298Fl in multi-nucleon transfer reactions in low-energy collisions of actinide nuclei (such as 238U and 248Cm). This inverse quasifission (partial fusion followed by fission, with a shift away from mass equilibrium that results in more asymmetric products) mechanism may provide a path to the island of stability if shell effects around \"Z\"\u00a0=\u00a0114 are sufficiently strong, though lighter elements such as nobelium and seaborgium (\"Z\"\u00a0=\u00a0102\u2013106) are predicted to have higher yields. Preliminary studies of the 238U\u00a0+\u00a0238U and 238U\u00a0+\u00a0248Cm transfer reactions have failed to produce elements heavier than mendelevium (\"Z\"\u00a0=\u00a0101), though the increased yield in the latter reaction suggests that the use of even heavier targets such as 254Es (if available) may enable production of superheavy elements. This result is supported by a later calculation suggesting that the yield of superheavy nuclides (with \"Z\"\u00a0\u2264\u00a0109) will likely be higher in transfer reactions using heavier targets. A 2018 study of the 238U\u00a0+\u00a0232Th reaction at the Texas A&amp;M Cyclotron Institute by Sara Wuenschel et al. found several unknown alpha decays that may possibly be attributed to new, neutron-rich isotopes of superheavy elements with 104\u00a0&lt;\u00a0\"Z\"\u00a0&lt;\u00a0116, though further research is required to unambiguously determine the atomic number of the products. This result strongly suggests that shell effects have a significant influence on cross sections, and that the island of stability could possibly be reached in future experiments with transfer reactions.\nOther islands of stability.\nFurther shell closures beyond the main island of stability in the vicinity of \"Z\"\u00a0=\u00a0112\u2013114 may give rise to additional islands of stability. Although predictions for the location of the next magic numbers vary considerably, two significant islands are thought to exist around heavier doubly magic nuclei; the first near 354126 (with 228 neutrons) and the second near 472164 or 482164 (with 308 or 318 neutrons). Nuclides within these two islands of stability might be especially resistant to spontaneous fission and have alpha decay half-lives measurable in years, thus having comparable stability to elements in the vicinity of flerovium. Other regions of relative stability may also appear with weaker proton shell closures in beta-stable nuclides; such possibilities include regions near 342126 and 462154. Substantially greater electromagnetic repulsion between protons in such heavy nuclei may greatly reduce their stability, and possibly restrict their existence to localized islands in the vicinity of shell effects. This may have the consequence of isolating these islands from the main chart of nuclides, as intermediate nuclides and perhaps elements in a \"sea of instability\" would rapidly undergo fission and essentially be nonexistent. It is also possible that beyond a region of relative stability around element 126, heavier nuclei would lie beyond a fission threshold given by the liquid drop model and thus undergo fission with very short lifetimes, rendering them essentially nonexistent even in the vicinity of greater magic numbers.\nIt has also been posited that in the region beyond \"A\"\u00a0&gt;\u00a0300, an entire \"continent of stability\" consisting of a hypothetical phase of stable quark matter, comprising freely flowing up and down quarks rather than quarks bound into protons and neutrons, may exist. Such a form of matter is theorized to be a ground state of baryonic matter with a greater binding energy per baryon than nuclear matter, favoring the decay of nuclear matter beyond this mass threshold into quark matter. If this state of matter exists, it could possibly be synthesized in the same fusion reactions leading to normal superheavy nuclei, and would be stabilized against fission as a consequence of its stronger binding that is enough to overcome Coulomb repulsion.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "66395", "revid": "58193", "url": "https://en.wikipedia.org/wiki?curid=66395", "title": "Walking pneumonia", "text": ""}
{"id": "66397", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=66397", "title": "Hemophiliacs", "text": ""}
{"id": "66399", "revid": "48442598", "url": "https://en.wikipedia.org/wiki?curid=66399", "title": "Fighter plane", "text": ""}
{"id": "66400", "revid": "50479220", "url": "https://en.wikipedia.org/wiki?curid=66400", "title": "Transfermium Wars", "text": "Disputes between American and Soviet scientists over element naming\nThe names for the chemical elements 104 to 106 were the subject of a major controversy starting in the 1960s, described by some nuclear chemists as the Transfermium Wars because it concerned the elements following fermium (element 100) on the periodic table.\nThis controversy arose from disputes between American scientists and Soviet scientists as to which had first isolated these elements. The final resolution of this controversy in 1997 also decided the names of elements 107 to 109.\nControversy.\nBy convention, naming rights for newly discovered chemical elements go to their discoverers. For elements 104, 105, and 106, there was a controversy between Soviet researchers at the Joint Institute for Nuclear Research and American researchers at Lawrence Berkeley National Laboratory regarding which group had discovered them first. Both parties suggested their own names for elements 104 and 105, not recognizing the other's name.\nThe American name of seaborgium for element 106 was also objectionable to some, because it referred to American chemist Glenn T. Seaborg who was still alive at the time this name was proposed. (Einsteinium and fermium had also been proposed as names of new elements while Albert Einstein and Enrico Fermi were still living, but only made public after their deaths, due to Cold War secrecy.)\nOpponents.\nThe two principal groups which were involved in the conflict over element naming were:\nand, as a kind of arbiter,\nThe German group at the Gesellschaft f\u00fcr Schwerionenforschung (GSI) in Darmstadt, who had (undisputedly) discovered elements 107 to 109, were dragged into the controversy when the Commission suggested that the name \"hahnium\", proposed for element 105 by the Americans, be used for GSI's element 108 instead.\nProposals.\nDarmstadt.\nThe names suggested for the elements 107 to 109 by the German group were:\nIUPAC.\nIn 1994, the IUPAC Commission on Nomenclature of Inorganic Chemistry proposed the following names:\nThis attempted to resolve the dispute by sharing the namings of the disputed elements between Russians and Americans, replacing the name for 104 with one honoring the Dubna research center, and not naming 106 after Seaborg.\nObjections to the IUPAC 94 proposal.\nThis solution drew objections from the American Chemical Society (ACS) on the grounds that the right of the American group to propose the name for element 106 was not in question, and that group should have the right to name the element. Indeed, IUPAC decided that the credit for the discovery of element 106 should be awarded to Berkeley.\nAlong the same lines, the German group protested against naming element 108 by the American suggestion \"hahnium\", mentioning the long-standing convention that an element is named by its discoverers.\nIn addition, given that many American books had already used rutherfordium and hahnium for 104 and 105, the ACS objected to those names being used for other elements.\nIn 1995, IUPAC abandoned the controversial rule and established a committee of national representatives aimed at finding a compromise. They suggested \"seaborgium\" for element 106 in exchange for the removal of all the other American proposals, except for the established name \"lawrencium\" for element 103. The equally entrenched name \"nobelium\" for element 102 was replaced by \"flerovium\" after Georgy Flyorov, following the recognition by the 1993 report that that element had been first synthesized in Dubna. This was rejected by American scientists and the decision was retracted. The name \"flerovium\" was later used for element 114.\nResolution (IUPAC 97).\nIn 1996, IUPAC held another meeting, reconsidered all names in hand, and accepted another set of recommendations; finally, it was approved and published in 1997 on the 39th IUPAC General Assembly in Geneva, Switzerland. Element 105 was named \"dubnium\" (Db), after Dubna in Russia, the location of the JINR; the American suggestions were used for elements 102, 103, 104, and 106. The name \"dubnium\" had been used for element 104 in the previous IUPAC recommendation. The American scientists \"reluctantly\" approved this decision. IUPAC pointed out that the Berkeley laboratory had already been recognized several times, in the naming of berkelium, californium, and americium, and that the acceptance of the names \"rutherfordium\" and \"seaborgium\" for elements 104 and 106 should be offset by recognizing JINR's contributions to the discovery of elements 104, 105, and 106.\nThe following names were agreed in 1997 on the 39th IUPAC General Assembly in Geneva, Switzerland:\nThus, the convention of the discoverer's right to name their elements was respected for elements 106 to 109, and the two disputed claims were \"shared\" between the two opponents.\nSummary.\nIn some countries uninvolved in the dispute, such as Poland, Denmark, India, and Indonesia, both \"kurchatovium\" for element 104 and \"hahnium\" for element 105 were used until 1997. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66401", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=66401", "title": "Baden-W\u00fcrttemberg", "text": "State in southwest Germany\nBaden-W\u00fcrttemberg ( ; ), commonly shortened to BW or BaW\u00fc, is a German state () in Southwest Germany, east of the Rhine, which forms the southern part of Germany's western border with France. With more than 11.07 million inhabitants as of 2019[ [update]] across a total area of nearly , it is the third-largest German state by both area (behind Bavaria and Lower Saxony) and population (behind North Rhine-Westphalia and Bavaria). The largest city in Baden-W\u00fcrttemberg is the state capital of Stuttgart, followed by Mannheim and Karlsruhe. Other major cities are Freiburg im Breisgau, Heidelberg, Heilbronn, Konstanz, Pforzheim, Reutlingen, T\u00fcbingen, and Ulm.\nModern Baden-W\u00fcrttemberg includes the historical territories of Baden, Prussian Hohenzollern, and W\u00fcrttemberg. Baden-W\u00fcrttemberg became a state of West Germany in April 1952 through the merger of South Baden, W\u00fcrttemberg-Baden, and W\u00fcrttemberg-Hohenzollern. These states had been created by the Allies as they separated traditional states into occupation zones after World War II.\nBaden-W\u00fcrttemberg is especially known for its strong economy with various industries like car manufacturing, electrical engineering, mechanical engineering, the service sector, and more. It has the third-highest gross regional product (GRP) in Germany. Part of the Four Motors for Europe and located in the Blue Banana, some of the largest German companies are headquartered in Baden-W\u00fcrttemberg, including Mercedes-Benz Group, Schwarz Group, Porsche, Bosch and SAP.\nThe sobriquet , a diminutive of the word in the local Swabian, Alemannic and Franconian dialects, is sometimes used as a synonym for Baden-W\u00fcrttemberg.\nHistory.\nBaden-W\u00fcrttemberg is formed from the historical territories of W\u00fcrttemberg, Baden and Prussian Hohenzollern. Baden spans along the flat right bank of the river Rhine from north-west to the south (Lake Constance) of the present state, whereas W\u00fcrttemberg and Hohenzollern lie more inland and hillier, including areas such as the Swabian Jura mountain range. The Black Forest formed part of the border between Baden and W\u00fcrttemberg.\nIn 100 AD, the Roman Empire invaded and occupied W\u00fcrttemberg, constructing a limes along its northern borders. Over the course of the third century AD, the Alemanni forced the Romans to retreat west beyond the Rhine and Danube rivers. In 496 AD the Alemanni were defeated by a Frankish invasion led by Clovis I.\nThe Holy Roman Empire was later established. The majority of people in this region continued to be Roman Catholics, even after the Protestant Reformation influenced populations in northern Germany.\nIn the late 18th and early 19th century, K\u00fcnzelsau, the capital of the Hohenlohe district, became the centre of emigration to the UK of pork butchers and bacon factors. The pioneers noticed a niche for speciality pork products in the rapidly growing English cities, especially those in the industrial centre and North. Many married local women and sent word home that a good living could be made in England; others followed.\nIn the late 19th and early 20th centuries, numerous people emigrated from this primarily rural area to the United States for economic reasons.\n20th century to present.\nAt the beginning of the 20th century, the territory of modern-day Baden-W\u00fcrttemberg consisted of the Grand Duchy of Baden, the Kingdom of W\u00fcrttemberg and the province of Hohenzollern of the Kingdom of Prussia. Since 1871, these had been part of the German Empire. In the aftermath of World War I and as part of the German revolution of 1918, the monarchs of Baden, W\u00fcrttemberg and Prussia were deposed, and these states became democratic republics: the Republic of Baden, the Free People's State of W\u00fcrttemberg and the Free State of Prussia.\nFollowing Adolf Hitler becoming chancellor of Germany in 1933, the democratic institutions of Baden, W\u00fcrttemberg and Prussia were abolished as part of the \"Gleichschaltung\".\nAfter World War II, the Allies established three states in the territory of modern-day Baden-W\u00fcrttemberg: (South) Baden, W\u00fcrttemberg-Baden and W\u00fcrttemberg-Hohenzollern. Baden and W\u00fcrttemberg-Hohenzollern were occupied by France, while W\u00fcrttemberg-Baden was occupied by the United States. The new artificial borders were a consequence of France requesting its own occupation zone in Germany after World War II, and the Americans' wish to keep the A8 motorway, which spans east-west across northern Baden and northern W\u00fcrttemberg, wholly within their occupation zone.\nIn 1949, each state became a founding member of the Federal Republic of Germany (West Germany), with Article 118 of the German constitution providing an accession procedure. On 9 December 1951, a referendum was held in W\u00fcrttemberg-Baden, W\u00fcrttemberg-Hohenzollern and (South) Baden over a possible merger, or the restoration of the former pre-war states. There was strong support for the merger in W\u00fcrttemberg and Hohenzollern, but opposition in Baden. While a majority in the historic area of Baden (52%) voted to restore the former pre-war states, the majority of voters overall (69%) voted in favor of a merger. Baden-W\u00fcrttemberg officially became a state on 25 April 1952.\nThere were still opponents to the merger of Baden and W\u00fcrttemberg, however. In 1956 the Federal Constitutional Court decided that the population of Baden should have their say in a separate referendum. The second referendum was delayed, however, and the Federal Constitutional Court decided in 1969 that another referendum should be held by 30 June 1970. The referendum in the historic area of Baden was finally held on 7 June 1970, with 81.9% of the voters voting in favour of the merger of Baden and W\u00fcrttemberg.\nGeography.\nBaden-W\u00fcrttemberg shares borders with the German states of Rhineland-Palatinate, Hesse, and Bavaria, and also shares borders with France (Alsace, within the region of Grand Est), and Switzerland (cantons of Basel-Landschaft, Basel-Stadt, Aargau, Z\u00fcrich, Schaffhausen and Thurgau).\nMost of the major cities of Baden-W\u00fcrttemberg straddle the banks of the Neckar River, which has its source in Villingen-Schwenningen and runs downstream (from southwest to the centre, then northwest) through the state past T\u00fcbingen, Stuttgart, Heilbronn, Heidelberg, and Mannheim.\nThe Rhine () forms the western border as well as large portions of the southern border. The Black Forest (Schwarzwald), the central mountain range of the state, rises east of the Upper Rhine valley. The high plateau of the Swabian Alb, between the Neckar, the Black Forest, and the Danube, is an essential European watershed. Baden-W\u00fcrttemberg shares Lake Constance (Bodensee, also known regionally as the Swabian Sea) with Switzerland, Austria and Bavaria, the international borders within its waters not being clearly defined. It shares the foothills of the Alps (known as the Allg\u00e4u) with Bavaria and the Austrian Vorarlberg, but Baden-W\u00fcrttemberg itself has no mainland border with Austria.\nThe Danube is conventionally taken to be formed by the confluence of the two streams Brigach and Breg just east of Donaueschingen. The source of the Donaubach, which flows into the Danube, in Donaueschingen is often referred to as the \"source of the Danube\" (\"Donauquelle\"). Hydrologically, the source of the Danube is the source of the Breg as the larger of the two formative streams, which rises near Furtwangen.\nThe forests in this region are home to common pests such as \"Melolontha Hippocastrani\", that cause damage to the foliage and soil.\nClimate.\nBaden-W\u00fcrttemberg is \u2013 along with Bavaria \u2013 the southernmost part of Germany.\nThe climate across the states varies. This is mostly due to a high amount of mountains and highlands inside of the state. Most parts in the western parts (Baden) lower than enjoy an almost year round mild oceanic climate (cfb in K\u00f6ppen classification). The climate in the eastern parts of the state is more continental. For instance, winters in the city of Ulm are colder than in Berlin. \nWhile winters in the warmest areas often lack snow, the Black Forest, Swabian Alb and the Alps tend to get snow frequently, especially in areas of high elevation. Summers here have more rain than in the valleys, but winters tend to have more sun.\nDue to the differences of the landscapes, average annual temperatures reach from only in the microclimates of Black Forest and Allg\u00e4u up to in the Upper Rhine Valley.\nGovernment.\nAdministration.\nBaden-W\u00fcrttemberg is divided into thirty-five districts (\"Landkreise\") and nine independent cities (\"Stadtkreise\"), both grouped into the four Administrative Districts () of Freiburg, Karlsruhe, Stuttgart, and T\u00fcbingen.\n The 35 districts:\nBaden-W\u00fcrttemberg contains nine additional independent cities not belonging to any district:\nOther state institutions.\nThe Baden-W\u00fcrttemberg General Auditing Office acts as an independent body to monitor public offices' correct use of public funds.\nPolitics.\nThe state parliament of Baden-W\u00fcrttemberg is the \"Landtag\", located in Stuttgart. The state government is currently formed by a Greens-CDU coalition as the third cabinet of Minister-President Winfried Kretschmann (Greens).\nThe politics of Baden-W\u00fcrttemberg have traditionally been dominated by the conservative Christian Democratic Union of Germany (CDU), which had led all but one government since 1952 until 2011. In the \"Landtag\" elections held on 27 March 2011, voters replaced the Christian Democrats and centre-right Free Democrats (FDP) coalition with an alliance of the Greens and Social Democrats (SPD), which secured a four-seat majority in the state parliament. The alliance elected the Greens-led first Kretschmann cabinet under Winfried Kretschmann because the Greens had surprisingly won 36 seats, one more than the Social Democrats' 35 seats. In the 2016 election, the popular Kretschmann and his Greens were reelected and, with their nationwide best result, turned out first place for the first time in any election in German history. However, because of the Social Democrats' heavy losses, the Greens formed a coalition government with the Christian Democrats, the second Kretschmann cabinet. After the most recent election in 2021, the Greens-CDU coalition was upheld.\nEconomy.\nAlthough Baden-W\u00fcrttemberg has relatively few natural resources compared to other regions of Germany, the state is among the most prosperous and wealthiest regions in Europe with a generally low unemployment rate historically. The state's economic performance benefits from and relies on its well-developed infrastructure. Apart from the city-states of Berlin, Bremen and Hamburg, Baden-W\u00fcrttemberg offers the fourth-shortest routes to trains and buses on average among all German states. Despite it not being predominantly reliant upon an industrial capacity, Baden-W\u00fcrttemberg is regarded as one of the strongest economic states in Germany.\nBaden-W\u00fcrttemberg has the highest exports (2019) and third-highest imports (2020), the second-lowest unemployment rate with 4.3% (March 2021), the most patents pending per capita (2020), the second-highest absolute and highest relative number of companies considered \"hidden champions\", and the highest absolute and relative research and development expenditure (2017) among all states in Germany, as well as the highest measured Innovationsindex (2012), making it the German state with the third-highest gross regional product (GRP) as of 2019[ [update]] (behind North Rhine-Westphalia and Bavaria) with \u20ac524,325 billion (around US$636.268 billion). Baden-W\u00fcrttemberg also has the most employees (233,296) in the automotive industry of all German states as of 2018[ [update]], as well as the third-highest number of motor vehicles of all German states (2020). If Baden-W\u00fcrttemberg were a sovereign country (2020), it would have an economy comparable to that of Sweden in terms of nominal gross domestic product (GDP).\nA number of well-known enterprises are headquartered in the state, for example Mercedes-Benz Group, Porsche, Robert Bosch GmbH (automobile industry), Carl Zeiss AG (optics), SAP (Europe's most valuable brand as well as the largest non-American software enterprise) and Heidelberger Druckmaschinen (precision mechanical engineering). Despite this, Baden-W\u00fcrttemberg's economy is dominated by small and medium-sized enterprises, like most companies in German-speaking countries are. Although poor in workable natural resources (formerly lead, zinc, iron, silver, copper, and salts) and still very rural in some areas, the region is heavily industrialised overall. In 2003, there were almost 8,800 manufacturing enterprises with more than 20 employees, but only 384 with more than 500. There are 3,779 companies in Baden-W\u00fcrttemberg corporate family which come to 1000\u20135000 employees in total.\nThe latter category accounts for 43% of the 1.2\u00a0million persons employed in the industry. The \"Mittelstand\" or mid-sized company model is the backbone of the Baden-W\u00fcrttemberg economy. Medium-sized businesses and a tradition of branching into different industrial sectors have ensured specialisation over a wide range. A fifth of the \"old\" Federal Republic's industrial gross value added is generated by Baden-W\u00fcrttemberg. Turnover for manufacturing in 2003 exceeded 240,000\u00a0million, 43% of which came from exports. The region depends to some extent on global economic developments, though the great adaptability of the region's economy has generally helped it through crises. Half of the employees in the manufacturing industry are in mechanical and electrical engineering and automobile construction. This is also where the largest enterprises are to be found. The importance of the precision mechanics industry also extends beyond the region's borders, as does that of the optical, clock making, toy, metallurgy and electronics industries. The textile industry, which formerly dominated much of the region, has disappeared from Baden-W\u00fcrttemberg. Research and development (R&amp;D) is funded jointly by the state and industry. In 2001, more than a fifth of the 100,000 or so persons working in R&amp;D in Germany were located in Baden-W\u00fcrttemberg, most of them in the Stuttgart area. Baden-W\u00fcrttemberg is also the region with the highest GDP of the Four Motors for Europe.\nA study performed in 2007 by the neo-liberal thinktank Initiative for New Social Market Economy and the trade newspaper \"Wirtschaftswoche\" awarded Baden-W\u00fcrttemberg for being the \"economically most successful and most dynamic state\" among the 16 states.\nThe unemployment rate stood at 3% in October 2018 and was the second lowest in Germany behind only Bavaria and one of the lowest in the European Union.\nTourism.\nBaden-W\u00fcrttemberg is a popular holiday destination. Main sights include the capital and biggest city, Stuttgart, modern and historical at the same time, with its urban architecture and atmosphere (and famously, its inner-city parks and historic Wilhelma zoo), its castles (such as Castle Solitude), its museums as well as a rich cultural programme (theatre, opera) and mineral spring baths in Bad Cannstatt (also the site of a Roman Castra); it is the only major city in Germany with vineyards in an urban territory.\nThe residential (court) towns of Ludwigsburg and Karlsruhe, the spas and casino of luxurious Baden-Baden, the medieval architecture of Ulm (Ulm M\u00fcnster is the tallest church in the world), the vibrant, young, but traditional university towns of Heidelberg and T\u00fcbingen with their old castles looking out above the river Neckar, are popular smaller towns. Sites of former monasteries such as the ones on Reichenau Island and at Maulbronn (both World Heritage Sites) as well as Bebenhausen Abbey are to be found. Baden-W\u00fcrttemberg also boasts rich old Free Imperial Cities such as Biberach, Esslingen am Neckar, Heilbronn, Ravensburg, Reutlingen, K\u00fcnzelsau, Schw\u00e4bisch Hall and Aalen as well as the southernmost and sunniest city of Germany, Freiburg, close to Alsace and Switzerland, being an ideal base for exploring the heights of the nearby Black Forest (e.g., for skiing in winter or for hiking in summer) with its traditional villages and the surrounding wine country of the Rhine Valley of South Baden.\nThe countryside of the lush Upper Neckar valley (where Rottweil is famous for its \"Fastnacht\" carnival) and the pristine Danube valley Swabian Alb (with Hohenzollern Castle and Sigmaringen Castle), as well as the largely pristine Swabian Forest, the Upper Rhine Valley, and Lake Constance, where all kinds of water sports are popular, with the former Imperial, today border town of Konstanz (where the Council of Constance took place), the Neolithic and Bronze Age village at Unteruhldingen, the flower island of Mainau, and the hometown of the Zeppelin, Friedrichshafen a.o., are especially popular for outdoor activities in the summer months.\nIn spring and autumn (April/May and September/October), beer festivals (fun fairs) take place at the Cannstatter Wasen in Stuttgart. The Cannstatter Volksfest, in the autumn, is the second-largest such festival in the world after the Munich Oktoberfest. In late November and early December Christmas markets are a tourist magnet in all major towns, the largest being in Stuttgart during the three weeks before Christmas.\nThe Bertha Benz Memorial Route is a 194\u00a0km signposted scenic route from Mannheim via Heidelberg and Wiesloch to Pforzheim and back, which follows the route of the world's first long-distance journey by automobile which Bertha Benz undertook in August 1888.\nBaden-W\u00fcrttemberg also is home to Europa-Park in Rust. The largest theme park in Germany, and the second most popular theme park in Europe, after Disneyland Paris.\nEducation.\nBaden-W\u00fcrttemberg is home to some of the oldest, most renowned, and prestigious universities in Germany, such as the universities of Heidelberg (founded in 1386, the oldest university within the territory of modern Germany), Freiburg (founded in 1457), and T\u00fcbingen (founded in 1477). It also contains three of the eleven German \"excellence universities\" (Heidelberg, T\u00fcbingen, Konstanz and Karlsruhe and formerly, Freiburg ).\nOther university towns are Mannheim and Ulm. Furthermore, two universities are located in the state capital Stuttgart, the University of Hohenheim, and the University of Stuttgart. Ludwigsburg is home to the national film school Filmakademie Baden-W\u00fcrttemberg (Film Academy Baden-Wuerttemberg). The private International University in Germany was situated in Bruchsal, but closed in 2009. Another private university is located in Friedrichshafen, Zeppelin University.\nFurthermore, there are more than a dozen , i.e., universities of applied sciences, as well as \"P\u00e4dagogische Hochschulen\", i.e., teacher training colleges, and other institutions of tertiary education in Baden-W\u00fcrttemberg. Pforzheim University is one of the oldest Fachhochschulen in Germany which is renowned and highly ranked for its engineering and MBA programs.\nThe state has the highest density of universities of any state in Germany.\nTransport.\nRailway.\nRailways form a major part of the transport infrastructure in Baden-W\u00fcrttemberg. As of 2017, the main standard gauge railway network managed by DB Netz consists of about of railway lines connecting all major settlements of the state, with about 6,500 trains operating every day. As part of high-speed rail in Germany, the Mannheim\u2013Stuttgart and Stuttgart\u2013Ulm high-speed lines were built, and the Karlsruhe\u2013Basel high-speed line, paralleling the traditional Mannheim\u2013Karlsruhe\u2013Basel railway, is currently under construction. In and around Stuttgart, the old terminal station is currently being replaced with an underground through station as part of the controversial Stuttgart 21 project.\nLocal branch lines of around , managed by the state-owned SWEG and Hohenzollerische Landesbahn (HzL), Karlsruhe-owned Albtal-Verkehrs-Gesellschaft (AVG), private W\u00fcrttembergische Eisenbahn (WEG) and other smaller rail infrastructure operators, complete the state's railway infrastructure.\nPassenger train services in Baden-W\u00fcrttemberg are operated partly by various subsidiaries of Deutsche Bahn, the national railway operator, such as DB Fernverkehr with its high-speed ICE, as well as IC trains; and DB Regio, operating some regional train services in the state. Cross-border train services are also provided by French Railways' TGV trains, as well as by Swiss Federal Railways and Austrian Federal Railways (including its Nightjet night trains).\nSince the 1990s, the around 120 individual regional train services in Baden-W\u00fcrttemberg have been managed by the state-owned Nahverkehrsgesellschaft Baden-W\u00fcrttemberg (NVBW), which has started to commercially tender out these train operations. DB Regio, SWEG, HzL, AVG and WEG have been joined in operating regional trains by Agilis, Bodensee-Oberschwaben-Bahn, Arverio, SBB, Schw\u00e4bische Alb-Bahn and VIAS, which entered the market.\nStarting in the 1970s, regional rail around major cities has been transformed into high-frequent S-Bahn networks, currently the following systems exist (partly) in Baden-W\u00fcrttemberg: Stuttgart S-Bahn, Rhine-Neckar S-Bahn, Breisgau S-Bahn and Basel S-Bahn. In and around the cities of Karlsruhe and Heilbronn, the Karlsruhe Stadtbahn system combines elements of traditional S-Bahns with tram operations within the urban cores (tram-trains).\nHistorically, the railway system in Baden-W\u00fcrttemberg was developed at first by the state's predecessors' state railways: the Grand Duchy of Baden State Railway opened its first railway line between Mannheim and Heidelberg on 12 September 1840; uniquely in Germany, it used a broad gauge of in its early years. In W\u00fcrttemberg, the Royal State Railways opened their first line on 22 October 1845 between two present-day suburbs of Stuttgart. In 1900, the Hohenzollerische Landesbahn (HzL) was founded to expand the rail system in the Prussian province, which had previously only been served by short sections of W\u00fcrttemberg lines passing through \"foreign\" territory. After the construction of the main lines, various private or local government initiatives constructed branch or local railway lines, some of which survive today as AVG, SWEG, RNV or WEG lines. In 1920, the Baden and W\u00fcrttemberg state railways merged with other state railways to form Deutsche Reichsbahn, which was replaced by Deutsche Bundesbahn after World War II. A lot of smaller railway lines, both DB and remaining local or private lines, closed in the decades after World War II, or were at least run-down with minimal service for passengers. Since the 1990s, some of these lines have been revived and revitalised marking good examples for increased ridership through attractive trains and timetables, with some examples being the Kraichgau Railway (taken over by AVG) and the Sch\u00f6nbuch Railway (reopened by WEG).\nA popular TV programme portraying railways around Germany and the world, \"Eisenbahn-Romantik\", originates in Baden-W\u00fcrttemberg and is produced by its public broadcaster SWR.\nUrban public transport.\nBaden-W\u00fcrttemberg's area is covered by 19 \"Verkehrsverb\u00fcnde\" (transport associations), organising and managing local public transport, as well as ensuring harmonised fares between different bus and train operators. For inter-\"Verb\u00fcnde\" journeys, the \"bwtarif\", created in 2018, offers seamless tickets across the state.\nAs of 2023, tram and light rail systems exist in Freiburg, Heidelberg, Heilbronn, Karlsruhe, Mannheim, Stuttgart and Ulm. International tram lines also reach Baden-W\u00fcrttemberg: Basel's tram 8 serves Weil am Rhein, while Strasbourg's tram D extends to Kehl. There is also a trolleybus system in Esslingen am Neckar.\nAirports.\nBaden-W\u00fcrttemberg is served by several airports. The largest is Stuttgart Airport, which functions as the state\u2019s main international hub. Other commercial airports include Karlsruhe/Baden-Baden Airport and Friedrichshafen Airport. The EuroAirport Basel Mulhouse Freiburg, located on the French\u2013German\u2013Swiss border, Frankfurt Airport and Zurich Airport are also frequently used by passengers from the region.\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nThe population of Baden-W\u00fcrttemberg was 10,486,660 in 2014, of which 5,354,105 were female and 5,132,555 male. In 2006, the birth rate of 8.61 per 1000 was almost equal to the death rate of 8.60 per 1000. 14.87 percent of the population was under the age of 15, whereas the proportion of people aged 65 and older was at 18.99 per cent (2008). The dependency ratio\u2013the ratio of people aged under 15 and over 64 in comparison to the working-age population (aged 15\u201364)\u2013was 512 per 1000 (2008). In 2018, Baden-W\u00fcrttemberg ranked 2 on the Human Development Index (HDI) among all states in Germany, after Hamburg. With an average life expectancy of 79.8 years for men and 84.2 years for women (2017\u20132019 life table), Baden-W\u00fcrttemberg ranks first in this category among all states in Germany for both sexes.\nBaden-W\u00fcrttemberg has long been a preferred destination of immigrants. As of 2013[ [update]], almost 28% of its population had a migration background as defined by the Federal Statistical Office of Germany; this number clearly surpassed the German average of 21% and was higher than in any other German state with the exception of the city states of Hamburg and Bremen. As of 2014[ [update]], 9,355,239 of the population held German citizenship, whereas 1,131,421 were foreign nationals.\n&lt;templatestyles src=\"Template:Largest_cities/styles.css\" /&gt;\nVital statistics.\nSource:\nReligion.\nNorthern and most of central W\u00fcrttemberg has been traditionally Protestant (particularly Lutheran) since the Reformation in 1534 (with its centre at the famous T\u00fcbinger Stift). The former Electorate of the Palatinate (Northwestern Baden) with its capital Heidelberg was shaped by Calvinism before being integrated into Baden. Upper Swabia, and the Upper Neckar Valley up to the bishop seat of Rottenburg, and Southern Baden (the Catholic archbishop has its seat in Freiburg) have traditionally been bastions of Roman Catholicism. Catholics have a very narrow plurality in the state, with 6% of the population adhering to Islam and 24% of the population disclaiming any religion or adhering to other faiths.\nSports.\nFootball.\nFootball is the biggest sport in Baden-W\u00fcrttemberg. Clubs currently competing in the Bundesliga include SC Freiburg, TSG 1899 Hoffenheim and the most successful club in the state, VfB Stuttgart, meanwhile Karlsruher SC, 1. FC Heidenheim, SV Sandhausen and Waldhof Mannheim also compete in the top three German soccer divisions.\nHandball.\nHandball-Bundesliga multiple champions Frisch Auf G\u00f6ppingen and Rhein-Neckar L\u00f6wen, as well as TVB 1898 Stuttgart are based in Baden-W\u00fcrttemberg. Frisch Auf G\u00f6ppingen won EHF Champions League (Europe's premier club tournament) twice, in 1960 and 1962. Several major women's handball clubs are also based here, including 3-time Frauen Bundesliga champions SG BBM Bietigheim.\nBasketball.\nCompared to other German states, Baden-W\u00fcrttemberg has a particularly high density of professional basketball teams such as Riesen Ludwigsburg, ratiopharm Ulm, USC Heidelberg, PS Karlsruhe Lions and others.\nIce hockey.\nOne of the most decorated German ice hockey clubs, Adler Mannheim, is based in the city of Mannheim. Other DEL clubs, such as Bietigheim Steelers and Schwenninger Wild Wings are also based in the state.\nVolleyball.\nBaden-W\u00fcrttemberg is home to the most successful club in German volleyball history, the Volleyball-Bundesliga club VfB Friedrichshafen, which won CEV Champions League in 2006\u201307 season.\nMotorsport.\nThere are also multiple motorsport facilities, the most famous one being long-time Formula One circuit Hockenheimring.\nDialects.\nAlemannic and Franconian dialects of German are spoken in Baden-W\u00fcrttemberg. Slightly different variants of the Alemannic dialect Swabian are spoken in central and southern W\u00fcrttemberg, including in Upper Swabia, the Swabian Alb, and the central Neckar Valley of the Stuttgart region. In South Baden, the local dialects are Low Alemannic and High Alemannic (i.e., variants of what is also Swiss German). In the northern part of Baden, i.e., the area around Karlsruhe, Heilbronn and Mosbach, South Franconian dialects are predominant. In the \"Kurpfalz\", however, with the cities of Heidelberg and Mannheim, the idiom is Rhine Franconian (i.e., Palatinate German), while in the Northeast of Baden-W\u00fcrttemberg East Franconian is spoken.\nThe same or similar Alemannic dialects are also spoken in the neighboring regions, especially in Bavarian Swabia, Alsace (Alsatian), German-speaking Switzerland (Swiss German), and the Austrian Vorarlberg. In contrast, the other Franconian dialects range from the Netherlands over the Rhineland, Lorraine, and Hesse up to Franconia in northern Bavaria.\nYiddish and Plei\u00dfne were spoken while Romani is still being used by some.\nA variant of the Alemannic German of Baden developed into the Colonia Tovar dialect, spoken by descendants of immigrants from Baden who went to Venezuela in 1843.\nForeigners.\nAs of 2022[ [update]], the largest groups of foreign residents by country of origin were:\nSources.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "66402", "revid": "8660314", "url": "https://en.wikipedia.org/wiki?curid=66402", "title": "Brown v. Board of Education", "text": "1954 U.S. Supreme Court case on racial segregation\n1954 United States Supreme Court case\nBrown v. Board of Education of Topeka, 347 U.S. 483 (1954), was a landmark decision of the United States Supreme Court which ruled that U.S. state laws establishing racial segregation in public schools violate the Equal Protection Clause of the Fourteenth Amendment and hence are unconstitutional, even if the segregated facilities are presumed to be equal. The decision partially overruled the Court's 1896 decision \"Plessy v. Ferguson\", which had held that racial segregation laws did not violate the U.S. Constitution as long as the facilities for each race were equal in quality, a doctrine that had come to be known as \"separate but equal\" and was rejected in \"Brown\" based on the argument that separate facilities are inherently unequal. The Court's unanimous decision in \"Brown\" and its related cases paved the way for integration and was a major victory of the civil rights movement, and a model for many future impact litigation cases.\nThe case involved the public school system in Topeka, Kansas, which in 1951 had refused to enroll the daughter of local black resident Oliver Brown at the school closest to her home, instead requiring her to ride a bus to a segregated black school farther away. The Browns and twelve other local black families in similar situations filed a class-action lawsuit in U.S. federal court against the Topeka Board of Education, alleging its segregation policy was unconstitutional. A special three-judge court of the U.S. District Court for the District of Kansas heard the case and ruled against the Browns, relying on the precedent of \"Plessy\" and its \"separate but equal\" doctrine. The Browns, represented by NAACP chief counsel Thurgood Marshall, appealed the ruling directly to the Supreme Court, who issued a unanimous 9\u20130 decision in favor of the Browns. However, the decision's 14 pages did not spell out any sort of method for ending racial segregation in schools, and the Court's second decision in \"Brown II\" (1955) only ordered states to desegregate \"with all deliberate speed\".\nIn the Southern United States, the reaction to \"Brown\" among most white people was \"noisy and stubborn\", especially in the Deep South where racial segregation was deeply entrenched in society. Many Southern governmental and political leaders embraced a plan known as \"massive resistance\", created by Senator Harry F. Byrd, in order to frustrate attempts to force them to de-segregate their school systems, most notably immortalised by the Little Rock crisis. The Court reaffirmed its ruling in \"Brown\" in \"Cooper v. Aaron\", explicitly stating that state officials and legislators had no jurisdiction to nullify its ruling.\nBackground.\nFor much of the 60 years preceding the \"Brown\" case, race relations in the United States had been dominated by racial segregation. Such state policies had been endorsed by the United States Supreme Court ruling in \"Plessy v. Ferguson\" (1896), which held that as long as the separate facilities for separate races were equal, state segregation did not violate the Fourteenth Amendment's Equal Protection Clause (\"no State shall ... deny to any person ... the equal protection of the laws\"). Racial segregation in education varied widely from the 17 states that required racial segregation to the 16 in which it was prohibited. Beginning in the 1930s, a legal strategy was pursued, led by scholars at Howard University and activists at the NAACP, that sought to undermine states' public education segregation by first focusing on the graduate school setting. This led to success in the cases of \"Sweatt v. Painter\", 339 U.S. 629 (1950) and \"McLaurin v. Oklahoma State Regents\", 339 U.S. 637 (1950), suggesting that racial segregation was inherently unequal (at least in some settings), which paved the way for \"Brown\".\nThe plaintiffs in \"Brown\" asserted that the system of racial separation in all schools, while masquerading as providing separate but equal treatment of both white and black Americans, instead perpetuated inferior accommodations, services, and treatment for black Americans. \"Brown\" was influenced by UNESCO's 1950 Statement, signed by a wide variety of internationally renowned scholars, titled \"The Race Question\". This declaration denounced previous attempts at scientifically justifying racism as well as morally condemning racism. Another work that the Supreme Court cited was Gunnar Myrdal's \"\" (1944). Myrdal had been a signatory of the UNESCO declaration.\nThe United States and the Soviet Union were both at the height of the Cold War during this time, and U.S. officials, including Supreme Court justices, were highly aware of the harm that segregation and racism were doing to America's international image. When Justice William O. Douglas traveled to India in 1950, the first question he was asked was, \"Why does America tolerate the lynching of Negroes?\" Douglas later wrote that he had learned from his travels that \"the attitude of the United States toward its colored minorities is a powerful factor in our relations with India.\" Chief Justice Earl Warren, nominated to the Supreme Court by President Dwight D. Eisenhower, echoed Douglas's concerns in a 1954 speech to the American Bar Association, proclaiming that \"Our American system like all others is on trial both at home and abroad,\u00a0... the extent to which we maintain the spirit of our constitution with its Bill of Rights, will in the long run do more to make it both secure and the object of adulation than the number of hydrogen bombs we stockpile.\"\nDistrict court case.\nFiling and arguments.\nIn 1951, a class-action lawsuit was filed against the Board of Education of the City of Topeka, Kansas, in the United States District Court for the District of Kansas. The plaintiffs were thirteen Topeka parents on behalf of their 20 children.\nThe suit called for the school district to reverse its policy of racial segregation. The Topeka Board of Education operated separate elementary schools due to a 1879 Kansas law, which permitted (but did not require) districts to maintain separate elementary school facilities for black and white students in 12 communities with populations over 15,000. The plaintiffs had been recruited by the leadership of the Topeka NAACP. Notable among the Topeka NAACP leaders were the chairman McKinley Burnett; Charles Scott, one of three serving as legal counsel for the chapter; and Lucinda Todd.\nThe named African-American plaintiff, Oliver Brown, was a parent, a welder in the shops of the Santa Fe Railroad, as well as an assistant pastor at his local church. He was convinced to join the lawsuit by a childhood friend, Charles Scott. Brown's daughter Linda Carol Brown, a third grader, had to walk six blocks to her school bus stop to ride to Monroe Elementary, her segregated black school away, while Sumner Elementary, a white school, was seven blocks from her house.\nAs directed by the NAACP leadership, the parents each attempted to enroll their children in the closest neighborhood school in the fall of 1951. They were each refused enrollment and redirected to the segregated schools.\nThe case \"Oliver Brown et al. v. The Board of Education of Topeka, Kansas\" was named after Oliver Brown as a legal strategy to have a man at the head of the roster. The lawyers, and the National Chapter of the NAACP, also felt that having Mr. Brown at the head of the roster would be better received by the U.S. Supreme Court justices. The 13 plaintiffs were: Oliver Brown, Darlene Brown, Lena Carper, Sadie Emmanuel, Marguerite Emerson, Shirley Fleming, Zelma Henderson, Shirley Hodison, Maude Lawton, Alma Lewis, Iona Richardson, Vivian Scales, and Lucinda Todd. The last surviving plaintiff, Zelma Henderson, died in Topeka, on May 20, 2008, at age 88.\nDistrict court opinion.\nThe District Court ruled in favor of the Board of Education, citing the U.S. Supreme Court precedent set in \"Plessy v. Ferguson\". Judge Walter Huxman wrote the opinion for the three-judge District Court panel, including nine \"findings of fact,\" based on the evidence presented at trial. Although finding number eight stated that segregation in public education has a detrimental effect on negro children, the court denied relief on the ground that the negro and white schools in Topeka were substantially equal with respect to buildings, transportation, curricula, and educational qualifications of teachers. This finding would be specifically cited in the subsequent Supreme Court opinion of this case.\nSupreme Court arguments.\nThe case of \"Brown v. Board of Education\" as heard before the Supreme Court combined five cases: \"Brown\" itself, \"Briggs v. Elliott\" (filed in South Carolina), \"Davis v. County School Board of Prince Edward County\" (filed in Virginia), \"Gebhart v. Belton\" (filed in Delaware), and \"Bolling v. Sharpe\" (filed in Washington, D.C.).\nAll were NAACP-sponsored cases. The \"Davis\" case, the only case of the five originating from a student protest, began when 16-year-old Barbara Rose Johns organized and led a 450-student walkout of Moton High School. The \"Gebhart\" case was the only one where a trial court, affirmed by the Delaware Supreme Court, found that discrimination was unlawful; in all the other cases the plaintiffs had lost as the original courts had found discrimination to be lawful.\nThe Kansas case was unique among the group in that there was no contention of gross inferiority of the segregated schools' physical plant, curriculum, or staff. The district court found substantial equality as to all such factors. The lower court, in its opinion, noted that, in Topeka, \"the physical facilities, the curricula, courses of study, qualification and quality of teachers, as well as other educational facilities in the two sets of schools [were] comparable.\" The lower court observed that \"colored children in many instances are required to travel much greater distances than they would be required to travel could they attend a white school\" but also noted that the school district \"transports colored children to and from school free of charge\" and that \"no such service [was] provided to white children.\" In the Delaware case the district court judge in \"Gebhart\" ordered that the black students be admitted to the white high school due to the substantial harm of segregation and the differences that made the separate schools unequal.\nUnder the leadership of Walter Reuther, the United Auto Workers donated $75,000 to help pay for the NAACP's efforts at the Supreme Court. The NAACP's chief counsel, Thurgood Marshall\u2014who was later appointed to the U.S. Supreme Court in 1967\u2014argued the case before the Supreme Court for the plaintiffs. Assistant attorney general Paul Wilson\u2014later distinguished emeritus professor of law at the University of Kansas\u2014conducted the state's ambivalent defense in his first appellate argument.\nIn December 1952, the Justice Department filed an \"amicus curiae\" (\"friend of the court\") brief in the case. The brief was unusual in its heavy emphasis on foreign-policy considerations of the Truman administration in a case ostensibly about domestic issues. Of the seven pages covering \"the interest of the United States,\" five focused on the way school segregation hurt the United States in the Cold War competition for the friendship and allegiance of non-white peoples in countries then gaining independence from colonial rule. Attorney General James P. McGranery noted that \"the existence of discrimination against minority groups in the United States has an adverse effect upon our relations with other countries. Racial discrimination furnishes grist for the Communist propaganda mills.\" The brief also quoted a letter by Secretary of State Dean Acheson lamenting that \"the United States is under constant attack in the foreign press, over the foreign radio, and in such international bodies as the United Nations because of various practices of discrimination in this country.\"\nBritish barrister and parliamentarian Anthony Lester has written that \"Although the Court's opinion in \"Brown\" made no reference to these considerations of foreign policy, there is no doubt that they significantly influenced the decision.\"\nConsensus building.\nIn spring 1953, the court heard the case, but was unable to decide the issue and asked to rehear the case in fall 1953, with special attention to whether the Fourteenth Amendment's Equal Protection Clause prohibited the operation of separate public schools for whites and blacks.\nConference notes and draft decisions illustrate the division of opinions before the decision was issued. Justices William O. Douglas, Hugo Black, Harold Hitz Burton, and Sherman Minton were predisposed to overturn \"Plessy\". Fred M. Vinson noted that Congress had not adopted desegregation legislation; Stanley F. Reed discussed incomplete cultural assimilation and states' rights, and was inclined to the view that segregation worked to the benefit of the African-American community; Tom C. Clark wrote that \"we had led the states on to think segregation is OK and we should let them work it out.\" Felix Frankfurter and Robert H. Jackson disapproved of segregation, but were also opposed to judicial activism and expressed concerns about the proposed decision's enforceability. Chief Justice Vinson had been a key stumbling block. After Vinson died in September 1953, President Dwight D. Eisenhower appointed Earl Warren as Chief Justice. Warren had supported the integration of Mexican-American students in California school systems following \"Mendez v. Westminster\". However, Eisenhower invited Earl Warren to a White House dinner along with John W. Davis, who was the oral advocate against desegregation in \"Briggs v. Elliott\", where the president told Warren privately: \"These [southern whites] are not bad people. All they are concerned about is to see that their sweet little girls are not required to sit in school alongside some big overgrown Negroes.\" Nevertheless, the Justice Department sided with the African-American plaintiffs.\nWhile all but one justice personally rejected segregation, the judicial restraint faction questioned whether the Constitution gave the court the power to order its end. The activist faction believed the Fourteenth Amendment did give the necessary authority and were pushing to go ahead. Warren, who held only a recess appointment, held his tongue until the Senate confirmed his appointment.\nWarren convened a meeting of the justices, and presented to them the simple argument that the only reason to sustain segregation was an honest belief in the inferiority of Negroes. Warren further submitted that the court must overrule \"Plessy\" to maintain its legitimacy as an institution of liberty, and it must do so unanimously to avoid massive Southern resistance. He began to build a unanimous opinion. Although most justices were immediately convinced, Warren spent some time after this famous speech convincing everyone to sign onto the opinion. Justice Jackson dropped his concurrence and Reed finally decided to drop his dissent. The final decision was unanimous. Warren drafted the basic opinion and kept circulating and revising it until he had an opinion endorsed by all the members of the court. Reed was the last holdout and reportedly cried during the reading of the opinion.\nSupreme Court decision.\nOn May 17, 1954, the Supreme Court issued a unanimous 9\u20130 decision in favor of the Brown family and the other plaintiffs. The decision consists of a single opinion written by chief justice Earl Warren, which all the justices joined.\nThe Court's opinion began by discussing whether the Fourteenth Amendment, adopted in 1868, was meant to abolish segregation in public education. The Court said that it had been unable to reach a conclusion on the question, even after hearing a second round of oral arguments from the parties' lawyers specifically on the historical sources.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Reargument was largely devoted to the circumstances surrounding the adoption of the Fourteenth Amendment in 1868. It covered exhaustively consideration of the Amendment in Congress, ratification by the states, then-existing practices in racial segregation, and the views of proponents and opponents of the Amendment. This discussion and our own investigation convince us that, although these sources cast some light, it is not enough to resolve the problem with which we are faced. At best, they are inconclusive.\u2014\u200a\nThe Court said the question was complicated by the major social and governmental changes that had taken place in the late 19th and early 20th centuries. It observed that public schools had been uncommon in the American South in the late 1860s. At that time, Southern white children whose families could afford schooling usually attended private schools, while the education of Southern black children was \"almost nonexistent\", to the point that in some Southern states the education of black people was forbidden by law. The Court contrasted this with the situation in 1954: \"Today, education is perhaps the most important function of our local and state governments.\" The Court concluded that, in making its ruling, it would have to \"consider public education in light of its full development and its present place in American life throughout the Nation.\"\nDuring the segregation era, it was common for black schools to have fewer resources and poorer facilities than white schools despite the equality required by the \"separate but equal\" doctrine. The \"Brown\" Court did not address this issue, however, probably because some of the school districts involved in the case had improved their black schools in order to \"equalize\" them with the quality of the white schools. This prevented the Court from finding a violation of the Fourteenth Amendment's Equal Protection Clause in \"measurable inequalities\" between all white and black schools and forced the Court to look to the effects of segregation itself. The Court therefore framed the case around the more general question of whether the principle of \"separate but equal\" was constitutional when applied to public education. \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We come then to the question presented: Does segregation of children in public schools solely on the basis of race, even though the physical facilities and other \"tangible\" factors may be equal, deprive the children of the minority group of equal educational opportunities?\u2014\u200a\nIn answer, the Court held that it did. The Court ruled that state-mandated segregation, even if implemented in schools of otherwise equal quality, is inherently unequal because of its psychological effect upon the segregated black children.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;To separate them [black children] from others of similar age and qualifications solely because of their race generates a feeling of inferiority as to their status in the community that may affect their hearts and minds in a way unlikely to ever be undone. The effect of this separation on their educational opportunities was well stated by a finding in the Kansas case ... :\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Segregation of white and colored children in public schools has a detrimental effect upon the colored children. The impact is greater when it has the sanction of the law, for the policy of separating the races is usually interpreted as denoting the inferiority of the negro group. A sense of inferiority affects the motivation of a child to learn. Segregation with the sanction of law, therefore, has a tendency to retard the educational and mental development of negro children and to deprive them of some of the benefits they would receive in a racially integrated school system.\"\nWhatever may have been the extent of psychological knowledge at the time of \"Plessy v. Ferguson\", this finding is amply supported by modern authority. Any language in \"Plessy v. Ferguson\" contrary to this finding is rejected.\u2014\u200a\nThe Court supported its conclusion with citations\u2014in a footnote, not the main text of the opinion\u2014to several psychological studies concluding that segregating black children made them feel inferior and interfered with their learning. These studies included those of Kenneth and Mamie Clark, whose experiments in the 1940s had suggested that black American children from segregated environments preferred white dolls over black dolls.\nThe Court then concluded its relatively short opinion by declaring that segregated public education was inherently unequal, violated the Equal Protection Clause, and therefore was unconstitutional:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We conclude that in the field of public education the doctrine of \"separate but equal\" has no place. Separate educational facilities are inherently unequal. Therefore, we hold that the plaintiffs and others similarly situated for whom the actions have been brought are, by reason of the segregation complained of, deprived of the equal protection of the laws guaranteed by the Fourteenth Amendment.\u2014\u200a\nThe Court did not close with an order to implement the integration of the schools of the various jurisdictions. Instead, it requested the parties re-appear before the Court the following Term to hold arguments on what the appropriate remedy should be. This became the case known as \"Brown II\", described below.\nReaction and aftermath.\nAmericans mostly cheered the Court's decision in \"Brown\", but most white Southerners decried it. Many white Southerners viewed \"Brown\" as \"a day of catastrophe\u2014a Black Monday\u2014a day something like Pearl Harbor.\" In the face of entrenched Southern opposition, progress on integrating American schools was slow. The American political historian Robert G. McCloskey described:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The reaction of the white South to this judicial onslaught on its institutions was noisy and stubborn. Certain \"border states,\" which had formerly maintained segregated school systems, did integrate, and others permitted the token admission of a few Negro students to schools that had once been racially unmixed. However, the Deep South made no moves to obey the judicial command, and in some districts there can be no doubt that the Desegregation decision hardened resistance to integration proposals.\nIn Virginia, Senator Harry F. Byrd organized the Massive Resistance movement that included the closing of schools rather than desegregating them.\nFor several decades after the \"Brown\" decision, African-American teachers, principals, and other school staff who worked in segregated Black schools were fired or laid off as Southerners sought to create a system of integrated schools with White leadership. According to historian Michael Fultz, \"In many ways the South moved faster, with more 'deliberate speed' in displacing Black educators than it did in desegregating schools.\"\nDeep South.\nTexas Attorney General John Ben Shepperd organized a campaign to generate legal obstacles to the implementation of desegregation.\nIn September 1957, Arkansas governor Orval Faubus called out the Arkansas Army National Guard to block the entry of nine black students, later known as the \"Little Rock Nine\" after the desegregation of Little Rock Central High School. President Dwight D. Eisenhower responded by asserting federal control over the Arkansas National Guard and deploying troops from the U.S. Army's 101st Airborne Division stationed at Fort Campbell to ensure the black students could safely register for and attend classes.\nAlso in 1957, Florida's response was mixed. Its legislature passed an Interposition Resolution denouncing the decision and declaring it null and void. But Florida Governor LeRoy Collins, though joining in the protest against the court decision, refused to sign it, arguing that the attempt to overturn the ruling must be done by legal methods.\nIn Mississippi, fear of violence prevented any plaintiff from bringing a school desegregation suit for the next nine years. When Medgar Evers sued in 1963 to desegregate schools in Jackson, Mississippi, White Citizens Council member Byron De La Beckwith murdered him. Two subsequent trials resulted in hung juries. Beckwith was not convicted of the murder until 1994.\nIn June 1963, Alabama governor George Wallace personally blocked the door to the University of Alabama's Foster Auditorium to prevent the enrollment of two black students in what became known as the \"Stand in the Schoolhouse Door\" incident. Wallace sought to uphold his \"segregation now, segregation tomorrow, segregation forever\" promise he had given in his 1963 inaugural address. Wallace moved aside only when confronted by General Henry V. Graham of the Alabama National Guard, whom President John F. Kennedy had ordered to intervene.\nNative American communities were also heavily impacted by segregation laws with native children also being prohibited from attending white institutions. Native American children considered light-complexioned were allowed to ride school buses to previously all white schools, while dark-skinned Native children from the same band were still barred from riding the same buses. Tribal leaders, having learned about Martin Luther King Jr.'s desegregation campaign in Birmingham, Alabama, contacted him for assistance. King promptly responded to the tribal leaders and through his intervention the problem was quickly resolved.\nUpper South.\nIn North Carolina, there was often a strategy of nominally accepting \"Brown\", but tacitly resisting it. On May 18, 1954, the Greensboro, North Carolina school board declared that it would abide by the \"Brown\" ruling. This was the result of the initiative of D. E. Hudgins Jr., a former Rhodes Scholar and prominent attorney, who chaired the school board. This made Greensboro the first, and for years the only, city in the South, to announce its intent to comply. However, others in the city resisted integration, putting up legal obstacles to the actual implementation of school desegregation for years afterward, and in 1969, the federal government found the city was not in compliance with the 1964 Civil Rights Act. Transition to a fully integrated school system did not begin until 1971, after numerous local lawsuits and both nonviolent and violent demonstrations. Historians have noted the irony that Greensboro, which had heralded itself as such a progressive city, was one of the last holdouts for school desegregation.\nIn Moberly, Missouri, the schools were desegregated, as ordered. However, after 1955, the African-American teachers from the local \"negro school\" were not retained; this was ascribed to poor performance. They appealed their dismissal in \"Naomi Brooks et al., Appellants, v. School District of City of Moberly, Missouri, Etc., et al.\"; but it was upheld, and SCOTUS declined to hear a further appeal.\nVirginia had one of the companion cases in \"Brown\", involving the Prince Edward County schools. Significant opposition to the \"Brown\" verdict included U.S. Senator Harry F. Byrd, who led the Byrd Organization and promised a strategy of Massive Resistance. Governor Thomas Stanley, a member of the Byrd Organization, appointed the Gray Commission, 32 Democrats led by state senator Garland Gray, to study the issue and make recommendations. The commission recommended giving localities \"broad discretion\" in meeting the new judicial requirements. However, in 1956, a special session of the Virginia legislature adopted a legislative package which allowed the governor to simply close all schools under desegregation orders from federal courts. In early 1958, newly elected Governor J. Lindsay Almond closed public schools in Charlottesville, Norfolk, and Warren County rather than comply with desegregation orders, leaving 10,000 children without schools despite efforts of various parent groups. However, he reconsidered when on the Lee-Jackson state holiday, the Virginia Supreme Court ruled the closures violated the state constitution, and a panel of federal judges ruled they violated the U.S. Constitution. In early February 1959, both the Arlington County (also subject to an NAACP lawsuit, and which had lost its elected school board pursuant to other parts of the Stanley Plan) and Norfolk schools desegregated peacefully. Soon, all counties reopened and integrated with the exception of Prince Edward County that took the extreme step of choosing not to appropriate any funding for its school system, thus forcing all its public schools to close, although Prince Edward County provided tuition grants for all students, regardless of their race, to use for private, nonsectarian education. Since no private schools existed for blacks within the county, black children in the county either had to leave the county to receive any education between 1959 and 1963, or received no education. All private schools in the region remained racially segregated. This lasted until 1964, when the U.S. Supreme Court ruled Prince Edward County's decision to provide tuition grants for private schools that only admitted whites violated the Equal Protection Clause of the 14th Amendment, in the case of \"Griffin v. County School Board of Prince Edward County\".\nNorth.\nMany Northern cities also had de facto segregation policies, which resulted in a vast gulf in educational resources between black and white communities. In Harlem, New York, for example, not a single new school had been built since the turn of the century, nor did a single nursery school exist, even as the Second Great Migration caused overcrowding of existing schools. Existing schools tended to be dilapidated and staffed with inexperienced teachers. Northern officials were in denial of the segregation, but \"Brown\" helped stimulate activism among African-American parents like Mae Mallory who, with support of the NAACP, initiated a successful lawsuit against the city and State of New York on \"Brown\"'s principles. Mallory and thousands of other parents bolstered the pressure of the lawsuit with a school boycott in 1959. During the boycott, some of the first Freedom Schools of the period were established. The city responded to the campaign by permitting more open transfers to high-quality, historically white schools. (New York's African-American community, and Northern desegregation activists generally, now found themselves contending with the problem of white flight, however.)\nTopeka.\nThe Topeka junior high schools had been integrated since 1941. Topeka High School was integrated from its inception in 1871 and its sports teams from 1949 onwards. The Kansas law permitting segregated schools allowed them only \"below the high school level\".\nSoon after the district court decision, election outcomes and the political climate in Topeka changed. The Board of Education of Topeka began to end segregation in the Topeka elementary schools in August 1953, integrating two attendance districts. All the Topeka elementary schools were changed to neighborhood attendance centers in January 1956, although existing students were allowed to continue attending their prior assigned schools at their option. Plaintiff Zelma Henderson, in a 2004 interview, recalled that no demonstrations or tumult accompanied desegregation in Topeka's schools: \"They accepted it\u00a0... It wasn't too long until they integrated the teachers and principals.\"\nThe Topeka Public Schools administration building is named in honor of McKinley Burnett, NAACP chapter president who organized the case.\nMonroe Elementary was designated a National Historic Site under the National Park Service on October 26, 1992, and redesignated a National Historical Park on May 12, 2022.\nThe intellectual roots of \"Plessy v. Ferguson\", the landmark United States Supreme Court decision upholding the constitutionality of racial segregation in 1896 under the doctrine of \"separate but equal\" were, in part, tied to the scientific racism of the era. However, the popular support for the decision was more likely a result of the racist beliefs held by many whites at the time. In deciding \"Brown v. Board of Education\", the Supreme Court rejected the ideas of scientific racists about the need for segregation, especially in schools. The court buttressed its holding by citing (in http://) social science research about the harms to black children caused by segregated schools.\nBoth scholarly and popular ideas of hereditarianism played an important role in the attack and backlash that followed the \"Brown\" decision. \"Mankind Quarterly\" was founded in 1960, in part in response to the \"Brown\" decision.\nLegal criticism and praise.\nWilliam Rehnquist wrote a memo titled \"A Random Thought on the Segregation Cases\" when he was a law clerk for Justice Robert H. Jackson in 1952, during early deliberations that led to the \"Brown v. Board of Education\" decision. In his memo, Rehnquist argued: \"I realize that it is an unpopular and unhumanitarian position, for which I have been excoriated by 'liberal' colleagues but I think \"Plessy v. Ferguson\" was right and should be reaffirmed.\" Rehnquist continued, \"To the argument\u00a0... that a majority may not deprive a minority of its constitutional right, the answer must be made that while this is sound in theory, in the long run it is the majority who will determine what the constitutional rights of the minorities are.\" Rehnquist also argued for \"Plessy\" with other law clerks.\nHowever, during his 1971 confirmation hearings, Rehnquist said, \"I believe that the memorandum was prepared by me as a statement of Justice Jackson's tentative views for his own use.\" Jackson had initially planned to join a dissent in \"Brown\". Later, at his 1986 hearings for the slot of Chief Justice, Rehnquist put further distance between himself and the 1952 memo: \"The bald statement that Plessy was right and should be reaffirmed, was not an accurate reflection of my own views at the time.\" In any event, while serving on the Supreme Court, Rehnquist made no effort to reverse or undermine the \"Brown\" decision, and frequently relied upon it as precedent.\nChief Justice Warren's reasoning was broadly criticized by contemporary legal academics with Judge Learned Hand decrying that the Supreme Court had \"assumed the role of a third legislative chamber\" and Herbert Wechsler finding \"Brown\" impossible to justify based on neutral principles.\nSome aspects of the \"Brown\" decision are still debated. Notably, Supreme Court Justice Clarence Thomas, himself an African American, wrote in \"Missouri v. Jenkins\" (1995) that at the very least, \"Brown I\" has been misunderstood by the courts.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Brown I\" did not say that \"racially isolated\" schools were inherently inferior; the harm that it identified was tied purely to de jure segregation, not de facto segregation. Indeed, \"Brown I\" itself did not need to rely upon any psychological or social-science research in order to announce the simple, yet fundamental truth that the Government cannot discriminate among its citizens on the basis of race. ...\nSegregation was not unconstitutional because it might have caused psychological feelings of inferiority. Public school systems that separated blacks and provided them with superior educational resources making blacks \"feel\" superior to whites sent to lesser schools\u2014would violate the Fourteenth Amendment, whether or not the white students felt stigmatized, just as do school systems in which the positions of the races are reversed. Psychological injury or benefit is irrelevant ...\nGiven that desegregation has not produced the predicted leaps forward in black educational achievement, there is no reason to think that black students cannot learn as well when surrounded by members of their own race as when they are in an integrated environment. (...) Because of their \"distinctive histories and traditions,\" black schools can function as the center and symbol of black communities, and provide examples of independent black leadership, success, and achievement.\nSome Constitutional originalists, notably Raoul Berger in his influential 1977 book \"Government by Judiciary,\" make the case that \"Brown\" cannot be defended by reference to the original understanding of the 14th Amendment. They support this reading of the 14th Amendment by noting that the Civil Rights Act of 1875 did not ban segregated schools and that the same Congress that passed the 14th Amendment also voted to segregate schools in the District of Columbia. Other originalists, including Michael W. McConnell, a federal judge on the United States Court of Appeals for the Tenth Circuit, in his article \"Originalism and the Desegregation Decisions,\" argue that the Radical Reconstructionists who spearheaded the 14th Amendment were in favor of desegregated southern schools. Evidence supporting this interpretation of the 14th Amendment has come from archived Congressional records showing that proposals for federal legislation which would enforce school integration were debated in Congress a few years following the amendment's ratification.\nIn response to Michael McConnell's research, Raoul Berger argued that the Congressmen and Senators who were advocating in favor of school desegregation in the 1870s were trying to rewrite the 14th Amendment in order to make the 14th Amendment fit their political agenda and that the actual understanding of the 14th Amendment from 1866 to 1868 (which is when the 14th Amendment was actually passed and ratified) does, in fact, permit US states to have segregated schools. Berger criticized McConnell for being unable to find any reference to school segregation\u2014let alone any reference to a desire to prohibit it\u2014among supporters of the 14th Amendment in the congressional history of this amendment (specifically in the recordings of the 39th United States Congress, since that was the US Congress that actually passed the 14th Amendment) and also criticized McConnell's view that the 1954 view of \"civil rights\" should be decisive in interpreting the 14th Amendment as opposed to the 1866 view of \"civil rights.\" Berger also argues that McConnell failed to provide any evidence that the state legislatures who ratified the 14th Amendment understood it at the time as prohibiting school segregation and that whenever the question of school segregation's compatibility with the US Constitution (as opposed to the separate question of school segregation's compatibility with US state law and/or US state constitutions, where courts have often ruled against school segregation) reached the judiciary in the couple of decades after the passage and ratification of the 14th Amendment (whether in Ohio, Nevada, California, Indiana, or New York), courts have always affirmed the constitutionality of school segregation\u2014as did Michigan Supreme Court Chief Justice Thomas M. Cooley in his 1880 treatise \"The General Principles of Constitutional Law in the United States of America\". In addition, Berger argues that the views of the draftsmen of the 14th Amendment in 1866 are decisive\u2014as opposed to the views of later readers of the 14th Amendment (including the views of supporters of the 14th Amendment after this amendment's passage and ratification due to the fact that even their views and beliefs about the meaning and scope of this Amendment could and sometimes did change over time\u2014like with Nevada U.S. Senator William Morris Stewart, who initially opposed school desegregation but later changed his mind and supported it). To back up his view about original intent being decisive, Berger cites\u2014among other things\u2014an 1871 quote by James A. Garfield to John Bingham where Garfield challenged Bingham's recollection of a statement that Bingham had previously made in 1866\u2014with Garfield telling Bingham that he can make but not unmake history.\nThe case also has attracted some criticism from more liberal authors, including some who say that Chief Justice Warren's reliance on psychological criteria to find a harm against segregated blacks was unnecessary. For example, Drew S. Days III has written: \"we have developed criteria for evaluating the constitutionality of racial classifications that do not depend upon findings of psychic harm or social science evidence. They are based rather on the principle that 'distinctions between citizens solely because of their ancestry are by their very nature odious to a free people whose institutions are founded upon the doctrine of equality,' \"Hirabayashi v. United States\", 320 U.S. 81 (1943).\u00a0.\u00a0.\u00a0.\"\nIn his book \"The Tempting of America\" (page 82), Robert Bork endorsed the \"Brown\" decision as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;By 1954, when Brown came up for decision, it had been apparent for some time that segregation rarely if ever produced equality. Quite aside from any question of psychology, the physical facilities provided for blacks were not as good as those provided for whites. That had been demonstrated in a long series of cases ... The Court's realistic choice, therefore, was either to abandon the quest for equality by allowing segregation or to forbid segregation in order to achieve equality. There was no third choice. Either choice would violate one aspect of the original understanding, but there was no possibility of avoiding that. Since equality and segregation were mutually inconsistent, though the ratifiers did not understand that, both could not be honored. When that is seen, it is obvious the Court must choose equality and prohibit state-imposed segregation. The purpose that brought the fourteenth amendment into being was equality before the law, and equality, not separation, was written into the law.\nIn June 1987, Philip Elman, a civil rights attorney who served as an associate in the Solicitor General's office during Harry Truman's term, claimed he and Associate Justice Felix Frankfurter were mostly responsible for the Supreme Court's decision, and stated that the NAACP's arguments did not present strong evidence. Elman has been criticized for offering a self-aggrandizing history of the case, omitting important facts, and denigrating the work of civil rights attorneys who had laid the groundwork for the decision over many decades. However, Frankfurter was also known for being one of court's most outspoken advocates of the judicial restraint philosophy of basing court rulings on existing law rather than personal or political considerations. Public officials in the United States today are nearly unanimous in lauding the ruling. In May 2004, the fiftieth anniversary of the ruling, President George W. Bush spoke at the opening of the \"Brown v. Board of Education\" National Historic Site, calling \"Brown\" \"a decision that changed America for the better, and forever.\" Most Senators and Representatives issued press releases hailing the ruling.\nIn a 2016 article in Townhall.com, an outlet of the Salem Media Group, economist Thomas Sowell argued that when Chief Justice Earl Warren declared in the landmark 1954 case of \"Brown v. Board of Education\" that racially separate schools were \"inherently unequal,\" Dunbar High School was a living refutation of that assumption. And it was within walking distance of the Supreme Court.\" In Sowell's estimation, \"Dunbar, which had been accepting outstanding black students from anywhere in the city, could now accept only students from the rough ghetto neighborhood in which it was located\" as a detrimental consequence of the SCOTUS decision.\n\"Brown II\".\nIn 1955, the Supreme Court considered arguments by the schools requesting relief concerning the task of desegregation. In their decision, which became known as \"Brown II\", the court delegated the task of carrying out school desegregation to district courts with orders that desegregation occur \"with all deliberate speed\", a phrase traceable to Francis Thompson's poem \"The Hound of Heaven\".\nSupporters of the earlier decision were displeased with this decision. The language \"all deliberate speed\" was seen by critics as too ambiguous to ensure reasonable haste for compliance with the court's instruction. Many Southern states and school districts interpreted \"Brown II\" as legal justification for resisting, delaying, and avoiding significant integration for years\u2014and in some cases for a decade or more\u2014using such tactics as closing down school systems, using state money to finance segregated \"private\" schools, and \"token\" integration where a few carefully selected black children were admitted to former white-only schools but the vast majority remained in underfunded, unequal black schools.\nFor example, based on \"Brown II\", the U.S. District Court ruled that Prince Edward County, Virginia did not have to desegregate immediately. When faced with a court order to finally begin desegregation in 1959 the county board of supervisors stopped appropriating money for public schools, which remained closed for five years, from 1959 to 1964.\nWhite students in the county were given assistance to attend white-only \"private academies\" that were taught by teachers formerly employed by the public school system, while black students had no education at all unless they moved out of the county. But the public schools reopened after the Supreme Court overturned \"Brown II\" in \"Griffin v. County School Board of Prince Edward County\", declaring that \"the time for mere 'deliberate speed' has run out\" and that the county must provide a public school system for all children regardless of race.\n\"Brown III\".\nIn 1978, Topeka attorneys Richard Jones, Joseph Johnson and Charles Scott Jr. (son of the original \"Brown\" team member), with assistance from the American Civil Liberties Union, persuaded Linda Brown Smith\u2014who now had her own children in Topeka schools\u2014to be a plaintiff in reopening \"Brown\". They were concerned that the Topeka Public Schools' policy of \"open enrollment\" had led to and would lead to further segregation. They also believed that with a choice of open enrollment, white parents would shift their children to \"preferred\" schools that would create both predominantly African-American and predominantly European-American schools within the district. The district court reopened the \"Brown\" case after a 25-year hiatus, but denied the plaintiffs' request finding the schools \"unitary\". In 1989, a three-judge panel of the Tenth Circuit on a 2\u20131 vote found that the vestiges of segregation remained with respect to student and staff assignment. In 1993, the Supreme Court denied the appellant School District's request for \"certiorari\" and returned the case to District Court Judge Richard Rodgers for implementation of the Tenth Circuit's mandate.\nAfter a 1994 plan was approved and a bond issue passed, additional elementary magnet schools were opened and district attendance plans redrawn, which resulted in the Topeka schools meeting court standards of racial balance by 1998. Unified status was eventually granted to Topeka Unified School District No. 501 on July 27, 1999. One of the new magnet schools is named after the Scott family attorneys for their role in the \"Brown\" case and civil rights.\nSubsequent developments.\nIn 1977 the Supreme Court said in \"Dayton Board of Education v. Brinkman\" that the equitable power of federal courts to restructure the operation of local school boards is \"not plenary\" and may be exercised \"only on the basis of a constitutional violation\". Quoting post-\"Brown\" cases like \"Swann v. Charlotte-Mecklenburg\" and \"Milliken v. Bradley\", the Court ruled that federal courts finding a violation of constitutional significance may apply the \"Swann\" standard \u2014\"the scope of the remedy is determined by the nature and extent of the constitutional violation\"\u2014to design a remedy that redresses the difference between the incremental effect of discriminatory violations on school demographics and \"what it would have been in the absence of such constitutional violations\".\nOther comments.\nA PBS film called \"Simple Justice\" retells the story of the \"Brown vs. Board of Education\" case, beginning with the work of the NAACP's Legal Defense Fund's efforts to combat 'separate but equal' in graduate school education and culminating in the historical 1954 decision.\nLinda Brown Thompson later recalled the experience of being refused enrollment:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...we lived in an integrated neighborhood and I had all of these playmates of different nationalities. And so when I found out that day that I might be able to go to their school, I was just thrilled, you know. And I remember walking over to Sumner school with my dad that day and going up the steps of the school and the school looked so big to a smaller child. And I remember going inside and my dad spoke with someone and then he went into the inner office with the principal and they left me out ... to sit outside with the secretary. And while he was in the inner office, I could hear voices and hear his voice raised, you know, as the conversation went on. And then he immediately came out of the office, took me by the hand and we walked home from the school. I just couldn't understand what was happening because I was so sure that I was going to go to school with Mona and Guinevere, Wanda, and all of my playmates.\nLinda Brown died on March 25, 2018, at the age of 75.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n is available from: https:// https:// https:// https:// http:// \n is available from: https:// https:// https:// https:// https:// https:// "}
{"id": "66403", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66403", "title": "Qatar/History", "text": ""}
{"id": "66404", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66404", "title": "Qatar/Transnational issues", "text": ""}
{"id": "66406", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66406", "title": "Qatar/Military", "text": ""}
{"id": "66407", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66407", "title": "Qatar/Communications", "text": ""}
{"id": "66408", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66408", "title": "Qatar/Economy", "text": ""}
{"id": "66409", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66409", "title": "Qatar/Cities", "text": ""}
{"id": "66410", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66410", "title": "Qatar/Government", "text": ""}
{"id": "66411", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66411", "title": "Qatar/People", "text": ""}
{"id": "66412", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=66412", "title": "Qatar/Geography", "text": ""}
{"id": "66414", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=66414", "title": "Baconian method", "text": "Investigative process\nThe Baconian method is the investigative method developed by Francis Bacon, one of the founders of modern science, and thus a first formulation of a modern scientific method. The method was put forward in Bacon's book \"Novum Organum\" (1620), or 'New Method', to replace the old methods put forward in Aristotle's \"Organon\". It influenced the early modern rejection of medieval Aristotelianism.\nDescription in the \"Novum Organum\".\nBacon's view of induction.\nBacon's method is an example of the application of inductive reasoning. However, Bacon's method of induction is much more complex than the essential inductive process of making generalisations from observations. Bacon's method begins with description of the requirements for making the careful, systematic observations necessary to produce quality facts. He then proceeds to use induction, the ability to generalise from a set of facts to one or more axioms. However, he stresses the necessity of not generalising beyond what the facts truly demonstrate. The next step may be to gather additional data, or the researcher may use existing data and the new axioms to establish additional axioms. Specific types of facts can be particularly useful, such as negative instances, exceptional instances and data from experiments. The whole process is repeated in a stepwise fashion to build an increasingly complex base of knowledge, but one which is always supported by observed facts, or more generally speaking, empirical data.\nHe argues in the \"Novum Organum\" that our only hope for building true knowledge is through this careful method. Old knowledge-building methods were often not based in facts, but on broad, ill-proven deductions and metaphysical conjecture. Even when theories were based in fact, they were often broad generalisations and/or abstractions from few instances of casually gathered observations. Using Bacon's process, man could start fresh, setting aside old superstitions, over-generalisations, and traditional (often unproven) \"facts\". Researchers could slowly but accurately build an essential base of knowledge from the ground up. Describing then-existing knowledge, Bacon claims:\nThere is the same degree of licentiousness and error in forming axioms as [there is] in abstracting notions, and [also] in the first principles, which depend in common induction [versus Bacon's induction]; still more is this the case in axioms and inferior propositions derived from syllogisms.\nWhile he advocated a very empirical, observational, reasoned method that did away with metaphysical conjecture, Bacon was a religious man, believed in God, and believed his work had a religious role. He contended, like other researchers at the time, that by doing this careful work man could begin to understand God's wonderful creation, to reclaim the knowledge that had been lost in Adam and Eve's \"fall\", and to make the most of his God-given talents.\nRole of the English Reformation.\nThere is a wider array of seminal works about the interaction of Puritanism and early science. Among others, Dorothy Stimson, Richard Foster Jones, and Robert Merton saw Puritanism as a major driver of the reforms initiated by Bacon and the development of science overall. Steven Matthews is cautious about the interaction with a single confession, as the English Reformation allowed a higher doctrinal diversity compared to the continent. However, Matthews is quite outspoken that \"Bacon's entire understanding of what we call 'science,' and what he called 'natural philosophy,' was fashioned around the basic tenets of his belief system.\"\nApproach to causality.\nThe method consists of procedures for isolating and further investigating the \"form nature\", or cause, of a phenomenon, including the method of agreement, method of difference, and method of concomitant variation.\nBacon suggests that you draw up a list of all things in which the phenomenon you are trying to explain occurs, as well as a list of things in which it does not occur. Then you rank your lists according to the degree in which the phenomenon occurs in each one. Then you should be able to deduce what factors match the occurrence of the phenomenon in one list and don't occur in the other list, and also what factors change in accordance with the way the data had been ranked.\nThus, if an army is successful when commanded by Essex, and not successful when not commanded by Essex: and when it is more or less successful according to the degree of involvement of Essex as its commander, then it is scientifically reasonable to say that being commanded by Essex is causally related to the army's success.\nFrom this Bacon suggests that the underlying cause of the phenomenon, what he calls the \"form\", can be approximated by interpreting the results of one's observations. This approximation Bacon calls the \"First Vintage\". It is not a final conclusion about the formal cause of the phenomenon but merely a hypothesis. It is only the first stage in the attempt to find the form and it must be scrutinised and compared to other hypotheses. In this manner, the truth of natural philosophy is approached \"by gradual degrees\", as stated in his \"Novum Organum\".\nRefinements.\nThe \"Baconian method\" does not end at the First Vintage. Bacon described numerous classes of \"Instances with Special Powers,\" cases in which the phenomenon one is attempting to explain is particularly relevant. These instances, of which Bacon describes 27 in the \"Novum Organum\", aid and accelerate the process of induction.\nAside from the First Vintage and the Instances with Special Powers, Bacon enumerates additional \"aids to the intellect\" which presumably are the next steps in his method. These additional aids, however, were never explained beyond their initial limited appearance in \"Novum Organum\".\nNatural history.\nThe \"Natural History\" of Pliny the Elder was a classical Roman encyclopedia work. Induction, for Bacon's followers, meant a type of rigour applied to factual matters. Reasoning should not be applied in plain fashion to just any collection of examples, an approach identified as \"Plinian\". In considering natural facts, a fuller survey was required to form a basis for going further. Bacon made it clear he was looking for more than \"a botany\" with discursive accretions.\nIn concrete terms, the cabinet of curiosities, exemplifying the Plinian approach, was to be upgraded from a source of wonderment to a challenge to science. The main source in Bacon's works for the approach was his \"Sylva Sylvarum\", and it suggested a more systematic collection of data in the search for causal explanations.\nUnderlying the method, as applied in this context, are therefore the \"tables of natural history\" and the ways in which they are to be constructed. Bacon's background in the common law has been proposed as a source for this concept of investigation.\nAs a general intellectual programme, Bacon's ideas on \"natural history\" have been seen as a broad influence on British writers later in the 17th century, in particular in economic thought and within the Royal Society.\nIdols of the mind (\"idola mentis\").\nBacon also listed what he called the idols (false images) of the mind. He described these as things which obstructed the path of correct scientific reasoning.\nInfluence.\nThe physician Thomas Browne (1605\u20131682) was one of the first scientists to adhere to the empiricism of the Baconian method. His encyclopaedia \"Pseudodoxia Epidemica\" (1st edition 1646 \u2013 5th edition 1672) includes numerous examples of Baconian investigative methodology, while its preface echoes lines from Bacon's \"On Truth\" from \"The Advancement of Learning\" (1605). Isaac Newton's saying \"hypotheses non fingo\" (I don't frame hypotheses) occurs in later editions of the \"Principia\". It represents his preference for rules that could be demonstrated, as opposed to unevidenced hypotheses.\nThe Baconian method was further developed and promoted by John Stuart Mill. His 1843 book, \"A System of Logic\", was an effort to shed further light on issues of causation. In this work, he formulated the five principles of inductive reasoning now known as Mill's methods.\nFrankfurt School critique of Baconian method.\nMax Horkheimer and Theodor Adorno observe that Bacon shuns \"knowledge that tendeth but to satisfaction\" in favor of effective procedures. While the Baconian method disparages idols of the mind, its requirement for effective procedures compels it to adopt a credulous, submissive stance toward worldly power. \n Power confronts the individual as the universal, as the reason which informs reality.\nKnowledge, which is power, knows no limits, either in its enslavement of creation or in its deference to worldly masters.\nHorkheimer and Adorno offer a plea to recover the virtues of the \"metaphysical apologia\", which is able to reveal the injustice of effective procedures rather than merely employing them.\nThe metaphysical apologia at least betrayed the injustice of the established order through the incongruence of concept and reality. The impartiality of scientific language deprived what was powerless of the strength to make itself heard and merely provided the existing order with a neutral sign for itself. Such neutrality is more metaphysical than metaphysics.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66417", "revid": "47085960", "url": "https://en.wikipedia.org/wiki?curid=66417", "title": "Ichiro Suzuki", "text": "Japanese baseball player (born 1973)\n, also known mononymously as , is a Japanese former professional baseball outfielder who played for 28 seasons. He played the first nine years of his career with the Orix BlueWave of Nippon Professional Baseball (NPB), and the next 12 years with the Seattle Mariners of Major League Baseball (MLB). Suzuki then played two and a half seasons with the New York Yankees and three with the Miami Marlins before returning to the Mariners for his final two seasons. He won two World Baseball Classic titles as part of the Japanese national team. One of the greatest contact hitters, leadoff hitters and defensive outfielders in baseball history, he is also considered as one of the greatest baseball players of all time.\nIn his combined playing time in the NPB and MLB, Suzuki received 17 consecutive selections as an All-Star and Gold Glove winner, won nine league batting titles, and was named his league's most valuable player (MVP) four times. In the NPB, he won seven consecutive batting titles and three consecutive Pacific League MVP Awards. In 2001, Suzuki became the first Japanese-born position player to be posted and signed to an MLB club. He led the American League (AL) in batting average and stolen bases en route to being named AL Rookie of the Year and AL MVP.\nSuzuki was the first MLB player to enter the Meikyukai (The Golden Players Club). He was a ten-time MLB All-Star and won the 2007 All-Star Game MVP Award for a three-hit performance that included the event's first-ever inside-the-park home run. Suzuki won a Rawlings Gold Glove Award in each of his first 10 years in the majors and had an American League\u2013record seven hitting streaks of 20 or more games, with a high of 27. He was also noted for the longevity of his career, continuing to produce at a high level with slugging and on-base percentages above .300 in 2016, while approaching 43 years of age. Suzuki also set a number of batting records, including MLB's single-season record for hits with 262. He achieved 10 consecutive 200-hit seasons, the longest streak by any player in history. In 2016, Suzuki notched the 3,000th hit of his MLB career, becoming only the 30th player ever to do so. In total, he finished with 4,367 hits in his professional career across Japan and the United States, the most of any player in history at the top level of baseball. Since retiring as a player in 2019, he became the Mariners' special assistant to the chairman. \nIn 2025, Suzuki was elected to the National Baseball Hall of Fame in his first year of eligibility. He became the first Japanese player to be elected into the Hall of Fame, receiving 99.7% of the vote, tied with Derek Jeter for the second-highest total ever. That same year, Suzuki was also elected to the Japanese Baseball Hall of Fame. On August 9, 2025, the Seattle Mariners retired Suzuki's number 51.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nEarly life.\nSuzuki was born in Nishikasugai-gun, Aichi, and grew up in Toyoyama, a small town just outside Nagoya. At the age of seven, Suzuki joined his first baseball team and asked his father, Nobuyuki Suzuki (\u9234\u6728\u5ba3\u4e4b), to teach him to be a better player. The two began a daily routine, which included throwing 50 pitches, fielding 50 infield balls and 50 outfield balls, and hitting 500 pitches, 250 from a pitching machine and 250 from his father.\nAs a little leaguer in Toyoyama, Suzuki had the word written on his glove. By age 12, he had dedicated himself to pursuing a career in professional baseball, and their training sessions were no longer for leisure, and less enjoyable. The elder Suzuki claimed, \"Baseball was fun for both of us,\" but Ichiro later said, \"It might have been fun for him, but for me it was a lot like \"Star of the Giants,\"\" a popular Japanese manga and anime series about a young baseball prospect's difficult road to success, with rigorous training demanded by the father. According to Ichiro, \"It bordered on hazing and I suffered a lot.\"\nWhen Suzuki joined his high-school baseball team, his father told the coach, \"No matter how good Ichiro is, don't ever praise him. We have to make him spiritually strong.\" When he was ready to enter high school, Suzuki was selected by a school with a prestigious baseball program, Nagoya's \"Aikodai Meiden\" () High School. Suzuki was primarily used as a pitcher instead of as an outfielder, owing to his exceptionally strong arm. His cumulative high-school batting average was .505, with 19 home runs. He had known Hideki Matsui (then at Seiryo High School, Ishikawa - , one grade below him) through practice matches since that time.\nHe built strength and stamina by hurling car tires and hitting Wiffle balls with a heavy shovel, among other regimens. These exercises helped develop his wrists and hips, adding power and endurance to his thin frame. Despite his outstanding numbers in high school, Suzuki was not drafted until the fourth round of the NPB draft in November 1991, because many teams were discouraged by his small size of and . Years later, Suzuki told an interviewer, \"I'm not a big guy, and hopefully kids could look at me and see that I'm not muscular and not physically imposing, that I'm just a regular guy. So if somebody with a regular body can get into the record books, kids can look at that. That would make me happy.\"\nProfessional career.\nOrix BlueWave (1992\u20132000).\nSuzuki made his NPB Pacific League debut in 1992 for the Orix BlueWave at the age of 18, but he spent most of his first two seasons in the farm system (accumulating 156 minor league hits and a .368 batting average) because his then-manager, Sh\u014dz\u014d Doi, refused to accept Suzuki's unorthodox swing. The swing was nicknamed because of the pendulum-like motion of his leg, which shifts his weight forward as he swings the bat, and goes against conventional hitting theory. In his second career game, he recorded his first \"ichi-gun\" (Japan's Nippon Professional Baseball League) hit in the Pacific League against Fukuoka Daiei Hawks pitcher Keiji Kimura. Despite hitting a home run in 1993 against Hideo Nomo, who later won the National League Rookie of the Year Award, Suzuki was nevertheless sent back to the farm system on that very day. In 1994, he benefited from the arrival of a new manager, Akira \u014cgi, who played him every day in the second spot of the lineup. He was eventually moved to the leadoff spot, where his immediate productivity dissolved any misgivings about his unconventional swing. He set a Japanese single-season record with 210 hits, the first player ever to top 200 hits in a single season. Five other players have since done so: Matt Murton, Norichika Aoki (twice), Alex Ram\u00edrez, Tsuyoshi Nishioka, and Shogo Akiyama.\nSuzuki's .385 batting average in 1994 was a Pacific League record and won the young outfielder the first of a record seven consecutive batting titles. Suzuki also hit 13 home runs and had 29 stolen bases, helping him to earn his first of three straight Pacific League MVP (Most Valuable Player) awards. It was during the 1994 season that he began to use his given name, \"Ichiro,\" instead of his family name, \"Suzuki,\" on the back of his uniform. Suzuki is the second-most-common family name in Japan, and his manager introduced the idea as a publicity move to help create a new image for what had been a relatively weak team, as well as a way to distinguish their rising star. Initially, Suzuki disliked the practice and was embarrassed by it; however, \"Ichiro\" was a household name by the end of the season, and he was flooded with endorsement offers.\nIn 1995, Suzuki led the Blue Wave to its first Pacific League pennant in 12 years. In addition to his second batting title, he led the league with 80 RBI and 49 stolen bases, while his career-high 25 home runs were third in the league. By this time, the Japanese press had begun calling him the . The following year, with Suzuki winning his third-straight MVP award, the team defeated the Central League champion, Yomiuri Giants, in the Japan Series. Following the 1996 season, playing in an exhibition series against a visiting team of Major League All-Stars kindled Suzuki's desire to travel to the United States to play in the Major Leagues.\nIn November 1998, Suzuki participated in a seven-game exhibition series between Japanese and American all-stars. Suzuki batted .380 and collected seven stolen bases in the series, winning praise from several of his MLB counterparts, including Sammy Sosa and Jamie Moyer, who would become his teammate with the Mariners.\nIn 2000, Suzuki was still a year away from being eligible for free agency, but the Blue Wave was no longer among Japan's best teams. Because the team would probably not be able to afford to keep him and would lose him without compensation in another year, Orix allowed him to negotiate with Major League clubs. Suzuki used the posting system, and the Seattle Mariners won the right to negotiate with him with a bid of approximately $13\u00a0million. In November, Suzuki signed a three-year, $14\u00a0million contract with the Seattle Mariners. In his nine NPB seasons in Japan, Suzuki had 1,278 hits, a .353 career batting average, and won seven Golden Glove Awards. Suzuki's time in the Japanese baseball leagues matured him as a player and a person, and he often credits it for his success.\nSeattle Mariners (2001\u20132012).\n2001: Rookie of the Year and AL MVP.\nDue to an agreement between Japanese baseball and the MLB, Suzuki was not allowed to play in the United States before 2001. His move to the United States was viewed with some interest because he was among the first Japanese position players to play for an MLB team. In the same way that many Japanese teams had considered the 18-year-old Suzuki too small to draft in 1992, many Americans believed he would prove too frail to succeed against Major League pitching or endure the longer 162-game season. Suzuki made an auspicious debut with Seattle, and in the Mariners' eighth game revealed his tremendous throwing arm by gunning down Oakland's Terrence Long, who had tried to advance from first to third on a teammate's single to right field. That play would be dubbed \"The Throw\" by Japanese media covering Suzuki's progress.\nAfter expressing no preference as to a uniform number, Suzuki was issued #51 by the Mariners, which was his number when he played in Japan. He was initially hesitant because it had previously been worn by pitching star Randy Johnson. To avoid insulting Johnson, Suzuki sent a personal message to the pitcher promising not to \"bring shame\" to the uniform. His trepidation was unfounded, as he had a spectacular 2001 season, accumulating a rookie-record 242 hits, breaking Lloyd Waner\u2019s rookie record of 223 hits dating back to 1927, and the most hits by any MLB player since 1930. His perennial Gold Glove fielding led Safeco's right field to be dubbed \"Area 51\". With a .350 batting average and 56 stolen bases, Suzuki was the first player to lead his league in both categories since Jackie Robinson in 1949. The season included hitting streaks of 25 and 23 games, an appearance on the cover of \"Sports Illustrated\", and intense media attention on both sides of the Pacific. Fans from Japan were taking $2,000 baseball tours, sometimes flying in and out of the U.S. just to watch Suzuki's games. More than 150 Japanese reporters and photographers were given media access. Safeco Field's sushi stands began selling \"Ichirolls\", a spicy tuna roll served with wasabi and ginger.\nAided by Major League Baseball's decision to allow All-Star voting in Japan, Suzuki was the first rookie to lead all players in voting for the All-Star Game. That winter, he won the American League Most Valuable Player and the Rookie of the Year awards, becoming only the second player in MLB history (after Fred Lynn) to receive both honors in the same season. Suzuki is also the only player in MLB history to win an MVP, Rookie of the Year, Gold Glove Award, and Silver Slugger Award and start in the All-Star Game in the same season.\n2001 had been an exceptionally successful regular season for the Mariners, as they matched the 1906 Chicago Cubs' Major League record of 116 wins. In his only postseason appearance with the Mariners, Suzuki continued his hot hitting, batting .600 in the ALDS against the Cleveland Indians. However, on Suzuki's 28th birthday, Seattle's stellar season ended against the New York Yankees in the ALCS, as Suzuki was held to a .222 average during the series. Yankees manager Joe Torre had emphasized to his pitchers, \"Do not let Ichiro beat you. He is the key to Seattle's offense.\" Informed of this assessment, Suzuki said, \"If that is true, it would give me great joy. I don't believe he is right.\"\n2002.\nSuzuki finished his second year in American baseball with 208 total hits, making him the first Mariners player ever with two consecutive seasons of at least 200 hits. He was the fifth player in MLB history to start a career with two 200-hit seasons. He got off to a hot start, but a late-season slump drove his batting average down to .321, 29 points below his batting average as a rookie. Suzuki finished the season second in the AL in hits, fourth in batting average, and fourth in steals. Suzuki led All-Star balloting for the second straight year. Although the Mariners had a 93\u201369 record, that was good for only a third-place finish in the competitive AL West.\n2003.\nIn 2003, Suzuki became just the third player in history to begin his career with three 200-hit seasons, by garnering 212. He again finished in the top 10 in the AL in hits, batting average, steals, and runs. Again, a late-season slump brought his average down by 42 points from mid-July to .312. Suzuki was elected to his third All-Star game, and he was again the vote leader in both leagues. However, the second-place Mariners again fell short of the playoffs. Following the season, Suzuki signed a 4-year, $44 million contract that kept him with the Mariners through 2007.\n2004: Single season hit record.\n Suzuki had his best offensive season in 2004, highlighted by his breaking of George Sisler's 84-year-old record for most hits (257) in a season. An increase in games played benefited Suzuki, as he accumulated only 251 hits through the first 154 games of the season. Suzuki recorded 50 hits in four different months of the year (September and October are combined by MLB for this computational purpose), becoming the first player ever to have four in a season. With 51 hits in August 2001, Suzuki joined Pete Rose as the only players with four 50-hit months in a career.\nOn May 21, Suzuki recorded his 2,000th professional hit. His 200th hit of 2004 came in just his 126th game. By the end of September, with one three-game series remaining, Suzuki's hit total stood at 256\u2014one shy of Sisler. Suzuki singled off the Rangers' Ryan Drese on October 1 to tie Sisler's record. In the third inning, on a 3\u20132 count, Suzuki singled up the middle for his 258th hit of the year, which Suzuki later called \"the greatest moment of my baseball career.\" He was greeted by a swarm of teammates, and a standing ovation from the fans. Sisler's daughter Frances Sisler Drochelman attended the game and was greeted by Suzuki after his hit. Suzuki finished the 2004 season with a record of 262 hits, giving him the single-season records for both the United States and Japanese baseball.\nIn July 2009, while in St. Louis for his ninth All-Star appearance, Suzuki made a trip to Sisler's grave. He later told reporters, \"There's not many chances to come to St. Louis. In 2004, it was the first time I crossed paths with him, and his family generously came all the way to Seattle. Above all, it was a chance. I wanted to do that for a grand upperclassman of the baseball world. I think it's only natural for someone to want to do that, to express my feelings in that way. I'm not sure if he's happy about it.\"\nFrom 2001 to 2004, Suzuki had more hits (924) than anyone in history over any four-year period, breaking the record of 918 that Bill Terry accumulated from 1929 to 1932; Terry, however, played in 34 fewer games than Suzuki during their respective four-year spans. He would later surpass his own mark by recording 930 hits from 2004 to 2007. During one 56-game stretch in 2004, Suzuki batted over .450. By comparison, Joe DiMaggio batted .408 during his record 56-game hitting streak. Suzuki batted over .400 against left-handed pitching in 2004.\n2005.\nDuring the off-season, then-manager Bob Melvin's contract was not extended and the Mariners brought in Mike Hargrove as the new manager with a contract through 2007. It was Hargrove who had predicted that Suzuki would be no better than \"a fourth outfielder on [an American] major league team\" back when Suzuki was still in Japan. Speculation started that Hargrove and Suzuki did not get along very well in the season.\nIn 2005, Suzuki had his second worst year in his MLB career to date, collecting only 206 hits, the lowest total of his career to that point. However, he reached the plateau of a .300 batting average, 100+ runs, 30+ steals, and 200+ hits for the fifth straight season. That allowed Suzuki to become the first player to collect 200 hits per season in each of his first five years in the major leagues. Only Willie Keeler, Wade Boggs, Chuck Klein, Al Simmons, and Charlie Gehringer had five consecutive 200-hit seasons at any point in their careers. During the season, he accumulated 1,000 career hits, reaching the career milestone faster than any player in MLB history. Suzuki hit a career-high 15 home runs. In the off-season, Suzuki played himself in \"Furuhata Ninzabur\u014d\", a Japanese \"Columbo\"-like TV drama that he loves. In the drama, he kills a person and is arrested.\n2006.\nSuzuki's 2006 season got off to a disappointing start, with the outfielder hitting as low as .177 in the season's third week. He quickly rebounded, finishing the season with a .322 average (sixth in the AL and 11th in the majors). Suzuki's 224 hits led the majors, and he recorded 110 runs and 45 stolen bases. Suzuki was caught stealing only twice in 2006 for a 96% success rate. His 1,354 career U.S. hits topped Wade Boggs's record for the most hits in any six-year period. In his sixth year in the majors, Suzuki collected his sixth Gold Glove Award, and a sixth All-Star Game selection. He also won a Fielding Bible Award as the best fielding MLB right fielder.\nSuzuki began wearing high stocking baseball pants in the 2006 World Baseball Classic.\n2007.\nIn May and June, Suzuki hit in 25 consecutive games, breaking the previous Mariners record set by Joey Cora in 1997. Suzuki broke Tim Raines' American League record by stealing 41 consecutive bases without being caught. Suzuki extended the record to 45; the major league record of 50 belongs to Vince Coleman.\nOn July 10, 2007, he became the first player to hit an inside-the-park home run in any MLB All-Star Game after an unpredictable hop off the right field wall of AT&amp;T Park in San Francisco. It was the first inside-the-park home run of Suzuki's professional career. Suzuki was a perfect 3-for-3 in the game and was named the Most Valuable Player in the American League's 5\u20134 victory.\n2007 marked the end of Suzuki's second contract with the Mariners, and he initially told MLB.com that he would likely enter the free agent market, citing the team's lack of success in recent years. However, Suzuki signed a five-year contract extension with Seattle in July. The deal was reported to be worth $90\u00a0million, consisting of a $17\u00a0million annual salary and $5\u00a0million signing bonus. The Associated Press reported that Suzuki's contract extension defers $25\u00a0million of the $90\u00a0million at 5.5% interest until after his retirement, with payments through 2032. Other provisions in Suzuki's contract included a yearly housing allowance of more than $30,000, and four first-class round-trip tickets to Japan each year for his family. He was provided with either a new Jeep or Mercedes SUV, as well as a personal trainer and interpreter.\nOn July 29, 2007, Suzuki collected his 1,500th U.S. hit, the third fastest to reach the MLB milestone behind Al Simmons and George Sisler.\n2008.\nSuzuki had 213 hits in 2008, his eighth straight 200-hit season. This tied the 107-year-old record set by Wee Willie Keeler. Typically, Suzuki was among baseball's leaders in reaching base on an error (14 times in 2008, more than any other batter in the AL), and in infield hits (his 56 were the most in the majors). Suzuki amassed an estimated 694 infield hits in his U.S. career. Detroit third baseman Brandon Inge told \"The New York Times\", \"I wish you could put a camera at third base to see how he hits the ball and see the way it deceives you. You can call some guys' infield hits cheap, but not his. He has amazing technique.\" In May 2008, Suzuki stole two bases, giving him a career total of 292, surpassing the previous Seattle Mariners team record of 290 set by second baseman Julio Cruz. Cruz, who worked on Spanish-language broadcasts of Mariners games at the time, was watching from the broadcast booth as Suzuki broke his record.\nOn July 29, 2008, Suzuki became the second-youngest player to amass 3,000 top-level professional hits (1,278 in Japan and 1,722 in the U.S.) after Ty Cobb. He also became just the second Japanese professional to get 3,000 hits. (Nippon Professional Baseball's record holder is Isao Harimoto, with 3,085 hits).\nBy 2008, it had emerged in the media that Suzuki was known within baseball for his tradition of exhorting the American League team with a profanity-laced pregame speech in the clubhouse prior to the MLB All-Star Game. Asked if the speech had had any effect on the AL's decade-long winning streak, Suzuki deadpanned, \"I've got to say over 90 percent.\" Minnesota first baseman Justin Morneau describes the effect: \"If you've never seen it, it's definitely something pretty funny. It's hard to explain, the effect it has on everyone. It's such a tense environment. Everyone's a little nervous for the game, and then he comes out. He doesn't say a whole lot the whole time he's in there, and all of a sudden, the manager gets done with his speech, and he pops off.\" Boston's slugger David Ortiz says simply, \"It's why we win.\"\n2009.\nSuzuki began his 2009 season by going on the disabled list for the first time in his career. He had a bleeding ulcer, which team doctor Mitch Storey said may have been caused in part by the stress of playing in the World Baseball Classic. After missing 8 games, Suzuki debuted on April 15 and went 2-for-5 against the Angels, including a grand slam for his 3,085th overall professional career hit. The home run matched Isao Harimoto's Japanese record for career hits, and Harimoto had been flown out to Seattle to witness the event. Suzuki surpassed the record the following night.\nSuzuki was named #30 on the \"Sporting News\"' 2009 list of the 50 greatest current players in baseball, voted upon by a 100-person panel of experts and former stars. In May and June, Suzuki surpassed his own franchise record with a 27-game hitting streak. Suzuki went on to record 44 hits in June 2009, his 20th career month with 40 or more hits. The previous players to have accomplished this were Stan Musial in the NL and Lou Gehrig in the AL.\nOn September 6 against the Oakland A's, Suzuki collected his 2,000th MLB hit on the second pitch of the game, a double along the first base foul line. He is the second-fastest player to reach the milestone, behind Al Simmons. On September 13 against the Texas Rangers, Suzuki collected his 200th hit of the season for the ninth consecutive year, setting an all-time major league record. Suzuki recorded 210 hits with Orix in 1994, thereby giving him a total of ten 200 hit seasons in his professional career.\nWith two outs in the bottom of the 9th inning on September 18, Suzuki hit a walk-off, two-run home run against Yankees closer Mariano Rivera, scoring Michael Saunders in one of the more memorable victories of the season. His homer made a winner out of F\u00e9lix Hern\u00e1ndez, who was in line for the loss despite having allowed only one run in 9 innings pitched.\nOn September 26, 2009, Suzuki was ejected from a game for the first time in his professional career. Arguing that a strikeout pitch from Toronto's David Purcey had been outside, Suzuki used his bat to draw a line on the outer edge of the plate, and was immediately tossed by umpire Brian Runge. He was the only Mariner to be ejected from a game all season. The ejection may have hurt Suzuki's chances regarding an esoteric record: the longest playing streak without going hitless in consecutive games. Suzuki's stretch was at 180 games, the longest in the majors since Doc Cramer went 191 consecutive games without back-to-back 0-fers in 1934\u201335. Suzuki went hitless in the following afternoon's game.\nSuzuki again led the majors in hits in 2009, with 225. In spite of hitting ground balls at a rate of 55 percent, he grounded into only one double play all season; in the April 15 game, his first game played in 2009. He won his second Fielding Bible Award as the best fielding right fielder in MLB. In total, Suzuki had 2,030 hits in the 2000s, the most for all MLB players in that era.\n2010.\nSuzuki's 37 career leadoff home runs rank 13th all time, as of 2024[ [update]]. Nevertheless, in 2009, Suzuki told \"The New York Times\":\nChicks who dig home runs aren't the ones who appeal to me. I think there's sexiness in infield hits because they require technique. I'd rather impress the chicks with my technique than with my brute strength. Then, every now and then, just to show I can do that, too, I might flirt a little by hitting one out.\nAfter playing in the season opener against the Oakland Athletics, Suzuki became eligible for Hall of Fame consideration, by playing in his tenth MLB season. On June 5, 2010, Suzuki scored his 1,000th career MLB run against the Angels on Franklin Gutierrez's RBI groundout. On September 1, 2010, Suzuki also collected his 2,200th hit, a leadoff infield single against Cleveland Indians pitcher Josh Tomlin.\nDuring the August 2010 series against the New York Yankees, Suzuki traveled to the Calvary Cemetery in Queens, New York, to pay his respects at the grave of Hall-of-Famer \"Wee Willie\" Keeler, whose record for consecutive 200-hit seasons he had broken in 2009.\nOn September 23, Suzuki hit a single to center field against Toronto Blue Jays pitcher Shawn Hill to become the first MLB player in history to reach the 200 hit mark for 10 consecutive seasons. This feat also tied him with Pete Rose for the most career seasons of 200+ hits, and he surpassed Ty Cobb for most career seasons of 200+ hits in the AL. He finished the season with 214 hits, topping MLB in that category. Suzuki also finished the season \"ironman\" style, playing in all 162 games. Only Suzuki and Matt Kemp did so for the 2010 season. This was Suzuki's 3rd season playing in all 162 games. Also, Suzuki was nominated for the This Year in Baseball Award. Suzuki finished first or second in hits in all of his first 10 MLB seasons.\nSuzuki won his tenth consecutive Rawlings Gold Glove Award in 2010, tying Ken Griffey Jr., Andruw Jones, and Al Kaline, and trailing only Roberto Clemente and Willie Mays (twelve each) for major league outfielders. Suzuki also won his second consecutive and third overall Fielding Bible Award for his statistically based defensive excellence in right field. Suzuki was the first right fielder in MLB history to win multiple Bible awards.\n2011.\nOn April 2, 2011, Suzuki broke the Mariners' all-time career hits record with his 2,248th hit in the 9th inning versus the Oakland Athletics, overtaking the team's previous leader Edgar Mart\u00ednez. 2011 marked the first time in Suzuki's 11 seasons that he failed to make the All-Star team. He batted under .300 (.277) before the All-Star break for the first time in his career. On July 10, manager Eric Wedge did not start Suzuki, ending his then-major league-best active streak of 255 consecutive starts. Suzuki followed with an 11-game hitting streak, but Wedge noted \"it's not that easy to give that guy a day off\" due to Suzuki's iconic stature. On August 22, Suzuki hit his 35th career leadoff homer, tying him for 6th place with Bobby Bonds. Suzuki finished the season batting a career-low .272 with 184\u00a0hits, the first time in his 11-year MLB career he did not record 200\u00a0hits. It was also his first season not playing in the All-Star game, as well as his first season not winning a Gold Glove.\n2012.\nOn June 19, 2012, Suzuki led off a game against the Arizona Diamondbacks with a single to center field, the 2,500th hit of his MLB career. Suzuki reached the milestone in the fourth-fewest games in major league history, after Al Simmons, Ty Cobb, and George Sisler. In a 13-inning road loss to the Oakland A's on July 8, Suzuki was placed second in the batting order and responded by going 2 for 6 to bring his season batting average to .261 heading into the All-Star break. In the previous night's game, Suzuki recorded two hits to break a career-worst 0-for-23 hitless streak. Suzuki had also been tried at the three-spot in the batting order during a season for which he earned $18 million. Former teammate Jay Buhner stated he felt Suzuki was the recipient of too much blame for the Mariners' difficulties but \"at the same time, they need help desperately.\" Buhner stated that if Suzuki were awarded a three-year contract extension for somewhere between $35 million and $40 million, \"I'd vomit. I mean, really, no offense. No offense, we've got to get this organization turned around. You can't be spending all the money on one guy.\"\nWith a contract extension with the Mariners unknown, Suzuki stated, \"It's going to go both ways. It can't just come from the player. It's got to come from the team, too. If the team is saying they need you, you're necessary, then it becomes a piece. But if it's just coming from the player, it's not going to happen.\" Suzuki's agent, Tony Attanasio, said, \"He knows that the club has to grow. He knows they have to play the younger guys and get them more playing time. The only way he knows to do that is to move on. He doesn't want to stop playing. He wants to continue.\"\nNew York Yankees (2012\u20132014).\nRest of 2012: second postseason appearance.\nSuzuki approached the Mariners to ask for a trade at midseason in 2012. His first choice was to play for the New York Yankees. The Mariners traded him to the Yankees for minor league pitchers D. J. Mitchell and Danny Farquhar on July 23. Seattle also received cash in the trade. Suzuki left Seattle hitting .261 with a .288 on-base percentage (OBP), four home runs, 28 RBIs and 15 stolen bases in 95 games. His first game as a Yankee was played the night of the trade, at Safeco Field against the Mariners. Before the trade was consummated, Suzuki agreed to the Yankees' conditions, which stated that they would play Suzuki primarily in left field, bat him at the bottom of the lineup, and occasionally sit him against left-handed pitching. Suzuki hit safely in his first 12 games as a Yankee, tying a record set by Don Slaught. He wore number 31 during his tenure with the Yankees, as his traditional 51 had not been used since the 2006 retirement of Bernie Williams, who wore it while playing for the Yankees.\nSuzuki hit his first home run as a Yankee, and the 100th of his career, on July 30 against the Baltimore Orioles. For the week ending September 23, Suzuki was named AL Player of the Week after hitting .600 (15-for-25) with three doubles, two home runs, five RBI, seven runs scored, and six stolen bases in six games. He led all MLB players in batting average, hits, steals and OBP (.630). In 67 games with New York, Suzuki batted .322 with a .340 OBP, 28 runs, five home runs and 27 RBIs. With his improved performance, the Yankees at times batted him second and also started him against left-handers.\nAgainst the Baltimore Orioles in the 2012 ALDS, Suzuki ran home on a ball hit by Robinson Can\u00f3. Despite the ball beating him to the plate, Orioles catcher Matt Wieters had difficulty tagging Suzuki, who evaded multiple tag attempts by jumping over and around Wieters. In Game 1 of the 2012 ALCS, Suzuki hit his first career postseason home run; however, the Yankees lost the series to the Detroit Tigers in 4 games.\nOn December 19, 2012, Suzuki finalized a $13 million deal for two years with the Yankees.\n2013.\nOn June 25, 2013, Suzuki hit a walk-off home run against Rangers pitcher Tanner Scheppers with two outs in the bottom of the ninth inning. Earlier in the game, three of his teammates had led off the fourth, fifth, and sixth innings with home runs, so all of the Yankees' runs in the game were provided by solo home runs.\nOn August 21, 2013, Suzuki collected his 4,000th professional career hit with a single off Toronto Blue Jays pitcher R. A. Dickey, becoming the seventh player in professional baseball history known to have reached the mark after Pete Rose, Ty Cobb, Julio Franco, Hank Aaron, Jigger Statz, and Stan Musial.\n2014.\nOn July 10, 2014, Suzuki collected his 2,800th MLB hit off of Cleveland Indians pitcher Scott Atchison in the top of the eighth inning at Progressive Field. On August 9, 2014, Suzuki hit a single in a game against the Astros to pass George Sisler on the all-time hit list with his 2,811th hit. Suzuki had previously broken Sisler's single season hit record in the 2004 season.\nMiami Marlins (2015\u20132017).\n2015.\nOn January 23, 2015, Suzuki agreed to a one-year, $2 million contract with the Miami Marlins. The Marlins originally planned for him to be their fourth outfielder, but he finished the season with 439 plate appearances due to team injuries\u2014primarily Giancarlo Stanton, who suffered a season-ending injury on June 26. On April 25, Suzuki scored his 1,310th major league run, which, combined with the 658 runs he scored in Japan, surpassed the record for runs scored by a Japanese player set by Sadaharu Oh. On June 18, he was batting .294 after playing in 64 of the Marlins' 68 games, but his average fell to .229 by season's end. On August 14 at Busch Stadium, Suzuki singled off St. Louis Cardinals starter John Lackey to earn his 4,192nd top-level hit, passing Ty Cobb. On July 29, Suzuki recorded his 2,900th major league hit, against Washington Nationals pitcher Doug Fister. On August 18, Suzuki had his first four-hit game since 2013. On August 31, against the Atlanta Braves, Suzuki scored his 2,000th professional run, combining his runs scored in MLB (1,342) and in Japan's NPB (658). On September 5, Suzuki recorded his 100th right field assist in the major leagues. In the season finale against the Philadelphia Phillies on October 4, he made his major league pitching debut, completing the final inning and allowing one run and two hits in a 7\u20132 loss. For the 2015 season, he batted .229/.282/.279 with 11 stolen bases.\n2016.\nOn October 6, 2015, Suzuki and the Marlins agreed on a one-year, $2 million contract for the 2016 season. The deal also came with a $2 million club option for 2017. He stole his 500th career MLB base on April 29, 2016, against the Milwaukee Brewers, and led off the game with a single against Zach Davies to move ahead of Frank Robinson into 33rd place on the all-time MLB hit list with the 2,944th hit of his career.\nOn June 15, Suzuki recorded his 4,257th career hit, breaking Pete Rose's all-time record for hits in top tier professional baseball. Rose commented that \"I'm not trying to take anything away from Ichiro, he's had a Hall of Fame career, but the next thing you know you'll be counting his high school hits.\" This was in response to the Japanese media labeling Suzuki as the \"Hit King\", claiming that Suzuki should be considered to be the all-time hits leader when his hits in Japan are included. American media was more divided on the significance of the accomplishment, though some major sources acknowledged the milestone as indicating Suzuki had become the greatest hitter in baseball.\nOn August 7, Suzuki collected the 3,000th hit of his MLB career when he hit a triple off the right field wall at Coors Field playing against the Colorado Rockies. He is just the second player to reach that milestone by way of a triple, joining Hall of Famer Paul Molitor. He also became one of only seven players to have collected 3,000 hits and 500 stolen bases. At the end of his 16th season, Suzuki had played in exactly 2,500 major league games. Suzuki and Pete Rose are the only two players in MLB history to have accomplished playing in 2,500 games in their first 16 seasons.\nAfter the 2016 season, the Marlins exercised their option on Suzuki's contract for the 2017 season, and added an option for the 2018 season.\n2017.\nOn April 19, Suzuki hit his first home run against his former team the Seattle Mariners, a 9th-inning drive off Evan Marshall. Suzuki scored his 1400th run in a May 23 game against the Oakland A's.\nOn June 14, Suzuki singled for his 365th interleague hit, passing Derek Jeter (364) to become the all-time leader in interleague hits. Suzuki finished 2017 with 368 interleague hits. This total would be surpassed by Miguel Cabrera on September 7, 2021.\nOn June 25, Suzuki (age 43 and 246 days) became the oldest player to start a game in center field since at least 1900, breaking the record previously held by Rickey Henderson. On June 29, Suzuki became the oldest active MLB player when Bartolo Col\u00f3n was designated for assignment by the Atlanta Braves, although Colon latched on with the Minnesota Twins a few weeks later. On July 6, Suzuki hit two singles against the St. Louis Cardinals, bringing his hits total to 3,054 and surpassing Panamanian-born Rod Carew as the all-time leader in MLB hits among foreign-born players. Dominican-born Adrian Beltre surpassed Suzuki as the foreign hits leader on June 13, 2018.\nOn August 26, Suzuki set the Marlins' single-season franchise record for pinch-hits with his 22nd pinch hit. On September 3, he set a major league record for most pinch-hit at-bats in a season, with 84, and four days later he set a major league record for most pinch-hit plate appearances in a season, with 95. On September 8, Suzuki became the sixth player all-time to hit 2,500 career singles, as well as the 8th right fielder of all-time to record over 4,000 putouts at the position. On October 1, Suzuki flied out in his last chance to tie John Vander Wal's MLB record of 28 pinch hits in a season, finishing with 27.\nFor the season, he batted .255/.318/.332 with one stolen base (the first season in which he did not steal at least 10 bases). After the season, the Marlins declined a $2 million club option for the 2018 season, instead paying Suzuki a $500,000 buyout.\nReturn to the Mariners (2018\u20132019).\n2018.\nOn March 7, 2018, Suzuki signed a one-year contract to return to the Mariners after several team outfielders were injured during spring training. On Opening Day, March 29, against the Cleveland Indians, Suzuki became the 20th outfielder all-time to record 5,000 career putouts at the position. At 44 years old, he entered the 2018 season as the second-oldest active player in baseball, behind only Bartolo Col\u00f3n.\nOn May 3, the Mariners announced that Suzuki would move to the front office as a special assistant to the chairman for the remainder of the season, but Suzuki did not rule out a possible return as a player for the 2019 season. In his final game for the year on the previous day, he went 0-for-3 with a walk, a strikeout, and a run in a 3\u20132 loss to the Oakland Athletics. This would end up being his last game played at Safeco Field. In 15 games played with the 2018 Mariners, Suzuki batted 9-for-44 (.205/.255/.205) without an extra base hit, stolen base, or RBI.\nOn May 11, he became the interim bench coach for two games as manager Scott Servais was gone to attend his daughter's college graduation and regular bench coach Manny Acta was filling in as manager.\n2019.\nOn October 2, 2018, it was announced that Suzuki would be on the Mariners' active roster when they opened the 2019 season against the Oakland Athletics in Japan. Suzuki was re-signed to a minor league deal on January 23, 2019.\nOn March 20, 2019, the Mariners opened the MLB season against the Athletics at the Tokyo Dome and Suzuki started the game in right field, becoming at 45 years old the second oldest position player (behind Julio Franco) to start for a team on its opening day. The next night, the Mariners again played the Athletics at the Tokyo Dome and Suzuki played in his final professional game. He went 0\u20134 at the plate and in the bottom of the eighth inning walked off the field to applause. Later in the day, Suzuki officially announced his retirement. He was the oldest active MLB player at the time.\n2019 was Suzuki's 19th season in MLB, and including the nine years he played in Japan's NPB, Suzuki's 28 seasons of playing in baseball's top-tiered leagues eclipsed the record of most seasons played by a position player held previously by 19th century MLB player Cap Anson. (MLB pitcher Nolan Ryan, like Anson, also played 27 seasons, while NPB pitcher Kimiyasu Kudo played 29 seasons.)\nOn April 30, 2019, Suzuki renewed his role with the Mariners from the previous year as special assistant to the chairman.\nInternational career.\n2006: Inaugural World Baseball Classic.\nSuzuki played for the Japan national baseball team in the inaugural World Baseball Classic in March 2006. During the March 15 Japan-Korea game, Suzuki was booed by some spectators during every at-bat, reportedly in response to a previous statement that he wanted to \"beat South Korea so badly that the South Koreans won't want to play Japan for another 30 years.\" That, however, was an incorrect translation mostly spread to the public through ESPN. Suzuki was variously quoted as saying \"I want to win in a way that the opponent would think, 'we cannot catch up with Japan for the coming 30 years'. We should not merely win the games.\" Japan would later beat Korea in the playoffs and win the tournament after defeating Cuba 10\u20136 in the finals. For the tournament, Suzuki had twelve hits including a home run, seven runs, and four stolen bases.\n2009: Second WBC championship.\nDespite struggling uncharacteristically during most of the tournament, Suzuki provided the game-winning hit in the Championship game against South Korea. With two outs in the top of the tenth inning, he broke a 3\u20133 tie with a two-run single off a ball in the dirt. This would prove to be the margin of victory in Japan's 5\u20133 defeat of South Korea. Suzuki ended the night 4-for-6, bringing his total to 6-for-10 in WBC championship games.\nPlaying style.\nSportswriter Bruce Jenkins, writing in the \"San Francisco Chronicle\", described Suzuki's distinctive style of play:\nThere's nobody like Ichiro in either league\u2014now or ever. He exists strictly within his own world, playing a game 100 percent unfamiliar to everyone else. The game has known plenty of 'slap' hitters, but none who sacrifice so much natural ability for the sake of the art. And he'll go deep occasionally in games, looking very much like someone who could do it again, often ... [but] the man lives for hits, little tiny ones, and the glory of standing atop the world in that category. Every spring, scouts or media types write him off, swearing that opposing pitchers have found the key, and they are embarrassingly wrong.\nWhile he is known for his hitting ability, he did not draw many walks. In 2004, when he set the single-season record for hits, his low walk total (49) led to him being on base a total of 315 times. It was the 58th-most times a player has reached base in a season and short of the major league record of 379 set by Babe Ruth in 1923.\n\"The New York Times\" criticized his inability to improve his power when his Mariner teams were often low-scoring while noting that he also did not steal bases as frequently as Rickey Henderson or Tim Raines. Suzuki, however, once commented, \"If I'm allowed to hit .220, I could probably hit 40 [home runs], but nobody wants that.\"\nSuzuki has long been interested in pitching professionally, and he actually took the mound to pitch to one batter in the 1996 NPB All Star game, reaching close to in warm up pitches. In 2009, it was reported that during an early February workout at the World Baseball Classic his fastball was clocked at 92\u00a0mph. On the final day of the 2015 season on October 4, Suzuki pitched in his first MLB game, throwing one complete inning at the end of a 7\u20132 Marlins loss against the Philadelphia Phillies, allowing one run on two hits. Less than three weeks before turning 42, he was still able to touch 88\u00a0mph with his fastball to go along with a mid 80s slider.\nSuzuki is the only left-handed hitter in Major League history with at least 2,000 plate appearances against left-handed pitching to display a reverse platoon split\u2014that is, he had better results hitting off left-handed pitchers than right-handed pitchers.\nSuzuki received recognition for playing superior defense in right field, with above-average range and a strong and accurate throwing arm. During his career, he won 10 Gold Glove Awards.\nPersonality and influence.\nSuzuki is noted for his work ethic in arriving early for his team's games, and for his calisthenic stretching exercises to stay limber even during the middle of the game. Continuing in Seattle the custom he began in Japan, he used his given name (written in r\u014dmaji) on the back of his uniform instead of his family name, becoming the first player in Major League Baseball to do so since Vida Blue.\nIn addition to being a ten-time Gold Glove winner, Suzuki was a ten-time All-Star selection from 2001 to 2010. His success has been credited with opening the door for other Japanese players like former Yomiuri Giants slugger Hideki Matsui, former Fukuoka SoftBank Hawks catcher Kenji Johjima, former teammate So Taguchi, and former Seibu Lions infielder Kazuo Matsui and active players Shohei Ohtani and Seiya Suzuki to enter the Major Leagues. Suzuki's career was followed closely in Japan, with national television news programs covering each of his at-bats, and with special tour packages arranged for Japanese fans to visit the United States to attend his games.\nSuzuki's agent, Tony Attanasio, described his client's status: \"When you mail Ichiro something from the States, you only have to use that name on the address and he gets it [in Japan]. He's that big.\" Suzuki's status in Japan fueled interest in Major League Baseball in Japan, including the $275 million broadcasting rights deal between MLB and Dentsu Inc. in 2003.\nSuzuki performs in TV commercials in Japan for ENEOS. His likeness is used as the basis of the character \"Kyoshiro\" in the anime and manga \"Major\".\nWhen he first came to the United States, he especially enjoyed trips to Kansas City to talk with former Negro leagues star Buck O'Neil. When O'Neil died in 2006, Suzuki sent a very large memorial wreath to the funeral service. The following year, he visited the Negro Leagues Baseball Museum while on a road trip to Kansas City and made what, as of 2016, remains the largest contribution ever made to the museum by an active MLB player.\nWhen Suzuki was traded to the Yankees in July 2012, longtime Mariners fan Ben Gibbard (of Death Cab for Cutie) posted his tribute song, \"Ichiro's Theme\", on his SoundCloud page. The previous year, The Baseball Project had released the tribute song \"Ichiro Goes To The Moon\" on their album \"\", which Gibbard contributed backing vocals to. Suzuki was also featured in his rookie season with Seattle in a hip-hop song recorded by Xola Malik and remixed by Sir Mix-A-Lot. KIRO Radio's Dave Ross also wrote a tribute song to Suzuki, drawing inspiration from on \"The Barber of Seville\".\nLegacy.\nIn 2018, researcher Jose Fernandez-Triana named a newly discovered species of wasp after Suzuki; \"Diolcogaster ichiroi\", collected at the Archbold Biological Station in Highlands County, Florida, was named after Ichiro \"to honor the truly unique and remarkable Ichiro Suzuki, [Fernandez-Triana's] favorite baseball player and one of the best ever to play the game\" and \"was still playing for a Florida team and thus naming a species endemic from Florida after him made complete sense.\"\nOn August 27, 2022, Suzuki was inducted into the Seattle Mariners Hall of Fame.\nOn January 16, 2025, Suzuki was elected to the Japanese Baseball Hall of Fame in his first year of eligibility. He appeared on 323 of the 349 valid ballots submitted, becoming the sixth overall highest vote percentage in the Hall's history at 92.6%.\nLater that month, Suzuki was elected to the National Baseball Hall of Fame in his first year on the ballot, receiving 393 of a possible 394 votes, tying Derek Jeter for the second highest vote share in history. He became the first Asian player to be inducted into the Hall of Fame. On the same day, the Mariners announced that a ceremony would be held in August of that year to formally retire Ichiro's uniform number 51 jersey. Later that year, on July 27, Suzuki was officially inducted into the Hall of Fame in a ceremony at Cooperstown, New York.\nEndorsements.\nOver the course of his career, Suzuki has endorsed numerous Japanese brands, although he was more reluctant to enter endorsement deals with American companies. According to \"Forbes\", at one point in his career, Suzuki earned roughly $7 million annually from endorsements, most of which came from Japanese companies.\nHe was the face of Kirin Brewery, a Japanese beer brand. He has endorsed Japanese brands such as sporting goods company Mizuno Corporation, Nikko Cordial, NTT Communications, Asics, JXTG Nippon Oil &amp; Energy. Suzuki also endorsed Yunker energy drink on behalf of Sato Pharmaceutical and Oakley sunglasses.\nIn 2001, Suzuki had deals with the Cutter &amp; Buck, Upper Deck trading cards, and sporting goods company Majestic Athletic.\nSuzuki's agent Tony Attanasio stated that Suzuki had rejected around $40 million in endorsements due to him being \"very selective when it comes to putting his name out in the public.\"\nPersonal life.\nHe has an elder brother, Kazuyasu Suzuki.\nSuzuki married , a former TBS TV announcer, on December 3, 1999, at a small church in Santa Monica, California. As of 2019[ [update]], they had a pet dog (Shiba Inu) named Ikkyu, a combination of the first character in each of his and his wife's first names. The couple resided in Issaquah, Washington, during the seasons while he played in Seattle and in Greenwich, Connecticut, while he played for the Yankees. They resided in Miami Beach during seasons with the Marlins.\nOn March 18, 2011, Suzuki donated \u00a5100 million ($1.25 million) to the Japanese Red Cross for earthquake and tsunami relief efforts.\nSuzuki's father, Nobuyuki, handled Ichiro's finances early in his career until, in 2002, due to Nobuyuki underreporting Ichiro's income, Ichiro was saddled with a significant bill for unpaid taxes. The scandal cost Ichiro an undisclosed amount of money and caused him embarrassment. This incident, along with Nobuyuki's relentless training and unforgiving attitude toward his son, caused their relationship to collapse. Subsequently, Ichiro's finances have been looked after by his wife Yumiko.\nSince November 2000, Nobuyuki has run the Ichiro exhibition room named \"I-fain\" in Toyoyama, Suzuki's hometown. It exhibits a wide variety of Ichiro Suzuki memorabilia, including personal items from his childhood and up-to-date baseball gear.\nSuzuki is the honorary chairman of the Ichiro Cup, a six-month-long boys' league tournament with more than 200 teams, held in Toyoyama and surrounding cities and towns since 1996. Suzuki watches the final game and attends its awards ceremony every year.\nSuzuki speaks English well and often spoke it with his teammates in his playing days, but uses an interpreter during interviews so that he is not misunderstood. He also learned Spanish early in his MLB career, using it to banter with other players. Suzuki further explains he did it because he felt a kinship to the Latin American players who, like him, were foreigners trying to succeed in the U.S.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66419", "revid": "50936967", "url": "https://en.wikipedia.org/wiki?curid=66419", "title": "Affricate", "text": "Consonant that begins as a stop and releases as a fricative\n&lt;templatestyles src=\"IPA common/styles.css\" /&gt;\nAn affricate is a consonant that begins as a stop and releases as a fricative, generally with the same place of articulation (most often coronal). It is often difficult to decide if a stop and fricative form a single phoneme or a consonant pair. English has two affricate phonemes, and , generally spelled \"ch\" and \"j\", respectively.\nExamples.\nThe English sounds spelled \"ch\" and \"j\" (broadly transcribed as and in the IPA), German and Italian \"z\" and Italian \"z\" are typical affricates, and sounds like these are fairly common in the world's languages, as are other affricates with similar sounds, such as those in Polish and Chinese. However, voiced affricates other than are relatively uncommon. For several places of articulation they are not attested at all.\nMuch less common are labiodental affricates, such as in German, Kinyarwanda and Izi, or velar affricates, such as in Tswana (written \"kg\") or in High Alemannic Swiss German dialects. Worldwide, relatively few languages have affricates in these positions even though the corresponding stop consonants, and , are common or virtually universal. Also less common are alveolar affricates where the fricative release is lateral, such as the sound found in Nahuatl and Navajo. Some other Athabaskan languages, such as Dene Suline, have unaspirated, aspirated, and ejective series of affricates whose release may be dental, alveolar, postalveolar, or lateral: , , , , , , , , , , , and .\nNotation.\nAffricates are transcribed in the International Phonetic Alphabet by a combination of two letters, one for the stop element and the other for the fricative element. In order to clarify that these are parts of a single consonant, a tie bar may be used. The tie bar appears most commonly above the two letters, but may be placed under them if it fits better there, or simply because it is more legible. Thus:\nor\nA less common notation indicates the release of the affricate with a superscript:\nThis is derived from the IPA convention of indicating other releases with a superscript. However, this convention is more typically used for a fricated release that is too brief to be considered a true affricate.\nThough they are no longer standard IPA, ligatures are available in Unicode for the sibilant affricates, which remain in common use:\nApproved for Unicode 18 in 2026, per request from the IPA, are the remaining coronal affricates:\n\u27e8\u27e9 for .\nLigatures \u27e8\u27e9 for the non-coronal affricates have also been used. Similar affricate ligatures can be found in Luciano Canepari's canIPA alphabet.\nAny of these notations can be used to distinguish an affricate from a sequence of a plosive plus a fricative, which is contrastive in languages such as Polish. However, in languages where there is no such distinction within a syllable, such as English or Turkish, a simple sequence of letters such as \u27e8\u27e9 is commonly used, with no overt indication that they form an affricate. In such cases the syllable boundary may be written to distinguish the plosive-fricative sequence in \"petshop\" from the similar affricate in \"ketchup\" .\nIn other phonetic transcription systems, such as the Americanist system, affricates may be transcribed with single letters. The affricate may be transcribed as \u27e8c\u27e9 or \u27e8\u023c\u27e9; as \u27e8\u0292\u27e9, \u27e8\u01b6\u27e9 or \u27e8j\u27e9; as \u27e8c\u27e9 or \u27e8\u010d\u27e9; as \u27e8\u01f0\u27e9, \u27e8\u01e7\u27e9 or \u27e8\u01ef\u27e9; as \u27e8\u019b\u27e9; and as \u27e8\u03bb\u27e9.\nSingle letters may also be used with phonemic transcription in IPA: and are sometimes transcribed with the symbols for the palatal stops, \u27e8\u27e9 and \u27e8\u27e9, for example in the IPA \"Handbook\".\nAffricates vs. stop\u2013fricative sequences.\nIn some languages, affricates contrast phonemically with stop\u2013fricative sequences:\nThe exact phonetic difference varies between languages. In stop\u2013fricative sequences, the stop has a release burst before the fricative starts; but in affricates, the fricative element \"is\" the release. Phonologically, stop\u2013fricative sequences may have a syllable boundary between the two segments, but not necessarily.\nIn English, and (\"nuts\", \"nods\") are considered phonemically stop\u2013fricative sequences. They often contain a morpheme boundary (for example, \"nuts\" = \"nut\" + \"s\"). The English affricate phonemes and do not contain morpheme boundaries.\nThe phonemic distinction in English between the affricate and the stop\u2013fricative sequence (found across syllable boundaries) can be observed by minimal pairs such as the following:\nThe in 'worst shin' can be elided: .\nStop\u2013fricatives can be distinguished acoustically from affricates by the rise time of the frication noise, which is shorter for affricates.\nGeminate affricates.\nWhen affricates are geminated, it is the duration of the plosive closure that is lengthened, not that of the frication. For example, is pronounced , not *.\nList of affricates.\nIn the case of coronals, the symbols \u27e8\u27e9 are normally used for the stop portion of the affricate regardless of place. For example, \u27e8\u27e9 is commonly seen for \u27e8\u27e9, \u27e8\u27e9 for \u27e8\u27e9 and \u27e8\u27e9 for \u27e8\u27e9.\nThe exemplar languages are ones that have been reported to have these sounds, but in several cases, they may need confirmation.\nSibilant affricates.\nThe Northwest Caucasian languages Abkhaz and Ubykh both contrast sibilant affricates at four places of articulation: alveolar, postalveolar, alveolo-palatal and retroflex. They also distinguish voiceless, voiced, and ejective affricates at each of these.\nWhen a language has only one type of affricate, it is usually a sibilant; this is the case in e.g. Arabic (), most dialects of Spanish (), and Thai ().\nTrilled affricates.\nPirah\u00e3 and Wari' have a dental stop with bilabial trilled release .\nHeterorganic affricates.\nAlthough most affricates are homorganic, Navajo and Chiricahua Apache have a heterorganic alveolar-velar affricate . Wari' and Pirah\u00e3 have a voiceless dental bilabially trilled affricate [t\u032a\u0299\u0325] (see #Trilled affricates). Blackfoot has and . Other heterorganic affricates are reported for Northern Sotho and other Bantu languages such as Phuthi, which has alveolar\u2013labiodental affricates and , and Sesotho, which has bilabial\u2013palatoalveolar affricates and . Djeoromitxi has and .\nPhonation, coarticulation and other variants.\nThe coronal and dorsal places of articulation attested as ejectives as well: . Several Khoisan languages such as Taa are reported to have voiced ejective affricates, but these are actually \"pre\"-voiced: . Affricates are also commonly aspirated: , murmured: , and prenasalized: (as in Hmong). Labialized, palatalized, velarized, and pharyngealized affricates are also common. Affricates may also have phonemic length, that is, affected by a chroneme, as in Italian and Karelian.\nPhonological representation.\nIn phonology, affricates tend to behave similarly to stops, taking part in phonological patterns that fricatives do not. analyzes phonetic affricates as phonological stops. A sibilant or lateral (and presumably trilled) stop can be realized phonetically only as an affricate and so might be analyzed phonemically as a sibilant or lateral stop. In that analysis, affricates other than sibilants and laterals are a phonetic mechanism for distinguishing stops at similar places of articulation (like more than one labial, coronal, or dorsal place). For example, Chipewyan has laminal dental vs. apical alveolar ; other languages may contrast velar with palatal and uvular .\nAffricates may also be a strategy to increase the phonetic contrast between aspirated or ejective and tenuis consonants.\nAccording to , no language contrasts a non-sibilant, non-lateral affricate with a stop at the same place of articulation and with the same phonation and airstream mechanism, such as and or and .\nIn feature-based phonology, affricates are distinguished from stops by the feature [+delayed release].\nAffrication.\nAffrication (sometimes called \"affricatization\") is a sound change by which a consonant, usually a stop or fricative, changes into an affricate. Examples include:\nPre-affrication.\nIn rare instances, a fricative\u2013stop contour may occur. This is the case in dialects of Scottish Gaelic that have velar frication where other dialects have pre-aspiration. For example, in the Harris dialect there is 'seven' and 'eight' (or , ). Richard Wiese argues this is the case for word-initial fricative-plosive sequences in German, and coined the term suffricate for such contours. Awngi has 2 suffricates and according to some analyses.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"IPA common/styles.css\" /&gt;&lt;templatestyles src=\"IPA pulmonic consonants/styles.css\" /&gt;&lt;templatestyles src=\"IPA co-articulated consonants/styles.css\" /&gt;&lt;templatestyles src=\"IPA vowels/styles.css\" /&gt;"}
{"id": "66422", "revid": "1317397027", "url": "https://en.wikipedia.org/wiki?curid=66422", "title": "MOVE (Philadelphia organization)", "text": "American Black separatist group\n&lt;templatestyles src=\"Template:Subinfobox bodystyle/styles.css\" /&gt;\nMOVE (pronounced like the word \"move\"), originally the Christian Movement for Life, is a communal organization that advocates for nature laws and natural living, founded in 1972 in Philadelphia, Pennsylvania, United States, by John Africa (born Vincent Leaphart). MOVE lived in a communal setting in West Philadelphia, abiding by philosophies of anarcho-primitivism. The group combined revolutionary ideology, similar to that of the Black Panthers, with work for animal rights.\nMOVE is particularly known for two major conflicts with the Philadelphia Police Department (PPD). In 1978, a standoff resulted in the death of police officer James J. Ramp and injuries to 16 officers and firefighters, as well as members of the MOVE organization. Nine members were convicted of killing the officer and each received prison sentences of 30 to 100 years. In 1985, another firefight ended when a police helicopter dropped two bombs onto the roof of the MOVE compound, a townhouse located at 6221 Osage Avenue. The resulting fire killed six MOVE members and five of their children, and destroyed 65 houses in the neighborhood.\nThe police bombing was strongly condemned. The MOVE survivors later filed a civil suit against the City of Philadelphia and the PPD and were awarded $1.5 million in a 1996 settlement. Other residents displaced by the destruction of the bombing filed a civil suit against the city and in 2005 were awarded $12.83 million in damages in a jury trial.\nOrigins.\nThe group's name, MOVE, is not an acronym. Its founder, John Africa, chose this name to say what they intended to do. Members intend to be active because they say, \"Everything that's alive moves. If it didn't, it would be stagnant, dead.\" When members greet each other they say \"on the MOVE\".\nWhen the organization was founded in 1972, John Africa was functionally illiterate. He dictated his thoughts to Donald Glassey, a social worker from the University of Pennsylvania, and created what he called \"The Guidelines\" as the basis for his communal group. Africa and his mostly African-American followers wore their hair in dreadlocks, as popularized by Rastafari. MOVE advocated a radical form of green politics and a return to a hunter-gatherer society, while stating their opposition to science, medicine, and technology.\nMembers of MOVE identify as deeply religious and advocate for life. They believe that as all living beings are dependent, their lives should be treated as equally important. They advocate for justice that is not always based within institutions. MOVE members believe that for something to be just, it must be just for all living creatures. As John Africa had done, his followers changed their surnames to \"Africa\" to show reverence to what they regarded as their mother continent.\nIn a 2018 article about the group, Ed Pilkington of \"The Guardian\" described their political views as \"a strange fusion of black power and flower power. The group that formed in the early 1970s melded the revolutionary ideology of the Black Panthers with the nature- and animal-loving communalism of 1960s hippies. You might characterise them as black liberationists-cum-eco warriors.\" He noted the group also functioned as an animal rights advocacy organization. Pilkington quoted member Janine Africa, who wrote to him from prison: \"We demonstrated against puppy mills, zoos, circuses, any form of enslavement of animals. We demonstrated against Three Mile Island and industrial pollution. We demonstrated against police brutality. And we did so uncompromisingly. Slavery never ended, it was just disguised.\"\nJohn Africa and his followers lived in a commune in a house owned by Glassey in the Powelton Village section of West Philadelphia. As activists, they staged demonstrations against institutions that they opposed, such as zoos, and speakers whose views they opposed. MOVE activities were scrutinized by law enforcement authorities, particularly under the administration of Mayor Frank Rizzo, a former police commissioner known for his hard line against activist groups.\nIn 1977, three MOVE members were jailed for inciting a riot, occasioning further tension, protests, and armed displays from the group.\n1978 shoot-out.\nIn 1977, according to police accounts, the Philadelphia Police Department (PPD) obtained a court order for MOVE to vacate the Powelton Village property in response to a series of complaints made by neighbors. MOVE members agreed to vacate and surrender their weapons if the PPD released members of their group held in city jails.\nNearly a year later, on August 8, 1978, the PPD came to a standoff with members of MOVE who had not left the Powelton Village property. When police attempted to enter the house, a shootout ensued. PPD Officer James J. Ramp of the Stakeout Unit (now known as the S.W.A.T. Unit), was killed by a gunshot to the neck. 16 police officers and firefighters were also injured in the firefight. MOVE representatives claimed that Ramp was facing the house at the time and denied that the group was responsible for his death, insisting that he was killed by fire from fellow police officers. Prosecutors alleged that MOVE members fired the fatal shot and charged Debbie Sims Africa and eight other MOVE members with collective responsibility for his death.\nAccording to a 2018 article in \"The Guardian\", \n\"Eyewitnesses, however, gave accounts suggesting that the shot may have come from the opposite direction to the basement, raising the possibility that Ramp was accidentally felled by police fire. MOVE members continue to insist that they had no workable guns in their house at the time of the siege. Several months earlier, in May 1978, several guns \u2013 most of them inoperative \u2013 had been handed over to police at the MOVE house; however, prosecutors at the trial of the MOVE Nine told the jury that at the time of the August siege there had been functioning firearms in the house.\"The standoff lasted about an hour before MOVE members began to surrender.\nThe MOVE 9.\nThe nine members of MOVE charged with third-degree murder for Ramp's death became known as the MOVE 9. Each was sentenced to a maximum of 100 years in prison. They were Chuck, Delbert, Eddie, Janet, Janine, Merle, Michael, Phil, and Debbie Sims Africa.\nIn 1998, at age 47, Merle Africa died in prison. Seven of the surviving eight members first became eligible for parole in the spring of 2008, but they were denied. Parole hearings for each of these prisoners were to be held yearly from that time. In 2015, at age 59, Phil Africa died in prison. \nThe first of the MOVE 9 to be released was Debbie Sims Africa on June 16, 2018. Debbie Sims Africa, who was 22 when sentenced, was released on parole and reunited with her 39-year-old son, Michael Davis Africa, Jr. She gave birth to him a month after she was imprisoned, and he was taken from her a week later. The release of Debbie Sims Africa renewed attention on members of MOVE and the Black Panthers who remain imprisoned in the U.S. from the period of the 1960s and 1970s. \"The Guardian\" journalist Ed Pilkington reported in June 2018 that there were at least 25 still in prison.\nOn October 23, 2018, Michael Davis Africa, the husband of Debbie Sims Africa, was released on parole. In May 2019, Janine and Janet Africa were released on parole after 41 years of imprisonment. On June 21, 2019, Eddie Goodman Africa was released on parole. Delbert Orr Africa was granted parole on December 20, 2019, and released January 18, 2020. The last of the MOVE 9 either to be paroled or to die behind bars was Chuck Sims Africa, who was released on parole on February 7, 2020, after 41 years of imprisonment. Both Delbert and Chuck died of cancer in 2020 and 2021, respectively.\n1985 bombing.\nIn 1981, MOVE relocated to a row house at 6221 Osage Avenue in the Cobbs Creek area of West Philadelphia. Neighbors complained to the city for years about trash around their building, confrontations with neighbors, and bullhorn announcements of sometimes obscene political messages by MOVE members. The bullhorn was broken and inoperable for the three weeks prior to the police bombing of the row house.\nThe police obtained arrest warrants in 1985 charging four MOVE occupants with crimes including parole violations, contempt of court, illegal possession of firearms, and making terrorist threats. Mayor Wilson Goode and police commissioner Gregore J. Sambor classified MOVE as a terrorist organization. Police evacuated residents of the area from the neighborhood prior to their action. Residents were told that they would be able to return to their homes after a 24-hour period.\nOn Monday, May 13, 1985, nearly five hundred police officers, along with city manager Leo Brooks, arrived in force and attempted to clear the building and execute the arrest warrants. Nearby houses were evacuated. Water and electricity were shut off in order to force MOVE members out of the house. Commissioner Sambor read a long speech addressed to MOVE members that started with, \"Attention MOVE: This is America. You have to abide by the laws of the United States.\" When the MOVE members did not respond, the police decided to forcibly remove the 13 members from the house, which consisted of seven adults and six children.\nThere was an armed standoff with police, who lobbed tear gas canisters at the building. The MOVE members fired at them in return, and a 90-minute gunfight ensued, in which one officer was bruised in the back by gunfire. Police used more than ten thousand rounds of ammunition before Commissioner Sambor ordered that the compound be bombed. From a Pennsylvania State Police helicopter, Philadelphia Police Department Lt. Frank Powell proceeded to drop two one-pound bombs (which the police referred to as \"entry devices\") made of FBI-supplied Tovex, a dynamite substitute, targeting a cubicle on the roof of the house. The ensuing fire killed eleven of the people in the house (John Africa, five other adults, and five children aged 7 to 13). The fire spread and eventually destroyed approximately 65 nearby houses on Osage Avenue and nearby Pine Street. Although firefighters had earlier drenched the building prior to the bombing, after the fire broke out, officials said they feared that MOVE would shoot at the firefighters, so held them back.\nRamona Africa, one of the two MOVE survivors from the house, said that police fired at those trying to escape.\nAftermath.\nGoode appointed an investigative commission called the Philadelphia Special Investigation Commission (PSIC, aka MOVE Commission), chaired by attorney William H. Brown, III. Sambor resigned in November 1985; in a speech the following year, he said that he was made a \"surrogate\" by Goode.\nThe MOVE Commission issued its report on March 6, 1986. The report denounced the actions of the city government, stating that \"Dropping a bomb on an occupied row house was unconscionable.\" Following the release of the report, Goode made a formal public apology. No one from the city government was criminally charged in the attack. The only surviving adult MOVE member, Ramona Africa, was charged and convicted on charges of riot and conspiracy; she served seven years in prison.\nIn 1996 a federal jury ordered the city to pay a $1.5 million civil suit judgment to survivor Ramona Africa and relatives of two people killed in the bombing. The jury had found that the city used excessive force and violated the members' constitutional protections against unreasonable search and seizure. In 1985 Philadelphia was given the sobriquet \"The City that Bombed Itself\".\nIn 2005 federal judge Clarence Charles Newcomer presided over a civil trial brought by residents seeking damages for having been displaced by the widespread destruction following the 1985 police bombing of MOVE. A jury awarded them a $12.83 million verdict against the City of Philadelphia.\nOn November 12, 2020, the City Council of Philadelphia passed a resolution apologizing \"for the decisions and events preceding and leading to the devastation that occurred on May 13, 1985.\" The council established \"an annual day of observation, reflection and recommitment\" to remember the MOVE Bombing.\nIn 2021, former members of MOVE came forward with allegations of abuse within the organization. As Jason Nark writes in \"The Philadelphia Inquirer,\" \u201cMore than a half-dozen ex-MOVE members have gone on the record in both the \"Murder at Ryan\u2019s Run\" podcast and the blog (started by an ex-MOVE supporter) titled \"Leaving MOVE 2021\", alleging physical and mental abuse in MOVE, a doctrine of homophobia and colorism, and what they describe as a manipulation of the public and the media under the banner of social justice.\"\n2002 shooting of John Gilbride.\nAfter John Africa's death, his widow, Alberta, married John Gilbride, Jr. Together they had a child, Zackary Africa, before divorcing in 1999. By 2002, Gilbride had no longer supported MOVE and resettled in Maple Shade, New Jersey. Alberta Africa was living in Cherry Hill, New Jersey, with their son, John Zachary Gilbride.\nOn September 10, 2002, in the course of their bitter custody dispute, Gilbride testified in court that MOVE had threatened to kill him. The court granted Gilbride partial custody of Zackary, allowing him unsupervised visits.\nOn September 27, shortly after midnight and prior to Gilbride's first visitation date with Zackary, an unknown assailant shot and killed him as he sat in a car parked outside his New Jersey apartment complex. Investigators did not name a suspect and the Burlington County Police did not release ballistics information.\nThe case remains unsolved. A MOVE spokeswoman initially said that the U.S. government had assassinated Gilbride in order to frame MOVE. His ex-wife Alberta Africa denied that the murder had occurred. She said in 2009 that Gilbride \"is out hiding somewhere\". Tony Allen, an ex-MOVE member, says that MOVE murdered Gilbride.\nIn 2012, \"The Philadelphia Inquirer\" reported that Gilbride had told friends and family that he had recorded incriminating evidence in a notebook as security against a \"hit\" by MOVE. Gilbride said he had placed the notebook inside a locker for safekeeping. The Burlington County Prosecutor's Office declined to follow up on the report.\nHuman remains.\nIn April 2021, the Penn Museum and the University of Pennsylvania apologized to the Africa family for allowing human remains from the MOVE house to be used in research and training. In 1985, the Philadelphia City Medical Examiner's Office gave burned human remains found at the MOVE house to the University of Pennsylvania Museum of Archaeology and Anthropology for verification that the bones were those of 14-year-old Tree Africa and 12-year-old Delisha Africa. The remains were kept in a cardboard box in storage for decades and studied by Alan Mann, a professor at Penn and Janet Monge, the curator of the Penn Museum. The bones were used as part of an online forensic course as a case study. When Mann transferred to Princeton University in 2001, he reportedly took the remains with him.\nPhiladelphia Health Commissioner Thomas Farley resigned in May 2021 upon revelations that he ordered the cremation of a set of victims' remains without notifying or obtaining permission from the families of the deceased or even releasing the names of the deceased. The day after his resignation, the remains were recovered in a box labeled MOVE.\nOther notable events.\nRamona Africa acts as a spokesperson for the group. Mumia Abu-Jamal, a member of MOVE, was convicted and sentenced to death for the unrelated 1981 murder of police officer Daniel Faulkner. The death sentence was overturned in 2011 by a federal judge and Abu-Jamal was resentenced to life imprisonment without the possibility of parole. MOVE continues to advocate for Abu-Jamal's release.\n Michael Moses Ward, known in MOVE as Birdie Africa, was the only child to survive the 1985 bombing. Ward was 13 years old at the time of the incident and suffered serious burns from the fire, which killed his mother. Ward's father, Andino Ward, sued the City of Philadelphia, and the parties reached a settlement. He lived with his father afterward and did not remain involved with MOVE. He died in 2013 in an accidental drowning.\nIn June 2020, MOVE member Delbert Africa died.\nLegacy.\nOn the 25th anniversary of the 1985 bombing, \"The Philadelphia Inquirer\" published a detailed multimedia website containing retrospective articles, archived articles, videos, interviews, photos, and a timeline of the events.\nJohn Edgar Wideman's 1990 novel \"Philadelphia Fire\" is based on the MOVE bombing.\nMischief Brew\u2019s 2006 song \"Save A City\u2026\" is about the MOVE bombing.\nDocumentaries.\n\"The Bombing of Osage Avenue\" (1986) by author Toni Cade Bambara and Louis Massiah provides context for the bombing by using the history of the Cobbs Creek community. It focuses on the bombing's effects on community residents who did not belong to MOVE. The film also uses footage of the hearings of the MOVE commission. It premiered on WHYY-TV, Philadelphia's public broadcasting station.\n\"Let the Fire Burn\" (2013) by producer/director Jason Osder about MOVE composed largely of archival footage.\n\"\" (2019) by Matt Wolf also featured footage of the group on the ABC news show \"Nightline\".\n\"40 Years a Prisoner\" (2020) by filmmaker Tommy Oliver chronicles the controversial 1978 Philadelphia police raid on MOVE and the aftermath that led to a Mike Africa Jr.'s decades-long fight to free his parents.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66424", "revid": "40890685", "url": "https://en.wikipedia.org/wiki?curid=66424", "title": "Stormbringer", "text": "Fictional sword from Michael Moorcock stories\nStormbringer is a magic sword featured in a number of fantasy stories by the author Michael Moorcock. It is described as a huge, black sword covered with strange runes, created by the forces of Chaos. The sword has a will of its own and it is hinted that the sword may be controlled by an inhabiting entity. It is wielded by the doomed albino emperor Elric of Melnibon\u00e9. Stormbringer makes its first appearance in the 1961 novella \"The Dreaming City\". In the four novellas collected in the 1965 book \"Stormbringer\", the sword's true nature is revealed.\nDescription.\nThis powerful enchanted black blade is a member of a demon race that takes on the form of a sword, and as such is an agent of Chaos. Stormbringer's edge is capable of cutting through virtually any material not protected by potent sorcery, and it can drink the soul from (and thereby kill) any unprotected living creature upon delivering any wound, even a scratch. Its most distinctive features are that it has a mind and will of its own, and that it feeds upon the souls of those it kills. Elric loathes the sword but is almost helpless without the strength and vitality it confers on him.\nStormbringer's hunger for souls is such that it frequently betrays Elric by creating a bloodlust in his mind, turning in his hands and killing friends and lovers. The cursed nature of the sword adds to Elric's guilt and self-loathing, even as he feels pleasure when the stolen lifeforce enters his body.\nStormbringer has a \"brother\" sword named Mournblade, which was at one time wielded by Elric's cousin and enemy Yyrkoon. It is identical to Stormbringer in most regards. Later stories reveal that there are thousands of identical demons, all taking the form of swords. Three such sibling blades appear in \"The Revenge of the Rose\" and many more \"brother blades\" are seen in the novel \"Stormbringer\", but only Mournblade and Stormbringer are named.\nIn \"Elric of Melnibon\u00e9\", Elric and cousin Yyrkoon find the runeblades in a realm of Limbo and commence battle. Elric and Stormbringer disarm Yyrkoon, and Mournblade disappears. Yyrkoon is defeated, and Elric and his cousin return to Imrryr.\nIn \"The Weird of the White Wolf\", Elric returns to Imrryr after a long journey and confronts Yyrkoon, who usurped the throne in his absence. Yyrkoon has regained Mournblade through unknown means and uses it to attack. Elric and Stormbringer kill Yyrkoon, and no further mention is made of Mournblade until it is later disclosed that it was recovered by the Seers of Nihrain, to be wielded by Elric's cousin, Dyvim Slorm. Imrryr is sacked, though the pillagers' fate is not much better, being pursued by the golden battle barges and the few dragons who were awakened, led by Dyvim Tvar. Only Elric's ship escapes, propelled by the aid of his sorcery.\nIn \"Stormbringer\", Elric learns that the representatives of Fate, which serve neither Chaos nor Law, recovered Mournblade from the netherworld. They present it to Elric and explain that the runeblades were designed to be wielded by those with Melnibon\u00e9an royal blood as a check against the might of powerful beings including the Dead Gods of Chaos. Elric gives Mournblade to his kinsman, Dyvim Slorm, and the two men become embroiled in a confrontation between the gods. Elric summons others of Stormbringer's demonic race (also in the form of swords) to fight against a number of Dukes of Hell, brought to the Young Kingdoms by Jagreen Lern, theocrat of Pan Tang.\nUltimately, Elric's reliance on Stormbringer proves his undoing: after the utter destruction of the Young Kingdoms in the battle of Law and Chaos, just as it seems that the cosmic Balance has been restored, Stormbringer kills Elric, transforms into a humanoid demon, and leaps laughing into the sky, to corrupt the newly-remade world once more. The sword-spirit says to the dead Elric: \"Farewell, friend. I was a thousand times more evil than thou!\"\nIn the book \"The Quest for Tanelorn\", a character claims that the demon in the sword is named Shaitan \u2013 a variant of 'Satan', and in Arabic a word meaning a devil, if not the Devil. In the same book it is revealed that the demon can inhabit either the black sword or the black jewel, the jewel which was once embedded in the skull of Dorian Hawkmoon. Hawkmoon was an avatar, like Elric, of the Eternal Champion.\nAnalysis.\nThe theme of a cursed magical sword which causes evil deeds when drawn goes back to the sword Tyrfing in Norse Mythology, with which Moorcock was likely familiar. Stormbringer was influential in popularizing this trope in the fantasy genre. Moorcock intended the sword character to serve as a key element of his discussion of \"how mankind's wish-fantasies can bring about the destruction of... part of mankind\". Claiming influence from Freud and Jung he says: \"The whole point of Elric's soul-eating sword, Stormbringer, was addiction: to sex, to violence, to big, black, phallic swords, to drugs, to escape. That's why it went down so well in the rock\u2019n\u2019roll world\".\nLiterature scholar Dennis Wilson Wise wrote that \"a weapon like Stormbringer reinforces liberal selfhood in a particularly concrete way. It carries a continuous external threat to personal autonomy, and it subverts a fully rational self-determination. Modern fantasy heroes, especially in epic fantasy, often rail against \"destiny\" or a prophecy, but such destinies and prophecies lack Stormbringer's sentient specificity.\"\nOntologist Levi Bryant stated that Stormbringer belongs to a special class of magical items which also appear in \"Dungeons &amp; Dragons\", which are not \"merely passive tools\", but have will, goals, alignment and a personality of their own. Stormbringer talks to, influences and struggles with its wielder Elric. Bryant saw the sword as an active entity, not unlike \"some of the artificial life we are developing today\", and also compared it to \"technologies unleashed on the world that are agents in their own right\".\nBooks featuring Stormbringer.\nElric's sword Stormbringer has appeared in all of Michael Moorcock's stories about Elric, except the prequel \"Folk of the Forest\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66425", "revid": "2", "url": "https://en.wikipedia.org/wiki?curid=66425", "title": "Anaerobe", "text": ""}
{"id": "66426", "revid": "13889901", "url": "https://en.wikipedia.org/wiki?curid=66426", "title": "Waterloo &amp; City line", "text": "London Underground line\nThe Waterloo &amp; City line, colloquially known as The Drain, is a shuttle line of the London Underground that runs between Waterloo and Bank stations with no intermediate stops. Its primary traffic consists of commuters from south-west London, Surrey and Hampshire arriving at Waterloo main line station and travelling forward to the City of London financial district. For this reason, the line has historically not operated on Sundays or public holidays, except in very limited circumstances. The line was closed during the COVID-19 pandemic; since reopening in October 2021 it is open only on weekdays. It is one of only two lines on the Underground network to run completely underground, the other being the Victoria line.\nPrinted in turquoise on the Tube map, it is by far the shortest line on the Underground network, being long, with an end-to-end journey lasting just four minutes. In absolute terms, it is the least-used Tube line, carrying just over 17\u00a0million passengers annually. However, in terms of the average number of journeys per kilometre it is the third-most intensively used line behind the Jubilee and Victoria lines. \nThe line was built by the Waterloo &amp; City Railway Company and was opened in 1898 (at the time, Bank station was named \"City\"). When it opened it was the second electric underground railway in London, following the City and South London Railway (now part of the Northern line). Its construction was supported by the London and South Western Railway, whose main line trains ran into Waterloo, and for many years it continued to be owned and operated by the LSWR and its successors as a part of the national railway network, not as part of the London Underground network it resembled. Following a major refurbishment and replacement of rolling stock by Network SouthEast in the early 1990s, operations were transferred to London Underground in 1994.\nHistory.\nBackground.\nThe London and South Western Railway (LSWR) reached Waterloo Bridge on 11 July 1848, serving routes from Southampton and Richmond. It was officially renamed Waterloo in October 1882.\nThat the station was not within walking distance of the City of London was viewed as a serious shortcoming. The LSWR had hoped to build a line eastwards to near London Bridge but because of the slump following the railway mania, and the high cost of building through the area, this idea was abandoned. When the South Eastern Railway opened an extension from London Bridge to Charing Cross in 1864, a connecting railway line from it to Waterloo was built, but friction and competitive hostility between the companies meant the line saw no regular passenger movements. Under pressure from the LSWR, the SER constructed Waterloo Junction station, now called Waterloo East, on the Charing Cross line. The station opened in January 1869, but through ticketing was refused and the onward connection remained frustratingly unsatisfactory.\nIndependent proposals.\nA Waterloo and Whitehall Railway was promoted in 1864, to construct a tube railway from Great Scotland Yard to Waterloo. It was to use air pressure to propel the vehicles northwards, and exhaust air to draw them southwards, using a pressure differential of &lt;templatestyles src=\"Fraction/styles.css\" /&gt;2+1\u20442 oz per sq in (about 11 mbar). The trains themselves would be the pistons. The company capital was to be \u00a3100,000. It was suggested that there could be a branch to where the Embankment station is now located: it is not clear how a junction would be managed in a pneumatic railway. There were to be three vehicles, one loading at each terminal and one in motion in the tube, so they must have been intended to pass at the terminals. There were to be three classes of accommodation in the coaches.\nWork started on 25 October 1865, but less than a year later it was obvious that the capital was grossly inadequate. Authority for extension of time and more capital was obtained, but by then few investors had any confidence that their investment would gain a return. In 1868, a further extension was granted, but little further work was done, and nearly all the money had gone.\nIn 1881, an independent Waterloo and City Railway was promoted, to build a surface line to Queen Street. The cost was formidable at \u00a32.3\u00a0million, and the proposal soon collapsed.\nThe Waterloo &amp; City Railway Bill.\nIn 1891, the Corporation of the City of London made a statistical survey which it published ancillary to the National Census taken in that year. 37,694 persons lived in the City, but the daytime occupation was 310,384. On 4 May 1891, 1,186,094 entries to the City were made, i.e., many people entered more than once. Separate statistical information is that about 50,000 persons arrived at Waterloo daily, of whom about 12,000 proceeded to the City by some means.\nIn November 1891, a bill was deposited to build an underground electric railway from Waterloo to the Mansion House in the City; the capital was to be \u00a3500,000; the proposal was supported by the LSWR but was independent. Three other \"tube\" railways were proposed in the same parliamentary session, the traditional cut-and-cover method being seen as impractical, as was an elevated railway on viaduct. Electric urban railways had been introduced in Germany in 1891 and in the United States of America, and were in daily, widespread use; but in the United Kingdom, only one example was in existence, the City and South London Railway.\nThe progress of the bill through Parliament was slow, partly because of the novelty of considering tube railway schemes; there were several petitions from the authorities responsible for public works in the city. London County Council tried to insist that the tubes should be made large enough to carry ordinary trains, and that all trains arriving at Waterloo should continue through them to the City. This idea would have required a new subterranean terminal station at the Bank of at least equal size to Waterloo itself.\nNumerous petitions against the bill, or requiring additional protections to be included in it, were presented, but eventually on 27 July 1893, the &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;Waterloo and City Railway Act 1893 (56 &amp; 57 Vict. c. clxxxvii) gained royal assent.\nConstruction.\nFollowing royal assent, the company prepared for construction. The new company issued its prospectus in March 1894 and the subscription list closed on 21 April; 54,000 shares at \u00a310 each were offered and there was a slight over-subscription. A dividend of 3% per annum payable out of capital was promised during the construction phase.\nTenders were acquired for the main tunnel work, and a contract was awarded to John Mowlem &amp; Co for the sum of \u00a3229,064 (). The consulting engineers were W. R. Galbraith (of the LSWR) and J. H. Greathead, developer of the tunnelling shield. The resident engineer was H. H. Dalrymple-Hay. Mowlems' engineer in charge was William Rowell.\nMowlem began work on 18 June 1894, first building staging in the river about west of Blackfriars Bridge. Piles were driven for a cofferdam and two vertical shafts of internal diameter were constructed as headings for the tunnel drive. The average depth of the tunnels is about , with its deepest points at the River Thames, at underground.\nDriving the running tunnels started in November 1894, using the Greathead system of shield excavation, cast iron segment lining, compressed air working, and compressed air grouting behind the tunnel lining. Twenty men worked in each heading.\nThe excavated material was removed from the staging near Blackfriars Bridge; it was conveyed there from the shields by a narrow gauge railway using electric locomotives supplied by the Siemens Company. Two were in use and a third was on order at August 1895. They operated on gauge track with a twin overhead trolley wire (i.e., not using the track for current return) at .\nThe station works at Waterloo were constructed by Perry and Co. The station tracks run in separate but adjacent arches supporting the main line station, which run transversely to the main line track. The arch piers needed to be underpinned to about lower than the original foundations.\nCivil engineering detail.\nThe route starts from a point south-east of Waterloo main line station, halfway between Lower Marsh and the now-vanished Aubyn Street, which was destroyed in the station's early 20th century expansion and was located more or less where today's platforms 3 and 4 are. Leaving towards the north west, the line turns in a curve towards the north east. The curve is constructed by cut-and-cover, and the twin tubes start immediately after it, under Stamford Street, turning north-north-east to pass under the River Thames, converging with Blackfriars Bridge on the north bank. The line turns east there, under Queen Victoria Street, to the station adjacent to the Mansion House, running for part of the way under the District line. The sharpest curves other than those at Waterloo are radius.\nThe northbound line falls at 1 in 30 for from Waterloo; then the line falls at 1 in 120 and then 1 in 800 to the shaft in the river. The westbound line (considered in reverse to the direction of running) falls at only 1 in 60, and then 1 in 550 to the shaft. From there they run together, level for and then climbing at 1 in 800 for , and then 1 in 88 to the terminus.\nThe tunnels are internal diameter, except for the curves, where they are . Each long section of tunnel wall was formed with a cast iron ring, made from seven segments and a key piece at the top. bolts connect all the segments. Between each section there was a creosoted timber strip thick, and varying the thickness of this enabled the forward course of the tube to be varied, except in the sharpest curves where the segments were cast to form the curve. There are seven cross-passages between the twin tubes.\nUnder the Thames the top of the tube is below the bed of the river. The total length of the line is .\nThe underground station at Waterloo was located within the existing transverse arches of the main line station, with the arrival and departure platforms in separate arches, and a staircase access. Siding accommodation and a reversing siding were provided beyond the platforms: after disembarkation of passengers, an arriving train would continue forward to the reversing sidings, and then return to the departure platform. An additional lay-by siding was provided later.\nAt the new City station there were two platforms and either could be used by an arriving train, reversing in the platform. The track connections at the approach were a double slip, not a scissors, so a train could not leave while another was arriving. The left hand platform line was extended by a train length and trains could be stabled in the extension. A large diameter Greathead shield was used to bore the section of tunnel where the track connections would be installed.\nThe tube section for the platform lines at the City station were in diameter, the largest in the world at the time.\nOriginal signalling.\nIn late 1897, contracts were let for the signalling equipment; the electric interlocking was to be carried out by W. R. Sykes, who had a call-off contract with the LSWR; a supplement to their standard prices for the tunnel work was agreed.\nThere were signalboxes at Waterloo at the south end of the northbound platform, and at south end of the northbound platform at City. There were conventional semaphore signals in the open south of Waterloo station, but all other signals were electric lights only. Sykes' lock-and-block system was used with depression-type treadles. Although there was only one signal section, advance starting signals were provided. The platform starting signals at Waterloo and at City had a lower arm, a \"shunt-by signal\" which when lowered indicated that the line was clear only to the advance starting signal. The main starting signal when lowered indicated that the line was clear to City.\nAn electrical traction current interrupt system was installed; a short length of contact bar was provided at each signal, connected to earth when the signal was at danger, and otherwise isolated. A \"slipper\" contact was fitted on the trains, and if it contacted the contact bar when it was earthed, the traction current was tripped.\nTraction electricity.\nOn 4 January 1897, a contract was signed with Siemens and Co for the electrical generating and distribution equipment, and the electrical train equipment, for \u00a355,913. Although a German firm, Siemens had a large presence in the UK at the time. There were three lower tenders.\nThere were five boilers working at driving five (later six) high speed steam engines developing directly coupled to dynamos. The two-pole compound-wound dynamos delivered at no load and under full load; this gave at . Special attention was given to the closeness of the governing to ensure a stable supply voltage. The station lighting circuits were fed from the main switchboard and specially led to maintain lighting supply in the event of a traction current disruption. Station lighting used four lamps in series, with return current via the running rails. (Gas lighting was provided as a back-up.)\nThere was a short high-level siding within the Waterloo yard area; coal to fuel the boilers was brought in by ordinary LSWR wagons lowered to the running line by the carriage lift; the wagons were drawn through the northbound platform by an electric shunting locomotive, and another lift elevated them to the siding. Boiler ash was disposed of correspondingly.\nCity station.\nThe City station was not originally called Bank. The Central London Railway (CLR, which became the central section of what is now the Central line) obtained an Act of Parliament in 1891 varying their previously intended route, to take them to the area of the present-day Bank station. The act required them to construct a central station and booking office and public subways connecting the surrounding streets. The subways were to be regarded as public, although maintained by the CLR. Any other railway intending to have a station nearby was entitled to connect to the CLR station by subways. This obviously referred to the Waterloo &amp; City line, and was designed to create a single station frontage in the congested street area. The CLR completed its construction after the W&amp;CR but was obliged to finish the facilities necessary for the earlier opening of the W&amp;CR. The City and South London Railway (CSLR) also operated from the station.\nGillham says:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Right from the start the joint station and circular subway area was always known as the 'City station' by the W&amp;C but as the 'Bank station' by the CLR and the CSLR.\"\nThe W&amp;CR station was located some considerable distance from the area near street level, and this later led to persistent complaints as it required passengers to climb a steep and lengthy gradient to reach the exit.\nPermanent way.\nThe ordinary LSWR permanent way was used, with rails, but in the tubes longitudinal timbers were used instead of cross-sleepers. The sharp curves had check rails. Cross-bonds paralleling the running rails electrically were provided every and between tracks at the cross passages. The track gauge was the standard .\nThe conductor rail was a steel inverted channel placed centrally, with its upper surface at the same level as the upper surface of the running rails. At pointwork a hardwood ramp was provided to raise the collector shoes above running rail level.\nShunting locomotives.\nPart of Siemens's work under the supply of electrical equipment including a shunting locomotive; this was a four-wheel electric locomotive with a cab at one end only, It had two traction motors and was delivered in 1898. Its main duty was the delivery of the generator station coal. Like the passenger vehicles, its brake system had air reservoirs charged from a static supply at Waterloo. It remained on the system until 1969, when it was transferred to the National Railway Museum at York.\nIn 1901, a second, more powerful shunting locomotive was acquired. Designed by the LSWR Chief Mechanical Engineer, Dugald Drummond it had two four wheel bogies and was intended for the rescue of failed passenger trains in the tunnel. In 1915, it was removed from the tunnel and put to work shunting coal wagons at Durnsford Road power station, having had its shoe collectors altered for the surface traction supply system.\nThe Armstrong Lift.\nAs the line had no connection to any other line, nor any ground level section, it was necessary to provide a hoist to bring the passenger cars to the line, and to get them out for heavy maintenance. This was provided to the west of the Windsor side of Waterloo main line station, and was known as the \"Armstrong\" lift, after the manufacturer, Sir W G Armstrong Whitworth &amp; Co Ltd, who was paid \u00a33,560. It was operated by water power; at the time of construction hydraulic power was commonly used in urban areas, supplied by utility companies, to operate hoists and lifts. The lift was to be capable of lifting . It was completed in April 1898. There was a smaller hoist within the low-level siding area at Waterloo for the boiler fuel wagons; this had a smaller travel and was installed by John Abbot &amp; Co for \u00a3595.\nBefore the construction of Waterloo International terminal in 1990, the vehicles were hoisted individually by the Armstrong Lift outside the north wall of Waterloo main line station. The procedure is now carried out using a road-mounted crane in a shaft adjacent to the depot, south of Waterloo main line station on Spur Road. This is only necessary for major maintenance work that requires lifting of the car body, as the Waterloo depot is fully equipped for routine maintenance work. The remaining stub of the siding tunnel that led to the Armstrong Lift can still be seen on the left-hand side of the train shortly after leaving Waterloo for Bank, but the lift itself was buried (along with the entire Western sidings) in 1992 as part of the construction of Waterloo International station.\nThe line in operation.\nOnce works were complete and the Board of Trade inspecting officer passed the line as fit, Prince George, Duke of Cambridge formally opened the line on 11 July 1898. About 400 persons travelled from Waterloo to the City station and immediately back to Waterloo.\nArrangements had been made for the LSWR to work the line, but not everything was in place for immediate opening: there was a delay of four weeks.\nThe Waterloo &amp; City Railway opened to the public at 8 a.m. on Monday 8 August 1898, with a train leaving each terminal simultaneously. The fares were 2d one class only, payable at a turnstile, but returns and season tickets, and add-ons to surface tickets were available. From 1900, the turnstiles were removed and conductors travelled on the trains, carrying Bell Punch ticket machines. The daily average receipts in January 1899 were \u00a386, and with steadily rising passenger usage and income the Company was able to pay a 3% dividend out of income following the annual general meeting of February 1902. Sunday services were not considered at this period, and in 1906 it was stated that \"it would cost \u00a320 each Sunday to run the trains, and they would not get that back in receipts.\"\nVery soon after operation, it was realised that the line was running to capacity at the business peaks, then referred to as \"the rush\", and very lightly used for the remainder of the day. Accordingly, in the spring of 1899 an order was placed with Dick, Kerr &amp; Co. for five new motor cars for single operation. The driving cabs were half width; the traction motors, two per car, were nose suspended with single reduction gear. As with the earlier cars, the air brake reservoir was charged from static equipment at Waterloo. Five of these single cars were delivered in February 1900 and entered service in the spring. From that time, they alone worked the off-peak service, and the original vehicles only worked the peak services.\nAbsorption by the LSWR.\nThe line had been worked by the LSWR from the outset, and in 1906 the LSWR made overtures to the W&amp;CR concerning an outright absorption. It was suggested at an extraordinary general meeting of the W&amp;CR that increasing competition motivated the LSWR. An enabling Act was passed on 20 July 1906 and shareholders' approval being obtained, the transfer took place on 1 January 1907, with the shareholders receiving LSWR shares, and the W&amp;CR ceased to exist.\nIn 1915, the LSWR started electrifying its suburban routes, and for the purpose it built a large generating station at Wimbledon, Durnsford Road. The power for train operation on the Waterloo &amp; City line was supplied from this from December 1915, and the original W&amp;CR generating plant now served only ancillary purposes in the line, but also heating and lighting of the main LSWR Waterloo offices. The traction voltage on the W&amp;CR was increased from the original to .\nIn 1921, it had been considered desirable to augment train lengths at the busy periods, and four new trailer coaches to the original specification were built at Eastleigh; 24 five-car trains were run per hour at the busiest times.\nSouthern Railway.\nBy the Railways Act 1921, the main line railway companies of Great Britain were grouped into four companies, effective at the beginning of 1923. The LSWR was now part of the Southern Railway. Due to the Waterloo &amp; City's status as part of one of the \"Big Four\" railway companies, it was not taken over by the London Passenger Transport Board (LPTB) at the latter's formation in 1933, making the W&amp;C the only tube railway in London not to fall under the control of the LPTB. Despite this anomaly, the line was included on most versions of the Underground map produced by the LPTB and its successors up until the line's absorption into the London Underground network in 1994.\nIn 1934, the LPTB proposed that the Waterloo &amp; City should have a new intermediate station at Blackfriars, connecting with the District line station there. They further proposed that the Waterloo &amp; City line should be extended to Liverpool Street station and Shoreditch, the trains there continuing over the East London Railway to New Cross and New Cross Gate. It is not clear whether the scheme had been costed, but nothing came of it.\nNew rolling stock.\nIn 1937, the Southern Railway carried out a thorough review of the technical aspects of the line, now 40 years old. This led to an immediate proposal to order new rolling stock in five-car formations, in association with the provision of escalators at the City station. The scheme was delayed and the declaration of war on 3 September 1939 led to cancellation of the escalator scheme. However, the rolling stock order was proceeded with, and the Art Deco style trains were delivered in 1940, later classified as Class 487.\nThe original central third rail to power the trains was replaced by a Southern Railway standard steel rail placed outside the running rails. Automatic signalling with train stops was also provided. The City signal box was abolished, and fully automatic working implemented there; the lay-by sidings were abolished. The new stock did not require travelling conductors, and tickets were issued at the terminals. When the line reopened with new trains on 28 October 1940, the City station was renamed \"Bank\" in conformity with the usage of the LPTB there.\nBritish Railways.\nOn 1 January 1948, the Southern Railway, as well as the other main line railways of Great Britain, was nationalised, forming \"British Railways\".\nOn 13 April 1948, a serious accident took place at the Waterloo Armstrong Lift; coal was still taken down to the original generating station which powered station offices at Waterloo. A shunt of wagons was being propelled on to the lift at the upper level; four pawls were supposed to be engaged to provide partial support to the lift table, but it appears that some had not engaged. The table tilted, drawing the wagons and M7 locomotive number 672 on to the table; the table and the entire shunt including the locomotive fell down the shaft. The locomotive and wagons were cut up in situ.\nThe Travolator.\nWhen the line was built, the platforms at Bank (then known as \"City\") were located a considerable distance from the surface exits, and a long sloping tunnel had to be negotiated on foot. This led to constant complaints and from 1929 there were many proposals to improve the arrangements, as passenger numbers increased, adding congestion to the physical exertion. The proposals had included new escalators, direct connection to adjacent Central London Railway (later Central line) platforms, and new, closer, tunnelled exits.\nIn the 1950s, a \"Speedwalk\" system of people movers consisting of a continuous rubber belt system, was implemented in certain American cities. After considerable delay considering this and alternatives, British Railways let a contract on 4 July 1957 for the civil engineering works in driving a new sloping access tunnel, in which a pair of travolators (at the time often written \"Trav-O-Lator\") would be installed by Waygood Otis. Otis did not, at this stage, gain a contract.\nHowever, as work was getting under way, the government imposed heavy cuts in capital expenditure on the railways, and after considerable deliberation, it was decided once again to defer alleviation of the problem; no financial benefit was anticipated from the scheme, whereas competing schemes would significantly reduce operational costs. The consulting engineers were directed to suspend work on 11 December 1957, although some enabling work, particularly a sewer diversion, proceeded.\nThe financial restrictions were not long-lasting, and on 10 July 1958 it was announced that the work would resume. It progressed without further major difficulties and a formal opening by the Lord Mayor of London took place on 27 September 1960, coming into public use immediately. There were two parallel travolators, each with a moving surface having 488 platform sections each ; the whole length is on an inclination of 1\u00a0in\u00a07 (about 14.3%). There was a moving handrail. In the morning peak both travolators would operate upwards, with arriving passengers being required to walk down the original ramps; at other times one travolator operated in each direction. The original Otis \"Trav-O-Lators\" have since been replaced by CNIM machines.\nIn association with the work, some improvements were made to the station environment at the Waterloo station, and a &lt;templatestyles src=\"Fraction/styles.css\" /&gt;2+1\u20442 minute frequency was implemented in the peaks; this involved some minor signalling changes, reversion to alternating platform use at Bank, and the use of turnover drivers and guards (where the arriving driver and guard are replaced by staff waiting at the appropriate place for the change of direction, sometimes referred to as \"stepping up\"). A \"Rear Cab Clear\" plunger is provided at Bank so that the arriving driver can confirm that he is clear of the cab and the \"step-back\" driver can depart when the signal clears. Overall, the work had cost \u00a3910,500.\nNetwork SouthEast.\nIn the mid 1980s, British Rail was split into business sectors, with the line falling under the purview of Network SouthEast (NSE). The line was branded as Waterloo and City, and the elderly Class 487 trains were repainted in the red, blue and white NSE livery. In September 1989, a total route modernisation project was agreed at a cost of \u00a319 million. Both stations would be refurbished in the NSE style, track and signals would be replaced and new rolling stock was ordered.\nAt the same time as the upgrade project, the Eurostar terminal at Waterloo International was being built over a large area on the north side of Waterloo station. This removed access to the Armstrong Lift that allowed rolling stock and other machinery to access the line, and therefore a replacement shaft near Spur Road was constructed to allow access to Waterloo Depot.\nThe modernisation project was completed by July 1993, following the delivery of five four-car Class 482 trains. These were built to a modified design from an order for 1992 Stock trains by London Underground for the Central line.\nLondon Underground.\nAs part of the privatisation of British Rail, on 1 April 1994 the line was transferred to London Underground Ltd for the sum of \u00a31. At the time, staff were given the option of transferring with the line or remaining in British Rail employment, and all except one chose the latter. The drivers are currently based at Leytonstone. The turquoise line colour was chosen by a lawyer working on the legal transfer of the line to London Underground. From 15 April 1996, the line began working to a new timetable, with three trains departing in each ten minutes during the morning peak.\nIn January 2003, the Waterloo &amp; City was closed for over three weeks for safety checks after a major derailment on the Central line, which required all 1992 tube stock trains to be modified. That same year, responsibility for the line's maintenance was given to the Metronet consortium under the terms of a public\u2013private partnership arrangement.\n2006 refurbishment.\nBetween April and September 2006, the line was closed for five months for a \u00a340million upgrade by Metronet. The work included refurbishment of the tunnels, platforms and depot, full replacement of the track and signalling, and repainting and refurbishment of the trains. Four new battery-powered locomotives, named \"Walter\", \"Lou\", \"Anne\" and \"Kitty\", were built by Clayton Equipment in Derby to haul materials and plant along the line during the closure. These works were expected to boost rush-hour capacity by 25% and line capability by 12%. It was also claimed that the average journey would be up to 40seconds shorter.\nDuring the 2012 Summer Olympics and 2012 Summer Paralympics between late July and early September 2012, trains ran on Sundays to cope with the demand for travel in the City.\nIn the late 2010s, a new entrance at Bank station was constructed at Bloomberg's new London headquarters, providing direct access to the line via four new escalators and two lifts \u2013 providing step free access to the Waterloo and City line platforms. Although step free access is available at Bank, there is no step free access at Waterloo \u2013 and therefore the line does not have step free access.\nClosure during the COVID-19 pandemic.\nIn March 2020, following the UK government's implementation of lockdown restricting all non-essential travel due to the COVID-19 pandemic, the Waterloo &amp; City line was suspended. The service remained suspended for 15 months, due to the low level of travel demand on the line, and TfL prioritising the use of train operators on the busier Central line.\nTfL stated that they did not expect to reopen the line until demand increased, despite calls from business groups in August 2020 to reopen the line to serve returning office workers. By March 2021, TfL stated that they expected the line to return to operation in May or June 2021, observing that services could be restarted at short notice if required. In May 2021, it was announced that the line would reopen from 21 June 2021. The line reopened ahead of schedule on 4 June 2021, initially operating a peak hour only service on weekday mornings and evenings, before expanding to a full timetable on weekdays in November 2021. It was noted that Saturday services will not be reintroduced for the \"foreseeable future\", with TfL stating that ridership on a Saturday pre pandemic was around one-sixth of an average weekday. The line remains closed on Sundays, as previously.\nRolling stock.\nToward the end of the 1980s, the Class 487 rolling stock fleet built in the 1940s was increasingly unreliable. As part of the total route modernisation project by Network SouthEast (NSE), the decision was taken to purchase new vehicles as an addition to an order for new 1992 Stock trains by London Underground for the Central line. Five 4-car trains were ordered, albeit in Network SouthEast livery and no provision for automatic train operation.\nThe trains were constructed in 1992\u201393 and were initially tested and commissioned on the Central line, before being delivered by road to Waterloo depot. Unlike the Class 487 trains, the new trains required a fourth rail traction current system, with a central aluminium negative rail installed as part of the upgrade works to the line.\nOn 28 May 1993, all of the old rolling stock was withdrawn, the train service being suspended temporarily. A temporary bus service was run while the old rolling stock was physically removed and the new rolling stock brought in. The line reopened on 19 July 1993, with a peak service frequency of &lt;templatestyles src=\"Fraction/styles.css\" /&gt;3+1\u20442 minutes. In April 1994, the trains transferred to London Underground following the privatisation of British Rail. Despite this, the trains kept their NSE livery.\nIn 2006, the 1992 stock trains were overhauled, refurbished and repainted as part of the line upgrade by Metronet. As part of the work, seats were replaced, CCTV was installed, and the original Network SouthEast livery was replaced by the London Underground corporate livery. A 500 tonne crane was required to lift the trains in and out of Waterloo depot, to allow the trains to be transported to Wabtec in Doncaster for the refurbishment work.\nSince its introduction, the stock on the Waterloo &amp; City has diverged significantly from that used on the Central line through modifications, including the adoption of automatic train operation on the latter, that the two are no longer interchangeable; the Waterloo &amp; City line continues to use train stops.\nFuture rolling stock.\nIn the mid 2010s, TfL began a process of ordering new rolling stock to replace trains on the Piccadilly, Central, Bakerloo and Waterloo &amp; City lines. A feasibility study into the new trains showed that new generation trains and track remodelling at Waterloo could increase capacity on the line by 50%, with 30 trains per hour.\nIn June 2018, the Siemens Mobility Inspiro design was selected. These trains would have an open gangway design, wider doorways, air conditioning and the ability to run automatically with a new signalling system. TfL could only afford to order Piccadilly line trains at a cost of \u00a31.5bn. However, the contract with Siemens includes an option for 10 trains for the Waterloo &amp; City line in the future. This would take place after the delivery of the Piccadilly line trains in the 2030s.\nAs part of the financial bailout of TfL in the aftermath of the COVID-19 pandemic, the Department for Transport is working with TfL to create a business case for making the line driverless.\nHistoric rolling stock.\nAfter tendering, a contract for supply of the passenger vehicles was let to the Jackson and Sharp Company of Wilmington, Delaware in the sum of \u00a321,675. The vehicles were to be shipped to Southampton in knock-down kit form, to be assembled at Eastleigh Works by the LSWR.\nBy 6 January 1898, a skeleton carriage could be run through the tunnels to verify clearances and the first fully assembled train of four carriages was run from Eastleigh to Waterloo on 4 March 1898. The lift for lowering rolling stock to the tunnel level, and some electrical work, were not ready, but on 4 June 1898 a successful trial run was made.\nThe motor coaches were overall length, and the trailers were , both being wide at floor level and high from rail level. There were 11 of each type, to run in four four-car formations with spares.\nThe accommodation was of the open saloon type, then a novelty in Britain; there were gate entrances at the end of the vehicles. The trailers seated 56 persons, and the motor coaches seated 46, with a raised section over the motor bogie.\nThe traction motors by Siemens were series-wound gearless motors on the axles. The trains ran in a formation of four cars, the two outer vehicles being motor coaches. The motor cars were constructed to allow an early form of multiple unit operation and the front car's controller was additionally able to control the rear car's motors. The two motors at each end were connected in series at starting, then reconnected in parallel (using open circuit transition) as the train accelerated in the well established (at that time) method. This required eight cables to be run the length of the train at roof level. A further cable making nine in all linked the collector shoes at opposite ends of the four-car set to avoid problems with the large gaps in the centrally mounted conductor rail.\nThere was a crew of six at first: driver, driver's assistant, guard and three gatemen; the driver's assistant was subsequently no longer required. The trains used Westinghouse brakes, and the air reservoirs were charged from static compressors at Waterloo. They were charged to , running down to before needing to be recharged. Lighting was run from the power circuit, with four lamps in series from the nominal.\nNew trains ordered by the Southern Railway.\nIn the late 1930s, new rolling stock was ordered by the Southern Railway. Despite the declaration of war in September 1939, the work was considered well advanced, and 12 motor coaches and 16 trailers were ordered from English Electric, and built at the Dick, Kerr &amp; Co. works at Preston. The Art Deco style trains were delivered through 1940, and the old cars were removed from the line on 25 October 1940, the new cars starting work on 28 October, with the line closed over the intervening weekend.\nConstructed of welded steel, trains were run in five-car formations, Motor coach + trailer + trailer + trailer + motor coach, with spares for overhaul. The motor coaches had cabs at each end, enabling single-car operation by them; they had two axle-hung traction motors rated at for one hour. The new trains had on-board compressors for the air brakes, and interior lights were in two circuits, one fed from the motor car at one end of the unit, and one from the other, avoiding total lighting loss in passing conductor rail gaps. The conductor rail was altered to the outside position normal for the third-rail system. There was no train power line, and each motor coach collected its own electric supply.\nExtension proposals.\nThere have been proposals to extend the Waterloo &amp; City line for over a century. After acquiring the Great Northern &amp; City Railway (GN&amp;C) in 1913 (the current Northern City Line), the Metropolitan Railway considered proposals to join the GN&amp;C to the Waterloo &amp; City or to the Circle line, but these never came to fruition. Any extension of the line north would be difficult because of the complex web of tube lines around Bank, and an extension south would be unlikely to provide demand that matched the cost. The narrow tunnels and short train lengths of the current route make any extension less cost-effective than larger projects such as Crossrail 2, which cost more but start with modern tunnels and promise far greater benefits.\nThe London Plan Working Party Report of 1949 envisaged as its Route G the electrification of the London, Tilbury &amp; Southend Railway (LTS), and its diversion away from Fenchurch Street to Bank and on through the Waterloo &amp; City tunnels to Waterloo and its suburban lines. The Waterloo &amp; City tunnels would have had to be bored out to main line size to enable this, at prohibitive cost. In the event, only the electrification of the LTS took place, though the Docklands Light Railway tunnel from Minories to the Bank follows part of the envisaged route.\nThe revised Working Party Report of 1965 did not mention the Route G proposal, though it does conclude that \"[t]he possibility of extending the Waterloo &amp; City line northwards to Liverpool Street has been examined, but found to be physically impracticable.\" \nAround 2009, the Green Party revived the Metropolitan's plan of connecting the Northern City and Waterloo &amp; City lines as a Crossrail route.\nMap and stations.\n&lt;templatestyles src=\"Routemap/styles.css\"/&gt;\nIn popular culture.\nThe Waterloo and City line is closed on weekends; as a result, it has become a well-established and convenient location for film companies. When it was owned by British Rail and its predecessors, it could be used when London Transport were unable or unwilling to allow access to their own stations or lines.\nOther details.\nThe remnants of one of the Greathead tunnelling shields used in the construction of the line can be seen in the interchange tunnel at Bank connecting the Waterloo and City line platforms with those of the Northern line and the Docklands Light Railway. It is painted red.\nThe Waterloo &amp; City line is colloquially known as \"The Drain\", and it was given the Engineer's Line Reference code 'DRN' by British Rail. The origins of this nickname seem to be uncertain; it may be due to the tunnels beneath the Thames continually leaking and the resulting water needing to be pumped out, or perhaps because passenger access to the platforms at Bank was by a lengthy sloping subway resembling a drain.\nUniquely among London's Underground lines, virtually all infrastructure on the Waterloo &amp; City line is completely underground, including all track, both stations, and the maintenance depot at Waterloo. (The Victoria line is also underground for the entire passenger route and all stations, but has a surface level, open-air depot for maintenance.) There are no track connections with any other railway line; all equipment transfers to and from the line are accomplished from the shaft and road crane at the Waterloo depot.\nMaps.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "66427", "revid": "36602871", "url": "https://en.wikipedia.org/wiki?curid=66427", "title": "Evel Knievel", "text": "American stunt performer (1938\u20132007)\nRobert Craig Knievel (October 17, 1938\u00a0\u2013\u00a0November 30, 2007), known professionally as Evel Knievel ( ), was an American stunt performer and entertainer. Throughout his career, he attempted more than 75 ramp-to-ramp motorcycle jumps. Knievel was inducted into the Motorcycle Hall of Fame in 1999.\nEvel Knievel was born in Butte, Montana. Raised by his paternal grandparents, Knievel was inspired to become a motorcycle daredevil after attending a Joie Chitwood auto daredevil show. He left high school early to work in the copper mines but was later fired for causing a city-wide power outage. After adopting the nickname \"Evel Knievel\", he participated in rodeos and ski jumping events, and served in the U.S. Army before marrying Linda Joan Bork and starting a semi-pro hockey team. To support his family, Knievel started the Sur-Kill Guide Service and later worked as an insurance salesman. Eventually, he opened a Honda motorcycle dealership in Washington, but faced difficulties promoting Japanese imports. After the dealership closed, Knievel worked at a motorcycle shop where he learned motocross stunts that would later contribute to his daredevil career.\nKnievel's most famous stunt was an attempt to jump the fountains at Caesars Palace, which resulted in severe injuries. Knievel became a legendary figure, breaking numerous records and bones throughout his career.\nOn September 8, 1974, Knievel attempted to jump across the Snake River Canyon in Idaho using a rocket-powered cycle called the Skycycle X-2. The jump failed after the parachute deployed prematurely, but Knievel survived with minor injuries.\nKnievel sought to profit from his image through endorsements and marketing deals. American Eagle Motorcycles signed him, and his popularity grew with young boys. From 1972 to 1977, Ideal Toy Company sold over $125 million worth of Knievel toys. Knievel's fame led to TV appearances and partnerships with companies like AMF and Harley-Davidson. However, after an assault conviction and jail time, he lost endorsements and declared bankruptcy. Despite a decline in his daredevil career, Knievel made a marketing comeback in the 1990s and continued to be involved in various ventures.\nKnievel died on November 30, 2007, at the age of 69 due to diabetes and idiopathic pulmonary fibrosis. He was buried in his hometown of Butte, Montana. Posthumously, Knievel has been honored through various exhibits, a museum, and tribute jumps. His legacy also lives on in television commercials featuring his iconic stunts.\nEarly life.\nKnievel was born on October 17, 1938, in Butte, Montana, the first of two children of Robert E. and Ann Marie Keough Knievel. His surname is of German origin; his paternal great-great-grandparents immigrated to the United States from Germany. His mother was of Irish ancestry. Robert and Ann divorced in 1940, after the 1939 birth of their second child, Nicolas, known as Nic. Both parents decided to leave Butte.\nKnievel and his brother were raised in Butte by their paternal grandparents, Ignatius and Emma Knievel. At the age of eight, Knievel attended a Joie Chitwood auto daredevil show, which he credited for his later career choice as a motorcycle daredevil.\nKnievel was a cousin of Democratic U.S. Representative from Montana, Pat Williams (1937\u00a0\u2013\u00a02025).\nKnievel left Butte High School after his sophomore year. He got a job in the copper mines as a diamond drill operator with the Anaconda Mining Company; however, Knievel preferred motorbiking to what he called \"unimportant stuff\". He was promoted to surface duty, where he drove a large earth mover. Knievel was fired when he made the earth mover do a motorcycle-type wheelie and accidentally drove it into Butte's main power line, leaving the city without electricity for several hours.\nKnievel's website says that he chose his nickname after spending a night in jail in 1956 after being arrested for reckless driving. In the same jail that night was a man named William Knofel, who had the nickname \u201cAwful Knofel\u201d; this led to Knievel being referred to as \u201cEvel Knievel\u201d.\nSeeking new thrills and challenges, Knievel participated in local professional rodeos and ski jumping events, including winning the Northern Rocky Mountain Ski Association Class A Men's ski jumping championship in 1959. During the late 1950s, Knievel joined the United States Army. His athletic ability allowed him to join the track team, where he was a pole vaulter. After his army stint, Knievel returned to Butte, where he met and married his first wife, Linda Joan Bork. Shortly after getting married, Knievel started the Butte Bombers, a semi-pro hockey team.\nTo help promote his team and earn some money, he convinced the Czechoslovak Olympic ice hockey team to play the Butte Bombers in a warm-up game to the 1960 Winter Olympics (to be held in California). Knievel was ejected from the game minutes into the third period and left the stadium. When the Czechoslovak officials went to the box office to collect the expense money that the team was promised, workers discovered the game receipts had been stolen. The United States Olympic Committee ended up paying the Czechoslovak team's expenses to avoid an international incident. Knievel tried out with the Charlotte Clippers of the Eastern Hockey League in 1959, but decided that a traveling team was not for him.\nAfter the birth of his first son, Kelly, Knievel realized that he needed to come up with a new way to support his family financially. Using the hunting and fishing skills taught to him by his grandfather, Knievel started the Sur-Kill Guide Service. He guaranteed that if a hunter employed his service and paid his fee, he would get the big game animal desired or Knievel would refund his fee.\nKnievel, after learning about the culling of elk in Yellowstone, decided to hitchhike from Butte to Washington, D.C., in December 1961 to raise awareness and to have the elk relocated to areas where hunting was permitted. After this conspicuous trek (he hitchhiked with a rack of elk antlers and a petition with 3,000 signatures), he presented his case to Representative Arnold Olsen, Senator Mike Mansfield, and Interior Secretary Stewart Udall. Culling was stopped in the late 1960s.\nAfter returning home to the west from Washington, D.C., he joined the motocross circuit and had moderate success, but he still could not make enough money to support his family. In 1962, Knievel broke his collarbone and shoulder in a motocross accident. The doctors said he could not race for at least six months. Still needing to help support his family, he again switched careers and sold insurance for the Combined Insurance Company of America, working for W. Clement Stone. Stone suggested that Knievel read \"Success Through a Positive Mental Attitude\", a book that Stone wrote with Napoleon Hill. Knievel credited much of his later success to Stone and his book.\nKnievel was successful as an insurance salesman, but felt that his efforts were being unrecognized. When the company refused to promote him to vice president after he had been a few months on the job, he quit. Wanting a new start away from Butte, Knievel moved his family to Moses Lake, Washington. There, he opened a Honda motorcycle dealership and promoted motocross racing. During the early 1960s, he and other dealers had difficulty promoting and selling Japanese imports because of the steep competition of their auto industry, and the Moses Lake Honda dealership eventually closed. After the closure, Knievel went to work for Don Pomeroy at his motorcycle shop in Sunnyside, Washington. Pomeroy's son, Jim Pomeroy, who went on to compete in the Motocross World Championship, taught Knievel how to do a \"wheelie\" and ride while standing on the seat of the bike.\nCareer.\nStunt performance.\nAs a boy, Knievel had seen the Joie Chitwood show. He decided that he could do something similar using a motorcycle. Promoting the show himself, Knievel rented the venue, wrote the press releases, set up the show, sold the tickets, and served as his own master of ceremonies. After enticing the small crowd with a few wheelies, he proceeded to jump a box of rattlesnakes and two mountain lions. Despite landing short and his back wheel hitting the box containing the rattlesnakes, Knievel managed to land safely.\nKnievel realized that to make a more substantial amount of money he would need to hire more performers, stunt coordinators, and other personnel so that he could concentrate on the jumps. With little money, he went looking for a sponsor and found one in Bob Blair, owner of ZDS Motors, Inc., the West Coast distributor for Berliner Motor Corporation, a distributor for Norton Motorcycles. Blair offered to provide the needed motorcycles, but he wanted the name changed from \"Bobby Knievel and His Motorcycle Daredevils Thrill Show\" to \"Evil Knievel and His Motorcycle Daredevils\". Knievel did not want his image to be that of a Hells Angels rider, so he convinced Blair to at least allow him to use the spelling \"Evel\" instead of \"Evil\".\nKnievel and his daredevils debuted on January 3, 1966, at the National Date Festival in Indio, California. The second booking was in Hemet, California, but was canceled due to rain. The next performance was on February 10, in Barstow, California. During the performance, Knievel attempted a new stunt in which he would jump, spread-eagled, over a speeding motorcycle. Knievel jumped too late and the motorcycle hit him in the groin, tossing him into the air. He was hospitalized as a result of his injuries. When released, he returned to Barstow to finish the performance he had started almost a month earlier.\nKnievel's daredevil show broke up after the Barstow performance because injuries prevented him from performing. After recovering, Knievel started traveling from small town to small town as a solo act. To get ahead of other motorcycle stunt people who were jumping animals or pools of water, Knievel started jumping cars. He began adding more and more cars to his jumps when he would return to the same venue to get people to come out and see him again. Knievel had not had a serious injury since the Barstow performance, but on June 19 in Missoula, Montana, he attempted to jump twelve cars and a cargo van. The distance he had for takeoff did not allow him to get up enough speed. His back wheel hit the top of the van while his front wheel hit the top of the landing ramp. Knievel ended up with a severely broken arm and several broken ribs. The crash and subsequent stay in the hospital were a publicity windfall.\nWith each successful jump, the public wanted him to jump one more car. On March 25, 1967, Knievel cleared 15 cars at Ascot Park in Gardena, California. Then he attempted the same jump on July 28, 1967, in Graham, Washington, where he had his next serious crash. Landing his cycle on the last vehicle, a panel truck, Knievel was thrown from his bike. This time he suffered a serious concussion. After a month, he recovered and returned to Graham on August 18 to finish the show; but the result was the same, only this time the injuries were more serious. Again coming up short, Knievel crashed, breaking his left wrist, right knee, and two ribs.\nKnievel first received national exposure on March 18, 1968, when comedian and late-night talk show host Joey Bishop had him on as a guest of ABC's \"The Joey Bishop Show\".\nCaesars Palace.\nWhile in Las Vegas to watch Dick Tiger successfully defend his World Boxing Association (WBA) and World Boxing Council (WBC) light heavyweight titles at the Convention Center on November 17, 1967, Knievel first saw the fountains at Caesars Palace and decided to jump them.\nTo get an audience with casino CEO Jay Sarno, Knievel created a fictitious corporation called Evel Knievel Enterprises and three fictitious lawyers to make phone calls to Sarno. Knievel also placed phone calls to Sarno claiming to be from American Broadcasting Company (ABC) and \"Sports Illustrated\" inquiring about the jump. Sarno finally agreed to meet Knievel and arranged for Knievel to jump the fountains on December 31, 1967, New Year's Eve. After the deal was set, Knievel tried to get ABC to air the event live on their popular \"Wide World of Sports\". ABC declined but said that if Knievel had the jump filmed and it was as spectacular as he said it would be, they would consider using it later.\nKnievel, at the age of 29, used his own money to have actor/director John Derek produce a film of the Caesars jump. To keep costs low, Derek employed his then-wife Linda Evans as one of the camera operators. It was Evans who filmed the famous landing. On the morning of the jump, Knievel stopped in the casino and placed his last $100 on the blackjack table (which he lost), stopped by the bar, and had a shot of Wild Turkey, and then headed outside where he was joined by several members of the Caesars staff, as well as two showgirls.\nAfter doing his normal pre-jump show and a few warm-up approaches, Knievel began his real approach. When he hit the takeoff ramp, he said later, he felt the motorcycle unexpectedly decelerate. The sudden loss of power on the takeoff caused Knievel to come up short and land on the safety ramp which was supported by a van. This caused the handlebars to be ripped out of his hands as he tumbled over them onto the pavement where he skidded into the Dunes hotel parking lot.\nAs a result of the crash, Knievel suffered a crushed pelvis and femur, fractures to his hip, wrist, and both ankles, and a concussion that kept him in the hospital. Rumors circulated that he was in a coma for 29 days in the hospital, but this was refuted by his wife and others in the documentary film \"Being Evel\".\nThe Caesars Palace crash was Knievel's longest attempted motorcycle jump at 144\u00a0ft 6 in. After his crash and recovery, Knievel was more famous than ever. ABC declined to air the event live on \"Wide World of Sports\". The Caesars Palace historical jump video is now owned by K and K Promotions, Inc which is the successor in interest and owner of all Evel Knievel trademarks, film footage, and copyrights.\nInsurance.\nIn a 1971 interview with Dick Cavett, Knievel stated that he was uninsurable following the Caesars' crash, stating, \"I have trouble getting life insurance, accident insurance, hospitalization and even insurance for my automobile... Lloyd's of London has rejected me 37 times so if you hear the rumor that they insure anybody, don't pay too much attention to it.\" Four years later, a clause in Knievel's contract to jump 14 buses at Kings Island required a one-day $1million liability insurance to the amusement park. Lloyd's of London offered liability insurance for $17,500. Knievel eventually paid $2,500 to a U.S.-based insurance company.\nJumps and records.\nTo keep his name in the news, Knievel proposed his biggest stunt ever, a motorcycle jump across the Grand Canyon. Just five months after his near-fatal crash in Las Vegas, Knievel performed another jump. On May 25, 1968, in Scottsdale, Arizona, Knievel crashed while attempting to jump 15 Ford Mustangs. Knievel ended up breaking his right leg and foot as a result of the crash.\nOn August 3, 1968, Knievel returned to jumping, making more money than ever before. He was earning approximately $25,000 per performance, and he was making successful jumps almost weekly until October 13, in Carson City, Nevada. While trying to stick the landing, he lost control of the bike and crashed, breaking his hip again.\nBy 1971, Knievel realized that the U.S. government would never allow him to jump the Grand Canyon. To keep his fans interested, Knievel considered several other stunts that might match the publicity that would have been generated by jumping the canyon. Ideas included jumping across the Mississippi River, jumping from one skyscraper to another in New York City, and jumping over 13 cars inside the Houston Astrodome. While flying back to Butte from a performance tour, he looked out the window of his airplane and saw the Snake River Canyon. After finding a location just east of Twin Falls, Idaho, that was wide enough, deep enough, and on private property, he leased for $35,000 to stage his jump. He set the date for Labor Day (September 4), 1972.\nOn January 7\u20138, 1971, Knievel set a sales record at the Houston Astrodome by selling over 100,000 tickets to back-to-back performances there. On February 28, he set a new world record by jumping 19 cars with his Harley-Davidson XR-750 at the Ontario Motor Speedway in Ontario, California. The 19-car jump was shot for the biopic \"Evel Knievel.\" Knievel held the record for 27 years until Bubba Blackwell jumped 20 cars in 1998 with an XR-750. In 2015, Doug Danger surpassed that number with 22 cars, accomplishing this feat on Evel Knievel's actual vintage 1972 Harley-Davidson XR-750.\nOn May 10, 1970, Knievel crashed while attempting to jump 13 Pepsi delivery trucks in Yakima, WA. His approach was complicated by the fact that he had to start on pavement, cut across grass, and then return to pavement. His lack of speed caused the motorcycle to come down on its front wheel first. He managed to hold on until the cycle hit the base of the ramp. After being thrown off, he skidded for . He broke his collarbone, suffered a compound fracture of his right arm, and broke both legs.\nOn March 3, 1972, at the Cow Palace in Daly City, California, after making a successful jump, he tried to come to a quick stop because of a short landing area. He reportedly suffered a broken back and a concussion after getting thrown off and run over by his motorcycle, a Harley-Davidson. Knievel returned to jumping in November 1973, when he successfully jumped over 50 stacked cars at the Los Angeles Memorial Coliseum. For 35 years, Knievel held the record for jumping the most stacked cars on a Harley-Davidson XR-750 (the record was broken in October 2008). His XR-750 is now part of the collection of the Smithsonian's National Museum of American History. Made of steel, aluminum, and fiberglass, the customized motorcycle weighs about .\nDuring his career, Knievel may have suffered more than 433 bone fractures, earning an entry in the \"Guinness Book of World Records\" as the survivor of \"most bones broken in a lifetime\". However, this number could be exaggerated: his son Robbie told a reporter in June 2014 that his father had broken 40 to 50 bones; Knievel himself claimed he broke 35.\nThe Grand Canyon jump.\nAlthough Knievel never attempted to jump the Grand Canyon, rumors of the Canyon jump were started by Knievel himself in 1968, following the Caesars Palace crash. During a 1968 interview, Knievel stated, \"I don't care if they say, 'Look, kid, you're going to drive that thing off the edge of the Canyon and die,' I'm going to do it. I want to be the first. If they'd let me go to the moon, I'd crawl all the way to Cape Kennedy just to do it. I'd like to go to the moon, but I don't want to be the second man to go there.\" For the next several years, Knievel negotiated with the federal government to secure a jumping site and develop various concept bikes to make the jump, but the Interior Department denied him airspace over the northern Arizona canyon. Knievel switched his attention in 1971 to the Snake River Canyon in southern Idaho.\nIn the 1971 film \"Evel Knievel\", George Hamilton (as Knievel) alludes to the canyon jump in the final scene of the movie. One of the common movie posters for the film depicts Knievel jumping his motorcycle off a (likely) Grand Canyon cliff. In 1999, his son Robbie jumped a portion of the Grand Canyon owned by the Hualapai Indian Reservation.\nSnake River Canyon jump.\n\"ABC's Wide World of Sports\" was unwilling to pay the price Knievel wanted for the Snake River Canyon jump, so he hired boxing promoter Bob Arum's company, Top Rank Productions, to put the event on closed-circuit television and broadcast to movie Investors in the event took a substantial loss, including promoter DonE. Branker, as well as Vince McMahon of what was then called the World Wide Wrestling Federation. Arum partnered with Invest West Sports, Shelly Saltman's company, to secure from Invest West Sports two things: first, the necessary financing for the jump, and second, the services of Saltman, long recognized as one of America's premier public relations and promotion men, to do publicity so that Knievel could concentrate on his jumps. Knievel hired aeronautical engineer Doug Malewicki to build him a rocket-powered cycle to jump across the Snake River, and called it the Skycycle X-1. Malewicki's creation was powered by a steam engine built by former Aerojet engineer Robert Truax. On April 15, 1972, the X-1 was launched to test the feasibility of the launching ramp. The decision was then made to have Truax build two Skycycle X-2s, one to test and one for the actual jump. Both the X-1 and the X-2 test vehicles went into the river.\nThe launch took place at the south rim of the Snake River Canyon, west of Shoshone Falls, on September 8, 1974, at 3:36\u00a0p.m. MDT. The steam that powered the engine was superheated to a temperature of . The drogue parachute prematurely deployed as the Skycycle left the launching rail and induced significant drag. Even though the craft made it across the canyon to the north rim, the prevailing northwest winds caused it to drift back into the canyon. By the time it hit the bottom of the canyon, it landed only a few feet from the water on the same side of the canyon from which it had been launched. If he had landed in the water, Knievel said that he would have drowned, due to a harness malfunction that kept him strapped in the vehicle. He survived the failed jump with only minor physical injuries.\nSince the 1974 launch, seven daredevils have expressed interest in recreating the jump, including Knievel's two sons, Robbie and Kelly. In 2010, Robbie announced he would recreate the jump. Stuntman Eddie Braun announced he was working with Kelly and Robert Truax's son to recreate the jump using a replica of the Skycycle X-2. Braun's jump took place on September 16, 2016, and was completed successfully.\nWembley jump.\nAfter the Snake River jump, Knievel returned to motorcycle jumping with \"ABC's Wide World of Sports\" televising several jumps. On May 26, 1975, in front of 90,000 people at Wembley Stadium in London, Knievel crashed while trying to land a jump over 13 redundant single-deck AEC Merlin buses (the term \"London Buses\" used in earlier publicity had led to the belief that the attempt was to be made over the higher and more traditional AEC Routemaster double-decker type).\nAfter the crash, despite breaking his pelvis, Knievel addressed the audience and announced his retirement by stating, \"Ladies and gentlemen of this wonderful country, I've got to tell you that you are the last people in the world who will ever see me jump. Because I will never, ever, ever jump again. I'm through.\" Near shock and ignoring Frank Gifford's (of \"ABC's Wide World of Sports\") plea to use a stretcher, Knievel walked off the Wembley pitch stating, \"I came in walking, I went out walking!\"\nKings Island jump.\nAfter recuperating, Knievel decided that he had spoken too soon and that he would continue jumping. On October 25, 1975, Knievel jumped 14 Greyhound buses at Kings Island near Cincinnati, Ohio. Although Knievel landed on the safety deck above the 14th bus, his landing was successful and he held the record for jumping the most buses on a Harley-Davidson for 24 years (until broken by Bubba Blackwell in late 1999 with 15 at ). The Kings Island event scored the highest viewer ratings in the history of \"ABC's Wide World of Sports\" and would serve as Knievel's longest successful jump at (although the Caesars Palace jump was longer, it ended in a crash). In the end, Knievel was featured in seven of the ten highest-rated episodes of \"ABC's Wide World of Sports\". After the Kings Island jump, Knievel again announced his retirement.\nHis retirement was once again short-lived, and Knievel continued to jump. However, after the lengthy Kings Island jump, Knievel limited the remainder of his career jumps to shorter and more attainable lengths. Knievel jumped on October 31, 1976, at the Seattle Kingdome. He jumped only seven Greyhound buses but it was a success. Despite the crowd's pleasure, Knievel felt that it was not his best jump, and apologized to the crowd.\nShark jump.\nOn January 31, 1977, Knievel was scheduled for a major jump in Chicago, Illinois. The jump was inspired by the 1975 film \"Jaws\". Knievel was scheduled to jump a tank full of live sharks which would be televised live nationally. However, during his rehearsal, Knievel lost control of the motorcycle and crashed into a cameraman. Although Knievel broke his arms, he was more distraught over what he claimed was a permanent eye injury to cameraman Thomas Geren. The cameraman was admitted to the hospital and received treatment for an injury near his eye, but received no permanent injury. The footage of this crash was so upsetting to Knievel that he did not show the clip for 19 years until the documentary \"Absolute Evel: The Evel Knievel Story\".\nLater that year on the sitcom \"Happy Days\", motorcycle-riding character Fonzie (Henry Winkler) performed a similar trick, albeit on waterskis, inspiring the creation of the phrase \"jump the shark.\"\nAfterward, Knievel retired from major performances and limited his appearances to smaller venues to help launch Robbie's career. His last stunt show, not including a jump, took place in March 1980 in Puerto Rico. However, Knievel would officially finish his career as a daredevil as a touring \"companion\" of Robbie's, limiting his performance to speaking only, rather than stunt riding. His final tour appearance with Robbie was in March 1981 in Hollywood, Florida.\nFeature movies: \"Evel Knievel\" and \"Viva Knievel!\".\nA 1971 biopic film, \"Evel Knievel\", fictionalized Knievel's life and exploits. Knievel, portrayed by George Hamilton, calls himself \"the last gladiator in the new Rome\"; this was a nod to a January 1970 Esquire magazine article about the stunt rider, whose author, David Lyle, declared, \"Evel Knievel [...] may be the last great gladiator.\" (Later, Knievel titled his 1988 self-produced documentary \"Last of the Gladiators\".) A higher end B-movie, \"Evel Knievel\" was a minor hit, taking in $4 million in rentals (equivalent to approximately $ in 2024) against a $450,000 budget.\nKnievel played himself in the 1977 American action film \"Viva Knievel!\", directed by Gordon Douglas and co-starring Gene Kelly and Lauren Hutton, with an ensemble supporting cast including Red Buttons, Leslie Nielsen, Cameron Mitchell, Frank Gifford, Dabney Coleman and Marjoe Gortner. The film premiered in June 1977, three months later Knievel and his associates attacked promoter Shelly Saltman with an aluminum baseball bat on September 21, 1977.\nWith Knievel losing most of his sponsorship and marketing deals as a result of the bad publicity, \"Viva Knievel\" became much less commercially attractive, only opening in four further international markets after Knievel's conviction. In addition, the wholesome image of Knievel the movie promoted and the plot point concerning Knievel's promoter being corrupt seemed ill-judged in the light of the events that saw Knievel imprisoned.\nMotorcycles.\nKnievel briefly used a Honda 250cc motorcycle to jump a crate of rattlesnakes and two mountain lions, his first known jump. Knievel then used a Norton Motorcycle Company 750cc for only one year, 1966. Between 1967 and 1968, Knievel jumped using the Triumph Bonneville T120 (with a 650cc engine). Knievel used the Triumph at the Caesars Palace crash on New Year's Eve 1967. When Knievel returned to jumping after the crash, he used Triumph for the remainder of 1968.\nAttempting his jumps on motorcycles whose suspensions were designed primarily for street riding or flat track racing was a major factor in Knievel's many disastrous landings. The terrific forces these machines passed on to his body are well illustrated in the super slow-motion footage of his Caesars' landing.\nBetween December 1969 and April 1970, Knievel used the Laverda American Eagle 750cc motorcycle. On December 12, 1970, Knievel would switch to the Harley-Davidson XR-750, the motorcycle with which he is best known for jumping. Knievel would use the XR-750 in association with Harley-Davidson until 1977. However, after his 1977 conviction for the assault of Shelly Saltman, Harley-Davidson withdrew its sponsorship of Knievel.\nOn September 8, 1974, Knievel attempted to jump the Snake River Canyon on a rocket-propelled motorcycle designed by former NASA engineer Robert Truax, dubbed the Skycycle X-2. The State of Idaho registered the X-2 as an airplane rather than a motorcycle.\nAt the tail end of his career, while helping launch the career of his son, Robbie, Knievel returned to the Triumph T120. However, he used the bike only for wheelies and did not jump after retiring from the XR-750.\nIn 1997, Knievel signed with the California Motorcycle Company to release a limited \"Evel Knievel Motorcycle.\" The motorcycle was not built to jump but was rather a V-twin cruiser motorcycle intended to compete with Harley-Davidson street bikes. Knievel promoted the motorcycle at his various public appearances. After the company closed in 2003, Knievel returned to riding modern street Harley-Davidson motorcycles at his public appearances.\nRobbie sold limited-edition motorcycles from his company, Knievel Motorcycles Manufacturing Inc. Although two of the motorcycles refer to Evel (the Legend Series Evel Commemorative and the Snake River Canyon motorcycle), Evel did not ride Robbie's bikes.\nLeather jumpsuits.\nThroughout his daredevil career, Knievel was known for his leather jumpsuits that were compared to the jumpsuits worn by Elvis Presley. When Knievel began jumping, he used a black and yellow jumpsuit. When he switched to the Triumph motorcycle, his jumpsuit changed to a white suit with stripes down the legs and sleeves. In interviews, he said the reason for the switch was because he saw how Liberace had become not just a performer, but the epitome of what a showman should be, and Knievel sought to create his variation of that showmanship in his jumps. Two variations of the white suit appeared (one with three stars across the chest and one with the three stars on his right chest). The latter was worn at the Caesars Palace jump.\nWhen Knievel switched to the Laverda motorcycle in 1969, he switched his leathers to an All American Themed red-white-and-blue jumpsuit with an \"X\" across the chest. Later, Knievel adjusted the blue stripes to a V-shape (the first version of the V-shape was also used in the 1971 film's final jump). For the remainder of his career, variants of the V-shaped white-starred jumpsuit would be a constant, including a special nylon/canvas flight suit that matched his white leathers for the X-2 jump. Each variant would become more elaborate, including the addition of the red-white-blue cape and the Elvis-styled belt buckle with his initials, \"EK\". In 1975, Knievel premiered the blue leathers with red stars on the white stripes for the Wembley jump.\nCore values.\nEvel Knievel took great pride in his core values. Throughout his career and later life, he would repeatedly talk about the importance of \"keeping his word\". He stated that, although he knew he might not successfully make a jump or even survive the canyon jump, he followed through with each stunt because he gave his word that he would. Before the canyon jump, Knievel stated, \"If someone says to you, 'that guy should have never jumped the canyon. You knew if he did, that he'd lose his life and that he was crazy.' Do me a favor. Tell him that you saw me here and regardless of what I was, that you knew me, and that I kept my word.\"\nIn \"Last of the Gladiators\", Knievel discussed the crash of a 1970 Pepsi-Cola sponsored jump in Yakima, Washington. Knievel knew the jump was very questionable, but stated, \"I went ahead and did it anyway. When you give your word to somebody that you're going to do something, you've gotta do it.\" In the 1971 biopic, George Hamilton (as Knievel) emphasizes in the opening monologue that a man does not go back on his word.\nAnti-drug campaign.\nKnievel would regularly share his anti-drug message, one of his core values. Knievel would preach an anti-drug message to children and adults before each of his stunts.\nKnievel regularly spoke out against the Hells Angels due to their alleged involvement in the drug trade. A near-riot erupted during Knievel's show at the Cow Palace on March 3, 1971, when a Hells Angels member threw a metal object (either a tire iron or a Coca-Cola can, according to different witnesses) at Knievel. Knievel and a majority of the spectators fought back, injuring three of the fifteen Hells Angels members in attendance to the point that they required hospitalization.\nIn the film \"Viva Knievel!\", Knievel plays a fictionalized version of himself who foils a drug lord's attempt to smuggle narcotics into the United States.\nMotorcycle helmet safety.\nKnievel was a proponent of motorcycle helmet safety. He constantly encouraged his fans to wear motorcycle helmets. The Bell Star helmet he used in the Caesars Palace jump is credited for having saved Knievel's life after he fell off the motorcycle and struck his head on the ground. (Following the Caesars Palace crash, each of Knievel's full-face helmets bore the slogan, \"Color Me Lucky.\") Knievel once offered a cash reward for anyone who witnessed him stunting on a motorcycle without a helmet.\nIn 1987, Knievel supported a mandatory helmet bill in the State of California. During the Assembly Transportation Committee meeting, Knievel was introduced as \"the best walking commercial for a helmet law.\" Evel claimed the main reason he was still alive and walking was that he wore a helmet.\nMarketing image.\nKnievel sought to make more money from his image. He was no longer satisfied with just receiving free motorcycles to jump with. Knievel wanted to be paid to use and promote a company's brand of motorcycles. After Triumph, the British motorcycle brand he had been jumping with, refused to meet his demands (it was part of the bankrupt BSA group that was merged with Norton in 1972), Knievel started to propose the idea to other manufacturers. \"American Eagle Motorcycles\", the brand under which Italian Laverda machines were sold in the US, was the first company to sign Knievel to an endorsement deal. Knievel then used the new lightweight racing motorcycle Harley-Davidson XR-750 from December 1970 until his final jump in January 1977.\nAt approximately the same time, Fanfare Films started production on the George Hamilton biopic (Evel Knievel (1971 film)). Two other films about Knievel, a television pilot made in 1974 starring Sam Elliott, and a made-for-TV film in 2004 starring George Eads, were produced in later years. In 1974, Knievel and Amherst Records released at the Sound City Studios the self-titled album \"Evel Knievel\", which included a press conference, an anti-drug talk for his young fans, and four other tracks.\nIn 1972, Knievel appeared in the motorcycle safety film 'Not So Easy', together with \"Easy Rider\" Peter Fonda.\nKnievel kept up his pursuit of the United States government to allow him to jump the Grand Canyon. To push his case, he hired famed San Francisco defense attorney Melvin Belli to fight the legal battle for obtaining government permission. \"ABC's Wide World of Sports\" started showing Knievel's jumps on television regularly. His popularity, especially with young boys, was ever-increasing. He became a hero to a generation of young boys. A.\u00a0J. Foyt made Knievel part of his pit crew for the Indianapolis 500 in 1970. Evel Knievel's huge fame caused him to start traveling with bodyguards, who became life-long friends.\nIdeal Toys.\nBetween 1972 and 1977, Ideal Toy Company released a series of Evel Knievel-related merchandise, designed initially by Joseph M. Burck of Marvin Glass and Associates. During the six years the toys were manufactured, Ideal claimed to have sold more than $125million worth of Knievel toys. The toys included the original 1972 figures, which offered various outfits and accessories. In 1973, Ideal released the Evel Knievel Stunt Cycle. After the release of the Stunt Cycle, the Knievel toys were the best-selling item for Ideal.\nDuring the next four years, Ideal Toys released various models relating to Knievel's touring stunt show. The models included a Robbie Knievel doll, the Scramble Van, The Canyon Sky Cycle, a Dragster, a Stunt Car, and the Evel Knievel The Stunt World. Additionally, Ideal released non-Knievel-touring toys, including a Chopper Motorcycle, a Trail Bike, and a female counterpart, Derry Daring. The last item marketed by Ideal Toys before it discontinued the distribution of Knievel toys was the Strato-Cycle, based on the film \"Viva Knievel!\"\nIn 1977, Bally marketed its Knievel pinball machine as the \"first fully electronic commercial game\"; it has elsewhere been described as one of the \"last of the classic pre-digital games.\" (Both electromechanical and solid-state versions were produced. The electromechanical version is extremely rare, with only 155 made.)\nOther television appearances.\nIn the 1970s, Knievel partnered with AMF to release a series of bicycles, marketed with TV ads.\nThough Knievel had no involvement, a 30-minute ABC Saturday morning animated series \"Devlin\" produced by Hanna-Barbera aired in the fall of 1974. The series, inspired by his popularity, featured stunt motorcyclists.\nKnievel made several television appearances, including frequently as a guest on talk shows such as \"Dinah!\" and Johnny Carson's \"Tonight Show\". In 1977, he made a guest spot on \"The Bionic Woman\", where he played himself, getting inadvertently caught up in East German espionage while appearing in West Germany. Actual footage from Evel's L.A. Coliseum jump over crushed cars was used at the beginning of the episode, and an indoor jump over eleven cars and one van was used at the end of the show. Also in 1977, Warner Bros. released \"Viva Knievel!\" This movie starred Knievel as himself and co-starred Gene Kelly, Lauren Hutton, and Red Buttons. Similar to \"The Bionic Woman\", actual Wembley footage was used in the film. In addition, the 1999 children's TV series \"Hilltop Hospital\" featured a character based on Knievel called Weasel Kneasel, who was the focus of an episode of the same name. In Disney/Pixar's \"Toy Story 4\", a character named Duke Caboom (voiced by Keanu Reeves) was partially based on the Evel Knievel toy. On September 23, 2020, Knievel's son Kelly and K&amp;K Promotions filed a lawsuit against Disney and Pixar, claiming Duke was created illegally using Knievel's likeness.\nAssault conviction, jail, and bankruptcy.\nWhile Knievel was healing from injuries sustained from the Chicago jump, the book \"Evel Knievel on Tour\" was released. Written by Knievel's promoter for the Snake River Canyon jump, Shelly Saltman, the book painted him as \"an alcoholic, a pill addict, an anti-Semite and an immoral person\" through tape-recorded interviews done of Knievel and others.\nKnievel, with both arms still in casts, flew to California to confront Saltman, by then a vice president at 20th Century Fox. On September 21, 1977, outside the studio commissary, one of Knievel's friends grabbed Saltman and held him, while Knievel attacked him with an aluminum baseball bat, declaring \"I'm going to kill you!\" According to a witness to the attack, Knievel struck repeated blows at Saltman's head, with Saltman blocking the blows with his left arm. Saltman's arm and wrist were shattered in several places before he fell to the ground unconscious. Numerous surgeries were done to the shattered arm that resulted in Saltman having a steel plate and screws.\nSaltman's book was withdrawn by the publisher after Knievel threatened to sue. Saltman later produced documents in both criminal and civil court that proved that, although Knievel claimed to have been insulted by statements in Saltman's book, he and his lawyers had been given editorial access to the book and had approved and signed off on every word before its publication. On November 15, 1977, Knievel pleaded guilty to battery and was sentenced to three years probation and six months in county jail.\nAfter the Saltman assault and subsequent jail time, Knievel lost his marketing endorsements and deals, including Harley-Davidson and Ideal Toys. With no income from jumping or sponsorships, Knievel eventually declared bankruptcy. In 1981, Saltman was awarded a $12.75million judgment against Knievel in a civil trial, but he never received any money from either Knievel or Knievel's estate. Knievel expressed no remorse for the attack, once calling it \"frontier justice\".\nMarriages and children.\nKnievel was married twice. He and his wife Linda were married for 38 years. During their marriage, the couple had four children: two boys, Kelly and Robbie, and two girls, Tracey and Alicia. Throughout Kelly's and Robbie's adolescence, they performed at Knievel's stunt shows. Into adulthood, Robbie continued to perform as a professional motorcycle daredevil. After Evel's death, Kelly has overseen the Knievel legacy, including developing Knievel-related products and assisting Harley-Davidson to develop a museum exhibit. Knievel's courtship and marriage to Linda was the theme of the biopic 1971 film \"Evel Knievel\". Linda and Evel separated in the early 1990s and were divorced in 1997 in San Jose, California. \nA municipal judge ordered Evel to stand trial for a weapons possession charge in 1994. Knievel was arrested in October at a Sunnyvale go-go bar on suspicion of battering his girlfriend, 25-year-old Krystal Kennedy of Florida. Sunnyvale police later discovered two handguns and some ammunition in the trunk of his car. The battering charge was dropped when Kennedy declined to cooperate.\nIn 1999, Knievel married his girlfriend, Krystal Kennedy of Clearwater, Florida, whom he began dating in 1992. The wedding was held on November 19, 1999, on a special platform built on the fountains at Caesars Palace on the Las Vegas Strip (site of Evel's jump New Year's Eve 1967). Long-time friend Engelbert Humperdinck sent a recorded tribute to the couple.\nThe couple was married for two years, divorcing in 2001. Following the divorce, Krystal Knievel was granted a restraining order against him. However, Krystal and Evel would work out their differences, living together until Knievel's death. According to the investment magazine, \"Registered Rep\"., Knievel left his entire estate to Krystal.\nPost-daredevil years.\nDuring the 1980s, Knievel drove around the country in a recreational vehicle, selling works of art allegedly painted by him. After several years of obscurity, Knievel made a significant marketing comeback in the 1990s, representing Maxim Casino, Little Caesars, Harley-Davidson and other firms.\nIn 1999, Knievel celebrated the 25th anniversary of the Snake River Canyon jump at the Twin Falls mall. His memorabilia was then stored at Kent Knigge's farm in Filer, Idaho, seven miles west of Twin Falls. During the same year, Knievel was inducted into the Motorcycle Hall of Fame.\nKnievel once dreamed of housing all of his career memorabilia in an Evel Knievel Museum to be located in his home state of Montana. Those dreams were unfulfilled, and his artifacts are spread throughout transportation museums and private collections around the world. Knievel's original blueprints and handwritten notes about his desired museum are currently displayed at the Route 66 Vintage Iron Motorcycle Museum in downtown Miami, Oklahoma. The Route 66 site also houses Evel's Snake River Canyon Jump Mission Control Super Van. While Knievel's original dream of having all his significant memorabilia being centralized would go unfulfilled, a few public museums were opened in his honor, including the Evel Knievel Museum in Topeka, Kansas, which has the official approval of the Knievel estate.\nOn October 9, 2005, Knievel promoted his last public \"motorcycle ride\" at the Milwaukee Harley-Davidson dealership. The ride was to benefit victims of Hurricane Katrina. Although he was originally scheduled to lead a benefit ride through Milwaukee, Knievel never rode the motorcycle because he suffered a mild (non-debilitating) stroke before the appearance and limited his visit to a signing session.\n\"Evel Knievel: The Rock Opera\".\nIn 2003, Knievel signed over exclusive rights to Los Angeles composer Jef Bek, authorizing the production of a rock opera based on Knievel's life. Directed by \"\" co-creator Keythe Farley, the production opened in Los Angeles in September 2007 to some positive reviews.\nSix Flags Evel Knievel roller coaster.\nKnievel had partnered with Six Flags St. Louis to name a new wooden coaster after \"America's Legendary Daredevil\". The amusement park in Eureka, Missouri, outside of St. Louis, Missouri, opened the ride on June 20, 2008. \"The Evel Knievel Roller Coaster\" operated for three seasons before being renamed \"American Thunder\" in 2011.\nDeclining health.\nIn the late 1990s, Knievel required a life-saving liver transplant as a result of suffering the long-term effects of Hepatitis C, which he contracted from one of the numerous blood transfusions he received before 1992. In February 1999, Knievel was given only a few days to live and he requested to leave the hospital and die at his home. En route to his home, Knievel received a phone call from the hospital stating a young man had died in a motorcycle accident and could be a donor. Days later, Knievel received the transplant.\nIn 2005, he was diagnosed with idiopathic pulmonary fibrosis, an incurable and terminal lung disease that required him to be on supplemental oxygen 24 hours a day. In 2006, he had an internal morphine pain pump surgically implanted to help him with the excruciating pain in his deteriorated lower back, one of the costs of incurring so many traumas throughout his career as a daredevil. He also had two strokes after 2005, but neither left him with severe debilitation.\nOn July 27, 2006, he appeared on \"The Adam Carolla Show\" and discussed his health problems. The following day, he appeared on stage with Robbie at \"Evel Knievel Days\" in Butte, marking the last performance in which the two appeared together. Robbie jumped in a tribute to his father on a much lighter motorcycle with far superior suspension.\nShortly before his death, Evel Knievel was featured in a BBC Two Christmas special presented by Richard Hammond. The 60-minute program \"Richard Hammond Meets Evel Knievel\" aired on December 23, 2007, less than a month after Knievel's death. The documentary was filmed in July 2007 at the annual \"Evel Knievel Days\" festival in his old hometown of Butte.\nChristian conversion.\nOn April 1, 2007, Knievel appeared on Robert\u00a0H. Schuller's television program \"Hour of Power\" and announced that he \"believed in Jesus Christ\" for the first time. At his request, he was baptized at a televised congregation at the Crystal Cathedral by Schuller. Knievel's televised testimony triggered mass baptisms at the Crystal Cathedral.\nDeath.\nKnievel died in Clearwater, Florida on November 30, 2007, aged 69. He had been suffering from diabetes and idiopathic pulmonary fibrosis for many years. A longtime friend reported that Knievel had trouble breathing while at his residence in Clearwater and died on the way to hospital. The friend said, \"It's been coming for years but you just don't expect it. Superman just doesn't die, right?\"\nIn one of his last interviews, Knievel told \"Maxim\" magazine:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;You can't ask a guy like me why I performed. I really wanted to fly through the air. I was a daredevil, a performer. I loved the thrill, the money, the whole macho thing. All those things made me Evel Knievel. Sure, I was scared. You gotta be an ass not to be scared. But I beat the hell out of death. [...] You're in the air for four seconds, you're part of the machine and then if you make a mistake midair, you say to yourself, \"Oh, boy. I'm gonna crash\" and there's nothing you can do to stop it, not at all.\nKnievel was buried at Mountain View Cemetery in his hometown of Butte, Montana on December 10, 2007, following a funeral at the 7,500-seat Butte Civic Center presided over by Robert H. Schuller with actor Matthew McConaughey giving the eulogy. Before the Monday service, fireworks exploded in the Butte night sky as pallbearers carried Knievel's casket into the center.\nPosthumous recognition.\nOn July 10, 2010, a special temporary exhibit entitled \"True Evel: The Amazing Story of Evel Knievel\" was opened at the Harley-Davidson Museum in Milwaukee, Wisconsin. The exhibit was opened in collaboration with Harley-Davidson Motorcycles and Evel's oldest son, Kelly. Among the various artifacts from Knievel's life, the exhibit included his \"Shark Jump\" Harley-Davidson XR-750, the Skycycle X-2, a blue jumpsuit from late in his career without any sponsor patches, and his trademark red-white-and-blue jumpsuit complete with his helmet and walking stick. Evel Knievel merchandising, personal artifacts, and X-rays from his injuries were also exhibited. In December 2010, a traveling version of the exhibit began a one-year tour of the United Kingdom and Europe.\nOn September 16, 2016, professional stuntman Eddie Braun successfully jumped the Snake River Canyon in a replica of Knievel's Snake River rocket. Braun cited Knievel as an inspiration and wanted to show that Knievel's jump would have been successful had the parachute not been deployed too early. Braun stated that he was \"finishing out [the] dream\" of his hero, Knievel.\nIn 2017, the Evel Knievel Museum, a museum honoring Knievel was opened in Topeka, Kansas, by co-founders Lathan McKay and Mike Patterson. The museum features his motorcycles, leathers, helmets, wardrobe, and jewelry along with various displays and a virtual reality motorcycle jump.\nOn July 8, 2018, Travis Pastrana from Nitro Circus paid tribute to Evel on History Channel live event, \"Evel Live\", with 3 of Evel's most famous record-breaking Las Vegas jumps in one night. He was riding a Roland Sands Design\u2013prepared 450-pound Indian Scout FTR750, and dressed in a full Evel Knievel getup, down to wearing vintage-style-appearing dress boots from Bates, the manufacturer that had made Evel's.\nOn November 16, 2023, the New York Times crossword, created by Paolo Pascal, paid homage to Knievel with a themed crossword which included clues such as \"Description of this puzzle's subject\" (\"DAREDEVIL\") and \"Acting dangerously, like this puzzle's subject\" (\"LIVING ON THE EDGE\"). Upon completion of the puzzle through the NYT Games app or website, solvers were rewarded with a short animation of a person on a motorcycle leaping through the squares of the crossword over three buses.\nOn December 19, 2024, a new biographical film adaptation of Evel's life was reported to be in the works with La La Land director Damien Chazelle attached to direct, William Monahan set to pen the script, and negotiations with Leonardo DiCaprio to star as Knievel.\nIn September of 2025, an Evel Knievel comic book series was announced which would be published by Half Evil Comics and crowdfunded through Kickstarter. The series will be written by Rylend Grant and feature input from Knievel's son Kelly.\nTelevision commercials.\nIn November 2010, General Motors premiered a television commercial featuring footage of Knievel's Wembley Stadium crash in 1975, followed by Knievel getting onto his feet. The ad focused on GM's restructuring and emphasized the belief that \"we all fall down\".\nOn July 18, 2012, Audi of America recreated Knievel's Snake River jump in a promotional commercial for the Audi RS5. The commercial depicts the RS5 being driven by a professional driver and jumping the canyon off a jump ramp.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66428", "revid": "48539996", "url": "https://en.wikipedia.org/wiki?curid=66428", "title": "Acmeist poetry", "text": "School of poetry in early 20th-century Russia\nAcmeism, or the Guild of Poets, was a modernist transient poetic school, which emerged c.\u20091911 or in 1912 in Russia under the leadership of Nikolay Gumilev and Sergei Gorodetsky. Their ideals were compactness of form and clarity of expression. The term was coined after the Greek word \u1f00\u03ba\u03bc\u03ae (\"akm\u0113\"), i.e., \"the best age of man\".\nThe acmeist mood was first announced by Mikhail Kuzmin in his 1910 essay \"Concerning Beautiful Clarity\". The acmeists contrasted the ideal of Apollonian clarity (hence the name of their journal, \"Apollon\") to \"Dionysian frenzy\" propagated by the Russian symbolist poets like Bely and Vyacheslav Ivanov. To the Symbolists' preoccupation with \"intimations through symbols\" they preferred \"direct expression through images\".\nIn his later manifesto \"The Morning of Acmeism\" (1913), Osip Mandelstam defined the movement as \"a yearning for world culture\". As a \"neo-classical form of modernism\", which essentialized \"poetic craft and cultural continuity\", the Guild of Poets placed Alexander Pope, Th\u00e9ophile Gautier, Rudyard Kipling, Innokentiy Annensky, and the Parnassian poets among their predecessors.\nMajor poets in this school include Osip Mandelstam, Nikolay Gumilev, Mikhail Kuzmin, Anna Akhmatova, and Georgiy Ivanov. The group originally met in The Stray Dog Cafe, St. Petersburg, then a celebrated meeting place for artists and writers. Mandelstam's collection of poems \"Stone\" (1912) is considered the movement's finest accomplishment.\nAmongst the major acmeist poets, each interpreted acmeism in a different stylistic light, from Akhmatova's intimate poems on topics of love and relationships to Gumilev's narrative verse.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66430", "revid": "12311825", "url": "https://en.wikipedia.org/wiki?curid=66430", "title": "High blood pressure", "text": ""}
{"id": "66432", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=66432", "title": "Progesterone", "text": "Sex hormone\nProgesterone (; P4) is an endogenous steroid and progestogen sex hormone involved in the menstrual cycle, pregnancy, and embryogenesis of humans and other species. It belongs to a group of steroid hormones called the progestogens and is the major progestogen in the body. Progesterone has a variety of important functions in the body. It is also a crucial metabolic intermediate in the production of other endogenous steroids, including the sex hormones and the corticosteroids, and plays an important role in brain function as a neurosteroid.\nIn addition to its role as a natural hormone, progesterone is also used as a medication, such as in combination with estrogen for contraception, to reduce the risk of uterine or cervical cancer, in hormone replacement therapy, and in feminizing hormone therapy. It was first prescribed in 1934.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nBiological activity.\nProgesterone is the most important progestogen in the body. As a potent agonist of the nuclear progesterone receptor (nPR) (with an affinity of KD\u00a0=\u00a01\u00a0nM), the resulting effects on ribosomal transcription play a major role in regulation of female reproduction. In addition, progesterone is an agonist of the more recently discovered membrane progesterone receptors (mPRs), of which the expression has regulation effects in reproduction function (oocyte maturation, labor, and sperm motility) and cancer, although additional research is required to further define the roles. It also functions as a ligand of the PGRMC1 (progesterone receptor membrane component\u00a01) which impacts tumor progression, metabolic regulation, and viability control of nerve cells. Moreover, progesterone is also known to be an antagonist of the sigma \u03c31 receptor, a negative allosteric modulator of nicotinic acetylcholine receptors, and a potent antagonist of the mineralocorticoid receptor (MR). Progesterone prevents MR activation by binding to this receptor with an affinity exceeding even those of aldosterone, and glucocorticoids such as cortisol and corticosterone, and it produces antimineralocorticoid effects, such as natriuresis, at physiological concentrations. Progesterone also binds to, and behaves as a partial agonist of, the glucocorticoid receptor (GR), albeit with very low potency (EC50 &gt;100-fold less relative to cortisol).\nThrough its neurosteroid active metabolites, such as 5\u03b1-dihydroprogesterone and allopregnanolone, progesterone acts indirectly as a positive allosteric modulator of the GABAA receptor.\nProgesterone and some of its metabolites, such as 5\u03b2-dihydroprogesterone, are agonists of the pregnane X receptor (PXR), albeit weakly so (EC50 &gt;10\u00a0\u03bcM). In accordance, progesterone induces several hepatic cytochrome P450 enzymes, such as CYP3A4, especially during pregnancy when concentrations are much higher than usual. Perimenopausal women have been found to have greater CYP3A4 activity relative to men and postmenopausal women, and it has been inferred that this may be due to the higher progesterone levels present in perimenopausal women.\nProgesterone modulates the activity of CatSper (cation channels of sperm) voltage-gated Ca2+ channels. Since eggs release progesterone, sperm may use progesterone as a homing signal to swim toward eggs (chemotaxis). As a result, it has been suggested that substances that block the progesterone binding site on CatSper channels could potentially be used in male contraception.\nBiological function.\nHormonal interactions.\nProgesterone has a number of physiological effects that are amplified in the presence of estrogens. Estrogens through estrogen receptors (ERs) induce or upregulate the expression of the PR. One example of this is in breast tissue, where estrogens allow progesterone to mediate lobuloalveolar development.\nElevated levels of progesterone potently reduce the sodium-retaining activity of aldosterone, resulting in natriuresis and a reduction in extracellular fluid volume. Progesterone withdrawal, on the other hand, is associated with a temporary increase in sodium retention (reduced natriuresis, with an increase in extracellular fluid volume) due to the compensatory increase in aldosterone production, which combats the blockade of the mineralocorticoid receptor by the previously elevated level of progesterone.\nEarly sexual differentiation.\nPlacental progesterone can be converted into 5\u03b1-dihydrotestosterone (DHT), a potent androgen that is responsible for the development of male genitalia. This can be done both by conversion into testosterone, which is then converted to DHT, and via the androgen backdoor pathway, which is particularly important for fetal development. Progesterone is the precursor for both pathways and therefore plays a key role in sexual differentiation.\nReproductive system.\nProgesterone has key effects via non-genomic signalling on human sperm as they migrate through the female reproductive tract before fertilization occurs, though the receptor(s) as yet remain unidentified. Detailed characterisation of the events occurring in sperm in response to progesterone has elucidated certain events including intracellular calcium transients and maintained changes, slow calcium oscillations, now thought to possibly regulate motility. It is produced by the ovaries. Progesterone has also been shown to demonstrate effects on octopus spermatozoa.\nProgesterone is sometimes called the \"hormone of pregnancy\", and it has many roles relating to the development of the fetus:\nThe fetus metabolizes placental progesterone in the production of adrenal steroids.\nBreasts.\nLobuloalveolar development.\nProgesterone plays an important role in breast development. In conjunction with prolactin, it mediates lobuloalveolar maturation of the mammary glands during pregnancy to allow for milk production, and thus lactation and breastfeeding of offspring following parturition (childbirth). Estrogen induces expression of the progesterone receptors (PR) in breast tissue, and hence progesterone is dependent on estrogen to mediate lobuloalveolar development. It has been found that RANKL is a critical downstream mediator of progesterone-induced lobuloalveolar maturation. RANKL knockout mice show an almost identical mammary phenotype to PR knockout mice, including normal mammary ductal development, but complete failure of the development of lobuloalveolar structures.\nDuctal development.\nThough to a far lesser extent than estrogen, which is the major mediator of mammary ductal development (via the ER\u03b1), progesterone may also be involved in ductal development of the mammary glands to some extent. PR knockout mice or mice treated with the PR antagonist mifepristone show delayed although otherwise normal mammary ductal development at puberty. In addition, mice modified to have overexpression of PRA display ductal hyperplasia, and progesterone induces ductal growth in the mouse mammary gland. Progesterone mediates ductal development mainly via induction of the expression of amphiregulin, the same growth factor that estrogen primarily induces the expression of to mediate ductal development. These animal findings suggest that, while not essential for full mammary ductal development, progesterone seems to play a potentiating or accelerating role in estrogen-mediated mammary ductal development.\nBreast cancer risk.\nProgesterone also appears to be involved in the pathophysiology of breast cancer, though its role, and whether it is a promoter or inhibitor of breast cancer risk, has not been fully elucidated. Most progestins, or synthetic progestogens, like medroxyprogesterone acetate, have been found to increase the risk of breast cancer in postmenopausal people in combination with estrogen as a component of menopausal hormone therapy. The combination of natural oral progesterone or the atypical progestin dydrogesterone with estrogen has been associated with less risk of breast cancer than progestins plus estrogen. However, this may simply be an artifact of the low progesterone levels produced with oral progesterone. More research is needed on the role of progesterone in breast cancer.\nSkin health.\nThe estrogen receptor, as well as the progesterone receptor, have been detected in the skin, including in keratinocytes and fibroblasts. At menopause and thereafter, decreased levels of female sex hormones result in atrophy, thinning, and increased wrinkling of the skin, and a reduction in skin elasticity, firmness, and strength. These skin changes constitute an acceleration in skin aging and are the result of decreased collagen content, irregularities in the morphology of epidermal skin cells, decreased ground substance between skin fibers, and reduced capillaries and blood flow. The skin also becomes more dry during menopause, as a result of reduced skin hydration and surface lipids (sebum production). Along with chronological aging and photoaging, estrogen deficiency in menopause is one of the three main factors that predominantly influences skin aging.\nHormone replacement therapy, consisting of systemic treatment with estrogen alone or in combination with a progestogen, has well-documented and considerable beneficial effects on the skin of postmenopausal people. These benefits include increased skin collagen content, skin thickness and elasticity, and skin hydration and surface lipids. Topical estrogen has been found to have similar beneficial effects on the skin. In addition, a study has found that topical 2% progesterone cream significantly increases skin elasticity and firmness and observably decreases wrinkles in peri- and postmenopausal people. Skin hydration and surface lipids, on the other hand, did not significantly change with topical progesterone.\nThese findings suggest that progesterone, like estrogen, also has beneficial effects on the skin and may be independently protective against skin aging.\nSexuality.\nLibido.\nProgesterone and its neurosteroid active metabolite, allopregnanolone, appear to be importantly involved in libido in females.\nHomosexuality.\nDr. Diana Fleischman, of the University of Portsmouth, and colleagues looked for a relationship between progesterone and sexual attitudes in 92 women. Their research, published in the Archives of Sexual Behavior found that women who had higher levels of progesterone scored higher on a questionnaire measuring homoerotic motivation. They also found that men who had high levels of progesterone were more likely to have higher homoerotic motivation scores after affiliative priming compared to men with low levels of progesterone.\nNervous system.\nProgesterone, like pregnenolone and dehydroepiandrosterone (DHEA), belongs to an important group of endogenous steroids called neurosteroids. It can be metabolized within all parts of the central nervous system.\nNeurosteroids are neuromodulators and are neuroprotective, neurogenic, and regulate neurotransmission and myelination. The effects of progesterone as a neurosteroid are mediated predominantly through its interactions with non-nuclear PRs, namely the mPRs and PGRMC1, as well as certain other receptors, such as the \u03c31 and nACh receptors.\nBrain damage.\nPrevious studies have shown that progesterone supports the normal development of neurons in the brain, and that the hormone has a protective effect on damaged brain tissue. It has been observed in animal models that females have reduced susceptibility to traumatic brain injury, and this protective effect has been hypothesized to be caused by increased circulating levels of estrogen and progesterone in females.\nProposed mechanism.\nThe mechanism of progesterone protective effects may be the reduction of inflammation that follows brain trauma and hemorrhage.\nDamage incurred by traumatic brain injury is believed to be caused in part by mass depolarization leading to excitotoxicity. One way in which progesterone helps to alleviate some of this excitotoxicity is by blocking the voltage-dependent calcium channels that trigger neurotransmitter release. It does so by manipulating the signaling pathways of transcription factors involved in this release. Another method for reducing the excitotoxicity is by up-regulating the GABAA, a widespread inhibitory neurotransmitter receptor.\nProgesterone has also been shown to prevent apoptosis in neurons, a common consequence of brain injury. It does so by inhibiting enzymes involved in the apoptosis pathway specifically concerning the mitochondria, such as activated caspase-3 and cytochrome\u00a0c.\nNot only does progesterone help prevent further damage, it has also been shown to aid in neuroregeneration. One of the serious effects of traumatic brain injury includes edema. Animal studies show that progesterone treatment leads to a decrease in edema levels by increasing the concentration of macrophages and microglia sent to the injured tissue. This was observed in the form of reduced leakage from the blood brain barrier in secondary recovery in progesterone treated rats. In addition, progesterone was observed to have antioxidant properties, reducing the concentration of oxygen free radicals faster than without. There is also evidence that the addition of progesterone can also help remyelinate damaged axons due to trauma, restoring some lost neural signal conduction. Another way progesterone aids in regeneration includes increasing the circulation of endothelial progenitor cells in the brain. This aids the growth of new vasculature around scar tissue, helping to repair the area of insult.\nAddiction.\nProgesterone enhances the function of serotonin receptors in the brain, so an excess or deficit of progesterone has the potential to result in significant neurochemical issues. This provides an explanation for why some people resort to substances that enhance serotonin activity such as nicotine, alcohol, and cannabis when their progesterone levels fall below optimal levels.\nSocietal.\nIn a 2012 University of Amsterdam study of 120 women, the women's luteal phase (higher levels of progesterone, and increasing levels of estrogen) was correlated with a lower level of competitive behavior in gambling and math contest scenarios, while their premenstrual phase (sharply-decreasing levels of progesterone, and decreasing levels of estrogen) was correlated with a higher level of competitive behavior.\nBiochemistry.\nBiosynthesis.\nIn mammals, progesterone, like all other steroid hormones, is synthesized from pregnenolone, which itself is derived from cholesterol.\nCholesterol undergoes double oxidation to produce 22\"R\"-hydroxycholesterol and then 20\u03b1,22\"R\"-dihydroxycholesterol. This vicinal diol is then further oxidized with loss of the side chain starting at position C22 to produce pregnenolone. This reaction is catalyzed by cytochrome P450scc.\nThe conversion of pregnenolone to progesterone takes place in two steps. First, the 3\u03b2-hydroxyl group is oxidized to a keto group and second, the double bond is moved to C4, from C5 through a keto/enol tautomerization reaction. This reaction is catalyzed by 3\u03b2-hydroxysteroid dehydrogenase/\u03b45-4-isomerase.\nProgesterone in turn is the precursor of the mineralocorticoid aldosterone, and after conversion to 17\u03b1-hydroxyprogesterone, of cortisol and androstenedione. Androstenedione can be converted to testosterone, estrone, and estradiol, highlighting the critical role of progesterone in testosterone synthesis.\nPregnenolone and progesterone can also be synthesized by yeast.\nApproximately 30\u00a0mg of progesterone is secreted from the ovaries per day in reproductive-age women, while the adrenal glands produce about 1\u00a0mg of progesterone per day.\nDistribution.\nProgesterone binds extensively to plasma proteins, including albumin (50\u200d\u2013\u200d54%) and transcortin (43\u200d\u2013\u200d48%). It has similar affinity for albumin relative to the PR.\nMetabolism.\nThe metabolism of progesterone is rapid and extensive, and it occurs mainly in the liver, though enzymes that metabolize progesterone are also expressed widely in the brain, skin, and various other extrahepatic tissues. Progesterone has an elimination half-life of only approximately five minutes in circulation. The metabolism of progesterone is complex, and it may form as many as 35 different unconjugated metabolites when it is ingested orally. Progesterone is highly susceptible to enzymatic reduction via reductases and hydroxysteroid dehydrogenases because of its double bond (between the C4 and C5 positions) and its two ketones (at the C3 and C20 positions).\nThe major metabolic pathway of progesterone is reduction by 5\u03b1-reductase and 5\u03b2-reductase, into the dihydrogenated 5\u03b1-dihydroprogesterone and 5\u03b2-dihydroprogesterone, respectively. This is followed by the further reduction of these metabolites via 3\u03b1-hydroxysteroid dehydrogenase and 3\u03b2-hydroxysteroid dehydrogenase into the tetrahydrogenated allopregnanolone, pregnanolone, isopregnanolone, and epipregnanolone. Subsequently, 20\u03b1-hydroxysteroid dehydrogenase and 20\u03b2-hydroxysteroid dehydrogenase reduce these metabolites to form the corresponding hexahydrogenated pregnanediols (eight different isomers in total), which are then conjugated via glucuronidation and/or sulfation, released from the liver into circulation, and excreted by the kidneys into the urine. The major metabolite of progesterone in the urine is the 3\u03b1,5\u03b2,20\u03b1 isomer of pregnanediol glucuronide, which has been found to constitute 15\u200d\u2013\u200d30% of an injection of progesterone. Other metabolites of progesterone formed by the enzymes in this pathway include 3\u03b1-dihydroprogesterone, 3\u03b2-dihydroprogesterone, 20\u03b1-dihydroprogesterone, and 20\u03b2-dihydroprogesterone, as well as various combination products of the enzymes aside from those already mentioned. Progesterone can also first be hydroxylated (see below) and then reduced. Endogenous progesterone is metabolized approximately 50% into 5\u03b1-dihydroprogesterone in the corpus luteum, 35% into 3\u03b2-dihydroprogesterone in the liver, and 10% into 20\u03b1-dihydroprogesterone.\nRelatively small portions of progesterone are hydroxylated via 17\u03b1-hydroxylase (CYP17A1) and 21-hydroxylase (CYP21A2), into 17\u03b1-hydroxyprogesterone and 11-deoxycorticosterone (21-hydroxyprogesterone), respectively, and pregnanetriols are formed secondarily to 17\u03b1-hydroxylation. Even smaller amounts of progesterone may also be hydroxylated via 11\u03b2-hydroxylase (CYP11B1) and, to a lesser extent, via aldosterone synthase (CYP11B2) into 11\u03b2-hydroxyprogesterone. In addition, progesterone can be hydroxylated in the liver by other cytochrome P450 enzymes that are not steroid-specific. Catalyzed mainly by CYP3A4, 6\u03b2-Hydroxylation is the major transformation and is responsible for approximately 70% of cytochrome P450-mediated progesterone metabolism. Other routes include 6\u03b1-, 16\u03b1-, and 16\u03b2-hydroxylation. However, treatment of women with ketoconazole (a strong CYP3A4 inhibitor) had minimal effects on progesterone levels, producing only a slight and non-significant increase, suggesting that cytochrome P450 enzymes play only a small role in progesterone metabolism.\nLevels.\nRelatively low during the preovulatory phase of the menstrual cycle, progesterone levels rise after ovulation and are elevated during the luteal phase, as shown in the diagram. Progesterone levels tend to be less than 2\u00a0ng/mL prior to ovulation and greater than 5\u00a0ng/mL after ovulation. If pregnancy occurs, human chorionic gonadotropin is released, maintaining the corpus luteum and allowing it to maintain levels of progesterone. Between seven and nine weeks gestation, the placenta begins to produce progesterone in place of the corpus luteum in a process called the luteal-placental shift.\nAfter the luteal-placental shift, progesterone levels start to rise further and may reach 100 to 200\u00a0ng/mL at term. Whether a decrease in progesterone levels is critical for the initiation of labor has been argued and may be species-specific. After delivery of the placenta and during lactation, progesterone levels are very low.\nProgesterone levels are low in children and postmenopausal people. Adult males have levels similar to those in women during the follicular phase of the menstrual cycle.\nRanges.\nBlood test results should always be interpreted using the reference ranges provided by the laboratory that performed the results. Example reference ranges are listed below.\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;Reference ranges for the blood content of progesterone during the menstrual cycle\nSources.\nAnimal.\nAn additional animal source of progesterone is milk products. After consumption of milk products the level of bioavailable progesterone goes up.\nPlants.\nProgesterone has been positively identified in the plant \"Juglans regia\", a species of walnut. In addition, progesterone-like steroids are found in the plant \"Dioscorea mexicana\", part of the yam family native to Mexico. \"Dioscorea mexicana\" contains a steroid called diosgenin which is taken from the plant and converted into progesterone. Diosgenin and progesterone are also found in other \"Dioscorea\" species, as well as in other plants that are not closely related, such as fenugreek.\nAnother plant that contains substances readily convertible to progesterone is \"Dioscorea pseudojaponica\", native to Taiwan. Research has shown that the Taiwanese yam contains saponins\u2014steroids that can be converted to diosgenin and thence to progesterone.\nMany other \"Dioscorea\" species of the yam family contain steroidal substances from which progesterone can be produced. Among the more notable of these are \"Dioscorea villosa\" and \"Dioscorea polygonoides\". One study showed that the \"Dioscorea villosa\" contains 3.5% diosgenin. \"Dioscorea polygonoides\" has been found to contain 2.64% diosgenin, as shown by gas chromatography-mass spectrometry. Many of the \"Dioscorea\" species that originate from the yam family grow in countries with tropical and subtropical climates.\nMedical use.\nProgesterone is used as a medication. It is used in combination with estrogens mainly in hormone therapy for menopausal symptoms and low sex hormone levels. It may also be used alone to treat menopausal symptoms. Studies have shown that transdermal progesterone (skin patch) and oral micronized progesterone are effective treatments for certain symptoms of menopause such as hot flashes and night sweats, otherwise referred to as vasomotor symptoms or VMS.\nIt is also used to support pregnancy and fertility and to treat gynecological disorders. Progesterone has been shown to prevent miscarriage in those with vaginal bleeding early in their current pregnancy and having a previous history of miscarriage. Progesterone can be taken by mouth, through the vagina, and by injection into muscle or fat, among other routes.\nChemistry.\nProgesterone is a naturally occurring pregnane steroid and is also known as pregn-4-ene-3,20-dione. It has a double bond (4-ene) between the C4 and C5 positions, and two ketone groups (3,20-dione), one at the C3 position and the other at the C20 position.\nSynthesis.\nProgesterone is commercially produced by semisynthesis. Two main routes are used: one from yam diosgenin first pioneered by Marker in 1940, and one based on soy phytosterols scaled up in the 1970s. Additional (not necessarily economical) semisyntheses of progesterone have also been reported starting from a variety of steroids. For the example, cortisone can be simultaneously deoxygenated at the C-17 and C-21 position by treatment with iodotrimethylsilane in chloroform to produce 11-keto-progesterone (ketogestin), which in turn can be reduced at position-11 to yield progesterone.\nMarker semisynthesis.\nAn economical semisynthesis of progesterone from the plant steroid diosgenin isolated from yams was developed by Russell Marker in 1940 for the Parke-Davis pharmaceutical company. This synthesis is known as the Marker degradation.\nThe 16-DPA intermediate is important to the synthesis of many other medically important steroids. A very similar approach can produce 16-DPA from solanine.\nSoy semisynthesis.\nProgesterone can also be made from the stigmasterol found in soybean oil also. c.f. Percy Julian.\nTotal synthesis.\nA total synthesis of progesterone was reported in 1971 by William S. Johnson. The synthesis begins with reacting the phosphonium salt 7 with phenyl lithium to produce the phosphonium ylide 8. The ylide 8 is reacted with an aldehyde to produce the alkene 9. The ketal protecting groups of 9 are hydrolyzed to produce the diketone 10, which in turn is cyclized to form the cyclopentenone 11. The ketone of 11 is reacted with methyl lithium to yield the tertiary alcohol 12, which in turn is treated with acid to produce the tertiary cation 13. The key step of the synthesis is the \u03c0-cation cyclization of 13 in which the B-, C-, and D-rings of the steroid are simultaneously formed to produce 14. This step resembles the cationic cyclization reaction used in the biosynthesis of steroids and hence is referred to as \"biomimetic\". In the next step the enol orthoester is hydrolyzed to produce the ketone 15. The cyclopentene A-ring is then opened by oxidizing with ozone to produce 16. Finally, the diketone 17 undergoes an intramolecular aldol condensation by treating with aqueous potassium hydroxide to produce progesterone.\nHistory.\nGeorge W. Corner and Willard M. Allen discovered the hormonal action of progesterone in 1929. By 1931\u20131932, nearly pure crystalline material of high progestational activity had been isolated from the corpus luteum of animals; by 1934, pure crystalline progesterone had been refined and obtained, and the chemical structure of progesterone was determined. This was achieved by Adolf Butenandt at the \"Chemisches Institut\" of Gda\u0144sk Technical University in Danzig, who extracted this new compound from several thousand liters of urine.\nChemical synthesis of progesterone from stigmasterol and pregnanediol was accomplished later that year. Up to this point, progesterone, known generically as corpus luteum hormone, had been being referred to by several groups by different names, including corporin, lutein, luteosterone, and progestin. In 1935, at the time of the Second International Conference on the Standardization of Sex Hormones in London, England, a compromise was reached between the groups, and the name 'progesterone' (progestational steroidal ketone) was created.\nVeterinary use.\nThe use of progesterone tests in dog breeding to pinpoint ovulation is becoming more widely used. There are several tests available, the most reliable being a blood test with the blood sample drawn by a veterinarian and sent to a lab for processing. Results can usually be obtained within 24 to 72 hours. The rationale for using progesterone tests is that increased numbers begin in close proximity to preovulatory surge in gonadotrophins and continue through ovulation and estrus. When progesterone levels reach certain levels they can signal the stage of estrus the female is. Prediction of birth date of the pending litter can be very accurate if ovulation date is known. Puppies deliver within a day or two of nine weeks gestation in most cases. It is not possible to determine pregnancy using progesterone tests once a breeding has taken place, however. This is due to the fact that, in dogs, progesterone levels remain elevated throughout the estrus period.\nPricing.\nPricing for progesterone can vary depending location, insurance coverage, discount coupons, quantity, shortages, manufacturers, brand or generic versions, different pharmacies, and so on. As of 2023, 30 capsules of 100\u00a0mg of the generic version, Progesterone, from CVS Pharmacy is around $40 without any discounts or insurance applied. The brand version, Prometrium, is around $450 for 30 capsules without any discounts or insurance applied. In comparison, Walgreens offers 30 capsules of 100\u00a0mg in the generic version for $51 without insurance or coupons applied. The brand name costs around $431 for 30 capsules of 100\u00a0mg. "}
{"id": "66433", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=66433", "title": "Chronic obstructive lung disease", "text": ""}
{"id": "66434", "revid": "2077288", "url": "https://en.wikipedia.org/wiki?curid=66434", "title": "Bronchial asthma", "text": ""}
{"id": "66435", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=66435", "title": "Famous chess players", "text": ""}
{"id": "66436", "revid": "525927", "url": "https://en.wikipedia.org/wiki?curid=66436", "title": "Iraq disarmament crisis", "text": "Early 2000s diplomatic crisis\nThe Iraq disarmament crisis was claimed as one of the primary issues that led to the multinational invasion of Iraq on 20 March 2003.\nSince the 1980s, Iraq was widely assumed to have been producing and extensively running the programs of biological, chemical and nuclear weapons. Iraq made extensive use of chemical weapons during the Iran\u2013Iraq War in the 1980s, including against its own Kurdish population. France and the Soviet Union assisted Iraq in the development of its nuclear program, but its primary facility was destroyed by Israel in 1981 in a surprise air strike.\nAfter the Gulf War in 1990, the United Nations Special Commission located and destroyed large quantities of Iraqi chemical weapons and related equipment and materials with varying degrees of Iraqi cooperation and obstruction, but the Iraqi cooperation later diminished in 1998. The disarmament issue remained tense throughout the 1990s with U.S. at the UN, repeatedly demanding Iraq to allow inspections teams to its facilities. These crises reached their climax in 2002-2003, when U.S. president George W. Bush demanded a complete end to what he alleged was Iraqi production of weapons of mass destruction, and reasoned with Iraqi President Saddam Hussein to comply with UN Resolutions requiring UN weapons inspectors unfettered access to areas those inspectors thought might have weapons production facilities.\nSince the Gulf War in 1991, Iraq had been restricted by the United Nations (UN) from developing or possessing such weapons. It was also required to permit inspections to confirm Iraqi compliance. Bush repeatedly backed demands for unfettered inspection and disarmament with threats of invasion. On 20 March 2003, a multinational alliance containing the armed forces of the United States and United Kingdom launched an invasion of Iraq. After the withdrawal of U.S. troops from Iraq in 2011, a number of failed Iraqi peace initiatives were revealed.\nBackground.\nIn the decade following the 1991 Gulf War, the United Nations passed 16 Security Council resolutions calling for the complete elimination of Iraqi weapons of mass destruction. Member states communicated their frustration over the years that Iraq was impeding the work of the UN Special Commission and failing to take seriously its disarmament obligations. Iraqi security forces had on several occasions physically prevented weapons inspectors from doing their job and in at least one case, took documents from them.\nOn 29 September 1998, the United States Congress passed the \"Iraq Liberation Act\" supporting the efforts of Iraqi opposition groups to remove Saddam Hussein from office. The Act was signed by President Clinton on 31 October 1998. On the same day, Iraq announced it would no longer cooperate with United Nations weapons inspectors.\nThe UN, under Kofi Annan, brokered a deal wherein Iraq would allow weapons inspectors back into the country. Iraq ceased cooperating with inspectors only days later. The inspectors left the country in December. Inspectors returned the following year as part of The United Nations Monitoring, Verification and Inspection Commission (UNMOVIC).\nPaul Wolfowitz, the military analyst for the United States Department of Defense under Ronald Reagan, had formulated a new foreign policy with regard to Iraq and other \"potential aggressor states\", dismissing \"containment\" in favor of \"preemption\", with the goal of striking first to eliminate threats.\nThis policy was short-lived, however, and Clinton, along with George H. W. Bush, Colin Powell, and other former Bush administration officials, dismissed calls for preemption in favor of continued containment. This was the policy of George W. Bush as well for his first several months in office. The September 11, 2001 attacks brought to life Wolfowitz's and other \"hawks'\" advocacy for preemptive action; Iraq was widely agreed to be a likely subject of this new policy. Powell continued to support the philosophy behind containment.\nFollowing the Gulf War, the Iraqi Army was reduced to 23 divisions with a total of about 375,000 troops. The Iraqi Air Force was reduced to less than 300 aircraft. The Iraqi Navy was almost completely destroyed, and its few remaining operational vessels were in a poor state of repair, the crews were estimated to be in a poor state of readiness, and its capabilities were reduced to that of limited mining and raiding missions. Any rebuilding that was done went into the Republican Guard, and the formation of the Special Republican Guard.\n2002\u20132003.\nDuring late 2002 and into 2003, the United States government continued to call for \"regime change\" in Iraq and threatened to use military force to overthrow the Iraqi government unless Iraq rid itself of all weapons of mass destruction (WMD) it supposedly possessed and convinced the UN that it had done so.\nUS diplomatic pressure to bring Iraq to compliance quickly created a diplomatic crisis in the UN, where some members were in agreement with the U.S. position, while others dissented, notably the permanent Security Council members France, Russia and the People's Republic of China, and fellow NATO members Germany and Belgium.\nThe Bush administration began a military buildup in the region, and after pushing hard gained passage of UN Security Council Resolution 1441. Led by Hans Blix, Head of the United Nations Monitoring, Verification and Inspection Commission (UNMOVIC) and Mohamed ElBaradei Director General of the International Atomic Energy Agency (IAEA), the Resolution brought weapons inspectors back to Iraq in November 2002.\nInspectors began visiting sites where WMD production was suspected, but found no evidence of such activities, except for 18 undeclared empty 122\u00a0mm chemical rockets that were destroyed under UNMOVIC supervision. https:// Inspectors also found that the Al-Samoud-2 and Al-fatah missiles violated the UN range restrictions, the former also being partially destroyed under UNMOVIC supervision.\nOn March 7, 2003 Hans Blix reported accelerated cooperation throughout the month of February but it was still not \"immediate\" and \"unconditional\" as called for by UN Security Council Resolution 1441. He informed the UN security council that \"it will not take years, nor weeks, but months\" to verify whether Iraq had complied with its disarmament obligations.\nU.S. president George W. Bush and British prime minister Tony Blair met in the Azores islands for an \"emergency summit\" over the weekend of March 15\u201316, 2003, after which Bush declared that, despite Blix's report, \"diplomacy had failed\" to compel Iraq to comply with UN Resolution inspection requirements, and stated his intention to use military force to attack Iraq in what was, according to the Bush administration, compliance with the threat of \"serious consequences\" in UN 1441.\nUNSC disagreement.\nSeveral close allies of the U.S. (e.g. Germany, Belgium and France) opposed a military intervention because they asserted it would increase rather than decrease the risk of terrorist attacks. Although the British government and some governments of other members of the EU and NATO supported the US position, opinion polls show that in general their populations were against an attack, especially an attack without clear UN Security Council support. Millions of people in the major cities of Europe, and hundreds of thousands in major cities of North America, participated in peace marches on 15 February 2003.\nStatements by President Bush.\nOn 7 October 2002 President Bush stated:\n\"Eleven years ago, as a condition for ending the Persian Gulf War, the Iraqi regime was required to destroy its weapons of mass destruction, to cease all development of such weapons, and to stop all support for terrorist groups. The Iraqi regime has violated all of those obligations. It possesses and produces chemical and biological weapons. It is seeking nuclear weapons. It has given shelter and support to terrorism, and practices terror against its own people. The entire world has witnessed Iraq's eleven-year history of defiance, deception and bad faith.\"\nOn 17 March 2003 Bush stated in an address to the nation:\n\"Intelligence gathered by this and other governments leaves no doubt that the Iraq regime continues to possess and conceal some of the most lethal weapons ever devised. This regime has already used weapons of mass destruction against Iraq's neighbors and against Iraq's people.\"\nTwo days later on March 19, 2003, as the invasion of Iraq began, Bush stated in an address to the nation:\n\"\"My fellow citizens, at this hour, American and coalition forces are in the early stages of military operations to disarm Iraq, to free its people and to defend the world from grave danger.\"\nStatement by Russian president Vladimir Putin.\nOn October 11, 2002, Russian president Vladimir Putin met with then British prime minister Tony Blair. At a news conference, he said:\n \"Russia does not have in its possession any trustworthy data that supports the existence of nuclear weapons or any weapons of mass destruction in Iraq and we have not received any such information from our partners as yet.\"\nStatements by French president Jacques Chirac.\nIn a February 2003 joint declaration by Russia, Germany and France, Jacques Chirac remarked:\n \"As far as France is concerned, we are ready to envisage everything that can be done under UNSCR 1441. ... But I repeat that every possibility offered by the present resolution must be explored, that there are a lot of them and they still leave us with a lot of leeway when it comes to ways of achieving the objective of eliminating any weapons of mass destruction which may exist in Iraq. I'd like nevertheless to note that, as things stand at the moment, I have, to my knowledge, no indisputable proof in this sphere.\"\nLegality.\nAuthority under international law.\nThe position of whether the invasion was legal under international law is unclear. Article 2 of the United Nations Charter forbids UN members from employing \"the threat or use of force\" against other states in a manner inconsistent with the purposes of the United Nations. Two exceptions exist to the rule: self-defense (Article 51) or an authorization by the Security Council to protect international peace and security (Chapter VII).\nThe government of the United States said publicly, and the British pledged privately, that they were willing to invade Iraq with or without Security Council authorization.\nThere have been two military actions carried out with the approval of the Security Council. These two instances were the Korean War and the 1991 Gulf War.\nThe United States does not recognize the jurisdiction of any international court over its citizens or military, holding that the United States Supreme Court is its final authority. One example of this policy is that the United States did not ratify the International Criminal Court (ICC) treaty, and on 6 May 2002 it informed the UN that it has no intention to do so.\nAs of 24 February 2005 neither Iraq nor the United States have ratified the ICC treaty, and therefore neither the US attack on Iraq nor subsequent actions in Iraq fall under the jurisdiction of the ICC. The actions of signatories such as the United Kingdom and Spain could however fall under the ICC jurisdiction.\nOn March 17, 2003, Peter Goldsmith, Attorney General for England and Wales, set out his government's legal justification for an invasion of Iraq. He said that the 1990 Security Council Resolution 678 authorised force against Iraq, which was suspended but not terminated by the 1991 Resolution 687, which imposed continuing obligations on Iraq to eliminate its weapons of mass destruction. A material breach of resolution 687 would revive the authority to use force under resolution 678. In Resolution 1441 the Security Council determined that Iraq was in material breach of resolution 687 because it had not fully carried out its obligations to disarm, and in early 2003 sent teams of weapons inspectors to verify the facts on the ground.\nMost member governments of the United Nations Security Council made clear that in their view, after resolution 1441 there was still no authorization for the use of force and that the invasion was illegal under international law. However, the US and its allies argued that no resolution authorizing the invasion would be necessary as they acted in self-defense under Article 51 of the UN Charter and by customary international law. The exercise of that right could not be banned by ceasefire. Since Iraq was not actively disarming themselves of its alleged WMDs and hid them from UN inspectors, the US and its allies claimed they had the right to assume that Iraq was holding WMDs. If the UN failed to force compliance, the US and the UK - as parties of the 1991 conflict - would invade Iraq without the UN, as they had already done in their intervention in the Kosovo War. Yoram Dinstein equates this to police officers cornering a convicted violent felon and saying \"put your hands on your head\", but instead he pulls something small and black (whether a gun or not) out of his pocket. Officers would have been justified in shooting him because he could have possessed something that is dangerous.\nAuthority under US Constitution.\nThe Constitution grants the power to declare war exclusively to the United States Congress, but declares the President to be Commander-in-Chief of the US military. Because of this division of power, there has long been controversy regarding the authority of the president outside of a declared war. Nonetheless, of the many instances the United States has exercised force outside its borders, only five have been as part of a declared war.\nIn 1973, amid increasing domestic controversy about the Vietnam War, Congress passed the War Powers Resolution to limit the ability of the president to undertake prolonged military action without Congressional authority. No president since has recognized the constitutionality of this act, and most legal scholars believe it would not survive a challenge in court.\nTo avoid initiating a crisis under the War Powers Resolution, the Bush administration sought explicit approval from the Congress to exercise force in Iraq. On October 9, 2002, the Congress passed the Iraq Resolution which explicitly authorized the president to use the Armed Forces of the United States as he determines to be necessary and appropriate. This raises the issue of whether or not Congress has the authority to delegate legislative power to the executive branch. However, in a recent United States Supreme Court case, \"Hamdan v. Rumsfeld,\" the Supreme Court ruled that the military commissions that the President had established, (and defended by arguing that he was given the power to create military courts by this resolution), were unconstitutional because they were unauthorized by Congress.\nThe Constitution also provides that international treaties ratified by the United States are among the highest law of the land (US Constitution, Article VI). The UN Charter is a treaty ratified by the US, which forbids member states, including the US, from attacking fellow member states, including Iraq, except in two carefully circumscribed situations (see UN Charter).\nAftermath.\nAfter the invasion of Iraq, the Iraq Survey Group, headed by David Kay was formed to find the alleged weapons of mass destruction. Apart from a small quantity of degraded pre-1991 shells, nothing was found.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "66437", "revid": "33625371", "url": "https://en.wikipedia.org/wiki?curid=66437", "title": "Basilica of Saint-Denis", "text": "Historic church in Saint-Denis, Paris, France\nThe Basilica of Saint-Denis (, now formally known as the ) is a large former medieval abbey church and cathedral in the commune of Saint-Denis, a northern suburb of Paris. The building is of singular importance historically and architecturally as its choir, completed in 1144, is widely considered the first structure to employ all of the elements of Gothic architecture.\nThe basilica became a place of pilgrimage and a necropolis containing the tombs of the kings of France, including nearly every king from the 10th century to Louis XVIII in the 19th century. Henry IV of France came to Saint-Denis formally to renounce his Protestant faith and become a Catholic. The queens of France were crowned at Saint-Denis, and the regalia, including the sword used for crowning the kings and the royal sceptre, were kept at Saint-Denis between coronations.\nThe site originated as a Gallo-Roman cemetery in late Roman times. The archaeological remains still lie beneath the cathedral; the graves indicate a mixture of Christian and pre-Christian burial practices. Around the year 475, St. Genevieve purchased some land and built Saint-Denys de la Chapelle. In 636, on the orders of Dagobert I, the relics of St. Denis, a patron saint of France, were reinterred in the basilica. The relics of St. Denis, which had been transferred to the parish church of the town in 1795, were brought back again to the abbey in 1819.\nIn the 12th century, the abbot Suger rebuilt portions of the abbey church using innovative structural and decorative features. In doing so, he is said to have created the first truly Gothic building. In the following century the master-builder Pierre de Montreuil rebuilt the nave and the transepts in the new Rayonnant Gothic style.\nThe abbey church became a cathedral on the formation of the Diocese of Saint-Denis by Pope Paul VI in 1966 and is the seat of the Bishop of Saint-Denis, currently (since 2024) \u00c9tienne Guillet. Although known as the \"Basilica of St Denis\", the cathedral has not been granted the title of Minor Basilica by the Vatican.\nThe spire, dismantled in the 19th century, is to be rebuilt. The project initiated more than 30 years ago, was decided in 2018 with a signed agreement, with initial restoration work beginning in 2022. From 2025, the building project will commence, with visitors of the cathedral being able to observe the building works as part of their tour. The project is planned to be completed by 2029, with a cost of 37 million euro.\nHistory.\nEarly churches.\nThe cathedral is situated on the site where Saint Denis, the first bishop of Paris, is believed to have been buried. According to the \"Life of Saint Genevieve\", written in about 520, he was sent by Pope Clement I to evangelise the Parisii, but was arrested and condemned by the Roman authorities. Along with two of his followers, the priest Rusticus and deacon Eleutherus, he was decapitated on the hill of Montmartre in about 250. According to the legend, he is said to have carried his head four leagues to the Roman settlement of Catulliacus, the site of the current church, and indicated that it was where he wanted to be buried. A martyrium was erected on the site of his grave in about 313, and was enlarged into a basilica with the addition of tombs and monuments under Saint Genevieve. These included a royal tomb, that of Aregonde, the wife of King Clothar I.\nDagobert I, King of the Franks (reigned 628 to 637), transformed the church into the Abbey of Saint Denis, a Benedictine monastery in 632. It soon grew to a community of more than five hundred monks, plus their servants.Dagobert also commissioned a new shrine to house the saint's remains, which was created by his chief councillor, Eligius, a goldsmith by training. An early \"vita\" of Saint Eligius describes the shrine:\nAbove all, Eligius fabricated a mausoleum for the holy martyr Denis in the city of Paris with a wonderful marble ciborium over it marvelously decorated with gold and gems. He composed a crest [at the top of a tomb] and a magnificent frontal and surrounded the throne of the altar with golden axes in a circle. He placed golden apples there, round and jeweled. He made a pulpit and a gate of silver and a roof for the throne of the altar on silver axes. He made a covering in the place before the tomb and fabricated an outside altar at the feet of the holy martyr. So much industry did he lavish there, at the king's request, and poured out so much that scarcely a single ornament was left in Gaul, and it is the greatest wonder of all to this very day.\nThe Carolingian church.\nDuring his second coronation at Saint-Denis after Soissons, King Pepin the Short made a vow to rebuild the old abbey. The first church mentioned in the chronicles was begun in 754 and completed under Charlemagne, who was present at its consecration in 775. By 832, the abbey had been granted a remunerative whaling concession on the Cotentin Peninsula.\nAccording to one of the abbey's many foundation myths a leper, who was sleeping in the nearly completed church the night before its planned consecration, witnessed a blaze of light from which Christ, accompanied by St. Denis and a host of angels, emerged to conduct the consecration ceremony himself. Before leaving, Christ healed the leper, tearing off his diseased skin to reveal a perfect complexion underneath. A mis-shapen patch on a marble column was said to be the leper's former skin, which stuck there when Christ discarded it. Having been consecrated by Christ, the fabric of the building was itself regarded as sacred.\nMost of what is now known about the Carolingian church at Saint-Denis resulted from a lengthy series of excavations begun under the American art historian Sumner McKnight Crosby in 1937. The structure altogether was about eighty meters long, with an imposing facade, a nave divided into three sections by two rows of marble columns, a transept, and apse and at the east end. During important religious celebrations, the interior of the church was lit with 1250 lamps. Beneath the apse, in imitation of St. Peter's in Rome, a crypt was constructed, with a Confession, or martyr's chapel, in the center. Inside this was a platform on which the sarcophagus of Denis was displayed, with those of his companions Rusticus and Eleutherus on either side. Around the platform was a corridor where pilgrims could circulate, and bays with windows. Traces of painted decoration of this original crypt can be seen in some of the bays.\nThe crypt was not large enough for the growing number of pilgrims who came, so in about 832 the abbot Hilduin built a second crypt, to the west of the first, and a small new chapel dedicated to the Virgin Mary was constructed next to the apse. The new crypt was extensively rebuilt under Suger in the 12th century.\nSuger and the Early Gothic Church (12th century).\nAbbot Suger (c. 1081 \u2013 1151), the patron of the rebuilding of the abbey church, had begun his career in the church at the age of ten, and rose to become the abbot in 1122. He was a school companion and then confidant and minister of Louis VI and then of his son Louis VII, and was a regent of Louis VII when the king was absent on the Crusades. He was an accomplished fundraiser, acquiring treasures for the cathedral and collecting an enormous sum for its rebuilding. In about 1135 he began reconstructing and enlarging the abbey. In his account of the work undertaken, Suger explained his decision to rebuild the church was due to the decrepit state of the old structure and its inability to cope with the crowds of pilgrims visiting the shrine of St. Denis.\nIn the 12th century, thanks largely to Suger, the Basilica became a principal sanctuary of French Royalty, rivalling Reims Cathedral where other French kings were crowned. The abbey also kept the coronation regalia, including the robes, crowns and sceptre. Beginning in 1124, and until the mid-15th century, the kings departed for war carrying the oriflamme, or battle flag, of St. Denis, to give the king the protection of the Saint. It was taken to the abbey only when France was in danger. The flag was retired in 1488, when the Parisians opened the gates of Paris to invading English and Burgundian armies.\nFirst Phase: the west front (1135\u20131140).\nSuger began his rebuilding project at the western end of St. Denis, demolishing the old Carolingian fa\u00e7ade with its single, centrally located door. He extended the old nave westwards by an additional four bays and added a massive western narthex, incorporating a new fa\u00e7ade and three chapels on the first floor level. In the new design, massive vertical buttresses separated the three doorways and horizontal string-courses and window arcades clearly marked out the divisions. This clear delineation of parts was to influence subsequent west fa\u00e7ade designs as a common theme in the development of Gothic architecture and a marked departure from the Romanesque. The portals themselves were sealed by gilded bronze doors, ornamented with scenes from Christ's Passion. They clearly recorded Suger's patronage with the following inscription:\nOn the lintel below the great tympanum showing the Last Judgement, beneath a carved figure of the kneeling Abbot, was inscribed the more modest plea;\nReceive, stern Judge, the prayers of your Suger, Let me be mercifully numbered among your sheep.\nSecond Phase: the new choir (1140\u20131144).\nSuger's western extension was completed in 1140 and the three new chapels in the narthex were consecrated on 9 June of that year, but the Romanesque nave between was yet unchanged. He wrote about the new narthex at the west end and proposed chapels at the east: \"Once the new rear part is joined to the part in front, the church shines with its middle part brightened. For bright is that which is brightly coupled with the bright, and bright is the noble edifice which is pervaded by the new light.\"\nSuger's great innovation in the new choir was the replacement of the heavy dividing walls in the apse and ambulatory with slender columns, so that the interior of that part of the church was filled with light. He described \"A circular string of chapels, by virtue of which the whole church would shine with the wonderful and uninterrupted light of most luminous windows, pervading the interior beauty.\"\nOne of these chapels was dedicated to Saint Osmanna, and held her relics.\nSuger's masons drew on elements which evolved or had been introduced to Romanesque architecture: the rib vault with pointed arches, and exterior buttresses which made it possible to have larger windows and to eliminate interior walls. It was the first time that these features had all been drawn together; and the new style evolved radically from the previous Romanesque architecture by the lightness of the structure and the unusually large size of the stained glass windows.\nThe new architecture was full of symbolism. The twelve columns in the choir represented the twelve Apostles, and the light represented the Holy Spirit.\nLike many French clerics in the 12th century AD, he was a follower of Pseudo-Dionysius the Areopagite, a 6th-century mystic who equated the slightest reflection or glint with divine light. Suger's own words were carved in the nave: \"For bright is that which is brightly coupled with the bright/and bright is the noble edifice which is pervaded by the new light.\" Following Suger's example, large stained glass windows filling the interior with mystical light became a prominent feature of Gothic architecture.\nTwo different architects, or master masons, were involved in the 12th century rebuilding. Both remain anonymous but their work can be distinguished on stylistic grounds. The first, who was responsible for the initial work at the western end, favoured conventional Romanesque capitals and moulding profiles with rich and individualised detailing. His successor, who completed the western facade and upper storeys of the narthex, before going on to build the new choir, displayed a more restrained approach to decorative effects, relying on a simple repertoire of motifs, which may have proved more suitable for the lighter Gothic style that he helped to create.\nThe Portal of Valois was the last of the Gothic structures planned by Suger. It was designed for the original building, but was not yet begun when Suger died in 1151. In the 13th century it was moved to the end of the new transept on the north side of the church. The sculpture of the portal includes six standing figures in the embracements and thirty figures in the voussures, or arches, over the doorway, which probably represent the kings of the Old Testament. The scene in the Tympanum over the doorway depicts the martyrdom of Saint Denis. In their realism and finesse, they were a landmark in Gothic sculpture.\nThe new structure was finished and dedicated on 11 June 1144, in the presence of the king. The Abbey of St Denis thus became the prototype for further building in the royal domain of northern France. Through the rule of the Angevin dynasty, the style was introduced to England and spread throughout France, the Low Countries, Germany, Spain, northern Italy and Sicily.\nReconstruction of the Nave (13th century).\nSuger died in 1151 with the Gothic reconstruction incomplete. In 1231, Abbot Odo Clement began work on the rebuilding of the Carolingian nave, which remained sandwiched incongruously between Suger's Gothic works to the east and west. Both the nave and the upper parts of Suger's choir were replaced in the Rayonnant Gothic style. From the start it appears that Abbot Odo, with the approval of the regent Blanche of Castile and her son, the young King Louis IX, planned for the new nave and its large crossing to have a much clearer focus as the French 'royal necropolis', or burial place. That plan was fulfilled in 1264 under Abbot Matthew of Vend\u00f4me when the bones of 16 former kings and queens were relocated to new tombs arranged around the crossing, eight Carolingian monarchs to the south and eight Capetians to the north. These tombs, featuring lifelike carved recumbent effigies or \"gisants\" lying on raised bases, were badly damaged during the French revolution though all but two were subsequently restored by Viollet le Duc in 1860.\nThe dark Romanesque nave, with its thick walls and small window-openings, was rebuilt using the very latest techniques, in what is now known as Rayonnant Gothic. This new style, which differed from Suger's earlier works as much as they had differed from their Romanesque precursors, reduced the wall area to an absolute minimum. Solid masonry was replaced with vast window openings filled with brilliant stained glass (all destroyed in the Revolution) and interrupted only by the most slender of bar tracery\u2014not only in the clerestory but also, perhaps for the first time, in the normally dark triforium level. The upper facades of the two much-enlarged transepts were filled with two spectacular 12-metre-wide rose windows. As with Suger's earlier rebuilding work, the identity of the architect or master mason remains unknown. Although often attributed to Pierre de Montreuil, the only evidence for his involvement is an unrelated document of 1247 which refers to him as 'a mason from Saint-Denis'.\n15th\u201317th century.\nDuring the following centuries, the cathedral was pillaged twice; once during the Hundred Years' War (1337\u20131453) and again during the Wars of Religion (1562\u20131598). Damage was largely limited to broken tombs and precious objects stolen from the altars and treasury. Many modifications were made under Marie de' Medici and later royal families. These included the construction of chapel adjoining the north transept to serve as a tomb for the monarchs of the Valois dynasty (later demolished). A plan of c.\u20091700 by F\u00e9libien shows the Valois Chapel, a large mortuary chapel in the form of a domed colonnaded \"rotunda\", adjoining the north transept of the basilica and containing the tomb of the Valois. and the display of the skeleton of a baleine whale in the nave in 1771. Greater harm was done with the removal of the early Gothic column-statues which Suger had used to decorate the west front. (They were replaced with replicas in the 19th century). In 1700, reconstruction began of the monastic buildings adjacent to the church. This was not completed until the mid-18th century. Into these buildings Napoleon installed a school for the daughters of members of the French Legion of Honour, which still is in operation.\nThe French Revolution and Napoleon.\nDue to its connections to the French monarchy and proximity to Paris, the abbey of Saint-Denis was a prime target of revolutionary vandalism. On Friday, 14 September 1792, the monks celebrated their last services in the abbey church; the monastery was dissolved the next day. The church was used to store grain and flour. In 1793, the National Convention, the revolutionary government, ordered the violation of the sepulchres and the destruction of the royal tombs, but agreed to create a commission to select those monuments which were of historical interest for preservation. In 1798, these were transferred to the chapel of the Petit-Augustins, which later became the Museum of French Monuments.\nMost of the medieval monastic buildings were demolished in 1792. Although the church itself was left standing, it was profaned, its treasury confiscated and its reliquaries and liturgical furniture melted down for their metallic value. Some objects, including a chalice and aquamanile donated to the abbey in Suger's time, were successfully hidden and survive to this day. The jamb figures of the fa\u00e7ade representing Old Testament royalty, mistakenly identified as images of royal French kings and queens, were removed from the portals and the tympana sculpture defaced.\nIn 1794, the government decided to remove the lead tiles from the roof, to melt them down to make bullets. This left the interior of the church badly exposed to the weather.\n19th century \u2013 reconstruction and renovation.\nThe church was reconsecrated by Napol\u00e9on in 1806, and he designated it as the future site for his own tomb and those of his intended dynasty. He also ordered the construction of three chapels to honour the last French kings, created a chapel under the authority of his uncle, Cardinal Fesch, which was decorated with richly-carved choir stalls and marquetry from the Ch\u00e2teau de Gaillon. (See \"Choir Stalls\" section below).\nAfter Napoleon's downfall, the ashes of the previous king, Louis XVI, were ceremoniously moved from the cemetery of the Madeleine to Saint-Denis. The last king to be entombed in Saint-Denis was Louis XVIII in 1824.\nIn 1813 Fran\u00e7ois Debret was named the chief architect of the cathedral; he proceeded, over thirty years, to repair the Revolutionary damage. He was later best known for his design of the Salle Le Peletier, the primary opera house of Paris before the Op\u00e9ra Garnier in 1873. He replaced the upper stained-glass windows in the nave with depictions of the historic kings of France, and added new windows to the transept depicting the renovation, and the July 1837 visit to the Cathedral of King Louis Philippe. On 9 June, the spire of the tower was struck by lightning and destroyed. Debret rapidly put into place a new spire, but he did not fully understand the principles of Gothic architecture. His improperly built structure was heavily impacted by two hurricanes which occurred in 1842 and 1843, as well as the tornado of 1845. After inspection, Debret was ordered to dismantle the spire due to safety concerns. \nDebret resigned and was replaced by Eug\u00e8ne Viollet-le-Duc, who had the support of Prosper M\u00e9rim\u00e9e, the French author who led campaign for the restoration of ruined Gothic architecture in France. Viollet-le-Duc continued working on the abbey until his death in 1879, and replaced many of the creations conceived by Debret. Viollet-le-Duc focused on the tombs, rearranging and transforming portions of the interior into a vast museum of French sculpture. In the 1860s Emperor Napoleon III asked Viollet-le-Duc to construct an imperial section in the crypt for him and his dynasty, but he was deposed and went into exile before it was begun.\n20th and 21st centuries.\nIn 1895, when the chapter created by Napoleon was dissolved, the church lost its cathedral rank and reverted to being a parish church. It did not become a cathedral again until 1966, with the creation of the new diocese of Saint-Denis. The formal title is now the \"Baslilique-cath\u00e9drale de Saint-Denis\".\nIn December 2016, 170 years after the north tower's dismantlement and following several false starts, the Ministry of Culture again proposed its reconstruction after concluding it was technically feasible\u2014albeit without public funding. An association, \"Suivez la fl\u00e8che\" (\"Follow the Spire\"), chaired by Patrick Braouezec, has since been established to support the reconstruction, with the aim of raising the necessary funds by opening the reconstruction works to the general public, along the model of the Gu\u00e9delon Castle. In March 2018, the culture ministry signed an accord with the association, officially launching the reconstruction project, with works expected to commence in May 2020. A year later, French scholars were still divided on the \u20ac25 million proposal to reconstruct the spire. In 2023, hundreds of anonymous graves dating from the 5th to the 14th centuries were discovered in the Basilica. In the same year, the Basilica's stained glass windows which have been the central focus of a project spanning 25 years, entered the final stage of restoration with a total cost exceeding \u20ac2 million. The spire reconstruction project commenced after the site's inauguration on 14 March 2025, which was attended by Minister of Culture Rachida Dati, and is expected to continue for five years.\nExterior.\nThe west front.\nThe west front of the church, dedicated on 9 June 1140, is divided into three sections, each with its own entrance, representing the Holy Trinity. A crenellated parapet runs across the west front and connects the towers (still unfinished in 1140), illustrating that the church front was the symbolic entrance to the celestial Jerusalem.\nThis new fa\u00e7ade, wide and deep, has three portals, the central one larger than those on either side, reflecting the relative width of the central nave and lateral aisles. This tripartite arrangement was clearly influenced by the late-11th-century Norman-Romanesque fa\u00e7ades of the abbey churches of St Etienne. It also shared with them a three-storey elevation and flanking towers. Only the south tower survives; the north tower was dismantled following a tornado which struck in 1846.\nThe west front was originally decorated with a series of column statues, representing the kings and queens of the Old Testament. These were removed in 1771 and were mostly destroyed during the French Revolution, though a number of the heads can be seen in the Mus\u00e9e de Cluny in Paris.\nThe bronze doors of the central portal are modern, but are a faithful reproduction of the original doors, which depicted the Passion of Christ and the Resurrection.\nOne other original feature was added by Suger's builders; a rose window over the central portal. Although small circular windows (oculi) within triangular tympana were common on the west facades of Italian Romanesque churches, this was probably the first example of a rose window within a square frame, which was to become a dominant feature of the Gothic facades of northern France (soon to be imitated at Chartres Cathedral and many others).\nChevet and transepts.\nThe chevet, at the east end of the cathedral, was one of the first parts of the structure rebuilt into the Gothic style. The work was commissioned by Abbot Suger in 1140 and completed in 1144. It was considerably modified under the young King Louis IX and his mother, Blanche of Castille, the Regent of the Kingdom, beginning in 1231. The apse was built much higher, along with the nave. Large flying buttresses were added to the chevet, to support the upper walls, and to make possible the enormous windows installed there. The masons used the same engineering concept that was used at the Abbey of Saint-Martin-des-Champs to support the large chapel windows. At the same time, the transept was enlarged and given large rose windows in the new rayonnant style, divided into multiple lancet windows topped by trilobe windows and other geometric forms inscribed in circles. The walls of the nave on both sides were entirely filled with windows, each composed of four lancets topped by a rose, filling the entire space above the triforium. The upper walls, like the chevet, were supported by flying buttresses whose bases were placed between the chapels alongside the nave.\nNorth and south portals.\nThe Porte de Valois, or north portal, was originally built in the 12th century, near the end of Suger's life, then rebuilt at the end of the north transept in the 13th century. According to Suger, the original entrance on the north did not have sculpture, but mosaic, which Suger replaced by sculpture in 1540. It is considered an important step in the history of Gothic sculpture, because of the skill of the carving, and the lack of rigidity of the figures. There are six figures in the embrasures and thirty figures in the voussures, or arches above the door, which represent kings, probably those of the Old Testament, while the tympanum over the door illustrates the martyrdom of Saint-Denis and his companions Eleuthere and Rusticus. This portal was among the last works commissioned by Suger; he died in 1151, before it was completed.\nThe original sculpture that was destroyed in the Revolution was replaced with sculpture from the early 19th century, made by Felix Brun.\nThe tympanum of the south portal illustrates the last days of the Denis and his companions before their martyrdom. The piedroits are filled with medallions representing the labours of the days of month.\nInterior.\nThe nave and choir.\nThe nave, the portion to the west of the church reserved for ordinary worshippers, and the choir, the portion to the east reserved for the clergy, were rebuilt into the Gothic style in the 13th century, after the apse at the east and the west front. Like the other Gothic churches in the Ile-de-France, its walls had three levels; large arcades of massive pillars on the ground floor; a narrow triforium or passageway midway up the wall; originally windowless; and a row of high windows the clerestory, above. Slender columns rose from the pillars up the walls to support the four-part rib vaults. As a result of the Rayonnant reconstruction in the triforium was given windows, and the upper walls were entirely filled with glass, which reached upward into the arches of the vaults, flooding the church with light.\nThe disambulatory and chapels.\nThe chevet had been constructed by Suger in record time, in just four years, between 1140 and 1144, and was one of the first great realisations of Gothic architecture. The double disambulatory is divided not by walls but by two rows of columns, while the outside walls, thanks to buttresses on the exterior, are filled with windows. The new system allowed light to pass into the interior of the choir. The disambulatory connects with the five radiating chapels at the east end of the cathedral, which have their own large windows. To give them greater unity, the five chapels share the same system of vaulted roofs. To make the walls between the chapels even less visible, they are masked with networks of slender columns and tracery.\nThe apse with its two ambulatories and axial chapels was extensively rebuilt in the 12th century, to connect harmoniously with the new and larger nave, but a major effort was made to save the early Gothic features created by Suger, including the double disambulatory with its large windows. To accomplish this, four large pillars were installed in the crypt to support the upper level, and the walls of the first traverse of the sanctuary were placed at an angle to connect with the wider transept.\nThe basilica retains stained glass of many periods (although most of the panels from Suger's time have been removed for long-term conservation and replaced with photographic transparencies), including exceptional modern glass, and a set of twelve misericords.\nCrypt and royal tombs.\nThe role of St. Denis as the necropolis of French kings formally began under Hugh Capet (987\u2013996), but the tombs of several earlier kings were already located there. The site was chosen because of its association with St. Denis, the first Bishop of Paris and one of the earliest Christian leaders in France, who was buried there. All but three of the monarchs of France from the 10th century until 1793 have their remains here. The remains of some monarchs, including Clovis I (465\u2013511), were moved to St. Denis from other churches.\nThe crypt beneath the church is divided into two sections; the older, called archeological crypt, is located under the transept, and was originally built in about 775, when the abbey was reconstructed by Abbot Fuldiad. It had a disambulatory, passage which allowed pilgrims to circulate around the relics of Saint Denis and his companions on display in the center. It was lit by alternating small windows in the walls and lamps placed in niches.\nThe crypt was rebuilt and extended eastward by Suger. The walls were decorated with blind arches, divided by columns whose capitals illustrate Biblical scenes and scenes from the life of St. Denis. Thirty-nine of the original Romanesque sixty-two capitals are still in place. Suger constructed a new disambulatory connected with radiating chapels.\nDuring the reign of Henry IV, the central portion of this crypt was devoted the Bourbon dynasty, but the tombs themselves were simple lead coffins in wood cases. The effigies of many of the kings and queens are on their tombs, but during the French Revolution their bodies were thrown out of their coffins, dumped into three trenches and covered with lime to destroy them. The older monarchs were removed in August 1793 to celebrate the revolutionary Festival of Reunion, the Valois and Bourbon monarchs in October 1793 to celebrate the execution of Marie Antoinette. Preservationist Alexandre Lenoir saved many of the monuments by claiming them as artworks for his Museum of French Monuments. The bodies of several Plantagenet monarchs of England were likewise removed from Fontevraud Abbey during the French Revolution. Napoleon Bonaparte reopened the church in 1806, but left the royal remains in their mass graves. In 1817 the restored Bourbons ordered the mass graves to be opened, but only portions of three bodies remained intact. The remaining bones from 158 bodies were collected into an ossuary in the crypt of the church, behind marble plates bearing their names.\nIn later years, tombs were placed along the aisles that surrounded around the choir and the nave. In the 13th century King Louis IX (Saint Louis) commissioned a number of important tombs of earlier kings and French historical figures, whose remains were collected from other churches. These included the tombs of Clovis I, Charles Martel, Constance of Castile, Pepin the Short, Robert the Pious and Hugh Capet (which disappeared during the Revolution). The new tombs were all made in the same style and costume, with a reposing figure holding a staff, to illustrate the continuity of the French monarchy.\nThe tombs of the Renaissance expressed are theatrical and varied. The largest is that of Louis XII (died 1515) and his wife, Anne of Brittany (died 1514). It takes the form of a white marble temple filled and surrounded with figures. Inside it, the king and queen are depicted realistically in their dying agonies. Allegorical figures seated around the temple depict the virtues of the king and queen. On the roof of the tomb, the king and queen are shown again, kneeling and calmly praying, celebrating their victory over death, thanks to their virtues.\nThe monument to Henry II of France and Catherine de Medici (1559) followed a similar format; a Roman temple, in this case designed by the celebrated Renaissance architect Primatrice with sculpture on the roof depicting the king and queen in prayer. The king places his hand on his heart illustrating his Catholic faith a period of religious conflicts.\nIn the 19th century, following the restoration of the monarchy, Louis XVIII had the remains of Louis XVI and Marie-Antoinette brought to St. Denis. The body of the Dauphin, who died of illness and neglect at the hands of his revolutionary captors, was buried in an unmarked grave in a Parisian churchyard near the Temple. During Napoleon's exile in Elba, the restored Bourbons ordered a search for the corpses of Louis XVI and Marie Antoinette. They were found on 21 January 1815, brought to Saint-Denis and placed in the archeologi crypt. Their tombs are covered with black marble slabs installed in 1975.\nLouis XVIII, upon his death in 1824, was buried in the centre of the crypt, near the graves of Louis XVI and Marie Antoinette. The coffins of royal family members who died between 1815 and 1830 were also placed in the vaults. Under the direction of architect Viollet-le-Duc, church monuments that had been taken to the Museum of French Monuments were returned to the church. The corpse of Louis VII, who had been buried at Barbeau Abbey and whose tomb had not been touched by the revolutionaries, was brought to Saint-Denis and buried in the crypt. In 2004, the mummified heart of the Dauphin, the boy who would have been Louis XVII, verified to be authentic by DNA testing, was placed in a crystal vase and sealed into the wall of the crypt.\nSacristy.\nThe Sacristy, the room where the clergy traditionally donned their vestments, was transformed by the architect Jacques Cellerier in 1812 into a Neo-classical gallery of murals which depict scenes from the history of the cathedral. A work added to the Sacristy is \"Allegory of the Divine Word\", a painting by Simon Vouet, which originally had been commissioned by Louis XIII for the retable of the Chateau of Saint-Germain-en-Laye. It was acquired for the cathedral by the administration of national monuments in 1993. The wall cases also display a selection of precious objects from the cathedral's collection.\nArt and decoration.\nStained glass.\nAbbot Suger commissioned a large amount of stained glass for the new chevet, but only very small amount of the original glass from the time of Suger survived intact. In the 19th century it was collected by Eug\u00e8ne Viollet-le-Duc, and was integrated into windows in the chevet. Original glass includes the figure of Suger prostrating himself at the feet of the Virgin Mary, in the window called \"The Childhood of Christ\"; and kneeling in the lower right corner of the Tree of Jesse, illustrating the genealogy of Christ, in the Axis chapel; the \"Allegories of Saint Paul\" and \"The Life of Moses\" in the fourth radiating chapel on the north; \"The vision of Ezekiel under the sign of tau\", originally from a group illustrating the Passion of Christ, in the fourth rayonnant chapel on the south, in the left bay and third register. Another piece of original window from Suger's time, depicting mythical Griffonsa a symbol of Paradise, is found in the second radiating chapel on the north. Other scenes which Suger described, showing the pilgrimage of Charlemagne and the Crusades, have disappeared.\nMuch of the current stained glass dates to the 19th century, as the church began to be restored from the damage of the Revolution. The architect Fran\u00e7ois Debret designed the first Neo-Gothic windows of the nave in 1813. these include the upper windows of the nave, which represent the kings and queens of France. Later upper windows of the south transept depict the restoration of the church, and particularly the visit there of Louis Philippe I, the last king of France, in 1837. This large group of windows was designed by the painter Jean-Baptiste Debret, the brother of the architect.\nSculpture.\nThe new west front sculpture of St. Denis had an important influence on Gothic style. The influential features of the new fa\u00e7ade include the tall, thin statues of Old Testament prophets and kings attached to columns (\"jamb figures\") flanking the portals (destroyed in 1771 but recorded in Montfaucon's drawings). These were also adopted at the cathedrals of Paris and Chartres, constructed a few years later, and became a feature of almost every Gothic portal thereafter.\nThe statues on the portal of the Valois, on the transept of the Saint Denis, made in 1175, have very elongated and expressive figures, and also had an important effect on Gothic sculpture. They were the opposite of the more restrained and dignified figures of Chartres Cathedral, made about the same time.\nAbove the doorways, the central tympanum was carved with Christ in Majesty displaying his wounds with the dead emerging from their tombs below. Scenes from the martyrdom of St. Denis were carved above the south (right hand) portal, while above the north portal was a mosaic (lost), even though this was, as Suger put it 'contrary to the modern custom'. Of the original sculpture, very little remains, most of what is now visible being the result of rather clumsy restoration work in 1839. Some fragments of the original sculptures survive in the collection of the Mus\u00e9e de Cluny.\nChoir stalls.\nThe choir stalls, the seats reserved for the clergy, have particularly fine carvings, particularly on the misericord, the small seat on each stall on which the clergy could rest when standing for long periods of time. The stalls were made in the 16th century, and were originally located in the high chapel of the Chateau de Gaillon in the Eure Department. In 1805 Napoleon Bonaparte decided to create three new chapels at Saint-Denis, as well as a chapter of bishops under the authority of his uncle, Cardinal Fesch. The stalls were moved to Saint-Denis and installed for their use. Besides the carved wood, the stalls are decorated with elaborate multi-coloured religious scenes in marquetry.\nOrgan.\nThe organ is located on the tribune, at the west of the nave. An organ is recorded as existing at the basilica in 1520. A later organ, made by Crespin Carlier, is recorded in 1520, but this instrument was destroyed during the French Revolution. The church re-opened in 1806 without an organ. A competition was held in 1833 to find a new builder. It was won by Aristide Cavaill\u00e9-Coll, age twenty-three, and was his first organ. It was completed in 1843, and launched his career as an organ-maker.\nIt contains numerous innovations introduced in the romantic area, in particular the very first Barker lever. With three manuals and pedals, it is protected by the label. It was restored in 1901 by Charles Mutin, and between 1983 and 1987 by Jean-Loup Boisseau and Bertrand Cattiaux. Pierre Pincemaille, sole titular organist for 30 years (between 1987 and 2018), held many recitals (between 1989 and 1995, then between 2014 and 2017), and recorded eight CDs using this instrument.\nTreasury.\nThe cathedral contained an extensive treasury, mainly constituted by the Abbot Suger. It contained crowns (those of Charlemagne, Saint Louis, and Henry IV of France), a cross, and liturgical objects.\nBurials.\n \nKings.\nAll but five of the kings of France were buried in the basilica (with Charlemagne, Philip I, Louis XI, Charles X and Louis Philippe I buried elsewhere), as well as a few other monarchs. The remains of the early monarchs were removed from the destroyed Abbey of St Genevieve. Some of the more prominent monarchs buried in the basilica are:\nReferences and sources.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
