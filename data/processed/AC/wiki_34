{"id": "23590", "revid": "50831540", "url": "https://en.wikipedia.org/wiki?curid=23590", "title": "Pantheism", "text": "Belief that God and reality are identical\nPantheism refers to a diverse family of philosophical and religious beliefs, that equate reality with divinity. Pantheistic concepts date back thousands of years, and pantheistic elements have been identified in diverse religious traditions. Most notably, pantheism refers to the belief that the totality of being\u2014called by various names Nature, universe, cosmos\u2014is a self-organizing unity that needs no distinct creator, and can be met with the same sense of reveration and awe as theists attribute to their gods. \nPantheism is not to be confused with the panentheism, which maintains divinity as an entity greater than the universe out of which the universe arises. \nPantheist belief does not recognize a distinct personal god, anthropomorphic or otherwise, but instead characterizes a broad range of doctrines differing in forms of relationships between reality and divinity. One of the earliest uses of the term \"pantheism\" dates back to mathematician Joseph Raphson in 1697.\nPantheism was popularized in Western culture based on the work of the 17th-century philosopher Baruch Spinoza\u2014in particular, his book \"Ethics\". A pantheistic stance was also taken in the 16th century by philosopher and cosmologist Giordano Bruno, who for his pantheist views was burnt at the stake by the inquisition of the Catholic church.\nIn the East, Advaita Vedanta, a school of Hindu philosophy is thought to be similar to pantheism in Western philosophy. The early Taoism of Laozi and Zhuangzi is also sometimes considered pantheistic, although it could be more similar to panentheism. Cheondoism, which arose in the Joseon Dynasty of Korea, and Won Buddhism are also considered pantheistic.\nEtymology.\n\"Pantheism\" derives from the Greek \u03c0\u1fb6\u03bd \"pan\" \"all, of everything\" and \u03b8\u03b5\u03cc\u03c2 \"theos\" \"deity, divinity, god\". The first known combination of these roots appears in Latin, in Joseph Raphson's 1697 book \"De Spatio Reali seu Ente Infinito\", where he refers to \"pantheismus\". It was subsequently translated into English as \"pantheism\" in 1702.\nDefinitions.\nThere are numerous definitions of pantheism, including: \nHistory.\nPre-modern times.\nEarly traces of pantheist thought can be found within animistic beliefs and tribal religions throughout the world as an expression of unity with the divine, specifically in beliefs that have no central polytheist or monotheist personas. Hellenistic theology makes early recorded reference to pantheism within the ancient Greek religion of Orphism, where \"pan\" (the all) is made cognate with the creator God Phanes (symbolizing the universe), and with Zeus, after the swallowing of Phanes.\nPantheistic tendencies existed in a number of Gnostic groups, with pantheistic thought appearing throughout the Middle Ages. These included the beliefs of mystics such as Ortlieb of Strasbourg, David of Dinant, Amalric of Bena, and Eckhart.\nThe Catholic Church has long regarded pantheistic ideas as heresy. Sebastian Franck was considered an early Pantheist. Giordano Bruno, an Italian friar who evangelized about a transcendent and infinite God, was burned at the stake in 1600 by the Roman Inquisition. He has since become known as a celebrated pantheist and martyr of science.\nThe Hindu philosophy of Advaita Vedanta is thought to be similar to pantheism. The term \"Advaita\" (literally \"non-secondness\", but usually rendered as \"nondualism\", and often equated with monism) refers to the idea that \"Brahman\" alone is ultimately real, while the transient phenomenal world is an illusory appearance (\"maya\") of Brahman. In this view, \"jivatman\", the experiencing self, is ultimately non-different (\"na aparah\") from \"\u0100tman-Brahman\", the highest Self or Reality. The \"jivatman\" or individual self is a mere reflection or limitation of singular \"\u0100tman\" in a multitude of apparent individual bodies.\nBaruch Spinoza.\nIn the West, pantheism was formalized as a separate theology and philosophy based on the work of the 17th-century philosopher Baruch Spinoza. Spinoza was a Dutch philosopher of Portuguese descent raised in the Sephardi Jewish community in Amsterdam. He developed highly controversial ideas regarding the authenticity of the Hebrew Bible and the nature of the Divine, and was effectively excluded from Jewish society at age 23, when the local synagogue issued a \"herem\" against him. A number of his books were published posthumously, and shortly thereafter included in the Catholic Church's \"Index of Forbidden Books\".\nIn the posthumously published \"Ethics\", he opposed Ren\u00e9 Descartes' famous mind\u2013body dualism, the theory that the body and spirit are separate. Spinoza held the monist view that the two are the same, and monism is a fundamental part of his philosophy. He was described as a \"God-intoxicated man\" and used the word \"God\" to describe the unity of all substances. This view influenced philosophers such as Georg Wilhelm Friedrich Hegel, who said, \"You are either a Spinozist or not a philosopher at all.\" Spinoza earned praise as one of the great rationalists of 17th-century philosophy and one of Western philosophy's most important thinkers. Although the term \"pantheism\" was not coined until after his death, he is regarded as the most celebrated advocate of the concept. His book, \"Ethics\", was the major source from which Western pantheism spread.\n18th century.\nThe first known use of the term \"pantheism\" was in Latin (\"pantheismus\") by the English mathematician Joseph Raphson in his work \"De Spatio Reali seu Ente Infinito\", published in 1697. Raphson begins with a distinction between atheistic \"panhylists\" (from the Greek roots \"pan\", \"all\", and \"hyle\", \"matter\"), who believe everything is matter, and Spinozan \"pantheists\" who believe in \"a certain universal substance, material as well as intelligence, that fashions all things that exist out of its own essence.\" Raphson thought that the universe was immeasurable in respect to a human's capacity of understanding, and believed that humans would never be able to comprehend it. He referred to the pantheism of the Ancient Egyptians, Persians, Syrians, Assyrians, Greek, Indians, and Jewish Kabbalists, specifically referring to Spinoza.\nThe term was first used in English in a translation of Raphson's work in 1702. It was later used and popularized by Irish writer John Toland in his work of 1705 \"Socinianism Truly Stated, by a Pantheist\". Toland was influenced by both Spinoza and Bruno and had read Joseph Raphson's \"De Spatio Reali\", referring to it as \"the ingenious Mr. Ralphson's (sic) Book of Real Space\". Like Raphson, he used the terms \"pantheist\" and \"Spinozist\" interchangeably. In 1720 he wrote the \"Pantheisticon: or The Form of Celebrating the Socratic-Society\" in Latin, envisioning a pantheist society that believed, \"All things in the world are one, and one is all in all things ... what is all in all things is God, eternal and immense, neither born nor ever to perish.\" He clarified his idea of pantheism in a letter to Gottfried Leibniz in 1710 when he referred to \"the pantheistic opinion of those who believe in no other eternal being but the universe\".\nIn the mid-eighteenth century, the English theologian Daniel Waterland defined pantheism this way: \"It supposes God and nature, or God and the whole universe, to be one and the same substance\u2014one universal being; insomuch that men's souls are only modifications of the divine substance.\" In the early nineteenth century, the German theologian Julius Wegscheider defined pantheism as the belief that God and the world established by God are one and the same.\nBetween 1785\u201389, a controversy about Spinoza's philosophy arose between the German philosophers Friedrich Heinrich Jacobi (a critic) and Moses Mendelssohn (a defender). Known in German as the \"Pantheismusstreit\" (pantheism controversy), it helped spread pantheism to many German thinkers.\n19th century.\nGrowing influence.\nDuring the beginning of the 19th century, pantheism was the viewpoint of many leading writers and philosophers, attracting figures such as William Wordsworth and Samuel Coleridge in Britain; Johann Gottlieb Fichte, Schelling and Hegel in Germany; Knut Hamsun in Norway; and Walt Whitman, Ralph Waldo Emerson and Henry David Thoreau in the United States. Seen as a growing threat by the Vatican, in 1864, it was formally condemned by Pope Pius IX in the \"Syllabus of Errors\".\nA letter written in 1886 by William Herndon, Abraham Lincoln's law partner, was sold at auction for US$30,000 in 2011. In it, Herndon writes of the U.S. President's evolving religious views, which included pantheism.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Mr. Lincoln's religion is too well known to me to allow of even a shadow of a doubt; he is or was a Theist and a Rationalist, denying all extraordinary \u2013 supernatural inspiration or revelation. At one time in his life, to say the least, he was an elevated Pantheist, doubting the immortality of the soul as the Christian world understands that term. He believed that the soul lost its identity and was immortal as a force. Subsequent to this he rose to the belief of a God, and this is all the change he ever underwent.\"\nThe subject is understandably controversial, but the letter's content is consistent with Lincoln's fairly lukewarm approach to organized religion.\nComparison with non-Christian religions.\nSome 19th-century theologians thought that various pre-Christian religions and philosophies were pantheistic. They thought Pantheism was similar to the ancient Hinduism philosophy of Advaita (non-dualism).\n19th-century European theologians also considered Ancient Egyptian religion to contain pantheistic elements and pointed to Egyptian philosophy as a source of Greek Pantheism. The latter included some of the Presocratics, such as Heraclitus and Anaximander. The Stoics were pantheists, beginning with Zeno of Citium and culminating in the emperor-philosopher Marcus Aurelius. During the pre-Christian Roman Empire, Stoicism was one of the three dominant schools of philosophy, along with Epicureanism and Neoplatonism. The early Taoism of Laozi and Zhuangzi is also sometimes considered pantheistic, although it could be more similar to panentheism.\nCheondoism, which arose in the Joseon Dynasty of Korea, and Won Buddhism are also considered pantheistic. The Realist Society of Canada believes that the consciousness of the self-aware universe is reality, which is an alternative view of Pantheism.\n20th century.\nIn the late 20th century, some declared that pantheism was an underlying theology of Neopaganism, and pantheists began forming organizations devoted specifically to pantheism and treating it as a separate religion.\n21st century.\nDorion Sagan, son of scientist and science communicator Carl Sagan, published the 2007 book \"Dazzle Gradually: Reflections on the Nature of Nature\", co-written with his mother Lynn Margulis. In the chapter \"Truth of My Father\", Sagan writes that his \"father believed in the God of Spinoza and Einstein, God not behind nature, but as nature, equivalent to it.\"\nIn 2009, pantheism was mentioned in a Papal encyclical and in a statement on New Year's Day, 2010, criticizing pantheism for denying the superiority of humans over nature and seeing the source of man's salvation in nature.\nIn 2015, The Paradise Project, an organization \"dedicated to celebrating and spreading awareness about pantheism,\" commissioned Los Angeles muralist Levi Ponce to paint the 75-foot mural in Venice, California, near the organization's offices. The mural depicts Albert Einstein, Alan Watts, Baruch Spinoza, Terence McKenna, Carl Jung, Carl Sagan, Emily Dickinson, Nikola Tesla, Friedrich Nietzsche, Ralph Waldo Emerson, W.E.B. Du Bois, Henry David Thoreau, Elizabeth Cady Stanton, Rumi, Adi Shankara, and Laozi.\nCategorizations.\nThere are multiple varieties of pantheism and various systems of classifying them relying upon one or more spectra or in discrete categories.\nDegree of determinism.\nThe philosopher Charles Hartshorne used the term Classical Pantheism to describe the deterministic philosophies of Baruch Spinoza, the Stoics, and other like-minded figures. Pantheism (All-is-God) is often associated with monism (All-is-One) and some have suggested that it logically implies determinism (All-is-Now). Albert Einstein explained theological determinism by stating, \"the past, present, and future are an 'illusion'\". This form of pantheism has been referred to as \"extreme monism\", in which\u00a0\u2013 in the words of one commentator\u00a0\u2013 \"God decides or determines everything, including our supposed decisions.\" Other examples of determinism-inclined pantheisms include those of Ralph Waldo Emerson, and Hegel.\nHowever, some have argued against treating every meaning of \"unity\" as an aspect of pantheism, and there exist versions of pantheism that regard determinism as an inaccurate or incomplete view of nature. Examples include the beliefs of John Scotus Eriugena, Friedrich Wilhelm Joseph Schelling and William James.\nDegree of belief.\nIt may also be possible to distinguish two types of pantheism, one being more religious and the other being more philosophical. The Columbia Encyclopedia writes of the distinction:\n\"If the pantheist starts with the belief that the one great reality, eternal and infinite, is God, he sees everything finite and temporal as but some part of God. There is nothing separate or distinct from God, for God is the universe. If, on the other hand, the conception taken as the foundation of the system is that the great inclusive unity is the world itself, or the universe, God is swallowed up in that unity, which may be designated nature.\"\nForm of monism.\nPhilosophers and theologians have often suggested that pantheism implies monism.\nFor the Aztecs \"teotl\" was the metaphysical omnipresence creating the cosmos and all its contents \"from within\" itself as well as \"out of\" itself. This is conceptualized in a kind of monistic pantheism as manifest in the supreme god Ometeotl, as well as a large pantheon of lesser gods and idealizations of natural phenomena.\nOther.\nIn 1896, J. H. Worman, a theologian, identified seven categories of pantheism: Mechanical or materialistic (God the mechanical unity of existence); Ontological (fundamental unity, Spinoza); Dynamic; Psychical (God is the soul of the world); Ethical (God is the universal moral order, Fichte); Logical (Hegel); and Pure (absorption of God into nature, which Worman equates with atheism).\nIn 1984, Paul D. Feinberg, professor of biblical and systematic theology at Trinity Evangelical Divinity School, also identified seven: Hylozoistic; Immanentistic; Absolutistic monistic; Relativistic monistic; Acosmic; Identity of opposites; and Neoplatonic or emanationistic.\nDemographics.\nPrevalence.\nAccording to censuses of 2011, the UK was the country with the most Pantheists. As of 2011, about 1,000 Canadians identified their religion as \"Pantheist\", representing 0.003% of the population. By 2021, the number of Canadian pantheists had risen to 1,855 (0.005%). In Ireland, Pantheism rose from 202 in 1991, to 1106 in 2002, to 1,691 in 2006, 1,940 in 2011. In New Zealand, there was exactly one pantheist man in 1901. By 1906, the number of pantheists in New Zealand had septupled to 7 (6 male, 1 female). This number had further risen to 366 by 2006.\nAge, ethnicity, and gender.\nThe 2021 Canadian census showed that pantheists were somewhat more likely to be in their 20s and 30s compared to the general population. People under 15 were about four times less likely to be pantheist than the general population.\nThe 2021 Canadian census also showed that pantheists were less likely to be part of a recognized minority group compared to the general population, with 90.3% of pantheists not being part of any minority group (compared to 73.5% of the general population). The census did not register any pantheists who were Arab, Southeast Asian, West Asian, Korean, or Japanese.\nIn Canada (2011), there was no gender difference in regards to pantheism. However, in Ireland (2011), pantheists were slightly more likely to be female (1074 pantheists, 0.046% of women) than male (866 pantheists, 0.038% of men). In contrast, Canada (2021) showed pantheists to be slightly more likely to be male, with men representing 51.5% of pantheists.\nRelated concepts.\nNature worship or nature mysticism is often conflated and confused with pantheism. It is pointed out by at least one expert, Harold Wood, founder of the Universal Pantheist Society, that in pantheist philosophy Spinoza's identification of God with nature is very different from a recent idea of a self identifying pantheist with environmental ethical concerns. His use of the word nature to describe his worldview may be vastly different from the \"nature\" of modern sciences. He and other nature mystics who also identify as pantheists use \"nature\" to refer to the limited natural environment (as opposed to man-made built environment). This use of \"nature\" is different from the broader use from Spinoza and other pantheists describing natural laws and the overall phenomena of the physical world. Nature mysticism may be compatible with pantheism but it may also be compatible with theism and other views. Pantheism has also been involved in animal worship especially in primal religions.\nNontheism is an umbrella term which has been used to refer to a variety of religions not fitting traditional theism, and under which pantheism has been included.\nPanentheism (from Greek \u03c0\u1fb6\u03bd (p\u00e2n) \"all\"; \u1f10\u03bd (en) \"in\"; and \u03b8\u03b5\u03cc\u03c2 (the\u00f3s) \"God\"; \"all-in-God\") was formally coined in Germany in the 19th century in an attempt to offer a philosophical synthesis between traditional theism and pantheism, stating that God is substantially omnipresent in the physical universe but also exists \"apart from\" or \"beyond\" it as its Creator and Sustainer. Thus panentheism separates itself from pantheism, positing the extra claim that God exists above and beyond the world as we know it. The line between pantheism and panentheism can be blurred depending on varying definitions of God, so there have been disagreements when assigning particular notable figures to pantheism or panentheism.\nPandeism is another word derived from pantheism, and is characterized as a combination of reconcilable elements of pantheism and deism. It assumes a Creator-deity that is at some point distinct from the universe and then transforms into it, resulting in a universe similar to the pantheistic one in present essence, but differing in origin.\nPanpsychism is the philosophical view that consciousness, mind, or soul is a universal feature of all things. Some pantheists also subscribe to the distinct philosophical views hylozoism (or panvitalism), the view that everything is alive, and its close neighbor animism, the view that everything has a soul or spirit.\nPantheism in religion.\nTraditional religions.\nMany traditional and folk religions, including African traditional religions and Native American religions, can be seen as pantheistic or a mixture of pantheism and other worldviews, such as polytheism and animism. According to pantheists, there are elements of pantheism in some forms of Christianity.\nIdeas resembling pantheism existed in Eastern religions before the 18th century (notably Hinduism, Confucianism, and Taoism). Although there is no evidence that these influenced Spinoza's work, there is evidence regarding other contemporary philosophers, such as Leibniz, and later Voltaire. In the case of Hinduism, pantheistic views exist alongside panentheistic, polytheistic, monotheistic, and atheistic ones. \nSpirituality and new religious movements.\nPantheism is popular in modern spirituality and new religious movements, such as Neopaganism and Theosophy. Two organizations that specify the word pantheism in their title formed in the last quarter of the 20th century. The Universal Pantheist Society, open to all varieties of pantheists and supportive of environmental causes, was founded in 1975. The World Pantheist Movement is headed by Paul Harrison, an environmentalist, writer and a former vice president of the Universal Pantheist Society, from which he resigned in 1996. The World Pantheist Movement was incorporated in 1999 to focus exclusively on promoting naturalistic pantheism \u2013 a strict metaphysical naturalistic version of pantheism, considered by some a form of religious naturalism. It has been described as an example of \"dark green religion\" with a focus on environmental ethics.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23591", "revid": "20303702", "url": "https://en.wikipedia.org/wiki?curid=23591", "title": "Panentheism", "text": "Belief that the divine pervades all of space and time and also extends beyond it\nPanentheism (; \"all in God\", from the Greek , and ) is the belief that the divine intersects every part of the universe and also extends beyond space and time. The term was coined by the German philosopher Karl Krause in 1828 (after reviewing Hindu scripture) to distinguish the ideas of Georg Wilhelm Friedrich Hegel (1770\u20131831) and Friedrich Wilhelm Joseph Schelling (1775\u20131854) about the relation of God and the universe from the supposed pantheism of Baruch Spinoza. Unlike pantheism, which holds that the divine and the universe are identical, panentheism maintains an ontological distinction between the divine and the non-divine and the significance of both.\nIn panentheism, the universal spirit is present everywhere, which at the same time \"transcends\" all things created. Whilst pantheism asserts that \"all is God\", panentheism claims that God is greater than the universe. Some versions of panentheism suggest that the universe is nothing more than the manifestation of God. In addition, some forms indicate that the universe is contained within God, like in the Kabbalistic concept of \"tzimtzum\". Much of Hindu thought is highly characterized by panentheism and pantheism.\nIn philosophy.\nAncient Greek philosophy.\nThe religious beliefs of Neoplatonism can be regarded as panentheistic. Plotinus taught that there was an ineffable transcendent God (\"the One\", \"to En\", \u03c4\u1f78 \u1f1d\u03bd) of which subsequent realities were emanations. From \"the One\" emanates the Divine Mind (\"Nous\", \u039d\u03bf\u1fe6\u03c2) and the Cosmic Soul (\"Psyche\", \u03a8\u03c5\u03c7\u03ae). In Neoplatonism the world itself is God (according to Plato's Timaeus 37). This concept of divinity is associated with that of the \"Logos\" (\u039b\u03cc\u03b3\u03bf\u03c2), which had originated centuries earlier with Heraclitus (c. 535\u2013475 BC). The \"Logos\" pervades the cosmos, whereby all thoughts and all things originate, or as Heraclitus said: \"He who hears not me but the Logos will say: All is one.\" Neoplatonists such as Iamblichus attempted to reconcile this perspective by adding another hypostasis above the original monad of force or \"Dynamis\" (\u0394\u03cd\u03bd\u03b1\u03bc\u03b9\u03c2). This new all-pervasive monad encompassed all creation and its original uncreated emanations.\nModern philosophy.\nBaruch Spinoza later claimed that \"Whatsoever is, is in God, and without God nothing can be, or be conceived.\" \"Individual things are nothing but modifications of the attributes of God, or modes by which the attributes of God are expressed in a fixed and definite manner.\" Though Spinoza has been called the \"prophet\" and \"prince\" of pantheism, in a letter to Henry Oldenburg Spinoza states that: \"as to the view of certain people that I identify god with nature (taken as a kind of mass or corporeal matter), they are quite mistaken\". For Spinoza, our universe (cosmos) is a mode under two attributes of Thought and Extension. God has infinitely many other attributes which are not present in our world.\nAccording to German philosopher Karl Jaspers, when Spinoza wrote \"Deus sive Natura\" (God or Nature) Spinoza did not mean to say that God and Nature are interchangeable terms, but rather that God's transcendence was attested by God's infinitely many attributes, and that two attributes known by humans, namely Thought and Extension, signified God's immanence. Furthermore, Martial Gu\u00e9roult suggested the term \"panentheism\", rather than \"pantheism\" to describe Spinoza's view of the relation between God and the world. The world is not God, but it is, in a strong sense, \"in\" God. Yet, American philosopher and self-described panentheist Charles Hartshorne referred to Spinoza's philosophy as \"classical pantheism\" and distinguished Spinoza's philosophy from panentheism.\nIn 1828, the German philosopher Karl Christian Friedrich Krause (1781\u20131832) seeking to reconcile monotheism and pantheism, coined the term \"panentheism\" (from the Ancient Greek expression \u03c0\u1fb6\u03bd \u1f10\u03bd \u03b8\u03b5\u1ff7, \"p\u0101n en the\u1e53\", literally \"all in god\"). This conception of God influenced New England transcendentalists such as Ralph Waldo Emerson. The term was popularized by Charles Hartshorne in his development of process theology and has also been closely identified with the New Thought. The formalization of this term in the West in the 19th century was not new; philosophical treatises had been written on it in the context of Hinduism for millennia.\nPhilosophers who embraced panentheism have included Thomas Hill Green (1839\u20131882), James Ward (1843\u20131925), Andrew Seth Pringle-Pattison (1856\u20131931) and Samuel Alexander (1859\u20131938). Beginning in the 1940s, Hartshorne examined numerous conceptions of God. He reviewed and discarded pantheism, deism, and pandeism in favor of panentheism, finding that such a \"doctrine contains all of deism and pandeism except their arbitrary negations\". Hartshorne formulated God as a being who could become \"more perfect\": God has absolute perfection in categories for which absolute perfection is possible, and relative perfection (i.\u00a0e., is superior to all others) in categories for which perfection cannot be precisely determined.\nIn religion.\nBuddhism.\nZen Buddhism.\nThe Reverend Zen Master Soyen Shaku was the first Zen Buddhist Abbot to tour the United States in 1905\u20136. He wrote a series of essays collected in the book \"Zen For Americans\". In the essay titled \"The God Conception of Buddhism,\" he attempts to explain how a Buddhist looks at the Ultimate without an anthropomorphic God figure while still being able to relate to the term God in a Buddhist sense:\nAt the outset, let me state that Buddhism is not atheistic as the term is ordinarily understood. It has certainly a God, the highest reality and truth, through which and in which this universe exists. However, the followers of Buddhism usually avoid the term God, for it savors so much of Christianity, whose spirit is not always exactly in accord with the Buddhist interpretation of religious experience. Again, Buddhism is not pantheistic in the sense that it identifies the universe with God. On the other hand, the Buddhist God is absolute and transcendent; this world, being merely its manifestation, is necessarily fragmental and imperfect. To define more exactly the Buddhist notion of the highest being, it may be convenient to borrow the term very happily coined by a modern German scholar, \"panentheism,\" according to which God is \u03c0\u1fb6\u03bd \u03ba\u03b1\u1f76 \u1f15\u03bd (all and one) and more than the totality of existence.\nThe essay then goes on to explain first utilizing the term \"God\" for the American audience to get an initial understanding of what he means by \"panentheism,\" and then discusses the terms that Buddhism uses in place of \"God\" such as Dharmakaya, Buddha or Adi-Buddha, and Tathagata.\nChristianity.\nPanentheism is also a feature of some Christian philosophical theologies and resonates strongly within the theological tradition of the Eastern Orthodox Church. It also appears in process theology. Process theological thinkers are generally regarded as unorthodox in the Christian West. Furthermore, process philosophy is widely believed to have paved the way for open theism, a movement that tends to associate itself primarily with the Evangelical branch of Protestantism but is also generally considered unorthodox by most evangelicals.\nCatholic panentheism.\nA number of ordained Catholic writers (including Richard Rohr, David Steindl-Rast, and Thomas Keating) have suggested that panentheism is the original view of Christianity. They hold that such a view is directly supported by mystical experience and the teachings of Jesus and Paul the Apostle. Richard Rohr surmises this in his 2019 book \"The Universal Christ\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;But Paul merely took incarnationalism to its universal and logical conclusions. We see that in his bold exclamation \u201cThere is only Christ. He is everything and he is in everything\u201d (Colossians 3:11). If I were to write that today, people would call me a pantheist (the universe is God), whereas I am really a panentheist (God lies within all things, but also transcends them), exactly like both Jesus and Paul.\nSimilarly, David Steindl-Rast posits that Christianity's original panentheism is being revealed through contemporary mystical insight:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;What characterizes our moment in history is the collapse of Christian theism. Gratefulness mysticism makes us realize that Christianity never was theistic, but panentheistic. Faith in God as triune implied this from the very beginning; now we are becoming aware of it. It becomes obvious, at the same time, that we share this Trinitarian experience of divine life with all human beings as a spiritual undercurrent in all religions, an undercurrent older and more powerful than the various doctrines. At the core of interreligious dialogue flows this shared spirituality of gratefulness, a spirituality strong enough to restore to our broken world unity. This sentiment is mirrored in Thomas Keating's 1993 article, \"Clarifications Regarding Centering Prayer\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Pantheism is usually defined as the identification of God with creation in such a way that the two are indistinguishable. Panentheism means that God is present in all creation by virtue of his omnipresence and omnipotence, sustaining every creature in being without being identified with any creature. The latter understanding is what Jesus seems to have been describing when he prays \"that all might be one, Father, as we are one\" and \"that they may also be in us\" (John 17:22). Again and again, in the Last Supper discourse, he speaks of this oneness and his intentions to send his Spirit to dwell within us. If we understand the writings of the great mystics rightly, they experience God living within them all the time. Thus the affirmation of God's transcendence must always be balanced by the affirmation of his imminence both on the natural plane and on the plane of grace.\nPanentheism in other Christian confessions.\nPanentheistic conceptions of God occur amongst some modern theologians. Process theology and Creation Spirituality, two recent developments in Christian theology, contain panentheistic ideas. Charles Hartshorne (1897\u20132000), who conjoined process theology with panentheism, maintained a lifelong membership in the Methodist church but was also a Unitarian. In later years, he joined the Austin, Texas, Unitarian Universalist congregation and was an active participant in that church. Referring to ideas such as Thomas Oord's \"theocosmocentrism\" (2010), the soft panentheism of open theism, Keith Ward's comparative theology and John Polkinghorne's critical realism (2009), Raymond Potgieter observes distinctions such as dipolar and bipolar:\nThe former suggests two poles separated such as God influencing creation and it in turn its creator (Bangert 2006:168), whereas bipolarity completes God\u2019s being implying interdependence between temporal and eternal poles. (Marbaniang 2011:133), in dealing with Whitehead\u2019s approach, does not make this distinction. I use the term bipolar as a generic term to include suggestions of the structural definition of God\u2019s transcendence and immanence; to for instance accommodate a present and future reality into which deity must reasonably fit and function, and yet maintain separation from this world and evil whilst remaining within it.\nSome argue that panentheism should also include the notion that God has always been related to some world or another, which denies the idea of creation out of nothing (\"creatio ex nihilo\"). Nazarene Methodist theologian Thomas Jay Oord (born 1965) advocates panentheism, but he uses the word \"theocosmocentrism\" to highlight the notion that God and some world or another are the primary conceptual starting blocks for eminently fruitful theology. This form of panentheism helps overcome the problem of evil and proposes that God's love for the world is essential to who God is.\nThe Latter Day Saint movement teaches that the Light of Christ \"proceeds from God through Christ and gives life and light to all things\".\nGnosticism.\nManichaeists, being of another gnostic sect, preached a very different doctrine in positioning the true Manichaean God against matter as well as other deities, that it described as enmeshed with the world, namely the gods of Jews, Christians, and pagans. Nevertheless, this dualistic teaching included an elaborate cosmological myth that narrates the defeat of primal man by the powers of darkness that devoured and imprisoned the particles of light.\nValentinianism taught that matter came about through emanations of the supreme being, even if, to some, this event is held to be more accidental than intentional. To other gnostics, these emanations were akin to the \"Sephirot\" of the Kabbalists and deliberate manifestations of a transcendent God through a complex system of intermediaries.\nHinduism.\nThe earliest reference to panentheistic thought in Hindu philosophy is in a creationism contained in the later section of Rig Veda called the Purusha Sukta, which was compiled before 1100 BCE. The Purusha Sukta gives a description of the spiritual unity of the cosmos. It presents the nature of Purusha, or the cosmic being, as both immanent in the manifested world and yet transcendent. From this being the sukta holds, the original creative will proceeds, by which this vast universe is projected in space and time.\nThe most influential and dominant school of Indian philosophy, Advaita Vedanta, rejects theism and dualism by insisting that \"Brahman [ultimate reality] is without parts or attributes...one without a second.\" Since Brahman has no properties, contains no internal diversity and is identical with the whole reality it cannot be understood as an anthropomorphic personal God. The relationship between Brahman and the creation is often thought to be panentheistic.\nPanentheism is also expressed in the Bhagavad Gita. In verse IX.4, Krishna states: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;By Me all this universe is pervaded through My unmanifested form. All beings abide in Me but I do not abide in them.\nMany schools of Hindu thought espouse monistic theism, which is believed to be similar to a panentheistic viewpoint. Nimbarka's school of differential monism (Dvaitadvaita), Ramanuja's school of qualified monism (Vishistadvaita), and Saiva Siddhanta and Kashmir Shaivism are all considered to be panentheistic. Chaitanya Mahaprabhu's Gaudiya Vaishnavism, which elucidates the doctrine of Achintya Bheda Abheda (inconceivable oneness and difference), is also thought to be panentheistic. In Kashmir Shaivism, all things are believed to be a manifestation of Universal Consciousness (\"Cit\" or Brahman). So from the point of view of this school, the phenomenal world (\"\u015aakti\") is real, and it exists and has its being in Consciousness (\u0106\"it\"). Thus, Kashmir Shaivism also propounds theistic monism or panentheism.\nShaktism, or Tantra, is regarded as an Indian prototype of panentheism. Shakti is considered to be the cosmos itself \u2013 she is the embodiment of energy and dynamism and the motivating force behind all action and existence in the material universe. Shiva is her transcendent masculine aspect, providing the divine ground of all being. \"There is no Shiva without Shakti, or Shakti without Shiva. The two ... in themselves are One.\" Thus, it is she who becomes the time and space, the cosmos; it is she who becomes the five elements, and thus all animate life and inanimate forms. She is the primordial energy that holds all creation and destruction, all cycles of birth and death, all laws of cause and effect within herself, and yet is greater than the sum total of all these. She is transcendent but becomes immanent as the cosmos (\"Mula Prakriti\"). She, the primordial energy, directly becomes matter.\nJudaism.\nWhile mainstream Rabbinic Judaism is classically monotheistic and follows in the footsteps of Maimonides (c. 1135\u20131204), the panentheistic conception of God can be found among certain mystical Jewish traditions. A leading scholar of Kabbalah, Moshe Idel, ascribes this doctrine to the kabbalistic system of Moses ben Jacob Cordovero (1522\u20131570), and in the eighteenth century, to the Baal Shem Tov (c. 1700\u20131760), founder of the Hasidic movement, as well as his contemporaries, Rabbi Dov Ber of Mezeritch (died 1772) and Menahem Mendel, the Maggid of Bar. There is some debate as to whether Isaac Luria (1534\u20131572) and Lurianic Kabbalah, with its doctrine of \"tzimtzum\", can be regarded as panentheistic.\nAccording to Hasidism, the infinite Ein Sof is incorporeal and exists in a state that is both transcendent and immanent. This also appears to be the view of non-Hasidic Rabbi Chaim of Volozhin. Hasidic Judaism merges the ideal of nullification with a transcendent God via the intellectual articulation of inner dimensions through Kabbalah and with emphasis on the panentheistic divine immanence in everything.\nMany scholars would argue that \"panentheism\" is the best single-word description of the philosophical theology of Baruch Spinoza, a Jew. It is therefore no surprise that aspects of panentheism are also evident in the theology of Reconstructionist Judaism as presented in the writings of Mordecai Kaplan (1881\u20131983), who Spinoza strongly influenced.\nSikhism.\nConcept of God in Guru Granth Sahib \nFundamental Nature\nImmanence and Oneness: Guru Granth Sahib (SGGS) describes God (Kartaa Purakh, Universal Energy, Nirankaar) as immanent residing within all creation, not external or remote. Creation is seen as an extension or manifestation of the Creator, not something separate. This is foundational: \u0a0f\u0a15 \u0a30\u0a42\u0a2a \u0a38\u0a17\u0a32\u0a4b \u0a2a\u0a3e\u0a38\u0a3e\u0a30\u0a3e \u0965 \"Ek Roop Saglo Pasara\" (\u201cOne Form Extended into Everything\u201d) (sggs 803).\n\u0a74 (Ik Oankaar): Guru Nanak\u2019s symbol captures the oneness of Creator with creation. The numeric \u0a67 represents one-ness; the open \u0a13 represents the expanse of creation meaning the same Divine pervades all, and the universe itself is an expansion of that Oneness.\nDivine as Law and Order (\u201cHukam\u201d)\nHukam: SGGS strongly identifies God as Hukm (the Law of Nature, Cosmic Order). All operations of the universe, all natural and moral outcomes, occur within this Hukam. There is nothing outside of this Law \u201cEverything is within Hukam; none is exempt from Hukam\u201d (SGGS 1).\nGod is never presented as an anthropomorphic, interventionist being\u2014rather, the sustaining structure and principle by which all things unfold.\nDivine as Virtues\nDivine Virtues: God is also described as an infinite source of virtues (Daivi Gun). To \u201crealize God\u201d is to realize and embody divine attributes like compassion, truth, and humility. Spiritual progress is internal: \u201cTo become Godly is to become virtuous\u201d.\nSargun-Nirgun Doctrine\nSargun and Nirgun: SGGS teaches that God is both Nirgun (without attributes, unseen) and Sargun (with attributes, manifest in the visible world). These are not dualities but aspects of the same Universal Energy\u2014what appears unseen is also visible, and vice versa. Gurbani analogizes this as two sides of a coin: \u201cCoin is just one, but it has two sides\u201d.\nGod Within Creation\nUniversal Immanence: The Divine is said to reside within nature (Kudrat Vasia), within each human and particle of being. Searching for God externally is rejected; true realization comes from understanding the Divine already within. The phrase \" \u0a2c\u0a32\u0a3f\u0a39\u0a3e\u0a30\u0a40 \u0a15\u0a41\u0a26\u0a30\u0a24\u0a3f \u0a35\u0a38\u0a3f\u0a06 \u0965 \u0a24\u0a47\u0a30\u0a3e \u0a05\u0a70\u0a24\u0a41 \u0a28 \u0a1c\u0a3e\u0a08 \u0a32\u0a16\u0a3f\u0a06 \u0965\u0a67\u0965 \u0a30\u0a39\u0a3e\u0a09 \u0965 \u201cBalihaaree Kudharath Vasiaa || Thaeraa Anth N Jaaee Lakhiaa ||1|| Rehaao ||\u201d translates as \u201cI am in veneration of You existing within creation. Thine limit cannot be comprehended. Pause\u201d (SGGS 469).\nGod Is Not Separate\nSGGS rejects theological dualism: \u201cO Nanak! One (Universal Energy) is wide-spread, where can there be another seen?\u201d (SGGS 292).\nGod is not a separate entity to be pleased, feared, or worshipped for rewards or miracles. Rituals and symbolic worship are emphatically dismissed in favor of inner realization and living according to Hukam and Divine wisdom.\nConclusion: Form of Sikh God\nNot Pantheist, Not Classical Panentheist: The SGGS concept does not fully align with direct Pantheism (God is only the universe) or traditional Panentheism (God is both the universe and beyond it). Instead, it is a unique synthesis: the One without second, who manifests as all creation and is also the sustaining, transcendent law within it.\nThe SGGS God is immanent, non-anthropomorphic, formless, eternal, dynamic, the law and harmony of nature itself. God is to be realized within one\u2019s own consciousness and through virtuous living, never worshipped as a dualistic, supernatural \u201cother\u201d.\nSummary: The God of Guru Granth Sahib is best described as immanent, formless, both manifest and unmanifest, and identical with the cosmic principle/law (Hukam). Realization, not ritual, is the path to spiritual union, which is purely inward and ethical. This view resists classical Western theological categories and stands as a distinct Sikh revelation.\nIslam.\nWahdat ul-wujud (the Unity of All Things) is a concept sometimes described as pantheism or panentheism. It is primarily associated with the Asharite Sufi scholar Ibn Arabi. Some Sufi Orders, notably the Bektashis and the Universal Sufi movement, adhere to similar panentheistic beliefs. The same is said about the Nizari Ismaili who follow panentheism according to Ismaili doctrine.\nIn Pre-Columbian America.\nThe Mesoamerican empires of the Mayas, Aztecs as well as the South American Incas (Tawantinsuyu) have typically been characterized as polytheistic, with strong male and female deities. According to Charles C. Mann's history book \"\", only the lower classes of Aztec society were polytheistic. Philosopher James Maffie has argued that Aztec metaphysics was panentheistic rather than pantheistic since Teotl was considered by Aztec philosophers to be the ultimate all-encompassing yet all-transcending force defined by its inherited duality.\nNative American beliefs in North America have been characterized as panentheistic in that there is an emphasis on a single, unified divine spirit that is manifest in each individual entity. (North American Native writers have also translated the word for God as the Great Mystery or as the Sacred Other). This concept is referred to by many as the Great Spirit. Philosopher J. Baird Callicott has described Lakota theology as panentheistic, in that the divine both transcends and is immanent in everything.\nOne exception can be modern Cherokee, who are predominantly monotheistic but apparently not panentheistic. Yet in older Cherokee traditions, many observe both pantheism and panentheism and are often not beholden to exclusivity, encompassing other spiritual traditions without contradiction, a common trait among some tribes in the Americas. In the stories of Keetoowah storytellers Sequoyah Guess and Dennis Sixkiller, God is known as \u13a4\u13c1\u13b3\u13c5\u13af, commonly pronounced \"unehlanv,\" and visited earth in prehistoric times, but then left earth and her people to rely on themselves. This shows a parallel to Vaishnava cosmology.\nKonk\u014dky\u014d.\nKonkokyo is a form of sectarian Japanese Shinto and a faith within the Shinbutsu-sh\u016bg\u014d tradition. Traditional Shintoism holds that an impersonal spirit manifests/penetrates the material world, giving all objects consciousness and spontaneously creating a system of natural mechanisms, forces, and phenomena (Musubi). Konkokyo deviates from traditional Shintoism by holding that this spirit (Comparable to Brahman) has a personal identity and mind. This personal form is non-different from the energy itself, not residing in any particular cosmological location. In Konkokyo, this god is named \"Tenchi Kane no Kami-Sama,\" which can be translated directly as \"Spirit of the gilded/golden heavens and earth.\"\nThough practitioners of Konkokyo are small in number (~300,000 globally), the sect has birthed or influenced a multiplicity of Japanese New Religions, such as Oomoto. Many of these faiths carry on the Panentheistic views of Konkokyo.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23592", "revid": "11555324", "url": "https://en.wikipedia.org/wiki?curid=23592", "title": "Paraphilia", "text": "Atypical sexual attraction\nA paraphilia is an uncommon, intense, and persistent sexual arousal or attraction to anything not sexual by nature. It has also been defined as a sexual interest in anything other than a legally consenting human partner. Paraphilias are contrasted with normophilic (\"normal\") sexual interests, although the definition of what makes a sexual interest normal or atypical remains controversial.\nThe exact number and taxonomy of paraphilia is under debate; Anil Aggrawal has listed as many as 549 types of paraphilias. Several sub-classifications of paraphilia have been proposed; some argue that a fully dimensional, spectrum, or complaint-oriented approach would better reflect the evident diversity of human sexuality. Although paraphilias were believed in the 20th century to be rare among the general population, subsequent research has indicated that some level of paraphilic interests are relatively common.\nEtymology.\nCoinage of the term \"paraphilia\" (\"paraphilie\") has been credited to Friedrich Salomon Krauss in 1903, and it was used with some regularity by Wilhelm Stekel in the 1920s. The term comes from the Greek \u03c0\u03b1\u03c1\u03ac (\"para\"), meaning 'other' or 'outside of', and \u03c6\u03b9\u03bb\u03af\u03b1 (\"-philia\"), meaning 'loving'. The word was popularized by John Money in the 1980s as a non-pejorative designation for unusual sexual interests. It was first included in the DSM in its 1980 edition.\nDefinition.\nThere is no broad scientific consensus for definitive boundaries between what are considered \"unconventional sexual interests\", kinks, fetishes, and paraphilias. As such, these terms are often used loosely and interchangeably, especially in common parlance.\nHistory of paraphilic terminology.\nMany terms have been used to describe atypical sexual interests, and there remains debate regarding technical accuracy and perceptions of stigma. John Money described paraphilia as \"a sexuoerotic embellishment of, or alternative to the official, ideological norm\". Psychiatrist Glen Gabbard writes that despite efforts by Wilhelm Stekel and John Money, \"the term \"paraphilia\" remains pejorative in most circumstances.\"\nIn the late 19th century, psychologists and psychiatrists began to categorize various paraphilias so as to have a more descriptive system than the legal and religious constructs of sodomy, as well as perversion. In 1914, Albert Eulenburg observed a commonality across paraphilias, using the terminology of his time writing, \"All the forms of sexual perversion ... have one thing in common: their roots reach down into the matrix of natural and normal sex life; there they are somehow closely connected with the feelings and expressions of our physiological erotism. They are ... hyperbolic intensifications, distortions, monstrous fruits of certain partial and secondary expressions of this erotism which is considered 'normal' or at least within the limits of healthy sex feeling.\"\nBefore the introduction of the term \"paraphilia\" in the DSM-III (1980), the term \"sexual deviation\" was used to refer to paraphilias in the first two editions of the manual. In 1981, an article published in \"American Journal of Psychiatry\" described paraphilia as \"recurrent, intense sexually arousing fantasies, sexual urges, or behaviors generally involving\":\nDefinition of typical versus atypical interests.\nClinical literature contains reports of many paraphilias, only some of which receive separate entries in the diagnostic taxonomies of the American Psychiatric Association or the World Health Organization. There is disagreement regarding which sexual interests should be deemed paraphilic disorders versus normal variants of sexual interest. The DSM-IV-TR also acknowledges that the diagnosis and classification of paraphilias across cultures or religions \"is complicated by the fact that what is considered deviant in one cultural setting may be more acceptable in another setting\". Some argue that cultural relativism is important to consider when discussing paraphilias because there is wide variance concerning what is sexually acceptable across cultures. Consensual adult activities and adult entertainment involving sexual roleplay; novel, superficial, or trivial aspects of sexual fetishism; or incorporating the use of sex toys are not necessarily paraphilic.\nCriticism of common definitions.\nThere is scientific and political controversy regarding the continued inclusion of sex-related diagnoses such as the paraphilias in the DSM, due to the stigma of being classified as a mental illness. Some groups, seeking greater understanding and acceptance of sexual diversity, have lobbied for changes to the legal and medical status of unusual sexual interests and practices. Charles Allen Moser, a physician and advocate for sexual minorities, has argued that the diagnoses should be eliminated from diagnostic manuals. Ray Blanchard stated that the definition of paraphilia in the DSM done by concatenation (i.e., by listing a set of paraphilias) and that defining the term by exclusion (anything that is not normophilic) is preferable.\nMany paraphilic communities are unaware of the traumatic impact they reproduce and consider the strangeness of sexual practices to be purely subjective and dependent on the societal context. In BDSM, sadists and masochists reactivate their dissociative circuits by triggering a hormonal release similar to that experienced during the initial trauma: cortisol, adrenaline and oxytocin disrupt their pleasure and reward systems by associating them with violence and humiliation. Each time they act out, their brains replay the traumatic programme, reinforcing the neural circuits of traumatic functioning.\nInclusion and subsequent exclusion of homosexuality.\nHomosexuality, a widely accepted variant of human sexuality, was once categorized as a sexual deviation. Sigmund Freud and subsequent psychoanalytic thinkers considered homosexuality and paraphilias to result from psychosexual non-normative relations to the Oedipal complex, although not in the antecedent version of the 'Three Essays on Sexual Theory' where paraphilias are considered as stemming from an original polymorphous perversity. As such, the term \"sexual perversion\" or the epithet \"pervert\" have historically referred to gay men, as well as other non-heterosexuals (people who fall outside the perceived norms of sexual orientation).\nBy the mid-20th century, mental health practitioners began formalizing \"deviant sexuality\" classifications into categories. Originally coded as 000-x63, homosexuality was the top of the classification list (Code 302.0) until the American Psychiatric Association removed homosexuality from the DSM in 1973. Martin Kafka writes, \"Sexual disorders once considered paraphilias (e.g., homosexuality) are now regarded as variants of normal sexuality.\"\nA 2012 literature study by clinical psychologist James Cantor, when comparing homosexuality with paraphilias, found that both share \"the features of onset and course (both homosexuality and paraphilia being life-long), but they appear to differ on sex ratio, fraternal birth order, handedness, IQ and cognitive profile, and neuroanatomy.\" The research then concluded that the data seemed to suggest paraphilias and homosexuality as two distinct categories but regarded the conclusion as \"quite tentative\" given the current limited understanding of paraphilias.\nCharacteristics.\nParaphilias typically arise in late adolescence or early adulthood. Persons with paraphilias are generally egosyntonic and view their paraphilias as something inherent in their being, although they recognize that their sexual fantasies lie outside the norm and may attempt to conceal them. Paraphilic interests are rarely exclusive and some people have more than one paraphilia. Some people with paraphilias may seek occupations and avocations that increase their access to objects of their sexual fantasies (e.g., voyeurs working in rental properties to \"peep\" on others or pedophiles working with Boy Scouts).\nResearch has found that some paraphilias, such as voyeurism and sadomasochism, are associated with more lifetime sexual partners, contradicting theories that paraphilias are associated with courtship disorders and arrested social development. Scientific literature includes some single-case studies of very rare and idiosyncratic paraphilias. These include an adolescent male who had a strong fetishistic interest in the exhaust pipes of cars, a young man with a similar interest in a specific type of car, and a man who had a paraphilic interest in sneezing (both his own and the sneezing of others).\nEgo alien sexual interests in an individual can cause individuals to become suicidal due to the embarrassment or shame that it causes.\nCauses and correlations.\nThe causes of paraphilias in people are unclear, but it is believed to be a mixture of neurological, cultural, and psychodynamic factors. No two or more people with a paraphilia will develop it for the same reasons or be interested in the same qualities of the specific sexual interest. The specific causes of the development of paraphilic sexuality are numerous and vary from person to person, but psychological research has established that childhood trauma is strongly correlated with the development of paraphilic sexuality, particularly emotional abuse, neglect and sexual abuse.\nA 2008 study analyzing the sexual fantasies of 200 heterosexual men by using the Wilson Sex Fantasy Questionnaire exam determined that males with a pronounced degree of fetish interest had a greater number of older brothers, a high 2D:4D digit ratio (which would indicate excessive prenatal estrogen exposure), and an elevated probability of being left-handed, suggesting that \"disturbed\" hemispheric brain lateralization may play a role in paraphilic attractions. Behavioral explanations propose that paraphilias are conditioned early in life, during an experience that pairs the paraphilics stimulus with intense sexual arousal. Susan Nolen-Hoeksema suggests that, once established, masturbatory fantasies about the stimulus reinforce and broaden the paraphilic arousal.\nParaphiliacs often have other psychiatric comorbidities such as depression, attachment disorders, anxiety, emotional avoidance, obsessive or compulsive behaviours and identity disorders. All of these disorders are common symptoms of physical, psychological, emotional, or sexual abuse during childhood and can be treated with appropriate psychotrauma support.\nGenetic causes, particularly genes that encode the behavior of neurotransmitter receptors and androgen release have been implicated in studies, though others have shown no correlation between genetics and paraphilic behavior.\nIt is possible for an individual to acquire a paraphilia from various types of brain lesions and epilepsy.\nPrevalence.\nAlthough paraphilic interests in the general population were believed to be rare, research has shown that fantasies and behaviors related to voyeurism, sadomasochism and couple exhibitionism are not statistically uncommon among adults. The DSM-5 estimates that 2.2% of males and 1.3% of females in Australia engaged in bondage and discipline, sadomasochism, or dominance and submission within the past 12 months. The population prevalence of sexual masochism disorder is unknown.\nAmong Men.\nIn a study conducted in a population of men, 62% of participants reported at least one paraphilic interest. In another sample of college students, voyeurism was reported in 52% of men. \nAmong women.\nParaphilias are often observed in women, and there have been some studies focusing exclusively on females with paraphilias. Men and women differ on the content of their sexual fantasies, with the former reporting greater proportions of fetishism, exhibitionism and sadism, and the latter reporting greater proportions of masochism. Sexual masochism has been found to be the most commonly observed paraphilia in women, with approximately 1 in 20 cases.\nIn ancient cultures.\nParaphilic fantasies and behaviors have been registered in multiple old and ancient sources. Voyeurism, bestiality and exhibitionism have been described in the Bible. Sexual relations with animals have also been depicted in cave paintings. Some ancient sex manuals such as the \"Kama Sutra\" (450), \"Koka Shastra\" (1150) and \"Ananga Ranga\" (1500) discuss biting, marks left after sex and love blows. Although evidence suggests that paraphilic behaviors have existed prior to the Renaissance, it is difficult to ascertain how common they were and how many people had persistent paraphilic fantasies in ancient times.\nBestiality has been depicted multiple times in Greek mythology, although the act itself usually involved a deity in zoomorphic form, such as Zeus seducing Europa, Leda and Persephone while disguised as a bull, a swan and a serpent, respectively. Zeus was also depicted, in the form of an eagle, abducting Ganymede, an act that alludes to both bestiality and pederastry. Some fragments of Hittite law include prohibitions of and permissions to engage in specific acts of bestiality.\nHavelock Ellis pointed to an example of sexual masochism in the fifteenth century. The report, written by Giovanni Pico della Mirandola, described a man who could only be aroused by being beaten with a whip dipped in vinegar. Wilhelm Stekel also noted that Rousseau also discussed his own masochism in his \"Confessions\". Other similar instances of persistent paraphilic fantasies were reported between 1516 and 1643 by Coelius Sedulius, Rhodiginus, Brundel and Meibomius.\nDiagnostic and Statistical Manual of Mental Disorders (DSM).\nDSM-I and DSM-II.\nIn American psychiatry, prior to the publication of the DSM-I, paraphilias were classified as cases of \"psychopathic personality with pathologic sexuality\". The DSM-I (1952) included sexual deviation as a personality disorder of sociopathic subtype. The only diagnostic guidance was that sexual deviation should have been \"reserved for deviant sexuality which [was] not symptomatic of more extensive syndromes, such as schizophrenic or obsessional reactions\". The specifics of the disorder were to be provided by the clinician as a \"supplementary term\" to the sexual deviation diagnosis; there were no restrictions in the DSM-I on what this supplementary term could be. Researcher Anil Aggrawal writes that the now-obsolete DSM-I listed examples of supplementary terms for pathological behavior to include \"homosexuality, transvestism, pedophilia, fetishism, and sexual sadism, including rape, sexual assault, mutilation.\"\nThe DSM-II (1968) continued to use the term \"sexual deviations\", no longer ascribed them under personality disorders but rather alongside them in a broad category titled \"personality disorders and certain other nonpsychotic mental disorders\". The types of sexual deviations listed in the DSM-II were: sexual orientation disturbance (homosexuality), fetishism, pedophilia, transvestitism, exhibitionism, voyeurism, sadism, masochism, and \"other sexual deviation\". No definition or examples were provided for \"other sexual deviation\" but the general category of sexual deviation was meant to describe the sexual preference of individuals that was \"directed primarily toward objects other than people of opposite sex, toward sexual acts not usually associated with coitus, or toward coitus performed under bizarre circumstances, as in necrophilia, pedophilia, sexual sadism, and fetishism.\" Except for the removal of homosexuality from the DSM-III onwards, this definition provided a general standard that has guided specific definitions of paraphilias in subsequent DSM editions, up to DSM-IV-TR.\nDSM-III through DSM-IV.\nThe term \"paraphilia\" was introduced in the DSM-III (1980) as a subset of the new category of \"psychosexual disorders\". The DSM-III-R (1987) renamed the broad category to sexual disorders, renamed atypical paraphilia to paraphilia NOS (not otherwise specified), renamed transvestism as transvestic fetishism, added frotteurism, and moved zoophilia to the NOS category. It also provided seven nonexhaustive examples of NOS paraphilias, which besides zoophilia included exhibitionism, necrophilia, partialism, coprophilia, klismaphilia, and urophilia. The DSM-IV (1994) retained the sexual disorders classification for paraphilias, but added an even broader category, \"sexual and gender identity disorders\", which includes them. The DSM-IV retained the same types of paraphilias listed in DSM-III-R, including the NOS examples, but introduced some changes to the definitions of some specific types.\nDSM-IV-TR.\nThe DSM-IV-TR describes paraphilias as \"recurrent, intense sexually arousing fantasies, sexual urges or behaviors generally involving nonhuman objects, the suffering or humiliation of oneself or one's partner, or children or other nonconsenting persons that occur over a period of six months\" (criterion A), which \"cause clinically significant distress or impairment in social, occupational, or other important areas of functioning\" (criterion B). DSM-IV-TR names eight specific paraphilic disorders (exhibitionism, fetishism, frotteurism, pedophilia, sexual masochism, sexual sadism, voyeurism, and transvestic fetishism, plus a residual category, paraphilia\u2014not otherwise specified). Criterion B differs for exhibitionism, frotteurism, and pedophilia to include acting on these urges, and for sadism, acting on these urges with a nonconsenting person. Sexual arousal in association with objects that were designed for sexual purposes is not diagnosable. Some paraphilias may interfere with the capacity for sexual activity with consenting adult partners. In the current version of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR), a paraphilia is not diagnosable as a psychiatric disorder unless it causes distress to the individual or harm to others.\nDSM-5.\nThe DSM-5 adds a distinction between \"paraphilias\" and \"paraphilic disorders\", stating that paraphilias do not require or justify psychiatric treatment in themselves, and defining \"paraphilic disorder\" as \"a paraphilia that is currently causing distress or impairment to the individual or a paraphilia whose satisfaction has entailed personal harm, or risk of harm, to others\". The DSM-5 Paraphilias Subworkgroup reached a \"consensus that paraphilias are not \"ipso facto\" psychiatric disorders\", and proposed \"that the DSM-V make a distinction between \"paraphilias\" and paraphilic \"disorders\". One would \"ascertain\" a paraphilia (according to the nature of the urges, fantasies, or behaviors) but \"diagnose\" a paraphilic disorder (on the basis of distress and impairment). In this conception, having a paraphilia would be a necessary but not a sufficient condition for having a paraphilic disorder.\" The 'Rationale' page of any paraphilia in the electronic DSM-5 draft continues: \"This approach leaves intact the distinction between normative and non-normative sexual behavior, which could be important to researchers, but without automatically labeling non-normative sexual behavior as psychopathological. It also eliminates certain logical absurdities in the DSM-IV-TR. In that version, for example, a man cannot be classified as a transvestite\u2014however much he cross-dresses and however sexually exciting that is to him\u2014unless he is unhappy about this activity or impaired by it. This change in viewpoint would be reflected in the diagnostic criteria sets by the addition of the word 'Disorder' to all the paraphilias. Thus, Sexual Sadism would become Sexual Sadism Disorder; Sexual Masochism would become Sexual Masochism Disorder, and so on.\"\nBioethics professor Alice Dreger interpreted these changes as \"a subtle way of saying sexual kinks are basically okay \u2013 so okay, the sub-work group doesn't actually bother to define paraphilia. But a paraphilic disorder is defined: that's when an atypical sexual interest causes distress or impairment to the individual or harm to others.\" Interviewed by Dreger, Ray Blanchard, the Chair of the Paraphilias Sub-Work Group, stated, \"We tried to go as far as we could in depathologizing mild and harmless paraphilias, while recognizing that severe paraphilias that distress or impair people or cause them to do harm to others are validly regarded as disorders.\" Charles Allen Moser stated that this change is not really substantive, as the DSM-IV already acknowledged a difference between paraphilias and non-pathological but unusual sexual interests, a distinction that is virtually identical to what was being proposed for DSM-5, and it is a distinction that, in practice, has often been ignored. Linguist Andrew Clinton Hinderliter argued that \"including some sexual interests\u2014but not others\u2014in the DSM creates a fundamental asymmetry and communicates a negative value judgment against the sexual interests included,\" and leaves the paraphilias in a situation similar to ego-dystonic homosexuality, which was removed from the DSM because it was no longer recognized as a mental disorder.\nThe DSM-5 has specific listings for eight paraphilic disorders. These are voyeuristic disorder, exhibitionistic disorder, frotteuristic disorder, sexual masochism disorder, sexual sadism disorder, pedophilic disorder, fetishistic disorder, and transvestic disorder. Other paraphilic disorders can be diagnosed under the Other Specified Paraphilic Disorder or Unspecified Paraphilic Disorder listings, if accompanied by distress or impairment.\nInternational Classification of Diseases.\nICD-6,\u00a0 ICD-7,\u00a0 ICD-8.\nIn the ICD-6 (1948) and ICD-7 (1955), a category of \"sexual deviation\" was listed with \"other Pathological personality disorders\". In the ICD-8 (1965), \"sexual deviations\" were categorized as homosexuality, fetishism, pedophilia, transvestism, exhibitionism, voyeurism, sadism and masochism.\nICD-9.\nIn the ICD-9 (1975), the category of sexual deviations and disorders was expanded to include transsexualism, sexual dysfunctions, and psychosexual identity disorders. The list contained homosexuality, bestiality, pedophilia, transvestism,\u00a0exhibitionism, transexualism, Disorders of psychosexual identity, frigidity and impotence, Other sexual deviations and disorders (including fetishism, masochism, and sadism).\nICD-10.\nIn the ICD-10 (1990), the category \"sexual deviations and disorders\" was divided into several subcategories. Paraphilias were placed in subcategory of \"sexual preference disorders\". The list included fetishism, fetishistic transvestism, exhibitionism, voyeurism, pedophilia, sadomasochism and other disorders of sexual preference (including frotteurism, necrophilia, and zoophilia). Homosexuality was removed from the list, but ego-dystonic sexual orientation was still considered a deviation which was placed in subcategory \"psychological and behavioural disorders associated with sexual development and orientation\".\nICD-11.\nIn the ICD-11 (2022), \"paraphilia\" has been replaced with \"paraphilic disorder\". Any paraphilia and any other arousal pattern \"by itself\" no longer constitutes a disorder. To date, the diagnosis must meet criteria of paraphilia \"and\" one of the following: 1) a marked distress associated with arousal pattern (but not one that comes from rejection or fear of rejection); 2) the person has acted on the arousal pattern towards unwilling others or others considered as unable to give consent; 3) a serious risk of injury or death. The list of the paraphilic disorders includes: Exhibitionistic Disorder, Voyeuristic Disorder, Pedophilic Disorder, Coercive Sexual Sadism Disorder, Frotteuristic Disorder, Other Paraphilic Disorder Involving Non-Consenting Individuals, and Other Paraphilic Disorder Involving Solitary Behaviour or Consenting Individuals. As of now, disorders associated with sexual orientation have been removed from the ICD. Gender issues have been removed from the mental health category and have been placed under \"Conditions related to sexual health\".\nParaphilic disorders.\nTherapeutic management.\nMost clinicians and researchers believe that paraphilic sexual interests cannot be altered, although evidence is needed to support this. Instead, the goal of therapy is normally to reduce the person's discomfort with their paraphilia and limit the risk of any harmful, anti-social, or criminal behavior. Both psychotherapeutic and pharmacological methods are available to these ends. Cognitive behavioral therapy, at times, can help people with extreme paraphilic disorders develop strategies to avoid acting on their interests. Patients are taught to identify and cope with factors that make acting on their interests more likely, such as stress. It is currently the only form of psychotherapy for paraphilic disorders supported by randomized double-blind trials, as opposed to case studies and consensus of expert opinion.\nMedications.\nPharmacological treatments can help people control their sexual behaviors, but do not change the content of the paraphilia. They are typically combined with cognitive behavioral therapy for best effect.\nSSRIs.\nSelective serotonin reuptake inhibitors (SSRIs) have been well received and are considered an important pharmacological treatment of severe paraphilic disorders. They are proposed to work by reducing sexual arousal, compulsivity, and depressive symptoms. They have been used with exhibitionists, non-offending pedophiles, and compulsive masturbators.\nAntiandrogens.\nAntiandrogens are used in more extreme cases. Similar to physical castration, they work by reducing androgen levels, and have thus been described as chemical castration. The antiandrogen cyproterone acetate has been shown to substantially reduce sexual fantasies and offending behaviors. Medroxyprogesterone acetate and gonadotropin-releasing hormone agonists (such as leuprorelin) have also been used to lower sex drive. Due to the side effects, the World Federation of Societies of Biological Psychiatry recommends that hormonal treatments only be used when there is a serious risk of sexual violence, or when other methods have failed. Surgical castration has largely been abandoned because these pharmacological alternatives are similarly effective and less invasive.\nLegality.\nIn the United States, since 1990 a significant number of states have passed sexually violent predator laws. Following a series of landmark cases in the Supreme Court of the United States, persons diagnosed with extreme paraphilic disorders, particularly pedophilia (\"Kansas v. Hendricks\", 1997) and others that cause serious difficulty controlling behavior (\"Kansas v. Crane\", 2002), can be held indefinitely in civil confinement under various state legislation generically known as sexually violent predator laws and the federal Adam Walsh Act (\"United States v. Comstock\", 2010).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23593", "revid": "50865928", "url": "https://en.wikipedia.org/wiki?curid=23593", "title": "Pediatrics", "text": "Branch of medicine caring for children\nPediatrics (American English), also spelled paediatrics (British English), also known as underage medicine is the branch of medicine that involves the medical care of infants, children, adolescents, and young adults. Within the Commonwealth, pediatrics covers patients until the age of 18, except in India where the pediatric age is 12. The American Academy of Pediatrics recommends people seek pediatric care through the age of 21, but some pediatric subspecialists continue to care for adults up to 25. Worldwide age limits of pediatrics have been trending upward year after year. A medical doctor who specializes in this area is known as a pediatrician, or paediatrician. The word \"pediatrics\" and its cognates mean \"healer of children\", derived from the two Greek words: (\"pais\" \"child\") and (\"iatros\" \"doctor, healer\"). Pediatricians work in clinics, research centers, universities, general hospitals and children's hospitals, including those who practice pediatric subspecialties (e.g. neonatology requires resources available in a NICU).\nHistory.\nThe earliest mentions of child-specific medical problems appear in the \"Hippocratic Corpus\", published in the fifth century B.C., and the famous \"Sacred Disease\". These publications discussed topics such as childhood epilepsy and premature births. From the first to fourth centuries A.D., Greek philosophers and physicians Celsus, Soranus of Ephesus, Aretaeus, Galen, and Oribasius, also discussed specific illnesses affecting children in their works, such as rashes, epilepsy, and meningitis. Already Hippocrates, Aristotle, Celsus, Soranus, and Galen understood the differences in growing and maturing organisms that necessitated different treatment: \"\" (\"In general, boys should not be treated in the same way as men\"). Some of the oldest traces of pediatrics can be discovered in Ancient India where children's doctors were called \"kumara bhrtya\".\nEven though some pediatric works existed during this time, they were scarce and rarely published due to a lack of knowledge in pediatric medicine. \"Sushruta Samhita\", an ayurvedic text composed during the sixth century BCE, contains information about pediatrics. Another ayurvedic text from this period is \"Kashyapa Samhita\". A second century AD manuscript by the Greek physician and gynecologist Soranus of Ephesus dealt with neonatal pediatrics. Byzantine physicians Oribasius, A\u00ebtius of Amida, Alexander Trallianus, and Paulus Aegineta contributed to the field. The Byzantines also built \"brephotrophia\" (cr\u00eaches). Islamic Golden Age writers served as a bridge for Greco-Roman and Byzantine medicine and added ideas of their own, especially Haly Abbas, Yahya Serapion, Abulcasis, Avicenna, and Averroes. The Persian philosopher and physician al-Razi (865\u2013925), sometimes called the father of pediatrics, published a monograph on pediatrics titled \"Diseases in Children\". Also among the first books about pediatrics was \"Libellus [Opusculum] de aegritudinibus et remediis infantium\" 1472 (\"Little Book on Children's Diseases and Treatment\"), by the Italian pediatrician Paolo Bagellardo. In sequence came Bartholom\u00e4us Metlinger's \"Ein Regiment der Jungerkinder\" 1473, Cornelius Roelans (1450\u20131525) no title Buchlein, or Latin compendium, 1483, and Heinrich von Louffenburg (1391\u20131460) \"Versehung des Leibs\" written in 1429 (published 1491), together forming the \"Pediatric Incunabula\", four great medical treatises on children's physiology and pathology.\nWhile more information about childhood diseases became available, there was little evidence that children received the same kind of medical care that adults did. It was during the seventeenth and eighteenth centuries that medical experts started offering specialized care for children. The Swedish physician Nils Ros\u00e9n von Rosenstein (1706\u20131773) is considered to be the founder of modern pediatrics as a medical specialty, while his work \"The diseases of children, and their remedies\" (1764) is considered to be \"the first modern textbook on the subject\". However, it was not until the nineteenth century that medical professionals acknowledged pediatrics as a separate field of medicine. The first pediatric-specific publications appeared between the 1790s and the 1920s.\nEtymology.\nThe term pediatrics was first introduced in English in 1859 by Abraham Jacobi. In 1860, he became \"the first dedicated professor of pediatrics in the world.\" Jacobi is known as the \"father of American pediatrics\" because of his many contributions to the field. He received his medical training in Germany and later practiced in New York City.\nThe first generally accepted pediatric hospital is the \"H\u00f4pital des Enfants Malades\" (), which opened in Paris in June 1802 on the site of a previous orphanage. From its beginning, this famous hospital accepted patients up to the age of fifteen years, and it continues to this day as the pediatric division of the Necker-Enfants Malades Hospital, created in 1920 by merging with the nearby \"Necker Hospital\", founded in 1778.\nIn other European countries, the Charit\u00e9 (a hospital founded in 1710) in Berlin established a separate Pediatric Pavilion in 1830, followed by similar institutions at Saint Petersburg in 1834, and at Vienna and Breslau (now Wroc\u0142aw), both in 1837. In 1852 Britain's first pediatric hospital, the Hospital for Sick Children, Great Ormond Street was founded by Charles West. The first Children's hospital in Scotland opened in 1860 in Edinburgh. In the US, the first similar institutions were the Children's Hospital of Philadelphia, which opened in 1855, and then Boston Children's Hospital (1869). Subspecialties in pediatrics were created at the Harriet Lane Home at Johns Hopkins by Edwards A. Park.\nDifferences between adult and pediatric medicine.\nThe body size differences are paralleled by maturation changes. The smaller body of an infant or neonate is substantially different physiologically from that of an adult. Congenital defects, genetic variance, and developmental issues are of greater concern to pediatricians than they often are to adult physicians. A common adage is that children are not simply \"little adults\". The clinician must take into account the immature physiology of the infant or child when considering symptoms, prescribing medications, and diagnosing illnesses.\nPediatric physiology directly impacts the pharmacokinetic properties of drugs that enter the body. The absorption, distribution, metabolism, and elimination of medications differ between developing children and grown adults. Despite completed studies and reviews, continual research is needed to better understand how these factors should affect the decisions of healthcare providers when prescribing and administering medications to the pediatric population.\nAbsorption.\nMany drug absorption differences between pediatric and adult populations revolve around the stomach. Neonates and young infants have increased stomach pH due to decreased acid secretion, thereby creating a more basic environment for drugs that are taken by mouth. Acid is essential to degrading certain oral drugs before systemic absorption. Therefore, the absorption of these drugs in children is greater than in adults due to decreased breakdown and increased preservation in a less acidic gastric space.\nChildren also have an extended rate of gastric emptying, which slows the rate of drug absorption.\nDrug absorption also depends on specific enzymes that come in contact with the oral drug as it travels through the body. The Supply of these enzymes increases as children continue to develop their gastrointestinal tract. Pediatric patients have underdeveloped proteins, which leads to decreased metabolism and increased serum concentrations of specific drugs. However, prodrugs experience the opposite effect because enzymes are necessary to allow their active form to enter systemic circulation.\nDistribution.\nThe percentage of total body water and extracellular fluid volume both decrease as children grow and develop with time. Pediatric patients thus have a larger volume of distribution than adults, which directly affects the dosing of hydrophilic drugs such as beta-lactam antibiotics like ampicillin. Thus, these drugs are administered at greater weight-based doses or with adjusted dosing intervals in children to account for this key difference in body composition.\nInfants and neonates also have fewer plasma proteins. Thus, highly protein-bound drugs have fewer opportunities for protein binding, leading to increased distribution.\nMetabolism.\nDrug metabolism primarily occurs via enzymes in the liver and can vary according to which specific enzymes are affected in a specific stage of development. Phase I and Phase II enzymes have different rates of maturation and development, depending on their specific mechanism of action (i.e. oxidation, hydrolysis, acetylation, methylation, etc.). Enzyme capacity, clearance, and half-life are all factors that contribute to metabolism differences between children and adults. Drug metabolism can even differ within the pediatric population, separating neonates and infants from young children.\nElimination.\nDrug elimination is primarily facilitated via the liver and kidneys. In infants and young children, the larger relative size of their kidneys leads to increased renal clearance of medications that are eliminated through urine. In preterm neonates and infants, their kidneys are slower to mature and thus are unable to clear as much drug as fully developed kidneys. This can cause unwanted drug build-up, which is why it is important to consider lower doses and greater dosing intervals for this population. Diseases that negatively affect kidney function can also have the same effect and thus warrant similar considerations.\nPediatric autonomy in healthcare.\nA major difference between the practice of pediatric and adult medicine is that children, in most jurisdictions and with certain exceptions, cannot make decisions for themselves. The issues of guardianship, privacy, legal responsibility, and informed consent must always be considered in every pediatric procedure. Pediatricians often have to treat the parents and sometimes, the family, rather than just the child. Adolescents are in their own legal class, having rights to their own health care decisions in certain circumstances. The concept of legal consent combined with the non-legal consent (assent) of the child when considering treatment options, especially in the face of conditions with poor prognosis or complicated and painful procedures/surgeries, means the pediatrician must take into account the desires of many people, in addition to those of the patient.\nHistory of pediatric autonomy.\nThe term autonomy is traceable to ethical theory and law, where it states that autonomous individuals can make decisions based on their own logic. Hippocrates was the first to use the term in a medical setting. He created a code of ethics for doctors called the \"Hippocratic Oath\" that highlighted the importance of putting patients' interests first, making autonomy for patients a top priority in health care. \u00a0\nIn ancient times, society did not view pediatric medicine as essential or scientific. Experts considered professional medicine unsuitable for treating children. Children also had no rights. Fathers regarded their children as property, so their children's health decisions were entrusted to them. As a result, mothers, midwives, \"wise women\", and general practitioners treated the children instead of doctors. Since mothers could not rely on professional medicine to take care of their children, they developed their own methods, such as using alkaline soda ash to remove the vernix at birth and treating teething pain with opium or wine. The absence of proper pediatric care, rights, and laws in health care to prioritize children's health led to many of their deaths. Ancient Greeks and Romans sometimes even killed healthy female babies and infants with deformities since they had no adequate medical treatment and no laws prohibiting infanticide.\nIn the twentieth century, medical experts began to put more emphasis on children's rights. In 1989, in the United Nations Rights of the Child Convention, medical experts developed the Best Interest Standard of Child to prioritize children's rights and best interests. This event marked the onset of pediatric autonomy. In 1995, the American Academy of Pediatrics (AAP) finally acknowledged the Best Interest Standard of a Child as an ethical principle for pediatric decision-making, and it is still being used today.\nParental authority and current medical issues.\nThe majority of the time, parents have the authority to decide what happens to their child. Philosopher John Locke argued that it is the responsibility of parents to raise their children and that God gave them this authority. In modern society, Jeffrey Blustein, modern philosopher and author of the book \"Parents and Children: The Ethics of Family\", argues that parental authority is granted because the child requires parents to satisfy their needs. He believes that parental autonomy is more about parents providing good care for their children and treating them with respect than parents having rights. The researcher Kyriakos Martakis, MD, MSc, explains that research shows parental influence negatively affects children's ability to form autonomy. However, involving children in the decision-making process allows children to develop their cognitive skills and create their own opinions and, thus, decisions about their health. Parental authority affects the degree of autonomy the child patient has. As a result, in Argentina, the new National Civil and Commercial Code has enacted various changes to the healthcare system to encourage children and adolescents to develop autonomy. It has become more crucial to let children take accountability for their own health decisions.\nIn most cases, the pediatrician, parent, and child work as a team to make the best possible medical decision. The pediatrician has the right to intervene for the child's welfare and seek advice from an ethics committee. However, in recent studies, authors have denied that complete autonomy is present in pediatric healthcare. The same moral standards should apply to children as they do to adults. In support of this idea is the concept of paternalism, which negates autonomy when it is in the patient's interests. This concept aims to keep the child's best interests in mind regarding autonomy. Pediatricians can interact with patients and help them make decisions that will benefit them, thus enhancing their autonomy. However, radical theories that question a child's moral worth continue to be debated today. Authors often question whether the treatment and equality of a child and an adult should be the same. Author Tamar Schapiro notes that children need nurturing and cannot exercise the same level of authority as adults. Hence, continuing the discussion on whether children are capable of making important health decisions until this day.\nModern advancements.\nAccording to the Subcommittee of Clinical Ethics of the Argentinean Pediatric Society (SAP), children can understand moral feelings at all ages and can make reasonable decisions based on those feelings. Therefore, children and teens are deemed capable of making their own health decisions when they reach the age of 13. Recently, studies made on the decision-making of children have challenged that age to be 12.\nTechnology has made several modern advancements that contribute to the future development of child autonomy, for example, unsolicited findings (U.F.s) of pediatric exome sequencing. They are findings based on pediatric exome sequencing that explain in greater detail the intellectual disability of a child and predict to what extent it will affect the child in the future. Genetic and intellectual disorders in children make them incapable of making moral decisions, so people look down upon this kind of testing because the child's future autonomy is at risk. It is still in question whether parents should request these types of testing for their children. Medical experts argue that it could endanger the autonomous rights the child will possess in the future. However, the parents contend that genetic testing would benefit the welfare of their children since it would allow them to make better health care decisions. Exome sequencing for children and the decision to grant parents the right to request them is a medically ethical issue that many still debate today.\nEducation requirements.\nCanada.\nTo become a pediatrician in Canada, a candidate must complete a \"Doctor of Medicine\" (MD) or \"Medicin\u00e6 Doctorem et Chirurgi\u00e6 Magistrum\" (MDCM) degree from an accredited Canadian medical school, or hold an equivalent foreign qualification such as the \"Bachelor of Medicine, Bachelor of Surgery\" (MBBS) recognized by either the Canadian Medical Association or the Coll\u00e8ge des m\u00e9decins du Qu\u00e9bec. \nFollowing medical school, physicians must complete a four-year residency program in pediatrics accredited by the Royal College of Physicians and Surgeons of Canada, conducted in either English or French, depending on the province. Upon successful completion of residency, candidates are eligible for certification as pediatricians. Physicians may further pursue subspecialty training in fields such as Pediatric cardiology, Neonatology, or other pediatric subspecialties willingly.\nIndia.\nTo become a pediatrician in India, a student must clear the NEET (UG) examination, the most competitive examination in India after Class 12 to enrol into the \"Bachelor of Medicine, Bachelor of Surgery\" (MBBS) program in National Medical Commission (NMC) approved medical colleges. The MBBS is a 5.5-year program consisting of 4.5 years of academic coursework and one year of compulsory rotary internship. \nAfter completing MBBS and licensing, the physicians must undertake a postgraduate residency program leading to either \"Doctor of Medicine\" (MD) or \"Diplomate of National Board\" (DNB) degree through the NEET (PG) examination for state medical colleges or INI CET examination for \"Institutes of National Importance\" medical institutions. The postgraduate training-cum-residency extends for three years. Afterwards, upon completion of MD or DNB, the pediatricians can further super-specialize in various other sub-specialities on the basis of the NEET SS or INI SS examinations similarly. \nUnited States.\nAspiring medical students will need 4 years of undergraduate courses at a college or university, which will get them a BS, BA or other bachelor's degree. After completing college, future pediatricians will need to attend 4 years of medical school (MD/DO/MBBS) and later do 3 more years of residency training, the first year of which is called \"internship.\" After completing the 3 years of residency, physicians are eligible to become certified in pediatrics by passing a rigorous test that deals with medical conditions related to young children.\nIn high school, future pediatricians are required to take basic science classes such as biology, chemistry, physics, algebra, geometry, and calculus. It is also advisable to learn a foreign language (preferably Spanish in the United States) and be involved in high school organizations and extracurricular activities. After high school, college students simply need to fulfill the basic science course requirements that most medical schools recommend and will need to prepare to take the MCAT (Medical College Admission Test) in their junior or early senior year in college. Once attending medical school, student courses will focus on basic medical sciences like human anatomy, physiology, chemistry, etc., for the first three years, the second year of which is when medical students start to get hands-on experience with actual patients.\nTraining of pediatricians.\nThe training of pediatricians varies considerably across the world. Depending on jurisdiction and university, a medical degree course may be either undergraduate-entry or graduate-entry. The former commonly takes five or six years and has been usual in the Commonwealth. Entrants to graduate-entry courses (as in the US), usually lasting four or five years, have previously completed a three- or four-year university degree, commonly but by no means always in sciences. Medical graduates hold a degree specific to the country and university in and from which they graduated. This degree qualifies that medical practitioner to become licensed or registered under the laws of that particular country, and sometimes of several countries, subject to requirements for \"internship\" or \"conditional registration\".\nPediatricians must undertake further training in their chosen field. This may take from four to eleven or more years depending on jurisdiction and the degree of specialization.\nIn the United States, a medical school graduate wishing to specialize in pediatrics must undergo a three-year residency composed of outpatient, inpatient, and critical care rotations. Subspecialties within pediatrics require further training in the form of 3-year fellowships. Subspecialties include critical care, gastroenterology, neurology, infectious disease, hematology/oncology, rheumatology, pulmonology, child abuse, emergency medicine, endocrinology, neonatology, and others.\nIn most jurisdictions, entry-level degrees are common to all branches of the medical profession, but in some jurisdictions, specialization in pediatrics may begin before completion of this degree. In some jurisdictions, pediatric training is begun immediately following the completion of entry-level training. In other jurisdictions, junior medical doctors must undertake generalist (unstreamed) training for a number of years before commencing pediatric (or any other) specialization. Specialist training is often largely under the control of pediatric organizations (see below) rather than universities and depends on the jurisdiction.\nSubspecialties.\nSubspecialties of pediatrics include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23594", "revid": "122", "url": "https://en.wikipedia.org/wiki?curid=23594", "title": "Psychoacoustic model", "text": ""}
{"id": "23597", "revid": "20542576", "url": "https://en.wikipedia.org/wiki?curid=23597", "title": "Physiology", "text": "Science regarding functions in organisms or living systems\nPhysiology (; from grc \" ' ()\"\u00a0'nature, origin' and \" ' ()\"\u00a0'study of') is the scientific study of functions and mechanisms in a living system. As a subdiscipline of biology, physiology focuses on how organisms, organ systems, individual organs, cells, and biomolecules carry out chemical and physical functions in a living system. According to the classes of organisms, the field can be divided into medical physiology, animal physiology, plant physiology, cell physiology, and comparative physiology.\nCentral to physiological functioning are biophysical and biochemical processes, homeostatic control mechanisms, and communication between cells. \"Physiological state\" is the condition of normal function. In contrast, \"pathological state\" refers to abnormal conditions, including human diseases.\nThe Nobel Prize in Physiology or Medicine is awarded by the Royal Swedish Academy of Sciences for exceptional scientific achievements in physiology related to the field of medicine.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nFoundations.\nBecause physiology focuses on the functions and mechanisms of living organisms at all levels, from the molecular and cellular level to the level of whole organisms and populations, its foundations span a range of key disciplines:\nSubdisciplines.\nThere are many ways to categorize the subdisciplines of physiology:\nSubdisciplines by level of organisation.\nCell physiology.\nAlthough there are differences between animal, plant, and microbial cells, the basic physiological functions of cells can be divided into the processes of cell division, cell signaling, cell growth, and cell metabolism.\nSubdisciplines by taxa.\nPlant physiology.\nPlant physiology is a subdiscipline of botany concerned with the functioning of plants. Closely related fields include plant morphology, plant ecology, phytochemistry, cell biology, genetics, biophysics, and molecular biology. Fundamental processes of plant physiology include photosynthesis, respiration, plant nutrition, tropisms, nastic movements, photoperiodism, photomorphogenesis, circadian rhythms, seed germination, dormancy, and stomata function and transpiration. Absorption of water by roots, production of food in the leaves, and growth of shoots towards light are examples of plant physiology.\nHuman physiology.\nHuman physiology is the study of how the human body's systems and functions work together to maintain a stable internal environment. It includes the study of the nervous, endocrine, cardiovascular, respiratory, digestive, and urinary systems, as well as cellular and exercise physiology. Understanding human physiology is essential for diagnosing and treating health conditions and promoting overall wellbeing.\nIt seeks to understand the mechanisms that work to keep the human body alive and functioning, through scientific enquiry into the nature of mechanical, physical, and biochemical functions of humans, their organs, and the cells of which they are composed. The principal level of focus of physiology is at the level of organs and systems within systems. The endocrine and nervous systems play major roles in the reception and transmission of signals that integrate function in animals. Homeostasis is a major aspect with regard to such interactions within plants as well as animals. The biological basis of the study of physiology, integration refers to the overlap of many functions of the systems of the human body, as well as its accompanied form. It is achieved through communication that occurs in a variety of ways, both electrical and chemical.\nChanges in physiology can impact the mental functions of individuals. Examples of this would be the effects of certain medications or toxic levels of substances. Change in behavior as a result of these substances is often used to assess the health of individuals.\nMuch of the foundation of knowledge in human physiology was provided by animal experimentation. Due to the frequent connection between form and function, physiology and anatomy are intrinsically linked and are studied in tandem as part of a medical curriculum.\nSubdisciplines by research objective.\nComparative physiology.\nInvolving evolutionary physiology and environmental physiology, comparative physiology considers the diversity of functional characteristics across organisms.\nHistory.\nThe classical era.\nThe study of human physiology as a medical field originates in classical Greece, at the time of Hippocrates (late 5th century BC). Outside of Western tradition, early forms of physiology or anatomy can be reconstructed as having been present at around the same time in China, India and elsewhere. Hippocrates incorporated the theory of humorism, which consisted of four basic substances: earth, water, air and fire. Each substance is known for having a corresponding humor: black bile, phlegm, blood, and yellow bile, respectively. Hippocrates also noted some emotional connections to the four humors, on which Galen would later expand. The critical thinking of Aristotle and his emphasis on the relationship between structure and function marked the beginning of physiology in Ancient Greece. Like Hippocrates, Aristotle took to the humoral theory of disease, which also consisted of four primary qualities in life: hot, cold, wet and dry. Galen (c.\u2009130\u2013200\u00a0AD) was the first to use experiments to probe the functions of the body. Unlike Hippocrates, Galen argued that humoral imbalances can be located in specific organs, including the entire body. His modification of this theory better equipped doctors to make more precise diagnoses. Galen also played off of Hippocrates' idea that emotions were also tied to the humors, and added the notion of temperaments: sanguine corresponds with blood; phlegmatic is tied to phlegm; yellow bile is connected to choleric; and black bile corresponds with melancholy. Galen also saw the human body consisting of three connected systems: the brain and nerves, which are responsible for thoughts and sensations; the heart and arteries, which give life; and the liver and veins, which can be attributed to nutrition and growth. Galen was also the founder of experimental physiology. And for the next 1,400 years, Galenic physiology was a powerful and influential tool in medicine.\nEarly modern period.\nJean Fernel (1497\u20131558), a French physician, introduced the term \"physiology\". Galen, Ibn al-Nafis, Michael Servetus, Realdo Colombo, Amato Lusitano and William Harvey, are credited as making important discoveries in the circulation of the blood. Santorio Santorio in 1610s was the first to use a device to measure the pulse rate (the \"pulsilogium\"), and a thermoscope to measure temperature.\nIn 1791 Luigi Galvani described the role of electricity in the nerves of dissected frogs. In 1811, C\u00e9sar Julien Jean Legallois studied respiration in animal dissection and lesions and found the center of respiration in the medulla oblongata. In the same year, Charles Bell finished work on what would later become known as the Bell\u2013Magendie law, which compared functional differences between dorsal and ventral roots of the spinal cord. In 1824, Fran\u00e7ois Magendie described the sensory roots and produced the first evidence of the cerebellum's role in equilibration to complete the Bell\u2013Magendie law.\nIn the 1820s, the French physiologist Henri Milne-Edwards introduced the notion of physiological division of labor, which allowed to \"compare and study living things as if they were machines created by the industry of man.\" Inspired in the work of Adam Smith, Milne-Edwards wrote that the \"body of all living beings, whether animal or plant, resembles a factory ... where the organs, comparable to workers, work incessantly to produce the phenomena that constitute the life of the individual.\" In more differentiated organisms, the functional labor could be apportioned between different instruments or systems (called by him as \"appareils\").\nIn 1858, Joseph Lister studied the cause of blood coagulation and inflammation that resulted after previous injuries and surgical wounds. He later discovered and implemented antiseptics in the operating room, and as a result, decreased the death rate from surgery by a substantial amount.\nThe Physiological Society was founded in London in 1876 as a dining club. The American Physiological Society (APS) is a nonprofit organization that was founded in 1887. The Society is, \"devoted to fostering education, scientific research, and dissemination of information in the physiological sciences.\"\nIn 1891, Ivan Pavlov performed research on \"conditional responses\" that involved dogs' saliva production in response to a bell and visual stimuli.\nIn the 19th century, physiological knowledge began to accumulate at a rapid rate, in particular with the 1838 appearance of the Cell theory of Matthias Schleiden and Theodor Schwann. It radically stated that organisms are made up of units called cells. Claude Bernard's (1813\u20131878) further discoveries ultimately led to his concept of \"milieu interieur\" (internal environment), which would later be taken up and championed as \"homeostasis\" by American physiologist Walter B. Cannon in 1929. By homeostasis, Cannon meant \"the maintenance of steady states in the body and the physiological processes through which they are regulated.\" In other words, the body's ability to regulate its internal environment. William Beaumont was the first American to utilize the practical application of physiology.\nNineteenth-century physiologists such as Michael Foster, Max Verworn, and Alfred Binet, based on Haeckel's ideas, elaborated what came to be called \"general physiology\", a unified science of life based on the cell actions, later renamed in the 20th century as cell biology.\nLate modern period.\nIn the 20th century, biologists became interested in how organisms other than human beings function, eventually spawning the fields of comparative physiology and ecophysiology. Major figures in these fields include Knut Schmidt-Nielsen and George Bartholomew. Most recently, evolutionary physiology has become a distinct subdiscipline.\nIn 1920, August Krogh won the Nobel Prize for discovering how, in capillaries, blood flow is regulated.\nIn 1954, Andrew Huxley and Hugh Huxley, alongside their research team, discovered the sliding filaments in skeletal muscle, known today as the sliding filament theory.\nRecently, there have been intense debates about the vitality of physiology as a discipline (Is it dead or alive?). If physiology is perhaps less visible nowadays than during the golden age of the 19th century, it is in large part because the field has given birth to some of the most active domains of today's biological sciences, such as neuroscience, endocrinology, and immunology. Furthermore, physiology is still often seen as an integrative discipline, which can put together into a coherent framework data coming from various different domains.\nNotable physiologists.\nWomen in physiology.\nInitially, women were largely excluded from official involvement in any physiological society. The American Physiological Society, for example, was founded in 1887 and included only men in its ranks. In 1902, the American Physiological Society elected Ida Hyde as the first female member of the society. Hyde, a representative of the American Association of University Women, a global non-profit organization that advances equity for women and girls in education, attempted to promote gender equality in every aspect of science and medicine.\nSoon thereafter, in 1913, J.S. Haldane proposed that women be allowed to formally join The Physiological Society, which had been founded in 1876. On 3 July 1915, six women were officially admitted: Florence Buchanan, Winifred Cullis, Ruth Skelton, Sarah C. M. Sowton, Constance Leetham Terry, and Enid M. Tribe. The centenary of the election of women was celebrated in 2015 with the publication of the book \"Women Physiologists: Centenary Celebrations And Beyond For The Physiological Society.\" ()\nProminent women physiologists include:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\nHuman physiology\nAnimal physiology\nPlant physiology\nFungal physiology\nProtistan physiology\nAlgal physiology\nBacterial physiology"}
{"id": "23601", "revid": "9632047", "url": "https://en.wikipedia.org/wiki?curid=23601", "title": "Pi", "text": "Number, approximately 3.14\nThe number \u03c0 (; spelled out as pi) is a mathematical constant, approximately equal to 3.14159, that is the ratio of a circle's circumference to its diameter. It appears in many formulae across mathematics and physics, and some of these formulae are commonly used for defining \u03c0, to avoid relying on the definition of the length of a curve.\nThe number \u03c0 is an irrational number, meaning that it cannot be expressed exactly as a ratio of two integers, although fractions such as formula_1 are commonly used to approximate it. Consequently, its decimal representation never ends, nor enters a permanently repeating pattern. It is a transcendental number, meaning that it cannot be a solution of an algebraic equation involving only finite sums, products, powers, and integers. The transcendence of \u03c0 implies that it is impossible to solve the ancient challenge of squaring the circle with a compass and straightedge. The decimal digits of \u03c0 appear to be randomly distributed, but no proof of this conjecture has been found.\nFor thousands of years, mathematicians have attempted to extend their understanding of \u03c0, sometimes by computing its value to a high degree of accuracy. Ancient civilizations, including the Egyptians and Babylonians, required fairly accurate approximations of \u03c0 for practical computations. Around 250BC, the Greek mathematician Archimedes created an algorithm to approximate \u03c0 with arbitrary accuracy. In the 5th century AD, Chinese mathematicians approximated \u03c0 to seven digits, while Indian mathematicians made a five-digit approximation, both using geometrical techniques. The first computational formula for \u03c0, based on infinite series, was discovered a millennium later. The earliest known use of the Greek letter \u03c0 to represent the ratio of a circle's circumference to its diameter was by the Welsh mathematician William Jones in 1706. The invention of calculus soon led to the calculation of hundreds of digits of \u03c0, enough for all practical scientific computations. Nevertheless, in the 20th and 21st centuries, mathematicians and computer scientists have pursued new approaches that, when combined with increasing computational power, extended the decimal representation of \u03c0 to many trillions of digits. These computations are motivated by the development of efficient algorithms to calculate numeric series, as well as the human quest to break records. The extensive computations involved have also been used to test the correctness of new computer processors.\nBecause it relates to a circle, \u03c0 is found in many formulae in trigonometry and geometry, especially those concerning circles, ellipses and spheres. It is also found in formulae from other topics in science, such as cosmology, fractals, thermodynamics, mechanics, and electromagnetism. It also appears in areas having little to do with geometry, such as number theory and statistics, and in modern mathematical analysis can be defined without any reference to geometry. The ubiquity of \u03c0 makes it one of the most widely known mathematical constants inside and outside of science. Several books devoted to \u03c0 have been published, and record-setting calculations of the digits of \u03c0 often result in news headlines.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nFundamentals.\nName.\nThe symbol used by mathematicians to represent the ratio of a circle's circumference to its diameter is the lowercase Greek letter \u03c0, sometimes spelled out as \"pi.\" In English, \u03c0 is pronounced as \"pie\" ( ). In mathematical use, the lowercase letter \u03c0 is distinguished from its capitalized and enlarged counterpart \u03a0, which denotes a product of a sequence, analogously to how \u03a3 denotes summation.\nThe choice of the symbol \u03c0 is discussed in the section \u00a7\u00a0Adoption of the symbol \u03c0.\nDefinition.\n\u03c0 is commonly defined as the ratio of a circle's circumference \"C\" to its diameter \"d\":\nformula_2\nThe ratio formula_3 is constant, regardless of the circle's size. For example, if a circle has twice the diameter of another circle, it will also have twice the circumference, preserving the ratio formula_3.\nIn modern mathematics, this definition is not fully satisfactory for several reasons. Firstly, it lacks a rigorous definition of the length of a curved line. Such a definition requires at least the concept of a limit, or, more generally, the concepts of derivatives and integrals. Also, diameters, circles and circumferences can be defined in Non-Euclidean geometries, but, in such a geometry, the ratio &amp;NoBreak;&amp;NoBreak; need not to be a constant, and need not to equal to \u03c0. Also, there are many occurrences of \u03c0 in many branches of mathematics that are completely independent from geometry, and in modern mathematics, the trend is to built geometry from algebra and analysis rather than independently from the other branches of mathematics. For these reasons, the following characterizations can be taken as definitions of \u03c0:\nThe sine and the cosine satisfy the differential equation &amp;NoBreak;&amp;NoBreak;, and all solutions of this equation are periodic. This leads to the conceptual definition:\nIrrationality and normality.\n\u03c0 is an irrational number, meaning that it cannot be written as the ratio of two integers. Fractions such as and are commonly used to approximate \u03c0, but no common fraction (ratio of whole numbers) can be its exact value. Because \u03c0 is irrational, it has an infinite number of digits in its decimal representation, and does not settle into an infinitely repeating pattern of digits. There are several proofs that \u03c0 is irrational; they are generally proofs by contradiction and require calculus. The degree to which \u03c0 can be approximated by rational numbers (called the irrationality measure) is not precisely known; estimates have established that the irrationality measure is larger or at least equal to the measure of \"e\" but smaller than the measure of Liouville numbers.\nThe digits of \u03c0 have no apparent pattern and have passed tests for statistical randomness, including tests for normality; a number of infinite length is called normal when all possible sequences of digits (of any given length) appear equally often. The conjecture that \u03c0 is normal has not been proven or disproven.\nSince the advent of computers, a large number of digits of \u03c0 have been available on which to perform statistical analysis. Yasumasa Kanada has performed detailed statistical analyses on the decimal digits of \u03c0, and found them consistent with normality; for example, the frequencies of the ten digits 0 to 9 were subjected to statistical significance tests, and no evidence of a pattern was found. Any random sequence of digits contains arbitrarily long subsequences that appear non-random, by the infinite monkey theorem. Thus, because the sequence of \u03c0's digits passes statistical tests for randomness, it contains some sequences of digits that may appear non-random, such as a sequence of six consecutive 9s that begins at the 762nd decimal place of the decimal representation of \u03c0. This is also called the \"Feynman point\" in mathematical folklore, after Richard Feynman, although no connection to Feynman is known.\nTranscendence.\nIn addition to being irrational, \u03c0 is also a transcendental number, which means that it is not the solution of any non-constant polynomial equation with rational coefficients, such as formula_5. This follows from the so-called Lindemann\u2013Weierstrass theorem, which also establishes the transcendence of the constant \"e\".\nThe transcendence of \u03c0 has two important consequences: First, \u03c0 cannot be expressed using any finite combination of rational numbers and square roots or \"n\"th roots (such as formula_6 or formula_7). Second, since no transcendental number can be constructed with compass and straightedge, it is not possible to \"square the circle\". In other words, it is impossible to construct, using compass and straightedge alone, a square whose area is exactly equal to the area of a given circle. Squaring a circle was one of the important geometry problems of the classical antiquity. Amateur mathematicians in modern times have sometimes attempted to square the circle and claim success\u2014despite the fact that it is mathematically impossible.\nAn unsolved problem thus far is the question of whether or not the numbers \"\u03c0\" and \"e\" are algebraically independent (\"relatively transcendental\"). This would be resolved by Schanuel's conjecture \u2013 a currently unproven generalization of the Lindemann\u2013Weierstrass theorem.\nContinued fractions.\nAs an irrational number, \u03c0 cannot be represented as a common fraction. But every number, including \u03c0, can be represented by an infinite series of nested fractions, called a simple continued fraction:\nformula_8\nTruncating the continued fraction at any point yields a rational approximation for \u03c0; the first four of these are 3, , , and . These numbers are among the best-known and most widely used historical approximations of the constant. Each approximation generated in this way is a best rational approximation; that is, each is closer to \u03c0 than any other fraction with the same or a smaller denominator. Because \u03c0 is transcendental, it is by definition not algebraic and so cannot be a quadratic irrational. Therefore, \u03c0 cannot have a periodic continued fraction. Although the simple continued fraction for \u03c0 (with numerators all 1, shown above) also does not exhibit any other obvious pattern, several non-simple continued fractions do, such as:\nformula_9\nApproximate value and digits.\nSome approximations of \"pi\" include:\nDigits in other number systems\nComplex numbers and Euler's identity.\nAny complex number, say z, can be expressed using a pair of real numbers. In the polar coordinate system, one number (radius or r) is used to represent z's distance from the origin of the complex plane, and the other (angle or \u03c6) the counter-clockwise rotation from the positive real line:\nformula_10\nwhere i is the imaginary unit satisfying formula_11. The frequent appearance of \u03c0 in complex analysis can be related to the behaviour of the exponential function of a complex variable, described by Euler's formula:\nformula_12\nwhere the constant \"e\" is the base of the natural logarithm. This formula establishes a correspondence between imaginary powers of \"e\" and points on the unit circle centred at the origin of the complex plane. Setting formula_13 in Euler's formula results in Euler's identity, celebrated in mathematics due to it containing five important mathematical constants:\nformula_14\nThere are \"n\" different complex numbers z satisfying &amp;NoBreak;&amp;NoBreak;, and these are called the \"\"n\"th roots of unity\" and are given by the formula:\nformula_15\nHistory.\nSurviving approximations of \u03c0 prior to the 2nd century\u00a0AD are accurate to one or two decimal places at best. The earliest written approximations are found in Babylon and Egypt, both within one percent of the true value. In Babylon, a clay tablet dated 1900\u20131600\u00a0BC has a geometrical statement that, by implication, treats \u03c0 as \u00a0=\u00a03.125. In Egypt, the Rhind Papyrus, dated around 1650\u00a0BC but copied from a document dated to 1850\u00a0BC, has a formula for the area of a circle that treats \u03c0 as formula_16. Although some pyramidologists have theorized that the Great Pyramid of Giza was built with proportions related to \u03c0, this theory is not widely accepted by scholars.\nIn the Shulba Sutras of Indian mathematics, dating to an oral tradition from the 1st or 2nd millennium\u00a0BC, approximations are given which have been variously interpreted as approximately 3.08831, 3.08833, 3.004, 3, or 3.125.\nPolygon approximation era.\nThe first recorded algorithm for rigorously calculating the value of \u03c0 was a geometrical approach using polygons, devised around 250\u00a0BC by the Greek mathematician Archimedes, implementing the method of exhaustion. This polygonal algorithm dominated for over 1,000 years, and as a result \u03c0 is sometimes referred to as Archimedes's constant. Archimedes computed upper and lower bounds of \u03c0 by drawing a regular hexagon inside and outside a circle, and successively doubling the number of sides until he reached a 96-sided regular polygon. By calculating the perimeters of these polygons, he proved that (that is, ). Archimedes' upper bound of may have led to a widespread popular belief that \u03c0 is equal to . Around 150\u00a0AD, Greco-Roman scientist Ptolemy, in his \"Almagest\", gave a value for \u03c0 of 3.1416, which he may have obtained from Archimedes or from Apollonius of Perga. Mathematicians using polygonal algorithms reached 39 digits of \u03c0 in 1630, a record only broken in 1699 when infinite series were used to reach 71 digits.\nIn ancient China, values for \u03c0 included 3.1547 (around 1\u00a0AD), formula_7 (100\u00a0AD, approximately 3.1623), and (3rd century, approximately 3.1556). Around 265\u00a0AD, the Cao Wei mathematician Liu Hui created a polygon-based iterative algorithm, with which he constructed a 3,072-sided polygon to approximate \u03c0 as\u00a03.1416. Liu later invented a faster method of calculating \u03c0 and obtained a value of 3.14 with a 96-sided polygon, by taking advantage of the fact that the differences in area of successive polygons form a geometric series with a factor of\u00a04. Around 480\u00a0AD, Zu Chongzhi calculated that formula_18 and suggested the approximations formula_19 and formula_20, which he termed the \"mil\u00fc\" ('close ratio') and \"yuel\u00fc\" ('approximate ratio') respectively, iterating with Liu Hui's algorithm up to a 12,288-sided polygon. With a correct value for its seven first decimal digits, Zu's result remained the most accurate approximation of \u03c0 for the next 800 years.\nThe Indian astronomer Aryabhata used a value of 3.1416 in his \"\u0100ryabha\u1e6d\u012bya\" (499\u00a0AD). Around 1220, Fibonacci computed 3.1418 using a polygonal method devised independently of Archimedes. Italian author Dante apparently employed the value formula_21.\nThe Persian astronomer Jamsh\u012bd al-K\u0101sh\u012b produced nine sexagesimal digits, roughly the equivalent of 16 decimal digits, in 1424, using a polygon with formula_22 sides, which stood as the world record for about 180 years. French mathematician Fran\u00e7ois Vi\u00e8te in 1579 achieved nine digits with a polygon of formula_23 sides. Flemish mathematician Adriaan van Roomen arrived at 15 decimal places in 1593. In 1596, Dutch mathematician Ludolph van Ceulen reached 20 digits, a record he later increased to 35 digits (as a result, \u03c0 was called the \"Ludolphian number\" in Germany until the early 20th century). Dutch scientist Willebrord Snellius reached 34 digits in 1621, and Austrian astronomer Christoph Grienberger arrived at 38 digits in 1630 using 1040 sides. Christiaan Huygens was able to arrive at 10 decimal places in 1654 using a slightly different method equivalent to Richardson extrapolation.\nInfinite series.\nThe calculation of \u03c0 was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute \u03c0 with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for \u03c0 most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach also appeared in the Kerala school sometime in the 14th or 15th century. Around 1500, an infinite series that could be used to compute \u03c0, written in the form of Sanskrit verse, was presented in \"Tantrasamgraha\" by Nilakantha Somayaji. The series are presented without proof, but proofs are presented in the later work \"Yuktibh\u0101\u1e63\u0101\", published around 1530. Several infinite series are described, including series for sine (which Nilakantha attributes to Madhava of Sangamagrama), cosine, and arctangent which are now sometimes referred to as Madhava series. The series for arctangent is sometimes called Gregory's series or the Gregory\u2013Leibniz series. Madhava used infinite series to estimate \u03c0 to 11 digits around 1400.\nIn 1593, Fran\u00e7ois Vi\u00e8te published what is now known as Vi\u00e8te's formula, an infinite product (rather than an infinite sum, which is more typically used in \u03c0 calculations):\nformula_24\nIn 1655, John Wallis published what is now known as the Wallis product, also an infinite product:\nformula_25\nIn the 1660s, the English scientist Isaac Newton and German mathematician Gottfried Wilhelm Leibniz discovered calculus, which led to the development of many infinite series for approximating \u03c0. Newton himself used an arcsine series to compute a 15-digit approximation of \u03c0 in 1665 or 1666, writing, \"I am ashamed to tell you to how many figures I carried these computations, having no other business at the time.\"\nIn 1671, James Gregory, and independently, Leibniz in 1673, discovered the Taylor series expansion for arctangent:\nformula_26\nThis series, sometimes called the Gregory\u2013Leibniz series, equals formula_27 when evaluated with &amp;NoBreak;&amp;NoBreak;. But for &amp;NoBreak;&amp;NoBreak;, it converges impractically slowly (that is, approaches the answer very gradually), taking about ten times as many terms to calculate each additional digit.\nIn 1699, English mathematician Abraham Sharp used the Gregory\u2013Leibniz series for formula_28 to compute \u03c0 to 71 digits, breaking the previous record of 39 digits, which was set with a polygonal algorithm.\nIn 1706, John Machin used the Gregory\u2013Leibniz series to produce an algorithm that converged much faster:\nformula_29\nMachin reached 100 digits of \u03c0 with this formula. Other mathematicians created variants, now known as Machin-like formulae, that were used to set several successive records for calculating digits of \u03c0.\nIsaac Newton accelerated the convergence of the Gregory\u2013Leibniz series in 1684 (in an unpublished work; others independently discovered the result):\nformula_30\nLeonhard Euler popularized this series in his 1755 differential calculus textbook, and later used it with Machin-like formulae, including formula_31 with which he computed 20 digits of \u03c0 in one hour.\nMachin-like formulae remained the best-known method for calculating \u03c0 well into the age of computers, and were used to set records for 250 years, culminating in a 620-digit approximation in 1946 by Daniel Ferguson\u00a0\u2013 the best approximation achieved without the aid of a calculating device.\nIn 1844, a record was set by Zacharias Dase, who employed a Machin-like formula to calculate 200 decimals of \u03c0 in his head at the behest of German mathematician Carl Friedrich Gauss.\nIn 1853, British mathematician William Shanks calculated \u03c0 to 607 digits, but made a mistake in the 528th digit, rendering all subsequent digits incorrect. Though he calculated an additional 100 digits in 1873, bringing the total up to 707, his previous mistake rendered all the new digits incorrect as well.\nRate of convergence.\nSome infinite series for \u03c0 converge faster than others. Given the choice of two infinite series for \u03c0, mathematicians will generally use the one that converges more rapidly because faster convergence reduces the amount of computation needed to calculate \u03c0 to any given accuracy. A simple infinite series for \u03c0 is the Gregory\u2013Leibniz series:\nformula_32\nAs individual terms of this infinite series are added to the sum, the total gradually gets closer to \u03c0, and\u00a0\u2013 with a sufficient number of terms\u00a0\u2013 can get as close to \u03c0 as desired. It converges quite slowly, though\u00a0\u2013 after 500,000 terms, it produces only five correct decimal digits of \u03c0.\nAn infinite series for \u03c0 (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory\u2013Leibniz series is:\nformula_33\nThe following table compares the convergence rates of these two series:\nAfter five terms, the sum of the Gregory\u2013Leibniz series is within 0.2 of the correct value of \u03c0, whereas the sum of Nilakantha's series is within 0.002 of the correct value. Nilakantha's series converges faster and is more useful for computing digits of \u03c0. Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term.\nIrrationality and transcendence.\nNot all mathematical advances relating to \u03c0 were aimed at increasing the accuracy of approximations. When Euler solved the Basel problem in 1735, finding the exact value of the sum of the reciprocal squares, he established a connection between \u03c0 and the prime numbers that later contributed to the development and study of the Riemann zeta function:\nformula_34\nSwiss scientist Johann Heinrich Lambert in 1768 proved that \u03c0 is irrational, meaning it is not equal to the quotient of any two integers. Lambert's proof exploited a continued-fraction representation of the tangent function. French mathematician Adrien-Marie Legendre proved in 1794 that \u03c02 is also irrational. In 1882, German mathematician Ferdinand von Lindemann proved that \u03c0 is transcendental, confirming a conjecture made by both Legendre and Euler. Hardy and Wright states that \"the proofs were afterwards modified and simplified by Hilbert, Hurwitz, and other writers\".\nAdoption of the symbol \u03c0.\nThe first recorded use of the symbol \u03c0 in circle geometry is in Oughtred's \"Clavis Mathematicae\" (1648),\nwhere the Greek letters \u03c0 and \"\u03b4\" were combined into the fraction &amp;NoBreak;&amp;NoBreak; for denoting the ratios semiperimeter to semidiameter and perimeter to diameter, that is, what is presently denoted as \u03c0. (Before then, mathematicians sometimes used letters such as \"c\" or \"p\" instead.) Barrow likewise used the same notation, while Gregory instead used formula_35 to represent 6.28...\u2009.\nThe earliest known use of the Greek letter \u03c0 alone to represent the ratio of a circle's circumference to its diameter was by Welsh mathematician William Jones in his 1706 work \"; or, a New Introduction to the Mathematics\". The Greek letter appears on p.\u00a0243 in the phrase \"formula_36 Periphery (\u03c0)\", calculated for a circle with radius one. However, Jones writes that his equations for \u03c0 are from the \"ready pen of the truly ingenious Mr. John Machin\", leading to speculation that Machin may have employed the Greek letter before Jones. Jones' notation was not immediately adopted by other mathematicians, with the fraction notation still being used as late as 1767.\nEuler started using the single-letter form beginning with his 1727 \"Essay Explaining the Properties of Air\", though he used \"\u03c0\" = 6.28..., the ratio of periphery to radius, in this and some later writing. Euler first used \u03c0 = 3.14... in his 1736 work \"Mechanica\", and continued in his widely read 1748 work (he wrote: \"for the sake of brevity we will write this number as \u03c0; thus \u03c0 is equal to half the circumference of a circle of radius 1\"). Because Euler corresponded heavily with other mathematicians in Europe, the use of the Greek letter spread rapidly, and the practice was universally adopted thereafter in the Western world, though the definition still varied between 3.14... and 6.28... as late as 1761.\nModern quest for more digits.\nMotives for computing \u03c0.\nFor most numerical calculations involving \u03c0, a handful of digits provide sufficient precision. According to J\u00f6rg Arndt and Christoph Haenel, thirty-nine digits are sufficient to perform most cosmological calculations, because that is the accuracy necessary to calculate the circumference of the observable universe with a precision of one atom. Accounting for additional digits needed to compensate for computational round-off errors, Arndt concludes that a few hundred digits would suffice for any scientific application. Despite this, people have worked strenuously to compute \u03c0 to thousands and millions of digits. This effort may be partly ascribed to the human compulsion to break records, and such achievements with \u03c0 often make headlines around the world. They also have practical benefits, such as testing supercomputers, testing numerical analysis algorithms (including high-precision multiplication algorithms) \u2013and within pure mathematics itself, providing data for evaluating the randomness of the digits of \u03c0.\nComputer era and iterative algorithms.\nThe development of computers in the mid-20th century again revolutionized the hunt for digits of \u03c0. Mathematicians John Wrench and Levi Smith reached 1,120 digits in 1949 using a desk calculator. Using an inverse tangent (arctan) infinite series, a team led by George Reitwiesner and John von Neumann that same year achieved 2,037 digits with a calculation that took 70 hours of computer time on the ENIAC computer. The record, always relying on an arctan series, was broken repeatedly (3089 digits in 1955, 7,480 digits in 1957; 10,000 digits in 1958; 100,000 digits in 1961) until 1 million digits was reached in 1973.\nTwo additional developments around 1980 once again accelerated the ability to compute \u03c0. First, the discovery of new iterative algorithms for computing \u03c0, which were much faster than the infinite series; and second, the invention of fast multiplication algorithms that could multiply large numbers very rapidly. Such algorithms are particularly important in modern \u03c0 computations because most of the computer's time is devoted to multiplication. They include the Karatsuba algorithm, Toom\u2013Cook multiplication, and Fourier transform-based methods.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nThe Gauss\u2013Legendre iterative algorithm:Initialize\nformula_37\nIterate\nformula_38\nformula_39\nThen an estimate for \u03c0 is given by\nformula_40\nThe iterative algorithms were independently published in 1975\u20131976 by physicist Eugene Salamin and scientist Richard Brent. These avoid reliance on infinite series. An iterative algorithm repeats a specific calculation, each iteration using the outputs from prior steps as its inputs, and produces a result in each step that converges to the desired value. The approach was actually invented over 160 years earlier by Carl Friedrich Gauss, in what is now termed the arithmetic\u2013geometric mean method (AGM method) or Gauss\u2013Legendre algorithm. As modified by Salamin and Brent, it is also referred to as the Brent\u2013Salamin algorithm.\nThe iterative algorithms were widely used after 1980 because they are faster than infinite series algorithms: whereas infinite series typically increase the number of correct digits additively in successive terms, iterative algorithms generally \"multiply\" the number of correct digits at each step. For example, the Brent\u2013Salamin algorithm doubles the number of digits in each iteration. In 1984, brothers John and Peter Borwein produced an iterative algorithm that quadruples the number of digits in each step; and in 1987, one that increases the number of digits five times in each step. Iterative methods were used by Japanese mathematician Yasumasa Kanada to set several records for computing \u03c0 between 1995 and 2002. This rapid convergence comes at a price: the iterative algorithms require significantly more memory than infinite series.\nRapidly convergent series.\nModern \u03c0 calculators do not use iterative algorithms exclusively. New infinite series were discovered in the 1980s and 1990s that are as fast as iterative algorithms, yet are simpler and less memory intensive. The fast iterative algorithms were anticipated in 1914, when Indian mathematician Srinivasa Ramanujan published dozens of innovative new formulae for \u03c0, remarkable for their elegance, mathematical depth and rapid convergence. One of his formulae, based on modular equations, is\nformula_41\nThis series converges much more rapidly than most arctan series, including Machin's formula. Bill Gosper was the first to use it for advances in the calculation of \u03c0, setting a record of 17 million digits in 1985. Ramanujan's formulae anticipated the modern algorithms developed by the Borwein brothers (Jonathan and Peter) and the Chudnovsky brothers. The Chudnovsky formula developed in 1987 is\nformula_42\nIt produces about 14 digits of \u03c0 per term and has been used for several record-setting \u03c0 calculations, including the first to surpass 1 billion (109) digits in 1989 by the Chudnovsky brothers, 10 trillion (1013) digits in 2011 by Alexander Yee and Shigeru Kondo, and 100 trillion digits by Emma Haruka Iwao in 2022. For similar formulae, see also the Ramanujan\u2013Sato series.\nIn 2006, mathematician Simon Plouffe used the PSLQ integer relation algorithm to generate several new formulae for \u03c0, conforming to the following template:\nformula_43\nwhere \"q\" is \"e\"\"\u03c0\" (Gelfond's constant), \"k\" is an odd number, and \"a\", \"b\", \"c\" are certain rational numbers that Plouffe computed.\nMonte Carlo methods.\nMonte Carlo methods, which evaluate the results of multiple random trials, can be used to create approximations of \u03c0. Buffon's needle is one such technique: If a needle of length \"\u2113\" is dropped \"n\" times on a surface on which parallel lines are drawn \"t\" units apart, and if \"x\" of those times it comes to rest crossing a line (\"x\" &gt; 0), then one may approximate \u03c0 based on the counts:\nformula_44\nAnother Monte Carlo method for computing \u03c0 is to draw a circle inscribed in a square, and randomly place dots in the square. The ratio of dots inside the circle to the total number of dots will approximately equal \u03c0/4.\nAnother way to calculate \u03c0 using probability is to start with a random walk, generated by a sequence of (fair) coin tosses: independent random variables \"Xk\" such that \"Xk\" \u2208 {\u22121,1} with equal probabilities. The associated random walk is\nformula_45\nso that, for each n, \"Wn\" is drawn from a shifted and scaled binomial distribution. As n varies, \"Wn\" defines a (discrete) stochastic process. Then \u03c0 can be calculated by\nformula_46\nThis Monte Carlo method is independent of any relation to circles, and is a consequence of the central limit theorem, discussed below.\nThese Monte Carlo methods for approximating \u03c0 are very slow compared to other methods, and do not provide any information on the exact number of digits that are obtained. Thus they are never used to approximate \u03c0 when speed or accuracy is desired.\nSpigot algorithms.\nTwo algorithms were discovered in 1995 that opened up new avenues of research into \u03c0. They are called spigot algorithms because, like water dripping from a spigot, they produce single digits of \u03c0 that are not reused after they are calculated. This is in contrast to infinite series or iterative algorithms, which retain and use all intermediate digits until the final result is produced.\nMathematicians Stan Wagon and Stanley Rabinowitz produced a simple spigot algorithm in 1995. Its speed is comparable to arctan algorithms, but not as fast as iterative algorithms.\nAnother spigot algorithm, the BBP digit extraction algorithm, was discovered in 1995 by Simon Plouffe:\nformula_47\nThis formula, unlike others before it, can produce any individual hexadecimal digit of \u03c0 without calculating all the preceding digits. Individual binary digits may be extracted from individual hexadecimal digits, and octal digits can be extracted from one or two hexadecimal digits. An important application of digit extraction algorithms is to validate new claims of record \u03c0 computations: After a new record is claimed, the decimal result is converted to hexadecimal, and then a digit extraction algorithm is used to calculate several randomly selected hexadecimal digits near the end; if they match, this provides a measure of confidence that the entire computation is correct.\nBetween 1998 and 2000, the distributed computing project PiHex used Bellard's formula (a modification of the BBP algorithm) to compute the quadrillionth (1015th) bit of \u03c0, which turned out to be 0. In September 2010, a Yahoo employee used the company's Hadoop application on one thousand computers over a 23-day period to compute 256 bits of \u03c0 at the two-quadrillionth (2\u00d71015th) bit, which also happens to be zero.\nIn 2022, Plouffe found a base-10 algorithm for calculating digits of \u03c0.\nRole and characterizations in mathematics.\nBecause \u03c0 is closely related to the circle, it is found in many formulae from the fields of geometry and trigonometry, particularly those concerning circles, spheres, or ellipses. Other branches of science, such as statistics, physics, Fourier analysis, and number theory, also include \u03c0 in some of their important formulae.\nGeometry and trigonometry.\n\u03c0 appears in formulae for areas and volumes of geometrical shapes based on circles, such as ellipses, spheres, cones, and tori. Below are some of the more common formulae that involve \u03c0.\nSome of the formulae above are special cases of the volume of the \"n\"-dimensional ball and the surface area of its boundary, the (\"n\"\u22121)-dimensional sphere, given below.\nApart from circles, there are other curves of constant width. By Barbier's theorem, every curve of constant width has perimeter \u03c0 times its width. The Reuleaux triangle (formed by the intersection of three circles with the sides of an equilateral triangle as their radii) has the smallest possible area for its width and the circle the largest. There also exist non-circular smooth and even algebraic curves of constant width.\nDefinite integrals that describe circumference, area, or volume of shapes generated by circles typically have values that involve \u03c0. For example, an integral that specifies half the area of a circle of radius one is given by:\nformula_48\nIn that integral, the function formula_49 represents the height over the formula_50-axis of a semicircle (the square root is a consequence of the Pythagorean theorem), and the integral computes the area below the semicircle. The existence of such integrals makes \u03c0 an algebraic period.\nUnit of angle.\nThe trigonometric functions rely on angles, and mathematicians generally use the radian as a unit of measurement. \u03c0 plays an important role in angles measured in radians, which are defined so that a complete circle spans an angle of 2\u03c0\u00a0radians. The angle measure of 180\u00b0 is equal to \u03c0\u00a0radians, and 1\u00b0 = \u03c0/180 radians.\nCommon trigonometric functions have periods that are multiples of \u03c0; for example, sine and cosine have period 2\u03c0, so for any angle \"\u03b8\" and any integer \"k\",\nformula_51\nEigenvalues.\nMany of the appearances of \u03c0 in the formulae of mathematics and the sciences have to do with its close relationship with geometry. However, \u03c0 also appears in many natural situations having apparently nothing to do with geometry.\nIn many applications, it plays a distinguished role as an eigenvalue. For example, an idealized vibrating string can be modelled as the graph of a function \"f\" on the unit interval [0, 1], with fixed ends \"f\"(0) = \"f\"(1) = 0. The modes of vibration of the string are solutions of the differential equation &amp;NoBreak;&amp;NoBreak;, or &amp;NoBreak;&amp;NoBreak;. Thus \"\u03bb\" is an eigenvalue of the second derivative operator &amp;NoBreak;&amp;NoBreak;, and is constrained by Sturm\u2013Liouville theory to take on only certain specific values. It must be positive, since the operator is negative definite, so it is convenient to write \"\u03bb\" = \"\u03bd\"2, where \"\u03bd\" &gt; 0 is called the wavenumber. Then \"f\"(\"x\") = sin(\"\u03c0\" \"x\") satisfies the boundary conditions and the differential equation with \"\u03bd\" = \"\u03c0\".\nThe value \u03c0 is, in fact, the \"least\" such value of the wavenumber, and is associated with the fundamental mode of vibration of the string. One way to show this is by estimating the energy, which satisfies Wirtinger's inequality: for a function formula_52 with \"f\"(0) = \"f\"(1) = 0 and \"f\", \"f\"\u2032 both square integrable, we have:\nformula_53\nwith equality precisely when \"f\" is a multiple of sin(\u03c0 \"x\"). Here \u03c0 appears as an optimal constant in Wirtinger's inequality, and it follows that it is the smallest wavenumber, using the variational characterization of the eigenvalue. As a consequence, \u03c0 is the smallest singular value of the derivative operator on the space of functions on [0, 1] vanishing at both endpoints (the Sobolev space formula_54).\nAnalysis and topology.\nAbove, \u03c0 was defined as the ratio of a circle's circumference to its diameter. The circumference of a circle is the arc length around the perimeter of the circle, a quantity which can be formally defined independently of geometry using limits\u2014a concept in calculus. For example, one may directly compute the arc length of the top half of the unit circle, given in Cartesian coordinates by the equation &amp;NoBreak;&amp;NoBreak;, as the integral:\nformula_55\nAn integral such as this was proposed as a definition of \u03c0 by Karl Weierstrass, who defined it directly as an integral in 1841.\nIntegration is no longer commonly used in a first analytical definition because, as explains, differential calculus typically precedes integral calculus in the university curriculum, so it is desirable to have a definition of \u03c0 that does not rely on the latter. One such definition, due to Richard Baltzer and popularized by Edmund Landau, is the following: \u03c0 is twice the smallest positive number at which the cosine function equals 0. \u03c0 is also the smallest positive number at which the sine function equals zero, and the difference between consecutive zeroes of the sine function. The cosine and sine can be defined independently of geometry as a power series, or as the solution of a differential equation.\nIn a similar spirit, \u03c0 can be defined using properties of the complex exponential, exp \"z\", of a complex variable \"z\". Like the cosine, the complex exponential can be defined in one of several ways. The set of complex numbers at which exp \"z\" is equal to one is then an (imaginary) arithmetic progression of the form:\nformula_56\nand there is a unique positive real number \u03c0 with this property.\nA variation on the same idea, making use of sophisticated mathematical concepts of topology and algebra, is the following theorem: there is a unique (up to automorphism) continuous isomorphism from the group R/Z of real numbers under addition modulo integers (the circle group), onto the multiplicative group of complex numbers of absolute value one. The number \u03c0 is then defined as half the magnitude of the derivative of this homomorphism.\nInequalities.\nThe number \u03c0 serves appears in similar eigenvalue problems in higher-dimensional analysis. As mentioned above, it can be characterized via its role as the best constant in the isoperimetric inequality: the area A enclosed by a plane Jordan curve of perimeter P satisfies the inequality\nformula_57\nand equality is clearly achieved for the circle, since in that case \"A\" = \u03c0\"r\"2 and \"P\" = 2\u03c0\"r\".\nUltimately, as a consequence of the isoperimetric inequality, \u03c0 appears in the optimal constant for the critical Sobolev inequality in \"n\" dimensions, which thus characterizes the role of \u03c0 in many physical phenomena as well, for example those of classical potential theory. In two dimensions, the critical Sobolev inequality is\nformula_58\nfor \"f\" a smooth function with compact support in R2, formula_59 is the gradient of \"f\", and formula_60 and formula_61 refer respectively to the L2 and L1-norm. The Sobolev inequality is equivalent to the isoperimetric inequality (in any dimension), with the same best constants.\nWirtinger's inequality also generalizes to higher-dimensional Poincar\u00e9 inequalities that provide best constants for the Dirichlet energy of an \"n\"-dimensional membrane. Specifically, \u03c0 is the greatest constant such that\nformula_62\nfor all convex subsets \"G\" of R\"n\" of diameter 1, and square-integrable functions \"u\" on \"G\" of mean zero. Just as Wirtinger's inequality is the variational form of the Dirichlet eigenvalue problem in one dimension, the Poincar\u00e9 inequality is the variational form of the Neumann eigenvalue problem, in any dimension.\nFourier transform and Heisenberg uncertainty principle.\nThe constant \u03c0 also appears as a critical spectral parameter in the Fourier transform. This is the integral transform, that takes a complex-valued integrable function \"f\" on the real line to the function defined as:\nformula_63\nAlthough there are several different conventions for the Fourier transform and its inverse, any such convention must involve \u03c0 \"somewhere\". The above is the most canonical definition, however, giving the unique unitary operator on \"L\"2 that is also an algebra homomorphism of \"L\"1 to \"L\"\u221e.\nThe Heisenberg uncertainty principle also contains the number \u03c0. The uncertainty principle gives a sharp lower bound on the extent to which it is possible to localize a function both in space and in frequency: with our conventions for the Fourier transform,\nformula_64\nThe physical consequence, about the uncertainty in simultaneous position and momentum observations of a quantum mechanical system, is discussed below. The appearance of \u03c0 in the formulae of Fourier analysis is ultimately a consequence of the Stone\u2013von Neumann theorem, asserting the uniqueness of the Schr\u00f6dinger representation of the Heisenberg group.\nGaussian integrals.\nThe fields of probability and statistics frequently use the normal distribution as a simple model for complex phenomena; for example, scientists generally assume that the observational error in most experiments follows a normal distribution. The Gaussian function, which is the probability density function of the normal distribution with mean \u03bc and standard deviation \u03c3, naturally contains \u03c0:\nformula_65\nThe factor of formula_66 makes the area under the graph of \"f\" equal to one, as is required for a probability distribution. This follows from a change of variables in the Gaussian integral:\nformula_67\nwhich says that the area under the basic bell curve in the figure is equal to the square root of \u03c0.\nThe central limit theorem explains the central role of normal distributions, and thus of \u03c0, in probability and statistics. This theorem is ultimately connected with the spectral characterization of \u03c0 as the eigenvalue associated with the Heisenberg uncertainty principle, and the fact that equality holds in the uncertainty principle only for the Gaussian function. Equivalently, \u03c0 is the unique constant making the Gaussian normal distribution \"e\"\u2212\u03c0\"x\"2 equal to its own Fourier transform. Indeed, according to , the \"whole business\" of establishing the fundamental theorems of Fourier analysis reduces to the Gaussian integral.\nTopology.\nThe constant \u03c0 appears in the Gauss\u2013Bonnet formula which relates the differential geometry of surfaces to their topology. Specifically, if a compact surface \u03a3 has Gauss curvature \"K\", then\nformula_68\nwhere \"\u03c7\"(\u03a3) is the Euler characteristic, which is an integer. An example is the surface area of a sphere \"S\" of curvature 1 (so that its radius of curvature, which coincides with its radius, is also 1.) The Euler characteristic of a sphere can be computed from its homology groups and is found to be equal to two. Thus we have\nformula_69\nreproducing the formula for the surface area of a sphere of radius 1.\nThe constant appears in many other integral formulae in topology, in particular, those involving characteristic classes via the Chern\u2013Weil homomorphism.\nCauchy's integral formula.\nOne of the key tools in complex analysis is contour integration of a function over a positively oriented (rectifiable) Jordan curve \"\u03b3\". A form of Cauchy's integral formula states that if a point \"z\"0 is interior to \"\u03b3\", then\nformula_70\nAlthough the curve \"\u03b3\" is not a circle, and hence does not have any obvious connection to the constant \u03c0, a standard proof of this result uses Morera's theorem, which implies that the integral is invariant under homotopy of the curve, so that it can be deformed to a circle and then integrated explicitly in polar coordinates. More generally, it is true that if a rectifiable closed curve \u03b3 does not contain \"z\"0, then the above integral is 2\u03c0\"i\" times the winding number of the curve.\nThe general form of Cauchy's integral formula establishes the relationship between the values of a complex analytic function \"f\"(\"z\") on the Jordan curve \"\u03b3\" and the value of \"f\"(\"z\") at any interior point \"z\"0 of \u03b3:\nformula_71\nprovided \"f\"(\"z\") is analytic in the region enclosed by \"\u03b3\" and extends continuously to \"\u03b3\". Cauchy's integral formula is a special case of the residue theorem, that if \"g\"(\"z\") is a meromorphic function the region enclosed by \"\u03b3\" and is continuous in a neighbourhood of \"\u03b3\", then\nformula_72\nwhere the sum is of the residues at the poles of \"g\"(\"z\").\nVector calculus and physics.\nThe constant \u03c0 is ubiquitous in vector calculus and potential theory, for example in Coulomb's law, Gauss's law, Maxwell's equations, and even the Einstein field equations. Perhaps the simplest example of this is the two-dimensional Newtonian potential, representing the potential of a point source at the origin, whose associated field has unit outward flux through any smooth and oriented closed surface enclosing the source:\nformula_73\nThe factor of formula_74 is necessary to ensure that formula_75 is the fundamental solution of the Poisson equation in formula_76:\nformula_77\nwhere formula_78 is the Dirac delta function.\nIn higher dimensions, factors of \u03c0 are present because of a normalization by the n-dimensional volume of the unit n sphere. For example, in three dimensions, the Newtonian potential is:\nformula_79\nwhich has the 2-dimensional volume (i.e., the area) of the unit 2-sphere in the denominator.\nTotal curvature.\nIn the differential geometry of curves, the \"total curvature\" of a smooth plane curve is the amount it turns anticlockwise, in radians, from start to finish, computed as the integral of signed curvature with respect to arc length:\nformula_80\nFor a closed curve, this quantity is equal to 2\"\u03c0N\" for an integer N called the \"turning number\" or \"index\" of the curve. N is the winding number about the origin of the hodograph of the curve parametrized by arclength, a new curve lying on the unit circle, described by the normalized tangent vector at each point on the original curve. Equivalently, N is the degree of the map taking each point on the curve to the corresponding point on the hodograph, analogous to the Gauss map for surfaces.\nGamma function and Stirling's approximation.\nThe factorial function formula_81 is the product of all of the positive integers through \"n\". The gamma function extends the concept of factorial (normally defined only for non-negative integers) to all complex numbers, except the negative real integers, with the identity formula_82. When the gamma function is evaluated at half-integers, the result contains \u03c0. For example, formula_83 and formula_84.\nThe gamma function is defined by its Weierstrass product development:\nformula_85\nwhere \u03b3 is the Euler\u2013Mascheroni constant. Evaluated at &amp;NoBreak;&amp;NoBreak; and squared, the equation &amp;NoBreak;&amp;NoBreak; reduces to the Wallis product formula. The gamma function is also connected to the Riemann zeta function and identities for the functional determinant, in which the constant \u03c0 plays an important role.\nThe gamma function is used to calculate the volume \"V\"\"n\"(\"r\") of the \"n\"-dimensional ball of radius \"r\" in Euclidean \"n\"-dimensional space, and the surface area \"S\"\"n\"\u22121(\"r\") of its boundary, the (\"n\"\u22121)-dimensional sphere:\nformula_86\nformula_87\nFurther, it follows from the functional equation that\nformula_88\nThe gamma function can be used to create a simple approximation to the factorial function \"n\"! for large \"n\": formula_89 which is known as Stirling's approximation. Equivalently,\nformula_90\nAs a geometrical application of Stirling's approximation, let \u0394\"n\" denote the standard simplex in \"n\"-dimensional Euclidean space, and (\"n\"\u00a0+\u00a01)\u0394\"n\" denote the simplex having all of its sides scaled up by a factor of \"n\"\u00a0+\u00a01. Then\nformula_91\nEhrhart's volume conjecture is that this is the (optimal) upper bound on the volume of a convex body containing only one integer lattice point.\nNumber theory and Riemann zeta function.\nThe Riemann zeta function \"\u03b6\"(\"s\") is used in many areas of mathematics. When evaluated at \"s\" = 2 it can be written as\nformula_92\nFinding a simple solution for this infinite series was a famous problem in mathematics called the Basel problem. Leonhard Euler solved it in 1735 when he showed it was equal to \u03c02/6. Euler's result leads to the number theory result that the probability of two random numbers being relatively prime (that is, having no shared factors) is equal to 6/\u03c02. This probability is based on the observation that the probability that any number is divisible by a prime \"p\" is 1/\"p\" (for example, every 7th integer is divisible by 7.) Hence the probability that two numbers are both divisible by this prime is 1/\"p\"2, and the probability that at least one of them is not is 1\u00a0\u2212\u00a01/\"p\"2. For distinct primes, these divisibility events are mutually independent; so the probability that two numbers are relatively prime is given by a product over all primes:\nformula_93\nThis probability can be used in conjunction with a random number generator to approximate \u03c0 using a Monte Carlo approach.\nThe solution to the Basel problem implies that the geometrically derived quantity \u03c0 is connected in a deep way to the distribution of prime numbers. This is a special case of Weil's conjecture on Tamagawa numbers, which asserts the equality of similar such infinite products of \"arithmetic\" quantities, localized at each prime \"p\", and a \"geometrical\" quantity: the reciprocal of the volume of a certain locally symmetric space. In the case of the Basel problem, it is the hyperbolic 3-manifold SL2(R)/SL2(Z).\nThe zeta function also satisfies Riemann's functional equation, which involves \u03c0 as well as the gamma function:\nformula_94\nFurthermore, the derivative of the zeta function satisfies\nformula_95\nA consequence is that \u03c0 can be obtained from the functional determinant of the harmonic oscillator. This functional determinant can be computed via a product expansion, and is equivalent to the Wallis product formula. The calculation can be recast in quantum mechanics, specifically the variational approach to the spectrum of the hydrogen atom.\nFourier series.\nThe constant \u03c0 also appears naturally in Fourier series of periodic functions. Periodic functions are functions on the group T \n R/Z of fractional parts of real numbers. The Fourier decomposition shows that a complex-valued function \"f\" on T can be written as an infinite linear superposition of unitary characters of T. That is, continuous group homomorphisms from T to the circle group \"U\"(1) of unit modulus complex numbers. It is a theorem that every character of T is one of the complex exponentials formula_96.\nThere is a unique character on T, up to complex conjugation, that is a group isomorphism. Using the Haar measure on the circle group, the constant \u03c0 is half the magnitude of the Radon\u2013Nikodym derivative of this character. The other characters have derivatives whose magnitudes are positive integral multiples of 2\u03c0. As a result, the constant \u03c0 is the unique number such that the group T, equipped with its Haar measure, is Pontrjagin dual to the lattice of integral multiples of 2\u03c0. This is a version of the one-dimensional Poisson summation formula.\nIn Fourier analysis, the number \u03c0 rather than 2\u03c0 also appears, and sometimes this difference has important consequences. The basic exponential formula_97 is no longer a character of the group T, but instead is twisted by a sign after one turn of the circle group. This is known as a projective representation: it is a representation not of T but of its double cover. It is the most basic projective representation, being associated with the most elementary compact group, and \u03c0 (rather than 2\u03c0) often appears in projective representations requiring a double cover. Spinors, for instance, exhibit this behavior in physics, representing rotations with a twist by a sign. Certain representations of the group SL(2,R) of formula_98 real matrices of determinant one also require this extra twist, as do representations of the Heisenberg group.\nModular forms and theta functions.\nThe constant \u03c0 is connected in a deep way with the theory of modular forms and theta functions. For example, the Chudnovsky algorithm involves in an essential way the j-invariant of an elliptic curve.\nModular forms are holomorphic functions in the upper half plane characterized by their transformation properties under the modular group formula_99 (or its various subgroups), a lattice in the group &amp;NoBreak;&amp;NoBreak;. An example is the Jacobi theta function\nformula_100\nwhich is a kind of modular form called a Jacobi form. This is sometimes written in terms of the nome formula_101.\nThe constant \u03c0 is the unique constant making the Jacobi theta function an automorphic form, which means that it transforms in a specific way. Certain identities hold for all automorphic forms. An example is\nformula_102\nwhich implies that \u03b8 transforms as a representation under the discrete Heisenberg group. General modular forms and other theta functions also involve \u03c0, once again because of the Stone\u2013von Neumann theorem.\nCauchy distribution and potential theory.\nThe Cauchy distribution\nformula_103\nis a probability density function. The total probability is equal to one, owing to the integral:\nformula_104\nThe Shannon entropy of the Cauchy distribution is equal to ln(4\u03c0), which also involves \u03c0.\nThe Cauchy distribution plays an important role in potential theory because it is the simplest Furstenberg measure, the classical Poisson kernel associated with a Brownian motion in a half-plane. Conjugate harmonic functions and so also the Hilbert transform are associated with the asymptotics of the Poisson kernel. The Hilbert transform \"H\" is the integral transform given by the Cauchy principal value of the singular integral\nformula_105\nThe constant \u03c0 is the unique (positive) normalizing factor such that \"H\" defines a linear complex structure on the Hilbert space of square-integrable real-valued functions on the real line. The Hilbert transform, like the Fourier transform, can be characterized purely in terms of its transformation properties on the Hilbert space L2(R): up to a normalization factor, it is the unique bounded linear operator that commutes with positive dilations and anti-commutes with all reflections of the real line. The constant \u03c0 is the unique normalizing factor that makes this transformation unitary.\nIn the Mandelbrot set.\nAn occurrence of \u03c0 in the fractal called the Mandelbrot set was discovered by David Boll in 1991. He examined the behaviour of the Mandelbrot set near the \"neck\" at (\u22120.75, 0). When the number of iterations until divergence for the point (\u22120.75, \"\u03b5\") is multiplied by \u03b5, the result approaches \u03c0 as \u03b5 approaches zero. The point (0.25 + \"\u03b5\", 0) at the cusp of the large \"valley\" on the right side of the Mandelbrot set behaves similarly: the number of iterations until divergence multiplied by the square root of \u03b5 tends to \u03c0.\nOutside mathematics.\nDescribing physical phenomena.\nAlthough not a physical constant, \u03c0 appears routinely in equations describing physical phenomena, often because of \u03c0's relationship to the circle and to spherical coordinate systems. A simple formula from the field of classical mechanics gives the approximate period \"T\" of a simple pendulum of length \"L\", swinging with a small amplitude (\"g\" is the earth's gravitational acceleration):\nformula_106\nOne of the key formulae of quantum mechanics is Heisenberg's uncertainty principle, which shows that the uncertainty in the measurement of a particle's position (\u0394\"x\") and momentum (\u0394\"p\") cannot both be arbitrarily small at the same time (where \"h\" is the Planck constant):\nformula_107\nThe fact that \u03c0 is approximately equal to 3 plays a role in the relatively long lifetime of orthopositronium. The inverse lifetime to lowest order in the fine-structure constant \"\u03b1\" is\nformula_108\nwhere \"m\"e is the mass of the electron.\n\u03c0 is present in some structural engineering formulae, such as the buckling formula derived by Euler, which gives the maximum axial load \"F\" that a long, slender column of length \"L\", modulus of elasticity \"E\", and area moment of inertia \"I\" can carry without buckling:\nformula_109\nThe field of fluid dynamics contains \u03c0 in Stokes' law, which approximates the frictional force \"F\" exerted on small, spherical objects of radius \"R\", moving with velocity \"v\" in a fluid with dynamic viscosity \"\u03b7\":\nformula_110\nIn electromagnetics, the vacuum permeability constant \"\u03bc\"0 appears in Maxwell's equations, which describe the properties of electric and magnetic fields and electromagnetic radiation. Before 20 May 2019, it was defined as exactly\nformula_111\nMemorizing digits.\nPiphilology is the practice of memorizing large numbers of digits of \u03c0, and world-records are kept by the \"Guinness World Records\". The record for memorizing digits of \u03c0, certified by Guinness World Records, is 70,000 digits, recited in India by Rajveer Meena in 9 hours and 27 minutes on 21 March 2015. In 2006, Akira Haraguchi, a retired Japanese engineer, claimed to have recited 100,000 decimal places, but the claim was not verified by Guinness World Records.\nOne common technique is to memorize a story or poem in which the word lengths represent the digits of \u03c0: The first word has three letters, the second word has one, the third has four, the fourth has one, the fifth has five, and so on. Such memorization aids are called mnemonics. An early example of a mnemonic for pi, originally devised by English scientist James Jeans, is \"How I want a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\" When a poem is used, it is sometimes referred to as a \"piem\". Poems for memorizing \u03c0 have been composed in several languages in addition to English. Record-setting \u03c0 memorizers typically do not rely on poems, but instead use methods such as remembering number patterns and the method of loci.\nA few authors have used the digits of \u03c0 to establish a new form of constrained writing, where the word lengths are required to represent the digits of \u03c0. The \"Cadaeic Cadenza\" contains the first 3835 digits of \u03c0 in this manner, and the full-length book \"Not a Wake\" contains 10,000 words, each representing one digit of \u03c0.\nIn popular culture.\nPerhaps because of the simplicity of its definition and its ubiquitous presence in formulae, \u03c0 has been represented in popular culture more than other mathematical constructs.\nIn the Palais de la D\u00e9couverte (a science museum in Paris) there is a circular room known as the \"pi room\". On its wall are inscribed 707 digits of \u03c0. The digits are large wooden characters attached to the dome-like ceiling. The digits were based on an 1873 calculation by English mathematician William Shanks, which included an error beginning at the 528th digit. The error was detected in 1946 and corrected in 1949.\nIn Carl Sagan's 1985 novel \"Contact\" it is suggested that the creator of the universe buried a message deep within the digits of \u03c0. This part of the story was omitted from the film adaptation of the novel. The digits of \u03c0 have also been incorporated into the lyrics of the song \"Pi\" from the 2005 album \"Aerial\" by Kate Bush. In the 1967 \"\" episode \"Wolf in the Fold\", a computer possessed by a demonic entity is contained by being instructed to \"Compute to the last digit the value of \u03c0\".\nIn the United States, Pi Day falls on 14\u00a0March (written 3/14 in the US style), and is popular among students. \u03c0 and its digital representation are often used by self-described \"math geeks\" for inside jokes among mathematically and technologically minded groups. A college cheer variously attributed to the Massachusetts Institute of Technology or the Rensselaer Polytechnic Institute includes \"3.14159\". Pi Day in 2015 was particularly significant because the date and time 3/14/15 9:26:53 reflected many more digits of pi. In parts of the world where dates are commonly noted in day/month/year format, 22 July represents \"Pi Approximation Day\", as 22/7\u00a0\u2248\u00a03.142857.\nSome have proposed replacing \u03c0 by \"\u03c4\" = 2\"\u03c0\", arguing that \u03c4, as the number of radians in one turn or the ratio of a circle's circumference to its radius, is more natural than \u03c0 and simplifies many formulae. This use of \u03c4 has not made its way into mainstream mathematics, but since 2010 this has led to people celebrating Two Pi Day or Tau Day on June 28.\nIn 1897, an amateur mathematician attempted to persuade the Indiana legislature to pass the Indiana Pi Bill, which described a method to square the circle and contained text that implied various incorrect values for \u03c0, including 3.2. The bill is notorious as an attempt to establish a value of mathematical constant by legislative fiat. The bill was passed by the Indiana House of Representatives, but rejected by the Senate, and thus it did not become a law.\nIn contemporary internet culture, individuals and organizations frequently pay homage to the number \u03c0. For instance, the computer scientist Donald Knuth let the version numbers of his program TeX approach \u03c0. The versions are 3, 3.1, 3.14, and so forth.\nReferences.\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23602", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=23602", "title": "Post Modernism", "text": ""}
{"id": "23603", "revid": "50865928", "url": "https://en.wikipedia.org/wiki?curid=23603", "title": "Postmodernism", "text": "Artistic, cultural, and theoretical movement\nPostmodernism encompasses a variety of artistic, cultural, and philosophical movements. It emerged in the mid-20th century as a skeptical response to modernism, emphasizing the instability of meaning, rejection of universal truths, and critique of grand narratives. While its definition varies across disciplines, it commonly involves skepticism toward established norms, blending of styles, and attention to the socially constructed nature of knowledge and reality.\nThe term began to acquire its current range of meanings in literary criticism and architectural theory during the 1950s\u20131960s. In opposition to modernism's alleged self-seriousness, postmodernism is characterized by its playful use of eclectic styles and performative irony, among other features. Critics claim it supplants moral, political, and aesthetic ideals with mere style and spectacle.\nIn the 1990s, \"postmodernism\" came to denote a generally celebratory response to cultural pluralism. Proponents align themselves with feminism, multiculturalism, and postcolonialism. Building upon poststructural theory, postmodern thought defined itself by the rejection of any single, foundational historical narrative. This called into question the legitimacy of the Enlightenment account of progress and rationality. Critics allege that its premises lead to a nihilistic form of relativism. In this sense, it has become a term of abuse in popular culture.\nHistorically, it arose alongside industrialization, globalization, and cultural upheaval, with early uses in art and literature evolving into philosophical and social theory through figures like Lyotard, Derrida, Foucault, Baudrillard, and Jameson. In practice, postmodernism manifests in arts, architecture, literature, music, dance, theater, fashion, marketing, and academic fields by embracing plurality, pastiche, reflexivity, and relativism. Although some argue postmodernism has waned, its influence persists in contemporary culture, now sometimes transitioning into so-called post-postmodern or reconstructive movements.\nDefinitions.\n\"Postmodernism\" is \"a highly contested term\", referring to \"a particularly unstable concept\", that \"names many different kinds of cultural objects and phenomena in many different ways\". It may be described simply as a general mood or \"Zeitgeist\".\nAlthough postmodernists are generally united in their effort to transcend the perceived limits of modernism, \"modernism\" also means different things to different critics in various arts. Further, there are outliers on even this basic stance; for instance, literary critic William Spanos conceives postmodernism not in period terms but in terms of a certain kind of literary imagination so that pre-modern texts such as Euripides' \"Orestes\" or Cervantes' \"Don Quixote\" count as postmodern.\nAccording to scholar Louis Menand, \"Postmodernism is the Swiss Army knife of critical concepts. It's definitionally overloaded, and it can do almost any job you need done.\" From an opposing perspective, media theorist Dick Hebdige criticized the vagueness of the term, enumerating a long list of otherwise unrelated concepts that people have designated as postmodernism, from \"the d\u00e9cor of a room\" or \"a 'scratch' video\", to fear of nuclear armageddon and the \"implosion of meaning\", and stated that anything that could signify all of those things was \"a buzzword\".\nAll this notwithstanding, scholar Hans Bertens offers the following:\nIf there is a common denominator to all these postmodernisms, it is that of a crisis in representation: a deeply felt loss of faith in our ability to represent the real, in the widest sense. No matter whether they are aesthestic [sic], epistemological, moral, or political in nature, the representations that we used to rely on can no longer be taken for granted.\nIn practical terms, postmodernisms share an attitude of skepticism towards grand explanations and established ways of doing things. In art, literature, and architecture, this attitude blurs boundaries between styles and genres, and encourages freely mixing elements, challenging traditional distinctions like high art versus popular art. In science, it emphasizes multiple ways of seeing things, and how our cultural and personal backgrounds shape how we see the world, making it impossible to be completely objective. In philosophy, education, history, politics, and many other fields, it encourages critical re-examination of established institutions and social norms, embracing diversity, and breaking down disciplinary boundaries. Though these ideas weren't strictly new, postmodernism amplified them, using an often playful, at times deeply critical, attitude of pervasive skepticism to turn them into defining features.\nHistorical overview.\nTwo broad cultural movements, modernism and postmodernism, emerged in response to profound changes in the Western world. The Industrial Revolution, urbanization, secularization, technological advances, two world wars, and globalization deeply disrupted the social order. Modernism emerged in the late 1800s, seeking to redefine fundamental truths and values through a radical rethinking of traditional ideas and forms across many fields. Postmodernism emerged in the mid-20th century with a skeptical perspective that questioned the notion of universal truths and reshaped modernist approaches by embracing the complexity and contradictions of modern life.\nThe term \"postmodernism\" first appeared in print in 1870, but it only began to enter circulation with its current range of meanings in the 1950s\u201460s.\nEarly appearances.\nThe term \"postmodern\" was first used in 1870 by the artist John Watkins Chapman, who described \"a Postmodern style of painting\" as a departure from French Impressionism. Similarly, the first citation given by the \"Oxford English Dictionary\" is dated to 1916, describing Gus Mager as \"one of the few 'post' modern painters whose style is convincing\".\nEpiscopal priest and cultural commentator J. M. Thompson, in a 1914 article, uses the term to describe changes in attitudes and beliefs in the critique of religion, writing, \"the \"raison d'\u00eatre\" of Post-Modernism is to escape from the double-mindedness of modernism by being thorough in its criticism by extending it to religion as well as theology, to Catholic feeling as well as to Catholic tradition\". Cultural critic Randolph Bourne used the word to describe Japan in his essay \"Trans-National America.\" In 1926, Bernard Iddings Bell, president of St. Stephen's College and also an Episcopal priest, published \"Postmodernism and Other Essays\", which marks the first use of the term to describe a historical period following modernity. The essay criticizes lingering socio-cultural norms, attitudes, and practices of the Enlightenment. It is also critical of a purported cultural shift away from traditional Christian beliefs.\nThe term \"postmodernity\" was first used in an academic historical context as a general concept for a movement by Arnold J. Toynbee in a 1939 essay, which states that \"Our own Post-Modern Age has been inaugurated by the general war of 1914\u20131918\".\nIn 1942, the literary critic and author H. R. Hays described postmodernism as a new literary form. Also in the arts, the term was first used in 1949 to describe a dissatisfaction with the modernist architectural movement known as the International Style.\nAlthough these early uses anticipate some of the concerns of the debate in the second part of the 20th century, there is little direct continuity in the discussion. Just when the new discussion begins, however, is also a matter of dispute. Various authors place its beginnings in the 1950s, 1960s, 1970s, and 1980s.\nTheoretical development.\nIn the mid-1970s, the American sociologist Daniel Bell provided a general account of the postmodern as an effectively nihilistic response to modernism's alleged assault on the Protestant work ethic and its rejection of what he upheld as traditional values. The ideals of modernity, per his diagnosis, were degraded to the level of consumer choice. This research project, however, was not taken up in a significant way by others until the mid-1980s when the work of Jean Baudrillard and Fredric Jameson, building upon art and literary criticism, reintroduced the term to sociology.\nDiscussion about the postmodern in the second part of the 20th century was most articulate in areas with a large body of critical discourse around the modernist movement. Even here, however, there continued to be disagreement about such basic issues as whether postmodernism is a break with modernism, a renewal and intensification of modernism, or even, both at once, a rejection and a radicalization of its historical predecessor.\nWhile discussions in the 1970s were dominated by literary criticism, these were supplanted by architectural theory in the 1980s. Some of these conversations made use of French poststructuralist thought, but only after these innovations and critical discourse in the arts did postmodernism emerge as a philosophical term in its own right.\nIn literary and architectural theory.\nAccording to Hans Bertens and Perry Anderson, the Black Mountain poets Charles Olson and Robert Creeley first introduced the term \"postmodern\" in its current sense during the 1950s.\nTheir stance against modernist poetry \u2013 and Olson's Heideggerian orientation \u2013 was influential in the identification of postmodernism as a polemical position opposed to the rationalist values championed by the Enlightenment project.\nDuring the 1960s, this affirmative use gave way to a pejorative use by the New Left, who used it to describe a waning commitment among youth to the political ideals of socialism and communism. The literary critic Irving Howe, for instance, denounced postmodern literature for being content to merely reflect, rather than actively attempt to refashion, what he saw as the \"increasingly shapeless\" character of contemporary society.\nIn the 1970s, this changed again, largely under the influence of the literary critic Ihab Hassan's large-scale survey of works that he said could no longer be called modern. Taking the Black Mountain poets as an exemplary instance of the new postmodern type, Hassan celebrates its Nietzschean playfulness and cheerfully anarchic spirit, which he sets off against the high seriousness of modernism.\nIf literature were at the center of the discussion in the 1970s, architecture was at the center in the 1980s. The architectural theorist Charles Jencks, in particular, connected the artistic avant-garde to social change in a way that captured attention outside of academia. Jencks, much influenced by the American architect Robert Venturi, celebrated a plurality of forms and encourages participation and active engagement with the local context of the built environment. He presented this as in opposition to the \"authoritarian style\" of International Modernism.\nThe influence of poststructuralism.\nIn the 1970s, postmodern criticism increasingly came to incorporate poststructuralist theory, particularly the deconstructive approach to texts most strongly associated with Jacques Derrida, who attempted to demonstrate that the whole foundationalist approach to language and knowledge was untenable and misguided. It is during this period that postmodernism came to be particularly equated with a kind of anti-representational self-reflexivity.\nIn the 1980s, some critics began to take an interest in the work of Michel Foucault. This introduced a political concern about social power-relations into discussions about postmodernism. This was also the beginning of the affiliation of postmodernism with feminism and multiculturalism. The art critic Craig Owens, in particular, not only made the connection to feminism explicit, but went so far as to claim feminism for postmodernism wholesale, a broad claim resisted by even many sympathetic feminists such as Nancy Fraser and Linda Nicholson.\nGeneralization.\nAlthough postmodern criticism and thought drew on philosophical ideas from early on, \"postmodernism\" was only introduced to the expressly philosophical lexicon by Jean-Fran\u00e7ois Lyotard in his 1979 \"\". This work served as a catalyst for many of the subsequent intellectual debates around the term.\nBy the 1990s, postmodernism had become increasingly identified with critical and philosophical discourse directly about postmodernity or the postmodern idiom itself. No longer centered on any particular art or even the arts in general, it instead turned to address the more general problems posed to society in general by a new proliferation of cultures and forms. It is during this period that it also came to be associated with postcolonialism and identity politics.\nAround this time, postmodernism also began to be conceived in popular culture as a general \"philosophical disposition\" associated with a loose sort of relativism. In this sense, the term also started to appear as a \"casual term of abuse\" in non-academic contexts. Others identified it as an aesthetic \"lifestyle\" of eclecticism and playful self-irony.\nThe \"Science Wars\".\nThe basis for what became known later as the Science Wars was the 1962 publication of \"The Structure of Scientific Revolutions\" by the physicist and historian of science Thomas Kuhn. Kuhn presented the direction of scientific inquiry \u2014 the kind of questions that can be asked, and what counts as a correct answer \u2014 as governed by a \"paradigm\" defining what counts as \"normal science\" during any given period. While not based on postmodern ideas or Continental philosophy, Kuhn's intervention set the agenda for much of \"The Postmodern Condition\" and has subsequently been presented as the beginning of \"postmodern epistemology\" in the philosophy of science.\nIn Kuhn's 1962 framework, the assumptions introduced by new paradigms make them \"mutually incommensurable\" with previous ones, although they may provide improved explanations of the material world. A more radical version of incommensurablity, introduced by the philosopher of science Paul Feyerabend, made stronger claims that connected the largely Anglo-American debate about science to the development of poststructuralism in France.\nTo some, the stakes were more than epistemological. The philosopher Israel Scheffler, for instance, argued that the ever-expanding body of scientific knowledge embodies a sort of \"moral principle\" protecting society from its authoritarian and tribal tendencies. In this way, with the addition of the poststructuralist influence, the debate about science expanded into a debate about Western culture in general. \nThe French political philosophers Alain Renaut and Luc Ferry began a series of responses to this interpretation of postmodernism, and these inspired the physicist Alan Sokal to submit a deliberately nonsensical paper to a postmodernist journal, where it was accepted and published in 1996. Although the so-called Sokal hoax proved nothing about postmodernism or science, it added to the public perception of a high-stakes intellectual \"war\" that had already been introduced to the general public by popular books published in the late '80s and '90s. By the late '90s, however, the debate had largely subsided, in part due to the recognition that it had been staged between strawman versions of postmodernism and science alike.\nIn the arts.\nPostmodernism encompasses a wide range of artistic movements and styles. In visual arts, pop art, conceptual art, feminist art, video art, minimalism, and neo-expressionism are among the approaches recognized as postmodern. The label extends to diverse musical genres and artists: John Cage, Madonna, and punk rock all meet postmodern definitions. Literature, film, architecture, theater, fashion, dance, and many other creative disciplines saw postmodern expression. As an example, Andy Warhol's pop art across multiple mediums challenged traditional distinctions between high and low culture, and blurred the lines between fine art and commercial design. His work, exemplified by the iconic \"Campbell's Soup Cans\" series during the 1960s, brought the postmodernist sensibility to mainstream attention.\nCriticism of postmodernist movements in the arts include objections to departure from beauty, the reliance on language for the art to have meaning, a lack of coherence or comprehensibility, deviation from clear structure, and consistent use of dark and negative themes.\nArchitecture.\nScholarship regarding postmodernism and architecture is closely linked with the writings of critic-turned-architect Charles Jencks, beginning with lectures in the early 1970s and his essay \"The Rise of Post-Modern Architecture\" from 1975. His \"magnum opus\", however, is the book \"The Language of Post-Modern Architecture\", first published in 1977, and since running to seven editions (in which he famously wrote: \"Modern architecture died in St. Louis, Missouri, on 15 July 1972 at 3:32 p.m. (or thereabouts) when the infamous Pruitt\u2013Igoe scheme, or rather several of its slab blocks, were given the final coup de gr\u00e2ce by dynamite.\").\nJencks makes the point that postmodernism (like modernism) varies for each field of art, and that for architecture it is not just a reaction to modernism but what he terms \"double coding\": \"Double Coding: the combination of Modern techniques with something else (usually traditional building) in order for architecture to communicate with the public and a concerned minority, usually other architects.\"\nIn their book, \"Revisiting Postmodernism\", Terry Farrell and Adam Furman argue that postmodernism brought a more joyous and sensual experience to the culture, particularly in architecture. For instance, in response to the modernist slogan of Ludwig Mies van der Rohe that \"less is more\", the postmodernist Robert Venturi rejoined that \"less is a bore\".\nDance.\nThe term \"postmodern dance\" is most strongly associated with the Judson Dance Theater, located in New York's Greenwich Village during the 1960s and 1970s. Perhaps its most important principle is taken from the composer John Cage's efforts to break down the distinction between art and life, developed in particular by the American dancer and choreographer Merce Cunningham, Cage's partner. The Judson dancers \"[stripped] dance of its theatrical conventions such as virtuoso technique, fanciful costumes, complex storylines, and the traditional stage [and] drew on everyday movements (sitting, walking, kneeling, and other gestures) to create their pieces, often performing them in ordinary spaces.\" Anna Halprin's San Francisco Dancers' Workshop, established in the 1950s to explore beyond the technical constraints of modern dance, pioneered ideas later developed at Judson; Halprin, Simone Forti, and Yvonne Rainer are considered \"giants of the field\".\nThe Judson collective included trained dancers, visual artists, filmmakers, writers, and composers, exchanging approaches, and critiquing traditional dance, with a focus \"more on the intellectual process of creating dance than the end result\". The end of the 1970s saw a distancing from this analytical postmodern dance, and a return to the expression of meaning. In the 1980s and 1990s, dance began to incorporate other typically postmodern features such as the mixing of genres, challenging high\u2013low cultural distinctions, and incorporating a political dimension.\nFilm.\nPostmodern film aims to subvert the mainstream conventions of narrative structure and characterization, and to test the audience's suspension of disbelief. Typically, such films also break down the cultural divide between high and low art and often upend typical portrayals of gender, race, class, genre, and time with the goal of creating something that does not abide by traditional narrative expression.\nCertain key characteristics are used to separate the postmodern from modernist cinema and traditional narrative film. One is an extensive use of homage or pastiche, imitating the style or character of other artistic works. A second is meta-reference or self-reference, highlighting the relation of the image to other images in media and not to any kind of external reality. Viewers are reminded that the film itself is only a film, perhaps through the use of intertextuality, in which the film's characters reference other works of fiction. A third characteristic is stories that unfold out of chronological order, deconstructing or fragmenting time to emphasize the constructed nature of film. Another common element is a bridging of the gap between highbrow and lowbrow. Contradictions of all sorts are crucial to postmodernism.\nRidley Scott's \"Blade Runner\" (1982) has been widely studied as a prime example of postmodernism. The setting is a future dystopia where \"replicants\", enhanced android workers nearly indistinguishable from humans, are hunted down when they escape from their jobs. The film blurs boundaries between genres and cultures, and fuses disparate styles and periods: futuristic visuals \"mingle with drab 1940s clothes and offices, punk rock hairstyles, pop Egyptian styles and oriental culture.\" The blending of film noir and science-fiction into tech noir illustrates the deconstruction of both cinema and genre. The film can also be seen as an example of major studios using the \"mystique and cachet of the term 'postmodern' as a sales pitch\", resulting in Hollywood movies that \"demonstrate all the postmodern characteristics\". From another perspective, \"critical responses to \"Blade Runner\" fall on either side of a modern/postmodern line\" \u2013 critical analysis from \"modernist\" and \"postmodernist\" approaches produce entirely different interpretations.\nLiterature.\nIn 1971, the American literary theorist Ihab Hassan made \"postmodernism\" popular in literary studies with his influential book, \"The Dismemberment of Orpheus: Toward a Postmodern Literature\". According to scholar David Herwitz, American writers such as John Barth (who had controversially declared that the novel was \"exhausted\" as a genre), Donald Barthelme, and Thomas Pynchon responded in various ways to the stylistic innovations of \"Finnegans Wake\" and the late work of Samuel Beckett. Postmodern literature often calls attention to issues regarding its own complicated connection to reality. The postmodern novel plays with language, twisted plots, multiple narrators, and unresolved endings, unsettling the conventional idea of the novel as faithfully reflecting the world.\nIn \"Postmodernist Fiction\" (1987), Brian McHale details the shift from modernism to postmodernism, arguing that postmodernist works developed out of modernism, moving from concern with questions about the nature and limits of knowledge about one's \"world\" (\"epistemological dominant\") to concern with questions of modes of being and existence in relation to \"different kinds of worlds\" (\"ontological dominant\"). McHale's \"What Was Postmodernism?\" (2007) follows Raymond Federman's lead in now using the past tense when discussing postmodernism. Others argue that postmodernism in literature utilizes compositional and semantic practices such as inclusivity, intentional indiscrimination, nonselection, and \"logical impossibility.\"\nMusic.\nPostmodern influence extends across all areas of music; its accessibility to a general audience requires an understanding of references, irony, and pastiche that varies widely between artists and their works. In popular music, Madonna, David Bowie, and Talking Heads have been singled out by critics and scholars as postmodern icons. The belief that art music \u2013 serious, classical music \u2013 holds higher cultural and technical value than folk and popular traditions, lost influence under postmodern analysis, as musical hybrids and crossovers attracted scholarly attention.\nAcross musical traditions, postmodernism can be identified through several core characteristics: genre mixing; irony, humor, and self-parody; \"surface\" exploration with less concern for formal structure than in modernist approaches; and a return to tonality. This represents a loss of authority of the Eurocentric perspective on music and the rise of world music as influenced by postmodern values. Composers took different routes: some returned to traditional modes over experimentation, others challenged the authority of dominant musical structures, others intermingled disparate sources.\nThe composer Jonathan Kramer has written that avant-garde musical compositions (which some would consider modernist rather than postmodernist) \"defy more than seduce the listener, and they extend by potentially unsettling means the very idea of what music is.\" In the 1960s, composers such as Henryk G\u00f3recki and Philip Glass reacted to the perceived elitism and dissonant sound of atonal academic modernism by producing music with simple textures and relatively consonant harmonies, whilst others, most notably John Cage challenged the modernist account of structure by including the contingent in the structure of his compositions themselves.\nIn 2023, music critic Andy Cush described Talking Heads as \"New York art-punks\" whose \"blend of nervy postmodernism and undeniable groove made them one of the defining rock bands of the late 1970s and '80s.\" Media theorist Dick Hebdige, examining the \"Road to Nowhere\" music video in 1989, said the group \"draw eclectically on a wide range of visual and aural sources to create a distinctive pastiche or hybrid 'house style' which they have used since their formation in the mid-1970s deliberately to stretch received (industrial) definitions of what rock/pop/video/Art/performance/audience are\", calling them \"a properly postmodernist band.\" According to lead vocalist/guitarist/songwriter David Byrne, commenting in 2011, \"Anything could be mixed and matched \u2013 or mashed up, as is said today \u2013 and anything was fair game for inspiration.\"\nAvant-garde academics labelled American singer Madonna a \"personification of the postmodern\" and created a sub-discipline of cultural studies known as Madonna studies. Her self-aware constructs of gender and identity, and classic film references in music videos for \"Material Girl\" (1984) and \"Express Yourself\" (1989), made her a favorite of cultural theorists, who saw her as \"enacting postmodernist models of subjectivity.\" Madonna was seen to embody fragmentation, pastiche, retrospection, anti-foundationalism, and de-differentiation; her \"subversion of the subversion of the subversion of the male gaze\" in the \"Material Girl\" video was analyzed.\nPerformance and theater.\nPostmodern theater emerged as a reaction against modernist theater. Most postmodern productions are centered on highlighting the fallibility of definite truth, instead encouraging the audience to reach their own individual understanding. Essentially, thus, postmodern theater raises questions rather than attempting to supply answers.\nSculpture.\nSculptor Claes Oldenberg, at the forefront of the pop art movement, declared in 1961: \"I am for an art that is political-erotical-mystical \u2026 I am for an art that embroils itself with everyday crap and still comes out on top.\" That year, he opened \"The Store\" in a dime store area of New York's Lower East Side, where he blurred the line between art and commerce by producing and selling brightly painted plaster replicas of hamburgers and cans of soda, dresses, underwear, and other everyday objects: \"Museum in b[ourgeois] concept equals store in mine\".\nIn philosophy.\nPoststructuralist precursors.\nIn the 1970s, a disparate group of French theorists \u2013 often grouped together as \"poststructuralists\" \u2013 developed a critique of modern philosophy with roots discernible in Friedrich Nietzsche and Martin Heidegger's critique of metaphysics. Although few themselves relied upon the term, they became known to many as postmodern theorists. Poststructuralism is sometimes treated as distinct from or a subcategory of postmodernism and sometimes is treated as having been subsumed by postmodernism. While their ideas exerted a great influence on debates about the postmodern, the French poststucturalists themselves did not intervene or attempt to provide their own definitions of the postmodern.\nPoststructuralists, like structuralists, start from the assumption that people's identities, values, and economic conditions determine each other as parts of a common whole, rather than having intrinsic properties that can be understood in isolation. While structuralism explores how meaning is produced by a set of essential relationships in an overarching quasi-linguistic system, poststructuralism accepts this premise, but rejects the assumption that such systems can ever be fixed or centered. Instead, poststructuralists stress the various ways that cultural structures are produced in history. They also emphasize how meaning is generated, rather than discovered, and they replace the traditional concept of \"representation\" (according to which meaning is determined by the objected signified) to focus instead upon the elastic potentialities of language to generate new meanings.\nPolitically, all of them began with Marxist sympathies, became disillusioned, and eventually opposed the French Communist Party and its application of theory. The chaos following the briefly successful communist revolution of May '68 in France was a particular point of rupture.\nJacques Derrida and deconstruction.\nDeconstruction is a practice in philosophy, literary criticism, and close reading developed by Jacques Derrida. It is based on the assumption, which it seeks to validate by textual analysis, that any text harbors inherent points of \"undecidability\" that undermine any stable meaning intended by the author. The process of writing inevitably, he aims to show, reveals suppressed elements, challenging the oppositions that are thought to sustain the text. Nevertheless, Derrida does not wish to do away with such concepts as \"origin\" or \"truth\". What he challenges is any claim to finality. Such metaphysical concepts are, as he puts it, \"under erasure\", and this, he says, makes deconstructive reading a kind of \"double play\".\nFrom this perspective, Derrida argues that the practice of metaphysics in the Western tradition depends upon hierarchies and orders of subordination within various dualisms that it does not acknowledge. It prioritizes presence and purity over the contingent and complicated, dismissing them as aberrations irrelevant to philosophical analysis. In essence, according to Derrida, metaphysical thought prioritizes one side of an opposition while ignoring or marginalizing the alternative. He uses the term metaphysics of presence to describe the foundationalist approach to knowledge, taking himself to have demonstrated that we do not have unmediated access to reality. This project of deconstructing and challenging the assumptions of modern philosophy was influential for many postmodern thinkers.\nMichel Foucault on power relations.\nFrench philosopher and social theorist Michel Foucault argued that power operates according to the logics of social institutions that have become unmoored from the intentions of any actual individuals. Individuals, according to Foucault, are both products and participants in these dynamics. Among other strategies, he employed a Nietzsche-inspired \"genealogical method\" to analyze power-relations across their historical permutations.\nBoth Foucault's political orientation and the consistency of his positions continue to be debated among critics and defenders alike. Nevertheless, Foucault's political works share two common elements: a historical perspective and a discursive methodology. He analyzed social phenomena in historical contexts and focused on how they have evolved over time. Additionally, he employed the study of written texts, usually academic texts, as the material for his inquiries. In this way, Foucault sought to understand how the historical formation of discourses has shaped contemporary political thinking and institutions.\nJean Baudrillard on hyperreality.\nAlthough trained in sociology, Jean Baudrillard worked across many disciplines. Drawing upon some of the technical vocabulary of the psychoanalyst Jacques Lacan, Baudrillard argued that social production had shifted from creating real objects to instead producing signs and symbols. This system of symbolic exchange, detached from the real, constitutes hyperreality. In the words of one commentator, \"the hyperreal is a system of simulation that simulates itself.\"\nPostmodernity, Baudrillard said, is the condition in which the domain of reality has become so heavily mediated by signs as to become inaccessible in itself, leaving us entirely in the domain of the simulacra, images that bear no relation to anything outside of themselves. This hyperreality is presented as the terminal stage of simulation, where signs and images become entirely self-referential.\nBaudrillard's vision of postmodernity has been described as \"apocalyptic\", and scholars disagree about whether his later works are intended as science fiction or truthful theoretical claims. Another interpretation is that Baudrillard deliberately adopts the role of \"agent provocateur\".\nA crisis of legitimacy.\nAt the center of the intellectual debate about postmodernism is the question of what, if anything, grounds theory. What establishes that a statement is true or that an action is right? This foundational debate is most prominently on display in Habermas's rejoinder to Lyotard's anti-foundational, postmodern challenge to Habermas's own foundational version of modernism.\n\"The Postmodern Condition\".\nJean-Fran\u00e7ois Lyotard is credited with being the first to use the term \"postmodern\" in a philosophical context. This appeared in his 1979 \"\". In this influential work, Lyotard provided the following definition: \"Simplifying to the extreme, I define postmodern as incredulity towards metanarratives\".\nBy \"metanarratives\", Lyotard meant such overarching narrative frameworks as those provided by Christianity, G. W. F. Hegel, and Karl Marx that unite and determine our basic sense of our place and significance in the world. It was his early disillusionment with his early Marxism that would later be generalized into the universal claim about metanarratives. In a society with no unifying narrative, he argued, we are left with heterogeneous, group-specific narratives (or \"language games\", as adopted from Ludwig Wittgenstein) with no universal perspective from which to adjudicate among them.\nAccording to Lyotard, this introduced a general crisis of legitimacy, a theme he adopts from the philosopher J\u00fcrgen Habermas, whose theory of communicative rationality Lyotard rejected. While he was particularly concerned in that report with the way that this insight undermined claims of scientific objectivity, Lyotard's argument undermines the entire principle of transcendent legitimization. Instead, proponents of a language game must make the case for their legitimacy with reference to such considerations as efficiency or practicality. Far from celebrating the apparently relativistic consequences of this argument, however, Lyotard focused much of his subsequent work on how links among games could be established, particularly with respect to ethics and politics.\nThe philosophical criticism of J\u00fcrgen Habermas.\nThe philosopher J\u00fcrgen Habermas, a prominent critic of philosophical postmodernism, argued in his 1985 work \"The Philosophical Discourse of Modernity\" that postmodern thinkers were caught in a performative contradiction, more specifically, that their critiques of modernity rely on concepts and methods that are themselves products of modern reason.\nHabermas criticized these thinkers for their rejection of the subject and their embrace of experimental, avant-garde strategies. He asserted that their critiques of modernism ultimately lead to a longing for the very subject they seek to dismantle. Habermas also took issue with postmodernists' leveling of the distinction between philosophy and literature. He argued that such rhetorical strategies undermine the importance of argument and communicative reason.\nHabermas's critique of postmodernism set the stage for much of the subsequent debate by clarifying some of its key underlying issues. According to scholar Gary Aylesworth \u2013 against those who would dismiss postmodernist discourse as simple nonsense \u2013 the fact that Habermas was \"able to read postmodernist texts closely and discursively testifies to their intelligibility\". His engagement with their ideas has led some postmodern philosophers, following Lyotard, to similarly engage with Habermas's criticisms.\nFrederic Jameson's Marxist rejoinder.\nThe appearance of linguistic relativism also inspired an extensive rebuttal by the Marxist critic Fredric Jameson. Building upon the theoretical foundations laid out by the Marxist economist Ernst Mandel and observations in the early work of the sociologist Jean Baudrillard, Jameson developed his own conception of the postmodern as \"the cultural logic of late capitalism\" in the form of an enormous cultural expansion into an economy of spectacle and style, rather than the production of goods. According to Jameson, because the postmodernism is result of political and historical circumstances that make up the social world, it is not something that can be simply embraced or condemned. Instead, it must be analyzed and understood so that we may confront the world as it is.\nJameson categorizes a variety of features of the postmodern. One is the elision of the distinction between high culture and mass culture. Also, because of our loss of a unified \"bourgeois ego\", subjectivity is less focused, and we experience what he terms a \"waning of the affect\", an emotional disengagement from the social world. This loss of significance leads to what he calls \"depthlessness\", a difficulty in getting beneath the surfaces of cultural objects to find any deeper significance than is offered directly to the subject. Reduced to a set of styles, history looses its political force. This phenomenon finds expression, for instance, in the shift from \"parody\", in which styles are mixed in the interest of making a point, to \"pastiche\", in which styles are mixed together without attention to their original contexts.\nRichard Rorty's neopragmatism.\nRichard Rorty was an American philosopher known for his linguistic form of neopragmatism. Initially attracted to analytic philosophy, Rorty later rejected its representationalism. His major influences, rather than the poststructuralists, include Charles Darwin, Hans Georg Gadamer, G. W. F. Hegel, and Martin Heidegger.\nRorty challenged the notion of a mind-independent, language-independent reality. He argued that language is a tool used to adapt to the environment and achieve desired ends. This naturalistic approach led him to abandon the traditional quest for a privileged mental power that allows direct access to things-in-themselves.\nInstead, Rorty advocated for a focus on imaginative alternatives to present beliefs rather than the pursuit of independently grounded truths. He believed that creative, secular humanism, free from authoritarian assertions about truth and goodness, is the key to a better future. Rorty saw his neopragmatism as a continuation of the Enlightenment project, aiming to demystify human life and replace traditional power relations with those based on tolerance and freedom.\nIn other fields.\nPostmodernism is more fully understood by observing its effects in such diverse fields as law, education, urban planning, religious studies, politics and many others. Its influence varies widely across disciplines, reflecting the extent to which postmodern theories and ideas have been integrated into actual practices.\nAnthropology.\nPostmodern theory in anthropology originated in the 1960s, alongside the literary postmodern movement. Reflexivity is central to postmodern anthropology, a continuous practice of critical self-awareness that attempts to address the subjectivity inherent in interpretation. Other key practices are an emphasis on including the perspectives of the people being studied; cultural relativism, which considers values and beliefs within their cultural context; skepticism towards the notion that science can produce objective and universally valid knowledge; and rejection of grand narratives or theories that attempt to explain other cultures.\nAnthropologists working in a postmodern vein seek to dissect, interpret, and write cultural critiques, analyzing of cultural texts and practices, rather than relying on empirical observation. The issue of subjectivity is a concern: as ethnographies are influenced by the perspective of the author, the question arises in the study of individual cultures as to whether the author's opinions should be considered scientific. Clifford Geertz, considered a founding member of postmodernist anthropology, holds that, \"anthropological writings are themselves interpretations, and second and third order ones to boot. (By definition, only a 'native' makes first order ones: it's \"his\" culture.)\" In the 21st century, some anthropologists use a form of standpoint theory, which prioritizes the perspectives of the subject over the perspective of the observer in cultural interpretation.\nFeminism.\nPostmodern feminism mixes postmodern theory and French feminism that rejects a universal female subject. The goal is to destabilize the patriarchal norms entrenched in society that have led to gender inequality. Essentialism, philosophy, and universal truths are opposed, in favor of embracing the differences that exist amongst women to demonstrate that not all women are the same. Applying universal truths to all women in a society minimizes individual experience; ideas displayed as the norm in society stem from masculine notions of how women should be portrayed.\nPostmodern feminism seeks to analyze notions that have led to gender inequality, and attempts to promote equality through critiquing logocentrism, supporting multiple discourses, deconstructing texts, and seeking to promote subjectivity. This approach is not readily accepted by all feminists\u2014some believe postmodern thought undermines the attacks that feminist theory attempts to create, while other feminists are in favor of the union.\nLaw.\nIn response to the perceived shortcomings of legal formalism and positivism, postmodern legal scholars developed several new approaches to address both formal and ethical issues in jurisprudence. In particular, they emphasize the inequalities introduced to the legal system by such matters as race, gender, and economic status.\nPsychology.\nIn 1992, the \"Los Angeles Times\" reported on \"a group of increasingly influential psychologists \u2013 postmodern psychologists seems to be the name that is sticking\", who had come to the conclusion that \"the American conception of an isolated, unified self\" does not exist. People are composed of many different selves, constructed for different situations. In this way, postmodernism challenges the modernist view of psychology as the science of the individual, in favor of seeing humans as a cultural/communal product, dominated by language rather than by an inner self. \nIn 2001, Kenneth Gergen, a pioneer in postmodern psychological theory,\u00a0identified \"emphasis on the individual mind, an objectively knowable world, and language as carrier of truth\" as the cornerstones of traditional modernist psychology. He noted criticism of these assumptions coming from \"every quarter of the humanities and the sciences\", and the emergence of a psychology in which \"colonialist universalism is replaced by a global conversation among equals\". He also considered the \"strong critical reservation\", including the realist argument that a socially constructed world cannot negate a clearly observable objective reality; the claim of incoherence, wherein postmodernism denies truth and objectivity while simultaneously making truth claims; and its moral relativism, which fails to take a principled ethical stand. Ultimately, he concluded that psychology's future is \"hanging in the balance\".\nIn 2021, psychologist Jan Smedslund discussed how psychology tried for decades to emulate the natural sciences and address unpredictable individual behavior. He described how the dominant methodology came to rely exclusively on statistical analysis of group-level data and average findings, whereby it \"lost contact with the psychological processes going on in individual persons.\" He advocated for abandoning the natural science approach that had \"led into a clearly discernible blind alley.\"\nIn 2024, American psychology professor Edwin Gantt wrote that psychology remains in a state of continual struggle \"to decide whether its true intellectual home is to be found among the humanities, especially philosophy and literature, or among the STEM disciplines.\" He finds psychology \"a key site where the intellectual tug-of-war between modernism and postmodernism plays itself out in academia.\"\nUrban planning.\nModernism sought to design and plan cities that followed the logic of the new model of industrial mass production; reverting to large-scale solutions, aesthetic standardization, and prefabricated design solutions. This approach was found to have eroded urban living by its failure to recognize differences and aim towards homogeneous landscapes. Jane Jacobs's 1961 book \"The Death and Life of Great American Cities,\" was a sustained critique of urban planning as it had developed within modernism, and played a major role in turning public opinion against modernist planners, notably Robert Moses.\nPostmodern urban planning involves theories that embrace and aim to create diversity, elevating uncertainty, flexibility, and change, and rejecting utopianism while embracing a utopian way of thinking and acting. The postmodernity of \"resistance\" seeks to deconstruct modernism, a critique of the origins without necessarily returning to them. \nTheology.\nThe postmodern theological movement interprets Christian theology in light of postmodern theory and various forms of post-Heideggerian thought, using approaches such as poststructuralism, phenomenology, and deconstruction to question fixed interpretations, explore the role of lived experience, and uncover hidden textual assumptions and contradictions. The movement emerged in the 1980s and 1990s when a handful of philosophers who took philosopher Martin Heidegger as a common point of departure began publishing books engaging with Christian theology.\nTheologian Kevin J. Vanhoozer combines and expands on other scholarly classifications to present seven types of postmodern theology: postliberal, postmetaphysical, deconstructive, reconstructive, feminist, Anglo-American postmodernity, and radical orthodoxy. He notes that the typology should be considered \"provisional and fallible [yet] not entirely arbitrary\", having met two main criteria: each is an approach taken by more than one theologian, and each \"believes itself to be responding to, rejecting, or passing through modernity, not inhabiting it.\"\nIn popular culture.\nFashion.\nOne manifestation of postmodernism in fashion explored alternatives to conventional concepts of elegance: Rei Kawakubo\u2019s Spring/Summer 1997 collection featured \"dresses asymmetrically padded with goose down, creating bumps in unexpected areas of the body\". Issey Miyake's 1985 dreadlocks hat \"offered an immediate, yet impermanent, 'multi-culti' fashion experience\". Vivienne Westwood took \"an extremely polyglot approach\", from early work with copies of 1950s clothes, to exploration of historic modes and cultural influences. In 1981, her first runway show, \"Pirate\", merged British history, 18th- and 19th-century dress, and African textile design, with a rap and ethnic music soundtrack.\nThe postmodern fashion sensibility appeared also through the subcultures of the 1960s and 1970s. Hippies, punks and other countercultural groups constructed their own nonconformist identities through choices in music, drugs, slang, and appearance. As these styles gained mainstream popularity, critics claim they lost their deeper meaning: \"the adoption of surface attributes offers the frisson of rebellion without a commitment to a subcultural lifestyle.\"\nGraphic design.\nEarly mention of postmodernism in graphic design appeared in the British magazine, \"Design\", during the late 1960s. The discussion took a pragmatic if not entirely comfortable view of graphic design as engaging with the economic necessities of a changing world. Graphic design had the role of \"active stylization of product surfaces (such as those of packaging and promotion)\", engaging without moralizing with consumer desires. Editor Corin Hughes-Stanton concluded, \"Post-Modernism' is an attitude that takes the form of a creative response to unfolding developments in the socio-economic sphere; it is a sign of active engagement rather than an academic retreat from its commercial and professional concerns.\"\nMarketing.\nPostmodernism in marketing focuses on customized experiences where broad market generalizations are no longer applied. According to academic Stephen Brown, \"Marketers know about consumers, consumers know about marketers, marketers know consumers know about marketers, and consumers know marketers know consumers know about marketers.\" Brown, writing in 1993, stated that the postmodern approach in many ways rejects attempts to impose order and work in silos. Instead marketers should work collectively with \"artistic\" attributes of intuition, creativity, spontaneity, speculation, emotion, and involvement.\nOngoing influence.\nSince the late 1990s, there has been a growing sentiment in popular culture and in academia that postmodernism \"has gone out of fashion\". Others argue that postmodernism is dead in the context of current cultural production.\nA 2020 study investigated the reported transition from postmodernism to post-postmodernism, those \"changing social conditions that lead the consumer to consume in a particular manner\". Song lyrics were selected from Madonna (postmodern), Taylor Swift (post-postmodern), and Lady Gaga as a transitional example. Five postmodern characteristics consistently found in marketing literature were compared to their post-postmodern counterparts: anti-foundationalism to rewriting; dedifferentiation to redifferentiation; fragmentation to reengagement; reversal of production and consumption to rebalancing of production and consumption; and hyperreality to alternative reality. Postmodernism, it finds, \"remains vibrant, re-inventive, and calls for its demise may be somewhat overblown.\" Swift's success \"suggests a significant shift from deconstructive to reconstructive positions regarding the self and its surroundings\", noting that her \"post-postmodern engagement, enthusiasm and sincerity\" appeared to be \"somewhat superficial, sociopathic, and couched in fabulation.\"\nThe connection between postmodernism, posthumanism, and cyborgism has led to a challenge to postmodernism, for which the terms \"Post-postmodernism\" and \"postpoststructuralism\" were first coined in 2003. A small group of critics has put forth a range of theories that aim to describe culture or society in the alleged aftermath of postmodernism, most notably Raoul Eshelman (performatism), Gilles Lipovetsky (hypermodernity), Nicolas Bourriaud (altermodern), and Alan Kirby (digimodernism, formerly called pseudo-modernism). None of these new theories or labels have so far gained very widespread acceptance.\nWriting in 2022, Steven Connor argues that, despite continuing reports of its death or imminent demise, postmodernism has instead undergone a kind of disappearance into our culture by way of assimilation. He notes there is little that can now be called postmodern style because \"the clashing or commingling of styles has become entirely routine at all levels of culture.\" The energizing antagonism between high and low culture has been \"pestled into a tepid porridge.\" And the general postmodern condition is now \"universal, irreversible and metastable, embodied above all in the massive increase in digitally mediated information technologies.\" According to Connor, postmodernism in the 2020s is a sensibility that has been integrated into everyday life, having been subject to a considerable degree of shifting, perhaps temporarily, from irony, pluralism and ambivalence to urgency, indignation, and reductive absolutism.\nSee also.\n&lt;templatestyles src=\"Col-float/styles.css\" /&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23604", "revid": "24013162", "url": "https://en.wikipedia.org/wiki?curid=23604", "title": "Photography", "text": "Art and practice of creating images by recording light\nPhotography is the art, application, and practice of creating images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication. A person who operates a camera to capture or take photographs is called a photographer, while the captured image, also known as a photograph, is the result produced by the camera.\nTypically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically \"developed\" into a visible image, either negative or positive, depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.\nBefore the emergence of digital photography, photographs that utilized film had to be developed to produce negatives or projectable slides, and negatives had to be printed as positive images, usually in enlarged form. This was typically done by photographic laboratories, but many amateur photographers, students, and photographic artists did their own processing.\nEtymology.\nThe word \"photography\" was created from the Greek roots ('), genitive of ('), \"light\" and (\"\") \"representation by means of lines\" or \"drawing\", together meaning \"drawing with light\".\nSeveral people may have coined the same new term from these roots independently. H\u00e9rcules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, \"\", in private notes which a Brazilian historian believes were written in 1834. This claim is widely reported but is not yet largely recognized internationally. The first use of the word by Florence became widely known after the research of Boris Kossoy in 1980.\nOn 25 February 1839, the German newspaper ' published an article titled ', discussing several priority claims, especially that of Henry Fox Talbot's, in relation to Daguerre's claim of invention. The article is the earliest known occurrence of the word in public print. It was signed \"J.M.\", believed to have been Berlin astronomer Johann von Maedler. The astronomer John Herschel is also credited with coining the word, independent of Talbot, in 1839.\nThe inventors Nic\u00e9phore Ni\u00e9pce, Talbot, and Louis Daguerre seem not to have known or used the word \"photography\", but referred to their processes as \"Heliography\" (Ni\u00e9pce), \"Photogenic Drawing\"/\"Talbotype\"/\"Calotype\" (Talbot), and \"Daguerreotype\" (Daguerre).\nHistory.\nPrecursor technologies.\nPhotography is the result of combining several technical discoveries relating to seeing an image and capturing the image. The discovery of the camera obscura (\"dark chamber\" in Latin) that provides an image of a scene dates back to ancient China. Greek mathematicians Aristotle and Euclid independently described a camera obscura in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments.\nThe Arab physicist Ibn al-Haytham (Alhazen) (965\u20131040) also invented a camera obscura as well as the first true pinhole camera. The invention of the camera has been traced back to the work of Ibn al-Haytham. While the effects of a single light passing through a pinhole had been described earlier, Ibn al-Haytham gave the first correct analysis of the camera obscura, including the first geometrical and quantitative descriptions of the phenomenon, and was the first to use a screen in a dark room so that an image from one side of a hole in the surface could be projected onto a screen on the other side. He also first understood the relationship between the focal point and the pinhole, and performed early experiments with afterimages, laying the foundations for the invention of photography in the 19th century.\nLeonardo da Vinci mentions natural camerae obscurae that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western art. It is a box with a small hole in one side, which allows specific light rays to enter, projecting an inverted image onto a viewing screen or paper.\nThe birth of photography was then concerned with inventing means to capture and keep the image produced by the camera obscura. Albertus Magnus (1193\u20131280) discovered silver nitrate, and Georg Fabricius (1516\u20131571) discovered silver chloride.\nDaniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. Around 1717, Johann Heinrich Schulze used a light-sensitive slurry to capture images of cut-out letters on a bottle and on that basis many German sources and some international ones credit Schulze as the inventor of photography.\nThe fiction book \"Giphantie\", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.\nIn June 1802, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that \"the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver.\" The shadow images eventually darkened all over.\nInvention.\nThe first permanent photoetching was an image produced in 1822 by the French inventor Nic\u00e9phore Ni\u00e9pce, but it was destroyed in a later attempt to make prints from it. Ni\u00e9pce was successful again in 1825. In 1826 he made the \"View from the Window at Le Gras\", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).\nBecause Ni\u00e9pce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.\nNi\u00e9pce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Ni\u00e9pce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements\u2014a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and \"fixed\" with hot saturated salt water\u2014were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait.\nIn Brazil, Hercules Florence had started working out a silver-salt-based paper process in 1832, later naming it \"photographia\", at least four years before John Herschel coined the English word \"photography\". In 1834, having settled on silver nitrate on paper, a combination which had been the subject of experiments by Thomas Wedgwood around the year 1800, Florence's notebooks indicate that he eventually succeeded in creating light-fast, durable images. Partly because he never published his invention adequately, partly because he was an obscure inventor living in a remote and undeveloped province, H\u00e9rcules Florence died, in Brazil, unrecognized internationally as one of the inventors of photography during his lifetime.\nMeanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method in a paper to the Royal Society and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.\nIn March 1837, Franz von Kobell, used silver chloride and a cardboard camera to make pictures in negative of the Frauenkirche and other buildings in Munich, then taking another picture of the negative to get a positive, the actual black and white reproduction of a view on the object. In 1839, Kobell, together with Carl August von Steinheil, reported on their experiments to the Bavarian Academy of Sciences. The pictures produced were round with a diameter of 4\u00a0cm, the method was later named the \"Steinheil method\".\nIn France, Hippolyte Bayard invented his own process for producing direct positive paper prints and claimed to have invented photography earlier than Daguerre or Talbot.\nBritish chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the \"blueprint\". He was the first to use the terms \"photography\", \"negative\" and \"positive\". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to \"fix\" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.\nIn the March 1851 issue of \"The Chemist\", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.\nMany advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908.\nGlass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 21st century.\nFilm.\nHurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.\nThe first flexible photographic roll film was marketed by George Eastman, founder of Kodak in 1885, but this original \"film\" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose known as nitrate film.\nAlthough cellulose acetate or \"safety film\" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16\u00a0mm and 8\u00a0mm home movies, nitrate film remained standard for theatrical 35\u00a0mm motion pictures until it was finally discontinued in 1951.\nFilms remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive \"look\" of film based photographs compared to digital images is likely due to a combination of factors, including (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&amp;D curve) with film vs. linear response curve for digital CCD sensors), (2) resolution, and (3) continuity of tone.\nBlack-and-white.\nOriginally, all photography was monochrome, or \"black-and-white\". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost, chemical stability, and its \"classic\" photographic look. The tones and contrast between light and dark areas define black-and-white photography. Monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process, publicly revealed in 1847, produces brownish tones.\nMany photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.\nColor.\nColor photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not \"fix\" the photograph to prevent the color from quickly fading when exposed to white light.\nThe first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855. The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.\nRussian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color \"fringes\" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.\nImplementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.\nAutochrome, the first commercially successful color process, was introduced by the Lumi\u00e8re brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.\nKodachrome, the first modern \"integral tripack\" (or \"monopack\") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.\nAgfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.\nInstant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.\nColor photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995\u20132005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive \"look\".\nDigital.\nIn 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital.\nThe first digital camera to both record and save images in a digital format was the Fujix DS-1P created by Fujifilm in 1988.\nIn 1991, Kodak unveiled the DCS 100, the first commercially available digital single-lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.\nDigital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.\nDigital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.\nTechniques.\nA large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; dual photography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques.\nCameras.\nThe camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.\nPhotographers control the camera and lens to \"expose\" the light recording material to the required amount of light to form a \"latent image\" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal\u2013oxide\u2013semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on paper.\nThe camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).\nAs soon as photographic materials became \"fast\" (sensitive) enough for taking candid or surreptitious pictures, small \"detective\" cameras were made, some actually disguised as a book or handbag or pocket watch (the \"Ticka\" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.\nThe movie camera is a type of photographic camera that takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a \"frame\". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the \"frame rate\" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures to create the illusion of motion.\nStereoscopic.\nPhotographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as \"3-D\" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras).\nDualphotography.\nDualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.\nFull-spectrum, ultraviolet and infrared.\nUltraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.\nModified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350\u00a0nm to 1000\u00a0nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400\u00a0nm to 700\u00a0nm.\nReplacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).\nUses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.\nLayering.\nLayering is a photographic composition technique that manipulates the foreground, subject or middle-ground, and background layers in a way that they all work together to tell a story through the image. Layers may be incorporated by altering the focal length, distorting the perspective by positioning the camera in a certain spot. People, movement, light and a variety of objects can be used in layering.\nLight field.\nDigital methods of image capture and display processing have enabled the new technology of \"light field photography\" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected \"after\" the photograph has been captured. As explained by Michael Faraday in 1846, the \"light field\" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.\nThese additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.\nOther.\nBesides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.\nTypes.\nAmateur.\nAmateur photographers take photos for personal use, as a hobby or out of casual interest, rather than as a business or job. The quality of amateur work can be comparable to that of many professionals. Amateurs can fill a gap in subjects or topics that might not otherwise be photographed if they are not commercially useful or salable. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera. Twenty-first century social media and near-ubiquitous camera phones have made photographic and video recording pervasive in everyday life. In the mid-2010s smartphone cameras added numerous automatic assistance features like color management, autofocus face detection and image stabilization that significantly decreased skill and effort needed to take high quality images.\nCommercial.\nCommercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. The commercial photographic world could include:\nArt.\nDuring the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.\nAt first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.\nThe aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images \"written with light\"; Nic\u00e9phore Ni\u00e9pce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.\nClive Bell in his classic essay \"Art\" states that only \"significant form\" can distinguish art from what is not art.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There must be some one quality without which a work of art cannot exist; possessing which, in the least degree, no work is altogether worthless. What is this quality? What quality is shared by all objects that provoke our aesthetic emotions? What quality is common to Sta. Sophia and the windows at Chartres, Mexican sculpture, a Persian bowl, Chinese carpets, Giotto's frescoes at Padua, and the masterpieces of Poussin, Piero della Francesca, and Cezanne? Only one answer seems possible\u00a0\u2013 significant form. In each, lines and colors combined in a particular way, certain forms and relations of forms, stir our aesthetic emotions.\nOn 7 February 2007, Sotheby's London sold the 2001 photograph \"99 Cent II Diptychon\" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.\nConceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.\nIn parallel to this development, the then largely separate interface between painting and photography was closed in the second half of the 20th century with the chemigram of Pierre Cordier and the chemogram of Josef H. Neumann. In 1974 the chemograms by Josef H. Neumann concluded the separation of the painterly background and the photographic layer by showing the picture elements in a symbiosis that had never existed before, as an unmistakable unique specimen, in a simultaneous painterly and at the same time real photographic perspective, using lenses, within a photographic layer, united in colors and shapes. This Neumann chemogram from the 1970s thus differs from the beginning of the previously created cameraless chemigrams of a Pierre Cordier and the photogram Man Ray or L\u00e1szl\u00f3 Moholy-Nagy of the previous decades. These works of art were almost simultaneous with the invention of photography by various important artists who characterized Hippolyte Bayard, Thomas Wedgwood, William Henry Fox Talbot in their early stages, and later Man Ray and L\u00e1szl\u00f3 Moholy-Nagy in the twenties and by the painter in the thirties Edmund Kesting and Christian Schad by draping objects directly onto appropriately sensitized photo paper and using a light source without a camera.\nPhotojournalism.\nPhotojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining, including sports photography.\nScience and forensics.\nThe camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are usually taken from three vantage points: overview, mid-range, and close-up.\nIn 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24- hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century. Charles Brooke a little later developed similar instruments for the Greenwich Observatory.\nScience regularly uses image technology that has derived from the design of the pinhole camera to avoid distortions that can be caused by lenses. X-ray machines are similar in design to pinhole cameras, with high-grade filters and laser radiation.\nPhotography has become universal in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.\nThe first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an \"Ion\" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.\nWildlife photography.\nWildlife photography involves capturing images of various forms of wildlife. Unlike other forms of photography such as product or food photography, successful wildlife photography requires a photographer to choose the right place and right time when specific wildlife are present and active. It often requires great patience and considerable skill and command of the right photographic equipment.\nSocial and cultural implications.\nThere are many ongoing questions about different aspects of photography. In her \"On Photography\" (1977), Susan Sontag dismisses the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, \"To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power.\" Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation.\nModern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's \"Rear Window\" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.\nThe camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate \u2013 all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.\nDigital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make \"photomontages\", passing them as \"real\" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography.\nPhotography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that \"to photograph is to turn people into objects that can be symbolically possessed\". Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.\nOne of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a \"tourist gaze\"\nin which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a \"reverse gaze\" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.\nLaw.\nPhotography is both restricted and protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK, the Counter-Terrorism Act (2008) has increased the power of the police to prevent people, even press photographers, from taking pictures in public places. In South Africa, any person may photograph any other person, without their permission, in public spaces and the only specific restriction placed on what may not be photographed by government is related to anything classed as national security. Each country has different laws.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23607", "revid": "7852030", "url": "https://en.wikipedia.org/wiki?curid=23607", "title": "Pentateuch (disambiguation)", "text": "The Pentateuch is the first part of the Bible, consisting of Genesis, Exodus, Leviticus, Numbers, and Deuteronomy. It is also known as the Torah.\nPentateuch may also refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "23608", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=23608", "title": "POP", "text": ""}
{"id": "23609", "revid": "14383484", "url": "https://en.wikipedia.org/wiki?curid=23609", "title": "Phases", "text": ""}
{"id": "23610", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=23610", "title": "Postmodernism/Philosophy", "text": ""}
{"id": "23611", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=23611", "title": "Postmodernism/Music", "text": ""}
{"id": "23612", "revid": "32191395", "url": "https://en.wikipedia.org/wiki?curid=23612", "title": "Postmodern philosophy", "text": "Philosophical movement\nPostmodern philosophy is a philosophical movement that arose in the second half of the 20th century as a critical response to assumptions allegedly present in modernist philosophical ideas regarding culture, identity, history, or language that were developed during the 18th-century Age of Enlightenment. Postmodernist thinkers developed concepts like \"diff\u00e9rance\", repetition, trace, and hyperreality to subvert \"grand narratives\", univocity of being, and epistemic certainty. Postmodern philosophy questions the importance of power relationships, personalization, and discourse in the \"construction\" of truth and world views. Many postmodernists appear to deny that an objective reality exists, and appear to deny that there are objective moral values.\nJean-Fran\u00e7ois Lyotard defined philosophical postmodernism in \"The Postmodern Condition\", writing \"Simplifying to the extreme, I define postmodern as incredulity towards meta narratives...\" where what he means by metanarrative is something like a unified, complete, universal, and epistemically certain story about everything that is. Postmodernists reject metanarratives because they reject the conceptualization of truth that metanarratives presuppose. Postmodernist philosophers in general argue that truth is always contingent on historical and social context rather than being absolute and universal and that truth is always partial and \"at issue\" rather than being complete and certain.\nPostmodern philosophy is often particularly skeptical about simple binary oppositions characteristic of structuralism, emphasizing the problem of the philosopher cleanly distinguishing knowledge from ignorance, social progress from reversion, dominance from submission, good from bad, and presence from absence.\nSubjects.\nOn literature.\nPostmodern philosophy has had strong relations with the substantial literature of critical theory, although some critical theorists such as J\u00fcrgen Habermas have opposed postmodern philosophy.\nOn the Enlightenment.\nMany postmodern claims are critical of certain eighteenth-century Enlightenment values. Postmodern writings often focus on deconstructing the role that power and ideology play in shaping discourse and belief. Postmodern philosophy shares ontological similarities with classical skeptical and relativistic belief systems.\nOn truth and objectivity.\nAccording to David Novitz, philosophical postmodernism challenges the notion that truth can be achieved. The \"Routledge Encyclopedia of Philosophy\" states that \"The assumption that there is no common denominator in 'nature' or 'truth' ... that guarantees the possibility of neutral or objective thought\" is a key assumption of postmodernism. The Stanford Encyclopedia of Philosophy describes it as \"a set of critical, strategic and rhetorical practices employing concepts such as difference, repetition, the trace, the simulacrum, and hyperreality to destabilize other concepts such as presence, identity, historical progress, epistemic certainty, and the univocity of meaning.\" The National Research Council has characterized the belief that \"social science research can never generate objective or trustworthy knowledge\" as an example of a postmodernist belief. Jean-Fran\u00e7ois Lyotard's seminal 1979 \"The Postmodern Condition\" stated that its hypotheses \"should not be accorded predictive value in relation to reality, but strategic value in relation to the questions raised\". Lyotard's statement in 1984 that \"I define postmodern as incredulity toward meta-narratives\" extends to incredulity toward science. Jacques Derrida, who is generally identified as a postmodernist, stated that \"every referent, all reality has the structure of a differential trace\". There are strong similarities with post-modernism in the work of Paul Feyerabend; Feyerabend held that modern science is no more justified than witchcraft, and has denounced the \"tyranny\" of \"abstract concepts such as 'truth', 'reality', or 'objectivity', which narrow people's vision and ways of being in the world\". Defenders of postmodernism state that many descriptions of postmodernism exaggerate its antipathy to science; for example, Feyerabend denied that he was \"anti-science\", accepted that some scientific \"theories\" are superior to other theories (even if science itself is not superior to other modes of inquiry), and attempted conventional medical treatments during his fight against cancer.\nInfluences.\nPostmodern philosophy was greatly influenced by the writings of S\u00f8ren Kierkegaard and Friedrich Nietzsche in the 19th century and other early-to-mid 20th-century philosophers, including the phenomenologist Martin Heidegger, the psychoanalyst Jacques Lacan, cultural critic Roland Barthes, theorist Georges Bataille, and the later work of Ludwig Wittgenstein.\nPostmodern philosophy also drew from the world of the arts and architecture, particularly Marcel Duchamp, John Cage, and artists who practiced collage.\nPostmodern philosophers.\nMichel Foucault.\nMichel Foucault is often cited as an early postmodernist although he personally rejected that label. Following Nietzsche, Foucault argued that knowledge is produced through the operations of \"power\", and changes fundamentally in different historical periods.\nJean Baudrillard.\nBaudrillard, known for his simulation theory, argued that the individual's experience and perception of reality derives its basis entirely from media-propagated ideals and images. The real and fantasy become indistinguishable, leading to the emergence of a wide-spread simulation of reality.\nJean Fran\u00e7ois Lyotard.\nThe writings of Lyotard were largely concerned with the role of narrative in human culture, and particularly how that role has changed as we have left modernity and entered a \"postindustrial\" or postmodern condition. He argued that modern philosophies legitimized their truth-claims not (as they themselves claimed) on logical or empirical grounds, but rather on the grounds of accepted stories (or \"metanarratives\") about knowledge and the world\u2014comparing these with Wittgenstein's concept of language-games. He further argued that in our postmodern condition, these metanarratives no longer work to legitimize truth-claims. He suggested that in the wake of the collapse of modern metanarratives, people are developing a new \"language-game\"\u2014one that does not make claims to absolute truth but rather celebrates a world of ever-changing relationships (among people and between people and the world).\nJacques Derrida.\nDerrida, the father of deconstruction, practiced philosophy as a form of textual criticism. He criticized Western philosophy as privileging the concept of presence and \"logos\", as opposed to absence and markings or writings.\nGilles Deleuze on productive difference.\nThe work of Gilles Deleuze developed a concept of difference as a productive mechanism, rather than as a merely negative phenomenon. He advocated for a critique of reason that emphasizes sense and affect over rational judgment. Following Nietzsche, Deleuze argued that philosophical critique is an encounter between thought and what forces it into action, and that this requires training, discipline, inventiveness, and even a certain \"cruelty\". He believed that thought cannot activate itself, but needs external forces to awaken and move it. Art, science, and philosophy can provide such activation through their transformative and experimental nature.\nRichard Rorty.\nIn the United States, a well-known pragmatist and self-proclaimed postmodernist was Richard Rorty. An analytic philosopher, Rorty believed that combining Willard Van Orman Quine's criticism of the analytic-synthetic distinction with Wilfrid Sellars's critique of the \"Myth of the Given\" allowed for an abandonment of the view of the thought or language as a mirror of a reality or an external world. Further, drawing upon Donald Davidson's criticism of the dualism between conceptual scheme and empirical content, he challenges the sense of questioning whether our particular concepts are related to the world in an appropriate way, whether we can justify our ways of describing the world as compared with other ways. He argued that truth was not about getting it right or representing reality, but was part of a social practice and language was what served our purposes in a particular time; ancient languages are sometimes untranslatable into modern ones because they possess a different vocabulary and are unuseful today. Donald Davidson is not usually considered a postmodernist, although he and Rorty have both acknowledged that there are few differences between their philosophies.\nDouglas Kellner.\nDouglas Kellner insists that the \"assumptions and procedures of modern theory\" must be forgotten. Kellner analyzes the terms of this theory in real-life experiences and examples. Kellner uses science and technology studies as a major part of his analysis; he urges that the theory is incomplete without it. The scale is larger than just postmodernism alone; it must be interpreted through cultural studies where science and technology studies play a large role. The reality of the 11 September attacks on the United States of America is the catalyst for his explanation. In response, Kellner continues to examine the repercussions of understanding the effects of the 11 September attacks. He questions if the attacks are only able to be understood in a limited form of postmodern theory due to the level of irony. The conclusion he depicts is simple: postmodernism, as most use it today, will decide what experiences and signs in one's reality will be one's reality as they know it.\nCriticism.\nSome criticism responds to postmodernist skepticism towards objective reality and claims that truth and morality are relative, including the argument that this relativism is self-contradictory. In part in reference to postmodernism, conservative English philosopher Roger Scruton wrote, \"A writer who says that there are no truths, or that all truth is 'merely relative,' is asking you not to believe him. So don't.\" In 2014, the philosophers Theodore Schick and Lewis Vaughn wrote: \"the statement that 'No unrestricted universal generalizations are true' is itself an unrestricted universal generalization. So if relativism in any of its forms is true, it's false.\" Some responses to postmodernist relativism argue that, contrary to its proponents' usual intentions, it does not necessarily benefit the political left. For example, the historian Richard J. Evans argued that if relativism rejects truth, it can legitimize far-right pseudohistory such as Holocaust denial.\nFurther lines of criticism are that postmodernist discourse is characterized by obscurantism, that the term itself is vaguely defined, and that postmodernism lacks a clear epistemology. The linguist and philosopher Noam Chomsky accused postmodernist intellectuals of failing to meaningfully answer questions such as \"what are the principles of their theories, on what evidence are they based, what do they explain that wasn't already obvious, etc.?\"\nThe French psychotherapist and philosopher, F\u00e9lix Guattari, rejected its theoretical assumptions by arguing that the structuralist and postmodernist visions of the world were not flexible enough to seek explanations in psychological, social, and environmental domains at the same time. In an interview with Truls Lie, Jean Baudrillard noted: \"[Transmodernism, etc.] are better terms than \"postmodernism\". It is not about modernity; it is about every system that has developed its mode of expression to the extent that it surpasses itself and its own logic. This is what I am trying to analyze.\" \"There is no longer any ontologically secret substance. I perceive this to be nihilism rather than postmodernism.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23613", "revid": "9755426", "url": "https://en.wikipedia.org/wiki?curid=23613", "title": "Postmodern music", "text": "Music of the postmodern era\nPostmodern music is music in the art music tradition produced in the postmodern era. It also describes any music that follows aesthetical and philosophical trends of postmodernism. As an aesthetic movement it was formed partly in reaction to modernism but is not primarily defined as oppositional to modernist music. Postmodernists question the tight definitions and categories of academic disciplines, which they regard simply as the remnants of modernity.\nThe postmodernist musical attitude.\nPostmodernism in music is not a distinct musical style, but rather refers to music of the postmodern era. Postmodernist music, on the other hand, shares characteristics with postmodernist art\u2014that is, art that comes after and reacts against modernism (see Modernism in Music). Rebecca Day, Lecturer in Music Analysis, writes \"within music criticism, postmodernism is seen to represent a conscious move away from the perceptibly damaging hegemony of binaries such as aestheticism/formalism, subject/object, unity/disunity, part/whole, that were seen to dominate former aesthetic discourse, and that when left unchallenged (as postmodernists claim of modernist discourse) are thought to de-humanise music analysis\".\nFredric Jameson, a major figure in the thinking on postmodernism and culture, calls postmodernism \"the cultural dominant of the logic of late capitalism\", meaning that, through globalization, postmodern culture is tied inextricably with capitalism (Mark Fisher, writing 20 years later, goes further, essentially calling it the sole cultural possibility). Drawing from Jameson and other theorists, David Beard and Kenneth Gloag argue that, in music, postmodernism is not just an attitude but also an inevitability in the current cultural climate of fragmentation. As early as 1938, Theodor Adorno had already identified a trend toward the dissolution of \"a culturally dominant set of values\", citing the commodification of all genres as beginning of the end of genre or value distinctions in music.\nIn some respects, Postmodern music could be categorized as simply the music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism, but with Jameson in mind, it is clear these definitions are inadequate. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of modernism, but in fact postmodern music is more to do with functionality and the effect of globalization than it is with a specific reaction, movement, or attitude. In the face of capitalism, Jameson says, \"It is safest to grasp the concept of the postmodern as an attempt to think the present historically in an age that has forgotten how to think historically in the first place\".\nCharacteristics.\nJonathan Kramer posits the idea (following Umberto Eco and Jean-Fran\u00e7ois Lyotard) that postmodernism (including \"musical\" postmodernism) is less a surface style or historical period (i.e., condition) than an \"attitude\". Each philosopher interprets the term Postmodernism differently. Lawrence Kramer uses it very loosely and explains it as susceptible to mere trends. He loosely follows Jean Francois Lyotard's views of synthesizing schemes, unity coherence, generality, totality, and structure losing their authority.\u00a0 These strategies of understanding are incorrigibly interdisciplinary and plural and rather see it made as a system over an ethos. Kramer enumerates 16 (arguably subjective) \"characteristics of postmodern music, by which I mean music that is understood in a postmodern manner, or that calls forth postmodern listening strategies, or that provides postmodern listening experiences, or that exhibits postmodern compositional practices.\" According to Kramer, postmodern music:\nDaniel Albright summarizes the main tendencies of musical postmodernism as:\nTimescale.\nOne author has suggested that the emergence of postmodern music in popular music occurred in the late 1960s, influenced in part by psychedelic rock and one or more of the later Beatles albums. Beard and Gloag support this position, citing Jameson's theory that \"the radical changes of musical styles and languages throughout the 1960s [are] now seen as a reflection of postmodernism\". Others have placed the beginnings of postmodernism in the arts, with particular reference to music, at around 1930.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "23615", "revid": "1300223463", "url": "https://en.wikipedia.org/wiki?curid=23615", "title": "Protocol", "text": "Protocol may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "23616", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=23616", "title": "Pumps", "text": ""}
{"id": "23617", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=23617", "title": "Pump", "text": "Device that imparts energy to the fluids by mechanical action\nA pump is a device that moves fluids (liquids or gases), or sometimes slurries, by mechanical action, typically converted from electrical energy into hydraulic or pneumatic energy.\nMechanical pumps serve in a wide range of applications such as pumping water from wells, aquarium filtering, pond filtering and aeration, in the car industry for water-cooling and fuel injection, in the energy industry for pumping oil and natural gas or for operating cooling towers and other components of heating, ventilation and air conditioning systems. In the medical industry, pumps are used for biochemical processes in developing and manufacturing medicine, and as artificial replacements for body parts, in particular the artificial heart and penile prosthesis.\nWhen a pump contains two or more pump mechanisms with fluid being directed to flow through them in series, it is called a \"multi-stage pump\". Terms such as \"two-stage\" or \"double-stage\" may be used to specifically describe the number of stages. A pump that does not fit this description is simply a \"single-stage pump\" in contrast.\nIn biology, many different types of chemical and biomechanical pumps have evolved; biomimicry is sometimes used in developing new types of mechanical pumps.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nTypes.\nMechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.\nPumps can be classified by their method of displacement into electromagnetic pumps, positive-displacement pumps, impulse pumps, velocity pumps, gravity pumps, steam pumps and valveless pumps. There are three basic types of pumps: positive-displacement, centrifugal and axial-flow pumps. In centrifugal pumps the direction of flow of the fluid changes by ninety degrees as it flows over an impeller, while in axial flow pumps the direction of flow is unchanged.\nCentrifugal pumps are produced by a wide range of manufacturers internationally, and their designs commonly follow global engineering standards such as ISO 2858, ISO 5199, and API 610. These standards define dimensional interchangeability, mechanical construction, performance tolerances, and material requirements suitable for industrial, agricultural, and municipal applications.\nManufacturers typically publish detailed technical data such as pump curves, efficiency ratings, head-flow characteristics, and impeller geometry to help engineers determine appropriate pump selection for specific operating conditions. These specifications are essential for ensuring correct pump sizing, energy efficiency, and long-term reliability in fluid-handling systems.\nPositive-displacement pumps.\nA positive-displacement pump makes a fluid move by trapping a fixed amount and forcing (displacing) that trapped volume into the discharge pipe.\nSome positive-displacement pumps use an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pump as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant through each cycle of operation.\nPositive-displacement pump behavior and safety.\nPositive-displacement pumps, unlike centrifugal, can theoretically produce the same flow at a given rotational speed no matter what the discharge pressure. Thus, positive-displacement pumps are \"constant flow machines\". However, a slight increase in internal leakage as the pressure increases prevents a truly constant flow rate.\nA positive-displacement pump must not operate against a closed valve on the discharge side of the pump, because it has no shutoff head like centrifugal pumps. A positive-displacement pump operating against a closed discharge valve continues to produce flow and the pressure in the discharge line increases until the line bursts, the pump is severely damaged, or both.\nA relief or safety valve on the discharge side of the positive-displacement pump is therefore necessary. The relief valve can be internal or external. The pump manufacturer normally has the option to supply internal relief or safety valves. The internal valve is usually used only as a safety precaution. An external relief valve in the discharge line, with a return line back to the suction line or supply tank, provides increased safety.\nPositive-displacement types.\nA positive-displacement pump can be further classified according to the mechanism used to move the fluid:\nRotary positive-displacement pumps.\nThese pumps move fluid using a rotating mechanism that creates a vacuum that captures and draws in the liquid.\n\"Advantages:\" Rotary pumps are very efficient because they can handle highly viscous fluids with higher flow rates as viscosity increases.\n\"Drawbacks:\" The nature of the pump requires very close clearances between the rotating pump and the outer edge, making it rotate at a slow, steady speed. If rotary pumps are operated at high speeds, the fluids cause erosion, which eventually causes enlarged clearances that liquid can pass through, which reduces efficiency.\nRotary positive-displacement pumps fall into five main types:\nReciprocating positive-displacement pumps.\nReciprocating pumps move the fluid using one or more oscillating pistons, plungers, or membranes (diaphragms), while valves restrict fluid motion to the desired direction. In order for suction to take place, the pump must first pull the plunger in an outward motion to decrease pressure in the chamber. Once the plunger pushes back, it will increase the chamber pressure and the inward pressure of the plunger will then open the discharge valve and release the fluid into the delivery pipe at constant flow rate and increased pressure.\nPumps in this category range from \"simplex\", with one cylinder, to in some cases \"quad\" (four) cylinders, or more. Many reciprocating-type pumps are \"duplex\" (two) or \"triplex\" (three) cylinder. They can be either \"single-acting\" with suction during one direction of piston motion and discharge on the other, or \"double-acting\" with suction and discharge in both directions. The pumps can be powered manually, by air or steam, or by a belt driven by an engine. This type of pump was used extensively in the 19th century\u2014in the early days of steam propulsion\u2014as boiler feed water pumps. Now reciprocating pumps typically pump highly viscous fluids like concrete and heavy oils, and serve in special applications that demand low flow rates against high resistance. Reciprocating hand pumps were widely used to pump water from wells. Common bicycle pumps and foot pumps for inflation use reciprocating action.\nThese positive-displacement pumps have an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pumps as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant given each cycle of operation and the pump's volumetric efficiency can be achieved through routine maintenance and inspection of its valves.\nTypical reciprocating pumps are:\nVarious positive-displacement pumps.\nThe positive-displacement principle applies in these pumps:\nGear pump.\nThis is the simplest form of rotary positive-displacement pumps. It consists of two meshed gears that rotate in a closely fitted casing. The tooth spaces trap fluid and force it around the outer periphery. The fluid does not travel back on the meshed part, because the teeth mesh closely in the center. Gear pumps see wide use in car engine oil pumps and in various hydraulic power packs.\nScrew pump.\nA screw pump is a more complicated type of rotary pump that uses two or three screws with opposing thread \u2014 e.g., one screw turns clockwise and the other counterclockwise. The screws are mounted on parallel shafts that often have gears that mesh so the shafts turn together and everything stays in place. In some cases the driven screw drives the secondary screw, without gears, often using the fluid to limit abrasion. The screws turn on the shafts and drive fluid through the pump. As with other forms of rotary pumps, the clearance between moving parts and the pump's casing is minimal.\nProgressing cavity pump.\nWidely used for pumping difficult materials, such as sewage sludge contaminated with large particles, a progressing cavity pump consists of a helical rotor, about ten times as long as its width, and a stator, mainly made out of rubber. This can be visualized as a central core of diameter \"x\" with, typically, a curved spiral wound around of thickness half \"x\", though in reality it is manufactured in a single lobe. This shaft fits inside a heavy-duty rubber sleeve or stator, of wall thickness also typically \"x\". As the shaft rotates inside the stator, the rotor gradually forces fluid up the rubber cavity. Such pumps can develop very high pressure at low volumes at a rate of 90 PSI per stage on water for standard configurations.\nRoots-type pump.\nNamed after the Roots brothers who invented it, this lobe pump displaces the fluid trapped between two long helical rotors, each fitted into the other when perpendicular at 90\u00b0, rotating inside a triangular shaped sealing line configuration, both at the point of suction and at the point of discharge. This design produces a continuous flow with equal volume and no vortex. It can work at low pulsation rates, and offers gentle performance that some applications require.\nApplications include:\nPeristaltic pump.\nA \"peristaltic pump\" is a type of positive-displacement pump. It contains fluid within a flexible tube fitted inside a circular pump casing (though linear peristaltic pumps have been made). A number of \"rollers\", \"shoes\", or \"wipers\" attached to a rotor compress the flexible tube. As the rotor turns, the part of the tube under compression closes (or \"occludes\"), forcing the fluid through the tube. Additionally, when the tube opens to its natural state after the passing of the cam it draws (\"restitution\") fluid into the pump. This process is called \"peristalsis\" and is used in many biological systems such as the gastrointestinal tract.\nPlunger pumps.\n\"Plunger pumps\" are reciprocating positive-displacement pumps.\nThese consist of a cylinder with a reciprocating plunger. The suction and discharge valves are mounted in the head of the cylinder. In the suction stroke, the plunger retracts and the suction valves open causing suction of fluid into the cylinder. In the forward stroke, the plunger pushes the liquid out of the discharge valve.\nEfficiency and common problems: With only one cylinder in plunger pumps, the fluid flow varies between maximum flow when the plunger moves through the middle positions, and zero flow when the plunger is at the end positions. A lot of energy is wasted when the fluid is accelerated in the piping system. Vibration and \"water hammer\" may be a serious problem. In general, the problems are compensated for by using two or more cylinders not working in phase with each other. Centrifugal pumps are also susceptible to water hammer., a specialized study, helps evaluate this risk in such systems.\nTriplex-style plunger pump.\nTriplex plunger pumps use three plungers, which reduces the pulsation relative to single reciprocating plunger pumps. Adding a pulsation dampener on the pump outlet can further smooth the \"pump ripple\", or ripple graph of a pump transducer. The dynamic relationship of the high-pressure fluid and plunger generally requires high-quality plunger seals. Plunger pumps with a larger number of plungers have the benefit of increased flow, or smoother flow without a pulsation damper. The increase in moving parts and crankshaft load is one drawback.\nCar washes often use these triplex-style plunger pumps (perhaps without pulsation dampers). In 1968, William Bruggeman reduced the size of the triplex pump and increased the lifespan so that car washes could use equipment with smaller footprints. Durable high-pressure seals, low-pressure seals and oil seals, hardened crankshafts, hardened connecting rods, thick ceramic plungers and heavier duty ball and roller bearings improve reliability in triplex pumps. Triplex pumps now are in a myriad of markets across the world.\nTriplex pumps with shorter lifetimes are commonplace to the home user. A person who uses a home pressure washer for 10 hours a year may be satisfied with a pump that lasts 100 hours between rebuilds. Industrial-grade or continuous duty triplex pumps on the other end of the quality spectrum may run for as much as 2,080 hours a year.\nThe oil and gas drilling industry uses massive semi-trailer-transported triplex pumps called mud pumps to pump drilling mud, which cools the drill bit and carries the cuttings back to the surface. Drillers use triplex or even quintuplex pumps to inject water and solvents deep into shale in the extraction process called \"fracking\".\nDiaphragm pump.\nTypically run on electricity compressed air, diaphragm pumps are relatively inexpensive and can perform a wide variety of duties, from pumping air into an aquarium, to liquids through a filter press. Double-diaphragm pumps can handle viscous fluids and abrasive materials with a gentle pumping process ideal for transporting shear-sensitive media.\nImpulse pump.\nImpulse pumps use pressure created by gas (usually air). In some impulse pumps the gas trapped in the liquid (usually water), is released and accumulated somewhere in the pump, creating a pressure that can push part of the liquid upwards.\nConventional impulse pumps include:\nInstead of a gas accumulation and releasing cycle, the pressure can be created by burning of hydrocarbons. Such combustion driven pumps directly transmit the impulse from a combustion event through the actuation membrane to the pump fluid. In order to allow this direct transmission, the pump needs to be almost entirely made of an elastomer (e.g. silicone rubber). Hence, the combustion causes the membrane to expand and thereby pumps the fluid out of the adjacent pumping chamber. The first combustion-driven soft pump was developed by ETH Zurich.\nHydraulic ram pump.\nA hydraulic ram is a water pump powered by hydropower.\nIt takes in water at relatively low pressure and high flow-rate and outputs water at a higher hydraulic-head and lower flow-rate. The device uses the water hammer effect to develop pressure that lifts a portion of the input water that powers the pump to a point higher than where the water started.\nThe hydraulic ram is sometimes used in remote areas, where there is both a source of low-head hydropower, and a need for pumping water to a destination higher in elevation than the source. In this situation, the ram is often useful, since it requires no outside source of power other than the kinetic energy of flowing water.\nVelocity pumps.\nRotodynamic pumps (or dynamic pumps) are a type of velocity pump in which kinetic energy is added to the fluid by increasing the flow velocity. This increase in energy is converted to a gain in potential energy (pressure) when the velocity is reduced prior to or as the flow exits the pump into the discharge pipe. This conversion of kinetic energy to pressure is explained by the \"First law of thermodynamics\", or more specifically by \"Bernoulli's principle\".\nDynamic pumps can be further subdivided according to the means in which the velocity gain is achieved.\nThese types of pumps have a number of characteristics:\nA practical difference between dynamic and positive-displacement pumps is how they operate under closed valve conditions. Positive-displacement pumps physically displace fluid, so closing a valve downstream of a positive-displacement pump produces a continual pressure build up that can cause mechanical failure of pipeline or pump. Dynamic pumps differ in that they can be safely operated under closed valve conditions (for short periods of time).\nRadial-flow pump.\nSuch a pump is also referred to as a \"centrifugal pump\". The fluid enters along the axis or center, is accelerated by the impeller and exits at right angles to the shaft (radially); an example is the centrifugal\u00a0fan, which is commonly used to implement a vacuum cleaner. Another type of radial-flow pump is a vortex pump. The liquid in them moves in tangential direction around the working wheel. The conversion from the mechanical energy of motor into the potential energy of flow comes by means of multiple whirls, which are excited by the impeller in the working channel of the pump. Generally, a radial-flow pump operates at higher pressures and lower flow rates than an axial- or a mixed-flow pump.\nAxial-flow pump.\nThese are also referred to as \"all-fluid pumps\". The fluid is pushed outward or inward to move fluid axially. They operate at much lower pressures and higher flow rates than radial-flow (centrifugal) pumps. Axial-flow pumps cannot be run up to speed without special precaution. If at a low flow rate, the total head rise and high torque associated with this pipe would mean that the starting torque would have to become a function of acceleration for the whole mass of liquid in the pipe system.\nMixed-flow pumps function as a compromise between radial and axial-flow pumps. The fluid experiences both radial acceleration and lift and exits the impeller somewhere between 0 and 90 degrees from the axial direction. As a consequence mixed-flow pumps operate at higher pressures than axial-flow pumps while delivering higher discharges than radial-flow pumps. The exit angle of the flow dictates the pressure head-discharge characteristic in relation to radial and mixed-flow.\nRegenerative turbine pump.\nAlso known as drag, friction, liquid-ring pump, peripheral, traction, turbulence, or vortex pumps, regenerative turbine pumps are a class of rotodynamic pump that operates at high head pressures, typically .\nThe pump has an impeller with a number of vanes or paddles which spins in a cavity. The suction port and pressure ports are located at the perimeter of the cavity and are isolated by a barrier called a stripper, which allows only the tip channel (fluid between the blades) to recirculate, and forces any fluid in the side channel (fluid in the cavity outside of the blades) through the pressure port. In a regenerative turbine pump, as fluid spirals repeatedly from a vane into the side channel and back to the next vane, kinetic energy is imparted to the periphery, thus pressure builds with each spiral, in a manner similar to a regenerative blower.\nAs regenerative turbine pumps cannot become vapor locked, they are commonly applied to volatile, hot, or cryogenic fluid transport. However, as tolerances are typically tight, they are vulnerable to solids or particles causing jamming or rapid wear. Efficiency is typically low, and pressure and power consumption typically decrease with flow. Additionally, pumping direction can be reversed by reversing direction of spin.\nSide-channel pump.\nA side-channel pump has a suction disk, an impeller, and a discharge disk.\nEductor-jet pump.\nThis uses a jet, often of steam, to create a low pressure. This low pressure sucks in fluid and propels it into a higher-pressure region.\nGravity pumps.\nGravity pumps include the \"syphon\" and \"Heron's fountain\". The \"hydraulic ram\" is also sometimes called a gravity pump. In a gravity pump the fluid is lifted by gravitational force.\nSteam pump.\nSteam pumps have been for a long time mainly of historical interest. They include any type of pump powered by a steam engine and also pistonless pumps such as Thomas Savery's or the Pulsometer steam pump.\nRecently there has been a resurgence of interest in low-power solar steam pumps for use in smallholder irrigation in developing countries. Previously small steam engines have not been viable because of escalating inefficiencies as vapour engines decrease in size. However the use of modern engineering materials coupled with alternative engine configurations has meant that these types of system are now a cost-effective opportunity.\nValveless pumps.\nValveless pumping assists in fluid transport in various biomedical and engineering systems. In a valveless pumping system, no valves (or physical occlusions) are present to regulate the flow direction. The fluid pumping efficiency of a valveless system, however, is not necessarily lower than that having valves. In fact, many fluid-dynamical systems in nature and engineering more or less rely upon valveless pumping to transport the working fluids therein. For instance, blood circulation in the cardiovascular system is maintained to some extent even when the heart's valves fail. Meanwhile, the embryonic vertebrate heart begins pumping blood long before the development of discernible chambers and valves. Similar to blood circulation in one direction, bird respiratory systems pump air in one direction in rigid lungs, but without any physiological valve. In microfluidics, valveless impedance pumps have been fabricated, and are expected to be particularly suitable for handling sensitive biofluids. Ink jet printers operating on the piezoelectric transducer principle also use valveless pumping. The pump chamber is emptied through the printing jet due to reduced flow impedance in that direction and refilled by capillary action.\nPump repairs.\nExamining pump repair records and mean time between failures (MTBF) is of great importance to responsible and conscientious pump users. In view of that fact, the preface to the 2006 Pump User's Handbook alludes to \"pump failure\" statistics. For the sake of convenience, these failure statistics often are translated into MTBF (in this case, installed life before failure).\nIn early 2005, Gordon Buck, John Crane Inc.'s chief engineer for field operations in Baton Rouge, Louisiana, examined the repair records for a number of refinery and chemical plants to obtain meaningful reliability data for centrifugal pumps. A total of 15 operating plants having nearly 15,000 pumps were included in the survey. The smallest of these plants had about 100 pumps; several plants had over 2000. All facilities were located in the United States. In addition, considered as \"new\", others as \"renewed\" and still others as \"established\". Many of these plants\u2014but not all\u2014had an alliance arrangement with John Crane. In some cases, the alliance contract included having a John Crane Inc. technician or engineer on-site to coordinate various aspects of the program.\nNot all plants are refineries, however, and different results occur elsewhere. In chemical plants, pumps have historically been \"throw-away\" items as chemical attack limits life. Things have improved in recent years, but the somewhat restricted space available in \"old\" DIN and ASME-standardized stuffing boxes places limits on the type of seal that fits. Unless the pump user upgrades the seal chamber, the pump only accommodates more compact and simple versions. Without this upgrading, lifetimes in chemical installations are generally around 50 to 60 percent of the refinery values.\nUnscheduled maintenance is often one of the most significant costs of ownership, and failures of mechanical seals and bearings are among the major causes. Keep in mind the potential value of selecting pumps that cost more initially, but last much longer between repairs. The MTBF of a better pump may be one to four years longer than that of its non-upgraded counterpart. Consider that published average values of avoided pump failures range from US$2600 to US$12,000. This does not include lost opportunity costs. One pump fire occurs per 1000 failures. Having fewer pump failures means having fewer destructive pump fires.\nAs has been noted, a typical pump failure, based on actual year 2002 reports, costs US$5,000 on average. This includes costs for material, parts, labor and overhead. Extending a pump's MTBF from 12 to 18 months would save US$1,667 per year \u2014 which might be greater than the cost to upgrade the centrifugal pump's reliability.\nApplications.\nPumps are used throughout society for a variety of purposes. Early applications includes the use of the windmill or watermill to pump water. Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.\nBecause of the wide variety of applications, pumps have a plethora of shapes and sizes: from very large to very small, from handling gas to handling liquid, from high pressure to low pressure, and from high volume to low volume.\nPriming a pump.\nTypically, a liquid pump cannot simply draw air. The feed line of the pump and the internal body surrounding the pumping mechanism must first be filled with the liquid that requires pumping: An operator must introduce liquid into the system to initiate the pumping, known as \"priming\" the pump. Loss of prime is usually due to ingestion of air into the pump, or evaporation of the working fluid if the pump is used infrequently. Clearances and displacement ratios in pumps for liquids are insufficient for pumping compressible gas, so air or other gasses in the pump can not be evacuated by the pump's action alone. This is the case with most velocity (rotodynamic) pumps \u2014 for example, centrifugal pumps. For such pumps, the position of the pump and intake tubing should be lower than the suction point so it is primed by gravity; otherwise the pump should be manually filled with liquid or a secondary pump should be used until all air is removed from the suction line and the pump casing. Liquid ring pumps have a dedicated intake for the priming liquid separate from the intake of the fluid being pumped, as the fluid being pumped may be a gas or mix of gas, liquid, and solids. For these pumps the priming liquid intake must be supplied continuously (either by gravity or pressure), however the intake for the fluid being pumped is capable of drawing a vacuum equivalent to the boiling point of the priming liquid.\nPositive\u2013displacement pumps, however, tend to have sufficiently tight sealing between the moving parts and the casing or housing of the pump that they can be described as \"self-priming\". Such pumps can also serve as \"priming pumps\", so-called when they are used to fulfill that need for other pumps in lieu of action taken by a human operator.\nPumps as public water supplies.\nOne sort of pump once common worldwide was a hand-powered water pump, or 'pitcher pump'. It was commonly installed over community water wells in the days before piped water supplies.\nIn parts of the British Isles, it was often called \"the parish pump\". Though such community pumps are no longer common, people still used the expression \"parish pump\" to describe a place or forum where matters of local interest are discussed.\nBecause water from pitcher pumps is drawn directly from the soil, it is more prone to contamination. If such water is not filtered and purified, consumption of it might lead to gastrointestinal or other water-borne diseases. A notorious case is the 1854 Broad Street cholera outbreak. At the time it was not known how cholera was transmitted, but physician John Snow suspected contaminated water and had the handle of the public pump he suspected removed; the outbreak then subsided.\nModern hand-operated community pumps are considered the most sustainable low-cost option for safe water supply in resource-poor settings, often in rural areas in developing countries. A hand pump opens access to deeper groundwater that is often not polluted and also improves the safety of a well by protecting the water source from contaminated buckets. Pumps such as the Afridev pump are designed to be cheap to build and install, and easy to maintain with simple parts. However, scarcity of spare parts for these types of pumps in some regions of Africa has diminished their utility for these areas.\nSealing multiphase pumping applications.\nMultiphase pumping applications, also referred to as tri-phase, have grown due to increased oil drilling activity. In addition, the economics of multiphase production is attractive to upstream operations as it leads to simpler, smaller in-field installations, reduced equipment costs and improved production rates. In essence, the multiphase pump can accommodate all fluid stream properties with one piece of equipment, which has a smaller footprint. Often, two smaller multiphase pumps are installed in series rather than having just one massive pump.\nTypes and features of multiphase pumps.\nHelico-axial (centrifugal).\nA rotodynamic pump with one single shaft that requires two mechanical seals, this pump uses an open-type axial impeller. It is often called a \"Poseidon pump\", and can be described as a cross between an axial compressor and a centrifugal pump.\nTwin-screw (positive-displacement).\nThe twin-screw pump is constructed of two inter-meshing screws that move the pumped fluid. Twin screw pumps are often used when pumping conditions contain high gas volume fractions and fluctuating inlet conditions. Four mechanical seals are required to seal the two shafts.\nProgressive cavity (positive-displacement).\nProgressive Cavity Pumps are well suited to pump sludge, slurries, viscous, and shear sensitive fluids. Progressive cavity pumps are single-screw types use in surface and downhole oil production. They serve a vast arrange of industries and applications ranging from Wastewater Treatment, Pulp and Paper, oil and gas, mining, and oil and gas.\nElectric submersible (centrifugal).\nThese pumps are basically multistage centrifugal pumps and are widely used in oil well applications as a method for artificial lift. These pumps are usually specified when the pumped fluid is mainly liquid.\n\"Buffer tank\"\nA buffer tank is often installed upstream of the pump suction nozzle in case of a slug flow. The buffer tank breaks the energy of the liquid slug, smooths any fluctuations in the incoming flow and acts as a sand trap.\nAs the name indicates, multiphase pumps and their mechanical seals can encounter a large variation in service conditions such as changing process fluid composition, temperature variations, high and low operating pressures and exposure to abrasive/erosive media. The challenge is selecting the appropriate mechanical seal arrangement and support system to ensure maximized seal life and its overall effectiveness.\nSpecifications.\nPumps are commonly rated by horsepower, volumetric flow rate, outlet pressure in metres (or feet) of head, inlet suction in suction feet (or metres) of head. The head can be simplified as the number of feet or metres the pump can raise or lower a column of water at atmospheric pressure.\nFrom an initial design point of view, engineers often use a quantity termed the specific speed to identify the most suitable pump type for a particular combination of flow rate and head.\nNet Positive Suction Head (NPSH) is crucial for pump performance. It has two key aspects:\nFor optimal pump operation, NPSHa must always exceed NPSHr. This ensures the pump has enough pressure to prevent cavitation, a damaging condition.\nPumping power.\nThe power imparted into a fluid increases the energy of the fluid per unit volume. Thus the power relationship is between the conversion of the mechanical energy of the pump mechanism and the fluid elements within the pump. In general, this is governed by a series of simultaneous differential equations, known as the Navier\u2013Stokes equations. However a more simple equation relating only the different energies in the fluid, known as Bernoulli's equation can be used. Hence the power, P, required by the pump:\n formula_1\nwhere \u0394p is the change in total pressure between the inlet and outlet (in Pa), and Q, the volume flow-rate of the fluid is given in m3/s. The total pressure may have gravitational, static pressure and kinetic energy components; i.e. energy is distributed between change in the fluid's gravitational potential energy (going up or down hill), change in velocity, or change in static pressure. \u03b7 is the pump efficiency, and may be given by the manufacturer's information, such as in the form of a pump curve, and is typically derived from either fluid dynamics simulation (i.e. solutions to the Navier\u2013Stokes for the particular pump geometry), or by testing. The efficiency of the pump depends upon the pump's configuration and operating conditions (such as rotational speed, fluid density and viscosity etc.)\n formula_2\nFor a typical \"pumping\" configuration, the work is imparted on the fluid, and is thus positive. For the fluid imparting the work on the pump (i.e. a turbine), the work is negative. Power required to drive the pump is determined by dividing the output power by the pump efficiency. Furthermore, this definition encompasses pumps with no moving parts, such as a siphon.\nEfficiency.\nPump efficiency is defined as the ratio of the power imparted on the fluid by the pump in relation to the power supplied to drive the pump. Its value is not fixed for a given pump, efficiency is a function of the discharge and therefore also operating head. For centrifugal pumps, the efficiency tends to increase with flow rate up to a point midway through the operating range (peak efficiency or Best Efficiency Point (BEP) ) and then declines as flow rates rise further. Pump performance data such as this is usually supplied by the manufacturer before pump selection. Pump efficiencies tend to decline over time due to wear (e.g. increasing clearances as impellers reduce in size).\nWhen a system includes a centrifugal pump, an important design issue is matching the \"head loss-flow characteristic\" with the pump so that it operates at or close to the point of its maximum efficiency.\nPump efficiency is an important aspect and pumps should be regularly tested. Thermodynamic pump testing is one method.\nMinimum flow protection.\nMost large pumps have a minimum flow requirement below which the pump may be damaged by overheating, impeller wear, vibration, seal failure, drive shaft damage or poor performance. A minimum flow protection system ensures that the pump is not operated below the minimum flow rate. The system protects the pump even if it is shut-in or dead-headed, that is, if the discharge line is completely closed. The simplest minimum flow system is a pipe running from the pump discharge line back to the suction line. This line is fitted with an orifice plate sized to allow the pump minimum flow to pass. The arrangement ensures that the minimum flow is maintained, although it is wasteful as it recycles fluid even when the flow through the pump exceeds the minimum flow. A more sophisticated, but more costly, system (see diagram) comprises a flow measuring device (FE) in the pump discharge which provides a signal into a flow controller (FIC) which actuates a flow control valve (FCV) in the recycle line. If the measured flow exceeds the minimum flow then the FCV is closed. If the measured flow falls below the minimum flow the FCV opens to maintain the minimum flowrate. As the fluids are recycled the kinetic energy of the pump increases the temperature of the fluid. For many pumps this added heat energy is dissipated through the pipework. However, for large industrial pumps, such as oil pipeline pumps, a recycle cooler is provided in the recycle line to cool the fluids to the normal suction temperature. Alternatively the recycled fluids may be returned to upstream of the export cooler in an oil refinery, oil terminal, or offshore installation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23618", "revid": "38455", "url": "https://en.wikipedia.org/wiki?curid=23618", "title": "Progressive", "text": "Progressive may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "23619", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=23619", "title": "Pressure", "text": "Force distributed over an area\nPressure (symbol: \"p\" or \"P\") is the force applied perpendicular to the surface of an object per unit area over which that force is distributed. Gauge pressure (also spelled \"gage\" pressure) is the pressure relative to the ambient pressure.\nVarious units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre (N/m2); similarly, the pound-force per square inch (psi, symbol lbf/in2) is the traditional unit of pressure in the imperial and US customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the unit atmosphere (atm) is equal to this pressure, and the torr is defined as &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044760 of this. Manometric units such as the centimetre of water, millimetre of mercury, and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.\nDefinition.\n\"Pressure\" is the amount of force applied perpendicular to the surface of an object per unit area. The symbol for it is \"p\" or \"P\".\nThe IUPAC recommendation for pressure is a lower-case \"p\".\nHowever, upper-case \"P\" is widely used. The usage of \"P\" vs \"p\" depends upon the field in which one is working, on the nearby presence of other symbols for quantities such as power and momentum, and on writing style.\nFormula.\nMathematically:\nformula_1\nwhere:\nPressure is a scalar quantity. It relates the vector area element (a vector normal to the surface) with the normal force acting on it. The pressure is the scalar proportionality constant that relates these two normal vectors:\nformula_5\nThe minus sign comes from the convention that the force is considered towards the surface element, while the normal vector points outward. The equation has meaning in that, for any surface \"S\" in contact with the fluid, the total force exerted by the fluid on that surface is the surface integral over \"S\" of the right-hand side of the above equation.\nIt is incorrect (although rather usual) to say \"the pressure is directed in such or such direction\". The pressure, as a scalar, has no direction. The force given by the previous relationship to the quantity has a direction, but the pressure does not. If we change the orientation of the surface element, the direction of the normal force changes accordingly, but the pressure remains the same.\nPressure is distributed to solid boundaries or across arbitrary sections of fluid \"normal to\" these boundaries or sections at every point. It is a fundamental parameter in thermodynamics, and it is conjugate to volume. It is defined as a derivative of the internal energy of a system:\nformula_6\nwhere:\nUnits.\nThe SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m2, or kg\u00b7m\u22121\u00b7s\u22122). This name for the unit was added in 1971; before that, pressure in SI was expressed in newtons per square metre.\nOther units of pressure, such as pounds per square inch (lbf/in2) and bar, are also in common use. The CGS unit of pressure is the barye (Ba), equal to 1\u00a0dyn\u00b7cm\u22122, or 0.1\u00a0Pa. Pressure is sometimes expressed in grams-force or kilograms-force per square centimetre (\"g/cm2\" or \"kg/cm2\") and the like without properly identifying the force units. But using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as units of force is deprecated in SI. The technical atmosphere (symbol: at) is 1\u00a0kgf/cm2 (98.0665\u00a0kPa, or 14.223\u00a0psi).\nPressure is related to energy density and may be expressed in units such as joules per cubic metre (J/m3, which is equal to Pa).\nMathematically:\nformula_11\nSome meteorologists prefer the hectopascal (hPa) for atmospheric air pressure, which is equivalent to the older unit millibar (mbar). Similar pressures are given in kilopascals (kPa) in most other fields, except aviation where the hecto- prefix is commonly used. The inch of mercury is still used in the United States. Oceanographers usually measure underwater pressure in decibars (dbar) because pressure in the ocean increases by approximately one decibar per metre depth.\nThe standard atmosphere (atm) is an established constant. It is approximately equal to typical air pressure at Earth mean sea level and is defined as (IUPAC recommends the value , but prior to 1982 the value (= 1 atm) was usually used).\nBecause pressure is commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (e.g., centimetres of water, millimetres of mercury or inches of mercury). The most common choices are mercury (Hg) and water; water is nontoxic and readily available, while mercury's high density allows a shorter column (and so a smaller manometer) to be used to measure a given pressure. The pressure exerted by a column of liquid of height \"h\" and density \"\u03c1\" is given by the hydrostatic pressure equation \"p\" = \"\u03c1gh\", where \"g\" is the gravitational acceleration. Fluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely.\nWhen millimetres of mercury (or inches of mercury) are quoted today, these units are not based on a physical column of mercury; rather, they have been given precise definitions that can be expressed in terms of SI units. One millimetre of mercury is approximately equal to one torr. The water-based units still depend on the density of water, a measured, rather than defined, quantity. These \"manometric units\" are still encountered in many fields. Blood pressure is measured in millimetres (or centimetres) of mercury in most of the world, and lung pressures in centimetres of water are still common.\nUnderwater divers use the metre sea water (msw or MSW) and foot sea water (fsw or FSW) units of pressure, and these are the units for pressure gauges used to measure pressure exposure in diving chambers and personal decompression computers. A msw is defined as 0.1\u00a0bar (= 10,000\u00a0Pa), is not the same as a linear metre of depth. 33.066\u00a0fsw = 1\u00a0atm (1\u00a0atm = 101,325\u00a0Pa / 33.066 = 3,064.326 Pa). The pressure conversion from msw to fsw is different from the length conversion: 10\u00a0msw = 32.6336\u00a0fsw, while 10\u00a0m = 32.8083\u00a0ft.\nGauge pressure is often given in units with \"g\" appended, e.g. \"kPag\", \"barg\" or \"psig\", and units for measurements of absolute pressure are sometimes given a suffix of \"a\", to avoid confusion, for example \"kPaa\", \"psia\". However, the US National Institute of Standards and Technology recommends that, to avoid confusion, any modifiers be instead applied to the quantity being measured rather than the unit of measure. For example, \"\"p\"g = 100 psi\" rather than \"\"p\" = 100 psig\".\nDifferential pressure is expressed in units with \"d\" appended; this type of measurement is useful when considering sealing performance or whether a valve will open or close.\nPresently or formerly popular pressure units include the following:\nExamples.\nAs an example of varying pressures, a finger can be pressed against a wall without making any lasting impression; however, the same finger pushing a thumbtack can easily damage the wall. Although the force applied to the surface is the same, the thumbtack applies more pressure because the point concentrates that force into a smaller area. Pressure is transmitted to solid boundaries or across arbitrary sections of fluid \"normal to\" these boundaries or sections at every point. Unlike stress, pressure is defined as a scalar quantity. The negative gradient of pressure is called the force density.\nAnother example is a knife. If the flat edge is used, force is distributed over a larger surface area resulting in less pressure, and it will not cut. Whereas using the sharp edge, which has less surface area, results in greater pressure, and so the knife cuts smoothly. This is one example of a practical application of pressure.\nFor gases, pressure is sometimes measured not as an \"absolute pressure\", but relative to atmospheric pressure; such measurements are called \"gauge pressure\". An example of this is the air pressure in an automobile tire, which might be said to be \"\", but is actually 220\u00a0kPa (32\u00a0psi) above atmospheric pressure. Since atmospheric pressure at sea level is about 100\u00a0kPa (14.7\u00a0psi), the absolute pressure in the tire is therefore about . In technical work, this is written \"a gauge pressure of \".\nWhere space is limited, such as on pressure gauges, name plates, graph labels, and table headings, the use of a modifier in parentheses, such as \"kPa (gauge)\" or \"kPa (absolute)\", is permitted. In non-SI technical work, a gauge pressure of is sometimes written as \"32\u00a0psig\", and an absolute pressure as \"32\u00a0psia\", though the other methods explained above that avoid attaching characters to the unit of pressure are preferred.\nGauge pressure is the relevant measure of pressure wherever one is interested in the stress on storage vessels and the plumbing components of fluidics systems. However, whenever equation-of-state properties, such as densities or changes in densities, must be calculated, pressures must be expressed in terms of their absolute values. For instance, if the atmospheric pressure is , a gas (such as helium) at (gauge) ( [absolute]) is 50% denser than the same gas at (gauge) ( [absolute]). Focusing on gauge values, one might erroneously conclude the first sample had twice the density of the second one.\nScalar nature.\nIn a static gas, the gas as a whole does not appear to move. The individual molecules of the gas, however, are in constant random motion. Because there are an extremely large number of molecules and because the motion of the individual molecules is random in every direction, no motion is detected. When the gas is at least partially confined (that is, not free to expand rapidly), the gas will exhibit a hydrostatic pressure. This confinement can be achieved with either a physical container, or in the gravitational well of a large mass, such as a planet, otherwise known as atmospheric pressure.\nIn the case of planetary atmospheres, the pressure-gradient force of the gas pushing outwards from higher pressure, lower altitudes to lower pressure, higher altitudes is balanced by the gravitational force, preventing the gas from diffusing into outer space and maintaining hydrostatic equilibrium.\nIn a physical container, the pressure of the gas originates from the molecules colliding with the walls of the container. The walls of the container can be anywhere inside the gas, and the force per unit area (the pressure) is the same. If the \"container\" is shrunk down to a very small point (becoming less true as the atomic scale is approached), the pressure will still have a single value at that point. Therefore, pressure is a scalar quantity, not a vector quantity. It has magnitude but no direction sense associated with it. Pressure force acts in all directions at a point inside a gas. At the surface of a gas, the pressure force acts perpendicular (at right angle) to the surface.\nA closely related quantity is the stress tensor \"\u03c3\", which relates the vector force formula_12 to the \nvector area formula_13 via the linear relation formula_14.\nThis tensor may be expressed as the sum of the viscous stress tensor minus the hydrostatic pressure. The negative of the stress tensor is sometimes called the pressure tensor, but in the following, the term \"pressure\" will refer only to the scalar pressure.\nAccording to the theory of general relativity, pressure increases the strength of a gravitational field (see stress\u2013energy tensor) and so adds to the mass-energy cause of gravity. This effect is unnoticeable at everyday pressures but is significant in neutron stars, although it has not been experimentally tested.\nTypes.\nFluid pressure.\n\"Fluid pressure\" is most often the compressive stress at some point within a fluid. (The term \"fluid\" refers to both liquids and gases \u2013 see below for more information specifically about liquid pressure or gas pressure.)\nFluid pressure occurs in one of two situations:\nPressure in open conditions usually can be approximated as the pressure in \"static\" or non-moving conditions (even in the ocean where there are waves and currents), because the motions create only negligible changes in the pressure. Such conditions conform with principles of fluid statics. The pressure at any given point of a non-moving (static) fluid is called the \"hydrostatic pressure\".\nClosed bodies of fluid are either \"static\", when the fluid is not moving, or \"dynamic\", when the fluid can move as in either a pipe or by compressing an air gap in a closed container. The pressure in closed conditions conforms with the principles of fluid dynamics.\nThe concepts of fluid pressure are predominantly attributed to the discoveries of Blaise Pascal and Daniel Bernoulli. Bernoulli's equation can be used in almost any situation to determine the pressure at any point in a fluid. The equation makes some assumptions about the fluid, such as the fluid being ideal and incompressible. An ideal fluid is a fluid in which there is no friction, it is inviscid (zero viscosity). The equation for all points of a system filled with a constant-density fluid is\nformula_15\nwhere:\nExplosion or deflagration pressures.\nExplosion or deflagration pressures are the result of the ignition of explosive gases, mists, dust/air suspensions, in unconfined and confined spaces.\nNegative pressures.\nWhile \"pressures\" are, in general, positive, there are several situations in which negative pressures may be encountered:\nStagnation pressure.\nStagnation pressure is the pressure a fluid exerts when it is forced to stop moving. Consequently, although a fluid moving at higher speed will have a lower static pressure, it may have a higher stagnation pressure when forced to a standstill. Static pressure and stagnation pressure are related by:\nformula_19\nwhere \nThe pressure of a moving fluid can be measured using a Pitot tube, or one of its variations such as a Kiel probe or Cobra probe, connected to a manometer. Depending on where the inlet holes are located on the probe, it can measure static pressures or stagnation pressures.\nSurface pressure and surface tension.\nThere is a two-dimensional analog of pressure \u2013 the lateral force per unit length applied on a line perpendicular to the force.\nSurface pressure is denoted by \u03c0:\nformula_24\nand shares many similar properties with three-dimensional pressure. Properties of surface chemicals can be investigated by measuring pressure/area isotherms, as the two-dimensional analog of Boyle's law, \"\u03c0A\" \n \"k\", at constant temperature.\nSurface tension is another example of surface pressure, but with a reversed sign, because \"tension\" is the opposite to \"pressure\".\nGas pressure.\nIn an ideal gas, molecules have no volume and do not interact. According to the ideal gas law, pressure varies linearly with temperature and quantity, and inversely with volume:\nformula_25\nwhere:\nReal gases exhibit a more complex dependence on the variables of state.\nVapour pressure.\nVapour pressure is the pressure of a vapour in thermodynamic equilibrium with its condensed phases in a closed system. All liquids and solids have a tendency to evaporate into a gaseous form, and all gases have a tendency to condense back to their liquid or solid form.\nThe atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapour bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher pressure, and therefore higher temperature, because the fluid pressure increases above the atmospheric pressure as the depth increases.\nThe vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial vapor pressure.\nLiquid pressure.\nWhen a person swims under the water, water pressure is felt acting on the person's eardrums. The deeper that person swims, the greater the pressure. The pressure felt is due to the weight of the water above the person. As someone swims deeper, there is more water above the person and therefore greater pressure. The pressure a liquid exerts depends on its depth.\nLiquid pressure also depends on the density of the liquid. If someone was submerged in a liquid more dense than water, the pressure would be correspondingly greater. Thus, we can say that the depth, density and liquid pressure are directly proportionate. The pressure due to a liquid in liquid columns of constant density and gravity at a depth within a substance is represented by the following formula:\nformula_26\nwhere:\nAnother way of saying the same formula is the following:\nformula_27\nThe pressure a liquid exerts against the sides and bottom of a container depends on the density and the depth of the liquid. If atmospheric pressure is neglected, liquid pressure against the bottom is twice as great at twice the depth; at three times the depth, the liquid pressure is threefold; etc. Or, if the liquid is two or three times as dense, the liquid pressure is correspondingly two or three times as great for any given depth. Liquids are practically incompressible \u2013 that is, their volume can hardly be changed by pressure (water volume decreases by only 50 millionths of its original volume for each atmospheric increase in pressure). Thus, except for small changes produced by temperature, the density of a particular liquid is practically the same at all depths.\nAtmospheric pressure pressing on the surface of a liquid must be taken into account when trying to discover the \"total\" pressure acting on a liquid. The total pressure of a liquid, then, is \"\u03c1gh\" plus the pressure of the atmosphere. When this distinction is important, the term \"total pressure\" is used. Otherwise, discussions of liquid pressure refer to pressure without regard to the normally ever-present atmospheric pressure.\nThe pressure does not depend on the \"amount\" of liquid present. Volume is not the important factor \u2013 depth is. The average water pressure acting against a dam depends on the average depth of the water and not on the volume of water held back. For example, a wide but shallow lake with a depth of exerts only half the average pressure that a small deep pond does. (The \"total force\" applied to the longer dam will be greater, due to the greater total surface area for the pressure to act upon. But for a given -wide section of each dam, the deep water will apply one quarter the force of deep water). A person will feel the same pressure whether their head is dunked a metre beneath the surface of the water in a small pool or to the same depth in the middle of a large lake.\nIf four interconnected vases contain different amounts of water but are all filled to equal depths, then a fish with its head dunked a few centimetres under the surface will be acted on by water pressure that is the same in any of the vases. If the fish swims a few centimetres deeper, the pressure on the fish will increase with depth and be the same no matter which vase the fish is in. If the fish swims to the bottom, the pressure will be greater, but it makes no difference which vase it is in. All vases are filled to equal depths, so the water pressure is the same at the bottom of each vase, regardless of its shape or volume. If water pressure at the bottom of a vase were greater than water pressure at the bottom of a neighboring vase, the greater pressure would force water sideways and then up the neighboring vase to a higher level until the pressures at the bottom were equalized. Pressure is depth dependent, not volume dependent, so there is a reason that water seeks its own level.\nRestating this as an energy equation, the energy per unit volume in an ideal, incompressible liquid is constant throughout its vessel. At the surface of a stationary liquid in a vessel gravitational potential energy is large but liquid pressure is low. At the bottom of the vessel, all the gravitational potential energy is converted to pressure. The two energy components change linearly with the depth so the sum of pressure and gravitational potential energy per unit volume is constant throughout the volume of the fluid. The units of pressure are equivalent to energy per unit volume. (In the SI system of units, the pascal is equivalent to the joule per cubic metre.) Mathematically, it is described by Bernoulli's equation, where velocity head is zero and comparisons per unit volume in the vessel are\nformula_28\nTerms have the same meaning as in section Fluid pressure.\nDirection of liquid pressure.\nAn experimentally determined fact about liquid pressure is that it is exerted equally in all directions. If someone is submerged in water, no matter which way that person tilts their head, the person will feel the same amount of water pressure on their ears. Because a liquid can flow, this pressure is not only downward. Pressure is seen acting sideways when water spurts sideways from a leak in the side of an upright can. Pressure also acts upward, as demonstrated when someone tries to push a beach ball beneath the surface of the water. The bottom of a ball is pushed upward by water pressure (buoyancy).\nWhen a liquid presses against a surface, there is a net force that is perpendicular to the surface. Although pressure does not have a specific direction, force does. A submerged triangular block has water forced against each point from many directions, but components of the force that are not perpendicular to the surface cancel each other out, leaving only a net perpendicular point. This is why liquid particles' velocity only alters in a normal component after they are collided to the container's wall. Likewise, if the collision site is a hole, water spurting from the hole in a bucket initially exits the bucket in a direction at right angles to the surface of the bucket in which the hole is located. Then it curves downward due to gravity. If there are three holes in a bucket (top, bottom, and middle), then the force vectors perpendicular to the inner container surface will increase with increasing depth \u2013 that is, a greater pressure at the bottom makes it so that the bottom hole will shoot water out the farthest. The force exerted by a fluid on a smooth surface is always at right angles to the surface. The speed of liquid out of the hole is formula_29, where \"h\" is the depth below the free surface. As predicted by Torricelli's law this is the same speed the water (or anything else) would have if freely falling the same vertical distance \"h\".\nKinematic pressure.\nformula_30\nis the kinematic pressure, where formula_2 is the pressure and formula_32 constant mass density. The SI unit of \"P\" is m2/s2. Kinematic pressure is used in the same manner as kinematic viscosity formula_33 in order to compute the Navier\u2013Stokes equation without explicitly showing the density formula_32.\nNavier\u2013Stokes equation with kinematic quantities.\nformula_35\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23621", "revid": "461300", "url": "https://en.wikipedia.org/wiki?curid=23621", "title": "Polygon", "text": "Plane figure bounded by line segments\nIn geometry, a polygon () is a plane figure made up of line segments connected to form a closed polygonal chain.\nThe segments of a closed polygonal chain are called its \"edges\" or \"sides\". The points where two edges meet are the polygon's \"vertices\" or \"corners\". An \"n\"-gon is a polygon with \"n\" sides; for example, a triangle is a 3-gon.\nA simple polygon is one which does not intersect itself. More precisely, the only allowed intersections among the line segments that make up the polygon are the shared endpoints of consecutive segments in the polygonal chain. A simple polygon is the boundary of a region of the plane that is called a \"solid polygon\". The interior of a solid polygon is its \"body\", also known as a polygonal region or polygonal area. In contexts where one is concerned only with simple and solid polygons, a \"polygon\" may refer only to a simple polygon or to a solid polygon. \nA polygonal chain may cross over itself, creating star polygons and other self-intersecting polygons. Some sources also consider closed polygonal chains in Euclidean space to be a type of polygon (a skew polygon), even when the chain does not lie in a single plane.\nA polygon is a 2-dimensional example of the more general polytope in any number of dimensions. There are many more generalizations of polygons defined for different purposes.\nEtymology.\nThe word \"polygon\" derives from the Greek adjective \u03c0\u03bf\u03bb\u03cd\u03c2 (\"pol\u00fas\") 'much', 'many' and \u03b3\u03c9\u03bd\u03af\u03b1 (\"g\u014dn\u00eda\") 'corner' or 'angle'. It has been suggested that \u03b3\u03cc\u03bd\u03c5 (\"g\u00f3nu\") 'knee' may be the origin of \"gon\".\nClassification.\nNumber of sides.\nPolygons are primarily classified by the number of sides.\nConvexity and intersection.\nPolygons may be characterized by their convexity or type of non-convexity:\nEquality and symmetry.\nThe property of regularity may be defined in other ways: a polygon is regular if and only if it is both isogonal and isotoxal, or equivalently it is both cyclic and equilateral. A non-convex regular polygon is called a \"regular star polygon\".\nProperties and formulas.\nEuclidean geometry is assumed throughout.\nAngles.\nAny polygon has as many corners as it has sides. Each corner has several angles. The two most important ones are:\nArea.\nIn this section, the vertices of the polygon under consideration are taken to be formula_6 in order. For convenience in some formulas, the notation (\"xn\", \"yn\") = (\"x\"0, \"y\"0) will also be used.\nSimple polygons.\nIf the polygon is non-self-intersecting (that is, simple), the signed area is\nformula_7\nor, using determinants\nformula_8\nwhere formula_9 is the squared distance between formula_10 and formula_11\nThe signed area depends on the ordering of the vertices and of the orientation of the plane. Commonly, the positive orientation is defined by the (counterclockwise) rotation that maps the positive x-axis to the positive y-axis. If the vertices are ordered counterclockwise (that is, according to positive orientation), the signed area is positive; otherwise, it is negative. In either case, the area formula is correct in absolute value. This is commonly called the \"shoelace formula\" or \"surveyor's formula\".\nThe area \"A\" of a simple polygon can also be computed if the lengths of the sides, \"a\"1, \"a\"2, ..., \"an\" and the exterior angles, \"\u03b8\"1, \"\u03b8\"2, ..., \"\u03b8n\" are known, from:\nformula_12\nThe formula was described by Lopshits in 1963.\nIf the polygon can be drawn on an equally spaced grid such that all its vertices are grid points, Pick's theorem gives a simple formula for the polygon's area based on the numbers of interior and boundary grid points: the former number plus one-half the latter number, minus 1.\nIn every polygon with perimeter \"p\" and area \"A \", the isoperimetric inequality formula_13 holds.\nFor any two simple polygons of equal area, the Bolyai\u2013Gerwien theorem asserts that the first can be cut into polygonal pieces which can be reassembled to form the second polygon.\nThe lengths of the sides of a polygon do not in general determine its area. However, if the polygon is simple and cyclic then the sides \"do\" determine the area. Of all \"n\"-gons with given side lengths, the one with the largest area is cyclic. Of all \"n\"-gons with a given perimeter, the one with the largest area is regular (and therefore cyclic).\nRegular polygons.\nMany specialized formulas apply to the areas of regular polygons.\nThe area of a regular polygon is given in terms of the radius \"r\" of its inscribed circle and its perimeter \"p\" by \nformula_14\nThis radius is also termed its apothem and is often represented as \"a\".\nThe area of a regular \"n\"-gon can be expressed in terms of the radius \"R\" of its circumscribed circle (the unique circle passing through all vertices of the regular \"n\"-gon) as follows:\nformula_15\nSelf-intersecting.\nThe area of a self-intersecting polygon can be defined in two different ways, giving different answers:\nCentroid.\nUsing the same convention for vertex coordinates as in the previous section, the coordinates of the centroid of a solid simple polygon are \nformula_16\nformula_17\nIn these formulas, the signed value of area formula_18 must be used.\nFor triangles (\"n\" = 3), the centroids of the vertices and of the solid shape are the same, but, in general, this is not true for \"n\" &gt; 3. The centroid of the vertex set of a polygon with n vertices has the coordinates\nformula_19\nformula_20\nGeneralizations.\nThe idea of a polygon has been generalized in various ways. Some of the more important include:\nNaming.\nThe word \"polygon\" comes from Late Latin \"polyg\u014dnum\" (a noun), from Greek \u03c0\u03bf\u03bb\u03cd\u03b3\u03c9\u03bd\u03bf\u03bd (\"polyg\u014dnon/polug\u014dnon\"), noun use of neuter of \u03c0\u03bf\u03bb\u03cd\u03b3\u03c9\u03bd\u03bf\u03c2 (\"polyg\u014dnos/polug\u014dnos\", the masculine adjective), meaning \"many-angled\". Individual polygons are named (and sometimes classified) according to the number of sides, combining a Greek-derived numerical prefix with the suffix \"-gon\", e.g. \"pentagon\", \"dodecagon\". The triangle, quadrilateral and nonagon are exceptions.\nBeyond decagons (10-sided) and dodecagons (12-sided), mathematicians generally use numerical notation, for example 17-gon and 257-gon.\nExceptions exist for side counts that are easily expressed in verbal form (e.g. 20 and 30), or are used by non-mathematicians. Some special polygons also have their own names; for example the regular star pentagon is also known as the pentagram.\nTo construct the name of a polygon with more than 20 and fewer than 100 edges, combine the prefixes as follows. The \"kai\" term applies to 13-gons and higher and was used by Kepler, and advocated by John H. Conway for clarity of concatenated prefix numbers in the naming of quasiregular polyhedra, though not all sources use it.\nHistory.\nPolygons have been known since ancient times. The regular polygons were known to the ancient Greeks, with the pentagram, a non-convex regular polygon (star polygon), appearing as early as the 7th century B.C. on a krater by Aristophanes, found at Caere and now in the Capitoline Museum.\nThe first known systematic study of non-convex polygons in general was made by Thomas Bradwardine in the 14th century.\nIn 1952, Geoffrey Colin Shephard generalized the idea of polygons to the complex plane, where each real dimension is accompanied by an imaginary one, to create complex polygons.\nIn nature.\nPolygons appear in rock formations, most commonly as the flat facets of crystals, where the angles between the sides depend on the type of mineral from which the crystal is made.\nRegular hexagons can occur when the cooling of lava forms areas of tightly packed columns of basalt, which may be seen at the Giant's Causeway in Northern Ireland, or at the Devil's Postpile in California.\nIn biology, the surface of the wax honeycomb made by bees is an array of hexagons, and the sides and base of each cell are also polygons.\nComputer graphics.\nIn computer graphics, a polygon is a primitive used in modelling and rendering. They are defined in a database, containing arrays of vertices (the coordinates of the geometrical vertices, as well as other attributes of the polygon, such as color, shading and texture), connectivity information, and materials.\nAny surface is modelled as a tessellation called polygon mesh. If a square mesh has \"n\" + 1 points (vertices) per side, there are \"n\" squared squares in the mesh, or 2\"n\" squared triangles since there are two triangles in a square. There are (\"n\" + 1)2 / 2(\"n\"2) vertices per triangle. Where \"n\" is large, this approaches one half. Or, each vertex inside the square mesh connects four edges (lines).\nThe imaging system calls up the structure of polygons needed for the scene to be created from the database. This is transferred to active memory and finally, to the display system (screen, TV monitors etc.) so that the scene can be viewed. During this process, the imaging system renders polygons in correct perspective ready for transmission of the processed data to the display system. Although polygons are two-dimensional, through the system computer they are placed in a visual scene in the correct three-dimensional orientation.\nIn computer graphics and computational geometry, it is often necessary to determine whether a given point formula_21 lies inside a simple polygon given by a sequence of line segments. This is called the point in polygon test.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23622", "revid": "861759", "url": "https://en.wikipedia.org/wiki?curid=23622", "title": "Player character", "text": "Character controlled by a game player\nA player character (also known as a playable character or PC) is a fictional character in a video game or tabletop role-playing game whose actions are controlled by a player rather than the rules of the game. The characters that are not controlled by a player are called non-player characters (NPCs). The actions of non-player characters are typically handled by the game itself in video games, or according to rules followed by a gamemaster refereeing tabletop role-playing games. The player character functions as a fictional, alternate body for the player controlling the character.\nVideo games typically have one player character for each person playing the game. Some games, such as multiplayer online battle arena, hero shooter, and fighting games, offer a group of player characters for the player to choose from, allowing the player to control one of them at a time. Where more than one player character is available, the characters may have distinctive abilities and differing styles of play.\nOverview.\nAvatars.\nA player character may sometimes be based on a real person, especially in sports games that use the names and likenesses of real athletes. Historical figures and leaders may sometimes appear as characters too, particularly in strategy or empire building games such as in Sid Meier's \"Civilization\" series. Such a player character is more properly an avatar as the player character's name and image typically have little bearing on the game itself. Avatars are also commonly seen in casino game simulations.\nBlank characters.\nIn many video games, and especially first-person shooters, the player character is a \"blank slate\" without any notable characteristics or even backstory. Pac-Man, Crono from \"Chrono Trigger\", Link from \"The Legend of Zelda\", Chell from \"Portal\", and Claude from \"Grand Theft Auto III\" are examples of such characters. These characters are generally silent protagonists.\nSome games will go even further, never showing or naming the player character at all. This is somewhat common in first-person videogames, such as in \"Myst\", but is more often done in strategy video games such as \"Dune 2000,\" \"\", and \"Command &amp; Conquer\" series. In such games, the only real indication that the player has a character (instead of an omnipresent status), is from the cutscenes during which the character is being given a mission briefing or debriefing; the player is usually addressed as \"general\", \"commander\", or another military rank.\nIn gaming culture, such a character was called Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person, abbreviated as AFGNCAAP; a term that originated in \"\" where it is used satirically to refer to the player.\nCharacter action games.\nCharacter action games (also called character-driven games, character games or just action games) are a broad category of action games, referring to a variety of games that are driven by the physical actions of player characters. The term dates back to the golden age of arcade video games in the early 1980s, when the terms \"action games\" and \"character games\" began being used to distinguish a new emerging genre of character-driven action games from the space shoot 'em ups that had previously dominated the arcades in the late 1970s. Classic examples of character action games from that period include maze games like \"Pac-Man\", platformers like \"Donkey Kong\", and \"Frogger\".\nSide-scrolling character action games (also called \"side-scrolling action games\" or \"side-scrollers\") are a broad category of character action games that were popular from the mid-1980s to the 1990s, which involve player characters defeating large groups of weaker enemies along a side-scrolling playfield. Examples include beat 'em ups like \"Kung-Fu Master\" and \"Double Dragon\", ninja action games like \"The Legend of Kage\" and \"Shinobi\", scrolling platformers like \"Super Mario Bros.\" and \"Sonic the Hedgehog\", and run and gun shooters like \"Rolling Thunder\" and \"Gunstar Heroes\".\n\"Character action games\" is also a term used for 3D hack and slash games modelled after \"Devil May Cry\", which represent an evolution of arcade character action games. Other examples of this sub-genre include \"Ninja Gaiden\", \"God of War\", and \"Bayonetta\".\nFighting games.\nFighting games typically have a larger number of player characters to choose from, with some basic moves available to all or most characters and some unique moves only available to one or a few characters. Having many distinctive characters to play as and against, all possessing different moves and abilities, is necessary to create a larger gameplay variety in such games.\nHero shooters.\nSimilarly to MOBAs, hero shooters emphasize pre-designed \"hero\" characters with distinctive abilities and weapons that are not available to the other characters. Hero shooters strongly encourage teamwork between players on a team, guiding players to select effective combinations of hero characters and coordinate the use of hero abilities during a match.\nMultiplayer online battle arena.\nMultiplayer online battle arena games offer a large group of viable player characters for the player to choose from, each of which having distinctive abilities, strengths, and weaknesses to make the game play style different. Characters can learn new abilities or augment existing ones over the course of a match by collecting experience points. Choosing a character who complements the player's teammates and counters their opponents opens up a strategy before the beginning of the match itself. Playable characters blend a variety of fantasy tropes, featuring numerous references to popular culture and mythology.\nRole-playing games.\nIn both tabletop role playing games such as \"Dungeons &amp; Dragons\" and role-playing video games such as \"Final Fantasy,\" a player typically creates or takes on the identity of a character that may have nothing in common with the player. The character is often of a certain (usually fictional) race and class (such as zombie, berserker, rifleman, elf, or cleric), each with strengths and weaknesses. The attributes of the characters (such as magic and fighting ability) are given as numerical values which can be increased as the character progresses and gains rank and experience points through accomplishing goals or fighting enemies.\nSports games.\nIn many sports games, player characters are often modelled after real-life athletes, as opposed to fictional characters. This is particularly the case for sports simulation games, whereas many arcade-style sports games often have fictional characters instead.\nSecret characters.\nA secret or unlockable character is a playable character in a video game available only after either completing the game or meeting another requirement. In some video games, characters that are not secret but appear only as non-player characters like bosses or enemies become playable characters after completing certain requirements, or sometimes cheating.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23623", "revid": "1302886432", "url": "https://en.wikipedia.org/wiki?curid=23623", "title": "Parish", "text": "Ecclesiastical subdivision of a diocese\nA parish is a territorial entity in many Christian denominations, constituting a division within a diocese. A parish is under the pastoral care and clerical jurisdiction of a priest, often termed a parish priest, who might be assisted by one or more curates, and who operates from a parish church. In England, a parish historically often covered the same geographical area as a manor. Its association with the parish church remains paramount.\nBy extension the term \"parish\" refers not only to the territorial entity but to the people of its community or congregation as well as to church property within it. In England this church property was technically in ownership of the parish priest \"ex officio\", vested in him on his institution to that parish.\nEtymology and use.\nFirst attested in English in the late 13th\u00a0century, the word \"parish\" comes from the Old French , in turn from , the Romanisation of the , \"sojourning in a foreign land\", itself from (\"paroikos\"), \"dwelling beside, stranger, sojourner\", which is a compound of (\"par\u00e1\"), \"beside, by, near\" and (\"o\u00eekos\"), \"house\".\nAs an ancient concept, the term \"parish\" occurs in the long-established Christian denominations: Catholic, Anglican Communion, the Eastern Orthodox Church, and Lutheran churches, and in some Methodist, Congregationalist and Presbyterian administrations.\nThe eighth Archbishop of Canterbury Theodore of Tarsus (c. 602\u2013690) appended the parish structure to the Anglo-Saxon township unit, where it existed, and where minsters catered to the surrounding district.\nTerritorial structure.\nBroadly speaking, the parish is the standard unit in episcopal polity of church administration, although parts of a parish may be subdivided as a \"chapelry\", with a chapel of ease or filial church serving as the local place of worship in cases of difficulty to access the main parish church.\nIn the wider picture of ecclesiastical polity, a \"parish\" comprises a division of a diocese or see. Parishes within a diocese may be grouped into a deanery or \"vicariate forane\" (or simply \"vicariate\"), overseen by a dean or \"vicar forane\", or in some cases by an archpriest. Some churches of the Anglican Communion have deaneries as units of an archdeaconry.\nOutstations.\nAn outstation is a newly created congregation, a term usually used where the church is evangelical, or a mission and particularly in African countries, but also historically in Australia. They exist mostly within the Catholic and Anglican parishes.\nThe Anglican Diocese of Cameroon describes their outstations as the result of outreach work \"initiated, sponsored and supervised by the mother parishes\". Once there is a big enough group of worshippers in the same place, the outstation is named by the bishop of the diocese. They are run by \"catechists/evangelists\" or lay readers, and supervised by the creator parish or archdeaconry.\nOutstations are not self-supporting, and in poor areas often consist of a very simple structure. The parish priest visits as often as possible. If and when the community has grown enough, the outstation may become a parish and have a parish priest assigned to it.\nCatholic Church.\nIn the Catholic Church, each parish normally has its own parish priest (in some countries called pastor or provost), who has responsibility and canonical authority over the parish.\nWhat in most English-speaking countries is termed the \"parish priest\" is referred to as the \"pastor\" in the United States, where the term \"parish priest\" is used of any priest assigned to a parish even in a subordinate capacity. These are called \"assistant priests\", \"parochial vicars\", \"curates\", or, in the United States, \"associate pastors\" and \"assistant pastors\".\nEach diocese (administrative region) is divided into parishes, each with their own central church called the parish church, where religious services take place. Some larger parishes or parishes that have been combined under one parish priest may have two or more such churches, or the parish may be responsible for chapels (or chapels of ease) located at some distance from the mother church for the convenience of distant parishioners. In addition to a parish church, each parish may maintain auxiliary organizations and their facilities such as a rectory, parish hall, parochial school, or convent, frequently located on the same campus or adjacent to the church. Part of the parish is the oratory (also called a patronage, parish center, or youth center), a location that is designated for the youth ministry of the Catholic Church and also a youth gathering place with facilities such as music rooms and football pitches.\nNormally, a parish comprises all Catholics living within its geographically defined area, but non-territorial parishes can also be established within a defined area on a personal basis for Catholics belonging to a particular rite, language, nationality, or community. An example is that of personal parishes established in accordance with the 7 July 2007 \"motu proprio\" \"Summorum Pontificum\" for those attached to the pre-Vatican II liturgy.\nLutheran Churches.\nIn the Lutheran Churches, parishes (Swedish: \"socken\" or \"f\u00f6rsamling\") are territorial, meaning that they include the people living within its boundaries.\nAt the end of the 19th century, the Church of Sweden possessed 2,000 parishes.\nAnglican Churches.\nChurch of England.\nThe Church of England's geographical structure uses the local parish church as its basic unit. The parish system survived the Reformation with the Anglican Church's secession from Rome remaining largely untouched; thus, it shares its roots with the Catholic Church's system described above. Parishes may extend into different counties or hundreds and historically many parishes comprised extra outlying portions in addition to its principal district, usually being described as 'detached' and intermixed with the lands of other parishes. Church of England parishes nowadays all lie within one of 42 dioceses divided between the provinces of Canterbury, 30 and York, 12.\nEach parish normally has its own parish priest (either a vicar or rector, owing to the vagaries of the feudal tithe system: rectories usually having had greater income) and perhaps supported by one or more curates or deacons\u2014although as a result of ecclesiastical pluralism some parish priests might have held more than one parish living, placing a curate in charge of those where they did not reside. Now, however, it is common for a number of neighbouring parishes to be placed under one benefice in the charge of a priest who conducts services by rotation, with additional services being provided by lay readers or other non-ordained members of the church community.\nA chapelry was a subdivision of an ecclesiastical parish in England, and parts of Lowland Scotland up to the mid 19th century. It had a similar status to a township but was so named as it had a chapel which acted as a subsidiary place of worship to the main parish church.\nIn England civil parishes and their governing parish councils evolved in the 19th century as ecclesiastical parishes began to be relieved of what became considered to be civic responsibilities. Thus their boundaries began to diverge. The word \"parish\" acquired a secular usage. Since 1895, a parish council elected by public vote or a (civil) parish meeting administers a civil parish and is formally recognised as the level of local government below a district council.\nThe traditional structure of the Church of England with the parish as the basic unit has been exported to other countries and churches throughout the Anglican Communion and Commonwealth but does not necessarily continue to be administered in the same way.\nChurch in Wales.\nThe Church in Wales was disestablished in 1920 and is made up of six dioceses. It retained the parish system and parishes were also civil administration areas until communities were established in 1974, but did not necessarily share the same boundaries. The reduction in the numbers of worshippers, and the increasing costs of maintaining often ancient buildings, led over time to parish reorganisation, parish groupings and Rectorial Benefices (merged parishes led by a Rector).\nIn 2010, the Church in Wales engaged the Rt Rev Richard Harries (Lord Harries of Pentregarth), a former Church of England Bishop of Oxford; Prof Charles Handy; and Prof Patricia Peattie, to carry out a review into the organisation of the Church and make recommendations as to its future shape. The group published its report (\"Church in Wales Review\") in July 2012 and proposed that parishes should be reorganised into larger Ministry Areas (Ardaloedd Gweinidogaeth). It stated that \"the parish system, as originally set up ... is no longer sustainable\" and suggested that the Ministry Areas should each have a leadership team containing lay people as well as clergy, following the principles of \"collaborative ministry\". Over the next decade, the six dioceses all implemented the report, with the final Ministry Areas being instituted in 2022. In the Diocese of Saint Asaph (Llanelwy), they are known as Mission Areas (Ardaloedd Cenhadaeth).\nPresbyterian Churches.\nChurch of Scotland.\nThe parish is also the basic level of church administration in the Church of Scotland. Spiritual oversight of each parish church in Scotland is responsibility of the congregation's Kirk Session. Patronage was regulated in 1711 (Patronage Act) and abolished in 1874, with the result that ministers must be elected by members of the congregation. Many parish churches in Scotland today are \"linked\" with neighbouring parish churches served by a single minister. Since the abolition of parishes as a unit of civil government in Scotland in 1929, Scottish parishes have purely ecclesiastical significance and the boundaries may be adjusted by the local Presbytery.\nMethodist Church.\nIn the United Methodist Church congregations are called parishes, though they are more often simply called congregations and have no geographic boundaries. A prominent example of this usage comes in \"The Book of Discipline of The United Methodist Church\", in which the committee of every local congregation that handles staff support is referred to as the committee on Pastor-Parish Relations. This committee gives recommendations to the bishop on behalf of the parish/congregation since it is the United Methodist Bishop of the episcopal area who appoints a pastor to each congregation. The same is true in the African Methodist Episcopal Church and the Christian Methodist Episcopal Church.\nIn New Zealand, a local grouping of Methodist churches that share one or more ministers (which in the United Kingdom would be called a circuit) is referred to as a parish.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23624", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=23624", "title": "Procopius", "text": "Byzantine historian (c. 500 \u2013 565)\nProcopius of Caesarea (; \"Prok\u00f3pios ho Kaisare\u00fas\"; ; c.\u2009500 \u2013 565) was a prominent late antique Greek scholar and historian from Caesarea Maritima. Accompanying the Roman general Belisarius in Emperor Justinian's wars, Procopius became the principal historian of the 6th century, writing the \"History of the Wars\", the \"Buildings\", and the infamous \"Secret History\".\nEarly life.\nApart from his own writings, the main source for Procopius's life is an entry in the \"Suda\", a Byzantine Greek encyclopaedia written sometime after 975 which (based on older sources) discusses his early life. He was a native of Caesarea in the province of \"Palaestina Prima\". He would have received a conventional upper-class education in the Greek classics and rhetoric, perhaps at the famous school at Gaza. He may have attended law school, possibly at Berytus (present-day Beirut) or Constantinople (now Istanbul), and became a lawyer (\"rhetor\"). He evidently knew Latin, as was natural for a man with legal training.\nCareer.\nIn 527, the first year of the reign of the emperor JustinianI, Procopius became the legal adviser (\"\") for Belisarius, a Roman general whom Justinian made his chief military commander in a great attempt to restore control over the lost western provinces of the empire.\nProcopius was with Belisarius on the eastern front until the latter was defeated at the Battle of Callinicum in 531 and recalled to Constantinople. Procopius witnessed the Nika riots of January, 532, which Belisarius and his fellow general Mundus repressed with a massacre in the Hippodrome there. In 533, he accompanied Belisarius on his victorious expedition against the Vandal kingdom in North Africa, took part in the capture of Carthage, and remained in Africa with Belisarius's successor Solomon the Eunuch when Belisarius returned east to the capital. Procopius recorded a few of the extreme weather events of 535\u2013536, although these were presented as a backdrop to Byzantine military activities, such as a mutiny in and around Carthage. He rejoined Belisarius for his campaign against the Ostrogothic kingdom in Italy and experienced the Gothic siege of Rome that lasted a year and nine days, ending in mid-March 538. He witnessed Belisarius's entry into the Gothic capital, Ravenna, in 540. Both the \"Wars\" and the \"Secret History\" suggest that his relationship with Belisarius cooled thereafter. Perhaps he accompanied the general once more to the Persian front in 542. When Belisarius was sent back to Italy in 544 to cope with a renewal of the war with the Goths, now led by the able king Totila, Procopius appears to have no longer been on Belisarius's staff.\nAs \"magister militum\", Belisarius was an \"illustrious man\" (; , \"illo\u00fastrios\"); being his ', Procopius must therefore have had at least the rank of a \"visible man\" (\"vir spectabilis\"). He thus belonged to the mid-ranking group of the senatorial order ('). However, the \"Suda\", which is usually well-informed in such matters, also describes Procopius himself as one of the '. Should this information be correct, Procopius would have had a seat in Constantinople's senate, which was restricted to the ' under Justinian.\nDeath.\nIt is not certain when Procopius died. Many historians\u2014including Howard-Johnson, Cameron, and Geoffrey Greatrex\u2014date his death to 554, but there was an urban prefect of Constantinople (\"\") who was called Procopius in 562. In that year, Belisarius was implicated in a conspiracy and was brought before this urban prefect.\nWritings.\nThe writings of Procopius are the primary source of information for the rule of the emperor JustinianI. Procopius was the author of a \"history\" in eight books on the wars prosecuted by Justinian, a panegyric on the emperor's public works projects throughout the empire, and a book known as the \"Secret History\" that claims to report the scandals that Procopius could not include in his officially sanctioned history for fear of angering the emperor, his wife, Belisarius, and the general's wife Antonia.\n\"History of the Wars\".\nProcopius's \"Wars\" or \"History of the Wars\" (, \"Hyp\u00e8r t\u014dn Pol\u00e9mon L\u00f3goi\", \"Words on the Wars\"; , \"On the Wars\") is his most important work, although less well known than the \"Secret History\". The first seven books seem to have been largely completed by 545 and may have been published as a set. They were, however, updated to mid-century before publication, with the latest mentioned event occurring in early 551. The eighth and final book brought the history to 553.\nThe first two books\u2014often known as \"The Persian War\" ()\u2014deal with the conflict between the Romans and Sassanid Persia in Mesopotamia, Syria, Armenia, Lazica, and Iberia (present-day Georgia). It details the campaigns of the Sassanid shah KavadhI, the 532 'Nika' revolt, the war by Kavadh's successor KhosrauI in 540, his destruction of Antioch and deportation of its inhabitants to Mesopotamia, and the great plague that devastated the empire from 542. The \"Persian War\" also covers the early career of Procopius's patron Belisarius in some detail.\nThe \"Wars\"\u2019 next two books\u2014known as \"The Vandal War\" or \"Vandalic War\" ()\u2014cover Belisarius's successful campaign against the Vandal kingdom that had occupied Rome's provinces in northwest Africa for the last century.\nThe final four books\u2014known as \"The Gothic War\" ()\u2014cover the Italian campaigns by Belisarius and others against the Ostrogoths. Procopius includes accounts of the 1st and 2nd sieges of Naples and the 1st, 2nd, and 3rd sieges of Rome. He also includes an account of the rise of the Franks (see \"Arborychoi\"). The last book describes the eunuch Narses's successful conclusion of the Italian campaign and includes some coverage of campaigns along the empire's eastern borders as well.\nThe War histories contain various longer excursions on different topics. These serve both literary and thematic purposes by providing the necessary background information as well as contextualising the acts of war described on different levels. The \"Wars\" proved influential on later Byzantine historiography. In the 570s Agathias wrote \"Histories\", a continuation of Procopius's work in a similar style.\n\"Secret History\".\nProcopius's now famous \"Anecdota\", also known as \"Secret History\" (, \"Ap\u00f3kryphe Histor\u00eda\"; ), was discovered centuries later at the Vatican Library in Rome and published in Lyon by Niccol\u00f2 Alamanni in 1623. Its existence was already known from the \"Suda\", which referred to it as Procopius's \"unpublished works\" containing \"comedy\" and \"invective\" of Justinian, Theodora, Belisarius and Antonina. The \"Secret History\" covers roughly the same years as the first seven books of \"The History of the Wars\" and appears to have been written after they were published. Current consensus generally dates it to 550, or less commonly 558. Since no author seems to have been aware of this work for centuries, even though Procopius was widely read and quoted, the \"Secret History\" appears to have remained unknown for several generations. How and when the text was published is unknown.\nIn the eyes of many scholars, the \"Secret History\" reveals an author who had become deeply disillusioned with Emperor Justinian, his wife Theodora, the general Belisarius, and his wife Antonina. The work claims to expose the secret springs of their public actions, as well as the private lives of the emperor and his entourage. In recent years, however, other scholars have warned against confusing the account in the \"Secret History\" with Procopius' actual opinion. Justinian is portrayed as cruel, venal, prodigal, and incompetent. In one passage, it is even claimed that he was possessed by demonic spirits or was himself a demon lord:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And some of those who have been with Justinian at the palace late at night, men who were pure of spirit, have thought they saw a strange demoniac form taking his place. One man said that the Emperor suddenly rose from his throne and walked about, and indeed he was never wont to remain sitting for long, and immediately Justinian's head vanished, while the rest of his body seemed to ebb and flow; whereat the beholder stood aghast and fearful, wondering if his eyes were deceiving him. But presently he perceived the vanished head filling out and joining the body again as strangely as it had left it.\nSimilarly, the Theodora of the \"Secret History\" is a garish portrait of vulgarity and insatiable lust juxtaposed with cold-blooded self-interest, shrewishness, and envious and fearful mean-spiritedness. Throughout the Secret History, Procopius both reminds us of Theodora's humble origins and criticizes her lack of moral virtue, reputation, and education. Among the more titillating (and dubious) revelations in the \"Secret History\" is Procopius's account of Theodora's thespian accomplishments:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Often, even in the theatre, in the sight of all the people, she removed her costume and stood nude in their midst, except for a girdle about the groin: not that she was abashed at revealing that, too, to the audience, but because there was a law against appearing altogether naked on the stage, without at least this much of a fig-leaf. Covered thus with a ribbon, she would sink to the stage floor and recline on her back. Slaves to whom the duty was entrusted would then scatter grains of barley from above into the calyx of this passion flower, whence geese, trained for the purpose, would next pick the grains one by one with their bills and eat.\nJustinian and Theodora are portrayed as the antithesis of good rulers, with each representing the opposite side of the emotional spectrum. Justinian was approachable and kindly, even while ordering property confiscations or people's destruction. Conversely, Theodora was described as irrational and driven by her anger, often by minor affronts.\nFurthermore, \"Secret History\" portrays Belisarius as a weak man completely emasculated by his wife, Antonina, who is portrayed in very similar terms to Theodora. They are both said to be former actresses and close friends. Procopius claimed Antonina worked as an agent for Theodora against Belisarius, and had an ongoing affair with Belisarius' godson, Theodosius.\n\"The Buildings\".\n\"The Buildings\" (, ; , \"On Buildings\") is a panegyric on Justinian's public works projects throughout the empire. The first book may date to before the collapse of the first dome of Hagia Sophia in 557, but some scholars think that it is possible that the work postdates the building of the bridge over the Sangarius in the late 550s. Historians consider \"Buildings\" to be an incomplete work due to evidence of the surviving version being a draft with two possible redactions.\n\"Buildings\" was likely written at Justinian's behest, and it is doubtful that its sentiments expressed are sincere. It tells us nothing further about Belisarius, and it takes a sharply different attitude towards Justinian. He is presented as an idealised Christian emperor who built churches for the glory of God and defenses for the safety of his subjects. He is depicted showing particular concern for the water supply, building new aqueducts and restoring those that had fallen into disuse. Theodora, who was dead when this panegyric was written, is mentioned only briefly, but Procopius's praise of her beauty is fulsome.\nDue to the panegyrical nature of Procopius's \"Buildings\", historians have discovered several discrepancies between claims made by Procopius and accounts in other primary sources. A prime example is Procopius's starting the reign of Justinian in 518, which was the start of the reign of his uncle and predecessor Justin I. By treating the uncle's reign as part of his nephew's, Procopius was able to credit Justinian with buildings erected or begun under Justin's administration. Such works include renovation of the walls of Edessa after its 525 flood and consecration of several churches in the region. Similarly, Procopius falsely credits Justinian for the extensive refortification of the cities of Tomis and Histria in Scythia Minor. This had been carried out under Anastasius I, who reigned before Justin.\nInterpretations of Procopius' works.\nProcopius is generally believed to be aligned with the senatorial ranks that disagreed with Justinian's tax policy (\"Secret History\" 12.12-14). Over time, Procopius' initial optimism may have been replaced by his disillusionment with Belisarius and increasing dislike of Justinian.\nHenning B\u00f6rm has argued that Procopius prepared the \"Secret History\" as an exaggerated document out of fear that a conspiracy might overthrow Justinian's regime, which\u2014as a kind of court historian\u2014might be reckoned to include him. The unpublished manuscript would then have been an insurance that could be offered to the new ruler as a way to avoid punishment. If this hypothesis is correct, the \"Secret History\" would not be proof that Procopius hated Justinian or Theodora.\nAnthony Kaldellis suggests that the \"Secret History\" tells the dangers of \"the rule of women\". For Procopius, it was not that women could not lead an empire, but only women demonstrating masculine virtues could. According to Averil Cameron, the definition of \"feminine\" behavior in the sixth century would be described as \"intriguing\" and \"interfering\". At his core, Procopius wanted to preserve the social order.\nAveril Cameron makes a case that all of his works form a continuous, unified discourse, rather than being contradictory to one another. In her view, Procopius was a better reporter than a historian, whose strength lay in descriptions rather than analyses. She argues that his vision is too black-and-white and remains almost silent on theological and ecclesiastical debates. However, Shaun Tougher notes Procopius' intention to write an ecclesiastical history, which may have provided a more holistic picture of his time, and argues that Procopius should not be assessed as negatively.\nStyle.\nProcopius belongs to the school of late antique historians who continued the traditions of the Second Sophistic. Writing in Attic Greek, these historians relied on Classical writers such as Herodotus, Polybius and in particular Thucydides as their models and, similar to these, dealt with secular subjects. The Classically influenced historians avoided vocabulary unknown to Attic Greek, inserting an explanation when they had to use contemporary words. Thus Procopius includes glosses of monks (\"the most temperate of Christians\") and churches (as equivalent to a \"temple\" or \"shrine\"), since monasticism was unknown to the ancient Athenians and their \"ekkles\u00eda\" had been a popular assembly.\nThe secular historians eschewed the history of the Christian church. Ecclesiastical history was left to a separate genre after Eusebius. Cameron has argued that Procopius's works reflect the tensions between the classical and Christian models of history in 6th-century Constantinople. This has been supported by Whitby's analysis of Procopius's depiction of the capital and its cathedral in comparison to contemporary pagan panegyrics. Procopius can be seen as depicting Justinian as essentially God's vicegerent, making the case for buildings being a primarily religious panegyric. Procopius indicates that he planned to write an ecclesiastical history himself and, if he had, he would probably have followed the rules of that genre. As far as known, however, such an ecclesiastical history was never written.\nSome historians have criticized Propocius's description of some barbarians, for example, he dehumanized the unfamiliar Moors as \"not even properly human\". This was, however, inline with Byzantine ethnographic practice in late antiquity.\nLegacy.\nA number of historical novels based on Procopius's works (along with other sources) have been written. \"Count Belisarius\" was written by poet and novelist Robert Graves in 1938. Procopius himself appears as a minor character in Felix Dahn's \"A Struggle for Rome\" and in L. Sprague de Camp's alternate history novel \"Lest Darkness Fall\". The novel's main character, archaeologist Martin Padway, derives most of his knowledge of historical events from the \"Secret History\".\nThe narrator in Herman Melville's novel \"Moby-Dick\" cites Procopius's description of a captured sea monster as evidence of the narrative's feasibility. A fictionalized version of Procopius, named Pertennius, appears in the fantasy novelist Guy Gavriel Kay's duology The Sarantine Mosaic.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23626", "revid": "50717717", "url": "https://en.wikipedia.org/wiki?curid=23626", "title": "Property", "text": "Entity owned by a person or a group of people\nProperty is a system of rights that gives people legal control of valuable things, and also refers to the valuable things themselves. Depending on the nature of the property, an owner of property may have the right to consume, alter, share, rent, sell, exchange, transfer, give away, or destroy it, or to exclude others from doing these things, as well as to perhaps abandon it; whereas regardless of the nature of the property, the owner thereof has the right to properly use it under the granted property rights.\nIn economics and political economy, there are three broad forms of property: private property, public property, and collective property (or \"cooperative propert\"y). Property may be jointly owned by more than one party equally or unequally, or according to simple or complex agreements; to distinguish ownership and easement from rent, there is an expectation that each party's will with regard to the property be clearly defined and unconditional.. The parties may expect their wills to be unanimous, or alternatively each may expect their own will to be sufficient when no opportunity for dispute exists.\nIn American law, the first Restatement defines property as anything, tangible or intangible, whereby a legal relationship between persons and the State enforces a possessory interest or legal title in that thing. This mediating relationship between individual, property, and State is called a property regime.\nIn sociology and anthropology, property is often defined as a relationship between two or more individuals and an object, in which at least one of these individuals holds a bundle of rights over the object. The distinction between collective and private property is regarded as confusion, since different individuals often hold differing rights over a single object.\nTypes of property include real property (the combination of land and any improvements to or on the ground), personal property (physical possessions belonging to a person), private property (property owned by legal persons, business entities or individual natural persons), public property (State-owned or publicly owned and available possessions) and intellectual property\u2014including exclusive rights over artistic creations and inventions. However, the latter is not always widely recognized or enforced. An article of property may have physical and incorporeal parts. A title, or a right of ownership, establishes the relation between the property and other persons, assuring the owner the right to dispose of the property as the owner sees fit. The unqualified term \"property\" is often used to refer specifically to real property.\nOverview.\nProperty is often defined by the code of the local sovereignty and protected wholly or \u2013 more usually, partially \u2013 by such entity, the owner being responsible for any remainder of protection. The standards of the proof concerning proofs of ownerships are also addressed by the code of the local sovereignty, and such entity plays a role accordingly, typically somewhat managerial. Some philosophers assert that property rights arise from social convention, while others find justifications for them in morality or in natural law.\nVarious scholarly disciplines (such as law, economics, anthropology or sociology) may treat the concept more systematically, but definitions vary, most particularly when involving contracts. Positive law defines such rights, and the judiciary can adjudicate and enforce property rights.\nAccording to Adam Smith (1723\u20131790), the expectation of profit from \"improving one's stock of capital\" rests on private-property rights. Capitalism has as a central assumption that property rights encourage their holders to develop the property, generate wealth, and efficiently allocate resources based on the operation of markets. From this has evolved the modern conception of property as a right enforced by positive law, in the expectation that this will produce more wealth and better standards of living. However, Smith also expressed a very critical view of the effects of property laws on inequality:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Wherever there is a great property, there is great inequality \u2026 Civil government, so far as it is instituted for the security of property, is in reality instituted for the defense of the rich against the poor, or of those who have some property against those who have none at all.\nIn his 1881 text \"The Common Law\", Oliver Wendell Holmes describes property as having two fundamental aspects. The first, possession, can be defined as control over a resource based on the practical inability to contradict the ends of the possessor. The second title is the expectation that others will recognize rights to control resources, even when not in possession. He elaborates on the differences between these two concepts and proposes a history of how they came to be attached to persons, as opposed to families or entities such as the church.\n \"Every man has a property in his person. This nobody has a right to, but himself.\" (John Locke, \"Second Treatise on Civil Government\", 1689)\n \"The reason why men enter into society is the preservation of their property.\" (John Locke, \"Second Treatise on Civil Government\", 1689)\n \"Life, liberty, and property do not exist because men have made laws. On the contrary, it was the fact that life, liberty, and property existed beforehand that caused men to make laws in the first place.\" (Fr\u00e9d\u00e9ric Bastiat, \"The Law\", 1850)\n \"Separate property from private possession and Leviathan becomes master of all... Upon the foundation of private property, great civilizations are built. The conservative acknowledges that the possession of property fixes certain duties upon the possessor; he accepts those moral and legal obligations cheerfully.\" (Russell Kirk, \"The Politics of Prudence\", 1993)\nBoth communism and some forms of socialism have also upheld the notion that private ownership of capital is inherently illegitimate. This argument centers on the idea that private ownership of capital always benefits one class over another, giving rise to domination through this privately owned capital. Communists do not oppose personal property that is \"hard-won, self-acquired, self-earned\" (as \"The Communist Manifesto\" puts it) by members of the proletariat. Both socialism and communism distinguish carefully between private ownership of capital (land, factories, resources, etc.) and private property (homes, material objects, and so forth).\nTypes of property.\nMost legal systems distinguish between different types of property, especially between land (immovable property, estate in land, real estate, real property) and all other forms of property\u2014goods and chattels, movable property or personal property, including the value of legal tender if not the legal tender itself, as the manufacturer rather than the possessor might be the owner. They often distinguish tangible and intangible property. One categorization scheme specifies three species of property: land, improvements (immovable man-made things), and personal property (movable man-made things).\nIn common law, real property (immovable property) is the combination of interests in land and improvements thereto, and personal property is interest in movable property. Real property rights are rights relating to the land. These rights include ownership and usage. Owners can grant rights to persons and entities in the form of leases, licenses, and easements.\nThroughout the last centuries of the second millennium, with the development of more complex theories of property, the concept of personal property had become divided into tangible property (such as cars and clothing) and intangible property (such as financial assets and related rights, including stocks and bonds; intellectual property, including patents, copyrights and trademarks; digital files; communication channels; and certain forms of identifier, including Internet domain names, some forms of network address, some forms of handle and again trademarks).\nTreatment of intangible property is such that an article of property is, by law or otherwise by traditional conceptualization, subject to expiration even when inheritable, which is a key distinction from tangible property. Upon expiration, the property, if of the intellectual category, becomes a part of public domain, to be used by but not owned by anybody, and possibly used by more than one party simultaneously due to the inapplicability of scarcity to intellectual property. Whereas things such as communications channels and pairs of electromagnetic spectrum bands and signal transmission power can only be used by a single party at a time, or a single party in a divisible context, if owned or used. Thus far or usually, those are not considered property, or at least not private property, even though the party bearing right of exclusive use may transfer that right to another.\nIn many societies the human body is considered property of some kind or other. The question of the ownership and rights to one's body arise in general in the discussion of human rights, including the specific issues of slavery, conscription, rights of children under the age of majority, marriage, abortion, prostitution, drugs, euthanasia and organ donation.\nRelated concepts.\nOf the following, only sale and at-will sharing involve no encumbrance.\nIssues in property theory.\nPrinciple.\nThe two major justifications are given for the original property, or the homestead principle, are effort and scarcity. John Locke emphasized effort, \"mixing your labor\" with an object, or clearing and cultivating virgin land. Benjamin Tucker preferred to look at the telos of property, i.e., what is the purpose of property? His answer: to solve the scarcity problem. Only when items are relatively scarce concerning people's desires, do they become property. For example, hunter-gatherers did not consider land to be property, since there was no shortage of land. Agrarian societies later made arable land property, as it was scarce. For something to be economically scarce, it must necessarily have the \"exclusivity property\"\u2014that use by one person excludes others from using it. These two justifications lead to different conclusions on what can be property. Intellectual property\u2014incorporeal things like ideas, plans, orderings and arrangements (musical compositions, novels, computer programs)\u2014are generally considered valid property to those who support an effort justification, but invalid to those who support a scarcity justification, since the things don't have the exclusivity property (however, those who support a scarcity justification may still support other \"intellectual property\" laws such as Copyright, as long as these are a subject of contract instead of government arbitration). Thus even ardent propertarians may disagree about IP. By either standard, one's body is one's property.\nFrom some anarchist points of view, the validity of property depends on whether the \"property right\" requires enforcement by the State. Different forms of \"property\" require different amounts of enforcement: intellectual property requires a great deal of state intervention to enforce, ownership of distant physical property requires quite a lot, ownership of carried objects requires very little. In contrast, requesting one's own body requires absolutely no state intervention. So some anarchists don't believe in property at all.\nMany things have existed that did not have an owner, sometimes called the commons. The term \"commons,\" however, is also often used to mean something entirely different: \"general collective ownership\"\u2014i.e. common ownership. Also, the same term is sometimes used by statists to mean government-owned property that the general public is allowed to access (public property). Law in all societies has tended to reduce the number of things not having clear owners. Supporters of property rights argue that this enables better protection of scarce resources due to the tragedy of the commons. At the same time, critics say that it leads to the 'exploitation' of those resources for personal gain and that it hinders taking advantage of potential network effects. These arguments have differing validity for different types of \"property\"\u2014things that are not scarce are, for instance, not subject to the tragedy of the commons. Some apparent critics advocate general collective ownership rather than ownerlessness.\nThings that do not have owners include: ideas (except for intellectual property), seawater (which is, however, protected by anti-pollution laws), parts of the seafloor (see the United Nations Convention on the Law of the Sea for restrictions), gases in Earth's atmosphere, animals in the wild (although in most nations, animals are tied to the land. In the United States and Canada, wildlife is generally defined in statute as property of the State. This public ownership of wildlife is referred to as the North American Model of Wildlife Conservation and is based on The Public Trust Doctrine.), celestial bodies and outer space, and land in Antarctica.\nThe nature of children under the age of majority is another contested issue here. In ancient societies, children were generally considered the property of their parents. However, children in most modern communities theoretically own their bodies but are not regarded as competent to exercise their rights. Their parents or guardians are given most of the fundamental rights of control over them.\nQuestions regarding the nature of ownership of the body also come up in the issue of abortion, drugs, and euthanasia.\nIn many ancient legal systems (e.g., early Roman law), religious sites (e.g. temples) were considered property of the God or gods they were devoted to. However, religious pluralism makes it more convenient to have sacred sites owned by the spiritual body that runs them.\nIntellectual property and air (airspace, no-fly zone, pollution laws, which can include tradable emissions rights) can be property in some senses of the word.\nOwnership of land can be held separately from the ownership of rights over that land, including sporting rights, mineral rights, development rights, air rights, and such other rights as may be worth segregating from simple land ownership.\nOwnership.\nOwnership laws may vary widely among countries depending on the nature of the property of interest (e.g., firearms, real property, personal property, animals). Persons can own property directly. In most societies legal entities, such as corporations, trusts and nations (or governments) own property.\nIn many countries women have limited access to property following restrictive inheritance and family laws, under which only men have actual or formal rights to own property.\nIn the Inca empire, the dead emperors, considered gods, still controlled property after death.\nGovernment interference.\nIn 17th-century England, the legal directive that nobody may enter a home (which in the 17th century would typically have been male-owned) unless by the owner's invitation or consent, was established as common law in Sir Edward Coke 's \"Institutes of the Lawes of England\". \"For a man's house is his castle, et domus sua cuique est tutissimum refugium [and each man's home is his safest refuge].\" It is the origin of the famous dictum, \"an Englishman's home is his castle\". The ruling enshrined into law what several English writers had espoused in the 16th century. Unlike the rest of Europe the British had a proclivity towards owning their own homes. British Prime Minister William Pitt, 1st Earl of Chatham defined the meaning of castle in 1763, \"The poorest man may in his cottage bid defiance to all the forces of the crown. It may be frail \u2013 its roof may shake \u2013 the wind may blow through it \u2013 the storm may enter \u2013 the rain may enter \u2013 but the King of England cannot enter.\"\nThat principle was carried to the United States. Under U.S. law, the principal limitations on whether and the extent to which the State may interfere with property rights are set by the Constitution. The Takings clause requires that the government (whether State or federal\u2014for the 14th Amendment's due process clause imposes the 5th Amendment's takings clause on state governments) may take private property only for a public purpose after exercising due process of law, and upon making \"just compensation.\" If an interest is not deemed a \"property\" right or the conduct is merely an intentional tort, these limitations do not apply, and the doctrine of sovereign immunity precludes relief. Moreover, if the interference does not almost completely make the property valueless, the interference will not be deemed a taking but instead a mere regulation of use. On the other hand, some governmental regulations of property use have been deemed so severe that they have been considered \"regulatory takings.\" Moreover, conduct is sometimes deemed only a nuisance, or another tort has been held a taking of property where the conduct was sufficiently persistent and severe.\nTheories.\nThere exist many theories of property. One is the relatively rare first possession theory of property, where ownership of something is seen as justified simply by someone seizing something before someone else does. Perhaps one of the most popular is the natural rights definition of property rights as advanced by John Locke. Locke advanced the theory that God granted dominion over nature to man through Adam in the book of Genesis. Therefore, he theorized that when one mixes one's labor with nature, one gains a relationship with that part of nature with which the labor is mixed, subject to the limitation that there should be \"enough, and as good, left in common for others.\" (see Lockean proviso)\nIn his encyclical letter \"Rerum novarum\" (1891), Pope Leo XIII wrote, \"It is surely undeniable that, when a man engages in remunerative labor, the impelling reason and motive of his work is to obtain property, and after that to hold it as his very own.\"\nAnthropology studies the diverse ownership systems, rights of use and transfer, and possession under the term \"theories of property\". As mentioned, western legal theory is based on the owner of property being a legal person. However, not all property systems are founded on this basis.\nIn every culture studied, ownership and possession are the subjects of custom and regulation, and \"law\" is where the term can meaningfully be applied. Many tribal cultures balance individual rights with the laws of collective groups: tribes, families, associations, and nations. For example, the 1839 Cherokee Constitution frames the issue in these terms:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Sec. 2. The lands of the Cherokee Nation shall remain common property. Still, the improvements made thereon, and in possession of the citizens respectively who made, or may rightfully own them: Provided, that the citizens of the Nation possessing the exclusive and indefeasible right to their improvements, as expressed in this article, shall possess no right or power to dispose of their improvements, in any manner whatever, to the United States, individual States, or individual citizens thereof; and that, whenever any citizen shall remove with his effects out of the limits of this Nation, and become a citizen of any other government, all his rights and privileges as a citizen of this Nation shall cease: Provided, nevertheless, That the National Council shall have power to re-admit, by law, to all the rights of citizenship, any such person or persons who may, at any time, desire to return to the Nation, on memorializing the National Council for such readmission.\nCommunal property systems describe ownership as belonging to the entire social and political unit. Common ownership in a hypothetical communist society is distinguished from primitive forms of common property that have existed throughout history, such as Communalism and primitive communism, in that communist common ownership is the outcome of social and technological developments leading to the elimination of material scarcity in society.\nCorporate systems describe ownership as being attached to an identifiable group with an identifiable responsible individual. The Roman property law was based on such a corporate system. In a well-known paper that contributed to the creation of the field of law and economics in the late 1960s, the American scholar Harold Demsetz described how the concept of property rights makes social interactions easier:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the world of Robinson Crusoe, property rights play no role. Property rights are an instrument of society and derive their significance from the fact that they help a man form those expectations which he can reasonably hold in his dealings with others. These expectations find expression in society's laws, customs, and more. An owner of property rights possesses the consent of fellowmen to allow him to act in particular ways. An owner expects the community to prevent others from interfering with his actions, provided that these actions are not prohibited in the specifications of his rights.\nDifferent societies may have other theories of property for differing types of ownership. For example, Pauline Peters argued that property systems are not isolable from the social fabric, and notions of property may not be stated as such but instead may be framed in negative terms: for example, the taboo system among Polynesian peoples.\nProperty in philosophy.\nIn medieval and Renaissance Europe the term \"property\" essentially referred to land. After much rethinking, land has come to be regarded as only a special case of the property genus. This rethinking was inspired by at least three broad features of early modern Europe: the surge of commerce, the breakdown of efforts to prohibit interest (then called \"usury\"), and the development of centralized national monarchies.\nAncient philosophy.\nUrukagina, the king of the Sumerian city-state Lagash, established the first laws that forbade compelling the sale of property.\nThe Bible in Leviticus 19:11 and 19:13 states that the Israelites are not to steal.\nAristotle, in \"Politics,\" advocates \"private property\". He argues that self-interest leads to neglect of the commons. \"[T]hat which is common to the greatest number has the least care bestowed upon it. Everyone thinks chiefly of his own, hardly at all of the common interest, and only when he is himself concerned as an individual.\"\nIn addition, he says that when property is common, there are natural problems that arise due to differences in labor: \"If they do not share equally enjoyments and toils, those who labor much and get little will necessarily complain of those who labor little and receive or consume much. But indeed, there is always a difficulty in men living together and having all human relations in common, but especially in their having common property.\" (http://)\nCicero held that there is no private property under natural law but only under human law. Seneca viewed property as only becoming necessary when men become avaricious. St. Ambrose later adopted this view and St. Augustine even derided heretics for complaining the Emperor could not confiscate property they had labored for.\nMedieval philosophy.\nThomas Aquinas (13th century).\nThe canon law \"Decretum Gratiani\" maintained that mere human law creates property, repeating the phrases used by St. Augustine. St. Thomas Aquinas agreed with regard to the private consumption of property but modified patristic theory in finding that the private possession of property is necessary. Thomas Aquinas concludes that, given certain detailed provisions,\nModern philosophy.\nThomas Hobbes (17th century).\nThe principal writings of Thomas Hobbes appeared between 1640 and 1651\u2014during and immediately following the war between forces loyal to King Charles I and those loyal to Parliament. In his own words, Hobbes' reflection began with the idea of \"giving to every man his own,\" a phrase he drew from the writings of Cicero. But he wondered: How can anybody call anything his own?\nJames Harrington (17th century).\nA contemporary of Hobbes, James Harrington, reacted to the same tumult differently: he considered property natural but not inevitable. The author of \"Oceana,\" he may have been the first political theorist to postulate that political power is a consequence, not the cause, of the distribution of property. He said that the worst possible situation is when the commoners have half a nation's property, with the crown and nobility holding the other half\u2014a circumstance fraught with instability and violence. He suggested a much better situation (a stable republic) would exist once the commoners own most property.\nIn later years, the ranks of Harrington's admirers included American revolutionary and founder John Adams.\nRobert Filmer (17th century).\nAnother member of the Hobbes/Harrington generation, Sir Robert Filmer, reached conclusions much like Hobbes', but through Biblical exegesis. Filmer said that the institution of kingship is analogous to that of fatherhood, that subjects are still, children, whether obedient or unruly and that property rights are akin to the household goods that a father may dole out among his children\u2014his to take back and dispose of according to his pleasure.\nJohn Locke (17th century).\nIn the following generation, John Locke sought to answer Filmer, creating a rationale for a balanced constitution in which the monarch had a part to play, but not an overwhelming part. Since Filmer's views essentially require that the Stuart family be uniquely descended from the patriarchs of the Bible, and even in the late 17th century, that was a difficult view to uphold, Locke attacked Filmer's views in his First Treatise on Government, freeing him to set out his own views in the Second Treatise on Civil Government. Therein, Locke imagined a pre-social world each of the unhappy residents which are willing to create a social contract because otherwise, \"the enjoyment of the property he has in this state is very unsafe, very insecure,\" and therefore, the \"great and chief end, therefore, of men's uniting into commonwealths, and putting themselves under government, is the preservation of their property.\" They would, he allowed, create a monarchy, but its task would be to execute the will of an elected legislature. \"To this end\" (to achieve the previously specified goal), he wrote, \"it is that men give up all their natural power to the society they enter into, and the community put the Legislative power into such hands as they think fit, with this trust, that they shall be governed by declared laws, or else their peace, quiet, and property will still be at the same uncertainty as it was in the state of nature.\"\nEven when it keeps to proper legislative form, Locke held that there are limits to what a government established by such a contract might rightly do.\n \"It cannot be supposed that [the hypothetical contractors] they should intend, had they a power so to do, to give anyone or more an absolute arbitrary power over their persons and estates, and put a force into the magistrate's hand to execute his unlimited will arbitrarily upon them; this were to put themselves into a worse condition than the State of nature, wherein they had a liberty to defend their right against the injuries of others, and were upon equal terms of force to maintain it, whether invaded by a single man or many in combination. Whereas by supposing they have given themselves up to the absolute arbitrary power and will of a legislator, they have disarmed themselves, and armed him to make a prey of them when he pleases...\"\nBoth \"persons\" and \"estates\" are to be protected from the arbitrary power of any magistrate, including legislative power and will.\" In Lockean terms, depredations against an estate are just as plausible a justification for resistance and revolution as are those against persons. In neither case are subjects required to allow themselves to become prey.\nTo explain the ownership of property, Locke advanced a labor theory of property.\nDavid Hume (18th century).\nIn contrast to the figures discussed in this section thus far David Hume lived a relatively quiet life that had settled down to a relatively stable social and political structure. He lived the life of a solitary writer until 1763 when, at 52 years of age, he went off to Paris to work at the British embassy.\nIn contrast, one might think to his polemical works on religion and his empiricism-driven skeptical epistemology, Hume's views on law and property were quite conservative.\nHe did not believe in hypothetical contracts or the love of humanity in general and sought to ground politics upon actual human beings as one knows them. \"In general,\" he wrote, \"it may be affirmed that there is no such passion in the human mind, as the love of mankind, merely as such, independent of personal qualities, or services, or of relation to ourselves.\" Existing customs should not lightly be disregarded because they have come to be what they are due to human nature. With this endorsement of custom comes an endorsement of existing governments because he conceived of the two as complementary: \"A regard for liberty, though a laudable passion, ought commonly to be subordinate to a reverence for established government.\"\nTherefore, Hume's view was that there are property rights because of and to the extent that the existing law, supported by social customs, secure them. He offered some practical home-spun advice on the general subject, though, as when he referred to avarice as \"the spur of industry,\" and expressed concern about excessive levels of taxation, which \"destroy industry, by engendering despair.\"\nAdam Smith.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Civil government, so far as it is instituted for the security of property, is, in reality, instituted for the defense of the rich against the poor, or of those who have property against those who have none at all.\"\u2014\u200a\n\"The property that every man has in his labour is the original foundation of all other property, so it is the most sacred and inviolable. The inheritance of a poor man lies in the strength and dexterity of his hands, and to hinder him from employing this strength and dexterity in what manner he thinks proper without injury to his neighbor, is a plain violation of this most sacred property. It is a manifest encroachment upon the just liberty of the workman and those who might be disposed to employ him. It hinders the one from working at what he thinks proper, so it hinders the others from employing whom they think proper. To judge whether he is fit to be employed may surely be trusted to the discretion of the employers whose interest it so much concerns. The affected anxiety of the law-giver lest they should employ an improper person is as impertinent as it is oppressive.\"\n\u2014 (Source: Adam Smith, \"The Wealth of Nations\", 1776, Book I, Chapter X, Part II.)\nBy the mid 19th century, the industrial revolution had transformed England and the United States and had begun in France. As a result, the conventional conception of what constitutes property expanded beyond land to encompass scarce goods. In France, the revolution of the 1790s had led to large-scale confiscation of land formerly owned by the church and king. The restoration of the monarchy led to claims by those dispossessed to have their former lands returned.\nKarl Marx.\nSection VIII, \"Primitive Accumulation\" of Capital involves a critique of Liberal Theories of property rights. Marx notes that under Feudal Law, peasants were legally entitled to their land as the aristocracy was to its manors. Marx cites several historical events in which large numbers of the peasantry were removed from their lands, then seized by the nobility. This seized land was then used for commercial ventures (sheep herding). Marx sees this \"Primitive Accumulation\" as integral to the creation of English Capitalism. This event created a sizeable un-landed class that had to work for wages to survive. Marx asserts that liberal theories of property are \"idyllic\" fairy tales that hide a violent historical process.\nCharles Comte: legitimate origin of property.\nCharles Comte, in \"Trait\u00e9 de la propri\u00e9t\u00e9\" (1834), attempted to justify the legitimacy of private property in response to the Bourbon Restoration. According to David Hart, Comte had three main points: \"firstly, that interference by the state over the centuries in property ownership has had dire consequences for justice as well as for economic productivity; secondly, that property is legitimate when it emerges in such a way as not to harm anyone; and thirdly, that historically some, but by no means all, property which has evolved has done so legitimately, with the implication that the present distribution of property is a complex mixture of legitimately and illegitimately held titles.\"\nComte, as Proudhon later did, rejected Roman legal tradition with its toleration of slavery. Instead, he posited a communal \"national\" property consisting of non-scarce goods, such as land in ancient hunter-gatherer societies. Since agriculture was so much more efficient than hunting and gathering, private property appropriated by someone for farming left remaining hunter-gatherers with more land per person and hence did not harm them. Thus this type of land appropriation did not violate the Lockean proviso \u2013 there was \"still enough, and as good left.\" Later theorists would use Comte's analysis in response to the socialist critique of property.\nPierre-Joseph Proudhon: property is theft.\nIn his 1840 treatise \"What is Property?\", Pierre Proudhon answers with \"Property is theft!\". In natural resources, he sees two types of property, \"de jure\" property (legal title) and \"de facto\" property (physical possession), and argues that the former is illegitimate. Proudhon's conclusion is that \"property, to be just and possible, must necessarily have equality for its condition.\"\nHis analysis of the product of labor upon natural resources as property (usufruct) is more nuanced. He asserts that land itself cannot be property, yet it should be held by individual possessors as stewards of humanity, with the product of labor being the producer's property. Proudhon reasoned that any wealth gained without labor was stolen from those who labored to create that wealth. Even a voluntary contract to surrender the product of work to an employer was theft, according to Proudhon, since the controller of natural resources had no moral right to charge others for the use of that which he did not labor to create did not own.\nProudhon's theory of property greatly influenced the budding socialist movement, inspiring anarchist theorists such as Mikhail Bakunin who modified Proudhon's ideas, as well as antagonizing theorists like Karl Marx.\nFr\u00e9d\u00e9ric Bastiat: property is value.\nFr\u00e9d\u00e9ric Bastiat 's main treatise on property can be found in chapter 8 of his book \"Economic Harmonies\" (1850). In a radical departure from traditional property theory, he defines property, not as a physical object, but rather as a relationship between people concerning a thing. Thus, saying one owns a glass of water is merely verbal shorthand for \"I may justly gift or trade this water to another person.\" In essence, what one owns is not the object but the object's value. By \"value,\" Bastiat means \"market value\"; he emphasizes this is quite different from utility. \"In our relations with one another, we are not owners of the utility of things, but their value, and value is the appraisal made of reciprocal services.\"\nBastiat theorized that, as a result of technological progress and the division of labor, the stock of communal wealth increases over time; that the hours of work an unskilled laborer expends to buy e.g., 100 liters of wheat, decreases over time, thus amounting to \"gratis\" satisfaction. Thus, private property continually destroys itself, becoming transformed into communal wealth. The increasing proportion of communal wealth to private property results in a tendency toward equality of humanity. \"Since the human race began in greatest poverty, that is, when there were the most obstacles to overcome, all that has been achieved from one era to the next is due to the spirit of property.\"\nThis transformation of private property into the communal domain, Bastiat points out, does not imply that personal property will ever totally disappear. On the contrary, this is because man, as he progresses, continually invents new and more sophisticated needs and desires.\nAndrew J. Galambos: a precise definition of property.\nAndrew J. Galambos (1924\u20131997) was an astrophysicist and philosopher who innovated a social structure that sought to maximize human peace and freedom. Galambos' concept of property was essential to his philosophy. He defined property as a man's life and all non-procreative derivatives of his life. (Because the English language is deficient in omitting the feminine from \"man\" when referring to humankind, it is implicit and obligatory that the feminine is included in the term \"man.\")\nGalambos taught that property is essential to a non-coercive social structure. He defined freedom as follows: \"Freedom is the societal condition that exists when every individual has full (100%) control over his property.\" Galambos defines property as having the following elements:\nProperty includes all non-procreative derivatives of an individual's life; this means children are not the property of their parents. and \"primary property\" (a person's own ideas).\nGalambos repeatedly emphasized that actual government exists to protect property and that the State attacks property.\nFor example, the State requires payment for its services in the form of taxes whether or not people desire such services. Since an individual's money is his property, the confiscation of money in the form of taxes is an attack on property. Military conscription is likewise an attack on a person's primordial property.\nContemporary views.\nContemporary political thinkers who believe that natural persons enjoy rights to own property and enter into contracts espouse two views about John Locke. On the one hand, some admire Locke, such as William H. Hutt (1956), who praised Locke for laying down the \"quintessence of individualism.\" On the other hand, those such as Richard Pipes regard Locke's arguments as weak and think that undue reliance thereon has weakened the cause of individualism in recent times. Pipes has written that Locke's work \"marked a regression because it rested on the concept of Natural Law\" rather than upon Harrington's sociological framework.\nHernando de Soto has argued that an essential characteristic of the capitalist market economy is the functioning state protection of property rights in a formal property system which records ownership and transactions. These property rights and the whole legal system of property make possible:\nAccording to de Soto, all of the above enhance economic growth. Academics have criticized the capitalist frame through which property is viewed pointing to the fact that commodifying property or land by assigning it monetary value takes away from the traditional cultural heritage, particularly from first nation inhabitants. These academics point to the personal nature of property and its link to identity being irreconcilable with wealth creation that contemporary Western society subscribes to.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nProperty-giving (legal)\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nProperty-taking (legal)\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nProperty-taking (illegal)\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23627", "revid": "47374908", "url": "https://en.wikipedia.org/wiki?curid=23627", "title": "Police", "text": "Law-enforcement body\nThe police are a constituted body of people empowered by a state with the aim of enforcing the law and protecting the public order as well as the public itself. This commonly includes ensuring the safety, health, and possessions of citizens, and to prevent crime and civil disorder. Their lawful powers encompass arrest and the use of force legitimized by the state via the monopoly on violence. The term is most commonly associated with the police forces of a sovereign state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from the military and other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing. Police forces are usually public sector services, funded through taxes.\nLaw enforcement is only part of policing activity. Policing has included an array of activities in different situations, but the predominant ones are concerned with the preservation of order. In some societies, in the late 18th and early 19th centuries, these developed within the context of maintaining the class system and the protection of private property. Police forces have become ubiquitous and a necessity in complex modern societies. However, their role can sometimes be controversial, as they may be involved to varying degrees in corruption, brutality, and the enforcement of authoritarian rule.\nA police force may also be referred to as a police department, police service, constabulary, gendarmerie, crime prevention, protective services, law enforcement agency, civil guard, or civic guard. Members may be referred to as police officers, troopers, sheriffs, constables, rangers, peace officers or civic/civil guards. Ireland differs from other English-speaking countries by using the Irish language terms \"Garda\" (singular) and \"Garda\u00ed\" (plural), for both the national police force and its members. The word \"police\" is the most universal and similar terms can be seen in many non-English speaking countries.\nNumerous slang terms exist for the police. Many slang terms for police officers are decades or centuries old with lost etymologies. One of the oldest, \"cop\", has largely lost its slang connotations and become a common colloquial term used both by the public and police officers to refer to their profession.\nEtymology.\nFirst attested in English in the early 15th century, originally in a range of senses encompassing '(public) policy; state; public order', the word \"police\" comes from Middle French ('public order, administration, government'), in turn from Latin , which is the romanization of the Ancient Greek () 'citizenship, administration, civil polity'. This is derived from () 'city'.\nDevelopment of theory.\nMichel Foucault wrote that the contemporary concept of police as a paid and funded functionary of the state was developed by German and French legal scholars and practitioners in public administration and statistics in the 17th and early 18th centuries, most notably with Nicolas Delamare's \"Trait\u00e9 de la Police\" (\"Treatise on the Police\"), first published in 1705. The German \"Polizeiwissenschaft\" (Science of Police) first theorized by Philipp von H\u00f6rnigk, a 17th-century Austrian political economist and civil servant, and much more famously by Johann Heinrich Gottlob Justi, who produced an important theoretical work known as Cameral science on the formulation of police. Foucault cites Magdalene Humpert author of \"Bibliographie der Kameralwissenschaften\" (1937) in which the author makes note of a substantial bibliography was produced of over 4,000 pieces of the practice of \"Polizeiwissenschaft\". However, this may be a mistranslation of Foucault's own work since the actual source of Magdalene Humpert states over 14,000 items were produced from the 16th century dates ranging from 1520 to 1850.\nAs conceptualized by the \"Polizeiwissenschaft\", according to Foucault the police had an administrative, economic and social duty (\"procuring abundance\"). It was in charge of demographic concerns and needed to be incorporated within the western political philosophy system of raison d'\u00e9tat and therefore giving the superficial appearance of empowering the population (and unwittingly supervising the population), which, according to mercantilist theory, was to be the main strength of the state. Thus, its functions largely overreached simple law enforcement activities and included public health concerns, urban planning (which was important because of the miasma theory of disease; thus, cemeteries were moved out of town, etc.), and surveillance of prices.\nThe concept of preventive policing, or policing to deter crime from taking place, gained influence in the late 18th century. Police Magistrate John Fielding, head of the Bow Street Runners, argued that \"...it is much better to prevent even one man from being a rogue than apprehending and bringing forty to justice.\"\nThe Utilitarian philosopher, Jeremy Bentham, promoted the views of Italian Marquis Cesare Beccaria, and disseminated a translated version of \"Essay on Crime in Punishment\". Bentham espoused the guiding principle of \"the greatest good for the greatest number\":\nIt is better to prevent crimes than to punish them. This is the chief aim of every good system of legislation, which is the art of leading men to the greatest possible happiness or to the least possible misery, according to calculation of all the goods and evils of life.\nPatrick Colquhoun's influential work, \"A Treatise on the Police of the Metropolis\" (1797) was heavily influenced by Benthamite thought. Colquhoun's Thames River Police was founded on these principles, and in contrast to the Bow Street Runners, acted as a deterrent by their continual presence on the riverfront, in addition to being able to intervene if they spotted a crime in progress.\nEdwin Chadwick's 1829 article, \"Preventive police\" in the \"London Review\", argued that prevention ought to be the \"primary\" concern of a police body, which was not the case in practice. The reason, argued Chadwick, was that \"A preventive police would act more immediately by placing difficulties in obtaining the objects of temptation.\" In contrast to a deterrent of punishment, a preventive police force would deter criminality by making crime cost-ineffective \u2013 \"crime doesn't pay\". In the second draft of his 1829 Police Act, the \"object\" of the new Metropolitan Police, was changed by Robert Peel to the \"principal object,\" which was the \"prevention of crime.\" Later historians would attribute the perception of England's \"appearance of orderliness and love of public order\" to the preventive principle entrenched in Peel's police system.\nDevelopment of modern police forces around the world was contemporary to the formation of the state, later defined by sociologist Max Weber as achieving a \"monopoly on the legitimate use of physical force\" and which was primarily exercised by the police and the military. Marxist theory situates the development of the modern state as part of the rise of capitalism, in which the police are one component of the bourgeoisie's repressive apparatus for subjugating the working class. By contrast, the Peelian principles argue that \"the power of the police ... is dependent on public approval of their existence, actions and behavior\", a philosophy known as policing by consent.\nPersonnel and organization.\nPolice forces include both preventive (uniformed) police and detectives. Terminology varies from country to country. Police functions include protecting life and property, enforcing criminal law, criminal investigations, regulating traffic, crowd control, public safety duties, civil defense, emergency management, searching for missing persons, lost property and other duties concerned with public order. Regardless of size, police forces are generally organized as a hierarchy with multiple ranks. The exact structures and the names of rank vary considerably by country.\nUniformed.\nThe police who wear uniforms make up the majority of a police service's personnel. Their main duty is to respond to calls for service. When not responding to these calls, they do work aimed at preventing crime, such as patrols. The uniformed police are known by varying names such as preventive police, the uniform branch/division, administrative police, order police, the patrol bureau/division, or patrol. In Australia and the United Kingdom, patrol personnel are also known as \"general duties\" officers. Atypically, Brazil's preventive police are known as Military Police.\nAs stated by the name, uniformed police wear uniforms. They perform functions that require an immediate recognition of an officer's legal authority and a potential need for force. Most commonly this means intervening to stop a crime in progress and securing the scene of a crime that has already happened. Besides dealing with crime, these officers may also manage and monitor traffic, carry out community policing duties, maintain order at public events or carry out searches for missing people (in 2012, the latter accounted for 14% of police time in the United Kingdom). As most of these duties must be available as a 24/7 service, uniformed police are required to do shift work.\nDetectives.\nPolice detectives are responsible for investigations and detective work. Detectives may be called Investigations Police, Judiciary/Judicial Police, or Criminal Police. In the United Kingdom, they are often referred to by the name of their department, the Criminal Investigation Department. Detectives typically make up roughly 15\u201325% of a police service's personnel.\nDetectives, in contrast to uniformed police, typically wear business-styled attire in bureaucratic and investigative functions, where a uniformed presence would be either a distraction or intimidating but a need to establish police authority still exists. \"Plainclothes\" officers dress in attire consistent with that worn by the general public for purposes of blending in.\nIn some cases, police are assigned to work \"undercover\", where they conceal their police identity to investigate crimes, such as organized crime or narcotics crime, that are unsolvable by other means. In some cases, this type of policing shares aspects with espionage.\nThe relationship between detective and uniformed branches varies by country. In the United States, there is high variation within the country itself. Many American police departments require detectives to spend some time on temporary assignments in the patrol division. The argument is that rotating officers helps the detectives to better understand the uniformed officers' work, to promote cross-training in a wider variety of skills, and prevent \"cliques\" that can contribute to corruption or other unethical behavior. Conversely, some countries regard detective work as being an entirely separate profession, with detectives working in separate agencies and recruited without having to serve in uniform. A common compromise in English-speaking countries is that most detectives are recruited from the uniformed branch, but once qualified they tend to spend the rest of their careers in the detective branch.\nAnother point of variation is whether detectives have extra status. In some forces, such as the New York Police Department and Philadelphia Police Department, a regular detective holds a higher rank than a regular police officer. In others, such as British police and Canadian police, a regular detective has equal status with regular uniformed officers. Officers still have to take exams to move to the detective branch, but the move is regarded as being a specialization, rather than a promotion.\nVolunteers and auxiliary.\nPolice services often include part-time or volunteer officers, some of whom have other jobs outside policing. These may be paid positions or entirely volunteer. These are known by a variety of names, such as reserves, auxiliary police or special constables.\nOther volunteer organizations work with the police and perform some of their duties. Groups in the U.S. including the Retired and Senior Volunteer Program, Community Emergency Response Team, and the Boy Scouts Police Explorers provide training, traffic and crowd control, disaster response, and other policing duties. In the U.S., the Volunteers in Police Service program assists over 200,000 volunteers in almost 2,000 programs. Volunteers may also work on the support staff. Examples of these schemes are Volunteers in Police Service in the US, Police Support Volunteers in the UK and Volunteers in Policing in New South Wales.\nSpecialized.\nSpecialized preventive and detective groups, or Specialist Investigation Departments, exist within many law enforcement organizations either for dealing with particular types of crime, such as traffic law enforcement, K9/use of police dogs, crash investigation, homicide, or fraud; or for situations requiring specialized skills, such as underwater search, aviation, explosive disposal (\"bomb squad\"), and computer crime.\nMost larger jurisdictions employ police tactical units, specially selected and trained paramilitary units with specialized equipment, weapons, and training, for the purposes of dealing with particularly violent situations beyond the capability of a patrol officer response, including standoffs, counterterrorism, and rescue operations.\nIn counterinsurgency-type campaigns, select and specially trained units of police armed and equipped as light infantry have been designated as police field forces who perform paramilitary-type patrols and ambushes whilst retaining their police powers in areas that were highly dangerous.\nBecause their situational mandate typically focuses on removing innocent bystanders from dangerous people and dangerous situations, not violent resolution, they are often equipped with non-lethal tactical tools like chemical agents, stun grenades, and rubber bullets. The Specialist Firearms Command (MO19) of the Metropolitan Police in London is a group of armed police used in dangerous situations including hostage taking, armed robbery/assault and terrorism.\nAdministrative duties.\nPolice may have administrative duties that are not directly related to enforcing the law, such as issuing firearms licenses. The extent that police have these functions varies among countries, with police in France, Germany, and other continental European countries handling such tasks to a greater extent than British counterparts.\nMilitary.\nMilitary police may refer to:\nReligious.\nSome jurisdictions with religious laws may have dedicated religious police to enforce said laws. These religious police forces, which may operate either as a unit of a wider police force or as an independent agency, may only have jurisdiction over members of said religion, or they may have the ability to enforce religious customs nationwide regardless of individual religious beliefs.\nReligious police may enforce social norms, gender roles, dress codes, and dietary laws per religious doctrine and laws, and may also prohibit practices that run contrary to said doctrine, such as atheism, proselytism, homosexuality, socialization between different genders, business operations during religious periods or events such as salah or the Sabbath, or the sale and possession of \"offending material\" ranging from pornography to foreign media.\nForms of religious law enforcement were relatively common in historical religious civilizations, but eventually declined in favor of religious tolerance and pluralism. One of the most common forms of religious police in the modern world are Islamic religious police, which enforce the application of Sharia (Islamic religious law). As of 2018, there are eight Islamic countries that maintain Islamic religious police: Afghanistan, Iran, Iraq, Mauritania, Pakistan, Saudi Arabia, Sudan, and Yemen.\nSome forms of religious police may not enforce religious law, but rather suppress religion or religious extremism. This is often done for ideological reasons; for example, communist states such as China and Vietnam have historically suppressed and tightly controlled religions such as Christianity.\nSecret.\nSecret police organizations are typically used to suppress dissidents for engaging in non-politically correct communications and activities, which are deemed counter-productive to what the state and related establishment promote. Secret police interventions to stop such activities are often illegal, and are designed to debilitate, in various ways, the people targeted in order to limit or stop outright their ability to act in a non-politically correct manner. The methods employed may involve spying, various acts of deception, intimidation, framing, false imprisonment, false incarceration under mental health legislation, and physical violence. Countries widely reported to use secret police organizations include China (The Ministry of State Security) and North Korea (The Ministry of State Security).\nBy country.\nPolice forces are usually organized and funded by some level of government. The level of government responsible for policing varies from place to place, and may be at the national, regional or local level. Some countries have police forces that serve the same territory, with their jurisdiction depending on the type of crime or other circumstances. Other countries, such as Austria, Chile, Israel, New Zealand, the Philippines, South Africa and Sweden, have a single national police force.\nIn some places with multiple national police forces, one common arrangement is to have a civilian police force and a paramilitary gendarmerie, such as the Police Nationale and National Gendarmerie in France. The French policing system spread to other countries through the Napoleonic Wars and the French colonial empire. Another example is the Polic\u00eda Nacional and Guardia Civil in Spain. In both France and Spain, the civilian force polices urban areas and the paramilitary force polices rural areas. Italy has a similar arrangement with the Polizia di Stato and Carabinieri, though their jurisdictions overlap more. Some countries have separate agencies for uniformed police and detectives, such as the Military Police and Civil Police in Brazil and the Carabineros and Investigations Police in Chile.\nOther countries have sub-national police forces, but for the most part their jurisdictions do not overlap. In many countries, especially federations, there may be two or more tiers of police force, each serving different levels of government and enforcing different subsets of the law. In Australia and Germany, the majority of policing is carried out by state (i.e. provincial) police forces, which are supplemented by a federal police force. Though not a federation, the United Kingdom has a similar arrangement, where policing is primarily the responsibility of a regional police force and specialist units exist at the national level. In Canada, the Royal Canadian Mounted Police (RCMP) are the federal police, while municipalities can decide whether to run a local police service or to contract local policing duties to a larger one. Most urban areas have a local police service, while most rural areas contract it to the RCMP, or to the provincial police in Ontario and Quebec.\nThe United States has a highly decentralized and fragmented system of law enforcement, with over 17,000 state and local law enforcement agencies. These agencies include local police, county law enforcement (often in the form of a sheriff's office, or county police), state police and federal law enforcement agencies. Federal agencies, such as the FBI, only have jurisdiction over federal crimes or those that involve more than one state. Other federal agencies have jurisdiction over a specific type of crime. Examples include the Federal Protective Service, which patrols and protects government buildings; the Postal Inspection Service, which protect United States Postal Service facilities, vehicles and items; the Park Police, which protect national parks; and Amtrak Police, which patrol Amtrak stations and trains. There are also some government agencies and uniformed services that perform police functions in addition to other duties, such as the Coast Guard.\nInternational.\nMost countries are members of the International Criminal Police Organization (Interpol), established to detect and fight transnational crime and provide for international co-operation and co-ordination of other police activities, such as notifying relatives of the death of foreign nationals. Interpol does not conduct investigations or arrests by itself, but only serves as a central point for information on crime, suspects and criminals. Political crimes are excluded from its competencies.\nThe terms international policing, transnational policing, and/or global policing began to be used from the early 1990s onwards to describe forms of policing that transcended the boundaries of the sovereign nation-state. These terms refer in variable ways to practices and forms for policing that, in some sense, transcend national borders. This includes a variety of practices, but international police cooperation, criminal intelligence exchange between police agencies working in different nation-states, and police development-aid to weak, failed or failing states are the three types that have received the most scholarly attention.\nHistorical studies reveal that policing agents have undertaken a variety of cross-border police missions for many years. For example, in the 19th century a number of European policing agencies undertook cross-border surveillance because of concerns about anarchist agitators and other political radicals. A notable example of this was the occasional surveillance by Prussian police of Karl Marx during the years he remained resident in London. The interests of public police agencies in cross-border co-operation in the control of political radicalism and ordinary law crime were primarily initiated in Europe, which eventually led to the establishment of Interpol before World War II. There are also many interesting examples of cross-border policing under private auspices and by municipal police forces that date back to the 19th century. It has been established that modern policing has transgressed national boundaries from time to time almost from its inception. It is also generally agreed that in the post\u2013Cold War era this type of practice became more significant and frequent.\nFew empirical works on the practices of inter/transnational information and intelligence sharing have been undertaken. A notable exception is James Sheptycki's study of police cooperation in the English Channel region, which provides a systematic content analysis of information exchange files and a description of how these transnational information and intelligence exchanges are transformed into police casework. The study showed that transnational police information sharing was routinized in the cross-Channel region from 1968 on the basis of agreements directly between the police agencies and without any formal agreement between the countries concerned. By 1992, with the signing of the Schengen Treaty, which formalized aspects of police information exchange across the territory of the European Union, there were worries that much, if not all, of this intelligence sharing was opaque, raising questions about the efficacy of the accountability mechanisms governing police information sharing in Europe.\nStudies of this kind outside of Europe are even rarer, so it is difficult to make generalizations, but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in North America and Europe confirmed that the low visibility of police information and intelligence sharing was a common feature. Intelligence-led policing is now common practice in most advanced countries and it is likely that police intelligence sharing and information exchange has a common morphology around the world. James Sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of \"organizational pathologies\" have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic. He argues that transnational police information circuits help to \"compose the panic scenes of the security-control society\". The paradoxical effect is that, the harder policing agencies work to produce security, the greater are feelings of insecurity.\nPolice development-aid to weak, failed or failing states is another form of transnational policing that has garnered attention. This form of transnational policing plays an increasingly important role in United Nations peacekeeping and this looks set to grow in the years ahead, especially as the international community seeks to develop the rule of law and reform security institutions in states recovering from conflict. With transnational police development-aid the imbalances of power between donors and recipients are stark and there are questions about the applicability and transportability of policing models between jurisdictions.\nOne topic concerns making transnational policing institutions democratically accountable. According to the Global Accountability Report for 2007, Interpol had the lowest scores in its category (IGOs), coming in tenth with a score of 22% on overall accountability capabilities.\nOverseas policing.\nA police force may establish its presence in a foreign country with or without the permission of the host state. In the case of China and the ruling Communist Party, this has involved setting up unofficial police service stations around the world, and using coercive means to influence the behaviour of members of the Chinese diaspora and especially those who hold Chinese citizenship. Political dissidents have been harassed and intimidated in a form of transnational repression and convinced to return to China. Many of these actions were illegal in the states where they occurred. Such police stations have been established in dozens of countries around the world, with some, such as the UK and the US, forcing them to close.\nEquipment.\nWeapons.\nIn many jurisdictions, police officers carry firearms, primarily handguns, in the normal course of their duties. In the United Kingdom (except Northern Ireland), Iceland, Ireland, New Zealand, Norway, and Malta, with the exception of specialist units, officers do not carry firearms as a matter of course. New Zealand and Norwegian police carry firearms in their vehicles, but not on their duty belts, and must obtain authorization before the weapons can be removed from the vehicle unless their life or the life of others are in danger.\nPolice often have specialized units for handling armed offenders or dangerous situations where combat is likely, such as police tactical units or authorised firearms officers. In some jurisdictions, depending on the circumstances, police can call on the military for assistance, as military aid to the civil power is an aspect of many armed forces. Perhaps the most high-profile example of this was in 1980, when the British Army's Special Air Service was deployed to resolve the Iranian Embassy siege on behalf of the Metropolitan Police.\nThey can also be armed with \"non-lethal\" (more accurately known as \"less than lethal\" or \"less-lethal\" given that they can still be deadly) weaponry, particularly for riot control, or to inflict pain against a resistant suspect to force them to surrender without lethally wounding them. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons, and electroshock weapons. Police officers typically carry handcuffs to restrain suspects.\nThe use of firearms or deadly force is typically a last resort only to be used when necessary to save the lives of others or themselves, though some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. Police officers in the United States are generally allowed to use deadly force if they believe their life is in danger, a policy that has been criticized for being vague. South African police have a \"shoot-to-kill\" policy, which allows officers to use deadly force against any person who poses a significant threat to them. With the country having one of the highest rates of violent crime, President Jacob Zuma stated that South Africa needs to handle crime differently from other countries.\nCommunications.\nModern police forces make extensive use of two-way radio communications equipment, carried both on the person and installed in vehicles, to coordinate their work, share information, and get help quickly. Vehicle-installed mobile data terminals enhance the ability of police communications, enabling easier dispatching of calls, criminal background checks on persons of interest to be completed in a matter of seconds, and updating officers' daily activity log and other required reports, on a real-time basis. Other common pieces of police equipment include flashlights, whistles, police notebooks, and \"ticket books\" or citations. Some police departments have developed advanced computerized data display and communication systems to bring real time data to officers, one example being the NYPD's Domain Awareness System.\nVehicles.\nPolice vehicles are used for detaining, patrolling, and transporting over wide areas that an officer could not effectively cover otherwise. The average police car used for standard patrol is a four-door sedan, SUV, or CUV, often modified by the manufacturer or police force's fleet services to provide better performance. Pickup trucks, off-road vehicles, and vans are often used in utility roles, though in some jurisdictions or situations (such as those where dirt roads are common, off-roading is required, or the nature of the officer's assignment necessitates it), they may be used as standard patrol cars. Sports cars are typically not used by police due to cost and maintenance issues, though those that are used are typically only assigned to traffic enforcement or community policing, and are rarely, if ever, assigned to standard patrol or authorized to respond to dangerous calls (such as armed calls or pursuits) where the likelihood of the vehicle being damaged or destroyed is high. Police vehicles are usually marked with appropriate symbols and equipped with sirens and flashing emergency lights to make others aware of police presence or response; in most jurisdictions, police vehicles with their sirens and emergency lights on have right of way in traffic, while in other jurisdictions, emergency lights may be kept on while patrolling to ensure ease of visibility. Unmarked or undercover police vehicles are used primarily for traffic enforcement or apprehending criminals without alerting them to their presence. The use of unmarked police vehicles for traffic enforcement is controversial, with the state of New York banning this practice in 1996 on the grounds that it endangered motorists who might be pulled over by police impersonators.\nMotorcycles, having historically been a mainstay in police fleets, are commonly used, particularly in locations that a car may not be able to reach, to control potential public order situations involving meetings of motorcyclists, and often in police escorts where motorcycle police officers can quickly clear a path for escorted vehicles. Bicycle patrols are used in some areas, often downtown areas or parks, because they allow for wider and faster area coverage than officers on foot. Bicycles are also commonly used by riot police to create makeshift barricades against protesters.\nPolice aviation consists of helicopters and fixed-wing aircraft, while police watercraft tend to consist of RHIBs, motorboats, and patrol boats. SWAT vehicles are used by police tactical units, and often consist of four-wheeled armored personnel carriers used to transport tactical teams while providing armored cover, equipment storage space, or makeshift battering ram capabilities; these vehicles are typically not armed and do not patrol and are only used to transport. Mobile command posts may also be used by some police forces to establish identifiable command centers at the scene of major situations.\nPolice cars may contain issued long guns, ammunition for issued weapons, less-lethal weaponry, riot control equipment, traffic cones, road flares, physical barricades or barricade tape, fire extinguishers, first aid kits, or defibrillators.\nStrategies.\nThe advent of the police car, two-way radio, and telephone in the early 20th century transformed policing into a reactive strategy that focused on responding to calls for service away from their beat. With this transformation, police command and control became more centralized.\nIn the United States, August Vollmer introduced other reforms, including education requirements for police officers. O.W. Wilson, a student of Vollmer, helped reduce corruption and introduce professionalism in Wichita, Kansas, and later in the Chicago Police Department. Strategies employed by O.W. Wilson included rotating officers from community to community to reduce their vulnerability to corruption, establishing of a non-partisan police board to help govern the police force, a strict merit system for promotions within the department, and an aggressive recruiting drive with higher police salaries to attract professionally qualified officers. During the professionalism era of policing, law enforcement agencies concentrated on dealing with felonies and other serious crime and conducting visible car patrols in between, rather than broader focus on crime prevention.\nThe Kansas City Preventive Patrol study in the early 1970s showed flaws in using visible car patrols for crime prevention. It found that aimless car patrols did little to deter crime and often went unnoticed by the public. Patrol officers in cars had insufficient contact and interaction with the community, leading to a social rift between the two. In the 1980s and 1990s, many law enforcement agencies began to adopt community policing strategies, and others adopted problem-oriented policing.\nBroken windows' policing was another, related approach introduced in the 1980s by James Q. Wilson and George L. Kelling, who suggested that police should pay greater attention to minor \"quality of life\" offenses and disorderly conduct. The concept behind this method is simple: broken windows, graffiti, and other physical destruction or degradation of property create an environment in which crime and disorder is more likely. The presence of broken windows and graffiti sends a message that authorities do not care and are not trying to correct problems in these areas. Therefore, correcting these small problems prevents more serious criminal activity. The theory was popularised in the early 1990s by police chief William J. Bratton and New York City Mayor Rudy Giuliani. It was emulated in 2010s in Kazakhstan through zero tolerance policing. Yet it failed to produce meaningful results in this country because citizens distrusted police while state leaders preferred police loyalty over police good behavior.\nBuilding upon these earlier models, intelligence-led policing has also become an important strategy. Intelligence-led policing and problem-oriented policing are complementary strategies, both of which involve systematic use of information. Although it still lacks a universally accepted definition, the crux of intelligence-led policing is an emphasis on the collection and analysis of information to guide police operations, rather than the reverse.\nA related development is evidence-based policing. In a similar vein to evidence-based policy, evidence-based policing is the use of controlled experiments to find which methods of policing are more effective. Leading advocates of evidence-based policing include the criminologist Lawrence W. Sherman and philanthropist Jerry Lee. Findings from controlled experiments include the Minneapolis Domestic Violence Experiment, evidence that patrols deter crime if they are concentrated in crime hotspots and that restricting police powers to shoot suspects does not cause an increase in crime or violence against police officers. A 2013 study found policing decreases crime through deterrence with an elasticity of roughly -0.5. Use of experiments to assess the usefulness of strategies has been endorsed by many police services and institutions, including the U.S. Police Foundation and the UK College of Policing.\nPower restrictions and accountability.\nIn many nations, criminal procedure law has been developed to regulate officers' discretion, so that they do not arbitrarily or unjustly exercise their powers of arrest, search and seizure, and use of force. In the United States, \"Miranda v. Arizona\" led to the widespread use of Miranda warnings or constitutional warnings.\nPolice forces also find themselves under criticism for their use of force, particularly deadly force.\nIn \"Miranda\" the court created safeguards against self-incriminating statements made after an arrest. The court held that \"The prosecution may not use statements, whether exculpatory or inculpatory, stemming from questioning initiated by law enforcement officers after a person has been taken into custody or otherwise deprived of his freedom of action in any significant way, unless it demonstrates the use of procedural safeguards effective to secure the Fifth Amendment's privilege against self-incrimination\"\nPolice in the United States are also prohibited from holding criminal suspects for more than a reasonable amount of time (usually 24\u201348 hours) before arraignment, using torture, abuse or physical threats to extract confessions, using excessive force to effect an arrest, and searching suspects' bodies or their homes without a warrant obtained upon a showing of probable cause. The four exceptions to the constitutional requirement of a search warrant are:\nIn \"Terry v. Ohio\" (1968) the court divided seizure into two parts, the investigatory stop and arrest. The court further held that during an investigatory stop a police officer's search \" [is] confined to what [is] minimally necessary to determine whether [a suspect] is armed, and the intrusion, which [is] made for the sole purpose of protecting himself and others nearby, [is] confined to ascertaining the presence of weapons\" (U.S. Supreme Court). Before Terry, every police encounter constituted an arrest, giving the police officer the full range of search authority. Search authority during a Terry stop (investigatory stop) is limited to weapons only.\nUsing deception for confessions is permitted, but not coercion. There are exceptions or exigent circumstances such as an articulated need to disarm a suspect or searching a suspect who has already been arrested (Search Incident to an Arrest). The Posse Comitatus Act severely restricts the use of the military for police activity, giving added importance to police SWAT units.\nBritish police officers are governed by similar rules, such as those introduced to England and Wales under the Police and Criminal Evidence Act 1984 (PACE), but generally have greater powers. They may, for example, legally search any suspect who has been arrested, or their vehicles, home or business premises, without a warrant, and may seize anything they find in a search as evidence.\nAll police officers in the United Kingdom, whatever their actual rank, are 'constables' in terms of their legal position. This means that a newly appointed constable has the same arrest powers as a Chief Constable or Commissioner. However, certain higher ranks have additional powers to authorize certain aspects of police operations, such as a power to authorize a search of a suspect's house (section 18 PACE in England and Wales) by an officer of the rank of Inspector, or the power to authorize a suspect's detention beyond 24 hours by a Superintendent.\nAccountability.\nPolice services commonly include units for investigating crimes committed by the police themselves. These units are typically called internal affairs or inspectorate-general units. In some countries separate organizations outside the police exist for such purposes, such as the British Independent Office for Police Conduct. In the United States, due to American laws around qualified immunity, it has become increasingly difficult to investigate and charge police misconduct and crimes.\nLikewise, some state and local jurisdictions, for example, Springfield, Illinois have similar outside review organizations. The Police Service of Northern Ireland is investigated by the Police Ombudsman for Northern Ireland, an external agency set up as a result of the Patten report into policing the province. In the Republic of Ireland, the Garda S\u00edoch\u00e1na is investigated by Fiosr\u00fa \u2013 the Office of the Police Ombudsman, a body founded as Garda S\u00edoch\u00e1na Ombudsman Commission to replace the Garda Complaints Board in May 2007, and reorganised under its new title in 2025.\nThe Special Investigations Unit of Ontario, Canada, is one of only a few civilian agencies around the world responsible for investigating circumstances involving police and others that have resulted in a death, serious injury, or allegations of sexual assault. The agency has made allegations of insufficient cooperation from various police services hindering their investigations.\nIn Hong Kong, any allegations of corruption within the police are investigated by the Independent Commission Against Corruption and the Independent Police Complaints Council, two agencies which are independent of the police force.\nPolice body cameras are often worn by police officers to record their interactions with the public and each other, providing audiovisual recorded evidence for review in the event an officer or agency's actions are investigated.\nSelective enforcement.\nTensions can increase when a police officer of one ethnic group harms or kills a suspect of another one. In the United States, such events occasionally spark protests and accusations of racism against police and allegations that police departments practice racial profiling. Similar incidents have also happened in other countries.\nIn the United States since the 1960s, concern over such issues has increasingly weighed upon law enforcement agencies, courts and legislatures at every level of government. Incidents such as the 1965 Watts riots, the videotaped 1991 beating by LAPD officers of Rodney King, and the riot following their acquittal have been suggested by some people to be evidence that U.S. police are dangerously lacking in appropriate controls.\nThe fact that this trend has occurred contemporaneously with the rise of the civil rights movement, the \"war on drugs\", and a precipitous rise in violent crime from the 1960s to the 1990s has made questions surrounding the role, administration and scope of police authority increasingly complicated.\nPolice departments and the local governments that oversee them in some jurisdictions have attempted to mitigate some of these issues through community outreach programs and community policing to make the police more accessible to the concerns of local communities, by working to increase hiring diversity, by updating training of police in their responsibilities to the community and under the law, and by increased oversight within the department or by civilian commissions.\nIn cases in which such measures have been lacking or absent, civil lawsuits have been brought by the United States Department of Justice against local law enforcement agencies, authorized under the 1994 Violent Crime Control and Law Enforcement Act. This has compelled local departments to make organizational changes, enter into consent decree settlements to adopt such measures, and submit to oversight by the Justice Department.\nIn May 2020, a global movement to increase scrutiny of police violence grew in popularity, starting in Minneapolis, Minnesota with the murder of George Floyd. Calls for defunding of the police and full abolition of the police gained larger support in the United States as more criticized systemic racism in policing.\nCritics also argue that sometimes this abuse of force or power can extend to police officer civilian life as well. For example, critics note that women in around 40% of police officer families have experienced domestic violence and that police officers are convicted of misdemeanors and felonies at a rate of more than six times higher than concealed carry weapon permit holders.\nProtection of individuals.\nThe Supreme Court of the United States has consistently ruled that law enforcement officers in the U.S. have no duty to protect any individual, only to enforce rule of law. This is despite the motto of many police departments in the U.S. being a variation of \"protect and serve\"; regardless, many departments generally expect their officers to protect individuals. The first case to make such a ruling was \"South v. State of Maryland\" in 1855, and the most recent was \"Town of Castle Rock v. Gonzales\" in 2005.\nIn contrast, the police are entitled to protect private rights in some jurisdictions. To ensure that the police would not interfere in the regular competencies of the courts of law, some police acts require that the police may only interfere in such cases where protection from courts cannot be obtained in time, and where, without interference of the police, the realization of the private right would be impeded. This would, for example, allow police to establish a restaurant guest's identity and forward it to the innkeeper in a case where the guest cannot pay the bill at nighttime because his wallet had just been stolen from the restaurant table.\nIn addition, there are federal law enforcement agencies in the United States whose mission includes providing protection for executives such as the president and accompanying family members, visiting foreign dignitaries, and other high-ranking individuals. Such agencies include the U.S. Secret Service and the U.S. Park Police.\nHistory.\nAncient.\nChina.\nLaw enforcement in ancient China was carried out by \"prefects\" for thousands of years since it developed in both the Chu and Jin kingdoms of the Spring and Autumn period. In Jin, dozens of prefects were spread across the state, each having limited authority and employment period. They were appointed by local magistrates, who reported to higher authorities such as governors, who in turn were appointed by the emperor, and they oversaw the civil administration of their \"prefecture\", or jurisdiction. Under each prefect were \"subprefects\" who helped collectively with law enforcement in the area. Some prefects were responsible for handling investigations, much like modern police detectives. Prefects could also be women. Local citizens could report minor judicial offenses against them such as robberies at a local prefectural office. The concept of the \"prefecture system\" spread to other cultures such as Korea and Japan.\nBabylonia.\nIn Babylonia, law enforcement tasks were initially entrusted to individuals with military backgrounds or imperial magnates during the Old Babylonian period, but eventually, law enforcement was delegated to officers known as , who were present in both cities and rural settlements. A was responsible for investigating petty crimes and carrying out arrests.\nEgypt.\nIn ancient Egypt evidence of law enforcement exists as far back as the Old Kingdom period. There are records of an office known as \"Judge Commandant of the Police\" dating to the fourth dynasty. During the fifth dynasty at the end of the Old Kingdom period, warriors armed with wooden sticks were tasked with guarding public places such as markets, temples, and parks, and apprehending criminals. They are known to have made use of trained monkeys, baboons, and dogs in guard duties and catching criminals. After the Old Kingdom collapsed, ushering in the First Intermediate Period, it is thought that the same model applied. During this period, Bedouins were hired to guard the borders and protect trade caravans. During the Middle Kingdom period, a professional police force was created with a specific focus on enforcing the law, as opposed to the previous informal arrangement of using warriors as police. The police force was further reformed during the New Kingdom period. Police officers served as interrogators, prosecutors, and court bailiffs, and were responsible for administering punishments handed down by judges. In addition, there were special units of police officers trained as priests who were responsible for guarding temples and tombs and preventing inappropriate behavior at festivals or improper observation of religious rites during services. Other police units were tasked with guarding caravans, guarding border crossings, protecting royal necropolises, guarding slaves at work or during transport, patrolling the Nile River, and guarding administrative buildings. By the Eighteenth Dynasty of the New Kingdom period, an elite desert-ranger police force called the Medjay was used to protect valuable areas, especially areas of pharaonic interest like capital cities, royal cemeteries, and the borders of Egypt. Though they are best known for their protection of the royal palaces and tombs in Thebes and the surrounding areas, the Medjay were used throughout Upper and Lower Egypt. Each regional unit had its own captain. The police forces of ancient Egypt did not guard rural communities, which often took care of their own judicial problems by appealing to village elders, but many of them had a constable to enforce state laws.\nGreece.\nIn ancient Greece, publicly owned slaves were used by magistrates as police. In Athens, the Scythian Archers (the 'rod-bearers'), a group of about 300 Scythian slaves, was used to guard public meetings to keep order and for crowd control, and also assisted with dealing with criminals, handling prisoners, and making arrests. Other duties associated with modern policing, such as investigating crimes, were left to the citizens themselves. Athenian police forces were supervised by the Areopagus. In Sparta, the Ephors were in charge of maintaining public order as judges, and they used Sparta's Hippeis, a 300-member Royal guard of honor, as their enforcers. There were separate authorities supervising women, children, and agricultural issues. Sparta also had a secret police force called the crypteia to watch the large population of helots, or slaves.\nRome.\nIn the Roman Empire, the army played a major role in providing security. Roman soldiers detached from their legions and posted among civilians carried out law enforcement tasks. The Praetorian Guard, an elite army unit which was primarily an Imperial bodyguard and intelligence-gathering unit, could also act as a riot police force if required. Local watchmen were hired by cities to provide some extra security. Lictors, civil servants whose primary duty was to act as bodyguards to magistrates who held \"imperium\", could carry out arrests and inflict punishments at their magistrate's command. Magistrates such as \"tresviri capitales\", and investigated crimes. There was no concept of public prosecution, so victims of crime or their families had to organize and manage the prosecution themselves. Under the reign of Augustus, when the capital had grown to almost one million inhabitants, 14 wards were created; the wards were protected by seven squads of 1,000 men called , who acted as night watchmen and firemen. In addition to firefighting, their duties included apprehending petty criminals, capturing runaway slaves, guarding the baths at night, and stopping disturbances of the peace. As well as the city of Rome, \"vigiles\" were also stationed in the harbor cities of Ostia and Portus. Augustus also formed the Urban Cohorts to deal with gangs and civil disturbances in the city of Rome, and as a counterbalance to the Praetorian Guard's enormous power in the city. They were led by the urban prefect. Urban Cohort units were later formed in Roman Carthage and Lugdunum.\nIndia.\nLaw enforcement systems existed in the various kingdoms and empires of ancient India. The Apastamba Dharmasutra prescribes that kings should appoint officers and subordinates in the towns and villages to protect their subjects from crime. Various inscriptions and literature from ancient India suggest that a variety of roles existed for law enforcement officials such as those of a constable, thief catcher, watchman, and detective. In ancient India up to medieval and early modern times, kotwals were in charge of local law enforcement.\nAchaemenid (First Persian) Empire.\nThe Achaemenid Empire had well-organized police forces. A police force existed in every place of importance. In the cities, each ward was under the command of a Superintendent of Police, known as a . Police officers also acted as prosecutors and carried out punishments imposed by the courts. They were required to know the court procedure for prosecuting cases and advancing accusations.\nIsrael.\nIn ancient Israel and Judah, officials with the responsibility of making declarations to the people, guarding the king's person, supervising public works, and executing the orders of the courts existed in the urban areas. They are repeatedly mentioned in the Hebrew Bible, and this system lasted into the period of Roman rule. The first century Jewish historian Josephus related that every judge had two such officers under his command. Levites were preferred for this role. Cities and towns also had night watchmen. Besides officers of the town, there were officers for every tribe. The temple in Jerusalem was protected by a special temple guard. The Talmud mentions various local officials in the Jewish communities of the Land of Israel and Babylon who supervised economic activity. Their Greek-sounding titles suggest that the roles were introduced under Hellenic influence. Most of these officials received their authority from local courts and their salaries were drawn from the town treasury. The Talmud also mentions city watchmen and mounted and armed watchmen in the suburbs.\nAfrica.\nIn many regions of pre-colonial Africa, particularly West and Central Africa, guild-like secret societies emerged as law enforcement. In the absence of a court system or written legal code, they carried out police-like activities, employing varying degrees of coercion to enforce conformity and deter antisocial behavior. In ancient Ethiopia, armed retainers of the nobility enforced law in the countryside according to the will of their leaders. The Songhai Empire had officials known as \"assara-munidios\", or \"enforcers\", acting as police.\nThe Americas.\nPre-Columbian civilizations in the Americas also had organized law enforcement. The city-states of the Maya civilization had constables known as . In the Aztec Empire, judges had officers serving under them who were empowered to perform arrests, even of dignitaries. Aztec markets were patrolled by commissioners to prevent fraud and disorder. In the Inca Empire, officials called kuraka enforced the law among the households they were assigned to oversee, with inspectors known as (lit.\u2009'he who sees all') also stationed throughout the provinces to keep order.\nPost-classical.\nIn medieval Spain, , or 'holy brotherhoods', peacekeeping associations of armed individuals, were a characteristic of municipal life, especially in Castile. As medieval Spanish kings often could not offer adequate protection, protective municipal leagues began to emerge in the twelfth century against banditry and other rural criminals, and against the lawless nobility or to support one or another claimant to a crown.\nThese organizations were intended to be temporary, but became a long-standing fixture of Spain. The first recorded case of the formation of an occurred when the towns and the peasantry of the north united to police the pilgrim road to Santiago de Compostela in Galicia, and protect the pilgrims against robber knights.\nThroughout the Middle Ages such alliances were frequently formed by combinations of towns to protect the roads connecting them, and were occasionally extended to political purposes. Among the most powerful was the league of North Castilian and Basque ports, the Hermandad de las marismas: Toledo, Talavera, and Villarreal.\nAs one of their first acts after end of the War of the Castilian Succession in 1479, Ferdinand\u00a0II of Aragon and Isabella\u00a0I of Castile established the centrally-organized and efficient \"Holy Brotherhood\" as a national police force. They adapted an existing brotherhood to the purpose of a general police acting under officials appointed by themselves, and endowed with great powers of summary jurisdiction even in capital cases. The original brotherhoods continued to serve as modest local police-units until their final suppression in 1835.\nThe Vehmic courts of Germany provided some policing in the absence of strong state institutions. Such courts had a chairman who presided over a session and lay judges who passed judgement and carried out law enforcement tasks. Among the responsibilities that lay judges had were giving formal warnings to known troublemakers, issuing warrants, and carrying out executions.\nIn the medieval Islamic Caliphates, police were known as . Bodies termed existed perhaps as early as the Rashidun Caliphate during the reign of Uthman. The \"Shurta\" is known to have existed in the Abbasid and Umayyad Caliphates. Their primary roles were to act as police and internal security forces but they could also be used for other duties such as customs and tax enforcement, rubbish collection, and acting as bodyguards for governors. From the 10th century, the importance of the \"Shurta\" declined as the army assumed internal security tasks while cities became more autonomous and handled their own policing needs locally, such as by hiring watchmen. In addition, officials called were responsible for supervising bazaars and economic activity in general in the medieval Islamic world.\nIn France during the Middle Ages, there were two Great Officers of the Crown of France with police responsibilities: The Marshal of France and the Grand Constable of France. The military policing responsibilities of the Marshal of France were delegated to the Marshal's provost, whose force was known as the Marshalcy because its authority ultimately derived from the Marshal. The marshalcy dates back to the Hundred Years' War, and some historians trace it back to the early 12th century. Another organisation, the Constabulary (), was under the command of the Constable of France. The constabulary was regularised as a military body in 1337. Under Francis\u00a0I (reigned 1515\u20131547), the was merged with the constabulary. The resulting force was also known as the , or, formally, the Constabulary and Marshalcy of France.\nIn late medieval Italian cities, police forces were known as \"berovierri\". Individually, their members were known as \"birri\". Subordinate to the city's podest\u00e0, the \"berovierri\" were responsible for guarding the cities and their suburbs, patrolling, and the pursuit and arrest of criminals. They were typically hired on short-term contracts, usually six months. Detailed records from medieval Bologna show that \"birri\" had a chain of command, with constables and sergeants managing lower-ranking \"birri\", that they wore uniforms, that they were housed together with other employees of the podest\u00e0 together with a number of servants including cooks and stable-keepers, that their parentage and places of origin were meticulously recorded, and that most were not native to Bologna, with many coming from outside Italy.\nThe English system of maintaining public order since the Norman conquest was a private system of tithings known as the mutual pledge system. This system was introduced under Alfred the Great. Communities were divided into groups of ten families called tithings, each of which was overseen by a chief tithingman. Every household head was responsible for the good behavior of his own family and the good behavior of other members of his tithing. Every male aged 12 and over was required to participate in a tithing. Members of tithings were responsible for raising \"hue and cry\" upon witnessing or learning of a crime, and the men of his tithing were responsible for capturing the criminal. The person the tithing captured would then be brought before the chief tithingman, who would determine guilt or innocence and punishment. All members of the criminal's tithing would be responsible for paying the fine. A group of ten tithings was known as a \"hundred\" and every hundred was overseen by an official known as a reeve. Hundreds ensured that if a criminal escaped to a neighboring village, he could be captured and returned to his village. If a criminal was not apprehended, then the entire hundred could be fined. The hundreds were governed by administrative divisions known as shires, the rough equivalent of a modern county, which were overseen by an official known as a shire-reeve, from which the term sheriff evolved. The shire-reeve had the power of , meaning he could gather the men of his shire to pursue a criminal. Following the Norman conquest of England in 1066, the tithing system was tightened with the frankpledge system. By the end of the 13th century, the office of constable developed. Constables had the same responsibilities as chief tithingmen and additionally as royal officers. The constable was elected by his parish every year. Eventually, constables became the first \"police\" official to be tax-supported. In urban areas, watchmen were tasked with keeping order and enforcing nighttime curfew. Watchmen guarded the town gates at night, patrolled the streets, arrested those on the streets at night without good reason, and also acted as firefighters. Eventually the office of justice of the peace was established, with a justice of the peace overseeing constables. There was also a system of investigative \"juries\".\nThe Assize of Arms of 1252, which required the appointment of constables to summon men to arms, quell breaches of the peace, and to deliver offenders to the sheriff or reeve, is cited as one of the earliest antecedents of the English police. The Statute of Winchester of 1285 is also cited as the primary legislation regulating the policing of the country between the Norman Conquest and the Metropolitan Police Act 1829.\nFrom about 1500, private watchmen were funded by private individuals and organisations to carry out police functions. They were later nicknamed 'Charlies', probably after the reigning monarch King Charles\u00a0II. Thief-takers were also rewarded for catching thieves and returning the stolen property. They were private individuals usually hired by crime victims.\nThe earliest English use of the word \"police\" seems to have been the term \"Polles\" mentioned in the book \"The Second Part of the Institutes of the Lawes of England\" published in 1642.\nEarly modern.\nThe first example of a statutory police force in the world was probably the High Constables of Edinburgh, formed in 1611 to police the streets of Edinburgh, then part of the Kingdom of Scotland. The constables, of whom half were merchants and half were craftsmen, were charged with enforcing 16 regulations relating to curfews, weapons, and theft. At that time, maintenance of public order in Scotland was mainly done by clan chiefs and feudal lords. The first centrally organised and uniformed police force was created by the government of King Louis\u00a0XIV in 1667 to police the city of Paris, then the largest city in Europe. The royal edict, registered by the of Paris on March 15, 1667, created the office of (\"lieutenant general of police\"), who was to be the head of the new Paris police force, and defined the task of the police as \"ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties\".\nThis office was first held by Gabriel Nicolas de la Reynie, who had 44 ('police commissioners') under his authority. In 1709, these commissioners were assisted by ('police inspectors'). The city of Paris was divided into 16 districts policed by the , each assigned to a particular district and assisted by a growing bureaucracy. The scheme of the Paris police force was extended to the rest of France by a royal edict of October 1699, resulting in the creation of lieutenants general of police in all large French cities and towns.\nAfter the French Revolution, Napol\u00e9on I reorganized the police in Paris and other cities with more than 5,000 inhabitants on February 17, 1800, as the Prefecture of Police. On March 12, 1829, a government decree created the first uniformed police in France, known as ('city sergeants'), which the Paris Prefecture of Police's website claims were the first uniformed policemen in the world.\nIn feudal Japan, samurai warriors were charged with enforcing the law among commoners. Some Samurai acted as magistrates called , who acted as judges, prosecutors, and as chief of police. Beneath them were other Samurai serving as , or assistant magistrates, who conducted criminal investigations, and beneath them were Samurai serving as , who were responsible for patrolling the streets, keeping the peace, and making arrests when necessary. The were responsible for managing the . and were typically drawn from low-ranking samurai families. Assisting the were the , non-Samurai who went on patrol with them and provided assistance, the , non-Samurai from the lowest outcast class, often former criminals, who worked for them as informers and spies, and or , ch\u014dnin, often former criminals, who were hired by local residents and merchants to work as police assistants in a particular neighborhood. This system typically did not apply to the Samurai themselves. Samurai clans were expected to resolve disputes among each other through negotiation, or when that failed through duels. Only rarely did Samurai bring their disputes to a magistrate or answer to police.\nIn Joseon-era Korea, the Podocheong emerged as a police force with the power to arrest and punish criminals. Established in 1469 as a temporary organization, its role solidified into a permanent one.\nIn Sweden, local governments were responsible for law and order by way of a royal decree issued by King Magnus Ladul\u00e5s in the 13th century. The cities financed and organized groups of watchmen who patrolled the streets. In the late 1500s in Stockholm, patrol duties were in large part taken over by a special corps of salaried city guards. The city guard was organized, uniformed and armed like a military unit and was responsible for interventions against various crimes and the arrest of suspected criminals. These guards were assisted by the military, fire patrolmen, and a civilian unit that did not wear a uniform, but instead wore a small badge around the neck. The civilian unit monitored compliance with city ordinances relating to e.g. sanitation issues, traffic and taxes. In rural areas, the King's bailiffs were responsible for law and order until the establishment of counties in the 1630s.\nUp to the early 18th century, the level of state involvement in law enforcement in Britain was low. Although some law enforcement officials existed in the form of parish constables and watchmen, there was no organized police force. A professional police force like the one already present in France would have been ill-suited to Britain, which saw examples such as the French one as a threat to the people's liberty and balanced constitution in favor of an arbitrary and tyrannical government. Law enforcement was mostly up to the private citizens, who had the right and duty to prosecute crimes in which they were involved or in which they were not. At the cry of 'murder!' or 'stop thief!' everyone was entitled and obliged to join the pursuit. Once the criminal had been apprehended, the parish constables and night watchmen, who were the only public figures provided by the state and who were typically part-time and local, would make the arrest. As a result, the state set a reward to encourage citizens to arrest and prosecute offenders. The first of such rewards was established in 1692 of the amount of \u00a340 for the conviction of a highwayman and in the following years it was extended to burglars, coiners and other forms of offense. The reward was to be increased in 1720 when, after the end of the War of the Spanish Succession and the consequent rise of criminal offenses, the government offered \u00a3100 for the conviction of a highwayman. Although the offer of such a reward was conceived as an incentive for the victims of an offense to proceed to the prosecution and to bring criminals to justice, the efforts of the government also increased the number of private thief-takers. Thief-takers became infamously known not so much for what they were supposed to do, catching real criminals and prosecuting them, as for \"setting themselves up as intermediaries between victims and their attackers, extracting payments for the return of stolen goods and using the threat of prosecution to keep offenders in thrall\". Some of them, such as Jonathan Wild, became infamous at the time for staging robberies in order to receive the reward.\nIn 1737, George II began paying some London and Middlesex watchmen with tax monies, beginning the shift to government control. In 1749, Judge Henry Fielding began organizing a force of quasi-professional constables known as the Bow Street Runners. The Bow Street Runners are considered to have been Britain's first dedicated police force. They represented a formalization and regularization of existing policing methods, similar to the unofficial 'thief-takers'. What made them different was their formal attachment to the Bow Street magistrates' office, and payment by the magistrate with funds from the central government. They worked out of Fielding's office and court at No. 4 Bow Street, and did not patrol but served writs and arrested offenders on the authority of the magistrates, travelling nationwide to apprehend criminals. Fielding wanted to regulate and legalize law enforcement activities due to the high rate of corruption and mistaken or malicious arrests seen with the system that depended mainly on private citizens and state rewards for law enforcement. Henry Fielding's work was carried on by his brother, Justice John Fielding, who succeeded him as magistrate in the Bow Street office. Under John Fielding, the institution of the Bow Street Runners gained more and more recognition from the government, although the force was only funded intermittently in the years that followed. In 1763, the Bow Street Horse Patrol was established to combat highway robbery, funded by a government grant. The Bow Street Runners served as the guiding principle for the way that policing developed over the next 80 years. Bow Street was a manifestation of the move towards increasing professionalisation and state control of street life, beginning in London.\nThe Macdaniel affair, a 1754 British political scandal in which a group of thief-takers was found to be falsely prosecuting innocent men in order to collect reward money from bounties, added further impetus for a publicly salaried police force that did not depend on rewards. Nonetheless, In 1828, there were privately financed police units in no fewer than 45 parishes within a 10-mile radius of London.\nThe word \"police\" was borrowed from French into the English language in the 18th century, but for a long time it applied only to French and continental European police forces. The word, and the concept of police itself, were \"disliked as a symbol of foreign oppression\". Before the 19th century, the first use of the word \"police\" recorded in government documents in the United Kingdom was the appointment of Commissioners of Police for Scotland in 1714 and the creation of the Marine Police in 1798.\nModern.\nScotland and Ireland.\nFollowing early police forces established in 1779 and 1788 in Glasgow, Scotland, the Glasgow authorities successfully petitioned the government to pass the Glasgow Police Act establishing the City of Glasgow Police in 1800. Other Scottish towns soon followed suit and set up their own police forces through acts of parliament. In Ireland, the Irish Constabulary Act 1822 marked the beginning of the Royal Irish Constabulary. The act established a force in each barony with chief constables and inspectors general under the control of the civil administration at Dublin Castle. By 1841 this force numbered over 8,600 men.\nLondon.\nIn 1797, Patrick Colquhoun was able to persuade the West Indies merchants who operated at the Pool of London on the River Thames to establish a police force at the docks to prevent rampant theft that was causing annual estimated losses of \u00a3500,000 worth of cargo in imports alone. The idea of a police, as it then existed in France, was considered as a potentially undesirable foreign import. In building the case for the police in the face of England's firm anti-police sentiment, Colquhoun framed the political rationale on economic indicators to show that a police dedicated to crime prevention was \"perfectly congenial to the principle of the British constitution\". Moreover, he went so far as to praise the French system, which had reached \"the greatest degree of perfection\" in his estimation.\nWith the initial investment of \u00a34,200, the new force, the Marine Police, was formed in 1798, composed of about 50 men charged with policing 33,000 workers in the river trades, of whom Colquhoun claimed 11,000 were known criminals and \"on the game\". The force was part funded by the London Society of West India Planters and Merchants. The force was a success after its first year, and his men had \"established their worth by saving \u00a3122,000 worth of cargo and by the rescuing of several lives\". Word of this success spread quickly, and the government passed the Depredations on the Thames Act 1800 on 28 July 1800, establishing a fully funded police force, the Thames River Police, together with new laws including police powers. Colquhoun published a book on the experiment, \"The Commerce and Policing of the River Thames\". It found receptive audiences far outside London, and inspired similar forces in other cities, notably, New York City, Dublin, and Sydney.\nColquhoun's utilitarian approach to the problem \u2013 using a cost-benefit argument to obtain support from businesses standing to benefit \u2013 allowed him to achieve what Henry and John Fielding failed for their Bow Street Runners. Unlike the stipendiary system at Bow Street, the river police were full-time, salaried officers prohibited from taking private fees. His other contribution was the concept of preventive policing; his police were to act as a highly visible deterrent to crime by their permanent presence on the Thames.\nMetropolitan.\nLondon was fast reaching a size unprecedented in world history, due to the onset of the Industrial Revolution. It became clear that the locally maintained system of volunteer constables and \"watchmen\" was ineffective, both in detecting and preventing crime. A parliamentary committee was appointed to investigate the system of policing in London. Upon Sir Robert Peel being appointed as Home Secretary in 1822, he established a second and more effective committee, and acted upon its findings.\nRoyal assent to the Metropolitan Police Act 1829 was given and the Metropolitan Police Service was established on September 29, 1829, in London. Peel, widely regarded as the father of modern policing, was heavily influenced by the social and legal philosophy of Jeremy Bentham, who called for a strong and centralised, but politically neutral, police force for the maintenance of social order, for the protection of people from crime and to act as a visible deterrent to urban crime and disorder. Peel decided to standardise the police force as an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public.\nDue to public fears concerning the deployment of the military in domestic matters, Peel organised the force along civilian lines, rather than paramilitary. To appear neutral, the uniform was deliberately manufactured in blue, rather than red which was then a military colour, along with the officers being armed only with a wooden truncheon and a rattle to signal the need for assistance. Along with this, police ranks did not include military titles, with the exception of Sergeant.\nTo distance the new police force from the initial public view of it as a new tool of government repression, Peel publicised the so-called Peelian principles, which set down basic guidelines for ethical policing:\nThe Metropolitan Police Act 1829 created a modern police force by limiting the purview of the force and its powers and envisioning it as merely an organ of the judicial system. Their job was apolitical; to maintain the peace and apprehend criminals for the courts to process according to the law. This was very different from the \"continental model\" of the police force that had been developed in France, where the police force worked within the parameters of the absolutist state as an extension of the authority of the monarch and functioned as part of the governing state.\nIn 1863, the Metropolitan Police were issued with the distinctive custodian helmet, and in 1884 they switched to the use of whistles that could be heard from much further away. The Metropolitan Police became a model for the police forces in many countries, including the United States and most of the British Empire. Bobbies can still be found in many parts of the Commonwealth of Nations.\nAustralia.\nIn Australia, organized law enforcement emerged soon after British colonization began in 1788. The first law enforcement organizations were the Night Watch and Row Boat Guard, which were formed in 1789 to police Sydney. Their ranks were drawn from well-behaved convicts deported to Australia. The Night Watch was replaced by the Sydney Foot Police in 1790. In New South Wales, rural law enforcement officials were appointed by local justices of the peace during the early to mid-19th century and were referred to as \"bench police\" or \"benchers\". A mounted police force was formed in 1825.\nThe first police force having centralised command as well as jurisdiction over an entire colony was the South Australia Police, formed in 1838 under Henry Inman. However, whilst the New South Wales Police Force was established in 1862, it was made up from a large number of policing and military units operating within the then Colony of New South Wales and traces its links back to the Royal Marines. The passing of the Police Regulation Act of 1862 essentially tightly regulated and centralised all of the police forces operating throughout the Colony of New South Wales.\nEach Australian state and territory maintain its own police force, while the Australian Federal Police enforces laws at the federal level. The New South Wales Police Force remains the largest police force in Australia in terms of personnel and physical resources. It is also the only police force that requires its recruits to undertake university studies at the recruit level and has the recruit pay for their own education.\nBrazil.\nIn 1566, the first police investigator of Rio de Janeiro was recruited. By the 17th century, most captaincies already had local units with law enforcement functions. On July 9, 1775, a Cavalry Regiment was created in the state of Minas Gerais for maintaining law and order. In 1808, the Portuguese royal family relocated to Brazil, because of the French invasion of Portugal. King Jo\u00e3o\u00a0VI established the ('General Police Intendancy') for investigations. He also created a Royal Police Guard for Rio de Janeiro in 1809. In 1831, after independence, each province started organizing its local \"military police\", with order maintenance tasks. The Federal Railroad Police was created in 1852, Federal Highway Police, was established in 1928, and Federal Police in 1967.\nCanada.\nDuring the early days of English and French colonization, municipalities hired watchmen and constables to provide security. Established in 1729, the Royal Newfoundland Constabulary (RNC) was the first policing service founded in Canada. The establishment of modern policing services in the Canadas occurred during the 1830s, modelling their services after the London Metropolitan Police, and adopting the ideas of the Peelian principles. The Toronto Police Service was established in 1834 as the first municipal police service in Canada. Prior to that, local able-bodied male citizens had been required to report for night watch duty as special constables for a fixed number of nights a year on penalty of a fine or imprisonment in a system known as \"watch and ward.\" The Quebec City Police Service was established in 1840.\nA national police service, the Dominion Police, was founded in 1868. Initially the Dominion Police provided security for parliament, but its responsibilities quickly grew. In 1870, Rupert's Land and the North-Western Territory were incorporated into the country. In an effort to police its newly acquired territory, the Canadian government established the North-West Mounted Police in 1873 (renamed Royal North-West Mounted Police in 1904). In 1920, the Dominion Police, and the Royal Northwest Mounted Police were amalgamated into the Royal Canadian Mounted Police (RCMP).\nThe RCMP provides federal law enforcement; and law enforcement in eight provinces, and all three territories. The provinces of Ontario, and Quebec maintain their own provincial police forces, the Ontario Provincial Police (OPP), and the S\u00fbret\u00e9 du Qu\u00e9bec (SQ). Policing in Newfoundland and Labrador is provided by the RCMP, and the RNC. The aforementioned services also provide municipal policing, although larger Canadian municipalities may establish their own police service.\nLebanon.\nIn Lebanon, the current police force was established in 1861, with creation of the Gendarmerie.\nIndia.\nUnder the Mughal Empire, provincial governors called subahdars (or nazims), as well as officials known as faujdars and thanadars were tasked with keeping law and order. Kotwals were responsible for public order in urban areas. In addition, officials called amils, whose primary duties were tax collection, occasionally dealt with rebels. The system evolved under growing British influence that eventually culminated in the establishment of the British Raj. In 1770, the offices of faujdar and amil were abolished. They were brought back in 1774 by Warren Hastings, the first Governor of the Presidency of Fort William (Bengal). In 1791, the first permanent police force was established by Charles Cornwallis, the Commander-in-Chief of British India and Governor of the Presidency of Fort William.\nA single police force was established after the formation of the British Raj with the Government of India Act 1858. A uniform police bureaucracy was formed under the Police Act 1861, which established the Superior Police Services. This later evolved into the Indian Imperial Police, which kept order until the Partition of India and independence in 1947. In 1948, the Indian Imperial Police was replaced by the Indian Police Service.\nIn modern India, the police are under the control of respective States and union territories and are known to be under State Police Services (SPS). The candidates selected for the SPS are usually posted as Deputy Superintendent of Police or Assistant Commissioner of Police once their probationary period ends. On prescribed satisfactory service in the SPS, the officers are nominated to the Indian Police Service. The service color is usually dark blue and red, while the uniform color is \"Khaki\".\nUnited States.\nIn Colonial America, the county sheriff was the most important law enforcement official. For instance, the New York Sheriff's Office was founded in 1626, and the Albany County Sheriff's Department in the 1660s. The county sheriff, who was an elected official, was responsible for enforcing laws, collecting taxes, supervising elections, and handling the legal business of the county government. Sheriffs would investigate crimes and make arrests after citizens filed complaints or provided information about a crime but did not carry out patrols or otherwise take preventive action. Villages and cities typically hired constables and marshals, who were empowered to make arrests and serve warrants. Many municipalities also formed a night watch, a group of citizen volunteers who would patrol the streets at night looking for crime and fires. Typically, constables and marshals were the main law enforcement officials available during the day while the night watch would serve during the night. Eventually, municipalities formed day watch groups. Rioting was handled by local militias.\nIn the 1700s, the Province of Carolina (later North- and South Carolina) established slave patrols in order to prevent slave rebellions and enslaved people from escaping. By 1785 the Charleston Guard and Watch had \"a distinct chain of command, uniforms, sole responsibility for policing, salary, authorized use of force, and a focus on preventing crime.\"\nIn 1751 moves towards a municipal police service in Philadelphia were made when the city's night watchmen and constables began receiving wages and a Board of Wardens was created to oversee the night watch. \nIn 1789 the United States Marshals Service was established, followed by other federal services such as the U.S. Parks Police (1791) and U.S. Mint Police (1792). Municipal police services were created in Richmond, Virginia in 1807, Boston in 1838, and New York City in 1845. The United States Secret Service was founded in 1865 and was for some time the main investigative body for the federal government.\nModern policing influenced by the British model of policing established in 1829 based on the Peelian principles began emerging in the United States in the mid-19th century, replacing previous law enforcement systems based primarily on night watch organizations. Cities began establishing organized, publicly funded, full-time professional police services. In Boston, a day police consisting of six officers under the command of the city marshal was established in 1838 to supplement the city's night watch. This paved the way for the establishment of the Boston Police Department in 1854. In New York City, law enforcement up to the 1840s was handled by a night watch as well as city marshals, municipal police officers, and constables. In 1845, the New York City Police Department was established. In Philadelphia, the first police officers to patrol the city in daytime were employed in 1833 as a supplement to the night watch system, leading to the establishment of the Philadelphia Police Department in 1854.\nIn the American Old West, law enforcement was carried out by local sheriffs, rangers, constables, and federal marshals. There were also town marshals responsible for serving civil and criminal warrants, maintaining the jails, and carrying out arrests for petty crime.\nIn addition to federal, state, and local forces, some special districts have been formed to provide extra police protection in designated areas. These districts may be known as neighborhood improvement districts, crime prevention districts, or security districts.\nIn 2022, San Francisco supervisors approved a policy allowing municipal police (San Francisco Police Department) to use robots for various law enforcement and emergency operations, permitting their employment as a deadly force option in cases where the \"risk of life to members of the public or officers is imminent and outweighs any other force option available to SFPD.\" This policy has been criticized by groups such as the Electronic Frontier Foundation and the ACLU, who have argued that \"killer robots will not make San Francisco better\" and \"police might even bring armed robots to a protest.\"\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23628", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=23628", "title": "PDP-10", "text": "36-bit computer by Digital (1966\u20131983)\nDigital Equipment Corporation (DEC)'s PDP-10, later marketed as the DECsystem-10, is a mainframe computer family manufactured from 1966, delivered from December 1967, and discontinued in 1983. 1970s models and beyond were marketed under the DECsystem-10 name, especially as the TOPS-10 operating system became widely used.\nThe PDP-10's architecture is almost identical to that of DEC's earlier PDP-6, sharing the same 36-bit word length and slightly extending the instruction set. The main difference was a greatly improved hardware implementation. Some aspects of the instruction set are unusual, most notably the \"byte\" instructions, which operate on bit fields of any size from 1 to 36 bits inclusive, according to the general definition of a byte as \"a contiguous sequence of a fixed number of bits\".\nThe PDP-10 was found in many university computing facilities and research labs during the 1970s, the most notable being Harvard University's Aiken Computation Laboratory, MIT's AI Lab and Project MAC, Stanford's SAIL, Computer Center Corporation (CCC), ETH (ZIR), and Carnegie Mellon University. Its main operating systems, TOPS-10 and TENEX, and to a lesser extent ITS at a small number of influential locations, were used to build out the early ARPANET. The 36-bit words were especially well suited to Lisp at the time, because split in two parts 18 bits each to be used as pointers, it could address 256K objects instead of 64K on an equivalent 32-bit machine. For these reasons, the PDP-10 looms large in early hacker folklore.\nProjects to extend the PDP-10 line were eclipsed by the success of DEC's unrelated VAX superminicomputer, and the cancellation of the PDP-10 line was announced in 1983. According to reports, DEC sold \"about 1500 DECsystem-10s by the end of 1980\".\nModels and technical evolution.\nThe original PDP-10 processor is the KA10, introduced in 1968. It uses discrete transistors packaged in DEC's Flip-Chip technology, with backplanes wire-wrapped via a semi-automated manufacturing process. Its cycle time is 1\u00a0\u03bcs and its add time 2.1\u00a0\u03bcs. In 1973, the KA10 was replaced by the KI10, which uses transistor\u2013transistor logic (TTL) SSI. This was joined in 1975 by the faster KL10 which is built from emitter-coupled logic (ECL), microprogrammed, and has cache memory. The KL10's performance was about 1\u00a0megaflops using 36-bit floating-point numbers on matrix row reduction. It was slightly faster than the newer VAX-11/750, although more limited in memory.\nA smaller, less expensive model, the KS10, was introduced in 1978, using TTL and Am2901 bit-slice components and including the PDP-11 Unibus to connect peripherals. The KS10 was marketed as the DECSYSTEM-2020, part of the DECSYSTEM-20 range; it was DEC's entry in the distributed processing arena, and it was introduced as \"the world's lowest cost mainframe computer system\".\nKA10.\nThe KA10 has a maximum main memory capacity (both virtual and physical) of 256\u00a0kilowords (equivalent to 1152\u00a0kilobytes); the minimum main memory required is 16\u00a0kilowords. As supplied by DEC, it did not include paging hardware; memory management consists of two sets of protection and relocation registers, called \"base and bounds\" registers. This allows each half of a user's address space to be limited to a set section of main memory, designated by the base physical address and size. This allows the model of separate read-only shareable code segment (normally the high segment) and read-write data/stack segment (normally the low segment) used by TOPS-10 and later adopted by Unix. Some KA10 machines, first at MIT, and later at Bolt, Beranek and Newman (BBN), were modified to add virtual memory and support for demand paging, and more physical memory.\nThe KA10 weighs about .\nThe 10/50 was the top-of-the-line uniprocessor KA machine at the time when the \"PA1050\" software package was introduced. Two other KA10 models were the uniprocessor 10/40, and the dual-processor 10/55.\nKI10.\nThe KI10 introduced support for paged memory management and for a larger physical address space of 4\u00a0megawords. KI10 models include 1060, 1070 and 1077, the latter incorporating two CPUs.\nKL10.\nThe original KL10 PDP-10 (also marketed as DECsystem-10) models (1080, 1088, etc.) use the original PDP-10 memory bus, with external memory modules. Module in this context meant a cabinet, dimensions roughly 30 in \u00d7 75 in \u00d7 30 in (W \u00d7 H \u00d7 D). with a capacity of 32 to 256\u00a0kWords of magnetic-core memory. The processors used in the DECSYSTEM-20 (2040, 2050, 2060, 2065), commonly but incorrectly called \"KL20\", use internal memory, mounted in the same cabinet as the CPU. The 10xx models also have different packaging; they come in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20. The differences between the 10xx and 20xx models were primarily which operating system they ran, either TOPS-10 or TOPS-20. Apart from that, differences are more cosmetic than real; some 10xx systems have \"20-style\" internal memory and I/O, and some 20xx systems have \"10-style\" external memory and an I/O bus. In particular, all ARPAnet TOPS-20 systems had an I/O bus because the AN20 IMP interface was an I/O bus device. Both could run either TOPS-10 or TOPS-20 microcode and thus the corresponding operating system.\nModel B.\nThe later Model B version of the 2060 processors removes the 256\u00a0kiloword limit on the virtual address space by supporting up to 32 \"sections\" of up to 256\u00a0kilowords each, along with substantial changes to the instruction set. The two versions are effectively different CPUs. The first operating system that takes advantage of the Model\u00a0B's capabilities is TOPS-20 release\u00a03, and user mode extended addressing is offered in TOPS-20 release\u00a04. TOPS-20 versions after release\u00a04.1 only run on a Model\u00a0B.\nTOPS-10 versions 7.02 and 7.03 also use extended addressing when run on a 1090 (or 1091) Model\u00a0B processor running TOPS-20 microcode.\nMCA25.\nThe final upgrade to the KL10 was the MCA25 upgrade of a 2060 to 2065 (or a 1091 to 1095), which gave some performance increases for programs running in multiple sections.\nMassbus.\nThe I/O architecture of the 20xx series KL machines is based on a DEC bus design called the Massbus. While many attributed the success of the PDP-11 to DEC's decision to make the PDP-11 Unibus an open architecture, DEC reverted to prior philosophy with the KL, making Massbus both unique and proprietary. Consequently, there were no aftermarket peripheral manufacturers who made devices for the Massbus, and DEC chose to price their own Massbus devices, notably the RP06 disk drive, at a substantial premium above comparable IBM-compatible devices. CompuServe for one, designed its own alternative disk controller that could operate on the Massbus, but connect to IBM style 3330 disk subsystems.\nFront-end processors.\nThe KL class machines have a PDP-11/40 front-end processor for system start-up and monitoring. The PDP-11 is booted from a dual-ported RP06 disk drive (or alternatively from an 8\" floppy disk drive or DECtape), and then commands can be given to the PDP-11 to start the main processor, which is typically booted from the same RP06 disk drive as the PDP-11. The PDP-11 performs watchdog functions once the main processor is running.\nCommunication with IBM mainframes, including Remote Job Entry (RJE), was accomplished via a DN61 or DN-64 front-end processor, using a PDP-11/40 or PDP-11/34a.\nKS10.\nThe KS10 is a lower-cost PDP-10 built using AMD 2901 bit-slice chips, with an Intel 8080A microprocessor as a control processor. The KS10 design was crippled to be a Model\u00a0A, even though most of the necessary data paths needed to support the Model\u00a0B architecture are present. This was no doubt intended to segment the market, but it greatly shortened the KS10's product life.\nThe KS system uses a similar boot procedure to the KL10. The 8080 control processor loads the microcode from an RM03, RM80, or RP06 disk or magnetic tape and then starts the main processor. The 8080 switches modes after the operating system boots and controls the console and remote diagnostic serial ports.\nMagnetic tape drives.\nTwo models of tape drives were supported by the TM10 Magnetic Tape Control subsystem:\nA mix of up to eight of these could be supported, using 7-track or 9-track devices. The TU20 and TU30 each came in A (9-track) and B (7-track) versions, and all of the aforementioned tape drives could read/write from/to 200\u00a0BPI, 556\u00a0BPI and 800\u00a0BPI IBM-compatible tapes.\nThe TM10 Magtape controller was available in two submodels:\nInstruction set architecture.\nFrom the first PDP-6s to the KL-10 and KS-10, the user-mode instruction set architecture is largely the same. This section covers that architecture. The only major change to the architecture is the addition of multi-section extended addressing in the KL-10; extended addressing, which changes the process of generating the effective address of an instruction, is briefly discussed at the end. Generally, the system has 36-bit words and instructions, and 18-bit addresses.\nRegisters.\nThere are 16 general-purpose, 36-bit registers. The right half of these registers (other than register 0) may be used for indexing. A few instructions operate on pairs of registers. The \"PC Word\" register is split in half; the right 18 bits contains the program counter and the left 13 bits contains the processor status flags, with five zeros between the two sections. The condition register bits, which record the results of arithmetic operations (\"e.g.\" overflow), can be accessed by only a few instructions.\nIn the original KA-10 systems, these registers are simply the first 16 words of main memory. The \"fast registers\" hardware option implements them as registers in the CPU, still addressable as the first 16 words of memory. Some software takes advantage of this by using the registers as an instruction cache by loading code into the registers and then jumping to the appropriate address; this is used, for example, in Maclisp to implement one version of the garbage collector. Later models all have registers in the CPU.\nSupervisor mode.\nThere are two operational modes, supervisor and user mode. Besides the difference in memory referencing described above, supervisor-mode programs can execute input/output operations.\nCommunication from user-mode to supervisor-mode is done through Unimplemented User Operations (UUOs): instructions which are not defined by the hardware, and are trapped by the supervisor. This mechanism is also used to emulate operations which may not have hardware implementations in cheaper models.\nData types.\nThe major datatypes which are directly supported by the architecture are two's complement 36-bit integer arithmetic (including bitwise operations), 36-bit floating-point, and halfwords. Extended, 72-bit, floating point is supported through special instructions designed to be used in multi-instruction sequences. Byte pointers are supported by special instructions. A word structured as a \"count\" half and a \"pointer\" half facilitates the use of bounded regions of memory, notably stacks.\nInstructions.\nInstructions are stored in 36-bit words. There are two formats, general instructions and input/output instructions.\nIn general instructions, the leftmost 9 bits, 0 to 8, contain an instruction opcode. Many of the possible 512 codes are not defined in the base model machines and are reserved for expansion like the addition of a hardware floating point unit. Following the opcode in bits 9 to 12 is the number of a register which will be used for the instruction. The input/output instructions all start with bits 0 through 2 being set to 1 (decimal value 7), bits 3 through 9 containing a device number, and 10 through 12 the instruction opcode.\nIn both formats, bits 13 through 35 are used to form the \"effective address\", E. Bits 18 through 35 contain a numerical constant address, Y. This address may be modified by adding the 18-bit value in a register, X, the register number indicated in bits 14 to 17. If these are set to zero, no indexing is used, meaning register 0 cannot be used for indexing. Bit 13, I, indicates indirection, meaning the ultimate effective address used by the instruction is not E, but the address stored in memory location E. When using indirection, the data in word E is interpreted in the same way as the layout of the instruction; bits 0 to 12 are ignored, and 13 through 35 form I, X and Y as above.\nInstruction execution begins by calculating E. It adds the contents of the given register X (if not 0) to the offset Y; then, if the indirect bit is 1, the value at E is fetched and the effective address calculation is repeated. If I is 1 in the stored value at E in memory, the system will then indirect through that address as well, possibly following many such steps. This process continues until an indirect word with a zero indirect bit is reached. Indirection of this sort was a common feature of processor designs of this era.\nIn supervisor mode, addresses correspond directly to physical memory. In user mode, addresses are translated to physical memory. Earlier models give a user process a \"high\" and a \"low\" memory: addresses with a 0 top bit use one base register and those with a 1 use another. Each segment is contiguous. Later architectures have paged memory access, allowing non-contiguous address spaces. The CPU's general-purpose registers can also be addressed as memory locations 0\u201315.\nGeneral instructions.\nThere are three main classes of general instructions: arithmetic, logical, and move; conditional jump; conditional skip (which may have side effects). There are also several smaller classes.\nThe arithmetic, logical, and move operations include variants which operate immediate-to-register, memory-to-register, register-to-memory, register-and-memory-to-both or memory-to-memory. Since registers may be addressed as part of memory, register-to-register operations are also defined. (Not all variants are useful, though they are well-defined.) For example, the ADD operation has as variants ADDI (add an 18-bit \"I\"mmediate constant to a register), ADDM (add register contents to a \"M\"emory location), ADDB (add to \"B\"oth, that is, add register contents to memory and put the result in the register). A more elaborate example is HLROM (\"H\"alf \"L\"eft to \"R\"ight, \"O\"nes to \"M\"emory), which takes the Left half of the register contents, places them in the Right half of the memory location, and replaces the left half of the memory location with Ones. Halfword instructions are also used for linked lists: HLRZ is the Lisp CAR operator; HRRZ is CDR.\nThe conditional jump operations examine register contents and jump to a given location depending on the result of the comparison. The mnemonics for these instructions all start with JUMP, JUMPA meaning \"jump always\" and JUMP meaning \"jump never\" \u2013 as a consequence of the symmetric design of the instruction set, it contains several no-ops such as JUMP. For an example of a conditional jump, JUMPN A,LOC jumps to the address LOC if the contents of register A is non-zero. There are also conditional jumps based on the processor's condition register using the JRST instruction. On the KA10 and KI10, JRST is faster than JUMPA, so the standard unconditional jump is JRST.\nThe conditional skip operations compare register and memory contents and skip the next instruction (which is often an unconditional jump) depending on the result of the comparison. A simple example is CAMN A,LOC which compares the contents of register A with the contents of location LOC and skips the next instruction if they are not equal. A more elaborate example is TLCE A,LOC (read \"Test Left Complement, skip if Equal\"), which using the contents of LOC as a mask, selects the corresponding bits in the left half of register A. If all those bits are \"E\"qual to zero, skip the next instruction; and in any case, replace those bits by their Boolean complement.\nSome smaller instruction classes include the shift/rotate instructions and the procedure call instructions. Particularly notable are the stack instructions PUSH and POP, and the corresponding stack call instructions PUSHJ and POPJ. The byte instructions use a special format of indirect word to extract and store arbitrary-sized bit fields, possibly advancing a pointer to the next unit.\nInput/output instructions.\nThe PDP-10 does not use memory-mapped devices, in contrast to the PDP-11 and later DEC machines. A separate set of instructions is used to move data to and from devices defined by a device number in the instruction. Bits 3 to 9 contain the device number, with the 7 bits allowing a total of 128 devices. Instructions allow for the movement of data to and from devices in word-at-a-time (DATAO and DATAI) or block-at-a-time (BLKO, BLKI).\nIn block mode, the value pointed to by E is a word in memory that is split in two, the right 18 bits indicate a starting address in memory where the data is located (or written into) and the left 18 bits are a counter. The block instructions increment both values every time they are called, thereby increasing the counter as well as moving to the next location in memory. It then performs a DATAO or DATAI. Finally, it checks the counter side of the value at E, if it is non-zero, it skips the next instruction. If it is zero, it performs the next instruction, normally a JUMP back to the top of the loop. The BLK instructions are effectively small programs that loop over a DATA and increment instructions, but by having this implemented in the processor itself, it avoids the need to repeatedly read the series of instructions from main memory and thus performs the loop much more rapidly.\nThe final set of I/O instructions are used to write and read condition codes on the device, CONO and CONI. Additionally, CONSZ will perform a CONI, bitmask the retrieved data against the value in E, and then skip the next instruction if it is zero, used in a fashion similar to the BLK commands. Only the right 18 bits are tested in CONSZ.\nInterrupt handling.\nA second use of the CONO instruction is to set the device's priority level for interrupt handling. There are three bits in the CONO instruction, 33 through 35, allowing the device to be set to level 0 through 7. Level 1 is the highest, meaning that if two devices raise an interrupt at the same time, the lowest-numbered device will begin processing. Level 0 means \"no interrupts\", so a device set to level 0 will not stop the processor even if it does raise an interrupt.\nEach device channel has two memory locations associated with it, one at 40+2N and the other at 41+2N, where N is the channel number. Thus, channel 1 uses locations 42 and 43. When the interrupt is received and accepted, meaning no higher-priority interrupt is already running, the system stops at the next memory read part of the instruction cycle and instead begins processing at the address stored in the first of those two locations. It is up to the interrupt handler to turn off the interrupt level when it is complete, which it can do by running a CONO, DATA or BLK instruction. As a special case, if the instruction is BLKI or BLKO, and the incremented count is not zero, the interrupt is dismissed immediately, otherwise the second instruction is executed to process the interrupt. This provided a low-cost medium performance version of direct memory access.\nTwo of the device numbers are set aside for special purposes. Device 0 is the computer's front-panel console; reading that device retrieves the settings of the panel switches while writing lights up the status lamps. Device 4 is the \"priority interrupt\", which can be read using CONI to gain additional information about an interrupt that has occurred.\nExtended addressing.\nIn processors supporting extended addressing, the address space is divided into \"sections\". An 18-bit address is a \"local address\", containing an offset within a section, and a \"global address\" is 30 bits, divided into a 12-bit section number at the bottom of the left 18 bits and an 18-bit offset within that section in the right 18 bits. A register can contain either a \"local index\", with an 18-bit unsigned displacement or local address in the right 18 bits, or a \"global index\", with a 30-bit unsigned displacement or global address in the right 30 bits. An indirect word can either be a \"local indirect word\", with its uppermost bit set, the next 12 bits reserved, and the remaining bits being an indirect bit, a 4-bit register code, and an 18-bit displacement, or a \"global indirect word\", with its uppermost bit clear, the next bit being an indirect bit, the next 4 bits being a register code, and the remaining 30 bits being a displacement.\nThe process of calculating the effective address generates a 12-bit section number and an 18-bit offset within that segment.\nSoftware.\nOperating systems.\nThe original PDP-10 operating system was simply called \"Monitor\", but was later renamed TOPS-10. Eventually the PDP-10 system itself was renamed the DECsystem-10. Early versions of Monitor and TOPS-10 formed the basis of Stanford's WAITS operating system and the CompuServe time-sharing system.\nOver time, some PDP-10 operators began running operating systems assembled from major components developed outside DEC. For example, the main Scheduler might come from one university, the Disk Service from another, and so on. The commercial timesharing services such as CompuServe, On-Line Systems, Inc. (OLS), and Rapidata maintained sophisticated inhouse systems programming groups so that they could modify the operating system as needed for their own businesses without being dependent on DEC or others. There are also strong user communities such as DECUS through which users can share software that they have developed.\nBBN developed their own alternative operating system, TENEX, which fairly quickly became popular in the research community. DEC later ported TENEX to the KL10, enhanced it considerably, and named it TOPS-20, forming the DECSYSTEM-20 line.\nMIT, which had developed CTSS, Compatible Time-Sharing System to run on their IBM 709 (and later a modified IBM 7094 system), also developed ITS, Incompatible Timesharing System to run on their PDP-6 (and later a modified PDP-10);\nTymshare developed TYMCOM-X, derived from TOPS-10 but using a page-based file system like TOPS-20.\nProgramming languages.\nDEC maintained DECsystem-10 FORTRAN IV (F40) for the PDP-10 from 1967 to 1975\nMACRO-10 (assembly language macro compiler), COBOL, BASIC and AID were supported under the multi processing and swapping monitors.\nIn practice a number of other programming environments were available including LISP and SNOBOL at the Hatfield Polytechnic site around 1970.\nClones.\nIn 1971 to 1972, researchers at Xerox PARC were frustrated by top company management's refusal to let them buy a PDP-10. Xerox had just bought Scientific Data Systems (SDS) in 1969, and wanted PARC to use an SDS machine. Instead, a group led by Charles P. Thacker designed and constructed two PDP-10 clone systems named MAXC (pronounced as Max, in honour of Max Palevsky, who had sold SDS to Xerox) for their own use. MAXC was also a backronym for Multiple Access Xerox Computer. MAXC ran a modified version of TENEX.\nThird-party attempts to sell PDP-10 clones were relatively unsuccessful; see Foonly, Systems Concepts, and XKL.\nUse by CompuServe.\nOne of the largest collections of DECsystem-10 architecture systems ever assembled was at CompuServe, which, at its peak, operated over 200 loosely coupled systems in three data centers in Columbus, Ohio. CompuServe used these systems as 'hosts', providing access to commercial applications, and the CompuServe Information Service. While the first such systems were bought from DEC, when DEC abandoned the PDP-10 architecture in favor of the VAX, CompuServe and other PDP-10 customers began buying plug compatible computers from Systems Concepts. As of January 2007[ [update]], CompuServe was operating a small number of PDP-10 architecture machines to perform some billing and routing functions.\nThe main power supplies used in the KL-series machines were so inefficient that CompuServe engineers designed a replacement supply that used about half the energy. CompuServe offered to license the design for its KL supply to DEC for free if DEC would promise that any new KL bought by CompuServe would have the more efficient supply installed. DEC declined the offer.\nAnother modification made to the PDP-10 by CompuServe engineers was replacing the hundreds of incandescent indicator lamps on the KI10 processor cabinet with LED lamp modules. The cost of conversion was easily offset by cost savings in electricity use, reduced heat, and labor needed to replace burned-out lamps. Digital followed this step all over the world. The picture on the right hand side shows the light panel of the MF10 memory which is contemporary with the KI10 CPU. This item is part of a computer museum, and was populated with LEDs in 2008 for demonstration purposes only. There were no similar banks of indicator lamps on KL and KS processors themselves - only on legacy memory and peripheral devices.\nCancellation and influence.\nThe PDP-10 was eventually eclipsed by the VAX superminicomputer machines (descendants of the PDP-11) when DEC recognized that the PDP-10 and VAX product lines were competing with each other and decided to concentrate its software development effort on the more profitable VAX. The PDP-10 product line cancellation was announced in 1983, including cancelling the ongoing Jupiter project to produce a new high-end PDP-10 processor (despite that project being in good shape at the time of the cancellation) and the Minnow project to produce a desktop PDP-10, which may then have been at the prototyping stage.\nThis event spelled the doom of ITS and the technical cultures that had spawned the original jargon file, but by the 1990s it had become something of a badge of honor among old-time hackers to have cut one's teeth on a PDP-10.\nThe PDP-10 assembly language instructions LDB and DPB (load/deposit byte) live on as functions in the programming language Common Lisp. See the \"References\" section on the LISP article. The 36-bit word size of the PDP-6 and PDP-10 was influenced by the programming convenience of having 2 LISP pointers, each 18 bits, in one word.\nWill Crowther created \"Adventure\", the prototypical computer adventure game, for a PDP-10. Don Daglow created the first computer baseball game (1971) and \"Dungeon\" (1975), the first role-playing video game on a PDP-10. Walter Bright originally created \"Empire\" for the PDP-10. Roy Trubshaw and Richard Bartle created the first MUD on a PDP-10. \"Zork\" was written on the PDP-10. Infocom used PDP-10s for game development and testing.\nBill Gates and Paul Allen originally wrote Altair BASIC using an Intel 8080 simulator running on a PDP-10 at Harvard University. Allen repurposed the PDP-10 assembler as a cross assembler for the 8080 chip. They founded Microsoft shortly after.\nEmulation or simulation.\nThe software for simulation of historical computers, SIMH, contains modules to emulate all the PDP-10 CPU models on a Windows or Unix-based machine. Copies of DEC's original distribution tapes are available as downloads from the Internet so that a running TOPS-10 or TOPS-20 system may be established. ITS and WAITS are also available for SIMH. A PDP-10 front panel replica for the KA10 processor is available for the SIMH emulation running on a Raspberry Pi.\nKen Harrenstien's KLH10 software for Unix-like systems emulates a KL10B processor with extended addressing and 4 MW of memory or a KS10 processor with 512 KW of memory. The KL10 emulation supports v.442 of the KL10 microcode, which enables it to run the final versions of both TOPS-10 and TOPS-20. The KS10 emulation supports both ITS v.262 microcode for the final version of KS10 ITS and DEC v.130 microcode for the final versions of KS TOPS-10 and TOPS-20.\n\"This article is based in part on the Jargon File, which is in the public domain.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23629", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=23629", "title": "DECSYSTEM-20", "text": "Type of mainframe computer\nThe DECSYSTEM-20 was a family of 36-bit Digital Equipment Corporation PDP-10 mainframe computers running the TOPS-20 operating system and was introduced in 1977.\nPDP-10 computers running the TOPS-10 operating system were labeled \"DECsystem-10\" as a way of differentiating them from the PDP-11. Later on, those systems running TOPS-20 (on the KL10 PDP-10 processors) were labeled \"DECSYSTEM-20\" (the block capitals being the result of a lawsuit brought against DEC by Singer, which made its own System Ten model, occasionally referenced in reporting as the \"System 10\"). The DECSYSTEM-20 was sometimes called PDP-20, although this designation was never used by DEC.\nModels.\nThe following models were produced:\nThe only significant difference the user could see between a DECsystem-10 and a DECSYSTEM-20 was the operating system and the color of the paint. Most (but not all) machines sold to run TOPS-10 were painted \"Blasi Blue\", whereas most TOPS-20 machines were painted \"Terracotta\" (often mistakenly called \"Chinese Red\" or orange; the actual name of the color on the paint cans was Terra Cotta).\nThere were some significant internal differences between the earlier KL10 Model A processors, used in the earlier DECsystem-10s running on KL10 processors, and the later KL10 Model Bs, used for the DECSYSTEM-20s. Model As used the original PDP-10 memory bus, with external memory modules. The later Model B processors used in the DECSYSTEM-20 used internal memory, mounted in the same cabinet as the CPU. The Model As also had different packaging; they came in the original tall PDP-10 cabinets, rather than the short ones used later on for the DECSYSTEM-20.\nThe last released implementation of DEC's 36-bit architecture was the single cabinet DECSYSTEM-2020, using a KS10 processor.\nThe DECSYSTEM-20 was primarily designed and used as a small mainframe for timesharing. That is, multiple users would concurrently log on to individual user accounts and share use of the main processor to compile and run applications. Separate disk allocations were maintained for all users by the operating system, and various levels of protection could be maintained by for System, Owner, Group, and World users. A model 2060, for example, could typically host up to 40 to 60 simultaneous users before exhibiting noticeably delayed response time.\nRemaining machines.\nThe Living Computer Museum of Seattle, Washington, maintained a 2065 running TOPS-10, which was available to interested parties via SSH upon registration (at no cost) at their website.\nTrivia.\nThe first ever email spam message on 1 May 1978 was an advertisement for west coast users of the ARPANET to come see a DECSYSTEM-20.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23630", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=23630", "title": "Programmed Data Processor", "text": "Name used for several lines of minicomputers\nProgrammed Data Processor (PDP), referred to by some customers, media and authors as \"Programmable Data Processor,\" is a term used by the Digital Equipment Corporation from 1957 to 1990 for several lines of minicomputers.\nThe name \"PDP\" intentionally avoids the use of the term \"computer\". At the time of the first PDPs, computers had a reputation of being large, complicated, and expensive machines. The venture capitalists behind Digital (especially Georges Doriot) would not support Digital's attempting to build a \"computer\" and the term \"minicomputer\" had not yet been coined. So instead, Digital used their existing line of logic modules to build a \"Programmed Data Processor\" and aimed it at a market that could not afford the larger computers.\nSeries.\nThe various PDP machines can generally be grouped into families based on word length. Members of the PDP series include:\nThe last of DEC's 53 PDP-1 computers was built in 1969, a decade after the first, and nearly all of them were still in use as of 1975. \"An average configuration cost $120,000\" at a time \"when most computer systems sold for a million dollars or more.\"\nIts architectural successors as 18-bit machines were the PDP-4, PDP-7, PDP-9, and the PDP-15.\nThe first version of Unix, and the first version of B, a predecessor of C, were written for the PDP-7 at Bell Labs, as was the first version (by DEC) of MUMPS.\nOver 50,000 units among various models of the family (A, E, F, I, S, L, M) were sold. Later models are also used in the DECmate word processor and the VT-78 workstation. \nThe KL was also used for the DECSYSTEM-20. The KS was used for the 2020, DEC's entry in the distributed processing market, introduced as \"the world's lowest cost mainframe computer system.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nVarious sites list documents by Charles Lasner, the creator of the alt.sys.pdp8 discussion group, and related documents by various members of the alt.sys.pdp8 readership with even more authoritative information about the various models, especially detailed focus upon the various members of the PDP-8 \"family\" of computers both made and not made by DEC."}
{"id": "23631", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=23631", "title": "Primary mirror", "text": "Main light-gathering source of reflecting telescope\nA primary mirror (or primary) is the principal light-gathering surface (the objective) of a reflecting telescope.\nDescription.\nThe primary mirror of a reflecting telescope is a spherical, parabolic, or hyperbolic shaped disks of polished reflective metal (speculum metal up to the mid 19th century), or in later telescopes, glass or other material coated with a reflective layer. One of the first known reflecting telescopes, Newton's reflector of 1668, used a 3.3\u00a0cm polished metal primary mirror. The next major change was to use silver on glass rather than metal, in the 19th century such was with the Crossley reflector. This was changed to vacuum deposited aluminum on glass, used on the 200-inch Hale telescope.\nSolid primary mirrors have to sustain their own weight and not deform under gravity, which limits the maximum size for a single piece primary mirror.\nSegmented mirror configurations are used to get around the size limitation on single primary mirrors. For example, the Giant Magellan Telescope will have seven 8.4 meter primary mirrors, with the resolving power equivalent to a optical aperture.\nSuperlative primary mirrors.\nThe largest optical telescope in the world as of 2009 to use a non-segmented single-mirror as its primary mirror is the Subaru telescope of the National Astronomical Observatory of Japan, located in Mauna Kea Observatory on Hawaii since 1997; however, this is not the largest diameter single mirror in a telescope, the U.S./German/Italian Large Binocular Telescope has two mirrors (which can be used together for interferometric mode). Both of these are smaller than the 10 m segmented primary mirrors on the dual Keck telescope. The Hubble Space Telescope has a primary mirror.\nRadio and submillimeter telescopes use much larger dishes or antennae, which do not have to be made as precisely as the mirrors used in optical telescopes. The Arecibo Telescope used a 305 m dish, which was the world largest single-dish radio telescope fixed to the ground. The Green Bank Telescope has the world's largest steerable single radio dish with 100 m in diameter. There are larger radio arrays, composed of multiple dishes which have better image resolution but less sensitivity.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23632", "revid": "4899266", "url": "https://en.wikipedia.org/wiki?curid=23632", "title": "Platonic idealism", "text": ""}
{"id": "23633", "revid": "50697145", "url": "https://en.wikipedia.org/wiki?curid=23633", "title": "List of physicists", "text": "Following is a list of physicists who are notable for their achievements.\n&lt;templatestyles src=\"TOC top/styles.css\"/&gt;\n&lt;templatestyles src=\"Hlist/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23634", "revid": "38205273", "url": "https://en.wikipedia.org/wiki?curid=23634", "title": "Protein", "text": "Biomolecule consisting of chains of amino acid residues\nProteins are large biomolecules and macromolecules that comprise one or more long chains of amino acid residues. Proteins perform a vast array of functions within organisms, including catalysing metabolic reactions, DNA replication, responding to stimuli, providing structure to cells and organisms, and transporting molecules from one location to another. Proteins differ from one another primarily in their sequence of amino acids, which is dictated by the nucleotide sequence of their genes, and which usually results in protein folding into a specific 3D structure that determines its activity.\nA linear chain of amino acid residues is called a polypeptide. A protein contains at least one long polypeptide. Short polypeptides, containing less than 20\u201330 residues, are rarely considered to be proteins and are commonly called peptides. The individual amino acid residues are bonded together by peptide bonds and adjacent amino acid residues. The sequence of amino acid residues in a protein is defined by the sequence of a gene, which is encoded in the genetic code. In general, the genetic code specifies 20 standard amino acids; but in certain organisms the genetic code can include selenocysteine and\u2014in certain archaea\u2014pyrrolysine. Shortly after or even during synthesis, the residues in a protein are often chemically modified by post-translational modification, which alters the physical and chemical properties, folding, stability, activity, and ultimately, the function of the proteins. Some proteins have non-peptide groups attached, which can be called prosthetic groups or cofactors. Proteins can work together to achieve a particular function, and they often associate to form stable protein complexes.\nOnce formed, proteins only exist for a certain period and are then degraded and recycled by the cell's machinery through the process of protein turnover. A protein's lifespan is measured in terms of its half-life and covers a wide range. They can exist for minutes or years with an average lifespan of 1\u20132 days in mammalian cells. Abnormal or misfolded proteins are degraded more rapidly either due to being targeted for destruction or due to being unstable.\nLike other biological macromolecules such as polysaccharides and nucleic acids, proteins are essential parts of organisms and participate in virtually every process within cells. Many proteins are enzymes that catalyse biochemical reactions and are vital to metabolism. Some proteins have structural or mechanical functions, such as actin and myosin in muscle, and the cytoskeleton's scaffolding proteins that maintain cell shape. Other proteins are important in cell signaling, immune responses, cell adhesion, and the cell cycle. In animals, proteins are needed in the diet to provide the essential amino acids that cannot be synthesized. Digestion breaks the proteins down for metabolic use.\nHistory and etymology.\nDiscovery and early studies.\nProteins have been studied and recognized since the 1700s by Antoine Fourcroy and others, who often collectively called them \"albumins\", or \"albuminous materials\" (\"Eiweissk\u00f6rper\", in German). Gluten, for example, was first separated from wheat in published research around 1747, and later determined to exist in many plants. In 1789, Antoine Fourcroy recognized three distinct varieties of animal proteins: albumin, fibrin, and gelatin. Vegetable (plant) proteins studied in the late 1700s and early 1800s included gluten, plant albumin, gliadin, and legumin.\nProteins were first described by the Dutch chemist Gerardus Johannes Mulder and named by the Swedish chemist J\u00f6ns Jacob Berzelius in 1838. Mulder carried out elemental analysis of common proteins and found that nearly all proteins had the same empirical formula, C400H620N100O120P1S1. He came to the erroneous conclusion that they might be composed of a single type of (very large) molecule. The term \"protein\" to describe these molecules was proposed by Mulder's associate Berzelius; protein is derived from the Greek word (), meaning \"primary\", \"in the lead\", or \"standing in front\", + \"-in\". Mulder went on to identify the products of protein degradation such as the amino acid leucine for which he found a (nearly correct) molecular weight of .\nEarly nutritional scientists such as the German Carl von Voit believed that protein was the most important nutrient for maintaining the structure of the body, because it was generally believed that \"flesh makes flesh\". Around 1862, Karl Heinrich Ritthausen isolated the amino acid glutamic acid. Thomas Burr Osborne compiled a detailed review of the vegetable proteins at the Connecticut Agricultural Experiment Station. Osborne, alongside Lafayette Mendel, established several nutritionally essential amino acids in feeding experiments with laboratory rats. Diets lacking an essential amino acid stunts the rats' growth, consistent with Liebig's law of the minimum. The final essential amino acid to be discovered, threonine, was identified by William Cumming Rose.\nThe difficulty in purifying proteins impeded work by early protein biochemists. Proteins could be obtained in large quantities from blood, egg whites, and keratin, but individual proteins were unavailable. In the 1950s, the Armour Hot Dog Company purified 1\u00a0kg of bovine pancreatic ribonuclease A and made it freely available to scientists. This gesture helped ribonuclease A become a major target for biochemical study for the following decades.\nPolypeptides.\nThe understanding of proteins as polypeptides, or chains of amino acids, came through the work of Franz Hofmeister and Hermann Emil Fischer in 1902. The central role of proteins as enzymes in living organisms that catalyzed reactions was not fully appreciated until 1926, when James B. Sumner showed that the enzyme urease was in fact a protein.\nLinus Pauling is credited with the successful prediction of regular protein secondary structures based on hydrogen bonding, an idea first put forth by William Astbury in 1933. Later work by Walter Kauzmann on denaturation, based partly on previous studies by Kaj Linderstr\u00f8m-Lang, contributed an understanding of protein folding and structure mediated by hydrophobic interactions.\nThe first protein to have its amino acid chain sequenced was insulin, by Frederick Sanger, in 1949. Sanger correctly determined the amino acid sequence of insulin, thus conclusively demonstrating that proteins consisted of linear polymers of amino acids rather than branched chains, colloids, or cyclols. He won the Nobel Prize for this achievement in 1958. Christian Anfinsen's studies of the oxidative folding process of ribonuclease A, for which he won the nobel prize in 1972, solidified the thermodynamic hypothesis of protein folding, according to which the folded form of a protein represents its free energy minimum.\nStructure.\nWith the development of X-ray crystallography, it became possible to determine protein structures as well as their sequences. The first protein structures to be solved were hemoglobin by Max Perutz and myoglobin by John Kendrew, in 1958. The use of computers and increasing computing power has supported the sequencing of complex proteins. In 1999, Roger Kornberg sequenced the highly complex structure of RNA polymerase using high intensity X-rays from synchrotrons.\nSince then, cryo-electron microscopy (cryo-EM) of large macromolecular assemblies has been developed. Cryo-EM uses protein samples that are frozen rather than crystals, and beams of electrons rather than X-rays. It causes less damage to the sample, allowing scientists to obtain more information and analyze larger structures. Computational protein structure prediction of small protein structural domains has helped researchers to approach atomic-level resolution of protein structures.\nAs of April 2024[ [update]], the Protein Data Bank contains 181,018 X-ray, 19,809 EM and 12,697 NMR protein structures.\nClassification.\nProteins are primarily classified by sequence and structure, although other classifications are commonly used. Especially for enzymes the EC number system provides a functional classification scheme. Similarly, gene ontology classifies both genes and proteins by their biological and biochemical function, and by their intracellular location.\nSequence similarity is used to classify proteins both in terms of evolutionary and functional similarity. This may use either whole proteins or protein domains, especially in multi-domain proteins. Protein domains allow protein classification by a combination of sequence, structure and function, and they can be combined in many ways. In an early study of 170,000 proteins, about two-thirds were assigned at least one domain, with larger proteins containing more domains (e.g. proteins larger than 600 amino acids having an average of more than 5 domains).\nBiochemistry.\nMost proteins consist of linear polymers built from series of up to 20 -\u03b1-amino acids. All proteinogenic amino acids have a common structure where an \u03b1-carbon is bonded to an amino group, a carboxyl group, and a variable side chain. Only proline differs from this basic structure as its side chain is cyclical, bonding to the amino group, limiting protein chain flexibility. The side chains of the standard amino acids have a variety of chemical structures and properties, and it is the combined effect of all amino acids that determines its three-dimensional structure and chemical reactivity.\nThe amino acids in a polypeptide chain are linked by peptide bonds between amino and carboxyl group. An individual amino acid in a chain is called a \"residue\", and the linked series of carbon, nitrogen, and oxygen atoms are known as the \"main chain\" or \"protein backbone\". The peptide bond has two resonance forms that confer some double-bond character to the backbone. The alpha carbons are roughly coplanar with the nitrogen and the carbonyl (C=O) group. The other two dihedral angles in the peptide bond determine the local shape assumed by the protein backbone. One consequence of the N-C(O) double bond character is that proteins are somewhat rigid. A polypeptide chain ends with a free amino group, known as the \"N-terminus\" or \"amino terminus\", and a free carboxyl group, known as the \"C-terminus\" or \"carboxy terminus\". By convention, peptide sequences are written N-terminus to C-terminus, correlating with the order in which proteins are synthesized by ribosomes.\nThe words \"protein\", \"polypeptide\", and \"peptide\" are a little ambiguous and can overlap in meaning. \"Protein\" is generally used to refer to the complete biological molecule in a stable conformation, whereas \"peptide\" is generally reserved for a short amino acid oligomers often lacking a stable 3D structure. But the boundary between the two is not well defined and usually lies near 20\u201330 residues.\nProteins can interact with many types of molecules and ions, including with other proteins, with lipids, with carbohydrates, and with DNA.\nAbundance in cells.\nA typical bacterial cell, e.g. \"E. coli\" and \"Staphylococcus aureus\", is estimated to contain about 2 million proteins. Smaller bacteria, such as \"Mycoplasma\" or \"spirochetes\" contain fewer molecules, on the order of 50,000 to 1 million. By contrast, eukaryotic cells are larger and thus contain much more protein. For instance, yeast cells have been estimated to contain about 50 million proteins and human cells on the order of 1 to 3 billion. The concentration of individual protein copies ranges from a few molecules per cell up to 20 million. Not all genes coding proteins are expressed in most cells and their number depends on, for example, cell type and external stimuli. For instance, of the 20,000 or so proteins encoded by the human genome, only 6,000 are detected in lymphoblastoid cells. The most abundant protein in nature is thought to be RuBisCO, an enzyme that catalyzes the incorporation of carbon dioxide into organic matter in photosynthesis. Plants can consist of as much as 1% by weight of this enzyme.\nSynthesis.\nBiosynthesis.\nProteins are assembled from amino acids using information encoded in genes. Each protein has its own unique amino acid sequence that is specified by the nucleotide sequence of the gene encoding this protein. The genetic code is a set of three-nucleotide sets called codons and each three-nucleotide combination designates an amino acid, for example AUG (adenine\u2013uracil\u2013guanine) is the code for methionine. Because DNA contains four nucleotides, the total number of possible codons is 64; hence, there is some redundancy in the genetic code, with some amino acids specified by more than one codon. Genes encoded in DNA are first transcribed into pre-messenger RNA (mRNA) by proteins such as RNA polymerase. Most organisms then process the pre-mRNA (a \"primary transcript\") using various forms of post-transcriptional modification to form the mature mRNA, which is then used as a template for protein synthesis by the ribosome. In prokaryotes the mRNA may either be used as soon as it is produced, or be bound by a ribosome after having moved away from the nucleoid. In contrast, eukaryotes make mRNA in the cell nucleus and then translocate it across the nuclear membrane into the cytoplasm, where protein synthesis then takes place. The rate of protein synthesis is higher in prokaryotes than eukaryotes and can reach up to 20 amino acids per second.\nThe process of synthesizing a protein from an mRNA template is known as translation. The mRNA is loaded onto the ribosome and is read three nucleotides at a time by matching each codon to its base pairing anticodon located on a transfer RNA molecule, which carries the amino acid corresponding to the codon it recognizes. The enzyme aminoacyl tRNA synthetase \"charges\" the tRNA molecules with the correct amino acids. The growing polypeptide is often termed the \"nascent chain\". Proteins are always biosynthesized from N-terminus to C-terminus.\nThe size of a synthesized protein can be measured by the number of amino acids it contains and by its total molecular mass, which is normally reported with the unit \"dalton\" (Da), or its derivative unit kilodalton (kDa). The average size of a protein increases from Archaea to Bacteria to Eukaryote (283, 311, 438 residues and 31, 34, 49\u00a0kDa respectively) due to a bigger number of protein domains constituting proteins in higher organisms. For instance, yeast proteins are on average 466 amino acids long and 53 kDa in mass. The largest known proteins are the titins, a component of the muscle sarcomere, with a molecular mass of almost and a total length of almost amino acids.\nChemical synthesis.\nShort proteins can be synthesized chemically by a family of peptide synthesis methods. These rely on organic synthesis techniques such as chemical ligation to produce peptides in high yield. Chemical synthesis allows for the introduction of non-natural amino acids into polypeptide chains, such as attachment of fluorescent probes to amino acid side chains. These methods are useful in laboratory biochemistry and cell biology, though generally not for commercial applications. Chemical synthesis is inefficient for polypeptides longer than about 300 amino acids, and the synthesized proteins may not readily assume their native tertiary structure. Most chemical synthesis methods proceed from C-terminus to N-terminus, opposite the biological reaction.\nStructure.\nMost proteins fold into unique 3D structures. The shape into which a protein naturally folds is known as its native conformation. Although many proteins can fold unassisted, simply through the chemical properties of their amino acids, others require the aid of molecular chaperones to fold into their native states. Biochemists often refer to four distinct aspects of a protein's structure:\nProteins are not entirely rigid molecules. In addition to these levels of structure, proteins may shift between several related structures while they perform their functions. In the context of these functional rearrangements, these tertiary or quaternary structures are usually referred to as \"conformations\", and transitions between them are called \"conformational changes\". Such changes are often induced by the binding of a substrate molecule to an enzyme's active site, or the physical region of the protein that participates in chemical catalysis. In solution, protein structures vary because of thermal vibration and collisions with other molecules.\nProteins can be informally divided into three main classes, which correlate with typical tertiary structures: globular proteins, fibrous proteins, and membrane proteins. Almost all globular proteins are soluble and many are enzymes. Fibrous proteins are often structural, such as collagen, the major component of connective tissue, or keratin, the protein component of hair and nails. Membrane proteins often serve as receptors or provide channels for polar or charged molecules to pass through the cell membrane.\nA special case of intramolecular hydrogen bonds within proteins, poorly shielded from water attack and hence promoting their own dehydration, are called dehydrons.\nProtein domains.\nMany proteins are composed of several protein domains, i.e. segments of a protein that fold into distinct structural units. Domains usually have specific functions, such as enzymatic activities (e.g. kinase) or they serve as binding modules.\nSequence motif.\nShort amino acid sequences within proteins often act as recognition sites for other proteins. For instance, SH3 domains typically bind to short PxxP motifs (i.e. 2 prolines [P], separated by two unspecified amino acids [x], although the surrounding amino acids may determine the exact binding specificity). Many such motifs has been collected in the Eukaryotic Linear Motif (ELM) database.\nCellular functions.\nProteins are the chief actors within the cell, said to be carrying out the duties specified by the information encoded in genes. With the exception of certain types of RNA, most other biological molecules are relatively inert elements upon which proteins act. Proteins make up half the dry weight of an \"Escherichia coli\" cell, whereas other macromolecules such as DNA and RNA make up only 3% and 20%, respectively. The set of proteins expressed in a particular cell or cell type is known as its proteome.\nThe chief characteristic of proteins that allows their diverse set of functions is their ability to bind other molecules specifically and tightly. The region of the protein responsible for binding another molecule is known as the binding site and is often a depression or \"pocket\" on the molecular surface. This binding ability is mediated by the tertiary structure of the protein, which defines the binding site pocket, and by the chemical properties of the surrounding amino acids' side chains. Protein binding can be extraordinarily tight and specific; for example, the ribonuclease inhibitor protein binds to human angiogenin with a sub-femtomolar dissociation constant (&lt;\u00a010\u221215\u00a0M) but does not bind at all to its amphibian homolog onconase (&gt;\u00a01\u00a0M). Extremely minor chemical changes such as the addition of a single methyl group to a binding partner can sometimes suffice to nearly eliminate binding; for example, the aminoacyl tRNA synthetase specific to the amino acid valine discriminates against the very similar side chain of the amino acid isoleucine.\nProteins can bind to other proteins as well as to small-molecule substrates. When proteins bind specifically to other copies of the same molecule, they can oligomerize to form fibrils; this process occurs often in structural proteins that consist of globular monomers that self-associate to form rigid fibers. Protein\u2013protein interactions regulate enzymatic activity, control progression through the cell cycle, and allow the assembly of large protein complexes that carry out many closely related reactions with a common biological function. Proteins can bind to, or be integrated into, cell membranes. The ability of binding partners to induce conformational changes in proteins allows the construction of enormously complex signaling networks.\nAs interactions between proteins are reversible and depend heavily on the availability of different groups of partner proteins to form aggregates that are capable to carry out discrete sets of function, study of the interactions between specific proteins is a key to understand important aspects of cellular function, and ultimately the properties that distinguish particular cell types.\nEnzymes.\nThe best-known role of proteins in the cell is as enzymes, which catalyse chemical reactions. Enzymes are usually highly specific and accelerate only one or a few chemical reactions. Enzymes carry out most of the reactions involved in metabolism, as well as manipulating DNA in processes such as DNA replication, DNA repair, and transcription. Some enzymes act on other proteins to add or remove chemical groups in a process known as posttranslational modification. About 4,000 reactions are known to be catalysed by enzymes. The rate acceleration conferred by enzymatic catalysis is often enormous\u2014as much as 1017-fold increase in rate over the uncatalysed reaction in the case of orotate decarboxylase (78 million years without the enzyme, 18 milliseconds with the enzyme).\nThe molecules bound and acted upon by enzymes are called substrates. Although enzymes can consist of hundreds of amino acids, it is usually only a small fraction of the residues that come in contact with the substrate, and an even smaller fraction\u2014three to four residues on average\u2014that are directly involved in catalysis. The region of the enzyme that binds the substrate and contains the catalytic residues is known as the active site.\nDirigent proteins are members of a class of proteins that dictate the stereochemistry of a compound synthesized by other enzymes.\nCell signaling and ligand binding.\nMany proteins are involved in the process of cell signaling and signal transduction. Some proteins, such as insulin, are extracellular proteins that transmit a signal from the cell in which they were synthesized to other cells in distant tissues. Others are membrane proteins that act as receptors whose main function is to bind a signaling molecule and induce a biochemical response in the cell. Many receptors have a binding site exposed on the cell surface and an effector domain within the cell, which may have enzymatic activity or may undergo a conformational change detected by other proteins within the cell.\nAntibodies are protein components of an adaptive immune system whose main function is to bind antigens, or foreign substances in the body, and target them for destruction. Antibodies can be secreted into the extracellular environment or anchored in the membranes of specialized B cells known as plasma cells. Whereas enzymes are limited in their binding affinity for their substrates by the necessity of conducting their reaction, antibodies have no such constraints. An antibody's binding affinity to its target is extraordinarily high.\nMany ligand transport proteins bind particular small biomolecules and transport them to other locations in the body of a multicellular organism. These proteins must have a high binding affinity when their ligand is present in high concentrations, and release the ligand when it is present at low concentrations in the target tissues. The canonical example of a ligand-binding protein is haemoglobin, which transports oxygen from the lungs to other organs and tissues in all vertebrates and has close homologs in every biological kingdom. Lectins are sugar-binding proteins which are highly specific for their sugar moieties. Lectins typically play a role in biological recognition phenomena involving cells and proteins. Receptors and hormones are highly specific binding proteins.\nTransmembrane proteins can serve as ligand transport proteins that alter the permeability of the cell membrane to small molecules and ions. The membrane alone has a hydrophobic core through which polar or charged molecules cannot diffuse. Membrane proteins contain internal channels that allow such molecules to enter and exit the cell. Many ion channel proteins are specialized to select for only a particular ion; for example, potassium and sodium channels often discriminate for only one of the two ions.\nStructural proteins.\nStructural proteins confer stiffness and rigidity to otherwise-fluid biological components. Most structural proteins are fibrous proteins; for example, collagen and elastin are critical components of connective tissue such as cartilage, and keratin is found in hard or filamentous structures such as hair, nails, feathers, hooves, and some animal shells. Some globular proteins can play structural functions, for example, actin and tubulin are globular and soluble as monomers, but polymerize to form long, stiff fibers that make up the cytoskeleton, which allows the cell to maintain its shape and size.\nOther proteins that serve structural functions are motor proteins such as myosin, kinesin, and dynein, which are capable of generating mechanical forces. These proteins are crucial for cellular motility of single celled organisms and the sperm of many multicellular organisms which reproduce sexually. They generate the forces exerted by contracting muscles and play essential roles in intracellular transport.\nMethods of study.\nMethods commonly used to study protein structure and function include immunohistochemistry, site-directed mutagenesis, X-ray crystallography, nuclear magnetic resonance and mass spectrometry. The activities and structures of proteins may be examined \"in vitro\", \"in vivo, and in silico\". In vitro studies of purified proteins in controlled environments are useful for learning how a protein carries out its function: for example, enzyme kinetics studies explore the chemical mechanism of an enzyme's catalytic activity and its relative affinity for various possible substrate molecules. By contrast, in vivo experiments can provide information about the physiological role of a protein in the context of a cell or even a whole organism, and can often provide more information about protein behavior in different contexts. In silico studies use computational methods to study proteins.\nProtein purification.\nProteins may be purified from other cellular components using a variety of techniques such as ultracentrifugation, precipitation, electrophoresis, and chromatography; the advent of genetic engineering has made possible a number of methods to facilitate purification.\nTo perform \"in vitro\" analysis, a protein must be purified away from other cellular components. This process usually begins with cell lysis, in which a cell's membrane is disrupted and its internal contents released into a solution known as a crude lysate. The resulting mixture can be purified using ultracentrifugation, which fractionates the various cellular components into fractions containing soluble proteins; membrane lipids and proteins; cellular organelles, and nucleic acids. Precipitation by a method known as salting out can concentrate the proteins from this lysate. Various types of chromatography are then used to isolate the protein or proteins of interest based on properties such as molecular weight, net charge and binding affinity. The level of purification can be monitored using various types of gel electrophoresis if the desired protein's molecular weight and isoelectric point are known, by spectroscopy if the protein has distinguishable spectroscopic features, or by enzyme assays if the protein has enzymatic activity. Additionally, proteins can be isolated according to their charge using electrofocusing.\nFor natural proteins, a series of purification steps may be necessary to obtain protein sufficiently pure for laboratory applications. To simplify this process, genetic engineering is often used to add chemical features to proteins that make them easier to purify without affecting their structure or activity. Here, a \"tag\" consisting of a specific amino acid sequence, often a series of histidine residues (a \"His-tag\"), is attached to one terminus of the protein. As a result, when the lysate is passed over a chromatography column containing nickel, the histidine residues ligate the nickel and attach to the column while the untagged components of the lysate pass unimpeded. A number of tags have been developed to help researchers purify specific proteins from complex mixtures.\nCellular localization.\nThe study of proteins \"in vivo\" is often concerned with the synthesis and localization of the protein within the cell. Although many intracellular proteins are synthesized in the cytoplasm and membrane-bound or secreted proteins in the endoplasmic reticulum, the specifics of how proteins are targeted to specific organelles or cellular structures is often unclear. A useful technique for assessing cellular localization uses genetic engineering to express in a cell a fusion protein or chimera consisting of the natural protein of interest linked to a \"reporter\" such as green fluorescent protein (GFP). The fused protein's position within the cell can then be cleanly and efficiently visualized using microscopy.\nOther methods for elucidating the cellular location of proteins requires the use of known compartmental markers for regions such as the ER, the Golgi, lysosomes or vacuoles, mitochondria, chloroplasts, plasma membrane, etc. With the use of fluorescently tagged versions of these markers or of antibodies to known markers, it becomes much simpler to identify the localization of a protein of interest. For example, indirect immunofluorescence will allow for fluorescence colocalization and demonstration of location. Fluorescent dyes are used to label cellular compartments for a similar purpose.\nOther possibilities exist, as well. For example, immunohistochemistry usually uses an antibody to one or more proteins of interest that are conjugated to enzymes yielding either luminescent or chromogenic signals that can be compared between samples, allowing for localization information. Another applicable technique is cofractionation in sucrose (or other material) gradients using isopycnic centrifugation. While this technique does not prove colocalization of a compartment of known density and the protein of interest, it indicates an increased likelihood.\nFinally, the gold-standard method of cellular localization is immunoelectron microscopy. This technique uses an antibody to the protein of interest, along with classical electron microscopy techniques. The sample is prepared for normal electron microscopic examination, and then treated with an antibody to the protein of interest that is conjugated to an extremely electro-dense material, usually gold. This allows for the localization of both ultrastructural details as well as the protein of interest.\nThrough another genetic engineering application known as site-directed mutagenesis, researchers can alter the protein sequence and hence its structure, cellular localization, and susceptibility to regulation. This technique even allows the incorporation of unnatural amino acids into proteins, using modified tRNAs, and may allow the rational design of new proteins with novel properties.\nProteomics.\nThe total complement of proteins present at a time in a cell or cell type is known as its proteome, and the study of such large-scale data sets defines the field of proteomics, named by analogy to the related field of genomics. Key experimental techniques in proteomics include 2D electrophoresis, which allows the separation of many proteins, mass spectrometry, which allows rapid high-throughput identification of proteins and sequencing of peptides (most often after in-gel digestion), protein microarrays, which allow the detection of the relative levels of the various proteins present in a cell, and two-hybrid screening, which allows the systematic exploration of protein\u2013protein interactions. The total complement of biologically possible such interactions is known as the interactome. A systematic attempt to determine the structures of proteins representing every possible fold is known as structural genomics.\nStructure determination.\nDiscovering the tertiary structure of a protein, or the quaternary structure of its complexes, can provide important clues about how the protein performs its function and how it can be affected, i.e. in drug design. As proteins are too small to be seen under a light microscope, other methods have to be employed to determine their structure. Common experimental methods include X-ray crystallography and NMR spectroscopy, both of which can produce structural information at atomic resolution. However, NMR experiments are able to provide information from which a subset of distances between pairs of atoms can be estimated, and the final possible conformations for a protein are determined by solving a distance geometry problem. Dual polarisation interferometry is a quantitative analytical method for measuring the overall protein conformation and conformational changes due to interactions or other stimulus. Circular dichroism is another laboratory technique for determining internal \u03b2-sheet / \u03b1-helical composition of proteins. Cryoelectron microscopy is used to produce lower-resolution structural information about very large protein complexes, including assembled viruses; a variant known as electron crystallography can produce high-resolution information in some cases, especially for two-dimensional crystals of membrane proteins. Solved structures are usually deposited in the Protein Data Bank (PDB), a freely available resource from which structural data about thousands of proteins can be obtained in the form of Cartesian coordinates for each atom in the protein.\nMany more gene sequences are known than protein structures. Further, the set of solved structures is biased toward proteins that can be easily subjected to the conditions required in X-ray crystallography, one of the major structure determination methods. In particular, globular proteins are comparatively easy to crystallize in preparation for X-ray crystallography. Membrane proteins and large protein complexes, by contrast, are difficult to crystallize and are underrepresented in the PDB. Structural genomics initiatives have attempted to remedy these deficiencies by systematically solving representative structures of major fold classes. Protein structure prediction methods attempt to provide a means of generating a plausible structure for proteins whose structures have not been experimentally determined.\nStructure prediction.\nComplementary to the field of structural genomics, \"protein structure prediction\" develops efficient mathematical models of proteins to computationally predict the molecular formations in theory, instead of detecting structures with laboratory observation. The most successful type of structure prediction, known as homology modeling, relies on the existence of a \"template\" structure with sequence similarity to the protein being modeled; structural genomics' goal is to provide sufficient representation in solved structures to model most of those that remain. Although producing accurate models remains a challenge when only distantly related template structures are available, it has been suggested that sequence alignment is the bottleneck in this process, as quite accurate models can be produced if a \"perfect\" sequence alignment is known. Many structure prediction methods have served to inform the emerging field of protein engineering, in which novel protein folds have already been designed. Many proteins (in eukaryotes ~33%) contain large unstructured but biologically functional segments and can be classified as intrinsically disordered proteins. Predicting and analysing protein disorder is an important part of protein structure characterisation.\nIn silico simulation of dynamical processes.\nA more complex computational problem is the prediction of intermolecular interactions, such as in molecular docking, protein folding, protein\u2013protein interaction and chemical reactivity. Mathematical models to simulate these dynamical processes involve molecular mechanics, in particular, molecular dynamics. In this regard, \"in silico\" simulations discovered the folding of small \u03b1-helical protein domains such as the villin headpiece, the HIV accessory protein and hybrid methods combining standard molecular dynamics with quantum mechanical mathematics have explored the electronic states of rhodopsins.\nBeyond classical molecular dynamics, quantum dynamics methods allow the simulation of proteins in atomistic detail with an accurate description of quantum mechanical effects. Examples include the multi-layer multi-configuration time-dependent Hartree method and the hierarchical equations of motion approach, which have been applied to plant cryptochromes and bacteria light-harvesting complexes, respectively. Both quantum and classical mechanical simulations of biological-scale systems are extremely computationally demanding, so distributed computing initiatives such as the Folding@home project facilitate the molecular modeling by exploiting advances in GPU parallel processing and Monte Carlo techniques.\nChemical analysis.\nThe total nitrogen content of organic matter is mainly formed by the amino groups in proteins. The total Kjeldahl nitrogen (TKN) is a measure of nitrogen widely used in the analysis of (waste) water, soil, food, feed and organic matter in general. As the name suggests, the Kjeldahl method is applied. More sensitive methods are available.\nDigestion.\nIn the absence of catalysts, proteins are slow to hydrolyze. The breakdown of proteins to small peptides and amino acids (proteolysis) is a step in digestion; these breakdown products are then absorbed in the small intestine. The hydrolysis of proteins relies on enzymes called proteases or peptidases. Proteases, which are themselves proteins, come in several types according to the particular peptide bonds that they cleave as well as their tendency to cleave peptide bonds at the terminus of a protein (exopeptidases) vs peptide bonds at the interior of the protein (endopeptidases). Pepsin is an endopeptidase in the stomach. Subsequent to the stomach, the pancreas secretes other proteases to complete the hydrolysis, these include trypsin and chymotrypsin.\nProtein hydrolysis is employed commercially as a means of producing amino acids from bulk sources of protein, such as blood meal, feathers, keratin. Such materials are treated with hot hydrochloric acid, which effects the hydrolysis of the peptide bonds.\nMechanical properties.\nThe mechanical properties of proteins are highly diverse and are often central to their biological function, as in the case of proteins like keratin and collagen. For instance, the ability of muscle tissue to continually expand and contract is directly tied to the elastic properties of their underlying protein makeup. Beyond fibrous proteins, the conformational dynamics of enzymes and the structure of biological membranes, among other biological functions, are governed by the mechanical properties of the proteins. Outside of their biological context, the unique mechanical properties of many proteins, along with their relative sustainability when compared to synthetic polymers, have made them desirable targets for next-generation materials design.\nYoung's modulus, \"E\", is calculated as the axial stress \"\u03c3\" over the resulting strain \"\u03b5\". It is a measure of the relative stiffness of a material. In the context of proteins, this stiffness often directly correlates to biological function. For example, collagen, found in connective tissue, bones, and cartilage, and keratin, found in nails, claws, and hair, have observed stiffnesses that are several orders of magnitude higher than that of elastin, which is thought to give elasticity to structures such as blood vessels, pulmonary tissue, and bladder tissue, among others. In comparison to this, globular proteins, such as Bovine Serum Albumin, which float relatively freely in the cytosol and often function as enzymes (and thus undergoing frequent conformational changes) have comparably much lower Young's moduli.\nThe Young's modulus of a single protein can be found through molecular dynamics simulation. Using either atomistic force-fields, such as CHARMM or GROMOS, or coarse-grained forcefields like Martini, a single protein molecule can be stretched by a uniaxial force while the resulting extension is recorded in order to calculate the strain. Experimentally, methods such as atomic force microscopy can be used to obtain similar data. The internal dynamics of proteins involve subtle elastic and plastic deformations induced by viscoelastic forces, which can be probed by nano-rheology techniques. These estimates yield typical spring constants around \"k\" \u2248 100\u00a0pN/nm, equivalent to Young's moduli of \"E\" \u2248 100\u00a0MPa, and typical friction coefficients of \"\u03b3\" \u2248 0.1\u00a0pN\u00b7s/nm, corresponding to viscosity of \"\u03b7\" \u2248 0.01\u00a0pN\u00b7s/nm2 = 107\u00a0cP (that is, 107 times more viscous than water). \nAt the macroscopic level, the Young's modulus of cross-linked protein networks can be obtained through more traditional mechanical testing. Experimentally observed values for a few proteins can be seen below.\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23635", "revid": "2952402", "url": "https://en.wikipedia.org/wiki?curid=23635", "title": "Physical chemistry", "text": "Physics applied to chemical systems\nPhysical chemistry is the study of macroscopic and microscopic phenomena in chemical systems in terms of the principles, practices, and concepts of physics such as motion, energy, force, time, thermodynamics, quantum chemistry, statistical mechanics, analytical dynamics and chemical equilibria.\nPhysical chemistry, in contrast to chemical physics, is predominantly (but not always) a supra-molecular science, as the majority of the principles on which it was founded relate to the bulk rather than the molecular or atomic structure alone (for example, chemical equilibrium and colloids).\nSome of the relationships that physical chemistry strives to understand include the effects of:\nKey concepts.\nThe key concepts of physical chemistry are the ways in which pure physics is applied to chemical problems.\nOne of the key concepts in classical chemistry is that all chemical compounds can be described as groups of atoms bonded together and chemical reactions can be described as the making and breaking of those bonds. Predicting the properties of chemical compounds from a description of atoms and how they bond is one of the major goals of physical chemistry. To describe the atoms and bonds precisely, it is necessary to know both where the nuclei of the atoms are, and how electrons are distributed around them.\nDisciplines.\nQuantum chemistry, a subfield of physical chemistry especially concerned with the application of quantum mechanics to chemical problems, provides tools to determine how strong and what shape bonds are, how nuclei move, and how light can be absorbed or emitted by a chemical compound. Spectroscopy is the related sub-discipline of physical chemistry which is specifically concerned with the interaction of electromagnetic radiation with matter.\nAnother set of important questions in chemistry concerns what kind of reactions can happen spontaneously and which properties are possible for a given chemical mixture. This is studied in chemical thermodynamics, which sets limits on quantities like how far a reaction can proceed, or how much energy can be converted into work in an internal combustion engine, and which provides links between properties like the thermal expansion coefficient and rate of change of entropy with pressure for a gas or a liquid. It can frequently be used to assess whether a reactor or engine design is feasible, or to check the validity of experimental data. To a limited extent, quasi-equilibrium and non-equilibrium thermodynamics can describe irreversible changes. However, classical thermodynamics is mostly concerned with systems in equilibrium and reversible changes and not what actually does happen, or how fast, away from equilibrium.\nWhich reactions do occur and how fast is the subject of chemical kinetics, another branch of physical chemistry. A key idea in chemical kinetics is that for reactants to react and form products, most chemical species must go through transition states which are higher in energy than either the reactants or the products and serve as a barrier to reaction. In general, the higher the barrier, the slower the reaction. A second is that most chemical reactions occur as a sequence of elementary reactions, each with its own transition state. Key questions in kinetics include how the rate of reaction depends on temperature and on the concentrations of reactants and catalysts in the reaction mixture, as well as how catalysts and reaction conditions can be engineered to optimize the reaction rate.\nThe fact that how fast reactions occur can often be specified with just a few concentrations and a temperature, instead of needing to know all the positions and speeds of every molecule in a mixture, is a special case of another key concept in physical chemistry, which is that to the extent an engineer needs to know, everything going on in a mixture of very large numbers (perhaps of the order of the Avogadro constant, 6 x 1023) of particles can often be described by just a few variables like pressure, temperature, and concentration. The precise reasons for this are described in statistical mechanics, a specialty within physical chemistry which is also shared with physics. Statistical mechanics also provides ways to predict the properties we see in everyday life from molecular properties without relying on empirical correlations based on chemical similarities.\nHistory.\nThe term \"physical chemistry\" was coined by Mikhail Lomonosov in 1752, when he presented a lecture course entitled \"A Course in True Physical Chemistry\" () before the students of Petersburg University. In the preamble to these lectures he gives the definition: \"Physical chemistry is the science that must explain under provisions of physical experiments the reason for what is happening in complex bodies through chemical operations\".\nModern physical chemistry originated in the 1860s to 1880s with work on chemical thermodynamics, electrolytes in solutions, chemical kinetics and other subjects. One milestone was the publication in 1876 by Josiah Willard Gibbs of his paper, \"On the Equilibrium of Heterogeneous Substances\". This paper introduced several of the cornerstones of physical chemistry, such as Gibbs energy, chemical potentials, and Gibbs' phase rule.\nThe first scientific journal specifically in the field of physical chemistry was the German journal, \"Zeitschrift f\u00fcr Physikalische Chemie\", founded in 1887 by Wilhelm Ostwald and Jacobus Henricus van 't Hoff. Together with Svante August Arrhenius, these were the leading figures in physical chemistry in the late 19th century and early 20th century. All three were awarded the Nobel Prize in Chemistry between 1901 and 1909.\nDevelopments in the following decades include the application of statistical mechanics to chemical systems and work on colloids and surface chemistry, where Irving Langmuir made many contributions. Another important step was the development of quantum mechanics into quantum chemistry from the 1930s, where Linus Pauling was one of the leading names. Theoretical developments have gone hand in hand with developments in experimental methods, where the use of different forms of spectroscopy, such as infrared spectroscopy, microwave spectroscopy, electron paramagnetic resonance and nuclear magnetic resonance spectroscopy, is probably the most important 20th century development.\nFurther development in physical chemistry may be attributed to discoveries in nuclear chemistry, especially in isotope separation (before and during World War II), more recent discoveries in astrochemistry, as well as the development of calculation algorithms in the field of \"additive physicochemical properties\" (practically all physicochemical properties, such as boiling point, critical point, surface tension, vapor pressure, etc.\u2014more than 20 in all\u2014can be precisely calculated from chemical structure alone, even if the chemical molecule remains unsynthesized), and herein lies the practical importance of contemporary physical chemistry.\nSee Group contribution method, Lydersen method, Joback method, Benson group increment theory, quantitative structure\u2013activity relationship\nJournals.\nSome journals that deal with physical chemistry include \nHistorical journals that covered both chemistry and physics include \"Annales de chimie et de physique\" (started in 1789, published under the name given here from 1815 to 1914).\nBranches and related topics.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23636", "revid": "47107812", "url": "https://en.wikipedia.org/wiki?curid=23636", "title": "Perimeter", "text": "Path that surrounds an area\nA perimeter is the length of a closed boundary that encompasses, surrounds, or outlines either a two-dimensional shape or a one-dimensional line. The perimeter of a circle or an ellipse is called its circumference.\nCalculating the perimeter has several practical applications. A calculated perimeter is the length of fence required to surround a yard or garden. The perimeter of a wheel/circle (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter; if the length of the string was exact, it would equal the perimeter.\nFormulas.\nThe perimeter is the distance around a shape. Perimeters for more general shapes can be calculated, as any path, with formula_1, where formula_2 is the length of the path and formula_3 is an infinitesimal line element. Both of these must be replaced by algebraic forms in order to be practically calculated. If the perimeter is given as a closed piecewise smooth plane curve formula_4 with\nformula_5 \nthen its length formula_2 can be computed as follows:\n formula_7\nA generalized notion of perimeter, which includes hypersurfaces bounding volumes in formula_8-dimensional Euclidean spaces, is described by the theory of Caccioppoli sets.\nPolygons.\nPolygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.\nThe perimeter of a polygon equals the sum of the lengths of its sides (edges). In particular, the perimeter of a rectangle of width formula_9 and length formula_10 equals formula_11\nAn equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.\nA regular polygon may be characterized by the number of its sides and by its circumradius, that is to say, the constant distance between its centre and each of its vertices. The length of its sides can be calculated using trigonometry. If \"R\" is a regular polygon's radius and \"n\" is the number of its sides, then its perimeter is \nformula_12\nA splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. The three splitters of a triangle all intersect each other at the Nagel point of the triangle.\nA cleaver of a triangle is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths. The three cleavers of a triangle all intersect each other at the triangle's Spieker center.\nCircumference of a circle.\nThe perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, \u03c0 (the Greek \"p\" for perimeter), such that if \"P\" is the circle's perimeter and \"D\" its diameter then,\nformula_13\nIn terms of the radius \"r\" of the circle, this formula becomes,\nformula_14\nTo calculate a circle's perimeter, knowledge of its radius or diameter and the number \u03c0 suffices. The problem is that \u03c0 is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of \u03c0 is important in the calculation. The computation of the digits of \u03c0 is relevant to many fields, such as mathematical analysis, algorithmics and computer science.\nPerception of perimeter.\nThe perimeter and the area are two main measures of geometric figures. Confusing them is a common error, as well as believing that the greater one of them is, the greater the other must be. Indeed, a commonplace observation is that an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/ scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by . The real area is 2 times the area of the shape on the map. Nevertheless, there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas are equal to 1.\nProclus (5th century) reported that Greek peasants \"fairly\" parted fields relying on their perimeters. However, a field's production is proportional to its area, not to its perimeter, so many naive peasants may have gotten fields with long perimeters but small areas (thus, few crops).\nIf one removes a piece from a figure, its area decreases but its perimeter may not. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. In the animated picture on the left, all the figures have the same convex hull; the big, first hexagon.\nIsoperimetry.\nThe isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive; it is the circle. In particular, this can be used to explain why drops of fat on a broth surface are circular.\nThis problem may seem simple, but its mathematical proof requires some sophisticated theorems. The isoperimetric problem is sometimes simplified by restricting the type of figures to be used. In particular, to find the quadrilateral, or the triangle, or another particular figure, with the largest area amongst those with the same shape having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with \"n\" sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is any irregular polygon with the same number of sides.\nEtymology.\nThe word comes from the Greek \u03c0\u03b5\u03c1\u03af\u03bc\u03b5\u03c4\u03c1\u03bf\u03c2 \"perimetros\", from \u03c0\u03b5\u03c1\u03af \"peri\" \"around\" and \u03bc\u03ad\u03c4\u03c1\u03bf\u03bd \"metron\" \"measure\".\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23637", "revid": "43056943", "url": "https://en.wikipedia.org/wiki?curid=23637", "title": "Phase (matter)", "text": "Region of uniform physical properties\nIn the physical sciences, a phase is a region of material that is chemically uniform, physically distinct, and (often) mechanically separable. In a system consisting of ice and water in a glass jar, the ice cubes are one phase, the water is a second phase, and the humid air is a third phase over the ice and water. The glass of the jar is a different material, in its own separate phase. (See .)\nMore precisely, a phase is a region of space (a thermodynamic system), throughout which all physical properties of a material are essentially uniform. Examples of physical properties include density, index of refraction, magnetization and chemical composition.\nThe term \"phase\" is sometimes used as a synonym for state of matter, but there can be several immiscible phases of the same state of matter (as where oil and water separate into distinct phases, both in the liquid state).\nTypes of phases.\nDistinct phases may be described as different states of matter such as gas, liquid, solid, plasma or Bose\u2013Einstein condensate. Useful mesophases between solid and liquid form other states of matter.\nDistinct phases may also exist within a given state of matter. As shown in the diagram for iron alloys, several phases exist for both the solid and liquid states. Phases may also be differentiated based on solubility as in polar (hydrophilic) or non-polar (hydrophobic). A mixture of water (a polar liquid) and oil (a non-polar liquid) will spontaneously separate into two phases. Water has a very low solubility (is insoluble) in oil, and oil has a low solubility in water. Solubility is the maximum amount of a solute that can dissolve in a solvent before the solute ceases to dissolve and remains in a separate phase. A mixture can separate into more than two liquid phases and the concept of phase separation extends to solids, i.e., solids can form solid solutions or crystallize into distinct crystal phases. Metal pairs that are mutually soluble can form alloys, whereas metal pairs that are mutually insoluble cannot.\nAs many as eight immiscible liquid phases have been observed. Mutually immiscible liquid phases are formed from water (aqueous phase), hydrophobic organic solvents, perfluorocarbons (fluorous phase), silicones, several different metals, and also from molten phosphorus. Not all organic solvents are completely miscible, e.g. a mixture of ethylene glycol and toluene may separate into two distinct organic phases.\nPhases do not need to macroscopically separate spontaneously. Emulsions and colloids are examples of immiscible phase pair combinations that do not physically separate.\nPhase equilibrium.\nLeft to equilibration, many compositions will form a uniform single phase, but depending on the temperature and pressure even a single substance may separate into two or more distinct phases. Within each phase, the properties are uniform but between the two phases properties differ.\nWater in a closed jar with an air space over it forms a two-phase system. Most of the water is in the liquid phase, where it is held by the mutual attraction of water molecules. Even at equilibrium molecules are constantly in motion and, once in a while, a molecule in the liquid phase gains enough kinetic energy to break away from the liquid phase and enter the gas phase. Likewise, every once in a while a vapor molecule collides with the liquid surface and condenses into the liquid. At equilibrium, evaporation and condensation processes exactly balance and there is no net change in the volume of either phase.\nAt room temperature and pressure, the water jar reaches equilibrium when the air over the water has a humidity of about 3%. This percentage increases as the temperature goes up. At 100\u00a0\u00b0C and atmospheric pressure, equilibrium is not reached until the air is 100% water. If the liquid is heated a little over 100\u00a0\u00b0C, the transition from liquid to gas will occur not only at the surface but throughout the liquid volume: the water boils.\nNumber of phases.\nFor a given composition, only certain phases are possible at a given temperature and pressure. The number and type of phases that will form is hard to predict and is usually determined by experiment. The results of such experiments can be plotted in phase diagrams.\nThe phase diagram shown here is for a single component system. In this simple system, phases that are possible, depend only on pressure and temperature. The markings show points where two or more phases can co-exist in equilibrium. At temperatures and pressures away from the markings, there will be only one phase at equilibrium.\nIn the diagram, the blue line marking the boundary between liquid and gas does not continue indefinitely, but terminates at a point called the critical point. As the temperature and pressure approach the critical point, the properties of the liquid and gas become progressively more similar. At the critical point, the liquid and gas become indistinguishable. Above the critical point, there are no longer separate liquid and gas phases: there is only a generic fluid phase referred to as a supercritical fluid. In water, the critical point occurs at around 647 K (374\u00a0\u00b0C or 705\u00a0\u00b0F) and 22.064 MPa.\nAn unusual feature of the water phase diagram is that the solid\u2013liquid phase line (illustrated by the dotted green line) has a negative slope. For most substances, the slope is positive as exemplified by the dark green line. This unusual feature of water is related to ice having a lower density than liquid water. Increasing the pressure drives the water into the higher density phase, which causes melting.\nAnother interesting though not unusual feature of the phase diagram is the point where the solid\u2013liquid phase line meets the liquid\u2013gas phase line. The intersection is referred to as the triple point. At the triple point, all three phases can coexist.\nExperimentally, phase lines are relatively easy to map due to the interdependence of temperature and pressure that develops when multiple phases form. Gibbs' phase rule suggests that different phases are completely determined by these variables. Consider a test apparatus consisting of a closed and well-insulated cylinder equipped with a piston. By controlling the temperature and the pressure, the system can be brought to any point on the phase diagram. From a point in the solid stability region (left side of the diagram), increasing the temperature of the system would bring it into the region where a liquid or a gas is the equilibrium phase (depending on the pressure). If the piston is slowly lowered, the system will trace a curve of increasing temperature and pressure within the gas region of the phase diagram. At the point where gas begins to condense to liquid, the direction of the temperature and pressure curve will abruptly change to trace along the phase line until all of the water has condensed.\nInterfacial phenomena.\nBetween two phases in equilibrium there is a narrow region where the properties are not that of either phase. Although this region may be very thin, it can have significant and easily observable effects, such as causing a liquid to exhibit surface tension. In mixtures, some components may preferentially move toward the interface. In terms of modeling, describing, or understanding the behavior of a particular system, it may be efficacious to treat the interfacial region as a separate phase.\nCrystal phases.\nA single material may have several distinct solid states capable of forming separate phases. Water is a well-known example of such a material. For example, water ice is ordinarily found in the hexagonal form ice Ih, but can also exist as the cubic ice Ic, the rhombohedral ice II, and many other forms. Polymorphism is the ability of a solid to exist in more than one crystal form. For pure chemical elements, polymorphism is known as allotropy. For example, diamond, graphite, and fullerenes are different allotropes of carbon.\nPhase transitions.\nWhen a substance undergoes a phase transition (changes from one state of matter to another) it usually either takes up or releases energy. For example, when water evaporates, the increase in kinetic energy as the evaporating molecules escape the attractive forces of the liquid is reflected in a decrease in temperature. The energy required to induce the phase transition is taken from the internal thermal energy of the water, which cools the liquid to a lower temperature; hence evaporation is useful for cooling. See Enthalpy of vaporization. The reverse process, condensation, releases heat. The heat energy, or enthalpy, associated with a solid to liquid transition is the enthalpy of fusion and that associated with a solid to gas transition is the enthalpy of sublimation.\nPhases out of equilibrium.\nWhile phases of matter are traditionally defined for systems in thermal equilibrium, work on quantum many-body localized (MBL) systems has provided a framework for defining phases out of equilibrium. MBL phases never reach thermal equilibrium, and can allow for new forms of order disallowed in equilibrium via a phenomenon known as localization protected quantum order. The transitions between different MBL phases and between MBL and thermalizing phases are novel dynamical phase transitions whose properties are active areas of research.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23638", "revid": "21857263", "url": "https://en.wikipedia.org/wiki?curid=23638", "title": "Outline of physical science", "text": "Hierarchical outline list of articles related to the physical sciences\nPhysical science is a branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a \"physical science\", together is called the \"physical sciences\".\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nDefinition.\nPhysical science can be described as all of the following:\nHistory.\nHistory of physical science \u2013 history of the branch of natural science that studies non-living systems, in contrast to the life sciences. It in turn has many branches, each referred to as a \"physical science\", together called the \"physical sciences\". However, the term \"physical\" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example). The four main branches of physical science are astronomy, physics, chemistry, and the Earth sciences, which include meteorology and geology. There are also a number of others which have become important in the twenty-first such as materials science and computer science.\nGeneral principles.\nBasic principles of physics.\nPhysics \u2013 branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the \"fundamental sciences\" because the other natural sciences (like biology, geology, etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy, and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms, or subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:\nBasic principles of astronomy.\nAstronomy \u2013 science of celestial bodies and their interactions in space. Its studies include the following:\nBasic principles of chemistry.\nChemistry \u2013 branch of science that studies the composition, structure, properties and change of matter. Chemistry is chiefly concerned with atoms and molecules and their interactions and transformations, for example, the properties of the chemical bonds formed between atoms to create chemical compounds. As such, chemistry studies the involvement of electrons and various forms of energy in photochemical reactions, oxidation-reduction reactions, changes in phases of matter, and separation of mixtures. Preparation and properties of complex substances, such as alloys, polymers, biological molecules, and pharmaceutical agents are considered in specialized fields of chemistry.\nBasic principles of Earth science.\nEarth science \u2013 the science of the planet Earth, as of 2018[ [update]] the only identified life-bearing planet. Its studies include the following:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23639", "revid": "90542", "url": "https://en.wikipedia.org/wiki?curid=23639", "title": "Gasoline", "text": "Liquid fuel derived from petroleum\nGasoline (North American English) or petrol (Commonwealth English) is a petrochemical product characterized as a transparent, yellowish and flammable liquid normally used as a fuel for spark-ignited internal combustion engines. When formulated as a fuel for engines, gasoline is chemically composed of organic compounds derived from the fractional distillation of petroleum and later chemically enhanced with gasoline additives. It is a high-volume profitable product produced in crude oil refineries.\nThe ability of a particular gasoline blend to resist premature ignition (which causes knocking and reduces efficiency in reciprocating engines) is measured by its octane rating. Tetraethyl lead was once widely used to increase the octane rating but is not used in modern automotive gasoline due to the health hazard. Aviation, off-road motor vehicles, and racing car engines still use leaded gasolines. Other substances are frequently added to gasoline to improve chemical stability and performance characteristics, control corrosion, and provide fuel system cleaning. Gasoline may contain oxygen-containing chemicals such as ethanol, MTBE, or ETBE to improve combustion.\nHistory and etymology.\nEnglish dictionaries show that the term \"gasoline\" originates from \"gas\" plus the chemical suffixes \"-ole\" and \"-ine\". \"Petrol\" derives from the Medieval Latin word \"petroleum\" (L. \"petra\", rock + \"oleum\", oil).\nInterest in gasoline-like fuels started with the invention of internal combustion engines suitable for use in transportation applications. The so-called Otto engines were developed in Germany during the last quarter of the 19th century. The fuel for these early engines was a relatively volatile hydrocarbon obtained from coal gas. With a boiling point near (\"n\"-octane boils at ), it was well suited for early carburettors (evaporators). The development of a \"spray nozzle\" carburettor enabled the use of less volatile fuels. Further improvements in engine efficiency were attempted at higher compression ratios, but early attempts were blocked by the premature explosion of fuel, known as knocking. In 1891, the Shukhov cracking process became the world's first commercial method to break down heavier hydrocarbons in crude oil to increase the percentage of lighter products compared to simple distillation.\nChemical analysis and production.\nCommercial gasoline, as well as other liquid transportation fuels, are complex mixtures of hydrocarbons. The performance specification also varies with season, requiring less volatile blends during summer, in order to minimize evaporative losses.\nGasoline is produced in oil refineries. Roughly of gasoline is derived from a barrel of crude oil. Material separated from crude oil via distillation, called virgin or straight-run gasoline, does not meet specifications for modern engines (particularly the octane rating; see below), but can be pooled to the gasoline blend.\nThe bulk of a typical gasoline consists of a homogeneous mixture of hydrocarbons with between four and twelve carbon atoms per molecule (commonly referred to as C4\u2013C12). It is a mixture of paraffins (alkanes), olefins (alkenes), naphthenes (cycloalkanes), and aromatics. The use of the term \"paraffin\" in place of the standard chemical nomenclature \"alkane\" is particular to the oil industry (which relies extensively on jargon). The composition of a gasoline depends upon:\nThe various refinery streams blended to make gasoline have different characteristics. Some important streams include the following:\nThe terms above are the jargon used in the oil industry, and the terminology varies.\nCurrently, many countries set limits on gasoline aromatics in general, benzene in particular, and olefin (alkene) content. Such regulations have led to an increasing preference for alkane isomers, such as isomerate or alkylate, as their octane rating is higher than n-alkanes. In the European Union, the benzene limit is set at one percent by volume for all grades of automotive gasoline. This is usually achieved by avoiding feeding C6, in particular cyclohexane, to the reformer unit, where it would be converted to benzene. Therefore, only (desulfurized) heavy virgin naphtha (HVN) is fed to the reformer unit.\nGasoline can also contain other organic compounds, such as organic ethers (deliberately added), plus small levels of contaminants, in particular organosulfur compounds (which are usually removed at the refinery).\nOn average, U.S. petroleum refineries produce about 19 to 20 gallons of gasoline, 11 to 13 gallons of distillate fuel diesel fuel and 3 to 4 gallons of jet fuel from each barrel of crude oil. The product ratio depends upon the processing in an oil refinery and the crude oil assay.\nPhysical properties.\nDensity.\nThe specific gravity of gasoline ranges from 0.71 to 0.77, with higher densities having a greater volume fraction of aromatics. Finished marketable gasoline is traded (in Europe) with a standard reference of , (7,5668 lb/ imp gal). Its price is escalated or de-escalated according to its actual density. Because of its low density, gasoline floats on water, and therefore water cannot generally be used to extinguish a gasoline fire unless applied in a fine mist.\nStability.\nQuality gasoline should be stable for six months if stored properly, but can degrade over time. Gasoline stored for a year will most likely be able to be burned in an internal combustion engine without too much trouble. Gasoline should ideally be stored in an airtight container (to prevent oxidation or water vapor mixing in with the gas) that can withstand the vapor pressure of the gasoline without venting (to prevent the loss of the more volatile fractions) at a stable cool temperature (to reduce the excess pressure from liquid expansion and to reduce the rate of any decomposition reactions). When gasoline is not stored correctly, gums and solids may result, which can corrode system components and accumulate on wet surfaces, resulting in a condition called \"stale fuel\". Gasoline containing ethanol is especially subject to absorbing atmospheric moisture, then forming gums, solids, or two phases (a hydrocarbon phase floating on top of a water-alcohol phase).\nThe presence of these degradation products in the fuel tank or fuel lines plus a carburettor or fuel injection components makes it harder to start the engine or causes reduced engine performance. On resumption of regular engine use, the buildup may or may not be eventually cleaned out by the flow of fresh gasoline. The addition of a fuel stabilizer to gasoline can extend the life of fuel that is not or cannot be stored properly, though removal of all fuel from a fuel system is the only real solution to the problem of long-term storage of an engine or a machine or vehicle. Typical fuel stabilizers are proprietary mixtures containing mineral spirits, isopropyl alcohol, 1,2,4-trimethylbenzene or other additives. Fuel stabilizers are commonly used for small engines, such as lawnmower and tractor engines, especially when their use is sporadic or seasonal (little to no use for one or more seasons of the year). Users have been advised to keep gasoline containers more than half full and properly capped to reduce air exposure, to avoid storage at high temperatures, to run an engine for ten minutes to circulate the stabilizer through all components prior to storage, and to run the engine at intervals to purge stale fuel from the carburettor.\nGasoline stability requirements are set by the standard ASTM D4814. This standard describes the various characteristics and requirements of automotive fuels for use over a wide range of operating conditions in ground vehicles equipped with spark-ignition engines.\nCombustion energy content.\nA gasoline-fueled internal combustion engine obtains energy from the combustion of gasoline's various hydrocarbons with oxygen from the ambient air, yielding carbon dioxide and water as exhaust. The combustion of octane, a representative species, performs the chemical reaction:\nBy weight, combustion of gasoline releases about or by volume , quoting the lower heating value. Gasoline blends differ, and therefore actual energy content varies according to the season and producer by up to 1.75 percent more or less than the average. On average, about of gasoline are available from a barrel of crude oil (about 46 percent by volume), varying with the quality of the crude and the grade of the gasoline. The remainder is products ranging from tar to naphtha.\nA high-octane-rated fuel, such as liquefied petroleum gas (LPG), has an overall lower power output at the typical 10:1 compression ratio of an engine design optimized for gasoline fuel. An engine tuned for LPG fuel via higher compression ratios (typically 12:1) improves the power output. This is because higher-octane fuels allow for a higher compression ratio without knocking, resulting in a higher cylinder temperature, which improves efficiency. Also, increased mechanical efficiency is created by a higher compression ratio through the concomitant higher expansion ratio on the power stroke, which is by far the greater effect. The higher expansion ratio extracts more work from the high-pressure gas created by the combustion process. An Atkinson cycle engine uses the timing of the valve events to produce the benefits of a high expansion ratio without the disadvantages, chiefly detonation, of a high compression ratio. A high expansion ratio is also one of the two key reasons for the efficiency of diesel engines, along with the elimination of pumping losses due to throttling of the intake airflow.\nThe lower energy content of LPG by liquid volume in comparison to gasoline is due mainly to its lower density. This lower density is a property of the lower molecular weight of propane (LPG's chief component) compared to gasoline's blend of various hydrocarbon compounds with heavier molecular weights than propane. Conversely, LPG's energy content by weight is higher than gasoline's due to a higher hydrogen-to-carbon ratio.\nMolecular weights of the species in the representative octane combustion are 114, 32, 44, and 18 for C8H18, O2, CO2, and H2O, respectively; therefore of fuel reacts with of oxygen to produce of carbon dioxide and of water.\nOctane rating.\nOctane rating is measured relative to a mixture of 2,2,4-trimethylpentane (an isomer of octane) and n-heptane. There are different conventions for expressing octane ratings, so the same physical fuel may have several different octane ratings based on the measure used. One of the best known is the research octane number (RON).\nThe octane rating of typical commercially available gasoline varies by country. In Finland, Sweden, and Norway, 95 RON is the standard for regular unleaded gasoline and 98 RON is also available as a more expensive option.\nIn the United Kingdom, over 95 percent of gasoline sold has 95 RON and is marketed as Unleaded or Premium Unleaded. Super Unleaded, with 97/98 RON and branded high-performance fuels (e.g., Shell V-Power, BP Ultimate) with 99 RON make up the balance. Gasoline with 102 RON may rarely be available for racing purposes.\nIn the U.S., octane ratings in unleaded fuels vary between 85 and 87 AKI (91\u201392 RON) for regular, 89\u201390 AKI (94\u201395 RON) for mid-grade (equivalent to European regular), up to 90\u201394 AKI (95\u201399 RON) for premium (European premium).\nAs South Africa's largest city, Johannesburg, is located on the Highveld at above sea level, the Automobile Association of South Africa recommends 95-octane gasoline at low altitude and 93-octane for use in Johannesburg because \"The higher the altitude the lower the air pressure, and the lower the need for a high octane fuel as there is no real performance gain\".\nOctane rating became important as the military sought higher output for aircraft engines in the late 1920s and the 1940s. A higher octane rating allows a higher compression ratio or supercharger boost, and thus higher temperatures and pressures, which translate to higher power output. Some scientists even predicted that a nation with a good supply of high-octane gasoline would have the advantage in air power. In 1943, the Rolls-Royce Merlin aero engine produced using 100 RON fuel from a modest displacement. By the time of Operation Overlord, both the RAF and USAAF were conducting some operations in Europe using 150 RON fuel (100/150 avgas), obtained by adding 2.5 percent aniline to 100-octane avgas. By this time, the Rolls-Royce Merlin 66 was developing using this fuel.\nAdditives.\nAntiknock additives.\nTetraethyl lead.\nGasoline, when used in high-compression internal combustion engines, tends to auto-ignite or \"detonate\" causing damaging engine knocking (also called \"pinging\" or \"pinking\"). To address this problem, tetraethyl lead (TEL) was widely adopted as an additive for gasoline in the 1920s. With a growing awareness of the seriousness of the extent of environmental and health damage caused by lead compounds, however, and the incompatibility of lead with catalytic converters, governments began to mandate reductions in gasoline lead.\nIn the U.S., the Environmental Protection Agency issued regulations to reduce the lead content of leaded gasoline over a series of annual phases, scheduled to begin in 1973 but delayed by court appeals until 1976. By 1995, leaded fuel accounted for only 0.6 percent of total gasoline sales and under () of lead per year. From 1 January 1996, the U.S. Clean Air Act banned the sale of leaded fuel for use in on-road vehicles in the U.S. The use of TEL also necessitated other additives, such as dibromoethane.\nEuropean countries began replacing lead-containing additives by the end of the 1980s and, by the end of the 1990s, leaded gasoline was banned within the entire European Union with an exception for Avgas 100LL for general aviation. The UAE started to switch to unleaded in the early 2000s.\nReduction in the average lead content of human blood may be a major cause for falling violent crime rates around the world including South Africa. A study found a correlation between leaded gasoline usage and violent crime (see Lead\u2013crime hypothesis). Other studies found no correlation.\nIn August 2021, the UN Environment Programme announced that leaded gasoline had been eradicated worldwide, with Algeria being the last country to deplete its reserves. UN Secretary-General Ant\u00f3nio Guterres called the eradication of leaded petrol an \"international success story\". He also added: \"Ending the use of leaded petrol will prevent more than one million premature deaths each year from heart disease, strokes and cancer, and it will protect children whose IQs are damaged by exposure to lead\". Greenpeace called the announcement \"the end of one toxic era\". However, leaded gasoline continues to be used in aeronautic, auto racing, and off-road applications. The use of leaded additives is still permitted worldwide for the formulation of some grades of aviation gasoline such as 100LL, because the required octane rating is difficult to reach without the use of leaded additives.\nDifferent additives have replaced lead compounds. The most popular additives include aromatic hydrocarbons, ethers (MTBE and ETBE), and alcohols, most commonly ethanol.\nLead replacement petrol.\nLead replacement petrol (LRP) was developed for vehicles designed to run on leaded fuels and incompatible with unleaded fuels. Rather than tetraethyllead, it contains other metals such as potassium compounds or methylcyclopentadienyl manganese tricarbonyl (MMT); these are purported to buffer soft exhaust valves and seats so that they do not suffer recession due to the use of unleaded fuel.\nLRP was marketed during and after the phaseout of leaded motor fuels in the United Kingdom, Australia, South Africa, and some other countries. Consumer confusion led to a widespread mistaken preference for LRP rather than unleaded, and LRP was phased out 8 to 10 years after the introduction of unleaded.\nLeaded gasoline was withdrawn from sale in Britain after 31 December 1999, seven years after EEC regulations signalled the end of production for cars using leaded gasoline in member states. At this stage, a large percentage of cars from the 1980s and early 1990s which ran on leaded gasoline were still in use, along with cars that could run on unleaded fuel. However, the declining number of such cars on British roads saw many gasoline stations withdrawing LRP from sale by 2003.\nMMT.\nMethylcyclopentadienyl manganese tricarbonyl (MMT) is used in Canada and the U.S. to boost octane rating. Its use in the U.S. has been restricted by regulations, although it is currently allowed. Its use in the European Union is restricted by Article 8a of the Fuel Quality Directive following its testing under the Protocol for the evaluation of effects of metallic fuel-additives on the emissions performance of vehicles.\nFuel stabilizers (antioxidants and metal deactivators).\nGummy, sticky resin deposits result from oxidative degradation of gasoline during long-term storage. These harmful deposits arise from the oxidation of alkenes and other minor components in gasoline (see drying oils). Improvements in refinery techniques have generally reduced the susceptibility of gasolines to these problems. Previously, catalytically or thermally cracked gasolines were most susceptible to oxidation. The formation of gums is accelerated by copper salts, which can be neutralized by additives called metal deactivators.\nThis degradation can be prevented through the addition of 5\u2013100 ppm of antioxidants, such as phenylenediamines and other amines. Hydrocarbons with a bromine number of 10 or above can be protected with the combination of unhindered or partially hindered phenols and oil-soluble strong amine bases, such as hindered phenols. \"Stale\" gasoline can be detected by a colorimetric enzymatic test for organic peroxides produced by oxidation of the gasoline.\nGasolines are also treated with metal deactivators, which are compounds that sequester (deactivate) metal salts that otherwise accelerate the formation of gummy residues. The metal impurities might arise from the engine itself or as contaminants in the fuel.\nDetergents.\nGasoline, as delivered at the pump, also contains additives to reduce internal engine carbon buildup, improve combustion and allow easier starting in cold climates. High levels of detergent can be found in Top Tier Detergent Gasolines. The specification for Top Tier Detergent Gasolines was developed by four automakers: GM, Honda, Toyota, and BMW. According to the bulletin, the minimal U.S. EPA requirement is not sufficient to keep engines clean. Typical detergents include alkylamines and alkyl phosphates at a level of 50\u2013100 ppm.\nEthanol.\nEuropean Union.\nIn the EU, 5 percent ethanol can be added within the common gasoline spec (EN 228). Discussions are ongoing to allow 10 percent blending of ethanol (available in Finnish, French and German gasoline stations). In Finland, most gasoline stations sell 95E10, which is 10 percent ethanol, and 98E5, which is 5 percent ethanol. Most gasoline sold in Sweden has 5\u201315 percent ethanol added. Three different ethanol blends are sold in the Netherlands\u2014E5, E10 and hE15. The last of these differs from standard ethanol\u2013gasoline blends in that it consists of 15 percent hydrous ethanol (i.e., the ethanol\u2013water azeotrope) instead of the anhydrous ethanol traditionally used for blending with gasoline.\nFrom 2009 to 2022, renewable percentage in gasoline slowly increased from 5% to 10%, even though EU-produced ethanol can achieve a climate-neutral production capability and most EU cars can use E10. E10 availability is low even in larger countries like Germany (26%) and France (58%). 8 countries in the EU have not adopted E10 as of 2024.\nBrazil.\nThe Brazilian National Agency of Petroleum, Natural Gas and Biofuels (ANP) requires gasoline for automobile use to have 27.5 percent of ethanol added to its composition. Pure hydrated ethanol is also available as a fuel.\nAustralia.\nAustralia uses both E10 (up to 10% ethanol) and E85 (up to 85% ethanol) in its gasoline. New South Wales mandated biofuel in its Biofuels Act 2007, and Queensland had a biofuel mandate since 2017. Fuel pumps must be clearly labelled with its ethanol/biodiesel content.\nU.S..\nThe federal Renewable Fuel Standard (RFS) effectively requires refiners and blenders to blend renewable biofuels (mostly ethanol) with gasoline, sufficient to meet a growing annual target of total gallons blended. Although the mandate does not require a specific percentage of ethanol, annual increases in the target combined with declining gasoline consumption have caused the typical ethanol content in gasoline to approach 10 percent. Most fuel pumps display a sticker that states that the fuel may contain up to 10 percent ethanol, an intentional disparity that reflects the varying actual percentage. In parts of the U.S., ethanol is sometimes added to gasoline without an indication that it is a component.\nIndia.\nIn October 2007, the Government of India decided to make five percent ethanol blending (with gasoline) mandatory. Currently, 10 percent ethanol blended product (E10) is being sold in various parts of the country. Ethanol has been found in at least one study to damage catalytic converters.\nDyes.\nThough gasoline is a naturally colourless liquid, many gasolines are dyed in various colours to indicate their composition and acceptable uses. In Australia, the lowest grade of gasoline (RON 91) was dyed a light shade of red/orange, but is now the same colour as the medium grade (RON 95) and high octane (RON 98), which are dyed yellow. In the U.S., aviation gasoline (avgas) is dyed to identify its octane rating and to distinguish it from kerosene-based jet fuel, which is left colourless. In Canada, the gasoline for marine and farm use is dyed red and is not subject to fuel excise tax in most provinces.\nOxygenate blending.\nOxygenate blending adds oxygen-bearing compounds such as methanol, MTBE, ETBE, TAME, TAEE, ethanol, and biobutanol. The presence of these oxygenates reduces the amount of carbon monoxide and unburned fuel in the exhaust. In many areas throughout the U.S., oxygenate blending is mandated by EPA regulations to reduce smog and other airborne pollutants. For example, in Southern California fuel must contain two percent oxygen by weight, resulting in a mixture of 5.6 percent ethanol in gasoline. The resulting fuel is often known as reformulated gasoline (RFG) or oxygenated gasoline, or, in the case of California, California reformulated gasoline (CARBOB). The federal requirement that RFG contain oxygen was dropped on 6 May 2006 because the industry had developed VOC-controlled RFG that did not need additional oxygen.\nMTBE was phased out in the U.S. due to groundwater contamination and the resulting regulations and lawsuits. Ethanol and, to a lesser extent, ethanol-derived ETBE are common substitutes. A common ethanol-gasoline mix of 10 percent ethanol mixed with gasoline is called gasohol or E10, and an ethanol-gasoline mix of 85 percent ethanol mixed with gasoline is called E85. The most extensive use of ethanol takes place in Brazil, where the ethanol is derived from sugarcane. In 2004, over of ethanol was produced in the U.S. for fuel use, mostly from corn and sold as E10. E85 is slowly becoming available in much of the U.S., though many of the relatively few stations vending E85 are not open to the general public.\nThe use of bioethanol and bio-methanol, either directly or indirectly by conversion of ethanol to bio-ETBE, or methanol to bio-MTBE is encouraged by the European Union Directive on the Promotion of the use of biofuels and other renewable fuels for transport. Since producing bioethanol from fermented sugars and starches involves distillation, though, ordinary people in much of Europe cannot legally ferment and distill their own bioethanol at present (unlike in the U.S., where getting a BATF distillation permit has been easy since the 1973 oil crisis).\nSafety.\nToxicity.\nThe safety data sheet for a 2003 Texan unleaded gasoline shows at least 15 hazardous chemicals occurring in various amounts, including benzene (up to five percent by volume), toluene (up to 35 percent by volume), naphthalene (up to one percent by volume), trimethylbenzene (up to seven percent by volume), methyl \"tert\"-butyl ether (MTBE) (up to 18 percent by volume, in some states), and about 10 others. Hydrocarbons in gasoline generally exhibit low acute toxicities, with LD50 of 700\u20132700\u00a0mg/kg for simple aromatic compounds. Benzene and many anti-knocking additives are carcinogenic.\nPeople can be exposed to gasoline in the workplace by swallowing it, breathing in vapours, skin contact, and eye contact. Gasoline is toxic. The National Institute for Occupational Safety and Health (NIOSH) has also designated gasoline as a carcinogen. Physical contact, ingestion, or inhalation can cause health problems. Since ingesting large amounts of gasoline can cause permanent damage to major organs, a call to a local poison control centre or emergency room visit is indicated.\nContrary to common misconception, swallowing gasoline does not generally require special emergency treatment, and inducing vomiting does not help, and can make it worse. According to poison specialist Brad Dahl, \"even two mouthfuls wouldn't be that dangerous as long as it goes down to your stomach and stays there or keeps going\". The U.S. CDC's Agency for Toxic Substances and Disease Registry says not to induce vomiting, lavage, or administer activated charcoal.\nInhalation for intoxication.\nInhaled (huffed) gasoline vapor is a common intoxicant. Users concentrate and inhale gasoline vapor in a manner not intended by the manufacturer to produce euphoria and intoxication. Gasoline inhalation has become epidemic in some poorer communities and indigenous groups in Australia, Canada, New Zealand, and some Pacific Islands. The practice is thought to cause severe organ damage, along with other effects such as intellectual disability and various cancers.\nIn Canada, Native children in the isolated Northern Labrador community of Davis Inlet were the focus of national concern in 1993, when many were found to be sniffing gasoline. The Canadian and provincial Newfoundland and Labrador governments intervened on several occasions, sending many children away for treatment. Despite being moved to the new community of Natuashish in 2002, serious inhalant abuse problems have continued. Similar problems were reported in Sheshatshiu in 2000 and also in Pikangikum First Nation. In 2012, the issue once again made the news media in Canada.\nAustralia has long faced a petrol (gasoline) sniffing problem in isolated and impoverished aboriginal communities. Although some sources argue that sniffing was introduced by U.S. servicemen stationed in the nation's Top End during World War II or through experimentation by 1940s-era Cobourg Peninsula sawmill workers, other sources claim that inhalant abuse (such as glue inhalation) emerged in Australia in the late 1960s. Chronic, heavy petrol sniffing appears to occur among remote, impoverished indigenous communities, where the ready accessibility of petrol has helped to make it a common substance for abuse.\nIn Australia, petrol sniffing now occurs widely throughout remote Aboriginal communities in the Northern Territory, Western Australia, northern parts of South Australia, and Queensland. The number of people sniffing petrol goes up and down over time as young people experiment or sniff occasionally. \"Boss\", or chronic, sniffers may move in and out of communities; they are often responsible for encouraging young people to take it up. In 2005, the Government of Australia and BP Australia began the usage of Opal fuel in remote areas prone to petrol sniffing. Opal is a non-sniffable fuel (which is much less likely to cause a high) and has made a difference in some indigenous communities.\nFlammability.\nGasoline is flammable with low flash point of . Gasoline has a lower explosive limit of 1.4 percent by volume and an upper explosive limit of 7.6 percent. If the concentration is below 1.4 percent, the air-gasoline mixture is too lean and does not ignite. If the concentration is above 7.6 percent, the mixture is too rich and also does not ignite. However, gasoline vapor rapidly mixes and spreads with air, making unconstrained gasoline quickly flammable.\nGasoline exhaust.\nThe exhaust gas generated by burning gasoline is harmful to both the environment and to human health. After CO is inhaled into the human body, it readily combines with haemoglobin in the blood, and its affinity is 300 times that of oxygen. Therefore, the haemoglobin in the lungs combines with CO instead of oxygen, causing the human body to be hypoxic, causing headaches, dizziness, vomiting, and other poisoning symptoms. In severe cases, it may lead to death. Hydrocarbons only affect the human body when their concentration is quite high, and their toxicity level depends on the chemical composition. The hydrocarbons produced by incomplete combustion include alkanes, aromatics, and aldehydes. Among them, a concentration of methane and ethane over will cause loss of consciousness or suffocation, a concentration of pentane and hexane over will have an anaesthetic effect, and aromatic hydrocarbons will have more serious effects on health, blood toxicity, neurotoxicity, and cancer. If the concentration of benzene exceeds 40 ppm, it can cause leukaemia, and xylene can cause headache, dizziness, nausea, and vomiting. Human exposure to large amounts of aldehydes can cause eye irritation, nausea, and dizziness. In addition to carcinogenic effects, long-term exposure can cause damage to the skin, liver, kidneys, and cataracts. After NOx enters the alveoli, it has a severe stimulating effect on the lung tissue. It can irritate the conjunctiva of the eyes, cause tearing, and cause pink eyes. It also has a stimulating effect on the nose, pharynx, throat, and other organs. It can cause acute wheezing, breathing difficulties, red eyes, sore throat, and dizziness causing poisoning. Fine particulates are also dangerous to health.\nEnvironmental effect.\nThe air pollution in many large cities has changed from coal-burning pollution to \"motor vehicle pollution\". In the U.S., transportation is the largest source of carbon emissions, accounting for 30 percent of the total carbon footprint of the U.S. Combustion of gasoline produces of carbon dioxide, a greenhouse gas.\nUnburnt gasoline and evaporation from the tank, when in the atmosphere, react in sunlight to produce photochemical smog. Vapor pressure initially rises with some addition of ethanol to gasoline, but the increase is greatest at 10 percent by volume. At higher concentrations of ethanol above 10 percent, the vapor pressure of the blend starts to decrease. At a 10 percent ethanol by volume, the rise in vapor pressure may potentially increase the problem of photochemical smog. This rise in vapor pressure could be mitigated by increasing or decreasing the percentage of ethanol in the gasoline mixture. The chief risks of such leaks come not from vehicles, but gasoline delivery truck accidents and leaks from storage tanks. Because of this risk, most (underground) storage tanks now have extensive measures in place to detect and prevent any such leaks, such as monitoring systems (Veeder-Root, Franklin Fueling).\nProduction of gasoline consumes of water by driven distance.\nGasoline use causes a variety of deleterious effects to the human population and to the climate generally. The harms imposed include a higher rate of premature death and ailments, such as asthma, caused by air pollution, higher healthcare costs for the public generally, decreased crop yields, missed work and school days due to illness, increased flooding and other extreme weather events linked to global climate change, and other social costs. The costs imposed on society and the planet are estimated to be $3.80 per gallon of gasoline, in addition to the price paid at the pump by the user. The damage to the health and climate caused by a gasoline-powered vehicle greatly exceeds that caused by electric vehicles.\nGasoline can be released into the environment as an uncombusted liquid fuel, as a flammable liquid, or as a vapor by way of leakages occurring during its production, handling, transport and delivery. Gasoline contains known carcinogens, and gasoline exhaust is a health risk. Gasoline is often used as a recreational inhalant and can be harmful or fatal when used in such a manner. When burned, of gasoline emits about of CO2, a greenhouse gas, contributing to human-caused climate change. Oil products, including gasoline, were responsible for about 32% of CO2 emissions worldwide in 2021.\nCarbon dioxide.\nAbout of carbon dioxide (CO2) are produced from burning gasoline that does not contain ethanol. Most of the retail gasoline now sold in the U.S. contains about 10 percent fuel ethanol (or E10) by volume. Burning E10 produces about of CO2 that is emitted from the fossil fuel content. If the CO2 emissions from ethanol combustion are considered, then about of CO2 are produced when E10 is combusted.\nWorldwide 7 litres of gasoline are burnt for every 100\u00a0km driven by cars and vans.\nIn 2021, the International Energy Agency stated, \"To ensure fuel economy and CO2 emissions standards are effective, governments must continue regulatory efforts to monitor and reduce the gap between real-world fuel economy and rated performance.\"\nContamination of soil and water.\nGasoline enters the environment through the soil, groundwater, surface water, and air. Therefore, humans may be exposed to gasoline through methods such as breathing, eating, and skin contact. For example, using gasoline-filled equipment, such as lawnmowers, drinking gasoline-contaminated water close to gasoline spills or leaks to the soil, working at a gasoline station, inhaling gasoline volatile gas when refuelling at a gasoline station is the easiest way to be exposed to gasoline.\nUse and pricing.\nThe International Energy Agency said in 2021 that \"road fuels should be taxed at a rate that reflects their impact on people's health and the climate\".\nEurope.\nCountries in Europe impose substantially higher taxes on fuels such as gasoline when compared to the U.S. The price of gasoline in Europe is typically higher than that in the U.S. due to this difference.\nU.S..\nFrom 1998 to 2004, the price of gasoline fluctuated between . After 2004, the price increased until the average gasoline price reached a high of in mid-2008 but receded to approximately by September 2009. The U.S. experienced an upswing in gasoline prices through 2011, and, by 1 March 2012, the national average was . California prices are higher because the California government mandates unique California gasoline formulas and taxes.\nIn the U.S., most consumer goods bear pre-tax prices, but gasoline prices are posted with taxes included. Taxes are added by federal, state, and local governments. As of 2009[ [update]], the federal tax was for gasoline and for diesel (excluding red diesel).\nAbout nine percent of all gasoline sold in the U.S. in May 2009 was premium grade, according to the Energy Information Administration. \"Consumer Reports\" magazine says, \"If [your owner's manual] says to use regular fuel, do so\u2014there's no advantage to a higher grade.\" The \"Associated Press\" said premium gas\u2014which has a higher octane rating and costs more per gallon than regular unleaded\u2014should be used only if the manufacturer says it is \"required\". Cars with turbocharged engines and high compression ratios often specify premium gasoline because higher octane fuels reduce the incidence of \"knock\", or fuel pre-detonation. The price of gasoline varies considerably between the summer and winter months.\nThere is a considerable difference between summer oil and winter oil in gasoline vapor pressure (Reid Vapor Pressure, RVP), which is a measure of how easily the fuel evaporates at a given temperature. The higher the gasoline volatility (the higher the RVP), the easier it is to evaporate. The conversion between the two fuels occurs twice a year, once in autumn (winter mix) and the other in spring (summer mix). The winter blended fuel has a higher RVP because the fuel must be able to evaporate at a low temperature for the engine to run normally. If the RVP is too low on a cold day, the vehicle will be difficult to start; however, the summer blended gasoline has a lower RVP. It prevents excessive evaporation when the outdoor temperature rises, reduces ozone emissions, and reduces smog levels. At the same time, vapor lock is less likely to occur in hot weather.\nComparison with other fuels.\nBelow is a table of the energy density (per volume) and specific energy (per mass) of various transportation fuels as compared with gasoline. In the rows with gross and net, they are from the Oak Ridge National Laboratory's Transportation Energy Data Book.\nSee also.\nChevron published a free technical guide https:// using common language that explains gasoline production, blending, and combustion in an engine. The report covers the US and other locations globally. \nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23640", "revid": "49402000", "url": "https://en.wikipedia.org/wiki?curid=23640", "title": "Pentose", "text": "5-Carbon simple sugar\nIn chemistry, a pentose is a monosaccharide (simple sugar) with five carbon atoms. The chemical formula of many pentoses is C5H10O5, and their molecular weight is 150.13 g/mol.\nPentoses are very important in biochemistry. Ribose is a constituent of RNA, and the related molecule, deoxyribose, is a constituent of DNA. Phosphorylated pentoses are important products of the pentose phosphate pathway, most importantly ribose 5-phosphate (R5P), which is used in the synthesis of nucleotides and nucleic acids.\nLike some other monosaccharides, pentoses exist in two forms, open-chain (linear) or closed-chain (cyclic), that easily convert into each other in water solutions. The linear form of a pentose, which usually exists only in solutions, has an open-chain backbone of five carbons. Four of these carbons have one hydroxyl functional group (\u2013OH) each, connected by a single bond, and one has an oxygen atom connected by a double bond (=O), forming a carbonyl group (C=O). The remaining bonds of the carbon atoms are satisfied by six hydrogen atoms. Thus the structure of the linear form is H\u2013(CHOH)\"x\"\u2013C(=O)\u2013(CHOH)4-\"x\"\u2013H, where \"x\" is 0, 1, or 2.\nThe term \"pentose\" sometimes is assumed to include deoxypentoses, such as deoxyribose: compounds with general formula C5H10O5-\"y\" that can be described as derived from pentoses by replacement of one or more hydroxyl groups with hydrogen atoms.\nClassification.\nThe aldopentoses are a subclass of the pentoses which, in the linear form, have the carbonyl at carbon 1, forming an aldehyde derivative with structure H\u2013C(=O)\u2013(CHOH)4\u2013H. The most important example is ribose. The ketopentoses instead have the carbonyl at positions 2 or 3, forming a ketone derivative with structure H\u2013CHOH\u2013C(=O)\u2013(CHOH)3\u2013H (2-ketopentose) or H\u2013(CHOH)2\u2013C(=O)\u2013(CHOH)2\u2013H (3-ketopentose). The latter is not known to occur in nature and are difficult to synthesize. \nIn the open form, there are eight aldopentoses and four 2-ketopentoses, stereoisomers that differ in the spatial position of the hydroxyl groups. These forms occur in pairs of optical isomers, generally labelled \"D\" or \"L\" by conventional rules (independently of their optical activity).\nAldopentoses.\nThe aldopentoses have three chiral centers; therefore, eight (23) different stereoisomers are possible.\nRibose is a constituent of RNA, and the related molecule, deoxyribose, is a constituent of DNA. Phosphorylated pentoses are important products of the pentose phosphate pathway, most importantly ribose 5-phosphate (R5P), which is used in the synthesis of nucleotides and nucleic acids, and erythrose 4-phosphate (E4P), which is used in the synthesis of aromatic amino acids.\nKetopentoses.\nThe 2-ketopentoses have two chiral centers; therefore, four (22) different stereoisomers are possible. The 3-ketopentoses are rare.\nCyclic forms.\nThe closed or cyclic form of a pentose forms when the carbonyl group reacts with a hydroxyl in another carbon, turning the carbonyl into a hydroxyl and creating an ether bridge \u2013O\u2013 between the two carbons. This intramolecular reaction yields a cyclic molecule, with a ring consisting of one oxygen atom and usually four carbon atoms; the cyclic compounds are then called furanoses, for having the same rings as the cyclic ether tetrahydrofuran.\nThe ring closure converts the carbonyl carbon into a chiral center, which may adopt either of two configurations, depending on the position of the new hydroxyl. Therefore, each linear form can produce two distinct closed forms, identified by prefixes \"\u03b1\" and \"\u03b2\".\nDeoxypentoses.\nThe one deoxypentose has two total stereoisomers.\nProperties.\nIn the cell, pentoses have a higher metabolic stability than hexoses.\nA polymer composed of pentose sugars is called a pentosan.\nTests for pentoses.\nThe most important tests for pentoses rely on converting the pentose to furfural, which then reacts with a chromophore. In Tollens' test for pentoses (not to be confused with Tollens' silver-mirror test for reducing sugars), the furfural ring reacts with phloroglucinol to produce a colored compound; in the aniline acetate test with aniline acetate; and in Bial's test, with orcinol. In each of these tests, pentoses react much more strongly and quickly than hexoses.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23641", "revid": "34714876", "url": "https://en.wikipedia.org/wiki?curid=23641", "title": "Protestant", "text": ""}
{"id": "23642", "revid": "10951369", "url": "https://en.wikipedia.org/wiki?curid=23642", "title": "Pharisee", "text": ""}
{"id": "23643", "revid": "47039979", "url": "https://en.wikipedia.org/wiki?curid=23643", "title": "Propane", "text": "Hydrocarbon compound (C3H8)\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nPropane () is a three-carbon chain alkane with the molecular formula . It is a gas at standard temperature and pressure, but becomes liquid when compressed for transportation and storage. A by-product of natural gas processing and petroleum refining, it is often a constituent of liquefied petroleum gas (LPG), which is commonly used as a fuel in domestic and industrial applications and in low-emissions public transportation; other constituents of LPG may include propylene, butane, butylene, butadiene, and isobutylene. Discovered in 1857 by the French chemist Marcellin Berthelot, it became commercially available in the US by 1911. Propane has lower volumetric energy density than gasoline or coal, but has higher gravimetric energy density than them and burns more cleanly.\nPropane gas has become a popular choice for barbecues and portable stoves because its low \u221242\u00a0\u00b0C boiling point makes it vaporise inside pressurised liquid containers (it exists in two phases, vapor above liquid). It retains its ability to vaporise even in cold weather, making it better-suited for outdoor use in cold climates than alternatives with higher boiling points like butane. LPG powers buses, forklifts, automobiles, outboard boat motors, and ice resurfacing machines, and is used for heat and cooking in recreational vehicles and campers. Propane is also becoming popular as a replacement refrigerant (R290) for heatpumps as it offers greater efficiency than the current refrigerants: R410A / R32, higher temperature heat output and less damage to the atmosphere for escaped gases\u2014at the expense of high gas flammability.\nHistory.\nPropane was first synthesized by the French chemist Marcellin Berthelot in 1857 during his researches on hydrogenation. Berthelot made propane by heating propylene dibromide (C3H6Br2) with potassium iodide and water.p. 9, \u00a71.1 Propane was found dissolved in Pennsylvanian light crude oil by Edmund Ronalds in 1864. Walter O. Snelling of the U.S. Bureau of Mines highlighted it as a volatile component in gasoline in 1910, which marked the \"birth of the propane industry\" in the United States. The volatility of these lighter hydrocarbons caused them to be known as \"wild\" because of the high vapor pressures of unrefined gasoline. On March 31, 1912, \"The New York Times\" reported on Snelling's work with liquefied gas, saying \"a steel bottle will carry enough gas to light an ordinary home for three weeks\".\nIt was during this time that Snelling\"\u2014\"in cooperation with Frank P. Peterson, Chester Kerr, and Arthur Kerr\"\u2014\"developed ways to liquefy the LP gases during the refining of gasoline. Together, they established American Gasol Co., the first commercial marketer of propane. Snelling had produced relatively pure propane by 1911, and on March 25, 1913, his method of processing and producing LP gases was issued patent #1,056,845. A separate method of producing LP gas through compression was developed by Frank Peterson and its patent was granted on July 2, 1912.\nThe 1920s saw increased production of LP gases, with the first year of recorded production totaling in 1922. In 1927, annual marketed LP gas production reached , and by 1935, the annual sales of LP gas had reached . Major industry developments in the 1930s included the introduction of railroad tank car transport, gas odorization, and the construction of local bottle-filling plants. The year 1945 marked the first year that annual LP gas sales reached a billion gallons. By 1947, 62% of all U.S. homes had been equipped with either natural gas or propane for cooking.\nIn 1950, 1,000 propane-fueled buses were ordered by the Chicago Transit Authority, and by 1958, sales in the U.S. had reached annually. In 2004, it was reported to be a growing $8-billion to $10-billion industry with over of propane being used annually in the U.S.\nDuring the COVID-19 pandemic, propane shortages were reported in the United States due to increased demand.\nEtymology.\nThe \"prop-\" root found in \"propane\" and names of other compounds with three-carbon chains was derived from \"propionic acid\", which in turn was named after the Greek words protos (meaning first) and pion (fat), as it was the \"first\" member of the series of fatty acids.\nProperties and reactions.\nPropane is a colorless, odorless gas. Ethyl mercaptan is added as a safety precaution as an odorizer, and is commonly called a \"rotten egg\" smell. At normal pressure it liquifies below its boiling point at \u221242\u00a0\u00b0C and solidifies below its melting point at \u2212187.7\u00a0\u00b0C. Propane crystallizes in the space group P21/n. The low space-filling of 58.5% (at 90 K), due to the bad stacking properties of the molecule, is the reason for the particularly low melting point.\nPropane undergoes combustion reactions in a similar fashion to other alkanes. In the presence of excess oxygen, propane burns to form water and carbon dioxide.\n&lt;chem display=\"block\"&gt;C3H8 + 5 O2 -&gt; 3 CO2 + 4 H2O + heat &lt;/chem&gt;\nWhen insufficient oxygen is present for complete combustion, carbon monoxide, soot (carbon), or both, are formed as well:\n&lt;chem display=\"block\"&gt;C3H8 + 9/2 O2 -&gt; 2 CO2 + CO + 4 H2O + heat &lt;/chem&gt;\n&lt;chem display=\"block\"&gt;C3H8 + 2 O2 -&gt; 3 C + 4 H2O + heat &lt;/chem&gt;\nThe complete combustion of propane produces about 50 MJ/kg of heat.\nPropane combustion is much cleaner than that of coal or unleaded gasoline. Propane's per-BTU production of CO2 is almost as low as that of natural gas. Propane burns hotter than home heating oil or diesel fuel because of the very high hydrogen content. The presence of C\u2013C bonds, plus the multiple bonds of propylene and butylene, produce organic exhausts besides carbon dioxide and water vapor during typical combustion. These bonds also cause propane to burn with a visible flame.\nEnergy content.\nThe enthalpy of combustion of propane gas where all products return to standard state, for example where water returns to its liquid state at standard temperature (known as higher heating value), is (2,219.2 \u00b1 0.5) kJ/mol, or (50.33 \u00b1 0.01) MJ/kg.\nThe enthalpy of combustion of propane gas where products do not return to standard state, for example where the hot gases including water vapor exit a chimney, (known as lower heating value) is 2,043.455 kJ/mol. The lower heat value is the amount of heat available from burning the substance where the combustion products are vented to the atmosphere; for example, the heat from a fireplace when the flue is open.\nDensity.\nThe density of propane gas at 25\u00a0\u00b0C (77\u00a0\u00b0F) is 1.808\u00a0kg/m3, about 1.5\u00d7 the density of air at the same temperature. The density of liquid propane at 25\u00a0\u00b0C (77\u00a0\u00b0F) is 0.493\u00a0g/cm3, which is equivalent to 4.11 pounds per U.S. liquid gallon or 493\u00a0g/L. Propane expands at 1.5% per 10\u00a0\u00b0F. Thus, liquid propane has a density of approximately 4.2 pounds per gallon (504\u00a0g/L) at 60\u00a0\u00b0F (15.6\u00a0\u00b0C).\nAs the density of propane changes with temperature, this fact must be considered every time when the application is connected with safety or custody transfer operations. If enough headspace is not left in a tank when filling, the tank could rupture when it heats up.\nUses.\nPortable stoves.\nPropane is a popular choice for barbecues and portable stoves because the low boiling point of makes it vaporize as soon as it is released from its pressurized container. Therefore, no carburetor or other vaporizing device is required; a simple metering nozzle suffices.\nRefrigerant.\nBlends of pure, dry \"isopropane\" [isobutane/propane mixtures of propane (R-290) and isobutane (R-600a)] can be used as the circulating refrigerant in suitably constructed compressor-based refrigeration. Compared to fluorocarbons, propane has a negligible ozone depletion potential and very low global warming potential (having a GWP value of 0.072, 13.9 times lower than the GWP of carbon dioxide) and can serve as a functional replacement for R-12, R-22, R-134a, and other chlorofluorocarbon or hydrofluorocarbon refrigerants in conventional stationary refrigeration and air conditioning systems. Because its global warming effect is far less than current refrigerants, propane was chosen as one of five replacement refrigerants approved by the EPA in 2015, for use in systems specially designed to handle its flammability.\nSuch substitution is widely prohibited or discouraged in motor vehicle air conditioning systems, on the grounds that using flammable hydrocarbons in systems originally designed to carry non-flammable refrigerant presents a significant risk of fire or explosion.\nVendors and advocates of hydrocarbon refrigerants argue against such bans on the grounds that there have been very few such incidents relative to the number of vehicle air conditioning systems filled with hydrocarbons.\nPropane is also instrumental in providing off-the-grid refrigeration, as the energy source for a gas absorption refrigerator and is commonly used for camping and recreational vehicles.\nIt has also been proposed to use propane as a refrigerant in heat pumps.\nDomestic and industrial fuel.\nSince it can be transported easily, it is a popular fuel for home heat and backup electrical generation in sparsely populated areas that do not have natural gas pipelines. In June 2023, Stanford researchers found propane combustion emitted detectable and repeatable levels of benzene that in some homes raised indoor benzene concentrations above well-established health benchmarks. The research also shows that gas and propane fuels appear to be the dominant source of benzene produced by cooking.\nIn rural areas of North America, as well as northern Australia, propane is used to heat livestock facilities, in grain dryers, and other heat-producing appliances. When used for heating or grain drying it is usually stored in a large, permanently-placed cylinder which is refilled by a propane-delivery truck. As of 2014[ [update]], 6.2 million American households use propane as their primary heating fuel.\nIn North America, local delivery trucks with an average cylinder size of , fill up large cylinders that are permanently installed on the property, or other service trucks exchange empty cylinders of propane with filled cylinders. Large tractor-trailer trucks, with an average cylinder size of , transport propane from the pipeline or refinery to the local bulk plant. The bobtail tank truck is not unique to the North American market, though the practice is not as common elsewhere, and the vehicles are generally called \"tankers\". In many countries, propane is delivered to end-users via small or medium-sized individual cylinders, while empty cylinders are removed for refilling at a central location.\nThere are also community propane systems, with a central cylinder feeding individual homes.\nMotor fuel.\nIn the U.S., over 190,000 on-road vehicles use propane, and over 450,000 forklifts use it for power. It is the third most popular vehicle fuel in the world, behind gasoline and diesel fuel. In other parts of the world, propane used in vehicles is known as autogas. In 2007, approximately 13 million vehicles worldwide use autogas.\nThe advantage of propane in cars is its liquid state at a moderate pressure. This allows fast refill times, affordable fuel cylinder construction, and price ranges typically just over half that of gasoline. Meanwhile, it is noticeably cleaner (both in handling, and in combustion), results in less engine wear (due to carbon deposits) without diluting engine oil (often extending oil-change intervals), and until recently was relatively low-cost in North America. The octane rating of propane is relatively high at 110. In the United States the propane fueling infrastructure is the most developed of all alternative vehicle fuels. Many converted vehicles have provisions for topping off from \"barbecue bottles\". Purpose-built vehicles are often in commercially owned fleets, and have private fueling facilities. A further saving for propane fuel vehicle operators, especially in fleets, is that theft is much more difficult than with gasoline or diesel fuels.\nPropane is also used as fuel for small engines, especially those used indoors or in areas with insufficient fresh air and ventilation to carry away the more toxic exhaust of an engine running on gasoline or diesel fuel. More recently, there have been lawn-care products like string trimmers, lawn mowers and leaf blowers intended for outdoor use, but fueled by propane in order to reduce air pollution.\nMany heavy-duty highway trucks use propane as a boost, where it is added through the turbocharger, to mix with diesel fuel droplets. The very high hydrogen content of propane helps the diesel fuel to burn hotter and therefore more completely. This provides more torque, more horsepower, and a cleaner exhaust for the trucks. It is normal for a 7-liter medium-duty diesel truck engine to increase fuel economy by 20 to 33 percent when a propane boost system is used. It is cheaper because propane is much cheaper than diesel fuel. The longer distance a cross-country trucker can travel on a full load of combined diesel and propane fuel means they can maintain federal hours of work rules with two fewer fuel stops in a cross-country trip. Truckers, tractor pulling competitions, and farmers have been using a propane boost system for over forty years in North America.\nPurity.\nThe North American standard grade of automotive-use propane is rated HD-5 (Heavy Duty 5%). HD-5 grade has a maximum of 5 percent butane, but propane sold in Europe has a maximum allowable amount of butane of 30 percent, meaning it is not the same fuel as HD-5. The LPG used as auto fuel and cooking gas in Asia and Australia also has very high butane content.\nPropylene (also called propene) can be a contaminant of commercial propane. Propane containing too much propene is not suited for most vehicle fuels. HD-5 is a specification that establishes a maximum concentration of 5% propene in propane. Propane and other LP gas specifications are established in ASTM D-1835. All propane fuels include an odorant, almost always ethanethiol, so that the gas can be smelled easily in case of a leak. Propane as HD-5 was originally intended for use as vehicle fuel. HD-5 is currently being used in all propane applications.\nTypically in the United States and Canada, LPG is primarily propane (at least 90%), while the rest is mostly ethane, propylene, butane, and odorants including ethyl mercaptan. This is the HD-5 standard, (maximum allowable propylene content, and no more than 5% butanes and ethane) defined by the American Society for Testing and Materials by its Standard 1835 for internal combustion engines. Not all products labeled \"LPG\" conform to this standard, however. In Mexico, for example, gas labeled \"LPG\" may consist of 60% propane and 40% butane. \"The exact proportion of this combination varies by country, depending on international prices, on the availability of components and, especially, on the climatic conditions that favor LPG with higher butane content in warmer regions and propane in cold areas\".\nComparison with natural gas.\nPropane is bought and stored in a liquid form, LPG. It can easily be stored in a relatively small space.\nBy comparison, compressed natural gas (CNG) cannot be liquefied by compression at normal temperatures, as these are well above its critical temperature. As a gas, very high pressure is required to store useful quantities. This poses the hazard that, in an accident, just as with any compressed gas cylinder (such as a CO2 cylinder used for a soda concession) a CNG cylinder may burst with great force, or leak rapidly enough to become a self-propelled missile. Therefore, CNG is much less efficient to store than propane, due to the large cylinder volume required. An alternative means of storing natural gas is as a cryogenic liquid in an insulated container as liquefied natural gas (LNG). This form of storage is at low pressure and is around 3.5 times as efficient as storing it as CNG.\nUnlike propane, if a spill occurs, CNG will evaporate and dissipate because it is lighter than air.\nPropane is much more commonly used to fuel vehicles than is natural gas, because that equipment costs less. Propane requires just of pressure to keep it liquid at .\nHazards.\nPropane is a simple asphyxiant. Unlike natural gas, it is denser than air. It may accumulate in low spaces and near the floor. When abused as an inhalant, it may cause hypoxia (lack of oxygen), pneumonia, cardiac failure or cardiac arrest. Propane has low toxicity since it is not readily absorbed and is not biologically active. Commonly stored under pressure at room temperature, propane and its mixtures will flash evaporate at atmospheric pressure and cool well below the freezing point of water. The cold gas, which appears white due to moisture condensing from the air, may cause frostbite.\nPropane is denser than air. If a leak in a propane fuel system occurs, the vaporized gas will have a tendency to sink into any enclosed area and thus poses a risk of explosion and fire. The typical scenario is a leaking cylinder stored in a basement; the propane leak drifts across the floor to the pilot light on the furnace or water heater, and results in an explosion or fire. This property makes propane generally unsuitable as a fuel for boats. In 2007, a heavily investigated vapor-related explosion occurred in Ghent, West Virginia, U.S., killing four people and completely destroying the Little General convenience store on Flat Top Road, causing several injuries.\nAnd Another hazard associated with propane storage and transport is known as a BLEVE or boiling liquid expanding vapor explosion. The Kingman Explosion involved a railroad tank car in Kingman, Arizona, U.S., in 1973 during a propane transfer. The fire and subsequent explosions resulted in twelve fatalities and numerous injuries.\nProduction.\nPropane is produced as a by-product of two other processes, natural gas processing and petroleum refining. The processing of natural gas involves removal of butane, propane, and large amounts of ethane from the raw gas, to prevent condensation of these volatiles in natural gas pipelines. Additionally, oil refineries produce some propane as a by-product of cracking petroleum into gasoline or heating oil.\nThe supply of propane cannot easily be adjusted to meet increased demand, because of the by-product nature of propane production. About 90% of U.S. propane is domestically produced. The United States imports about 10% of the propane consumed each year, with about 70% of that coming from Canada via pipeline and rail. The remaining 30% of imported propane comes to the United States from other sources via ocean transport.\nAfter it is separated from the crude oil, North American propane is stored in huge salt caverns. Examples of these are Fort Saskatchewan, Alberta; Mont Belvieu, Texas; and Conway, Kansas. These salt caverns can store of propane.\nRetail cost.\nUnited States.\nAs of \u00a02013[ [update]], the retail cost of propane was approximately $2.37 per gallon, or roughly $25.95 per 1 million BTUs. This means that filling a 500-gallon propane tank, which is what households that use propane as their main source of energy usually require, cost $948 (80% of 500 gallons or 400 gallons), a 7.5% increase on the 2012\u20132013 winter season average US price. However, propane costs per gallon change significantly from one state to another: the Energy Information Administration (EIA) quotes a $2.995 per gallon average on the East Coast for October 2013, while the figure for the Midwest was $1.860 for the same period.\nAs of \u00a02015[ [update]], the propane retail cost was approximately $1.97 per gallon, which meant filling a 500-gallon propane tank to 80% capacity costed $788, a 16.9% decrease or $160 less from November 2013. Similar regional differences in prices are present with the December 2015 EIA figure for the East Coast at $2.67 per gallon and the Midwest at $1.43 per gallon.\nAs of \u00a02018[ [update]], the average US propane retail cost was approximately $2.48 per gallon. The wholesale price of propane in the U.S. always drops in the summer as most homes do not require it for home heating. The wholesale price of propane in the summer of 2018 was between 86 cents to 96 cents per U.S. gallon, based on a truckload or railway car load. The price for home heating was exactly double that price; at 95 cents per gallon wholesale, a home-delivered price was $1.90 per gallon if ordered 500 gallons at a time. Prices in the Midwest are always less than in California. Prices for home delivery always go up near the end of August or the first few days of September when people start ordering their home tanks to be filled.\nOn other planets.\nPropane was first discovered in space in 1981 in the atmosphere of Saturn's moon Titan based on spectroscopic observations from the Voyager 1 IRIS instrument. This was subsequently confirmed by ground-based observations using the TEXES instrument on the IRTF telescope and mapped by the CIRS instrument on Cassini. \nPropane was subsequently been detected in the atmosphere of Saturn using TEXES, and predicted but unconfirmed on Jupiter, Neptune and Uranus. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23644", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=23644", "title": "PAN", "text": ""}
{"id": "23645", "revid": "43520964", "url": "https://en.wikipedia.org/wiki?curid=23645", "title": "Precambrian", "text": "History of Earth 4600\u2013539 million years ago\nThe Precambrian ( ; or pre-Cambrian, sometimes abbreviated pC, or Cryptozoic) is the earliest part of Earth's history, set before the current Phanerozoic Eon. The Precambrian is so named because it preceded the Cambrian, the first period of the Phanerozoic Eon, which is named after Cambria, the Latinized name for Wales, where rocks from this age were first studied. The Precambrian accounts for 88% of the Earth's geologic time.\nThe Precambrian is an informal unit of geologic time, subdivided into three eons (Hadean, Archean, Proterozoic) of the geologic time scale. It spans from the formation of Earth about 4.6 billion years ago (Ga) to the beginning of the Cambrian Period, about million years ago (Ma), when hard-shelled creatures first appeared in abundance.\nOverview.\nRelatively little is known about the Precambrian, despite it making up roughly seven-eighths of the Earth's history, and what is known has largely been discovered from the 1960s onwards. The Precambrian fossil record is poorer than that of the succeeding Phanerozoic, and fossils from the Precambrian (e.g. stromatolites) are of limited biostratigraphic use. This is because many Precambrian rocks have been heavily metamorphosed, obscuring their origins, while others have been destroyed by erosion, or remain deeply buried beneath Phanerozoic strata.\nIt is thought that the Earth coalesced from material in orbit around the Sun at roughly 4,543 Ma, and may have been struck by another planet called Theia shortly after it formed, splitting off material that formed the Moon (see Giant-impact hypothesis). A stable crust was apparently in place by 4,433 Ma, since zircon crystals from Western Australia have been dated at 4,404 \u00b1 8 Ma.\nThe term \"Precambrian\" is used by geologists and paleontologists for general discussions not requiring a more specific eon name. However, both the United States Geological Survey and the International Commission on Stratigraphy regard the term as informal. Because the span of time falling under the Precambrian consists of three eons (the Hadean, the Archean, and the Proterozoic), it is sometimes described as a \"supereon\", but this is also an informal term, not defined by the ICS in its chronostratigraphic guide.\n&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;Eozoic (from \"earliest\") was a synonym for \"pre-Cambrian\" or \"Precambrian\", or more specifically \"Archean\".\nLife forms.\nA specific date for the origin of life has not been determined. Carbon found in 3.8 billion-year-old rocks (Archean Eon) from islands off western Greenland may be of organic origin. Well-preserved microscopic fossils of bacteria older than 3.46 billion years have been found in Western Australia. Probable fossils 100 million years older have been found in the same area. However, there is evidence that life could have evolved over 4.280 billion years ago. There is a fairly solid record of bacterial life throughout the remainder (Proterozoic Eon) of the Precambrian.\nComplex multicellular organisms may have appeared as early as 2100 Ma. However, the interpretation of ancient fossils is problematic, and \"... some definitions of multicellularity encompass everything from simple bacterial colonies to badgers.\" Other possible early complex multicellular organisms include a possible 2450 Ma red alga from the Kola Peninsula, 1650 Ma carbonaceous biosignatures in north China, the 1600 Ma \"Rafatazmia\", and a possible 1047 Ma \"Bangiomorpha\" red alga from the Canadian Arctic. The earliest fossils widely accepted as complex multicellular organisms date from the Ediacaran Period. A very diverse collection of soft-bodied forms is found in a variety of locations worldwide and date to between 635 and 542 Ma. These are referred to as Ediacaran or Vendian biota. Hard-shelled creatures appeared toward the end of that time span, marking the beginning of the Phanerozoic Eon. By the middle of the following Cambrian Period, a very diverse fauna is recorded in the Burgess Shale, including some which may represent stem groups of modern taxa. The increase in diversity of lifeforms during the early Cambrian is called the Cambrian explosion of life.\nWhile land seems to have been devoid of plants and animals, cyanobacteria and other microbes formed prokaryotic mats that covered terrestrial areas.\nTracks from an animal with leg-like appendages have been found in what was mud 551 million years ago.\nEmergence of life.\nThe RNA world hypothesis asserts that RNA evolved before coded proteins and DNA genomes. During the Hadean Eon (4,567\u20134,031 Ma) abundant geothermal microenvironments were present that may have had the potential to support the synthesis and replication of RNA and thus possibly the evolution of a primitive life form. It was shown that porous rock systems comprising heated air-water interfaces could allow ribozyme-catalyzed RNA replication of sense and antisense strands that could be followed by strand-dissociation, thus enabling combined synthesis, release and folding of active ribozymes. This primitive RNA replicative system also may have been able to undergo template strand switching during replication (genetic recombination) as is known to occur during the RNA replication of extant coronaviruses.\nPlanetary environment and the oxygen catastrophe.\nEvidence of the details of plate motions and other tectonic activity in the Precambrian is difficult to interpret. It is generally believed that small proto-continents existed before 4280 Ma, and that most of the Earth's landmasses collected into a single supercontinent around 1130 Ma. The supercontinent, known as Rodinia, broke up around 750 Ma. A number of glacial periods have been identified going as far back as the Huronian epoch, roughly 2400\u20132100 Ma. One of the best studied is the Sturtian-Varangian glaciation, around 850\u2013635 Ma, which may have brought glacial conditions all the way to the equator, resulting in a \"Snowball Earth\".\nIt is believed that molecular oxygen was not a significant fraction of Earth's atmosphere until after photosynthetic life forms evolved and began to produce it in large quantities as a byproduct of their metabolism. This radical shift from a chemically inert to an oxidizing atmosphere caused an ecological crisis, sometimes called the oxygen catastrophe. At first, oxygen would have quickly combined with other elements in Earth's crust, primarily iron, removing it from the atmosphere. After the supply of oxidizable surfaces ran out, oxygen would have begun to accumulate in the atmosphere, and the modern high-oxygen atmosphere would have developed. Evidence for this lies in older rocks that contain massive banded iron formations that were laid down as iron oxides.\nSubdivisions.\nA terminology has evolved covering the early years of the Earth's existence, as radiometric dating has allowed absolute dates to be assigned to specific formations and features. The Precambrian is divided into three eons: the Hadean (\u2013 Ma), Archean (- Ma) and Proterozoic (- Ma). See Timetable of the Precambrian.\nIt has been proposed that the Precambrian should be divided into eons and eras that reflect stages of planetary evolution, rather than the current scheme based upon numerical ages. Such a system could rely on events in the stratigraphic record and be demarcated by GSSPs. The Precambrian could be divided into five \"natural\" eons, characterized as follows:\nPrecambrian supercontinents.\nThe movement of Earth's plates has caused the formation and break-up of continents over time, including occasional formation of a supercontinent containing most or all of the landmass. The earliest known supercontinent was Vaalbara. It formed from proto-continents and was a supercontinent 3.636 billion years ago. Vaalbara broke up c. 2.845\u20132.803 Ga ago. The supercontinent Kenorland was formed c. 2.72 Ga ago and then broke sometime after 2.45\u20132.1 Ga into the proto-continent cratons called Laurentia, Baltica, Yilgarn craton and Kalahari. The supercontinent Columbia, or Nuna, formed 2.1\u20131.8 billion years ago and broke up about 1.3\u20131.2 billion years ago. The supercontinent Rodinia is thought to have formed about 1300-900 Ma, to have included most or all of Earth's continents and to have broken up into eight continents around 750\u2013600 million years ago."}
{"id": "23647", "revid": "39368978", "url": "https://en.wikipedia.org/wiki?curid=23647", "title": "Polymerase chain reaction", "text": "Laboratory technique to multiply a DNA sample for study\nThe polymerase chain reaction (PCR) is a laboratory method widely used to amplify copies of specific DNA sequences rapidly, to enable detailed study. PCR was invented in 1983 by American biochemist Kary Mullis at Cetus Corporation. Mullis and biochemist Michael Smith, who had developed other essential ways of manipulating DNA, were jointly awarded the Nobel Prize in Chemistry in 1993.\nPCR is fundamental to many of the procedures used in genetic testing, research, including analysis of ancient samples of DNA and identification of infectious agents. Using PCR, copies of very small amounts of DNA sequences are exponentially amplified in a series of cycles of temperature changes. PCR is now a common and often indispensable technique used in medical laboratory research for a broad variety of applications including biomedical research and forensic science.\nThe majority of PCR methods rely on thermal cycling. Thermal cycling exposes reagents to repeated cycles of heating and cooling to permit different temperature-dependent reactions\u2014specifically, DNA melting and enzyme-driven DNA replication. PCR employs two main reagents\u2014primers (which are short single strand DNA fragments known as oligonucleotides that are a complementary sequence to the target DNA region) and a thermostable DNA polymerase. In the first step of PCR, the two strands of the DNA double helix are physically separated at a high temperature in a process called nucleic acid denaturation. In the second step, the temperature is lowered and the primers bind to the complementary sequences of DNA. The two DNA strands then become templates for DNA polymerase to enzymatically assemble a new DNA strand from free nucleotides, the building blocks of DNA. As PCR progresses, the DNA generated is itself used as a template for replication, setting in motion a chain reaction in which the original DNA template is exponentially amplified.\nAlmost all PCR applications employ a heat-stable DNA polymerase, such as \"Taq\" polymerase, an enzyme originally isolated from the thermophilic bacterium \"Thermus aquaticus\". If the polymerase used was heat-susceptible, it would denature under the high temperatures of the denaturation step. Before the use of \"Taq\" polymerase, DNA polymerase had to be manually added every cycle, which was a tedious and costly process.\nApplications of the technique include DNA cloning for sequencing, gene cloning and manipulation, gene mutagenesis; construction of DNA-based phylogenies, or functional analysis of genes; diagnosis and monitoring of genetic disorders; amplification of ancient DNA; analysis of genetic fingerprints for DNA profiling (for example, in forensic science and parentage testing); and detection of pathogens in nucleic acid tests for the diagnosis of infectious diseases.\nPrinciples.\nPCR amplifies a specific region of a DNA strand (the DNA target). Most PCR methods amplify DNA fragments of between 0.1 and 10 kilo-base pairs (kbp) in length, although some techniques allow for amplification of fragments up to 40 kbp. The amount of amplified product is determined by the available substrates in the reaction, which becomes limiting as the reaction progresses.\nA basic PCR set-up requires several components and reagents, including:\nThe reaction is commonly carried out in a volume of 10\u2013200\u00a0\u03bcL in small reaction tubes (0.2\u20130.5\u00a0mL volumes) in a thermal cycler. The thermal cycler heats and cools the reaction tubes to achieve the temperatures required at each step of the reaction (see below). Many modern thermal cyclers make use of a Peltier device, which permits both heating and cooling of the block holding the PCR tubes simply by reversing the device's electric current. Thin-walled reaction tubes permit favorable thermal conductivity to allow for rapid thermal equilibrium. Most thermal cyclers have heated lids to prevent condensation at the top of the reaction tube. Older thermal cyclers lacking a heated lid require a layer of oil on top of the reaction mixture or a ball of wax inside the tube.\nProcedure.\nTypically, PCR consists of a series of 20\u201340 repeated temperature changes, called thermal cycles, with each cycle commonly consisting of two or three discrete temperature steps (see figure below). The cycling is often preceded by a single temperature step at a very high temperature (&gt;), and followed by one hold at the end for final product extension or brief storage. The temperatures used and the length of time they are applied in each cycle depend on a variety of parameters, including the enzyme used for DNA synthesis, the concentration of bivalent ions and dNTPs in the reaction, and the melting temperature (\"Tm\") of the primers. The individual steps common to most PCR methods are : initialization, annealing and extension.\n It is critical to determine a proper temperature for the annealing step because efficiency and specificity are strongly affected by the annealing temperature. This temperature must be low enough to allow for hybridization of the primer to the strand, but high enough for the hybridization to be specific, i.e., the primer should bind \"only\" to a perfectly complementary part of the strand, and nowhere else. If the temperature is too low, the primer may bind imperfectly. If it is too high, the primer may not bind at all. A typical annealing temperature is about 3\u20135\u00a0\u00b0C below the \"Tm\" of the primers used. Stable hydrogen bonds between complementary bases are formed only when the primer sequence very closely matches the template sequence. During this step, the polymerase binds to the primer-template hybrid and begins DNA formation.\n The processes of denaturation, annealing and elongation constitute a single cycle. Multiple cycles are required to amplify the DNA target to millions of copies. The formula used to calculate the number of DNA copies formed after a given number of cycles is 2n, where \"n\" is the number of cycles. Thus, a reaction set for 30 cycles results in 230, or copies of the original double-stranded DNA target region.\nTo check whether the PCR successfully generated the anticipated DNA target region (also sometimes referred to as the amplimer or amplicon), agarose gel electrophoresis may be employed for size separation of the PCR products. The size of the PCR products is determined by comparison with a DNA ladder, a molecular weight marker which contains DNA fragments of known sizes, which runs on the gel alongside the PCR products.\nStages.\nAs with other chemical reactions, the reaction rate and efficiency of PCR are affected by limiting factors. Thus, the entire PCR process can further be divided into three stages based on reaction progress:\nOptimization.\nIn practice, PCR can fail for various reasons, such as sensitivity or contamination. Contamination with extraneous DNA can lead to spurious products and is addressed with lab protocols and procedures that separate pre-PCR mixtures from potential DNA contaminants. For instance, if DNA from a crime scene is analyzed, a single DNA molecule from lab personnel could be amplified and misguide the investigation. Hence the PCR-setup areas is separated from the analysis or purification of other PCR products, disposable plasticware used, and the work surface between reaction setups needs to be thoroughly cleaned.\nSpecificity can be adjusted by experimental conditions so that no spurious products are generated. Primer-design techniques are important in improving PCR product yield and in avoiding the formation of unspecific products. The usage of alternate buffer components or polymerase enzymes can help with amplification of long or otherwise problematic regions of DNA. For instance, Q5 polymerase is said to be \u2248280 times less error-prone than Taq polymerase. Both the running parameters (e.g. temperature and duration of cycles), or the addition of reagents, such as formamide, may increase the specificity and yield of PCR. Computer simulations of theoretical PCR results (Electronic PCR) may be performed to assist in primer design.\nApplications.\nSelective DNA isolation.\nPCR allows isolation of DNA fragments from genomic DNA by selective amplification of a specific region of DNA. This use of PCR augments, such as generating hybridization probes for Southern or northern hybridization and DNA cloning, which require larger amounts of DNA, representing a specific DNA region. PCR supplies these techniques with high amounts of pure DNA, enabling analysis of DNA samples even from very small amounts of starting material.\nOther applications of PCR include DNA sequencing to determine unknown PCR-amplified sequences in which one of the amplification primers may be used in Sanger sequencing, isolation of a DNA sequence to expedite recombinant DNA technologies involving the insertion of a DNA sequence into a plasmid, phage, or cosmid (depending on size) or the genetic material of another organism. Bacterial colonies \"(such as E. coli)\" can be rapidly screened by PCR for correct DNA vector constructs. PCR may also be used for genetic fingerprinting; a forensic technique used to identify a person or organism by comparing experimental DNAs through different PCR-based methods.\nSome PCR fingerprint methods have high discriminative power and can be used to identify genetic relationships between individuals, such as parent-child or between siblings, and are used in paternity testing (Fig. 4). This technique may also be used to determine evolutionary relationships among organisms when certain molecular clocks are used (i.e. the 16S rRNA and recA genes of microorganisms).\nAmplification and quantification of DNA.\nBecause PCR amplifies the regions of DNA that it targets, PCR can be used to analyze extremely small amounts of sample. This is often critical for forensic analysis, when only a trace amount of DNA is available as evidence. PCR may also be used in the analysis of ancient DNA that is tens of thousands of years old. These PCR-based techniques have been successfully used on animals, such as a forty-thousand-year-old mammoth, and also on human DNA, in applications ranging from the analysis of Egyptian mummies to the identification of a Russian tsar and the body of English king Richard III.\nQuantitative PCR or Real Time PCR, (qPCR not to be confused with RT-PCR) methods allow the estimation of the amount of a given sequence present in a sample\u2014a technique often applied to quantitatively determine levels of gene expression. The MIQE guidelines written by professors Stephen Bustin, Michael Pfaffl, Mikael Kubista and colleagues outline how qPCR experiments shall be performed and the results reported. Quantitative PCR is an established tool for DNA quantification that measures the accumulation of DNA product after each round of PCR amplification. \nqPCR allows the quantification and detection of a specific DNA sequence in real time since it measures concentration while the synthesis process is taking place. There are two methods for simultaneous detection and quantification. The first method consists of using fluorescent dyes that are retained nonspecifically in between the double strands. The second method involves probes that code for specific sequences and are fluorescently labeled. Detection of DNA using these methods can only be seen after the hybridization of probes with its complementary DNA (cDNA) takes place. An interesting technique combination is real-time PCR and reverse transcription. This sophisticated technique, called RT-qPCR, allows for the quantification of a small quantity of RNA. Through this combined technique, mRNA is converted to cDNA, which is further quantified using qPCR. This technique lowers the possibility of error at the end point of PCR, increasing chances for detection of genes associated with genetic diseases such as cancer. Laboratories use RT-qPCR for the purpose of sensitively measuring gene regulation. The mathematical foundations for the reliable quantification of the PCR and RT-qPCR facilitate the implementation of accurate fitting procedures of experimental data in research, medical, diagnostic and infectious disease applications.\nMedical and diagnostic applications.\nProspective parents can be tested for being genetic carriers, or their children might be tested for actually being affected by a disease. DNA samples for prenatal testing can be obtained by amniocentesis, chorionic villus sampling, or even by the analysis of rare fetal cells circulating in the mother's bloodstream. PCR analysis is also essential to preimplantation genetic diagnosis, where individual cells of a developing embryo are tested for mutations.\nInfectious disease applications.\nPCR allows for rapid and highly specific diagnosis of infectious diseases, including those caused by bacteria or viruses. PCR also permits identification of non-cultivatable or slow-growing microorganisms such as mycobacteria, anaerobic bacteria, or viruses from tissue culture assays and animal models. The basis for PCR diagnostic applications in microbiology is the detection of infectious agents and the discrimination of non-pathogenic from pathogenic strains by virtue of specific genes.\nCharacterization and detection of infectious disease organisms have been revolutionized by PCR in the following ways:\nForensic applications.\nThe development of PCR-based genetic (or DNA) fingerprinting protocols has seen widespread application in forensics:\nResearch applications.\nPCR has been applied to many areas of research in molecular genetics:\nAdvantages.\nPCR has a number of advantages. It is fairly simple to understand and to use, and produces results rapidly. The technique is highly sensitive with the potential to produce millions to billions of copies of a specific product for sequencing, cloning, and analysis. qRT-PCR shares the same advantages as the PCR, with an added advantage of quantification of the synthesized product. Therefore, it has its uses to analyze alterations of gene expression levels in tumors, microbes, or other disease states.\nPCR is a very powerful and practical research tool. The sequencing of unknown etiologies of many diseases are being figured out by the PCR. The technique can help identify the sequence of previously unknown viruses related to those already known and thus give us a better understanding of the disease itself. If the procedure can be further simplified and sensitive non-radiometric detection systems can be developed, the PCR will assume a prominent place in the clinical laboratory for years to come.\nLimitations.\nOne major limitation of PCR is that prior information about the target sequence is necessary in order to generate the primers that will allow its selective amplification. This means that, typically, PCR users must know the precise sequence(s) upstream of the target region on each of the two single-stranded templates in order to ensure that the DNA polymerase properly binds to the primer-template hybrids and subsequently generates the entire target region during DNA synthesis.\nLike all enzymes, DNA polymerases are also prone to error, which in turn causes mutations in the PCR fragments that are generated.\nAnother limitation of PCR is that even the smallest amount of contaminating DNA can be amplified, resulting in misleading or ambiguous results. To minimize the chance of contamination, investigators should reserve separate rooms for reagent preparation, the PCR, and analysis of product. Reagents should be dispensed into single-use aliquots. Pipettors with disposable plungers and extra-long pipette tips should be routinely used. It is moreover recommended to ensure that the lab set-up follows a unidirectional workflow. No materials or reagents used in the PCR and analysis rooms should ever be taken into the PCR preparation room without thorough decontamination.\nEnvironmental samples that contain humic acids may inhibit PCR amplification and lead to inaccurate results.\nHistory.\nThe heat-resistant enzymes that are a key component in polymerase chain reaction were discovered in the 1960s as a product of a microbial life form that lived in the superheated waters of Yellowstone's Mushroom Spring.\nA 1971 paper in the \"Journal of Molecular Biology\" by Kjell Kleppe and co-workers in the laboratory of H. Gobind Khorana first described a method of using an enzymatic assay to replicate a short DNA template with primers \"in vitro\". However, this early manifestation of the basic PCR principle did not receive much attention at the time and the invention of the polymerase chain reaction in 1983 is generally credited to Kary Mullis.\nWhen Mullis developed the PCR in 1983, he was working in Emeryville, California for Cetus Corporation, one of the first biotechnology companies, where he was responsible for synthesizing short chains of DNA. Mullis has written that he conceived the idea for PCR while cruising along the Pacific Coast Highway one night in his car. He was playing in his mind with a new way of analyzing changes (mutations) in DNA when he realized that he had instead invented a method of amplifying any DNA region through repeated cycles of duplication driven by DNA polymerase. In \"Scientific American\", Mullis summarized the procedure: \"Beginning with a single molecule of the genetic material DNA, the PCR can generate 100 billion similar molecules in an afternoon. The reaction is easy to execute. It requires no more than a test tube, a few simple reagents, and a source of heat.\" DNA fingerprinting was first used for paternity testing in 1988.\nMullis has credited his use of LSD as integral to his development of PCR: \"Would I have invented PCR if I hadn't taken LSD? I seriously doubt it. I learnt that partly on psychedelic drugs.\"\nMullis and biochemist Michael Smith, who had developed other essential ways of manipulating DNA, were jointly awarded the Nobel Prize in Chemistry in 1993, seven years after Mullis and his colleagues at Cetus first put his proposal to practice. Mullis's 1985 paper with R. K. Saiki and H. A. Erlich, \"Enzymatic Amplification of \u03b2-globin Genomic Sequences and Restriction Site Analysis for Diagnosis of Sickle Cell Anemia\"\u2014the polymerase chain reaction invention (PCR)\u2014was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society in 2017.\nAt the core of the PCR method is the use of a suitable DNA polymerase able to withstand the high temperatures of &gt; required for separation of the two DNA strands in the DNA double helix after each replication cycle. The DNA polymerases initially employed for in vitro experiments presaging PCR were unable to withstand these high temperatures. So the early procedures for DNA replication were very inefficient and time-consuming, and required large amounts of DNA polymerase and continuous handling throughout the process.\nThe discovery in 1976 of \"Taq\" polymerase\u2014a DNA polymerase purified from the thermophilic bacterium, \"Thermus aquaticus\", which naturally lives in hot () environments such as hot springs\u2014paved the way for dramatic improvements of the PCR method. The DNA polymerase isolated from \"T. aquaticus\" is stable at high temperatures remaining active even after DNA denaturation, thus obviating the need to add new DNA polymerase after each cycle. This allowed an automated thermocycler-based process for DNA amplification.\nPatent disputes.\nThe PCR technique was patented by Kary Mullis and assigned to Cetus Corporation, where Mullis worked when he invented the technique in 1983. The \"Taq\" polymerase enzyme was also covered by patents. There have been several lawsuits related to the technique. brought by DuPont. The Swiss pharmaceutical company Hoffmann-La Roche purchased the rights to the patents in 1992. The last of the commercial PCR patents expired in 2017.\nA related patent battle over the \"Taq\" polymerase enzyme is still ongoing in several jurisdictions around the world between Roche and Promega. The legal arguments have extended beyond the lives of the original PCR and \"Taq\" polymerase patents, which expired on 28 March 2005.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23648", "revid": "42219488", "url": "https://en.wikipedia.org/wiki?curid=23648", "title": "Polymerase", "text": "Class of enzymes which synthesize nucleic acid chains or polymers\nIn biochemistry, a polymerase is an enzyme (EC 2.7.7.6/7/19/48/49) that synthesizes long chains of polymers or nucleic acids. DNA polymerase and RNA polymerase are used to assemble DNA and RNA molecules, respectively, by copying a DNA template strand using base-pairing interactions or half ladder replication.\nA DNA polymerase from the thermophilic bacterium, \"Thermus aquaticus\" (\"Taq\") (PDB http://, EC 2.7.7.7), is used in the polymerase chain reaction, an important technique of molecular biology.\nA polymerase may be template-dependent or template-independent. Poly-A-polymerase is an example of template independent polymerase. Terminal deoxynucleotidyl transferase is also known to have template independent and template dependent activities.\nBy structure.\nPolymerases are generally split into two superfamilies, the \"right hand\" fold (InterPro:\u00a0\"https://\") and the \"double psi beta barrel\" (often simply \"double-barrel\") fold. The former is seen in almost all DNA polymerases and almost all viral single-subunit polymerases; they are marked by a conserved \"palm\" domain. The latter is seen in all multi-subunit RNA polymerases, in cRdRP, and in \"family D\" DNA polymerases found in archaea. The \"X\" family represented by DNA polymerase beta has only a vague \"palm\" shape, and is sometimes considered a different superfamily (InterPro:\u00a0\"https://\").\nPrimases generally don't fall into either category. Bacterial primases usually have the Toprim domain, and are related to topoisomerases and mitochondrial helicase twinkle. Archae and eukaryotic primases form an unrelated AEP family, possibly related to the polymerase palm. Both families nevertheless associate to the same set of helicases.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23649", "revid": "40730637", "url": "https://en.wikipedia.org/wiki?curid=23649", "title": "Pacific Scandal", "text": "Canadian political scandal in 1873\nThe Pacific Scandal was a political scandal in Canada involving large sums of money paid by private interests to the Conservative Party to cover election expenses in the 1872 Canadian federal election in order to influence the bidding for a national rail contract. As part of British Columbia's 1871 agreement to join the Canadian Confederation, the federal government had agreed to build a transcontinental railway linking the seaboard of British Columbia to the eastern provinces.\nThe scandal led to the resignation of Canada's first prime minister, John A. Macdonald, and a transfer of power from his Conservative government to a Liberal government, led by Alexander Mackenzie. One of the new government's first measures was to introduce secret ballots in an effort to improve the integrity of future elections. After the scandal broke, the railway plan collapsed, and the proposed line was not built. An entirely different operation later built the Canadian Pacific Railway to the Pacific.\nBackground.\nFor a young and loosely defined nation, the building of a national railway was an active attempt at state-making, as well as an aggressive capitalist venture. Canada, a nascent country with a population of 3.5 million in 1871, lacked the means to exercise meaningful \"de facto\" control within the \"de jure\" political boundaries of the recently acquired Rupert's Land, and building a transcontinental railway was a national policy of high order to change that situation. Moreover, after the American Civil War, land-hungry settlers rapidly pushed the American frontier westward, exacerbating talk of annexation. Sentiments of Manifest Destiny were abuzz at the time: in 1867, the year of Canada's Confederation, US Secretary of State William H. Seward surmised that the whole North American continent \"shall be, sooner or later, within the magic circle of the American Union.\" Consequently, preventing American investment in the project was considered to be in Canada's national interest. Thus the federal government favoured an \"all Canadian route\" through the rugged Canadian Shield of northern Ontario and refused to consider a less-costly route passing south through Wisconsin and Minnesota.\nHowever, a route across the Canadian Shield was highly unpopular with potential investors in not only the United States but also Canada and especially Great Britain, the only other viable sources of financing. For would-be investors, the objections were primarily based not on politics or nationalism but on economics. At the time, national governments lacked the finances needed to undertake such large projects. For the first transcontinental railroad, the United States government had made extensive grants of public land to the railway's builders, inducing private financiers to fund the railway on the understanding that they would acquire rich farmland along the route, which could then be sold for a large profit. However, the eastern terminus of the proposed Canadian Pacific route, unlike that of the first transcontinental, was not in rich Nebraskan farmland but deep within the Canadian Shield. Copying the American financing model while insisting on an all-Canadian route would require the railway's backers to build hundreds of miles of track across rugged shield terrain, with little economic value, at considerable expense before they could access lucrative farmland in Manitoba and the newly created Northwest Territories, which at that time included Alberta and Saskatchewan. Many financiers, who had expected to make a relatively quick profit, were not willing to make such a long-term commitment.\nNevertheless, the Montreal capitalist Hugh Allan, with his syndicate Canada Pacific Railway Company, sought the potentially lucrative charter for the project. The problem lay in that Allan and Macdonald were secretly in cahoots with American financiers such as George W. McMullen and Jay Cooke, who were deeply interested in the rival American undertaking, the Northern Pacific Railroad.\nScandal.\nTwo groups competed for the contract to build the railway, Hugh Allan's Canada Pacific Railway Company and David Lewis Macpherson's Inter-Oceanic Railway Company. On April 2, 1873, Lucius Seth Huntington, a Liberal Member of Parliament, created an uproar in the House of Commons. He announced he had uncovered evidence that Allan and his associates had been granted the Canadian Pacific Railway contract in return for political donations of $360,000.\nIn 1873, it became known that Allan had contributed a large sum of money to the Conservative government's re-election campaign of 1872; some sources quote a sum over $360,000. Allan had promised to keep American capital out of the railway deal but had lied to Macdonald over this vital point, as Macdonald later discovered. The Liberal Party, the opposition party in Parliament, accused the Conservatives of having made a tacit agreement to give the contract to Hugh Allan in exchange for money.\nIn making such allegations, the Liberals and their allies in the press (particularly George Brown's newspaper \"The Globe\") presumed that most of the money had been used to bribe voters in the 1872 election. The secret ballot, which was then considered a novelty, had not yet been introduced in Canada. Although it was illegal to offer, solicit, or accept bribes in exchange for votes, effective enforcement of the prohibition proved impossible.\nDespite Macdonald's claims of innocence, evidence came to light showing transfers of money from Allan to Macdonald and some of his political colleagues. Perhaps even more damaging to Macdonald was the Liberals' discovery of a telegram through a former employee of Allan, which was thought to have been stolen from the safe of Allan's lawyer, John Abbott.\nThe scandal proved fatal to Macdonald's government. Macdonald's control of Parliament had already been tenuous since the 1872 election. Since party discipline was not as strong as it is today, once Macdonald's culpability in the scandal became known, he could no longer expect to retain the confidence of the House of Commons.\nMacdonald resigned as prime minister on November 5, 1873. He also offered his resignation as the head of the Conservative Party, but it was not accepted, and he was convinced to stay. Perhaps as a direct result of this scandal, the Conservatives fell in the eyes of the public and were relegated to the Official Opposition status in the federal election of 1874, in which secret ballots were used for the first time. The election gave Alexander Mackenzie a firm mandate to succeed Macdonald as Canada's new prime minister.\nDespite the short-term defeat, the scandal was not a mortal wound to Macdonald, the Conservative Party, or the construction of the Canadian Pacific Railway. The Long Depression gripped Canada shortly after Macdonald left office, and although the causes of the depression were largely external to Canada, many Canadians blamed Mackenzie for the ensuing hard times. Macdonald returned as prime minister in the 1878 election thanks to his National Policy. He held the office until his death in 1891, and the Canadian Pacific was completed in 1885 while he was still in office, although by a completely different corporation.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23650", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=23650", "title": "Primer (molecular biology)", "text": "Short strand of RNA or DNA that serves as a starting point for DNA synthesis\nA primer is a short, single-stranded nucleic acid used by all living organisms in the initiation of DNA synthesis. A synthetic primer is a type of oligo, short for oligonucleotide. DNA polymerases (responsible for DNA replication) are only capable of adding nucleotides to the 3'-end of an existing nucleic acid, requiring a primer be bound to the template before DNA polymerase can begin a complementary strand.\nDNA polymerase adds nucleotides after binding to the RNA primer and synthesizes the whole strand. Later, the RNA strands must be removed accurately and replaced with DNA nucleotides. This forms a gap region known as a nick that is filled in using a ligase. The removal process of the RNA primer requires several enzymes, such as Fen1, Lig1, and others that work in coordination with DNA polymerase, to ensure the removal of the RNA nucleotides and the addition of DNA nucleotides.\nLiving organisms use solely RNA primers, while laboratory techniques in biochemistry and molecular biology that require in vitro DNA synthesis (such as DNA sequencing and polymerase chain reaction) usually use DNA primers, since they are more temperature stable. Primers can be designed in laboratory for specific reactions such as polymerase chain reaction (PCR). When designing PCR primers, there are specific measures that must be taken into consideration, like the melting temperature of the primers and the annealing temperature of the reaction itself.\nRNA primers \"in vivo\".\nRNA primers are used by living organisms in the initiation of synthesizing a strand of DNA. A class of enzymes called primases add a complementary RNA primer to the reading template \"de novo\" on both the leading and lagging strands. Starting from the free 3'-OH of the primer, known as the primer terminus, a DNA polymerase can extend a newly synthesized strand. The leading strand in DNA replication is synthesized in one continuous piece moving with the replication fork, requiring only an initial RNA primer to begin synthesis. In the lagging strand, the template DNA runs in the 5\u2032\u21923\u2032 direction. Since DNA polymerase cannot add bases in the 3\u2032\u21925\u2032 direction complementary to the template strand, DNA is synthesized 'backward' in short fragments moving away from the replication fork, known as Okazaki fragments. Unlike in the leading strand, this method results in the repeated starting and stopping of DNA synthesis, requiring multiple RNA primers. Along the DNA template, primase intersperses RNA primers that DNA polymerase uses to synthesize DNA from in the 5\u2032\u21923\u2032 direction.\nAnother example of primers being used to enable DNA synthesis is reverse transcription. Reverse transcriptase is an enzyme that uses a template strand of RNA to synthesize a complementary strand of DNA. The DNA polymerase component of reverse transcriptase requires an existing 3' end to begin synthesis.\nPrimer removal.\nAfter the insertion of Okazaki fragments, the RNA primers are removed (the mechanism of removal differs between prokaryotes and eukaryotes) and replaced with new deoxyribonucleotides that fill the gaps where the RNA primer was present. DNA ligase then joins the fragmented strands together, completing the synthesis of the lagging strand.\nIn prokaryotes, DNA polymerase I synthesizes the Okazaki fragment until it reaches the previous RNA primer. Then the enzyme simultaneously acts as a 5\u2032\u21923\u2032 exonuclease, removing primer ribonucleotides in front and adding deoxyribonucleotides behind. Both the activities of polymerization and excision of the RNA primer occur in the 5\u2032\u21923\u2032 direction, \u00a0and polymerase I can do these activities simultaneously; this is known as \"Nick Translation\". Nick translation refers to the synchronized activity of polymerase I in removing the RNA primer and adding deoxyribonucleotides. Later, a gap between the strands is formed called a nick, which is sealed using a DNA ligase.\nIn eukaryotes the removal of RNA primers in the lagging strand is essential for the completion of replication. Thus, as the lagging strand being synthesized by DNA polymerase \u03b4 in 5\u2032\u21923\u2032 direction, Okazaki fragments are formed, which are discontinuous strands of DNA. Then, when the DNA polymerase reaches to the 5' end of the RNA primer from the previous Okazaki fragment, it displaces the 5\u2032 end of the primer into a single-stranded RNA flap which is removed by nuclease cleavage. Cleavage of the RNA flaps involves three methods of primer removal. The first possibility of primer removal is by creating a short flap that is directly removed by flap structure-specific endonuclease 1 (FEN-1), which cleaves the 5' overhanging flap. This method is known as the short flap pathway of RNA primer removal. The second way to cleave a RNA primer is by degrading the RNA strand using a RNase, in eukaryotes it's known as the RNase H2. This enzyme degrades most of the annealed RNA primer, except the nucleotides close to the 5' end of the primer. Thus, the remaining nucleotides are displayed into a flap that is cleaved off using FEN-1. The last possible method of removing RNA primer is known as the long flap pathway. In this pathway several enzymes are recruited to elongate the RNA primer and then cleave it off. The flaps are elongated by a 5' to 3' helicase, known as Pif1. After the addition of nucleotides to the flap by Pif1, the long flap is stabilized by the replication protein A (RPA). The RPA-bound DNA inhibits the activity or recruitment of FEN1, as a result another nuclease must be recruited to cleave the flap. This second nuclease is DNA2 nuclease, which has a helicase-nuclease activity, that cleaves the long flap of RNA primer, which then leaves behind a couple of nucleotides that are cleaved by FEN1. At the end, when all the RNA primers have been removed, nicks form between the Okazaki fragments that are filled-in with deoxyribonucleotides using an enzyme known as ligase1, through a process called ligation.\nUses of synthetic primers.\nSynthetic primers are chemically synthesized oligonucleotides, usually of DNA, which can be customized to anneal to a specific site on the template DNA. In solution, the primer spontaneously hybridizes with the template through Watson-Crick base pairing before being extended by DNA polymerase. Both Sanger sequencing and next-generation sequencing require primers to initiate the reaction.\nPCR primer design.\nThe polymerase chain reaction (PCR) uses a pair of custom primers to direct DNA elongation toward each other at opposite ends of the sequence being amplified. These primers are typically between 18 and 24 bases in length and are complementary to the specific upstream and downstream sites of the sequence being amplified.\nPairs of primers are designed to have similar melting temperatures since annealing during PCR occurs for both strands simultaneously. The melting temperature is not be either too much higher or lower than the reaction's annealing temperature. If annealing temperatures are too low, non-specific structures can form, reducing the efficiency of the reaction.\nAdditionally, primer sequences need to be chosen to uniquely select for a region of DNA, avoiding the possibility of hybridization to a similar sequence nearby. A commonly used method for selecting a primer site is BLAST search, whereby all the possible regions to which a primer may bind can be seen. Both the nucleotide sequence as well as the primer itself can be BLAST searched. The free NCBI tool Primer-BLAST integrates primer design and BLAST search into one application, as do commercial software products such as ePrime and Beacon Designer. \"In silico\" PCR may be performed to evaluate the specificity of designed primers.\nSelecting a specific region of DNA for primer binding requires some additional considerations. Regions high in mononucleotide and dinucleotide repeats should be avoided, as loop formation can occur and contribute to mishybridization. Primers that are complementary to each other can lead to the formation of primer-dimers, lowering the efficiency of the desired reaction. Primers that are able to anneal to themselves can form internal hairpins and loops that hinder hybridization with the template DNA.\nWhen designing primers, additional nucleotide bases can be added to the back ends of each primer, resulting in a customized cap sequence on each end of the amplified region. One application for this practice is for use in TA cloning, a special subcloning technique similar to PCR, where efficiency can be increased by adding AG tails to the 5\u2032 and the 3\u2032 ends.\nDegenerate primers.\nSome situations may call for the use of \"degenerate primers.\" These are mixtures of primers that are similar, but not identical. These may be convenient when amplifying the same gene from different organisms, as the sequences are probably similar but not identical. This technique is useful because the genetic code itself is degenerate, meaning several different codons can code for the same amino acid. This allows different organisms to have a significantly different genetic sequence that code for a highly similar protein. For this reason, degenerate primers are also used when primer design is based on protein sequence, as the specific sequence of codons are not known. Therefore, primer sequence corresponding to the amino acid isoleucine might be \"ATH\", where A stands for adenine, T for thymine, and H for adenine, thymine, or cytosine, according to the genetic code for each codon, using the IUPAC symbols for degenerate bases. Degenerate primers may not perfectly hybridize with a target sequence, which can greatly reduce the specificity of the PCR amplification.\n\"Degenerate primers\" are widely used and extremely useful in the field of microbial ecology. They allow for the amplification of genes from thus far uncultivated microorganisms or allow the recovery of genes from organisms where genomic information is not available. Usually, degenerate primers are designed by aligning gene sequencing found in GenBank. Differences among sequences are accounted for by using IUPAC degeneracies for individual bases. PCR primers are then synthesized as a mixture of primers corresponding to all permutations of the codon sequence.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23652", "revid": "1292574707", "url": "https://en.wikipedia.org/wiki?curid=23652", "title": "Purine", "text": "Heterocyclic aromatic organic compound\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nPurine is a heterocyclic aromatic organic compound that consists of two rings (pyrimidine and imidazole) fused together. It is water-soluble. Purine also gives its name to the wider class of molecules, purines, which include substituted purines and their tautomers. They are the most widely occurring nitrogen-containing heterocycles in nature.\nDietary sources.\nPurines are found in high concentration in meat and meat products, especially internal organs, such as liver and kidney, and in various seafoods, high-fructose beverages, alcohol, and yeast products. Examples of high-purine food sources include anchovies, sardines, liver, beef, kidneys, brains, monkfish, dried mackerel, and shrimp. \nFoods particularly rich in hypoxanthine, adenine, and guanine lead to higher blood levels of uric acid. Foods having more than 200 mg of hypoxanthine per 100 g, particularly animal and fish meats containing hypoxanthine as more than 50% of total purines, are more likely to increase uric acid levels. Some vegetables, such as cauliflower, spinach, and peas, have considerable levels of purines, but do not contribute to elevated uric acid levels, possibily due to digestion and bioavailability factors. \nDairy products, soy foods, cereals, beans, mushrooms, and coffee are low-purine foods, characterized specifically by low levels of adenine and guanine comprising more than 60% of purines. A low-purine dietary plan that may reduce the risk of hyperuricemia and gout includes eggs, dairy products, fruits, vegetables, legumes, mushrooms, and soy products.\nBiochemistry.\nPurines and pyrimidines make up the two groups of nitrogenous bases, including the two groups of nucleotide bases. The purine bases are guanine (G) and adenine (A) which form corresponding nucleosides-deoxyribonucleosides (deoxyguanosine and deoxyadenosine) with deoxyribose moiety and ribonucleosides (guanosine, adenosine) with ribose moiety. These nucleosides with phosphoric acid form corresponding nucleotides (deoxyguanylate, deoxyadenylate and guanylate, adenylate) which are the building blocks of DNA and RNA, respectively. Purine bases also play an essential role in many metabolic and signalling processes within the compounds guanosine monophosphate (GMP) and adenosine monophosphate (AMP).\nIn order to perform these essential cellular processes, both purines and pyrimidines are needed by the cell, and in similar quantities. Both purine and pyrimidine are self-inhibiting and activating. When purines are formed, they inhibit the enzymes required for more purine formation. This self-inhibition occurs as they also activate the enzymes needed for pyrimidine formation. Pyrimidine simultaneously self-inhibits and activates purine in a similar manner. Because of this, there is nearly an equal amount of both substances in the cell at all times.\nProperties.\nPurine is both a very weak acid (pKa 8.93) and an even weaker base (pKa 2.39).\nPurine is aromatic, having four tautomers each with a hydrogen bonded to a different one of the four nitrogen atoms. These are identified as 1-H, 3-H, 7-H, and 9-H (see image of numbered ring). The common crystalline form favours the 7-H tautomer, while in polar solvents both the 9-H and 7-H tautomers predominate. Substituents to the rings and interactions with other molecules can shift the equilibrium of these tautomers.\nNotable purines.\nThere are many naturally occurring purines. They include the nucleotide bases adenine and guanine. In DNA, these bases form hydrogen bonds with their complementary pyrimidines, thymine and cytosine, respectively. This is called complementary base pairing. In RNA, the complement of adenine is uracil instead of thymine.\nOther notable purines are hypoxanthine, xanthine, theophylline, theobromine, caffeine, uric acid and isoguanine.\nFunctions.\nAside from the crucial roles of purines (adenine and guanine) in DNA and RNA, purines are also significant components in a number of other important biomolecules, such as ATP, GTP, cyclic AMP, NADH, and coenzyme A. Purine (1) itself, has not been found in nature, but it can be produced by organic synthesis.\nThey may also function directly as neurotransmitters, acting upon purinergic receptors. Adenosine activates adenosine receptors.\nHistory.\nThe word \"purine\" (\"pure urine\") was coined by the German chemist Emil Fischer in 1884. He synthesized it for the first time in 1898. The starting material for the reaction sequence was uric acid (8), which had been isolated from kidney stones by Carl Wilhelm Scheele in 1776. Uric acid was reacted with PCl5 to give 2,6,8-trichloropurine, which was converted with HI and PH4I to give 2,6-diiodopurine. The product was reduced to purine using zinc dust.\nMetabolism.\nMany organisms have metabolic pathways to synthesize and break down purines.\nPurines are biologically synthesized as nucleosides (bases attached to ribose).\nAccumulation of modified purine nucleotides is defective to various cellular processes, especially those involving DNA and RNA. To be viable, organisms possess a number of deoxypurine phosphohydrolases, which hydrolyze these purine derivatives removing them from the active NTP and dNTP pools. Deamination of purine bases can result in accumulation of such nucleotides as ITP, dITP, XTP and dXTP.\nDefects in enzymes that control purine production and breakdown can severely alter a cell's DNA sequences, which may explain why people who carry certain genetic variants of purine metabolic enzymes have a higher risk for some types of cancer.\nPurine biosynthesis in the three domains of life.\nOrganisms in all three domains of life, eukaryotes, bacteria and archaea, are able to carry out de novo biosynthesis of purines. This ability reflects the essentiality of purines for life. The biochemical pathway of synthesis is very similar in eukaryotes and bacterial species, but is more variable among archaeal species. A nearly complete, or complete, set of genes required for purine biosynthesis was determined to be present in 58 of the 65 archaeal species studied. However, also identified were seven archaeal species with entirely, or nearly entirely, absent purine encoding genes. Apparently the archaeal species unable to synthesize purines are able to acquire exogenous purines for growth., and are thus analogous to purine mutants of eukaryotes, e.g. purine mutants of the Ascomycete fungus \"Neurospora crassa\", that also require exogenous purines for growth.\nLaboratory synthesis.\nIn addition to \"in vivo\" synthesis of purines in purine metabolism, purine can also be synthesized artificially.\nPurine is obtained in good yield when formamide is heated in an open vessel at 170\u00a0\u00b0C for 28 hours.\nThis reaction and others like it have been discussed in the context of the origin of life.\nOro and Kamat (1961) and Orgel co-workers (1966, 1967) have shown that four molecules of HCN tetramerize to form diaminomaleodinitrile (12), which can be converted into almost all naturally occurring purines. For example, five molecules of HCN condense in an exothermic reaction to make adenine, especially in the presence of ammonia.\nThe Traube purine synthesis (1900) is a classic reaction (named after Wilhelm Traube) between an amine-substituted pyrimidine and formic acid.\nPrebiotic synthesis of purine ribonucleosides.\nIn order to understand how life arose, knowledge is required of the chemical pathways that permit formation of the key building blocks of life under plausible prebiotic conditions. Nam et al. (2018) demonstrated the direct condensation of purine and pyrimidine nucleobases with ribose to give ribonucleosides in aqueous microdroplets, a key step leading to RNA formation. Also, a plausible prebiotic process for synthesizing purine ribonucleosides was presented by Becker \"et al\". in 2016.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23653", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=23653", "title": "Pyrimidine", "text": "Aromatic compound (C4H4N2)\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nPyrimidine (; ) is an aromatic, heterocyclic, organic compound similar to pyridine (). One of the three diazines (six-membered heterocyclics with two nitrogen atoms in the ring), it has nitrogen atoms at positions 1 and 3 in the ring. The other diazines are pyrazine (nitrogen atoms at the 1 and 4 positions) and pyridazine (nitrogen atoms at the 1 and 2 positions). \nIn nucleic acids, three types of nucleobases are pyrimidine derivatives: cytosine (C), thymine (T), and uracil (U).\nOccurrence and history.\nThe pyrimidine ring system has wide occurrence in nature\nas substituted and ring fused compounds and derivatives, including the nucleotides cytosine, thymine and uracil, thiamine (vitamin B1) and alloxan. It is also found in many synthetic compounds such as barbiturates and the HIV drug zidovudine. Although pyrimidine derivatives such as alloxan were known in the early 19th century, a laboratory synthesis of a pyrimidine was not carried out until 1879, when Grimaux reported the preparation of barbituric acid from urea and malonic acid in the presence of phosphorus oxychloride.\nThe systematic study of pyrimidines began in 1884 with Pinner,\nwho synthesized derivatives by condensing ethyl acetoacetate with amidines. Pinner first proposed the name \u201cpyrimidin\u201d in 1885. The parent compound was first prepared by Gabriel and Colman in 1900,\nby conversion of barbituric acid to 2,4,6-trichloropyrimidine followed by reduction using zinc dust in hot water.\nNomenclature.\nThe nomenclature of pyrimidines is straightforward. However, like other heterocyclics, tautomeric hydroxyl groups yield complications since they exist primarily in the cyclic amide form. For example, 2-hydroxypyrimidine is more properly named 2-pyrimidone. A partial list of trivial names of various pyrimidines exists.\nPhysical properties.\nPhysical properties are shown in the data box. A more extensive discussion, including spectra, can be found in Brown \"et al.\"\nChemical properties.\nPer the classification by Albert, six-membered heterocycles can be described as \u03c0-deficient. Substitution by electronegative groups or additional nitrogen atoms in the ring significantly increase the \u03c0-deficiency. These effects also decrease the basicity.\nLike pyridines, in pyrimidines the \u03c0-electron density is decreased to an even greater extent. Therefore, electrophilic aromatic substitution is more difficult while nucleophilic aromatic substitution is facilitated. An example of the last reaction type is the displacement of the amino group in 2-aminopyrimidine by chlorine and its reverse.\nElectron lone pair availability (basicity) is decreased compared to pyridine. Compared to pyridine, \"N\"-alkylation and \"N\"-oxidation are more difficult. The p\"K\"a value for protonated pyrimidine is 1.23 compared to 5.30 for pyridine. Protonation and other electrophilic additions will occur at only one nitrogen due to further deactivation by the second nitrogen. The 2-, 4-, and 6- positions on the pyrimidine ring are electron deficient analogous to those in pyridine and nitro- and dinitrobenzene. The 5-position is less electron deficient and substituents there are quite stable. However, electrophilic substitution is relatively facile at the 5-position, including nitration and halogenation.\nReduction in resonance stabilization of pyrimidines may lead to addition and ring cleavage reactions rather than substitutions. One such manifestation is observed in the Dimroth rearrangement.\nPyrimidine is also found in meteorites, but scientists still do not know its origin. Pyrimidine also photolytically decomposes into uracil under ultraviolet light.\nSynthesis.\nPyrimidine biosynthesis creates derivatives \u2014like orotate, thymine, cytosine, and uracil\u2014 \"de novo\" from carbamoyl phosphate and aspartate.\nAs is often the case with parent heterocyclic ring systems, the synthesis of pyrimidine is not that common and is usually performed by removing functional groups from derivatives. Primary syntheses in quantity involving formamide have been reported.\nAs a class, pyrimidines are typically synthesized by the principal synthesis involving cyclization of \u03b2-dicarbonyl compounds with N\u2013C\u2013N compounds. Reaction of the former with amidines to give 2-substituted pyrimidines, with urea to give 2-pyrimidinones, and guanidines to give 2-aminopyrimidines are typical.\nPyrimidines can be prepared via the Biginelli reaction and other multicomponent reactions. Many other methods rely on condensation of carbonyls with diamines for instance the synthesis of 2-thio-6-methyluracil from thiourea and ethyl acetoacetate or the synthesis of 4-methylpyrimidine with 4,4-dimethoxy-2-butanone and formamide.\nA novel method is by reaction of \"N\"-vinyl and \"N\"-aryl amides with carbonitriles under electrophilic activation of the amide with 2-chloro-pyridine and trifluoromethanesulfonic anhydride:\nReactions.\nBecause of the decreased basicity compared to pyridine, electrophilic substitution of pyrimidine is less facile. Protonation or alkylation typically takes place at only one of the ring nitrogen atoms. Mono-\"N\"-oxidation occurs by reaction with peracids.\nElectrophilic \"C\"-substitution of pyrimidine occurs at the 5-position, the least electron-deficient. Nitration, nitrosation, azo coupling, halogenation, sulfonation, formylation, hydroxymethylation, and aminomethylation have been observed with substituted pyrimidines.\nNucleophilic \"C\"-substitution should be facilitated at the 2-, 4-, and 6-positions but there are only a few examples. Amination and hydroxylation have been observed for substituted pyrimidines. Reactions with Grignard or alkyllithium reagents yield 4-alkyl- or 4-aryl pyrimidine after aromatization.\nFree radical attack has been observed for pyrimidine and photochemical reactions have been observed for substituted pyrimidines. Pyrimidine can be hydrogenated to give tetrahydropyrimidine.\nDerivatives.\nNucleotides.\nThree nucleobases found in nucleic acids, cytosine (C), thymine (T), and uracil (U), are pyrimidine derivatives:\nIn DNA and RNA, these bases form hydrogen bonds with their complementary purines. Thus, in DNA, the purines adenine (A) and guanine (G) pair up with the pyrimidines thymine (T) and cytosine (C), respectively.\nIn RNA, the complement of adenine (A) is uracil (U) instead of thymine (T), so the pairs that form are adenine:uracil and guanine:cytosine.\nVery rarely, thymine can appear in RNA, or uracil in DNA, but when the other three major pyrimidine bases are represented, some minor pyrimidine bases can also occur in nucleic acids. These minor pyrimidines are usually methylated versions of major ones and are postulated to have regulatory functions.\nThese hydrogen bonding modes are for classical Watson\u2013Crick base pairing. Other hydrogen bonding modes (\"wobble pairings\") are available in both DNA and RNA, although the additional 2\u2032-hydroxyl group of RNA expands the configurations, through which RNA can form hydrogen bonds.\nTheoretical aspects.\nIn March 2015, NASA Ames scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds.\nPrebiotic synthesis of pyrimidine nucleotides.\nIn order to understand how life arose, knowledge is required of the chemical pathways that permit formation of the key building blocks of life under plausible prebiotic conditions. The RNA world hypothesis holds that in the primordial soup there existed free-floating ribonucleotides, the fundamental molecules that combine in series to form RNA. Complex molecules such as RNA must have emerged from relatively small molecules whose reactivity was governed by physico-chemical processes. RNA is composed of pyrimidine and purine nucleotides, both of which are necessary for reliable information transfer, and thus natural selection and Darwinian evolution. Becker et al. showed how pyrimidine nucleosides can be synthesized from small molecules and ribose, driven solely by wet-dry cycles. Purine nucleosides can be synthesized by a similar pathway. 5\u2019-mono-and diphosphates also form selectively from phosphate-containing minerals, allowing concurrent formation of polyribonucleotides with both the pyrimidine and purine bases. Thus a reaction network towards the pyrimidine and purine RNA building blocks can be established starting from simple atmospheric or volcanic molecules.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23654", "revid": "6289403", "url": "https://en.wikipedia.org/wiki?curid=23654", "title": "Play-by-mail game", "text": "Games played through postal mail, email or other digital media\nA play-by-mail game (also known as a PBM game, PBEM game, turn-based game, turn based distance game, or an interactive strategy game.) is a game played through postal mail, email, or other digital media. Correspondence chess and Go were among the first PBM games. \"Diplomacy\" has been played by mail since 1963, introducing a multi-player aspect to PBM games. Flying Buffalo Inc. pioneered the first commercially available PBM game in 1970. A small number of PBM companies followed in the 1970s, with an explosion of hundreds of startup PBM companies in the 1980s at the peak of PBM gaming popularity, many of them small hobby companies\u2014more than 90 percent of which eventually folded. A number of independent PBM magazines also started in the 1980s, including \"The Nuts &amp; Bolts of PBM\", \"Gaming Universal\", \"Paper Mayhem\" and \"Flagship\". These magazines eventually went out of print, replaced in the 21st century by the online PBM journal \"Suspense and Decision\".\nPlay-by-mail games (which became known as \"turn-based games\" in the digital age) have a number of advantages and disadvantages compared to other kinds of gaming. PBM games have wide ranges for turn lengths. Some games allow turnaround times of a day or less\u2014even hourly. Other games structure multiple days or weeks for players to consider moves or turns and players never run out of opponents to face. If desired, some PBM games can be played for years. Additionally, the complexity of PBM games can be far beyond that allowed by a board game in an afternoon, and pit players against live opponents in these conditions\u2014a challenge some players enjoy. PBM games allow the number of opponents or teams in the dozens\u2014with some previous examples over a thousand players. PBM games also allow gamers to interact with others globally. Games with low turn costs compare well with expensive board or video games. Drawbacks include the price for some PBM games with high setup and/or turn costs, and the lack of the ability for face-to-face roleplaying. Additionally, for some players, certain games can be overly complex, and delays in turn processing can be a negative.\nPlay-by-mail games are multifaceted. In their earliest form they involved two players mailing each other directly by postal mail, such as in correspondence chess. Multi-player games, such as \"Diplomacy\" or more complex games available today, involve a game master who receives and processes orders and adjudicates turn results for players. These games also introduced the element of diplomacy in which participants can discuss gameplay with each other, strategize, and form alliances. In the 1970s and 1980s, some games involved turn results adjudicated completely by humans. Over time, partial or complete turn adjudication by computer became the norm. Games also involve open- and closed-end variants. Open-ended games do not normally end and players can develop their positions to the fullest extent possible; in closed-end games, players pursue victory conditions until a game conclusion. PBM games enable players to explore a diverse array of roles, such as characters in fantasy or medieval settings, space opera, inner city gangs, or more unusual ones such as assuming the role of a microorganism or a monster.\nHistory.\nThe earliest play-by-mail games developed as a way for geographically separated gamers to compete with each other using postal mail. Chess and Go are among the oldest examples of this. In these two-player games, players sent moves directly to each other. Multi-player games emerged later: \"Diplomacy\" is an early example of this type, emerging in 1963, in which a central game master manages the game, receiving moves and publishing adjudications. According to Shannon Appelcline, there was some PBM play in the 1960s, but not much. For example, some wargamers began playing \"Stalingrad\" by mail in this period.\nIn the early 1970s, in the United States, Rick Loomis, of Flying Buffalo Inc., began a number of multi-player play-by-mail games; these included games such as \"Nuclear Destruction\", which launched in 1970. This began the professional PBM industry in the United States. Professional game moderation started in 1971 at Flying Buffalo which added games such as \"Battleplan\", \"Heroic Fantasy\", \"Starweb\", and others, which by the late 1980s were all computer moderated.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n \"Rick Loomis is generally recognized as the founder of the PBM industry.\"\n \u2014 \"The Editors of Space Gamer Magazine, 1985\".\nFor approximately five years, Flying Buffalo was the single dominant company in the US PBM industry until Schubel &amp; Son entered the field in roughly 1976 with the human-moderated \"Tribes of Crane\". Schubel &amp; Son introduced fee structure innovations which allowed players to pay for additional options or special actions outside of the rules. For players with larger bankrolls, this provided advantages and the ability to game the system. The next big entrant was Superior Simulations with its game \"Empyrean Challenge\" in 1978. Reviewer Jim Townsend asserted that it was \"the most complex game system on Earth\" with some large position turn results 1,000 pages in length.\nChris Harvey started the commercial PBM industry in the United Kingdom with a company called ICBM. After Harvey played Flying Buffalo's \"Nuclear Destruction\" game in the United States in approximately 1971, Rick Loomis suggested that he run the game in the UK with Flying Buffalo providing the computer moderation. ICBM Games led the industry in the UK as a result of this proxy method of publishing Flying Buffalo's PBM games, along with KJC games and Mitregames.\nIn the early 1980s, the field of PBM players was growing. Individual PBM game moderators were plentiful in 1980. However, the PBM industry in 1980 was still nascent: there were still only two sizable commercial PBM companies, and only a few small ones. The most popular PBM games of 1980 were \"Starweb\" and \"Tribes of Crane\".\nSome players, unhappy with their experiences with Schubel &amp; Son and Superior Simulations, launched their own company\u2014Adventures by Mail\u2014with the game, 'Beyond the Stellar Empire', which became \"immensely popular\". In the same way, many people launched PBM companies, trying their hand at finding the right mix of action and strategy for the gaming audience of the period. According to Jim Townsend: In the late 70's and all of the 80's, many small PBM firms have opened their doors and better than 90% of them have failed. Although PBM is an easy industry to get into, staying in business is another thing entirely. Literally hundreds of PBM companies have come and gone, most of them taking the money of would-be-customers with them.\nTownsend emphasized the risks for the PBM industry in that \"The new PBM company has such a small chance of surviving that no insurance company would write a policy to cover them. Skydivers are a better risk.\" W.G. Armintrout wrote a 1982 article in \"The Space Gamer\" magazine warning those thinking of entering the professional PBM field of the importance of playtesting games to mitigate the risk of failure. By the late 1980s, of the more than one hundred play-by-mail companies operating, the majority were hobbies, not run as businesses to make money. Townsend estimated that, in 1988, there were about a dozen profitable PBM companies in the United States\u2014with an additional few in the United Kingdom and the same in Australia. Sam Roads of Harlequin Games similarly assessed the state of the PBM industry in its early days while also noting the existence of few non-English companies.\nBy the 1980s, interest in PBM gaming in Europe increased. The first UK PBM convention was in 1986. In 1993, the founder of \"Flagship\" magazine, Nick Palmer, stated that \"recently there has been a rapid diffusion throughout continental Europe where now there are now thousands of players\". In 1992, Jon Tindall stated that the number of Australian players was growing, but limited by a relatively small market base. In a 2002 listing of 182 primarily European PBM game publishers and Zines, \"Flagship\" listed ten non-UK entries, to include one each from Austria and France, six from Germany, one from Greece, and one from the Netherlands.\nPBM games up to the 1980s came from multiple sources: some were adapted from existing games and others were designed solely for postal play. In 1985, Pete Tamlyn stated that most popular games had already been attempted in postal play, noting that none had succeeded as well as \"Diplomacy\". Tamlyn added that there was significant experimentation in adapting games to postal play at the time and that most games could be played by mail. These adapted games were typically run by a gamemaster using a fanzine to publish turn results. The 1980s were also noteworthy in that PBM games designed and published in this decade were written specifically for the genre versus adapted from other existing games. Thus they tended to be more complicated and gravitated toward requiring computer assistance.\nThe proliferation of PBM companies in the 1980s supported the publication of a number of newsletters from individual play-by-mail companies as well as independent publications which focused solely on the play-by-mail gaming industry. As of 1983, \"The Nuts &amp; Bolts of PBM\" was the primary magazine in this market. In July 1983, the first issue of \"Paper Mayhem\" was published. The first issue was a newsletter with a print run of 100. \"Flagship\" began publication in the United Kingdom in October 1983, the month before \"Gaming Universal's\" first issue was published in the United States. In the mid-1980s, general gaming magazines also began carrying articles on PBM and ran PBM advertisements. PBM games were featured in magazines like \"Games\" and \"Analog\" in 1984. In the early 1990s, Martin Popp also began publishing a quarterly PBM magazine in Sulzberg, Germany called \"Postspielbote\". The PBM genre's two preeminent magazines of the period were \"Flagship\" and \"Paper Mayhem\".\nIn 1984, the PBM industry created a Play-by-Mail Association. This organization had multiple charter members by early 1985 and was holding elections for key positions. One of its proposed functions was to reimburse players who lost money after a PBM business failed.\nPaul Brown, the president of Reality Simulations, Inc., estimated in 1988 that there were about 20,000 steady play-by-mail gamers, with potentially another 10\u201320,000 who tried PBM gaming but did not stay. Flying Buffalo Inc. conducted a survey of 167 of its players in 1984. It indicated that 96% of its players were male with most in their 20s and 30s. Nearly half were white collar workers, 28% were students, and the remainder engineers and military.\nThe 1990s brought changes to the PBM world. In the early 1990s, trending PBM games increased in complexity. In this period, email also became an option to transmit turn orders and results. These are called play-by-email (PBEM) games. \"Flagship\" reported in 1992 that they knew of 40 PBM gamemasters on Compuserve. One publisher in 2002 called PBM games \"Interactive Strategy Games\". Turn around time ranges for modern PBM games are wide enough that PBM magazine editors now use the term \"turn-based games\". \"Flagship\" stated in 2005 that \"play-by-mail games are often called turn-based games now that most of them are played via the internet\". In the 2023 issues of Suspense &amp; Decision, the publisher used the term \"Turn Based Distance Gaming\".\nIn the early 1990s, the PBM industry still maintained some of the player momentum from the 1980s. For example, in 1993, \"Flagship\" listed 185 active play-by-mail games. Patrick M. Rodgers also stated in \"Shadis\" magazine that the United States had over 300 PBM games. And in 1993, the \"Journal of the PBM Gamer\" stated that \"For the past several years, PBM gaming has increased in popularity.\" That year, there were a few hundred PBM games available for play globally. However, in 1994, David Webber, \"Paper Mayhem's\" editor in chief expressed concern about disappointing growth in the PBM community and a reduction in play by established gamers. At the same time, he noted that his analysis indicated that more PBM gamers were playing less, giving the example of an average drop from 5\u20136 games per player to 2\u20133 games, suggesting it could be due to financial reasons. In early 1997, David Webber stated that multiple PBM game moderators had noted a drop in players over the previous year.\nBy the end of the 1990s, the number of PBM publications had also declined. \"Gaming Universal's\" final publication run ended in 1988. \"Paper Mayhem\" ceased publication unexpectedly in 1998 after Webber's death. \"Flagship\" also later ceased publication.\nThe Internet affected the PBM world in various ways. Rick Loomis stated in 1999 that, \"With the growth of the Internet, [PBM] seems to have shrunk and a lot of companies dropped out of the business in the last 4 or 5 years.\" Shannon Appelcline agreed, noting in 2014 that, \"The advent of the Internet knocked most PBM publishers out of business.\" The Internet also enabled PBM to globalize between the 1990s and 2000s. Early PBM professional gaming typically occurred within single countries. In the 1990s, the largest PBM games were licensed globally, with \"each country having its own licensee\". By the 2000s, a few major PBM firms began operating globally, bringing about \"The Globalisation of PBM\" according to Sam Roads of Harlequin Games.\nBy 2014 the PBM community had shrunk compared to previous decades. A single PBM magazine exists\u2014\"Suspense and Decision\"\u2014which began publication in November 2013. The PBM genre has also morphed from its original postal mail format with the onset of the digital age. In 2010, Carol Mulholland\u2014the editor of \"Flagship\"\u2014stated that \"most turn-based games are now available by email and online\". The online Suspense &amp; Decision Games Index, as of June 2021, listed 72 active PBM, PBEM, and turn-based games. In a multiple-article examination of various online turn-based games in 2004 titled \"Turning Digital\", Colin Forbes concluded that \"the number and diversity of these games has been enough to convince me that turn-based gaming is far from dead\".\nAdvantages and disadvantages of PBM gaming.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"PBM games blow the doors off of anything in the face-to-face or computer game market.\"\n Jim Townsend, \"White Wolf\" No. 9. 1988.\nJudith Proctor noted that play-by-mail games have a number of advantages. These include (1) plenty of time\u2014potentially days\u2014to plan a move, (2) never lacking players to face who have \"new tactics and ideas\", (3) the ability to play an \"incredibly complex\" game against live opponents, (4) meeting diverse gamers from far-away locations, and (5) relatively low costs. In 2019, Rick McDowell, designer of \"Alamaze\", compared PBM costs favorably with the high cost of board games at Barnes &amp; Noble, with many of the latter going for about $70, and a top-rated game, \"Nemesis\", costing $189. Andrew Greenberg pointed to the high number of players possible in a PBM game, comparing it to his past failure at attempting once to host a live eleven-player \"Dungeons &amp; Dragons\" Game. \"Flagship\" noted in 2005 that \"It's normal to play these ... games with international firms and a global player base. Games have been designed that can involve large numbers of players \u2013 much larger than can gather for face-to-face gaming.\" Finally, some PBM games can be played for years, if desired.\nGreenberg identified a number of drawbacks for play-by-mail games. He stated that the clearest was the cost, because most games require a setup cost and a fee per turn, and some games can become expensive. Another drawback is the lack of face-to-face interaction inherent in play-by-mail games. Finally, game complexity in some cases and occasional turn processing delays can be negatives in the genre.\nDescription.\nPBM games can include combat, diplomacy, politics, exploration, economics, and role-playing, with combat a usual feature and open-ended games typically the most comprehensive. Jim Townsend identifies the two key figures in PBM games as the players and the moderators, the latter of which are companies that charge \"turn fees\" to players\u2014the cost for each game turn. In 1993, \"Paper Mayhem\"\u2014a magazine for play-by-mail gamers\u2014described play-by-mail games thusly: PBM Games vary in the size of the games, turn around time, length of time a game lasts, and prices. An average PBM game has 10\u201320 players in it, but there are also games that have hundreds of players. Turn around time is the length of time it takes to get your turn back from a company. ... Some games never end. They can go on virtually forever or until you decide to drop. Many games have victory conditions that can be achieved within a year or two. Prices vary for the different PBM games, but the average price per turn is about $5.00.\nThe earliest PBM games were played using the postal services of the respective countries. In 1990, the average turn-around time for a turn was 2\u20133 weeks. However, in the 1990s, email was introduced to PBM games. This was known as play-by-email (PBEM). Some games used email solely, while others, such as \"Hyborian War\", used email as options for a portion of turn transmittal, with postal service for the remainder. Other games use digital media or web applications to allow players to make turns at speeds faster than postal mail. Given these changes, the term \"turn-based games\" is now being used by some commentators.\nMechanics.\nAfter the initial setup of a PBM game, players begin submitting turn orders. In general, players fill out an order sheet for a game and return it to the gaming company. The company processes the orders and sends back turn results to the players so they can make subsequent moves.\nR. Danard further separates a typical PBM turn into four parts. First, the company informs players on the results of the last turn. Next players conduct diplomatic activities, if desired. Then, they send their next turns to the gamemaster (GM). Finally, the turns are processed and the cycle is repeated. This continues until the game or a player is done.\nComplexity.\nJim Townsend stated in a 1990 issue of \"White Wolf Magazine\" that the complexity of PBM games is much higher than other types on the average. He noted that PBM games at the extreme high end can have a thousand or more players as well as thousands of units to manage, while turn printouts can range from a simple one-page result to hundreds of pages (with three to seven as the average). According to John Kevin Loth, \"Novices should appreciate that some games are best played by veterans.\" In 1986, he highlighted the complexity of \"Midgard\" with its 100-page instruction manual and 255 possible orders. A.D. Young stated in 1982 that computers could assist PBM gamers in various ways including accounting for records, player interactions, and movements, as well as computation or analysis specific to individual games.\nReviewer Jim Townsend asserted that \"Empyrean Challenge\" was \"the most complex game system on Earth\". Other games, like \"Galactic Prisoners\" began simply and gradually increased in complexity. As of August 2021, Rick Loomis PBM Games' had four difficulty levels: easy, moderate, hard, and difficult, with games such as \"Nuclear Destruction\" and \"Heroic Fantasy\" on the easy end and \"Battleplan\"\u2014a military strategy game\u2014rated as difficult.\nDiplomacy.\nAccording to \"Paper Mayhem\" assistant editor Jim Townsend, \"The most important aspect of PBM games is the diplomacy. If you don't communicate with the other players you will be labeled a 'loner', 'mute', or just plain 'dead meat'. You must talk with the others to survive\". The editors of \"Paper Mayhem\" add that \"The interaction with other players is what makes PBM enjoyable.\"\nCommentator Rob Chapman in a 1983 \"Flagship\" article echoed this advice, recommending that players get to know their opponents. He also recommended asking direct questions of opponents on their future intentions, as their responses, true or false, provide useful information. However, he advises players to be truthful in PBM diplomacy, as a reputation for honesty is useful in the long-term. Chapman notes that \"everything is negotiable\" and advises players to \"Keep your plans flexible, your options open \u2013 don't commit yourself, or your forces, to any long term strategy\".\nEric Stehle, owner and operator of Empire Games in 1997, stated that some games cannot be won alone and require diplomacy. He suggested considering the following diplomatic points during gameplay:\nGame types and player roles.\nJim Townsend noted in 1990 that hundreds of PBM games were available, ranging from \"all science fiction and fantasy themes to such exotics as war simulations (generally more complex world war games than those which wargamers play), duelling games, humorous games, sports simulations, etc\". In 1993, Steve Pritchard described PBM game types as ancient wargames, diplomacy games, fantasy wargames, power games, roleplaying games, and sports games. Some PBM games defy easy categorization, such as \"Firebreather\", which Joey Browning, the editor of the U.S. \"Flagship\" described as a \"Fantasy Exploration\" game.\nPlay-by-mail games also provide a wide array of possible roles to play. These include \"trader, fighter, explorer, [and] diplomat\". Roles range from pirates to space characters to \"previously unknown creatures\". In the game \"Monster Island\", players assume the role of a monster which explores a massive island (see image). And the title of the PBM game \"You're An Amoeba, GO!\" indicates an unusual role as players struggle \"in a 3D pool of primordial ooze [directing] the evolution of a legion of micro-organisms\". Loth advises that closer identification with a role increases enjoyment, but prioritizing this aspect requires more time searching for the right PBM game.\nClosed versus open ended.\nAccording to John Kevin Loth III, open-ended games do not end and there is no final objective or way to win the game. Jim Townsend adds that, \"players come and go, powers grow and diminish, alliances form and dissolve and so forth\". Since surviving, rather than winning, is primary, this type of game tends to attract players more interested in role-playing, and Townsend echoes that open-ended games are similar to long-term RPG campaigns. A drawback of this type is that mature games have powerful groups that can pose an unmanageable problem for the beginner\u00a0\u2013 although some may see this situation as a challenge of sorts. Examples of open ended games are \"Heroic Fantasy\", \"Monster Island\", and \"SuperNova: Rise of the Empire\". Townsend noted in 1990 that some open-ended games had been in play for up to a decade.\nTownsend states that \"closed-ended games are like Risk or Monopoly\u00a0\u2013 once they're over, they're over\". Loth notes that most players in closed end games start equally and the games are \"faster paced, usually more intense... presenting frequent player confrontation; [and] the game terminates when a player or alliance of players has achieved specific conditions or eliminated all opposition\". Townsend stated in 1990 that closed-end games can have as few as ten and as many as eighty turns. Examples of closed-end games are \"Hyborian War\", \"It's a Crime\", and \"Starweb\".\nCompanies in the early 1990s also offered games with both open- and closed-ended versions. Additionally, games could have elements of both versions; for example, in \"Kingdom\", an open-ended PBM game published by Graaf Simulations, a player could win by accumulating 50,000 points.\nComputer versus human moderated.\nIn the 1980s, PBM companies began using computers to moderate games. This was in part for economic reasons, as computers allowed the processing of more turns than humans, but with less of a human touch in the prose of a turn result. According to John Kevin Loth III, one hundred percent computer-moderated games would also kill a player's character or empire emotionlessly, regardless of the effort invested. Alternatively, Loth noted that those preferring exquisite pages of prose would gravitate toward one hundred percent human moderation. Loth provided \"Beyond the Quadra Zone\" and \"Earthwood\" as popular computer-moderated examples in 1986 and \"Silverdawn\" and \"Sword Lords\" as one hundred percent human-moderated examples of the period. \"Borderlands of Khataj\" is an example of a game where the company transitioned from human- to computer-moderated to mitigate issues related to a growing player base.\nIn 1984, there was a shift toward mixed moderation\u2014human moderated games with computer-moderated aspects such as combat. Examples included \"Delenda est Carthago\", \"Star Empires\", and \"Starglobe\". In 1990, the editors of \"Paper Mayhem\" noted that there were games with a mix of computer and hand moderation, where games \"would have the numbers run by the computer and special actions in the game would receive attention from the game master\".\nCost and turn processing time.\nLoth noted that, in 1986, $3\u20135 per turn was the most prevalent cost. At the time, some games were free, while others cost as much as $100 per turn.\nPBM magazine \"Paper Mayhem\" stated that the average turn processing time in 1987 was two weeks, and Loth noted that this was also the most common. Some companies offered longer turnaround times for overseas players or other reasons. In 1985, the publisher for \"Angrelmar: The Court of Kings\" scheduled three month turn processing times after a break in operations.\nIn 1986, play-by-email was a nascent service only being offered by the largest PBM companies. By the 1990s, players had more options for online play-by-mail games. For example, in 1995, \"World Conquest\" was available to play with hourly turns. In the 21st century, many games of this genre are called \"turn-based games\" and are played via the Internet.\nGame turns can be processed simultaneously or serially. In simultaneously processed games, the publisher processes turns from all players together according to an established sequence. In serial-processed games, turns are processed when received within the determined turn processing window.\nInformation sources.\nRick Loomis of Flying Buffalo Games stated in 1985 that the \"Nuts &amp; Bolts of PBM\" (first called \"Nuts &amp; Bolts of Starweb\") was the first PBM magazine not published by a PBM company. The name changed to \"Nuts &amp; Bolts of Gaming\" and it eventually went out of print. In 1983, the U.S. PBM magazines \"Paper Mayhem\" and \"Gaming Universal\" began publication as well as \"Flagship\" in the UK. Also in 1983, PBM games were featured in magazines like \"Games\" and \"Analog\" in 1984 as well as Australia's gaming magazine \"Breakout\" in 1992.\nBy 1985, \"Nuts &amp; Bolts of Gaming\" and \"Gaming Universal\" in the U.S. were out of print. John Kevin Loth identified that, in 1986, the \"three major information sources in PBM\" were \"Paper Mayhem\", \"Flagship\", and the Play By Mail Association. These sources were solely focused on play-by-mail gaming. Additional PBM information sources included company-specific publications, although Rick Loomis stated that interest was limited to individual companies\". Finally, play-by-mail gamers could also draw from \"alliances, associations, and senior players\" for information.\nIn the mid-1980s, other gaming magazines also began venturing into PBM. For example, \"White Wolf Magazine\" began a regular PBM column beginning in issue #11 as well as publishing an annual PBM issue beginning with issue #16. \"The Space Gamer\" also carried PBM articles and reviews. Additional minor information sources included gaming magazines such as \"\"Different Worlds\" ... \"Game New\", \"Imagine\", and \"White Dwarf\"\". Dragon Publishing's \"Ares\", \"Dragon\", and \"Strategy and Tactics\" magazines provided PBM coverage along with Flying Buffalo's \"Sorcerer's Apprentice\". Gaming magazine \"Micro Adventurer\", which closed in 1985, also featured PBM games. Other PBM magazines in the late 1980s in the UK included \"Thrust\", and \"Warped Sense of Humour\".\nIn the early 1990s, Martin Popp also began publishing a quarterly PBM magazine in Sulzberg, Germany called \"Postspielbote\". In 1995, \"Post &amp; Play Unlimited\" stated that it was the only German-language PBM magazine. In its March 1992 issue, \"Flagship\" stated that it checked \"Simcoarum Bimonthly\" for PBM news. \"Shadis\" magazine stated in 1994 that it had begun carrying a 16-page PBM section. This section, called \"Post Marque\", was discontinued after the March/April 1995 issue (#18), after which PBM coverage was integrated into other magazine sections. In its January\u2013February 1995 issue, \"Flagship's\" editor noted that their \"main European competitor\" \"PBM Scroll\" had gone out of print.\n\"Flagship\" ran into the 21st century, but ceased publication in 2010. In November 2013, online PBM journal \"Suspense &amp; Decision\", began publication.\nFiction.\nBesides articles and reviews on PBM games, authors have also published PBM fiction articles according to Shannon Muir. An early example is \"Scapegoat\" by Mike Horn, which appeared in the May\u2013June 1984 issue of \"Paper Mayhem\" magazine. Other examples include \"A Loaf of Bread\" by Suzanna Y. Snow about the game \"A Duel of a Different Color\", \"Dark Beginnings\" by Dave Bennett about \"Darkness of Silverfall\", and Chris Harvey's \"It Was the Only Thing He Could Do...\" about a conglomeration of PBM games. Simon Williams, the gamemaster of the PBM game \"Chaos Trail\" in 2004, also wrote an article in \"Flagship\" about the possibility of writing a PBM fiction novel.\nThe main character of John Darnielle's 2014 novel \"Wolf in White Van\" runs a play-by-mail role-playing game.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23658", "revid": "313186", "url": "https://en.wikipedia.org/wiki?curid=23658", "title": "Philip K. Dick Award", "text": "American annual award for science fiction novels\nThe Philip K. Dick Award is an American science fiction award given annually at Norwescon and sponsored by the Philadelphia Science Fiction Society and (since 2005) the Philip K. Dick Trust. Named after science fiction writer Philip K. Dick, it has been awarded since 1983, the year after his death. It is awarded to the best original paperback published each year in the US.\nThe award was founded by Thomas Disch with assistance from David G. Hartwell, Paul S. Williams, and Charles N. Brown. As of 2025, it is administered by Pat LoBrutto, John Silbersack, and Gordon Van Gelder. Past administrators include Algis Budrys and David G. Hartwell.\nWinners and nominees.\nWinners are listed in bold.\nThe year in the table below indicates the year the winners are announced; this is the year after the novels were published. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23659", "revid": "39120993", "url": "https://en.wikipedia.org/wiki?curid=23659", "title": "Plug-in (computing)", "text": "Software component that extends the functionality of existing software\nIn computing, a plug-in (also spelled plugin), add-in (also addin, add-on, or addon) or extension is a software component that extends the functionality of an existing software system without requiring the system to be re-built. A plug-in feature is one way that a system can be customizable.\nApplications support plug-ins for a variety of reasons including:\nExamples.\nExamples of plug-in use for various categories of applications:\nMechanism.\nThe host application provides services which the plug-in can use, including a way for plug-ins to register themselves with the host application and a protocol for the exchange of data with plug-ins. Plug-ins depend on the services provided by the host application and do not usually work by themselves. Conversely, the host application operates independently of the plug-ins, making it possible for end-users to add and update plug-ins dynamically without needing to make changes to the host application.\nProgrammers typically implement plug-ins as shared libraries, which get dynamically loaded at run time. HyperCard supported a similar facility, but more commonly included the plug-in code in the HyperCard documents (called \"stacks\") themselves. Thus the HyperCard stack became a self-contained application in its own right, distributable as a single entity that end-users could run without the need for additional installation-steps. Programs may also implement plug-ins by loading a directory of simple script files written in a scripting language like Python or Lua.\nHelper application.\nIn the context of a web browser, a helper application is a separate program\u2014like IrfanView or Adobe Reader\u2014that extends the functionality of a browser. A helper application extends the functionality an application but unlike the typical plug-in that is loaded into the host application's address space, a helper application is a separate application. With a separate address space, the extension cannot crash the host application as is possible if they share an address space.\nHistory.\nIn the mid-1970s, the EDT text editor ran on the Unisys VS/9 operating system for the UNIVAC Series 90 mainframe computer. It allowed a program to be run from the editor, which can access the in-memory edit buffer. The plug-in executable could call the editor to inspect and change the text. The University of Waterloo Fortran compiler used this to allow interactive compilation of Fortran programs.\nEarly personal computer software with plug-in capability included HyperCard and QuarkXPress on the Apple Macintosh, both released in 1987. In 1988, Silicon Beach Software included plug-in capability in Digital Darkroom and SuperPaint.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23660", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=23660", "title": "Pierre Teilhard de Chardin", "text": "French philosopher and Jesuit priest (1881\u20131955)\nPierre Teilhard de Chardin, S.J., (; 1 May 1881 \u2013 10 April 1955) was a French Jesuit, Catholic priest, scientist, paleontologist, philosopher, mystic, and teacher. Teilhard de Chardin investigated the theory of evolution from a perspective influenced by Henri Bergson and Christian mysticism, writing multiple scientific and religious works on the subject. His mainstream scientific achievements include his palaeontological research in China, taking part in the discovery of the significant Peking Man fossils from the Zhoukoudian cave complex near Beijing. His more speculative ideas, sometimes criticized as pseudoscientific, have included a vitalist conception of the Omega Point. Along with Vladimir Vernadsky, he contributed to the development of the concept of the noosphere.\nIn 1962, the Holy Office issued a warning regarding Teilhard's works, alleging ambiguities and doctrinal errors without specifying them. Some eminent Catholic figures, including Pope Benedict XVI and Pope Francis, have made positive comments on some of his ideas since. The response to his writings by scientists has been divided. \nTeilhard served in World War I as a stretcher-bearer. He received several citations, and was awarded the M\u00e9daille militaire and the Legion of Honor, the highest French order of merit, both military and civil.\nLife.\nEarly years.\nPierre Teilhard de Chardin was born in the Ch\u00e2teau of Sarcenat, Orcines, about 2.5 miles north-west of Clermont-Ferrand, Auvergne, French Third Republic, on 1 May 1881, as the fourth of eleven children of librarian Emmanuel Teilhard de Chardin (1844\u20131932) and Berthe-Ad\u00e8le, n\u00e9e de Dompierre d'Hornoys of Picardy. His mother was a great-grandniece of the philosopher Voltaire. He inherited the double surname from his father, who was descended on the Teilhard side from an ancient family of magistrates from Auvergne originating in Murat, Cantal, ennobled under Louis XVIII.\nHis father, a graduate of the \u00c9cole Nationale des Chartes, served as a regional librarian and was a keen naturalist with a strong interest in natural science. He collected rocks, insects and plants and encouraged nature studies in the family. Pierre Teilhard's spirituality was awakened by his mother. When he was twelve, he went to the Jesuit college of Mongr\u00e9 in Villefranche-sur-Sa\u00f4ne, where he completed the Baccalaur\u00e9at in philosophy and mathematics. In 1899, he entered the Jesuit novitiate in Aix-en-Provence. In October 1900, he began his junior studies at the Coll\u00e9giale Saint-Michel de Laval. On 25 March 1901, he made his first vows. In 1902, Teilhard completed a licentiate in literature at the University of Caen.\nIn 1901 and 1902, due to an anti-clerical movement in the French Republic, the government banned the Jesuits and other religious orders from France. This forced the Jesuits to go into exile on the island of Jersey in the United Kingdom. While there, his brother and sister in France died of illnesses and another sister was incapacitated by illness. The unexpected losses of his siblings at young ages caused Teilhard to plan to discontinue his Jesuit studies in science, and change to studying theology. He wrote that he changed his mind after his Jesuit novice master encouraged him to follow science as a legitimate way to God. Due to his strength in science subjects, he was despatched to teach physics and chemistry at the Coll\u00e8ge de la Sainte Famille in Cairo, Khedivate of Egypt from 1905 until 1908. From there he wrote in a letter: \"[I]t is the dazzling of the East foreseen and drunk greedily ... in its lights, its vegetation, its fauna and its deserts.\"\nFor the next four years he was a Scholastic at Ore Place in Hastings, East Sussex where he acquired his theological formation. There he synthesized his scientific, philosophical and theological knowledge in the light of evolution. At that time he read \"Creative Evolution\" by Henri Bergson, about which he wrote that \"the only effect that brilliant book had upon me was to provide fuel at just the right moment, and very briefly, for a fire that was already consuming my heart and mind.\" Bergson was a French philosopher who was influential in the traditions of analytic philosophy and continental philosophy. His ideas were influential on Teilhard's views on matter, life, and energy. On 24 August 1911, aged 30, Teilhard was ordained a priest.\nIn the ensuing years, Bergson\u2019s protege, the mathematician and philosopher \u00c9douard Le Roy, was appointed successor to Bergson at the College de France. In 1921, Le Roy and Teilhard became friends and met weekly for long discussions. Teilhard wrote: \"I loved him like a father, and owed him a very great debt . . . he gave me confidence, enlarged my mind, and served as a spokesman for my ideas, then taking shape, on 'hominization' and the 'noosphere.'\" Le Roy later wrote in one of his books: \"I have so often and for so long talked over with Pierre Teilhard the views expressed here that neither of us can any longer pick out his own contribution.\"\nAcademic and scientific career.\nGeology.\nHis father's strong interest in natural science and geology instilled the same in Teilhard from an early age, and would continue throughout his lifetime. As a child, Teilhard was intensely interested in the stones and rocks on his family's land and the neighboring regions. His father helped him develop his skills of observation. At the University of Paris, he studied geology, botany and zoology. After the French government banned all religious orders from France and the Jesuits were exiled to the island of Jersey in the UK, Teilhard deepened his geology knowledge by studying the rocks and landscape of the island.\nIn 1920, he became a lecturer in geology at the Catholic University of Paris, and later a professor. He earned his doctorate in 1922. In 1923 he was hired to do geological research on expeditions in China by the Jesuit scientist and priest Emile Licent. In 1914, Licent with the sponsorship of the Jesuits founded one of the first museums in China and the first museum of natural science: the Mus\u00e9e Hoangho Paiho. In its first eight years, the museum was housed in the Chongde Hall of the Jesuits. In 1922, with the support of the Catholic Church and the French Concession, Licent built a special building for the museum on the land adjacent to the Tsin Ku University, which was founded by the Jesuits in China.\nWith help from Teilhard and others, Licent collected over 200,000 paleontology, animal, plant, ancient human, and rock specimens for the museum, which still make up more than half of its 380,000 specimens. Many of the publications and writings of the museum and its related institute were included in the world's database of zoological, botanical, and palaeontological literature, which is still an important basis for examining the early scientific records of the various disciplines of biology in northern China.\nTeilhard and Licent were the first to discover and examine the Shuidonggou (\u6c34\u6d1e\u6c9f) (Ordos Upland, Inner Mongolia) archaeological site in northern China. Recent analysis of flaked stone artifacts from the most recent (1980) excavation at this site has identified an assemblage which constitutes the southernmost occurrence of an Initial Upper Paleolithic blade technology proposed to have originated in the Altai region of Southern Siberia. The lowest levels of the site are now dated from 40,000 to 25,000 years ago.\nTeilhard spent the periods between 1926-1935 and 1939-1945 studying and researching the geology and palaeontology of the region. Among other accomplishments, he improved understanding of China\u2019s sedimentary deposits and established approximate ages for various layers. He also produced a geological map of China. It was during the period 1926-1935 that he joined the excavation that discovered Peking Man.\nPaleontology.\nFrom 1912 to 1914, Teilhard began his palaeontology education by working in the laboratory of the French National Museum of Natural History, studying the mammals of the middle Tertiary. Later he studied elsewhere in Europe. This included spending 5 days over the course of a 3-month period in the middle of 1913 as a volunteer assistant helping to dig with Arthur Smith Woodward and Charles Dawson at the Piltdown site. Teilhard\u2019s brief time assisting with digging there occurred many months after the discovery of the first fragments of the fraudulent \"Piltdown Man\". Stephen Jay Gould judged that Pierre Teilhard de Chardin conspired with Dawson in the Piltdown forgery. Most Teilhard experts (including all three Teilhard biographers) and many scientists (including the scientists who uncovered the hoax and investigated it) have rejected the suggestion that he participated in the hoax.\nAnthropologist H. James Birx wrote that Teilhard \"had questioned the validity of this fossil evidence from the very beginning, one positive result was that the young geologist and seminarian now became particularly interested in palaeoanthropology as the science of fossil hominids.\u201c Marcellin Boule, a palaeontologist and anthropologist, who as early as 1915 had recognized the non-hominid origins of the Piltdown finds, gradually guided Teilhard towards human paleontology. Boule was the editor of the journal \"L\u2019Anthropologie\" and the founder of two other scientific journals. He was also a professor at the Parisian Mus\u00e9um National d\u2019Histoire Naturelle for 34 years, and for many years director of the museum's Institute of Human Paleontology.\nIt was there that Teilhard became a friend of Henri Breuil, a Catholic priest, archaeologist, anthropologist, ethnologist and geologist. In 1913, Teilhard and Breuil did excavations at the prehistoric painted Cave of El Castillo in Spain. The cave contains the oldest known cave painting in the world. The site is divided into about 19 archeological layers in a sequence beginning in the Proto-Aurignacian and ending in the Bronze Age.\nLater after his return to China in 1926, Teilhard was hired by the Cenozoic Laboratory at the Peking Union Medical College. Starting in 1928, he joined other geologists and palaeontologists to excavate the sedimentary layers in the Western Hills near Zhoukoudian. At this site, the scientists discovered the so-called Peking man (Sinanthropus pekinensis), a fossil hominid dating back at least 350,000 years, which is part of the Homo erectus phase of human evolution. Teilhard became well-known as a result of his accessible explanations of the Sinanthropus discovery. He also made major contributions to the geology of this site. Teilhard's long stay in China gave him more time to think and write about evolution, as well as continue his scientific research.\nAfter the Peking Man discoveries, Breuil joined Teilhard at the site in 1931 and confirmed the presence of stone tools.\nScientific writings.\nDuring his career, Teilhard published many dozens of scientific papers in scholarly scientific journals. When they were published in collections as books, they took up 11 volumes. John Allen Grim, the co-founder and co-director of the Yale Forum on Religion and Ecology, said: \"I think you have to distinguish between the hundreds of papers that Teilhard wrote in a purely scientific vein, about which there is no controversy. In fact, the papers made him one of the top two or three geologists of the Asian continent. So this man knew what science was. What he's doing in \"The Phenomenon\" and most of the popular essays that have made him controversial is working pretty much alone to try to synthesize what he's learned about through scientific discovery - more than with scientific method - what scientific discoveries tell us about the nature of ultimate reality.\u201d Grim said those writing were controversial to some scientists because Teilhard combined theology and metaphysics with science, and controversial to some religious leaders for the same reason.\nService in World War I.\nMobilized in December 1914, Teilhard served in World War I as a stretcher-bearer in the 8th Moroccan Rifles. Priests were mobilized in the France of the First World War era, and not as military chaplains but as either stretcher-bearers or actual fighting soldiers. For his valor, he received several citations, including the M\u00e9daille militaire and the Legion of Honor.\nDuring the war, he developed his reflections in his diaries and in letters to his cousin, Marguerite Teillard-Chambon, who later published a collection of them. (See section below) He later wrote: \"...the war was a meeting ... with the Absolute.\" In 1916, he wrote his first essay: \"La Vie Cosmique\" (\"Cosmic life\"), where his scientific and philosophical thought was revealed just as his mystical life. While on leave from the military he pronounced his solemn vows as a Jesuit in Sainte-Foy-l\u00e8s-Lyon on 26 May 1918. In August 1919, in Jersey, he wrote \"Puissance spirituelle de la Mati\u00e8re\" (\"The Spiritual Power of Matter\").\nAt the University of Paris, Teilhard pursued three unit degrees of natural science: geology, botany, and zoology. His thesis treated the mammals of the French lower Eocene and their stratigraphy. After 1920, he lectured in geology at the Catholic Institute of Paris and after earning a science doctorate in 1922 became an assistant professor there.\nResearch in China.\nIn 1923 he traveled to China with Father \u00c9mile Licent, who was in charge of a significant laboratory collaboration between the National Museum of Natural History and Marcellin Boule's laboratory in Tianjin. Licent carried out considerable basic work in connection with Catholic missionaries who accumulated observations of a scientific nature in their spare time.\nTeilhard wrote several essays, including \"La Messe sur le Monde\" (\"The Mass on the World\"), in the Ordos Desert. In the following year, he continued lecturing at the Catholic Institute and participated in a cycle of conferences for the students of the Engineers' Schools. Two theological essays on original sin were sent to a theologian at his request on a purely personal basis:\nThe Church required him to give up his lecturing at the Catholic Institute in order to continue his geological research in China. Teilhard traveled again to China in April 1926. He would remain there for about twenty years, with many voyages throughout the world. He settled until 1932 in Tianjin with \u00c9mile Licent, then in Beijing. Teilhard made five geological research expeditions in China between 1926 and 1935. They enabled him to establish a general geological map of China.\nIn 1926\u201327, after a missed campaign in Gansu, Teilhard traveled in the Sanggan River Valley near Kalgan (Zhangjiakou) and made a tour in Eastern Mongolia. He wrote \"Le Milieu Divin\" (\"The Divine Milieu\"). Teilhard prepared the first pages of his main work \"Le Ph\u00e9nom\u00e8ne Humain\" (\"The Phenomenon of Man\"). The Holy See refused the Imprimatur for \"Le Milieu Divin\" in 1927. \nHe joined the ongoing excavations of the Peking Man Site at Zhoukoudian as an advisor in 1926 and continued in the role for the Cenozoic Research Laboratory of the China Geological Survey following its founding in 1928. Teilhard resided in Manchuria with \u00c9mile Licent, staying in western Shanxi and northern Shaanxi with the Chinese paleontologist Yang Zhongjian and with Davidson Black, Chairman of the China Geological Survey.\nAfter a tour in Manchuria in the area of Greater Khingan with Chinese geologists, Teilhard joined the team of American Expedition Center-Asia in the Gobi Desert, organized in June and July by the American Museum of Natural History with Roy Chapman Andrews. Henri Breuil and Teilhard discovered that the Peking Man, the nearest relative of \"Anthropopithecus\" from Java, was a \"faber\" (worker of stones and controller of fire). Teilhard wrote \"L'Esprit de la Terre\" (\"The Spirit of the Earth\").\nTeilhard took part as a scientist in the Croisi\u00e8re Jaune (Yellow Cruise) financed by Andr\u00e9 Citro\u00ebn in Central Asia. Northwest of Beijing in Kalgan, he joined the Chinese group who joined the second part of the team, the Pamir group, in Aksu City. He remained with his colleagues for several months in \u00dcr\u00fcmqi, capital of Xinjiang. In 1933, Rome ordered him to give up his post in Paris. Teilhard subsequently undertook several explorations in the south of China. He traveled in the valleys of the Yangtze and Sichuan in 1934, then, the following year, in Guangxi and Guangdong.\nDuring all these years, Teilhard contributed considerably to the constitution of an international network of research in human paleontology related to the whole of eastern and southeastern Asia. He would be particularly associated in this task with two friends, Davidson Black and the Scot George Brown Barbour. Often he would visit France or the United States, only to leave these countries for further expeditions.\nWorld travels.\nFrom 1927 to 1928, Teilhard was based in Paris. He journeyed to Leuven, Belgium, and to Cantal and Ari\u00e8ge, France. Between several articles in reviews, he met new people such as Paul Val\u00e9ry and Bruno de Solages, who were to help him in issues with the Catholic Church.\nAnswering an invitation from Henry de Monfreid, Teilhard undertook a journey of two months in Obock, in Harar in the Ethiopian Empire, and in Somalia with his colleague Pierre Lamarre, a geologist, before embarking in Djibouti to return to Tianjin. While in China, Teilhard developed a deep and personal friendship with Lucile Swan.\nDuring 1930\u20131931, Teilhard stayed in France and in the United States. During a conference in Paris, Teilhard stated: \"For the observers of the Future, the greatest event will be the sudden appearance of a collective humane conscience and a human work to make.\" From 1932 to 1933, he began to meet people to clarify issues with the Congregation for the Doctrine of the Faith regarding \"Le Milieu divin\" and \"L'Esprit de la Terre\". He met Helmut de Terra, a German geologist in the International Geology Congress in Washington, D.C.\nTeilhard participated in the 1935 Yale\u2013Cambridge expedition in northern and central India with the geologist Helmut de Terra and Patterson, who verified their assumptions on Indian Paleolithic civilisations in Kashmir and the Salt Range Valley. He then made a short stay in Java, on the invitation of Dutch paleontologist Gustav Heinrich Ralph von Koenigswald to the site of Java Man. A second cranium, more complete, was discovered. Professor von Koenigswald had also found a tooth in a Chinese apothecary shop in 1934 that he believed belonged to a three-meter-tall ape, \"Gigantopithecus,\" which lived between one hundred thousand and around a million years ago. Fossilized teeth and bone (\"dragon bones\") are often ground into powder and used in some branches of traditional Chinese medicine.\nIn 1937, Teilhard wrote \"Le Ph\u00e9nom\u00e8ne spirituel\" (\"The Phenomenon of the Spirit\") on board the boat Empress of Japan, where he met Sylvia Brett, Ranee of Sarawak The ship took him to the United States. He received the Mendel Medal granted by Villanova University during the Congress of Philadelphia, in recognition of his works on human paleontology. He made a speech about evolution, the origins and the destiny of man. \"The New York Times\" dated 19 March 1937 presented Teilhard as the Jesuit who held that man descended from monkeys. Some days later, he was to be granted the \"Doctor Honoris Causa\" distinction from Boston College.\nRome banned his work \"L'\u00c9nergie Humaine\" in 1939. By this point Teilhard was based again in France, where he was immobilized by malaria. During his return voyage to Beijing he wrote \"L'Energie spirituelle de la Souffrance\" (\"Spiritual Energy of Suffering\") (Complete Works, tome VII).\nIn 1941, Teilhard submitted to Rome his most important work, \"Le Ph\u00e9nom\u00e8ne Humain\". By 1947, Rome forbade him to write or teach on philosophical subjects. The next year, Teilhard was called to Rome by the Superior General of the Jesuits who hoped to acquire permission from the Holy See for the publication of \"Le Ph\u00e9nom\u00e8ne Humain\". However, the prohibition to publish it that was previously issued in 1944 was again renewed. Teilhard was also forbidden to take a teaching post in the Coll\u00e8ge de France. Another setback came in 1949, when permission to publish \"Le Groupe Zoologique\" was refused.\nTeilhard was nominated to the French Academy of Sciences in 1950. He was forbidden by his superiors to attend the International Congress of Paleontology in 1955. The Supreme Authority of the Holy Office, in a decree dated 15 November 1957, forbade the works of de Chardin to be retained in libraries, including those of religious institutes. His books were not to be sold in Catholic bookshops and were not to be translated into other languages.\nFurther resistance to Teilhard's work arose elsewhere. In April 1958, all Jesuit publications in Spain (\"Raz\u00f3n y Fe\", \"Sal Terrae\",\"Estudios de Deusto\", etc.) carried a notice from the Spanish Provincial of the Jesuits that Teilhard's works had been published in Spanish without previous ecclesiastical examination and in defiance of the decrees of the Holy See. A decree of the Holy Office dated 30 June 1962, under the authority of Pope John XXIII, warned:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[I]t is obvious that in philosophical and theological matters, the said works [Teilhard's] are replete with ambiguities or rather with serious errors which offend Catholic doctrine. That is why... the Rev. Fathers of the Holy Office urge all Ordinaries, Superiors, and Rectors... to effectively protect, especially the minds of the young, against the dangers of the works of Fr. Teilhard de Chardin and his followers.\nThe Diocese of Rome on 30 September 1963 required Catholic booksellers in Rome to withdraw his works as well as those that supported his views.\nDeath.\nTeilhard died in New York City, where he was in residence at the Jesuit Church of St. Ignatius Loyola, Park Avenue. On 15 March 1955, at the house of his diplomat cousin Jean de Lagarde, Teilhard told friends he hoped he would die on Easter Sunday. On the evening of Easter Sunday, 10 April 1955, during an animated discussion at the apartment of Rhoda de Terra, his personal assistant since 1949, Teilhard suffered a heart attack and died. He was buried in the cemetery for the New York Province of the Jesuits at the Jesuit novitiate, St. Andrew-on-Hudson, in Hyde Park, New York. With the moving of the novitiate, the property was sold to the Culinary Institute of America in 1970.\nTeachings.\nTeilhard de Chardin wrote two comprehensive works, \"The Phenomenon of Man\" and \"The Divine Milieu\".\nHis posthumously published book, \"The Phenomenon of Man\", set forth a sweeping account of the unfolding of the cosmos and the evolution of matter to humanity, to ultimately a reunion with Christ. In the book, Teilhard abandoned literal interpretations of creation in the Book of Genesis in favor of allegorical and theological interpretations. The unfolding of the material cosmos is described from primordial particles to the development of life, human beings and the noosphere, and finally to his vision of the Omega Point in the future, which is \"pulling\" all creation towards it. He was a leading proponent of orthogenesis, the idea that evolution occurs in a directional, goal-driven way. Teilhard argued in Darwinian terms with respect to biology, and supported the synthetic model of evolution, but argued in Lamarckian terms for the development of culture, primarily through the vehicle of education.\nTeilhard made a total commitment to the evolutionary process in the 1920s as the core of his spirituality, at a time when other religious thinkers felt evolutionary thinking challenged the structure of conventional Christian faith. He committed himself to what he thought the evidence showed.\nTeilhard made sense of the universe by assuming it had a vitalist evolutionary process. He interpreted complexity as the axis of evolution of matter into a geosphere, a biosphere, into consciousness (in man), and then to supreme consciousness (the Omega Point). Jean Houston's story of meeting Teilhard illustrates this point.\nTeilhard's unique relationship to both paleontology and Catholicism allowed him to develop a highly progressive, cosmic theology which took into account his evolutionary studies. Teilhard recognized the importance of bringing the Church into the modern world, and approached evolution as a way of providing ontological meaning for Christianity, particularly creation theology. For Teilhard, evolution was \"the natural landscape where the history of salvation is situated.\"\nTeilhard's cosmic theology is largely predicated on his interpretation of Pauline scripture, particularly Colossians 1:15-17 (especially verse 1:17b) and 1 Corinthians 15:28. He drew on the Christocentrism of these two Pauline passages to construct a cosmic theology which recognizes the absolute primacy of Christ. He understood creation to be \"a teleological process towards union with the Godhead, effected through the incarnation and redemption of Christ, 'in whom all things hold together' (Colossians 1:17).\" He further posited that creation would not be complete until each \"participated being is totally united with God through Christ in the Pleroma, when God will be 'all in all' (1 Corinthians 15:28).\" \nTeilhard's life work was predicated on his conviction that human spiritual development is moved by the same universal laws as material development. He wrote, \"...everything is the sum of the past\" and \"...nothing is comprehensible except through its history. 'Nature' is the equivalent of 'becoming', self-creation: this is the view to which experience irresistibly leads us. ... There is nothing, not even the human soul, the highest spiritual manifestation we know of, that does not come within this universal law.\"\n\"The Phenomenon of Man\" represents Teilhard's attempt at reconciling his religious faith with his academic interests as a paleontologist. One particularly poignant observation in Teilhard's book entails the notion that evolution is becoming an increasingly optional process. Teilhard points to the societal problems of isolation and marginalization as huge inhibitors of evolution, especially since evolution requires a unification of consciousness. He states that \"no evolutionary future awaits anyone except in association with everyone else.\" Teilhard argued that the human condition necessarily leads to the psychic unity of humankind, though he stressed that this unity can only be voluntary; this voluntary psychic unity he termed \"unanimization\". Teilhard also states that \"evolution is an ascent toward consciousness\", giving encephalization as an example of early stages, and therefore, signifies a continuous upsurge toward the Omega Point which, for all intents and purposes, is God.\nTeilhard also used his perceived correlation between spiritual and material to describe Christ, arguing that Christ not only has a mystical dimension but also takes on a physical dimension as he becomes the organizing principle of the universe\u2014that is, the one who \"holds together\" the universe. For Teilhard, Christ formed not only the eschatological end toward which his mystical/ecclesial body is oriented, but he also \"operates physically in order to regulate all things\" becoming \"the one from whom all creation receives its stability.\" In other words, as the one who holds all things together, \"Christ exercises a supremacy over the universe which is physical, not simply juridical. He is the unifying center of the universe and its goal. The function of holding all things together indicates that Christ is not only man and God; he also possesses a third aspect\u2014indeed, a third nature\u2014which is cosmic.\"\nIn this way, the Pauline description of the Body of Christ was not simply a mystical or ecclesial concept for Teilhard; it is cosmic. This cosmic Body of Christ \"extend[s] throughout the universe and compris[es] all things that attain their fulfillment in Christ [so that] ... the Body of Christ is the one single thing that is being made in creation.\" Teilhard describes this cosmic amassing of Christ as \"Christogenesis\". According to Teilhard, the universe is engaged in Christogenesis as it evolves toward its full realization at Omega, a point which coincides with the fully realized Christ. It is at this point that God will be \"all in all\" (1 Corinthians 15:28c).\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nRelationship with the Catholic Church.\nIn 1925, Teilhard was ordered by the Superior General of the Society of Jesus, W\u0142odzimierz Led\u00f3chowski, to leave his teaching position in France and to sign a statement withdrawing his controversial statements regarding the doctrine of original sin. Rather than quit the Society of Jesus, Teilhard obeyed and departed for China.\nThis was the first of a series of condemnations by a range of ecclesiastical officials that would continue until after Teilhard's death. In August 1939, he was told by his Jesuit superior in Beijing, \"Father, as an evolutionist and a Communist, you are undesirable here, and will have to return to France as soon as possible\". The climax of these condemnations was a 1962 \"monitum\" (warning) of the Congregation for the Doctrine of the Faith cautioning on Teilhard's works. It said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Several works of Fr. Pierre Teilhard de Chardin, some of which were posthumously published, are being edited and are gaining a good deal of success. Prescinding from a judgement about those points that concern the positive sciences, it is sufficiently clear that the above-mentioned works abound in such ambiguities and indeed even serious errors, as to offend Catholic doctrine. For this reason, the most eminent and most revered Fathers of the Holy Office exhort all Ordinaries as well as the superiors of Religious institutes, rectors of seminaries and presidents of universities, effectively to protect the minds, particularly of the youth, against the dangers presented by the works of Fr. Teilhard de Chardin and of his followers.\nThe Holy Office did not, however, place any of Teilhard's writings on the \"Index Librorum Prohibitorum\" (Index of Forbidden Books), which still existed during Teilhard's lifetime and at the time of the 1962 decree.\nShortly thereafter, prominent clerics mounted a strong theological defense of Teilhard's works. Henri de Lubac (later a Cardinal) wrote three comprehensive books on the theology of Teilhard de Chardin in the 1960s. While de Lubac mentioned that Teilhard was less than precise in some of his concepts, he affirmed the orthodoxy of Teilhard de Chardin and responded to Teilhard's critics: \"We need not concern ourselves with a number of detractors of Teilhard, in whom emotion has blunted intelligence\". Later that decade Joseph Ratzinger, a German theologian who became Pope Benedict XVI, spoke glowingly of Teilhard's Christology in Ratzinger's \"Introduction to Christianity\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It must be regarded as an important service of Teilhard de Chardin's that he rethought these ideas from the angle of the modern view of the world and, in spite of a not entirely unobjectionable tendency toward the biological approach, nevertheless on the whole grasped them correctly and in any case made them accessible once again.\nOn 20 July 1981, the Holy See stated that, after consultation of cardinals Casaroli and \u0160eper, the letter did not change the position of the warning issued by the Holy Office on 30 June 1962, which pointed out that Teilhard's work contained ambiguities and grave doctrinal errors.\nCardinal Ratzinger in his book \"The Spirit of the Liturgy\" incorporates Teilhard's vision as a touchstone of the Catholic Mass:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And so we can now say that the goal of worship and the goal of creation as a whole are one and the same\u2014divinization, a world of freedom and love. But this means that the historical makes its appearance in the cosmic. The cosmos is not a kind of closed building, a stationary container in which history may by chance take place. It is itself movement, from its one beginning to its one end. In a sense, creation is history. Against the background of the modern evolutionary world view, Teilhard de Chardin depicted the cosmos as a process of ascent, a series of unions. From very simple beginnings the path leads to ever greater and more complex unities, in which multiplicity is not abolished but merged into a growing synthesis, leading to the \"Noosphere\" in which spirit and its understanding embrace the whole and are blended into a kind of living organism. Invoking the epistles to the Ephesians and Colossians, Teilhard looks on Christ as the energy that strives toward the Noosphere and finally incorporates everything in its \"fullness\". From here Teilhard went on to give a new meaning to Christian worship: the transubstantiated Host is the anticipation of the transformation and divinization of matter in the christological \"fullness\". In his view, the Eucharist provides the movement of the cosmos with its direction; it anticipates its goal and at the same time urges it on.\nCardinal Avery Dulles said in 2004:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In his own poetic style, the French Jesuit Teilhard de Chardin liked to meditate on the Eucharist as the first fruits of the new creation. In an essay called The Monstrance he describes how, kneeling in prayer, he had a sensation that the Host was beginning to grow until at last, through its mysterious expansion, \"the whole world had become incandescent, had itself become like a single giant Host\". Although it would probably be incorrect to imagine that the universe will eventually be transubstantiated, Teilhard correctly identified the connection between the Eucharist and the final glorification of the cosmos.\nCardinal Christoph Sch\u00f6nborn wrote in 2007:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Hardly anyone else has tried to bring together the knowledge of Christ and the idea of evolution as the scientist (paleontologist) and theologian Fr. Pierre Teilhard de Chardin, S.J., has done.\u00a0... His fascinating vision\u00a0... has represented a great hope, the hope that faith in Christ and a scientific approach to the world can be brought together.\u00a0... These brief references to Teilhard cannot do justice to his efforts. The fascination which Teilhard de Chardin exercised for an entire generation stemmed from his radical manner of looking at science and Christian faith together.\nIn July 2009, Vatican spokesman Federico Lombardi said, \"By now, no one would dream of saying that [Teilhard] is a heterodox author who shouldn't be studied.\"\nPope Francis refers to Teilhard's eschatological contribution in his encyclical \"Laudato si\"'.\nThe philosopher Dietrich von Hildebrand criticized severely the work of Teilhard. According to Hildebrand, in a conversation after a lecture by Teilhard: \"He (Teilhard) ignored completely the decisive difference between nature and supernature. After a lively discussion in which I ventured a criticism of his ideas, I had an opportunity to speak to Teilhard privately. When our talk touched on St. Augustine, he exclaimed violently: 'Don't mention that unfortunate man; he spoiled everything by introducing the supernatural.'\" Von Hildebrand writes that Teilhardism is incompatible with Christianity, substitutes efficiency for sanctity, dehumanizes man, and describes love as merely cosmic energy.\nEvaluations by scientists.\nJulian Huxley.\nJulian Huxley, the evolutionary biologist, in the preface to the 1955 edition of \"The Phenomenon of Man\", praised the thought of Teilhard de Chardin for looking at the way in which human development needs to be examined within a larger integrated universal sense of evolution, though admitting he could not follow Teilhard all the way. In the publication Encounter, Huxley wrote: \"The force and purity of Teilhard's thought and expression ... has given the world a picture not only of rare clarity but pregnant with compelling conclusions.\"\nTheodosius Dobzhansky.\nTheodosius Dobzhansky, writing in 1973, drew upon Teilhard's insistence that evolutionary theory provides the core of how man understands his relationship to nature, calling him \"one of the great thinkers of our age\". Dobzhansky was renowned as the president of four prestigious scientific associations: the Genetics Society of America, the American Society of Naturalists, the Society for the Study of Evolution and the American Society of Zoologists. He also\u00a0called Teilhard\u00a0\"one of the greatest intellects of our time.\"\nDaniel Dennett.\nDaniel Dennett claimed \"it has become clear to the point of unanimity among scientists that Teilhard offered nothing serious in the way of an alternative to orthodoxy; the ideas that were peculiarly his were confused, and the rest was just bombastic redescription of orthodoxy.\"\nDavid Sloan Wilson.\nIn 2019, evolutionary biologist David Sloan Wilson praised Teilhard's book \"The Phenomenon of Man\" as \"scientifically prophetic in many ways\", and considers his own work as an updated version of it, commenting that \"[m]odern evolutionary theory shows that what Teilhard meant by the Omega Point is achievable in the foreseeable future.\"\nRobert Francoeur.\nRobert Francoeur (1931-2012), the American biologist, said the Phenomenon of Man \"will be one of the few books that will be remembered after the dust of the century has settled on many of its companions.\"\nStephen Jay Gould.\nIn an essay published in the magazine \"Natural History\" (and later compiled as the 16th essay in his book \"Hen's Teeth and Horse's Toes\"), American biologist Stephen Jay Gould made a case for Teilhard's guilt in the Piltdown Hoax, arguing that Teilhard has made several compromising slips of the tongue in his correspondence with paleontologist Kenneth Oakley, in addition to what Gould termed to be his \"suspicious silence\" about Piltdown despite having been, at that moment in time, an important milestone in his career. In a later book, Gould claims that Steven Rose wrote that \"Teilhard is revered as a mystic of genius by some, but among most biologists is seen as little more than a charlatan.\"\nNumerous scientists and Teilhard experts have refuted Gould\u2019s theories about Teilhard\u2019s guilt in the hoax, saying they are based on inaccuracies. In an article in New Scientist in September, 1981, Peter Costello said claims that Teilhard had been silent were factually wrong: \u201cMuch else of what is said about Teilhard is also wrong. \u2026. After the exposure of the hoax, he did not refuse to make a statement; he gave a statement to the press on 26 November 1953, which was published in New York and London the next day. ... If questions needed to be asked about Teilhard's role in the Piltdown affair, they could have been asked when he was in London during the summer of 1953. They were not asked. But enough is now known to prove Teilhard innocent of all involvement in the hoax.\u201d Teilhard also wrote multiple letters about the hoax at the request of and in reply to Oakley, one of the 3 scientists who uncovered it, in an effort to help them get to the bottom of what occurred 40 years earlier.\nAnother of the three scientists, S.J. Weiner said he spoke to Teilhard extensively about Piltdown and \"He (Teilhard) discussed all the points that I put to him perfectly frankly and openly.\" Weiner spent years investigating who was responsible for the hoax and concluded that Charles Dawson was the sole culprit. He also said: \"Gould would have you accept that Oakley was the same mind (as himself); but it is not so. When Gould's article came out Oakley dissociated himself from it. ...I have seen Oakley recently and he has no reservations... about his belief that Teilhard had nothing to do with the planting of this material and manufacture of the fraud.\"\nIn November, 1981, Oakley himself published a letter in New Scientist saying: \"There is no proved factual evidence known to me that supports the premise that Father Teilhard de Chardin gave Charles Dawson a piece of fossil elephant molar tooth as a souvenir of his time spent in North Africa. This faulty thread runs throughout the reconstruction ... After spending a year thinking about this accusation, I have at last become convinced that it is erroneous.\" Oakley also pointed out that after Teilhard got his degree in paleontology and gained experience in the field, he published scientific articles that show he found the scientific claims of the two Piltdown leaders to be incongruous, and that Teilhard did not agree they had discovered an ape-man that was a missing link between apes and humans.\nIn a comprehensive rebuttal of Gould in America magazine, Mary Lukas said his claims about Teilhard were \"patently ridiculous\u201d and \u201cwilder flights of fancy\u201d that were easily disprovable and weak. For example, she notes Teilhard was only briefly and minimally involved in the Piltdown project for four reasons: 1) He was only a student in his early days of studying paleontology. 2) His college was in France and he was at the Piltdown site in Britain for a total of just 5 days over a short period of three months out of the 7-year project. 3) He was simply a volunteer assistant, helping with basic digging. 4) This limited involvement ended prior to the most important claimed discovery, due to his being conscripted to serve in the French army. She added: \"Further, according to his letters, both published and unpublished, to friends, Teilhard's relationship to Dawson was anything but close.\"\nLukas said Gould made the claims for selfish reasons: \u201cThe charge gained Mr. Gould two weeks of useful publicity and prepared reviewers to give a friendly reception to the collection of essays\u201d that he was about to publish. She said Teilhard was \u201cbeyond doubt the most famous of\u201d all the people who were involved in the excavations\u201d and \u201cthe one who could gather headlines most easily\u2026. The shock value of the suggestion that the philosopher-hero was also a criminal was stunning.\u201d Two years later, Lukas published a more detailed article in the British scholarly journal Antiquity in which she further refuted Gould, including an extensive timeline of events.\nWinifred McCulloch wrote a very detailed rebuttal of Gould, calling his claim \u201chighly subjective,\u201d \u201cvery idiosyncratic,\u201d filled with clear \u201cweaknesses\u201d and \u201cshown to be impossible.\u201d She said Weiner had criticized Gould's accusations in a talk at Georgetown University in 1981. She also noted that Oakley wrote in a letter to Lukas in 1981 that her article in America constituted \"a total refutation of Gould's interpretation of Teilhard's letters to me in 1953-1954. . . . You have . . . unearthed evidence that will seriously undermine Gould's confidence in having any evidence against Teilhard in regard to what he (Teilhard) said in his letters to me.\" She wrote: \"Gould's method of presenting his main argument might be called \"inferred intent\" - projecting onto Teilhard ways of thinking and acting that have no evidential base and are completely foreign to all we know of Teilhard. With Gould it seems that the guilty verdict came first, then he created a persona to fit the crime.\u201d\nPeter Medawar.\nIn 1961, British immunologist and Nobel laureate Peter Medawar wrote a scornful review of \"The Phenomenon of Man\" for the journal \"Mind\": \"the greater part of it [...] is nonsense, tricked out with a variety of metaphysical conceits, and its author can be excused of dishonesty only on the grounds that before deceiving others he has taken great pains to deceive himself. [...] Teilhard practiced an intellectually unexacting kind of science [...]. He has no grasp of what makes a logical argument or what makes for proof. He does not even preserve the common decencies of scientific writing, though his book is professedly a scientific treatise. [...] Teilhard habitually and systematically cheats with words [...], uses in metaphor words like energy, tension, force, impetus, and dimension as if they retained the weight and thrust of their special scientific usages. [...] It is the style that creates the illusion of content.\"\nIn 2014, Donald Wayne Viney evaluated Medawar's review and concluded that the case made against Teilhard was \"remarkably thin, marred by misrepresentations and elementary philosophical blunders.\" These defects, Viney noted, were uncharacteristic of Medawar's other work.\nIn another response, John Allen Grim said when Teilhard \"wrote The Phenomenon of Man \u2026 he was using science there in a very broad sense. What he was really looking for was to be actually more radically empirical than conventional science is. Conventional science leaves out so much that's really there, especially our own subjectivity and some of the other things that are qualitative and value laden that are going on in the world. That science \u2026 has abstracted from values, meaning, subjectivity, purpose, God, and talked only about physical causation. Teilhard knew this, because when he wrote his [science journal] papers, he didn't bring God, value and so forth into it. But when he wrote The Phenomenon, he was doing something different. But it's not against the spirit of science. It was to actually expand the empirical orientation of science to take into account things that science unfortunately leaves out, like consciousness, for example, which today, in a materialist worldview, doesn't even exist, and yet it's the most palpable experience that any of us has. So if you try to construct a worldview that leaves out something so vital and important as mind to subjectivity, then that's unempirical, that's irrelevant. What we need is a radically empirical approach to the world that includes within what he calls hyperphysics, the experience of consciousness and also the experiences of faith, religions.\u201d\nRichard Dawkins.\nEvolutionary biologist and a New Atheist Richard Dawkins called Medawar's review \"devastating\" and \"The Phenomenon of Man\" \"the quintessence of bad poetic science\".\nKarl Stern.\nKarl Stern, the neurobiologist of the Montreal Neurological Institute, wrote: \"It happens so rarely that science and wisdom are blended as they were in the person of Teilhard de Chardin.\"\nGeorge Gaylord Simpson.\nGeorge Gaylord Simpson felt that if Teilhard were right, the lifework \"of Huxley, Dobzhansky, and hundreds of others was not only wrong, but meaningless\", and was mystified by their public support for him. He considered Teilhard a friend and his work in paleontology extensive and important, but expressed strongly adverse views of his contributions as scientific theorist and philosopher.\nWilliam G. Pollard.\nWilliam G. Pollard, the physicist and founder of the prestigious Oak Ridge Institute of Nuclear Studies (and its director until 1974), praised Teilhard\u2019s work as \"A fascinating and powerful presentation of the amazing fact of the emergence of man in the unfolding drama of the cosmos.\"\nJohn Barrow and Frank Tipler.\nJohn Barrow and Frank Tipler, both physicists and cosmologists, base much of their work on Teilhard and use some of his key terms such as the Omega point. However, Manuel Alfonseca, author of 50 books and 200 technical articles, said in an article in the quarterly Popular Science: \"Barrow and Tipler have not understood Teilhard (apparently they have just read '\"The Phenomenon of Man\"', at least this is the only work by Teilhard they mention). In fact, they have got everything backwards.\"\nWolfgang Smith.\nWolfgang Smith, an American scientist versed in Catholic theology, devotes an entire book to the critique of Teilhard's doctrine, which he considers neither scientific (assertions without proofs), nor Catholic (personal innovations), nor metaphysical (the \"Absolute Being\" is not yet absolute), and of which the following elements can be noted (all the words in quotation marks are Teilhard's, quoted by Smith):\nEvolution.\nSmith claims that for Teilhard, evolution is not only a scientific theory but an irrefutable truth \"immune from any subsequent contradiction by experience\"; it constitutes the foundation of his doctrine. Matter becomes spirit and humanity moves towards a super-humanity thanks to complexification (physico-chemical, then biological, then human), socialization, scientific research and technological and cerebral development; the explosion of the first atomic bomb is one of its milestones, while waiting for \"the vitalization of matter by the creation of super-molecules, the remodeling of the human organism by means of hormones, control of heredity and sex by manipulation of genes and chromosomes [...]\".\nMatter and spirit.\nTeilhard maintains that the human spirit (which he identifies with the \"anima\" and not with the \"spiritus\") originates in a matter which becomes more and more complex until it produces life, then consciousness, then the consciousness of being conscious, holding that the immaterial can emerge from the material. At the same time, he supports the idea of the presence of embryos of consciousness from the very genesis of the universe: \"We are logically forced to assume the existence [...] of some sort of psyche\" infinitely diffuse in the smallest particle.\nTheology.\nSmith believes that since Teilhard affirms that \"God creates evolutively\", he denies the Book of Genesis, not only because it attests that God created man, but that he created him in his own image, thus perfect and complete, then that man fell, that is to say the opposite of an ascending evolution. That which is metaphysically and theologically \"above\" - symbolically speaking - becomes for Teilhard \"ahead\", yet to come; even God, who is neither perfect nor timeless, evolves in symbiosis with the World, which Teilhard, a resolute pantheist, venerates as the equal of the Divine. As for Christ, not only is he there to activate the wheels of progress and complete the evolutionary ascent, but he himself evolves..\nNew religion.\nAs he wrote to a cousin: \"What dominates my interests increasingly is the effort to establish in me and define around me a new religion (call it a better Christianity, if you will)...\", and elsewhere: \"a Christianity re-incarnated for a second time in the spiritual energies of Matter\". The more Teilhard refines his theories, the more he emancipates himself from established Christian doctrine: a \"religion of the earth\" must replace a \"religion of heaven\". By their common faith in Man, he writes, Christians, Marxists, Darwinists, materialists of all kinds will ultimately join around the same summit: the Christic Omega Point.\nLucien Cu\u00e9not.\nLucien Cu\u00e9not, the biologist who proved that Mendelism applied to animals as well as plants through his experiments with mice, wrote: \"Teilhard's greatness lay in this, that in a world ravaged by neurosis he provided an answer to out modern anguish and reconciled man with the cosmos and with himself by offering him an \"ideal of humanity that, through a higher and consciously willed synthesis, would restore the instinctive equilibrium enjoyed in ages of primitive simplicity.\" Mendelism is a group of biological inheritance principles developed by the Catholic friar-scientist Gregor Mendel. Though for many years Mendelism was rejected by most biologists and other scientists, its principles - combined with the Boveri\u2013Sutton chromosome theory of inheritance - eventually became the core of classical genetics.\nLegacy.\nBrian Swimme wrote \"Teilhard was one of the first scientists to realize that the human and the universe are inseparable. The only universe we know about is a universe that brought forth the human.\"\nGeorge Gaylord Simpson named the most primitive and ancient genus of true primate, the Eocene genus \"Teilhardina\".\nOn June 25, 1947 Teilhard was honored by the French Ministry of Foreign Affairs for \"Outstanding services to the intellectual and scientific influence of France\" and was promoted to the rank of Officer in the \"Legion of Honor\". In 1950, Teilhard was elected a member of the French Academy of Sciences.\nInfluence on arts and culture.\nTeilhard and his work continue to influence the arts and culture.\nThere is a broad range of references to Teilhard ranging from quotations, as when an auto mechanic cites Teilhard in Philip K. Dick's \"A Scanner Darkly\" to philosophical underpinning of an entire plot, as Teilhard's work does in Julian May's 1987\u201394 Galactic Milieu Series.\nIn artworks:\n\"The Embracing Universe\", an oratorio for choir and 7 instruments, composed by Justin Grounds to a libretto by Fred LaHaye saw its first performance in 2019. It is based on the life and thought of Teilhard de Chardin.\nCollege campuses:\n\"The De Chardin Project\", a play celebrating Teilhard's life, ran from 20 November to 14 December 2014 in Toronto, Canada. \"The Evolution of Teilhard de Chardin\", a documentary film on Teilhard's life, was scheduled for release in 2015.\nFounded in 1978, George Addair based much of Omega Vector on Teilhard's work.\nThe American physicist Frank J. Tipler has further developed Teilhard's Omega Point concept in two controversial books, \"The Physics of Immortality\" and the more theologically based Physics of Christianity. While keeping the central premise of Teilhard's Omega Point (i.e. a universe evolving towards a maximum state of complexity and consciousness) Tipler has supplanted some of the more mystical/ theological elements of the OPT with his own scientific and mathematical observations (as well as some elements borrowed from Freeman Dyson's eternal intelligence theory).\nIn 1972, the Uruguayan priest Juan Luis Segundo, in his five-volume series \"A Theology for Artisans of a New Humanity,\" wrote that Teilhard \"noticed the profound analogies existing between the conceptual elements used by the natural sciences\u2014all of them being based on the hypothesis of a general evolution of the universe.\"\nInfluence of his cousin Marguerite Teilard Chambon.\nMarguerite Teillard-Chambon, (alias Claude Aragonn\u00e8s) was a French writer who edited and had published three volumes of correspondence with her cousin, Pierre Teilhard de Chardin, \"La gen\u00e8se d'une pens\u00e9e\" (\"The Making of a Mind\") being the last, after her own death in 1959. She furnished each with an introduction. Marguerite, a year older than Teilhard, was considered among those who knew and understood him best. They had shared a childhood in Auvergne; she it was who encouraged him to undertake a doctorate in science at the Sorbonne; she eased his entry into the Catholic Institute, through her connection to Emmanuel de Margerie and she introduced him to the intellectual life of Paris. Throughout the First World War, she corresponded with him, acting as a \"midwife\" to his thinking, helping his thought to emerge and honing it. In September 1959 she participated in a gathering organised at Saint-Babel, near Issoire, devoted to Teilhard's philosophical contribution. On the way home to Chambon-sur-Lac, she was fatally injured in a road traffic accident. Her sister, Alice, completed the final preparations for the publication of the final volume of her cousin Teilhard's wartime letters.\nInfluence on the New Age movement.\nTeilhard has had a profound influence on the New Age movements and has been described as \"perhaps the man most responsible for the spiritualization of evolution in a global and cosmic context\".\nAlleged support of eugenics and racism.\nTeilhard has been criticized by the theologian John P. Slattery for allegedly incorporating elements of scientific racism, social Darwinism, and eugenics into his work. He argued in 1929 that racial inequality was rooted in biological difference: \"Do the yellows\u2014[the Chinese]\u2014have the same human value as the whites? [Fr.] Licent and many missionaries say that their present inferiority is due to their long history of Paganism. I'm afraid that this is only a 'declaration of pastors.' Instead, the cause seems to be the natural racial foundation\u2026\" In a letter from 1936 explaining his Omega Point conception, he rejected both the Fascist quest for particularistic hegemony and the Christian/Communist insistence on egalitarianism: \"As not all ethnic groups have the same value, they must be dominated, which does not mean they must be despised\u2014quite the reverse \u2026 In other words, \"at one and the same time\" there should be official recognition of: (1) the primacy/priority of the earth over nations; (2) the inequality of peoples and races. Now the \"second\" point is currently reviled by Communism \u2026 and the Church, and the \"first\" point is similarly reviled by the Fascist systems (and, of course, by less gifted peoples!)\". In the essay 'Human Energy' (1937), he asked, \"What fundamental attitude \u2026 should the advancing wing of humanity take to fixed or definitely unprogressive ethnical groups? The earth is a closed and limited surface. To what extent should it tolerate, racially or nationally, areas of lesser activity? More generally still, how should we judge the efforts we lavish in all kinds of hospitals on saving what is so often no more than one of life's rejects? \u2026 To what extent should not the development of the strong \u2026 take precedence over the preservation of the weak?\" The theologian John P. Slattery interprets this last remark to suggest \"genocidal practices for the sake of eugenics\".\nEven after World War II Teilhard continued to argue for racial and individual eugenics in the name of human progress, and denounced the United Nations declaration of the Equality of Races (1950) as \"scientifically useless\" and \"practically dangerous\" in a letter to the agency's director Jaime Torres Bodet. In 1953, he expressed his frustration at the Church's failure to embrace the scientific possibilities for optimising human nature, including by the separation of sexuality from reproduction (a notion later developed e.g. by the second-wave feminist Shulamith Firestone in her 1970 book \"The Dialectic of Sex\"), and postulated \"the absolute right \u2026 to try everything right to the end\u2014even in the matter of human biology\".\nThe theologian John F. Haught has defended Teilhard from Slattery's charge of \"persistent attraction to racism, fascism, and genocidal ideas\" by pointing out that Teilhard's philosophy was not based on racial exclusion but rather on union through differentiation, and that Teilhard took seriously the human responsibility for continuing to remake the world. With regard to union through differentiation, he underlined the importance of understanding properly a quotation used by Slattery in which Teilhard writes, \"I hate nationalism and its apparent regressions to the past. But I am very interested in the primacy it returns to the collective. Could a passion for 'the race' represent a first draft of the Spirit of the Earth?\" Writing from China in October 1936, shortly after the outbreak of the Spanish Civil War, Teilhard expressed his stance towards the new political movement in Europe, \"I am alarmed at the attraction that various kinds of Fascism exert on intelligent (?) people who can see in them nothing but the hope of returning to the Neolithic\". He felt that the choice between what he called \"the American, the Italian, or the Russian type\" of political economy (i.e. liberal capitalism, Fascist corporatism, Bolshevik Communism) had only \"technical\" relevance to his search for overarching unity and a philosophy of action.\nOther.\nFritjof Capra's systems theory book \"The Turning Point: Science, Society, and the Rising Culture\" positively contrasts Teilhard to Darwinian evolution.\nBibliography.\nThe dates in parentheses are the dates of first publication in French and English. Most of these works were written years earlier, but Teilhard's ecclesiastical order forbade him to publish them because of their controversial nature. The essay collections are organized by subject rather than date, thus each one typically spans many years.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23661", "revid": "48535834", "url": "https://en.wikipedia.org/wiki?curid=23661", "title": "Phutball", "text": "Two-person board game\nPhutball (short for Philosopher's Football) is a two-player abstract strategy board game described in Elwyn Berlekamp, John Horton Conway, and Richard K. Guy's \"Winning Ways for your Mathematical Plays\".\nRules.\nPhutball is played on the intersections of a 19\u00d715 grid using one white stone and as many black stones as needed.\nIn this article the two players are named Ohs (O) and Eks (X).\nThe board is labeled A through P (omitting I) from left to right and 1 to 19 from bottom to top from Ohs' perspective. \nRows 0 and 20 represent \"off the board\" beyond rows 1 and 19 respectively.\nAs specialized phutball boards are hard to come by, the game is usually played on a 19\u00d719 Go board, with a white stone representing the football and black stones representing the men.\nThe objective is to score goals by using the men (the black stones) to move the football (the white stone) onto or over the opponent's goal line (rows 1 or 19). Ohs tries to move the football to rows 19 or 20 and Eks to rows 1 or 0. \nAt the start of the game the football is placed on the central point, unless one player gives the other a handicap, in which case the ball starts nearer one player's goal.\nPlayers alternate making moves. \nA move is either to add a man to any vacant point on the board or to move the ball. \nThere is no difference between men played by Ohs and those played by Eks.\nThe football is moved by a series of jumps over adjacent men. \nEach jump is to the first vacant point in a straight line horizontally, vertically, or diagonally over one or more men. \nThe jumped men are then removed from the board (before any subsequent jump occurs). \nThis process repeats for as long as there remain men available to be jumped and the player desires. Jumping is optional: there is no requirement to jump. \nIn contrast to checkers, multiple men in a row are jumped and removed as a group.\nThe diagram on the right illustrates a single move consisting of a series of jumps. \nIf the football ends the move on or over the opponent's goal line then a goal has been scored. \nIf the football passes through a goal line, but ends up elsewhere due to further jumps, the game continues.\nComputational complexity.\nThe game is sufficiently complex that checking whether there is a win in one (on an m\u00d7n board) is NP-complete. From the starting position, it is not known whether any player has a winning strategy or both players have a drawing strategy, but there exist other configurations from which both players have drawing strategies.\nGiven an arbitrary board position, with initially a white stone placed in the center, determining whether the current player has a winning strategy is PSPACE-hard.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23664", "revid": "635492", "url": "https://en.wikipedia.org/wiki?curid=23664", "title": "Papyrus", "text": "Writing material made from a reed-like plant\nPapyrus ( ) is a material similar to thick paper that was used in ancient times as a writing material. It was made from the pith of the papyrus plant, \"Cyperus papyrus\", a wetland sedge. \"Papyrus\" (plural: \"papyri\" or \"papyruses\") can also refer to a document written on sheets of such material, joined side by side and rolled up into a scroll, an early form of a book.\nPapyrus was first known to have been used in Egypt (at least as far back as the First Dynasty), as the papyrus plant was once abundant across the Nile Delta. It was also used throughout the Mediterranean region. Apart from writing material, ancient Egyptians employed papyrus in the construction of other artifacts, such as reed boats, mats, rope, sandals, and baskets.\nHistory.\nPapyrus was first manufactured in Egypt as far back as the 3rd millennium BCE. The earliest archaeological evidence of papyrus was excavated in 2012 and 2013 at Wadi al-Jarf, an ancient Egyptian harbor located on the Red Sea coast. These documents, the Diary of Merer, date from c.\u20092560\u20132550 BCE (end of the reign of Khufu). The papyrus rolls describe the last years of building the Great Pyramid of Giza.\nFor multiple millennia, papyrus was commonly rolled into scrolls as a form of storage. However, at some point late in its history, papyrus began being collected together in the form of codices akin to the modern book. This may have been mimicking the book-form of codices created with parchment. Early Christian writers soon adopted the codex form, and in the Greco-Roman world, it became common to cut sheets from papyrus rolls to form codices. Codices were an improvement on the papyrus scroll, as the papyrus was not pliable enough to fold without cracking, and a long roll, or scroll, was required to create large-volume texts. Papyrus had the advantage of being relatively cheap and easy to produce, but it was fragile and susceptible to both moisture and excessive dryness. Unless the papyrus was of perfect quality, the writing surface was irregular, and the range of media that could be used was also limited.\nPapyrus was gradually overtaken in Europe by a rival writing surface that rose in prominence known as parchment, which was made from animal skins. By the beginning of the 4th century CE, the most important books began to be manufactured in parchment, and works worth preserving were transferred from papyrus to parchment. Parchment had significant advantages over papyrus, including higher durability in moist climates and being more conducive to writing on both sides of the surface. The main advantage of papyrus had been its cheaper raw material \u2014 the papyrus plant is easy to cultivate in a suitable climate and produces more writing material than animal hides (the most expensive books, made from foetal vellum would take up to dozens of bovine fetuses to produce). However, as trade networks declined, the availability of papyrus outside the range of the papyrus plant became limited and it thus lost its cost advantage. \nPapyrus' last appearance in the Merovingian chancery was with a document from 692 CE, though it was known in Gaul until the middle of the following century. The latest certain dates for the use of papyrus in Europe are 1057 for a papal decree (typically conservative, all papal bulls were on papyrus until 1022), under Pope Victor II, and 1087 for an Arabic document. Its use in Egypt continued until it was replaced by less expensive paper introduced by the Islamic world, which originally learned of it from the Chinese. By the 12th century, parchment and paper were in use in the Byzantine Empire, but papyrus was still an option.\nUntil the middle of the 19th century, only some isolated documents written on papyrus were known, and museums simply showed them as curiosities. They did not contain literary works. The first modern discovery of papyri rolls was made at Herculaneum in 1752. Until then, the only papyri known had been a few surviving from medieval times. Scholarly investigations began with the Dutch historian Caspar Jacob Christiaan Reuvens (1793\u20131835). He wrote about the content of the Leyden papyrus, published in 1830. The first publication has been credited to the British scholar Charles Wycliffe Goodwin (1817\u20131878), who published for the Cambridge Antiquarian Society one of the Papyri Graecae Magicae V, translated into English with commentary in 1853.\nVarying quality.\nPapyrus was made in several qualities and prices. Pliny the Elder and Isidore of Seville described six variations of papyrus that were sold in the Roman market of the day. These were graded by quality based on how fine, firm, white, and smooth the writing surface was. Grades ranged from the superfine Augustan, which was produced in sheets of 13 digits (10 inches) wide, to the least expensive and most coarse, measuring six digits (four inches) wide. Materials deemed unusable for writing or less than six digits were considered commercial quality and were pasted edge to edge to be used only for wrapping.\nEtymology.\nThe English word \"papyrus\" derives, via Latin, from Greek \u03c0\u03ac\u03c0\u03c5\u03c1\u03bf\u03c2 (\"papyros\"), a loanword of unknown (perhaps Pre-Greek) origin. Greek has a second word for it, \u03b2\u03cd\u03b2\u03bb\u03bf\u03c2 (\"byblos\"), said to derive from the name of the Phoenician city of Byblos. The Greek writer Theophrastus, who flourished during the 4th century BCE, uses \"papyros\" when referring to the plant used as a foodstuff and \"byblos\" for the same plant when used for nonfood products, such as cordage, basketry, or writing surfaces. The more specific term \u03b2\u03af\u03b2\u03bb\u03bf\u03c2 \"biblos\", which finds its way into English in such words as 'bibliography', 'bibliophile', and 'bible', refers to the inner bark of the papyrus plant. \"Papyrus\" is also the etymon of 'paper', a similar substance.\nIn the Egyptian language, papyrus was called \"wadj\" (\"w3\u1e0f\"), \"tjufy\" (\"\u1e6fwfy\")5, or \"djet\" (\"\u1e0ft\").\nDocuments written on papyrus.\nThe word for the material papyrus is also used to designate documents written on sheets of it, often rolled up into scrolls. The plural for such documents is papyri. Historical papyri are given identifying names \u2013 generally the name of the discoverer, first owner, or institution where they are kept \u2013 and numbered, such as \"Papyrus Harris I\". Often an abbreviated form is used, such as \"pHarris I\". These documents provide important information on ancient writings; they give us the only extant copy of Menander, the Egyptian Book of the Dead, Egyptian treatises on medicine (the Ebers Papyrus) and on surgery (the Edwin Smith papyrus), Egyptian mathematical treatises (the Rhind papyrus), and Egyptian folk tales (the Westcar Papyrus). When, in the 18th century, a library of ancient papyri was found in Herculaneum, ripples of expectation spread among the learned men of the time. However, since these papyri were badly charred, their unscrolling and deciphering are still going on today.\nManufacture and use.\nPapyrus was made from the stem of the papyrus plant, \"Cyperus papyrus\". The outer rind was first removed, and the sticky fibrous inner pith is cut lengthwise into thin strips about long. The strips were then placed side by side on a hard surface with their edges slightly overlapping, and then another layer of strips is laid on top at right angles. The strips may have been soaked in water long enough for decomposition to begin, perhaps increasing adhesion, but this is not certain. The two layers possibly were glued together. While still moist, the two layers were hammered together, mashing the layers into a single sheet. The sheet was then dried under pressure. After drying, the sheet was polished with a rounded object, possibly a stone, seashell, or round hardwood.\nSheets, or \"mollema\", could be cut to fit the obligatory size or glued together to create a longer roll. The point where the mollema are joined with glue is called the \"kollesis\". A wooden stick would be attached to the last sheet in a roll, making it easier to handle. To form the long strip scrolls required, several such sheets were united and placed so all the horizontal fibres parallel with the roll's length were on one side and all the vertical fibres on the other. Normally, texts were first written on the \"recto\", the lines following the fibres, parallel to the long edges of the scroll. Secondarily, papyrus was often reused, writing across the fibres on the \"verso\".\nOne source used for determining the method by which papyrus was created in antiquity is through the examination of tombs in the ancient Egyptian city of Thebes, which housed a necropolis containing many murals displaying the process of papyrus-making. The Roman commander Pliny the Elder also describes the methods of preparing papyrus in his \"Naturalis Historia\".5\nIn a dry climate, like that of Egypt, papyrus is stable, formed as it is of highly rot-resistant cellulose, but storage in humid conditions can result in molds attacking and destroying the material. Library papyrus rolls were stored in wooden boxes and chests made in the form of statues. Papyrus scrolls were organized according to subject or author and identified with clay labels that specified their contents without having to unroll the scroll. In European conditions, papyrus seems to have lasted only a matter of decades; a 200-year-old papyrus was considered extraordinary. Imported papyrus once commonplace in Greece and Italy has since deteriorated beyond repair, but papyri are still being found in Egypt; extraordinary examples include the Elephantine papyri and the famous finds at Oxyrhynchus and Nag Hammadi. The Villa of the Papyri at Herculaneum, containing the library of Lucius Calpurnius Piso Caesoninus, Julius Caesar's father-in-law, was preserved by the eruption of Mount Vesuvius but has only been partially excavated.\nSporadic attempts to revive the manufacture of papyrus have been made since the mid-18th century. Scottish explorer James Bruce experimented in the late 18th century with papyrus plants from Sudan, for papyrus had become extinct in Egypt. Also in the 18th century, Sicilian Saverio Landolina manufactured papyrus at Syracuse, where papyrus plants had continued to grow in the wild. During the 1920s, when Egyptologist Battiscombe Gunn lived in Maadi, outside Cairo, he experimented with the manufacture of papyrus, growing the plant in his garden. He beat the sliced papyrus stalks between two layers of linen and produced successful examples of papyrus, one of which was exhibited in the Egyptian Museum in Cairo. The modern technique of papyrus production used in Egypt for the tourist trade was developed in 1962 by the Egyptian engineer Hassan Ragab using plants that had been reintroduced into Egypt in 1872 from France. Both Sicily and Egypt have centres of limited papyrus production.\nPapyrus is still used by communities living in the vicinity of swamps, to the extent that rural householders derive up to 75% of their income from swamp goods. Particularly in East and Central Africa, people harvest papyrus, which is used to manufacture items that are sold or used locally. Examples include baskets, hats, fish traps, trays or winnowing mats, and floor mats. Papyrus is also used to make roofs, ceilings, rope, and fences. Although alternatives, such as eucalyptus, are increasingly available, papyrus is still used as fuel.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23665", "revid": "50657366", "url": "https://en.wikipedia.org/wiki?curid=23665", "title": "Pixel", "text": "Physical point in a raster image\nIn digital imaging, a pixel (abbreviated px), pel, or picture element is the smallest addressable element in a raster image, or the smallest addressable element in a dot matrix display device. In most digital display devices, pixels are the smallest element that can be manipulated through software.\nEach pixel is a sample of an original image; more samples typically provide more accurate representations of the original. The intensity of each pixel is variable. In color imaging systems, a color is typically represented by three or four component intensities such as red, green, and blue, or cyan, magenta, yellow, and black.\nIn some contexts (such as descriptions of camera sensors), \"pixel\" refers to a single scalar element of a multi-component representation (called a \"photosite\" in the camera sensor context, although \"sensel\" 'sensor element' is sometimes used), while in yet other contexts (like MRI) it may refer to a set of component intensities for a spatial position.\nSoftware on early consumer computers was necessarily rendered at a low resolution, with large pixels visible to the naked eye; graphics made under these limitations may be called pixel art, especially in reference to video games. Modern computers and displays, however, can easily render orders of magnitude more pixels than was previously possible, necessitating the use of large measurements like the megapixel (one million pixels).\nEtymology.\nThe word \"pixel\" is a combination of \"pix\" (from \"pictures\", shortened to \"pics\") and \"el\" (for \"element\"); similar formations with '\"el\"' include the words \"voxel\" 'volume pixel', and \"texel\" 'texture pixel'. The word \"pix\" appeared in \"Variety\" magazine headlines in 1932, as an abbreviation for the word \"pictures\", in reference to movies. By 1938, \"pix\" was being used in reference to still pictures by photojournalists.\nThe word \"pixel\" was first published in 1965 by Frederic C. Billingsley of JPL, to describe the picture elements of scanned images from space probes to the Moon and Mars. Billingsley had learned the word from Keith E. McFarland, at the Link Division of General Precision in Palo Alto, who in turn said he did not know where it originated. McFarland said simply it was \"in use at the time\" (c.\u20091963).\nThe concept of a \"picture element\" dates to the earliest days of television, for example as \"Bildpunkt\" (the German word for \"pixel\", literally 'picture point') in the 1888 German patent of Paul Nipkow. According to various etymologies, the earliest publication of the term \"picture element\" itself was in \"Wireless World\" magazine in 1927, though it had been used earlier in various U.S. patents filed as early as 1911.\nSome authors explain \"pixel\" as \"picture cell,\" as early as 1972. In graphics and in image and video processing, \"pel\" is often used instead of \"pixel\". For example, IBM used it in their Technical Reference for the original PC.\nPixilation, spelled with a second \"i\", is an unrelated filmmaking technique that dates to the beginnings of cinema, in which live actors are posed frame by frame and photographed to create stop-motion animation. An archaic British word meaning \"possession by spirits (pixies)\", the term has been used to describe the animation process since the early 1950s; various animators, including Norman McLaren and Grant Munro, are credited with popularizing it.\nTechnical.\nA pixel is generally thought of as the smallest single component of a digital image. However, the definition is highly context-sensitive. For example, there can be \"printed pixels\" in a page, or pixels carried by electronic signals, or represented by digital values, or pixels on a display device, or pixels in a digital camera (photosensor elements). This list is not exhaustive and, depending on context, synonyms include pel, sample, byte, bit, dot, and spot. \"Pixels\" can be used as a unit of measure such as: 2400 pixels per inch, 640 pixels per line, or spaced 10 pixels apart.\nThe measures \"dots per inch\" (dpi) and \"pixels per inch\" (ppi) are sometimes used interchangeably, but have distinct meanings, especially for printer devices, where dpi is a measure of the printer's density of dot (e.g. ink droplet) placement. For example, a high-quality photographic image may be printed with 600 ppi on a 1200 dpi inkjet printer. Even higher dpi numbers, such as the 4800 dpi quoted by printer manufacturers since 2002, do not mean much in terms of achievable resolution.\nThe more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution, though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a \"three-megapixel\" digital camera, which has a nominal three million pixels, or as a pair of numbers, as in a \"640 by 480 display\", which has 640 pixels from side to side and 480 from top to bottom (as in a VGA display) and therefore has a total number of 640 \u00d7 480 = 307,200 pixels, or 0.3 megapixels.\nThe pixels, or color samples, that form a digitized image (such as a JPEG file used on a web page) may or may not be in one-to-one correspondence with screen pixels, depending on how a computer displays an image. In computing, an image composed of pixels is known as a \"bitmapped image\" or a \"raster image\". The word \"raster\" originates from television scanning patterns, and has been widely used to describe similar halftone printing and storage techniques.\nSampling patterns.\nFor convenience, pixels are normally arranged in a regular two-dimensional grid. By using this arrangement, many common operations can be implemented by uniformly applying the same operation to each pixel independently. Other arrangements of pixels are possible, with some sampling patterns even changing the shape (or kernel) of each pixel across the image. For this reason, care must be taken when acquiring an image on one device and displaying it on another, or when converting image data from one pixel format to another.\nFor example:\nResolution of computer monitors.\nComputer monitors (and TV sets) generally have a fixed native resolution. What it is depends on the monitor, and size. See below for historical exceptions.\nComputers can use pixels to display an image, often an abstract image that represents a GUI. The resolution of this image is called the display resolution and is determined by the video card of the computer. Flat-panel monitors (and TV sets), e.g. OLED or LCD monitors, or E-ink, also use pixels to display an image, and have a native resolution, and it should (ideally) be matched to the video card resolution. Each pixel is made up of triads, with the number of these triads determining the native resolution.\nOn older, historically available, CRT monitors the resolution was possibly adjustable (still lower than what modern monitor achieve), while on some such monitors (or TV sets) the beam sweep rate was fixed, resulting in a fixed native resolution. Most CRT monitors do not have a fixed beam sweep rate, meaning they do not have a native resolution at all \u2013 instead they have a set of resolutions that are equally well supported. To produce the sharpest images possible on a flat-panel, e.g. OLED or LCD, the user must ensure the display resolution of the computer matches the native resolution of the monitor.\nResolution of telescopes.\nThe pixel scale used in astronomy is the angular distance between two objects on the sky that fall one pixel apart on the detector (CCD or infrared chip). The scale \"s\" measured in radians is the ratio of the pixel spacing \"p\" and focal length \"f\" of the preceding optics, \"s\" \n \"p / f\". (The focal length is the product of the focal ratio by the diameter of the associated lens or mirror.)\nBecause \"s\" is usually expressed in units of arcseconds per pixel, because 1 radian equals (180/\u03c0) \u00d7 3600 \u2248 206,265 arcseconds, and because focal lengths are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as \"s\" \n 206 \"p / f\".\nBits per pixel.\nThe number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1 bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors:\nFor color depths of 15 or more bits per pixel, the depth is normally the sum of the bits allocated to each of the red, green, and blue components. Highcolor, usually meaning 16 bpp, normally has five bits for red and blue each, and six bits for green, as the human eye is more sensitive to errors in green than in the other two primary colors. For applications involving transparency, the 16 bits may be divided into five bits each of red, green, and blue, with one bit left for transparency. A 24-bit depth allows 8 bits per component. On some systems, 32-bit depth is available: this means that each 24-bit pixel has an extra 8 bits to describe its opacity (for purposes of combining with another image).\nSubpixels.\nMany display and image-acquisition systems are not capable of displaying or sensing the different color channels at the same site. Therefore, the pixel grid is divided into single-color regions that contribute to the displayed or sensed color when viewed at a distance. In some displays, such as LCD, LED, and plasma displays, these single-color regions are separately addressable elements, which have come to be known as subpixels, mostly RGB colors. For example, LCDs typically divide each pixel vertically into three subpixels. When the square pixel is divided into three subpixels, each subpixel is necessarily rectangular. In display industry terminology, subpixels are often referred to as \"pixels\", as they are the basic addressable elements in a viewpoint of hardware, and hence \"pixel circuits\" rather than \"subpixel circuits\" is used.\nMost digital camera image sensors use single-color sensor regions, for example using the Bayer filter pattern, and in the camera industry these are known as \"pixels\" just like in the display industry, not \"subpixels\".\nFor systems with subpixels, two different approaches can be taken:\nThis latter approach, referred to as subpixel rendering, uses knowledge of pixel geometry to manipulate the three colored subpixels separately, producing an increase in the apparent resolution of color displays. While CRT displays use red-green-blue-masked phosphor areas, dictated by a mesh grid called the shadow mask, it would require a difficult calibration step to be aligned with the displayed pixel raster, and so CRTs do not use subpixel rendering.\nThe concept of subpixels is related to samples.\nLogical pixel.\nIn graphic, web design, and user interfaces, a \"pixel\" may refer to a fixed length rather than a true pixel on the screen to accommodate different pixel densities. A typical definition, such as in CSS, is that a \"physical\" pixel is . Doing so makes sure a given element will display as the same size no matter what screen resolution views it.\nThere may, however, be some further adjustments between a \"physical\" pixel and an on-screen logical pixel. As screens are viewed at difference distances (consider a phone, a computer display, and a TV), the desired length (a \"reference pixel\") is scaled relative to a reference viewing distance ( in CSS). In addition, as true screen pixel densities are rarely multiples of 96 dpi, some rounding is often applied so that a logical pixel is an integer amount of actual pixels. Doing so avoids render artifacts. The final \"pixel\" obtained after these two steps becomes the \"anchor\" to which all other absolute measurements (e.g. the \"centimeter\") are based on.\nWorked example, with a 2160p TV placed away from the viewer:\nA browser will then choose to use the 1.721\u00d7 pixel size, or round to a 2\u00d7 ratio.\nMegapixel.\nA megapixel (MP) is a million pixels; the term is used not only for the number of pixels in an image but also to express the number of image sensor elements of digital cameras or the number of display elements of digital displays. For example, a camera that makes a 2048 \u00d7 1536 pixel image (3,145,728 finished image pixels) typically uses a few extra rows and columns of sensor elements and is commonly said to have \"3.2 megapixels\" or \"3.4 megapixels\", depending on whether the number reported is the \"effective\" or the \"total\" pixel count.\nThe number of pixels is sometimes quoted as the \"resolution\" of a photo. This measure of resolution can be calculated by multiplying the width and height of a sensor in pixels.\nDigital cameras use photosensitive electronics, either charge-coupled device (CCD) or complementary metal\u2013oxide\u2013semiconductor (CMOS) image sensors, consisting of a large number of single sensor elements, each of which records a measured intensity level. In most digital cameras, the sensor array is covered with a patterned color filter mosaic having red, green, and blue regions in the Bayer filter arrangement so that each sensor element can record the intensity of a single primary color of light. The camera interpolates the color information of neighboring sensor elements, through a process called demosaicing, to create the final image. These sensor elements are often called \"pixels\", even though they only record one channel (only red or green or blue) of the final color image. Thus, two of the three color channels for each sensor must be interpolated and a so-called \"N-megapixel\" camera that produces an N-megapixel image provides only one-third of the information that an image of the same size could get from a scanner. Thus, certain color contrasts may look fuzzier than others, depending on the allocation of the primary colors (green has twice as many elements as red or blue in the Bayer arrangement).\nDxO Labs invented the Perceptual MegaPixel (P-MPix) to measure the sharpness that a camera produces when paired to a particular lens \u2013 as opposed to the MP a manufacturer states for a camera product, which is based only on the camera's sensor. P-MPix claims to be a more accurate and relevant value for photographers to consider when weighing up camera sharpness. As of mid-2013, the Sigma 35 mm f/1.4 DG HSM lens mounted on a Nikon D800 has the highest measured P-MPix. However, with a value of 23 MP, it still more than one-third of the D800's 36.3 MP sensor. In August 2019, Xiaomi released the Redmi Note 8 Pro as the world's first smartphone with 64 MP camera. On December 12, 2019, Samsung released Samsung A71 that also has a 64 MP camera. In late 2019, Xiaomi announced the first camera phone with 108 MP 1/1.33-inch across sensor. The sensor is larger than most of bridge camera with 1/2.3-inch across sensor.\nOne new method to add megapixels has been introduced in a Micro Four Thirds System camera, which only uses a 16 MP sensor but can produce a 64 MP RAW (40 MP JPEG) image by making two exposures, shifting the sensor by a half pixel between them. Using a tripod to take level multi-shots within an instance, the multiple 16 MP images are then combined into a 64 MP image.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23666", "revid": "14383484", "url": "https://en.wikipedia.org/wiki?curid=23666", "title": "Prime number", "text": "Number divisible only by 1 or itself\nA prime number (or a prime) is a natural number greater than 1 that is not a product of two smaller natural numbers. A natural number greater than 1 that is not prime is called a composite number. For example, 5 is prime because the only ways of writing it as a product, 1 \u00d7 5 or 5 \u00d7 1, involve 5 itself. However, 4 is composite because it is a product (2\u00a0\u00d7\u00a02) in which both numbers are smaller than 4. Primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than 1 is either a prime itself or can be factorized as a product of primes that is unique up to their order.\nThe property of being prime is called primality. A simple but slow method of checking the primality of a given number &amp;NoBreak;&amp;NoBreak;, called trial division, tests whether &amp;NoBreak;&amp;NoBreak; is a multiple of any integer between 2 and &amp;NoBreak;}&amp;NoBreak;. Faster algorithms include the Miller\u2013Rabin primality test, which is fast but has a small chance of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. As of \u00a02024[ [update]] the largest known prime number is a Mersenne prime with 41,024,320 decimal digits.\nThere are infinitely many primes, as demonstrated by Euclid around 300\u00a0BC. No known simple formula separates prime numbers from composite numbers. However, the distribution of primes within the natural numbers in the large can be statistically modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says roughly that the probability of a randomly chosen large number being prime is inversely proportional to its number of digits, that is, to its logarithm.\nSeveral historical questions regarding prime numbers are still unsolved. These include Goldbach's conjecture, that every even integer greater than 2 can be expressed as the sum of two primes, and the twin prime conjecture, that there are infinitely many pairs of primes that differ by two. Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which relies on the difficulty of factoring large numbers into their prime factors. In abstract algebra, objects that behave in a generalized way like prime numbers include prime elements and prime ideals.\nDefinition and examples.\nA natural number (1, 2, 3, 4, 5, 6, etc.) is called a \"prime number\" (or a \"prime\") if it is greater than 1 and cannot be written as the product of two smaller natural numbers. The numbers greater than 1 that are not prime are called composite numbers. In other words, &amp;NoBreak;&amp;NoBreak; is prime if &amp;NoBreak;&amp;NoBreak; items cannot be divided up into smaller equal-size groups of more than one item, or if it is not possible to arrange &amp;NoBreak;&amp;NoBreak; dots into a rectangular grid that is more than one dot wide and more than one dot high. For example, among the numbers 1 through 6, the numbers 2, 3, and 5 are the prime numbers, as there are no other numbers that divide them evenly (without a remainder). 1 is not prime, as it is specifically excluded in the definition. 4 = 2 \u00d7 2 and 6 = 2 \u00d7 3 are both composite.\nThe divisors of a natural number &amp;NoBreak;&amp;NoBreak; are the natural numbers that divide &amp;NoBreak;&amp;NoBreak; evenly. Every natural number has both 1 and itself as a divisor. If it has any other divisor, it cannot be prime. This leads to an equivalent definition of prime numbers: they are the numbers with exactly two positive divisors. Those two are 1 and the number itself. As 1 has only one divisor, itself, it is not prime by this definition. Yet another way to express the same thing is that a number &amp;NoBreak;&amp;NoBreak; is prime if it is greater than one and if none of the numbers formula_1 divides &amp;NoBreak;&amp;NoBreak; evenly.\nThe first 25 prime numbers (all the prime numbers less than 100) are:\n 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97 (sequence in the OEIS).\nNo even number &amp;NoBreak;&amp;NoBreak; greater than 2 is prime because any such number can be expressed as the product &amp;NoBreak;&amp;NoBreak;. Therefore, every prime number other than 2 is an odd number, and is called an \"odd prime\". Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9. The numbers that end with other digits are all composite: decimal numbers that end in 0, 2, 4, 6, or 8 are even, and decimal numbers that end in 0 or 5 are divisible by 5.\nThe set of all primes is sometimes denoted by formula_2 (a boldface capital P) or by formula_3 (a blackboard bold capital P).\nHistory.\nThe Rhind Mathematical Papyrus, from around 1550 BC, has Egyptian fraction expansions of different forms for prime and composite numbers. However, the earliest surviving records of the study of prime numbers come from the ancient Greek mathematicians, who called them (). Euclid's \"Elements\" (c. 300 BC) proves the infinitude of primes and the fundamental theorem of arithmetic, and shows how to construct a perfect number from a Mersenne prime. Another Greek invention, the Sieve of Eratosthenes, is still used to construct lists of \nAround 1000\u00a0AD, the Islamic mathematician Ibn al-Haytham (Alhazen) found Wilson's theorem, characterizing the prime numbers as the numbers &amp;NoBreak;&amp;NoBreak; that evenly divide &amp;NoBreak;&amp;NoBreak;. He also conjectured that all even perfect numbers come from Euclid's construction using Mersenne primes, but was unable to prove it. Another Islamic mathematician, Ibn al-Banna' al-Marrakushi, observed that the sieve of Eratosthenes can be sped up by considering only the prime divisors up to the square root of the upper limit. Fibonacci took the innovations from Islamic mathematics to Europe. His book \"Liber Abaci\" (1202) was the first to describe trial division for testing primality, again using divisors only up to the square root.\nIn 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler). Fermat also investigated the primality of the Fermat numbers &amp;NoBreak;&amp;NoBreak;, and Marin Mersenne studied the Mersenne primes, prime numbers of the form formula_4 with &amp;NoBreak;&amp;NoBreak; itself a prime. Christian Goldbach formulated Goldbach's conjecture, that every even number is the sum of two primes, in a 1742 letter to Euler. Euler proved Alhazen's conjecture (now the Euclid\u2013Euler theorem) that all even perfect numbers can be constructed from Mersenne primes. He introduced methods from mathematical analysis to this area in his proofs of the infinitude of the primes and the divergence of the sum of the reciprocals of the primes &amp;NoBreak;&amp;NoBreak;. At the start of the 19th century, Legendre and Gauss conjectured that as &amp;NoBreak;&amp;NoBreak; tends to infinity, the number of primes up to &amp;NoBreak;&amp;NoBreak; is asymptotic to &amp;NoBreak;&amp;NoBreak;, where formula_5 is the natural logarithm of &amp;NoBreak;&amp;NoBreak;. A weaker consequence of this high density of primes was Bertrand's postulate, that for every formula_6 there is a prime between &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak;, proved in 1852 by Pafnuty Chebyshev. Ideas of Bernhard Riemann in his 1859 paper on the zeta-function sketched an outline for proving the conjecture of Legendre and Gauss. Although the closely related Riemann hypothesis remains unproven, Riemann's outline was completed in 1896 by Hadamard and de la Vall\u00e9e Poussin, and the result is now known as the prime number theorem. Another important 19th century result was Dirichlet's theorem on arithmetic progressions, that certain arithmetic progressions contain infinitely many primes.\nMany mathematicians have worked on primality tests for numbers larger than those where trial division is practicably applicable. Methods that are restricted to specific number forms include P\u00e9pin's test for Fermat numbers (1877), Proth's theorem (c. 1878), the Lucas\u2013Lehmer primality test (originated 1856), and the generalized Lucas primality test.\nSince 1951 all the largest known primes have been found using these tests on computers. The search for ever larger primes has generated interest outside mathematical circles, through the Great Internet Mersenne Prime Search and other distributed computing projects. The idea that prime numbers had few applications outside of pure mathematics was shattered in the 1970s when public-key cryptography and the RSA cryptosystem were invented, using prime numbers as their basis.\nThe increased practical importance of computerized primality testing and factorization led to the development of improved methods capable of handling large numbers of unrestricted form. The mathematical theory of prime numbers also moved forward with the Green\u2013Tao theorem (2004) that there are arbitrarily long arithmetic progressions of prime numbers, and Yitang Zhang's 2013 proof that there exist infinitely many prime gaps of bounded size.\nPrimality of one.\nMost early Greeks did not even consider 1 to be a number, so they could not consider its primality. A few scholars in the Greek and later Roman tradition, including Nicomachus, Iamblichus, Boethius, and Cassiodorus, also considered the prime numbers to be a subdivision of the odd numbers, so they did not consider &amp;NoBreak;&amp;NoBreak; to be prime either. However, Euclid and a majority of the other Greek mathematicians considered &amp;NoBreak;&amp;NoBreak; as prime. The medieval Islamic mathematicians largely followed the Greeks in viewing 1 as not being a number. By the Middle Ages and Renaissance, mathematicians began treating 1 as a number, and by the 17th century some of them included it as the first prime number. In the mid-18th century, Christian Goldbach listed 1 as prime in his correspondence with Leonhard Euler; however, Euler himself did not consider 1 to be prime. Many 19th century mathematicians still considered 1 to be prime, and Derrick Norman Lehmer included 1 in his \"list of primes less than ten million\" published in 1914. Lists of primes that included 1 continued to be published as recently However, by the early 20th century mathematicians began to agree that 1 should not be listed as prime, but rather in its own special category as a \"unit\".\nIf 1 were to be considered a prime, many statements involving primes would need to be awkwardly reworded. For example, the fundamental theorem of arithmetic would need to be rephrased in terms of factorizations into primes greater than 1, because every number would have multiple factorizations with any number of copies of\u00a01. Similarly, the sieve of Eratosthenes would not work correctly if it handled 1 as a prime, because it would eliminate all multiples of 1 (that is, all other numbers) and output only the single number\u00a01. Some other more technical properties of prime numbers also do not hold for the number 1: for instance, the formulas for Euler's totient function or for the sum of divisors function are different for prime numbers than they are for\u00a01.\nElementary properties.\nUnique factorization.\nWriting a number as a product of prime numbers is called a \"prime factorization\" of the number. For example:\n formula_7\nThe terms in the product are called \"prime factors\". The same prime factor may occur more than once; this example has two copies of the prime factor formula_8 When a prime occurs multiple times, exponentiation can be used to group together multiple copies of the same prime number: for example, in the second way of writing the product above, formula_9 denotes the square or second power of &amp;NoBreak;&amp;NoBreak;.\nThe central importance of prime numbers to number theory and mathematics in general stems from the \"fundamental theorem of arithmetic\". This theorem states that every integer larger than 1 can be written as a product of one or more primes. More strongly, this product is unique in the sense that any two prime factorizations of the same number will have the same numbers of copies of the same primes, although their ordering may differ. So, although there are many different ways of finding a factorization using an integer factorization algorithm, they all must produce the same result. Primes can thus be considered the \"basic building blocks\" of the natural numbers.\nSome proofs of the uniqueness of prime factorizations are based on Euclid's lemma: If &amp;NoBreak;&amp;NoBreak; is a prime number and &amp;NoBreak;&amp;NoBreak; divides a product formula_10 of integers &amp;NoBreak;&amp;NoBreak; and formula_11 then &amp;NoBreak;&amp;NoBreak; divides &amp;NoBreak;&amp;NoBreak; or &amp;NoBreak;&amp;NoBreak; divides &amp;NoBreak;&amp;NoBreak; (or both). Conversely, if a number &amp;NoBreak;&amp;NoBreak; has the property that when it divides a product it always divides at least one factor of the product, then &amp;NoBreak;&amp;NoBreak; must be prime.\nInfinitude.\nThere are infinitely many prime numbers. Another way of saying this is that the sequence\n formula_12\nof prime numbers never ends. This statement is referred to as \"Euclid's theorem\" in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers, Furstenberg's proof using general topology, and Kummer's elegant proof.\nEuclid's proof shows that every finite list of primes is incomplete. The key idea is to multiply together the primes in any given list and add formula_13 If the list consists of the primes formula_14 this gives the number\n formula_15\nBy the fundamental theorem, &amp;NoBreak;&amp;NoBreak; has a prime factorization\n formula_16\nwith one or more prime factors. &amp;NoBreak;&amp;NoBreak; is evenly divisible by each of these factors, but &amp;NoBreak;&amp;NoBreak; has a remainder of one when divided by any of the prime numbers in the given list, so none of the prime factors of &amp;NoBreak;&amp;NoBreak; can be in the given list. Because there is no finite list of all the primes, there must be infinitely many primes.\nThe numbers formed by adding one to the products of the smallest primes are called Euclid numbers. The first five of them are prime, but the sixth,\n formula_17\nis a composite number.\nFormulas for primes.\nThere is no known efficient formula for primes. For example, there is no non-constant polynomial, even in several variables, that takes \"only\" prime values. However, there are numerous expressions that do encode all primes, or only primes. One possible formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once. There is also a set of Diophantine equations in nine variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its \"positive\" values are prime.\nOther examples of prime-generating formulas come from Mills' theorem and a theorem of Wright. These assert that there are real constants formula_18 and formula_19 such that\n formula_20\nare prime for any natural number &amp;NoBreak;&amp;NoBreak; in the first formula, and any number of exponents in the second formula. Here formula_21 represents the floor function, the largest integer less than or equal to the number in question. However, these are not useful for generating primes, as the primes must be generated first in order to compute the values of &amp;NoBreak;&amp;NoBreak; or formula_22\nOpen questions.\nMany conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood proof for decades: all four of Landau's problems from 1912 are still unsolved. One of them is Goldbach's conjecture, which asserts that every even integer &amp;NoBreak;&amp;NoBreak; greater than &amp;NoBreak;&amp;NoBreak; can be written as a sum of two primes. As of 2014[ [update]], this conjecture has been verified for all numbers up to formula_23 Weaker statements than this have been proven; for example, Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes. Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime (the product of two primes). Also, any even integer greater than 10 can be written as the sum of six primes. The branch of number theory studying such questions is called additive number theory.\nAnother type of problem concerns prime gaps, the differences between consecutive primes.\nThe existence of arbitrarily large prime gaps can be seen by noting that the sequence formula_24 consists of formula_25 composite numbers, for any natural number formula_26 However, large prime gaps occur much earlier than this argument shows. For example, the first prime gap of length 8 is between the primes 89 and 97, much smaller than formula_27 It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2; this is the twin prime conjecture. Polignac's conjecture states more generally that for every positive integer formula_28 there are infinitely many pairs of consecutive primes that differ by formula_29\nAndrica's conjecture, Brocard's conjecture, Legendre's conjecture, and Oppermann's conjecture all suggest that the largest gaps between primes from 1 to &amp;NoBreak;&amp;NoBreak; should be at most approximately formula_30 a result that is known to follow from the Riemann hypothesis, while the much stronger Cram\u00e9r conjecture sets the largest gap size at &amp;NoBreak;&amp;NoBreak;. Prime gaps can be generalized to prime &amp;NoBreak;&amp;NoBreak;-tuples, patterns in the differences among more than two prime numbers. Their infinitude and density are the subject of the first Hardy\u2013Littlewood conjecture, which can be motivated by the heuristic that the prime numbers behave similarly to a random sequence of numbers with density given by the prime number theorem.\nAnalytic properties.\nAnalytic number theory studies number theory through the lens of continuous functions, limits, infinite series, and the related mathematics of the infinite and infinitesimal.\nThis area of study began with Leonhard Euler and his first major result, the solution to the Basel problem.\nThe problem asked for the value of the infinite sum formula_31\nwhich today can be recognized as the value formula_32 of the Riemann zeta function. This function is closely connected to the prime numbers and to one of the most significant unsolved problems in mathematics, the Riemann hypothesis. Euler showed that &amp;NoBreak;&amp;NoBreak;.\nThe reciprocal of this number, &amp;NoBreak;&amp;NoBreak;, is the limiting probability that two random numbers selected uniformly from a large range are relatively prime (have no factors in common).\nThe distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the &amp;NoBreak;&amp;NoBreak;-th prime is known. Dirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials\n formula_33\nwith relatively prime integers &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different linear polynomials with the same &amp;NoBreak;&amp;NoBreak; have approximately the same proportions of primes.\nAlthough conjectures have been formulated about the proportions of primes in higher-degree polynomials, they remain unproven, and it is unknown whether there exists a quadratic polynomial that (for integer arguments) is prime infinitely often.\nAnalytical proof of Euclid's theorem.\nEuler's proof that there are infinitely many primes considers the sums of reciprocals of primes,\n formula_34\nEuler showed that, for any arbitrary real number &amp;NoBreak;&amp;NoBreak;, there exists a prime &amp;NoBreak;&amp;NoBreak; for which this sum is greater than &amp;NoBreak;&amp;NoBreak;. This shows that there are infinitely many primes, because if there were finitely many primes the sum would reach its maximum value at the biggest prime rather than growing past every\u00a0&amp;NoBreak;&amp;NoBreak;.\nThe growth rate of this sum is described more precisely by Mertens' second theorem. For comparison, the sum\n formula_35\ndoes not grow to infinity as &amp;NoBreak;&amp;NoBreak; goes to infinity (see the Basel problem). In this sense, prime numbers occur more often than squares of natural numbers,\nalthough both sets are infinite. Brun's theorem states that the sum of the reciprocals of twin primes,\n formula_36\nis finite. Because of Brun's theorem, it is not possible to use Euler's method to solve the twin prime conjecture, that there exist infinitely many twin primes.\nNumber of primes below a given bound.\nThe prime-counting function formula_37 is defined as the number of primes not greater than &amp;NoBreak;&amp;NoBreak;. For example, &amp;NoBreak;&amp;NoBreak;, since there are five primes less than or equal to\u00a011. Methods such as the Meissel\u2013Lehmer algorithm can compute exact values of formula_37 faster than it would be possible to list each prime up to &amp;NoBreak;&amp;NoBreak;. The prime number theorem states that formula_37 is asymptotic to &amp;NoBreak;&amp;NoBreak;, which is denoted as\n formula_40\nand means that the ratio of formula_37 to the right-hand fraction approaches 1 as &amp;NoBreak;&amp;NoBreak; grows to infinity. This implies that the likelihood that a randomly chosen number less than &amp;NoBreak;&amp;NoBreak; is prime is (approximately) inversely proportional to the number of digits in\u00a0&amp;NoBreak;&amp;NoBreak;.\nIt also implies that the &amp;NoBreak;&amp;NoBreak;th prime number is proportional to formula_42\nand therefore that the average size of a prime gap is proportional to &amp;NoBreak;&amp;NoBreak;.\nA more accurate estimate for formula_37 is given by the offset logarithmic integral\n formula_44\nArithmetic progressions.\nAn arithmetic progression is a finite or infinite sequence of numbers such that consecutive numbers in the sequence all have the same difference. This difference is called the modulus of the progression. For example,\n formula_45\nis an infinite arithmetic progression with modulus 9. In an arithmetic progression, all the numbers have the same remainder when divided by the modulus; in this example, the remainder is 3. Because both the modulus 9 and the remainder 3 are multiples of 3, so is every element in the sequence. Therefore, this progression contains only one prime number, 3 itself. In general, the infinite progression\n formula_46\ncan have more than one prime only when its remainder &amp;NoBreak;&amp;NoBreak; and modulus &amp;NoBreak;&amp;NoBreak; are relatively prime. If they are relatively prime, Dirichlet's theorem on arithmetic progressions asserts that the progression contains infinitely many primes.\nThe Green\u2013Tao theorem shows that there are arbitrarily long finite arithmetic progressions consisting only of primes.\nPrime values of quadratic polynomials.\nEuler noted that the function\n formula_47\nyields prime numbers for &amp;NoBreak;&amp;NoBreak;, although composite numbers appear among its later values. The search for an explanation for this phenomenon led to the deep algebraic number theory of Heegner numbers and the class number problem. The Hardy\u2013Littlewood conjecture F predicts the density of primes among the values of quadratic polynomials with integer coefficients in terms of the logarithmic integral and the polynomial coefficients. No quadratic polynomial has been proven to take infinitely many prime values.\nThe Ulam spiral arranges the natural numbers in a two-dimensional grid, spiraling in concentric squares surrounding the origin with the prime numbers highlighted. Visually, the primes appear to cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than others.\nZeta function and the Riemann hypothesis.\nOne of the most famous unsolved questions in mathematics, dating from 1859, and one of the Millennium Prize Problems, is the Riemann hypothesis, which asks where the zeros of the Riemann zeta function formula_48 are located.\nThis function is an analytic function on the complex numbers. For complex numbers &amp;NoBreak;&amp;NoBreak; with real part greater than one it equals both an infinite sum over all integers, and an infinite product over the prime numbers,\n formula_49\nThis equality between a sum and a product, discovered by Euler, is called an Euler product. The Euler product can be derived from the fundamental theorem of arithmetic, and shows the close connection between the zeta function and the prime numbers.\nIt leads to another proof that there are infinitely many primes: if there were only finitely many,\nthen the sum-product equality would also be valid at &amp;NoBreak;&amp;NoBreak;, but the sum would diverge (it is the harmonic series &amp;NoBreak;&amp;NoBreak;) while the product would be finite, a contradiction.\nThe Riemann hypothesis states that the zeros of the zeta-function are all either negative even numbers, or complex numbers with real part equal to 1/2. The original proof of the prime number theorem was based on a weak form of this hypothesis, that there are no zeros with real part equal to 1, although other more elementary proofs have been found. The prime-counting function can be expressed by Riemann's explicit formula as a sum in which each term comes from one of the zeros of the zeta function; the main term of this sum is the logarithmic integral, and the remaining terms cause the sum to fluctuate above and below the main term. In this sense, the zeros control how regularly the prime numbers are distributed. If the Riemann hypothesis is true, these fluctuations will be small, and the\nasymptotic distribution of primes given by the prime number theorem will also hold over much shorter intervals (of length about the square root of &amp;NoBreak;&amp;NoBreak; for intervals near a number &amp;NoBreak;&amp;NoBreak;).\nAbstract algebra.\nModular arithmetic and finite fields.\nModular arithmetic modifies usual arithmetic by only using the numbers &amp;NoBreak;}&amp;NoBreak;, for a natural number &amp;NoBreak;&amp;NoBreak; called the modulus.\nAny other natural number can be mapped into this system by replacing it by its remainder after division by &amp;NoBreak;&amp;NoBreak;. Modular sums, differences and products are calculated by performing the same replacement by the remainder on the result of the usual sum, difference, or product of integers. Equality of integers corresponds to \"congruence\" in modular arithmetic: &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; are congruent (written formula_50 mod &amp;NoBreak;&amp;NoBreak;) when they have the same remainder after division by &amp;NoBreak;&amp;NoBreak;. However, in this system of numbers, division by all nonzero numbers is possible if and only if the modulus is prime. For instance, with the prime number 7 as modulus, division by 3 is possible: &amp;NoBreak;}&amp;NoBreak;, because clearing denominators by multiplying both sides by 3 gives the valid formula &amp;NoBreak;}&amp;NoBreak;. However, with the composite modulus 6, division by 3 is impossible. There is no valid solution to formula_51: clearing denominators by multiplying by 3 causes the left-hand side to become 2 while the right-hand side becomes either 0 or 3. In the terminology of abstract algebra, the ability to perform division means that modular arithmetic modulo a prime number forms a field or, more specifically, a finite field, while other moduli only give a ring but not a field.\nSeveral theorems about primes can be formulated using modular arithmetic. For instance, Fermat's little theorem states that if formula_52 (mod &amp;NoBreak;&amp;NoBreak;), then formula_53 (mod &amp;NoBreak;&amp;NoBreak;). Summing this over all choices of &amp;NoBreak;&amp;NoBreak; gives the equation\nformula_54\nvalid whenever &amp;NoBreak;&amp;NoBreak; is prime.\nGiuga's conjecture says that this equation is also a sufficient condition for &amp;NoBreak;&amp;NoBreak; to be prime. Wilson's theorem says that an integer formula_55 is prime if and only if the factorial formula_56 is congruent to formula_57 mod &amp;NoBreak;&amp;NoBreak;. For a composite number\u00a0&amp;NoBreak;&amp;NoBreak; this cannot hold, since one of its factors divides both n and &amp;NoBreak;&amp;NoBreak;, and so formula_58 is impossible.\n\"p\"-adic numbers.\nThe &amp;NoBreak;&amp;NoBreak;-adic order formula_59 of an integer &amp;NoBreak;&amp;NoBreak; is the number of copies of &amp;NoBreak;&amp;NoBreak; in the prime factorization of &amp;NoBreak;&amp;NoBreak;. The same concept can be extended from integers to rational numbers by defining the &amp;NoBreak;&amp;NoBreak;-adic order of a fraction formula_60 to be &amp;NoBreak;&amp;NoBreak;. The &amp;NoBreak;&amp;NoBreak;-adic absolute value formula_61 of any rational number &amp;NoBreak;&amp;NoBreak; is then defined as &amp;NoBreak;}&amp;NoBreak;. Multiplying an integer by its &amp;NoBreak;&amp;NoBreak;-adic absolute value cancels out the factors of &amp;NoBreak;&amp;NoBreak; in its factorization, leaving only the other primes. Just as the distance between two real numbers can be measured by the absolute value of their distance, the distance between two rational numbers can be measured by their &amp;NoBreak;&amp;NoBreak;-adic distance, the &amp;NoBreak;&amp;NoBreak;-adic absolute value of their difference. For this definition of distance, two numbers are close together (they have a small distance) when their difference is divisible by a high power of &amp;NoBreak;&amp;NoBreak;. In the same way that the real numbers can be formed from the rational numbers and their distances, by adding extra limiting values to form a complete field, the rational numbers with the &amp;NoBreak;&amp;NoBreak;-adic distance can be extended to a different complete field, the &amp;NoBreak;&amp;NoBreak;-adic numbers.\nThis picture of an order, absolute value, and complete field derived from them can be generalized to algebraic number fields and their valuations (certain mappings from the multiplicative group of the field to a totally ordered additive group, also called orders), absolute values (certain multiplicative mappings from the field to the real numbers, also called norms), and places (extensions to complete fields in which the given field is a dense set, also called completions). The extension from the rational numbers to the real numbers, for instance, is a place in which the distance between numbers is the usual absolute value of their difference. The corresponding mapping to an additive group would be the logarithm of the absolute value, although this does not meet all the requirements of a valuation. According to Ostrowski's theorem, up to a natural notion of equivalence, the real numbers and &amp;NoBreak;&amp;NoBreak;-adic numbers, with their orders and absolute values, are the only valuations, absolute values, and places on the rational numbers. The local\u2013global principle allows certain problems over the rational numbers to be solved by piecing together solutions from each of their places, again underlining the importance of primes to number theory.\nPrime elements of a ring.\nA commutative ring is an algebraic structure where addition, subtraction and multiplication are defined. The integers are a ring, and the prime numbers in the integers have been generalized to rings in two different ways, \"prime elements\" and \"irreducible elements\". An element &amp;NoBreak;&amp;NoBreak; of a ring &amp;NoBreak;&amp;NoBreak; is called prime if it is nonzero, has no multiplicative inverse (that is, it is not a unit), and satisfies the following requirement: whenever &amp;NoBreak;&amp;NoBreak; divides the product formula_62 of two elements of &amp;NoBreak;&amp;NoBreak;, it also divides at least one of &amp;NoBreak;&amp;NoBreak; or &amp;NoBreak;&amp;NoBreak;. An element is irreducible if it is neither a unit nor the product of two other non-unit elements. In the ring of integers, the prime and irreducible elements form the same set,\n formula_63\nIn an arbitrary ring, all prime elements are irreducible. The converse does not hold in general, but does hold for unique factorization domains.\nThe fundamental theorem of arithmetic continues to hold (by definition) in unique factorization domains. An example of such a domain is the Gaussian integers &amp;NoBreak;&amp;NoBreak;, the ring of complex numbers of the form formula_64 where &amp;NoBreak;&amp;NoBreak; denotes the imaginary unit and &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; are arbitrary integers. Its prime elements are known as Gaussian primes. Not every number that is prime among the integers remains prime in the Gaussian integers; for instance, the number 2 can be written as a product of the two Gaussian primes formula_65 and &amp;NoBreak;&amp;NoBreak;. Rational primes (the prime elements in the integers) congruent to 3 mod 4 are Gaussian primes, but rational primes congruent to 1 mod 4 are not. This is a consequence of Fermat's theorem on sums of two squares,\nwhich states that an odd prime &amp;NoBreak;&amp;NoBreak; is expressible as the sum of two squares, &amp;NoBreak;&amp;NoBreak;, and therefore factorable as &amp;NoBreak;&amp;NoBreak;, exactly when &amp;NoBreak;&amp;NoBreak; is 1 mod 4.\nPrime ideals.\nNot every ring is a unique factorization domain. For instance, in the ring of numbers formula_66 (for integers &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak;) the number formula_67 has two factorizations &amp;NoBreak;&amp;NoBreak;, where none of the four factors can be reduced any further, so it does not have a unique factorization. In order to extend unique factorization to a larger class of rings, the notion of a number can be replaced with that of an ideal, a subset of the elements of a ring that contains all sums of pairs of its elements, and all products of its elements with ring elements.\n\"Prime ideals\", which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals &amp;NoBreak;&amp;NoBreak;, &amp;NoBreak;&amp;NoBreak;, &amp;NoBreak;&amp;NoBreak;, &amp;NoBreak;&amp;NoBreak;, &amp;NoBreak;&amp;NoBreak;, &amp;NoBreak;&amp;NoBreak;, ... The fundamental theorem of arithmetic generalizes to the Lasker\u2013Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.\nThe spectrum of a ring is a geometric space whose points are the prime ideals of the ring. Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. These concepts can even assist with in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the existence of square roots modulo integer prime numbers. Early attempts to prove Fermat's Last Theorem led to Kummer's introduction of regular primes, integer prime numbers connected with the failure of unique factorization in the cyclotomic integers. The question of how many integer prime numbers factor into a product of multiple prime ideals in an algebraic number field is addressed by Chebotarev's density theorem, which (when applied to the cyclotomic integers) has Dirichlet's theorem on primes in arithmetic progressions as a special case.\nGroup theory.\nIn the theory of finite groups the Sylow theorems imply that, if a power of a prime number formula_68 divides the order of a group, then the group has a subgroup of order &amp;NoBreak;&amp;NoBreak;. By Lagrange's theorem, any group of prime order is a cyclic group,\nand by Burnside's theorem any group whose order is divisible by only two primes is solvable.\nComputational methods.\nFor a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of mathematics other than the use of prime numbered gear teeth to distribute wear evenly. In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance.\nThis vision of the purity of number theory was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public-key cryptography algorithms.\nThese applications have led to significant study of algorithms for computing with prime numbers, and in particular of primality testing, methods for determining whether a given number is prime. The most basic primality testing routine, trial division, is too slow to be useful for large numbers. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for numbers of special types. Most primality tests only tell whether their argument is prime or not. Routines that also provide a prime factor of composite arguments (or all of its prime factors) are called factorization algorithms. Prime numbers are also used in computing for checksums, hash tables, and pseudorandom number generators.\nTrial division.\nThe most basic method of checking the primality of a given integer &amp;NoBreak;&amp;NoBreak; is called \"trial division\". This method divides &amp;NoBreak;&amp;NoBreak; by each integer from 2 up to the square root of &amp;NoBreak;&amp;NoBreak;. Any such integer dividing &amp;NoBreak;&amp;NoBreak; evenly establishes &amp;NoBreak;&amp;NoBreak; as composite; otherwise it is prime. Integers larger than the square root do not need to be checked because, whenever &amp;NoBreak;&amp;NoBreak;, one of the two factors &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; is less than or equal to the square root of &amp;NoBreak;&amp;NoBreak;. Another optimization is to check only primes as factors in this range. For instance, to check whether 37 is prime, this method divides it by the primes in the range from 2 to &amp;NoBreak;}&amp;NoBreak;, which are 2, 3, and 5. Each division produces a nonzero remainder, so 37 is indeed prime.\nAlthough this method is simple to describe, it is impractical for testing the primality of large integers, because the number of tests that it performs grows exponentially as a function of the number of digits of these integers. However, trial division is still used, with a smaller limit than the square root on the divisor size, to quickly discover composite numbers with small factors, before using more complicated methods on the numbers that pass this filter.\nSieves.\nBefore computers, mathematical tables listing all of the primes or prime factorizations up to a given limit were commonly printed. The oldest known method for generating a list of primes is called the sieve of Eratosthenes. The animation shows an optimized variant of this method. Another more asymptotically efficient sieving method for the same problem is the sieve of Atkin. In advanced mathematics, sieve theory applies similar methods to other problems.\nPrimality testing versus primality proving.\nSome of the fastest modern tests for whether an arbitrary given number &amp;NoBreak;&amp;NoBreak; is prime are probabilistic (or Monte Carlo) algorithms, meaning that they have a small random chance of producing an incorrect answer. For instance the Solovay\u2013Strassen primality test on a given number &amp;NoBreak;&amp;NoBreak; chooses a number &amp;NoBreak;&amp;NoBreak; randomly from 2 through formula_69 and uses modular exponentiation to check whether formula_70 is divisible by &amp;NoBreak;&amp;NoBreak;. If so, it answers yes and otherwise it answers no. If &amp;NoBreak;&amp;NoBreak; really is prime, it will always answer yes, but if &amp;NoBreak;&amp;NoBreak; is composite then it answers yes with probability at most 1/2 and no with probability at least 1/2. If this test is repeated &amp;NoBreak;&amp;NoBreak; times on the same number, the probability that a composite number could pass the test every time is at most &amp;NoBreak;&amp;NoBreak;. Because this decreases exponentially with the number of tests, it provides high confidence (although not certainty) that a number that passes the repeated test is prime. On the other hand, if the test ever fails, then the number is certainly composite.\nA composite number that passes such a test is called a pseudoprime.\nIn contrast, some other algorithms guarantee that their answer will always be correct: primes will always be determined to be prime and composites will always be determined to be composite. For instance, this is true of trial division. The algorithms with guaranteed-correct output include both deterministic (non-random) algorithms, such as the AKS primality test,\nand randomized Las Vegas algorithms where the random choices made by the algorithm do not affect its final answer, such as some variations of elliptic curve primality proving.\nWhen the elliptic curve method concludes that a number is prime, it provides primality certificate that can be verified quickly.\nThe elliptic curve primality test is the fastest in practice of the guaranteed-correct primality tests, but its runtime analysis is based on heuristic arguments rather than rigorous proofs. The AKS primality test has mathematically proven time complexity, but is slower than elliptic curve primality proving in practice. These methods can be used to generate large random prime numbers, by generating and testing random numbers until finding one that is prime; when doing this, a faster probabilistic test can quickly eliminate most composite numbers before a guaranteed-correct algorithm is used to verify that the remaining numbers are prime.\nThe following table lists some of these tests. Their running time is given in terms of &amp;NoBreak;&amp;NoBreak;, the number to be tested and, for probabilistic algorithms, the number &amp;NoBreak;&amp;NoBreak; of tests performed. Moreover, formula_71 is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that each time bound should be multiplied by a constant factor to convert it from dimensionless units to units of time; this factor depends on implementation details such as the type of computer used to run the algorithm, but not on the input parameters &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak;.\nSpecial-purpose algorithms and the largest known prime.\nIn addition to the aforementioned tests that apply to any natural number, some numbers of a special form can be tested for primality more quickly. For example, the Lucas\u2013Lehmer primality test can determine whether a Mersenne number (one less than a power of two) is prime, deterministically, in the same time as a single iteration of the Miller\u2013Rabin test. This is why since 1992 (as of \u00a02024[ [update]]) the largest \"known\" prime has always been a Mersenne prime. It is conjectured that there are infinitely many Mersenne primes.\nThe following table gives the largest known primes of various types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits. The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively.\nInteger factorization.\nGiven a composite integer &amp;NoBreak;&amp;NoBreak;, the task of providing one (or all) prime factors is referred to as \"factorization\" of &amp;NoBreak;&amp;NoBreak;. It is significantly more difficult than primality testing, and although many factorization algorithms are known, they are slower than the fastest primality testing methods. Trial division and Pollard's rho algorithm can be used to find very small factors of &amp;NoBreak;&amp;NoBreak;, and elliptic curve factorization can be effective when &amp;NoBreak;&amp;NoBreak; has factors of moderate size. Methods suitable for arbitrary large numbers that do not depend on the size of its factors include the quadratic sieve and general number field sieve. As with primality testing, there are also factorization algorithms that require their input to have a special form, including the special number field sieve. As of \u00a02019[ [update]] the largest number known to have been factored by a general-purpose algorithm is RSA-240, which has 240 decimal digits (795 bits) and is the product of two large primes.\nShor's algorithm can factor any integer in a polynomial number of steps on a quantum computer. However, current technology can only run this algorithm for very small numbers. As of \u00a02012[ [update]], the largest number that has been factored by a quantum computer running Shor's algorithm is 21.\nOther computational applications.\nSeveral public-key cryptography algorithms, such as RSA and the Diffie\u2013Hellman key exchange, are based on large prime numbers (2048-bit primes are common). RSA relies on the assumption that it is much easier (that is, more efficient) to perform the multiplication of two (large) numbers &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; than to calculate &amp;NoBreak;&amp;NoBreak; and &amp;NoBreak;&amp;NoBreak; (assumed coprime) if only the product formula_62 is known. The Diffie\u2013Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation (computing &amp;NoBreak;}&amp;NoBreak;), while the reverse operation (the discrete logarithm) is thought to be a hard problem.\nPrime numbers are frequently used for hash tables. For instance the original method of Carter and Wegman for universal hashing was based on computing hash functions by choosing random linear functions modulo large prime numbers. Carter and Wegman generalized this method to &amp;NoBreak;&amp;NoBreak;-independent hashing by using higher-degree polynomials, again modulo large primes. As well as in the hash function, prime numbers are used for the hash table size in quadratic probing based hash tables to ensure that the probe sequence covers the whole table.\nSome checksum methods are based on the mathematics of prime numbers. For instance the checksums used in International Standard Book Numbers are defined by taking the rest of the number modulo 11, a prime number. Because 11 is prime this method can detect both single-digit errors and transpositions of adjacent digits. Another checksum method, Adler-32, uses arithmetic modulo 65521, the largest prime number less than &amp;NoBreak;}&amp;NoBreak;. Prime numbers are also used in pseudorandom number generators including linear congruential generators and the Mersenne Twister.\nOther applications.\nPrime numbers are of central importance to number theory but also have many applications to other areas within mathematics, including abstract algebra and elementary geometry. For example, it is possible to place prime numbers of points in a two-dimensional grid so that no three are in a line, or so that every triangle formed by three of the points has large area. Another example is Eisenstein's criterion, a test for whether a polynomial is irreducible based on divisibility of its coefficients by a prime number and its square.\nThe concept of a prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, \"prime\" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field of a given field is its smallest subfield that contains both 0 and 1. It is either the field of rational numbers or a finite field with a prime number of elements, whence the name. Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the connected sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots. The prime decomposition of 3-manifolds is another example of this type.\nBeyond mathematics and computing, prime numbers have potential connections to quantum mechanics, and have been used metaphorically in the arts and literature. They have also been used in evolutionary biology to explain the life cycles of cicadas.\nConstructible polygons and polygon partitions.\nFermat primes are primes of the form\n formula_73\nwith &amp;NoBreak;&amp;NoBreak; a nonnegative integer. They are named after Pierre de Fermat, who conjectured that all such numbers are prime. The first five of these numbers \u2013 3, 5, 17, 257, and 65,537 \u2013 are prime, but formula_74 is composite and so are all other Fermat numbers that have been verified as of 2017. A regular &amp;NoBreak;&amp;NoBreak;-gon is constructible using straightedge and compass if and only if the odd prime factors of &amp;NoBreak;&amp;NoBreak; (if any) are distinct Fermat primes. Likewise, a regular &amp;NoBreak;&amp;NoBreak;-gon may be constructed using straightedge, compass, and an angle trisector if and only if the prime factors of &amp;NoBreak;&amp;NoBreak; are any number of copies of 2 or 3 together with a (possibly empty) set of distinct Pierpont primes, primes of the form &amp;NoBreak;&amp;NoBreak;.\nIt is possible to partition any convex polygon into &amp;NoBreak;&amp;NoBreak; smaller convex polygons of equal area and equal perimeter, when &amp;NoBreak;&amp;NoBreak; is a power of a prime number, but this is not known for other values of &amp;NoBreak;&amp;NoBreak;.\nQuantum mechanics.\nBeginning with the work of Hugh Montgomery and Freeman Dyson in the 1970s, mathematicians and physicists have speculated that the zeros of the Riemann zeta function are connected to the energy levels of quantum systems. Prime numbers are also significant in quantum information science, thanks to mathematical structures such as mutually unbiased bases and symmetric informationally complete positive-operator-valued measures.\nBiology.\nThe evolutionary strategy used by cicadas of the genus \"Magicicada\" makes use of prime numbers. These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. Biologists theorize that these prime-numbered breeding cycle lengths have evolved in order to prevent predators from synchronizing with these cycles. In contrast, the multi-year periods between flowering in bamboo plants are hypothesized to be smooth numbers, having only small prime numbers in their factorizations.\nArts and literature.\nPrime numbers have influenced many artists and writers. The French composer Olivier Messiaen used prime numbers to create ametrical music through \"natural phenomena\". In works such as \"La Nativit\u00e9 du Seigneur\" (1935) and \"Quatre \u00e9tudes de rythme\" (1949\u20131950), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third \u00e9tude, \"Neumes rythmiques\". According to Messiaen this way of composing was \"inspired by the movements of nature, movements of free and unequal durations\".\nIn his science fiction novel \"Contact\", scientist Carl Sagan suggested that prime factorization could be used as a means of establishing two-dimensional image planes in communications with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975. In the novel \"The Curious Incident of the Dog in the Night-Time\" by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers as a way to convey the mental state of its main character, a mathematically gifted teen with Asperger syndrome. Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel \"The Solitude of Prime Numbers\", in which they are portrayed as \"outsiders\" among integers.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23669", "revid": "42778291", "url": "https://en.wikipedia.org/wiki?curid=23669", "title": "Piers Anthony", "text": "English-American writer (born 1934)\nPiers Anthony Dillingham Jacob (born August 6, 1934) is an American author in the science fiction and fantasy genres, publishing under the name Piers Anthony. He is best known for his set in the fictional realm of Xanth.\nMany of his books have appeared on \"The New York Times\" Best Seller list, and he claims one of his greatest achievements has been to publish a book beginning with every letter of the alphabet, from \"Anthonology\" to \"Zombie Lover\".\nEarly life.\nAnthony's parents, Alfred and Norma Jacob, were Quaker pacifists studying at Oxford University who interrupted their studies in 1936 to undertake relief work on behalf of the Quakers during the Spanish Civil War, establishing a food kitchen for children in Barcelona. Piers and his sister were left in England in the care of their maternal grandparents and a nanny. Alfred Jacob, although a British citizen, had been born in America near Philadelphia, and in 1940, after being forced out of Spain and with the situation in Britain deteriorating, the family sailed to the United States. In 1941 the family settled in a rustic \"back to the land\" utopian community near Winhall, Vermont, where a young Piers made the acquaintance of radical author Scott Nearing, a neighbor. Both parents resumed their academic studies, and Alfred eventually became a professor of Romance languages, teaching at a number of colleges in the Philadelphia area.\nPiers was moved around to a number of schools, eventually enrolling in Goddard College in Vermont where he graduated in 1956. On \"This American Life\" on July 27, 2012, Anthony revealed that his parents had divorced, he was bullied, and he had poor grades in school. Anthony referred to his high school as a \"very fancy private school\", and refuses to donate money to it. He recalls being part of \"the lower crust\", and that no one paid attention to, or cared about him. He said, \"I didn't like being a member of the underclass, of the peons like that\".\nMarriage and early career.\nAnthony met his future wife, Carol Marble, while both were attending college. They were married in 1956, the same year he graduated from Goddard College, and he subsequently worked as a handyman. In 1957, Anthony decided to join the United States Army, as his wife was pregnant and they needed both medical coverage and a steady source of income. During his two-year enlistment, he became a naturalized U.S. citizen in 1958 and was editor and cartoonist for his battalion's newspaper.\nAfter completing military service, he briefly taught at Admiral Farragut Academy in St. Petersburg, Florida before deciding to try to become a full-time writer.\nAnthony and his wife made a deal: if he could sell a piece of writing within one year, she would continue to work to support him. But if he could not sell anything in that year, then he would forever give up his dream of being a writer. At the end of the year, he managed to get a short story published. He credits his wife as the person who made his writing career possible, and he advises aspiring writers that they need to have a source of income other than their writing in order to get through the early years of a writing career.\nWriting.\nOn multiple occasions Anthony has moved from one publisher to another (taking a popular series with him) when he says he felt the editors were unduly tampering with his work. He has sued publishers for accounting malfeasance and won judgments in his favor. Anthony maintains an Internet Publishers Survey in the interest of helping aspiring writers. For this service, he won the 2003 \"Friend of EPIC\" award for service to the electronic publishing community. His website won the \"Special Recognition for Service to Writers\" award from Preditors and Editors, an author's guide to publishers and writing services.\nHis popular novel series Xanth has been optioned for movies. It inspired the MS-DOS video game \"Companions of Xanth\", by Legend Entertainment. The same series also spawned the board game \"Xanth\" by Mayfair Games.\nAnthony's novels usually end with a chapter-long Author's Note, in which he talks about himself, his life, and his experiences as they related to the process of writing the novel. He often discusses correspondence with readers and any real-world issues that influenced the novel.\nSince about 2000, Anthony has written his novels in a Linux environment.\nAnthony's \"Xanth\" series was ranked No. 99 in a 2011 NPR readers' poll of best science fiction and fantasy books.\nIn other media.\nAct One of episode 470 of the radio program \"This American Life\" is an account of boyhood obsessions with Piers Anthony. The act is written and narrated by writer Logan Hill who, as a 12-year-old, was consumed with reading Anthony's novels. For a decade he felt he must have been Anthony's number one fan, until, when he was 22, he met \"Andy\" at a wedding and discovered their mutual interest in the writer. Andy is interviewed for the story and explains that, as a teenager, he had used escapist novels in order to cope with his alienating school and home life in Buffalo, New York. In 1987, at age 15, he decided to run away to Florida in order to try to live with Piers Anthony. The story includes Anthony's reflections on these events.\n\"But What of Earth?\".\nEarly in Anthony's literary career, there was a dispute surrounding the original publication (1976) of \"But What of Earth?\". Editor Roger Elwood commissioned the novel for his nascent science-fiction line Laser Books. According to Anthony, he completed \"But What of Earth?\", and Elwood accepted and purchased it. Elwood then told Anthony that he wished to make several minor changes, and in order not to waste Anthony's time, he had hired copy editor (and author) Robert Coulson to retype the manuscript with the changes. Anthony described Coulson as a friend and was initially open to his contribution.\nHowever, Elwood told Coulson he was to be a full collaborator, free to make revisions to Anthony's text in line with suggestions made by other copy editors. Elwood promised Coulson a 50\u201350 split with Anthony on all future royalties. According to Anthony, the published novel was very different from his version, with changes to characters and dialog, and with scenes added and removed. Anthony felt the changes worsened the novel. Laser's ultimate publication of \"But What of Earth?\" listed Anthony and Coulson together as collaborators. Publication rights were reverted to Anthony under threat of legal action. In 1989, Anthony (re)published his original \"But What of Earth?\" in an annotated edition through Tor Books. This edition contains an introduction and conclusion setting out the story of the novel's permutations and roughly 60 pages of notes by Anthony giving examples of changes to plot and characters, and describing some of the comments made by copy editors on his manuscript.\nCriticism.\nSome activists have described Anthony's portrayal of female characters as stereotypical and misogynistic, particularly in the early parts of the Xanth series, and have taken issue with themes of underage sexuality and eroticism within Anthony's work. Anthony has stated in interviews that these critiques do not reflect his work accurately and has claimed he receives more fan mail from women than men.\nPersonal life.\nHe and his first wife, Carol Ann Marble Jacob, had two daughters, Penelope \"Penny\" Carolyn and Cheryl. Penny had one child, and died in 2009, due to complications from skin cancer. Carol Ann died at home on October 3, 2019, due to what is believed to have been heart related complications due to a 15-year-long battle with chronic inflammatory demyelinating polyneuropathy (CIDP).\nOn April 22, 2020, he married MaryLee Boyance. Anthony lived on his tree farm in Florida until March 2023, at which time he sold his farm and moved to California.\nAnthony is a vegetarian.\nReligious beliefs.\nRegarding his religious beliefs, Anthony wrote in the October 2004 entry of his personal website, \"I'm agnostic, which means I regard the case as unproven, but I'm much closer to the atheist position than to the theist one.\" In 2017 he stated, \"I am more certain about God and the Afterlife: they don't exist.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23670", "revid": "12923157", "url": "https://en.wikipedia.org/wiki?curid=23670", "title": "Perfect number", "text": "Number equal to the sum of its proper divisors\nIn number theory, a perfect number is a positive integer that is equal to the sum of its positive proper divisors, that is, divisors excluding the number itself. For instance, 6 has proper divisors 1, 2, and 3, and 1 + 2 + 3 = 6, so 6 is a perfect number. The next perfect number is 28, because 1 + 2 + 4 + 7 + 14 = 28.\nThe first seven perfect numbers are 6, 28, 496, 8128, 33550336, 8589869056, and 137438691328.\nThe sum of proper divisors of a number is called its aliquot sum, so a perfect number is one that is equal to its aliquot sum. Equivalently, a perfect number is a number that is half the sum of all of its positive divisors; in symbols, formula_1 where formula_2 is the sum-of-divisors function.\nThis definition is ancient, appearing as early as Euclid's \"Elements\" (VII.22) where it is called (\"perfect\", \"ideal\", or \"complete number\"). Euclid also proved a formation rule (IX.36) whereby formula_3 is an even perfect number whenever formula_4 is a prime of the form formula_5 for positive integer formula_6\u2014what is now called a Mersenne prime. Two millennia later, Leonhard Euler proved that all even perfect numbers are of this form. This is known as the Euclid\u2013Euler theorem.\nIt is not known whether there are any odd perfect numbers, nor whether infinitely many perfect numbers exist.\nHistory.\nIn about 300\u00a0BC Euclid showed that if 2\"p\"\u00a0\u2212\u00a01 is prime then 2\"p\"\u22121(2\"p\"\u00a0\u2212\u00a01) is perfect.\nThe first four perfect numbers were the only ones known to early Greek mathematics, and the mathematician Nicomachus noted 8128 as early as around AD\u00a0100. In modern language, Nicomachus states without proof that every perfect number is of the form formula_7 where formula_8 is prime. He seems to be unaware that n itself has to be prime. He also says (wrongly) that the perfect numbers end in 6 or 8 alternately. (The first 5 perfect numbers end with digits 6, 8, 6, 8, 6; but the sixth also ends in 6.) Philo of Alexandria in his first-century book \"On the creation\" mentions perfect numbers, claiming that the world was created in 6 days and the moon orbits in 28 days because 6 and 28 are perfect. Philo is followed by Origen, and by Didymus the Blind, who adds the observation that there are only four perfect numbers that are less than 10,000. (Commentary on Genesis 1. 14\u201319). Augustine of Hippo defines perfect numbers in \"The City of God\" (Book XI, Chapter 30) in the early 5th century AD, repeating the claim that God created the world in 6 days because 6 is the smallest perfect number. The Egyptian mathematician Ismail ibn Fall\u016bs (1194\u20131252) mentioned the next three perfect numbers (33,550,336; 8,589,869,056; and 137,438,691,328) and listed a few more which are now known to be incorrect. The first known European mention of the fifth perfect number is a manuscript written between 1456 and 1461 by an unknown mathematician. In 1588, the Italian mathematician Pietro Cataldi identified the sixth (8,589,869,056) and the seventh (137,438,691,328) perfect numbers, and also proved that every perfect number obtained from Euclid's rule ends with a 6 or an 8.\nEven perfect numbers.\n&lt;templatestyles src=\"Unsolved/styles.css\" /&gt;\nUnsolved problem in mathematics\nAre there infinitely many perfect numbers?\nMore unsolved problems in mathematics\nEuclid proved that formula_9 is an even perfect number whenever formula_5 is prime (\"Elements\", Prop. IX.36).\nFor example, the first four perfect numbers are generated by the formula formula_11 with p a prime number, as follows:\nformula_12\nPrime numbers of the form formula_5 are known as Mersenne primes, after the seventeenth-century monk Marin Mersenne, who studied number theory and perfect numbers. For formula_5 to be prime, it is necessary that p itself be prime. However, not all numbers of the form formula_5 with a prime p are prime; for example, 211 \u2212 1 = 2047 = 23 \u00d7 89 is not a prime number. In fact, Mersenne primes are very rare: of the approximately 4 million primes p up to 68,874,199, formula_5 is prime for only 48 of them.\nWhile Nicomachus had stated (without proof) that all perfect numbers were of the form formula_7 where formula_8 is prime (though he stated this somewhat differently), Ibn al-Haytham (Alhazen) circa AD 1000 was unwilling to go that far, declaring instead (also without proof) that the formula yielded only every even perfect number. It was not until the 18th century that Leonhard Euler proved that the formula formula_9 indeed yields all the even perfect numbers. Thus, there is a one-to-one correspondence between even perfect numbers and Mersenne primes; each Mersenne prime generates one even perfect number, and vice versa. This result is often referred to as the Euclid\u2013Euler theorem.\nAn exhaustive search by the GIMPS distributed computing project has shown that the first 50 even perfect numbers are formula_9 for\n p = 2, 3, 5, 7, 13, 17, 19, 31, 61, 89, 107, 127, 521, 607, 1279, 2203, 2281, 3217, 4253, 4423, 9689, 9941, 11213, 19937, 21701, 23209, 44497, 86243, 110503, 132049, 216091, 756839, 859433, 1257787, 1398269, 2976221, 3021377, 6972593, 13466917, 20996011, 24036583, 25964951, 30402457, 32582657, 37156667, 42643801, 43112609, 57885161, 74207281, 77232917 OEIS:\u00a0.\nTwo higher perfect numbers have also been discovered, namely those for which p = 82589933 and 136279841. Although it is still possible there may be others within this range, initial but exhaustive tests by GIMPS have revealed no other perfect numbers for p below 138277717. As of \u00a02024[ [update]], 52 Mersenne primes are known, and therefore 52 even perfect numbers (the largest of which is 2136279840 \u00d7 (2136279841 \u2212 1) with 82,048,640 digits). It is not known whether there are infinitely many perfect numbers, nor whether there are infinitely many Mersenne primes.\nAs well as having the form formula_9, each even perfect number is the formula_22-th triangular number (and hence equal to the sum of the integers from 1 to formula_5) and the formula_24-th hexagonal number. Furthermore, each even perfect number except for 6 is the formula_25-th centered nonagonal number and is equal to the sum of the first formula_26 odd cubes (odd cubes up to the cube of formula_27):\nformula_28\nEven perfect numbers (except 6) are of the form\nformula_29\nwith each resulting triangular number T7 \n 28, T31 \n 496, T127 \n 8128 (after subtracting 1 from the perfect number and dividing the result by 9) ending in 3 or 5, the sequence starting with T2 \n 3, T10 \n 55, T42 = 903, T2730 = 3727815, ... It follows that by adding the digits of any even perfect number (except 6), then adding the digits of the resulting number, and repeating this process until a single digit (called the digital root) is obtained, always produces the number 1. For example, the digital root of 8128 is 1, because 8 + 1 + 2 + 8 = 19, 1 + 9 = 10, and 1 + 0 = 1. This works with all perfect numbers formula_9 with odd prime p and, in fact, with all numbers of the form formula_31 for odd integer (not necessarily prime) m.\nOwing to their form, formula_11 every even perfect number is represented in binary form as p ones followed by \"p\" \u2212 1 zeros; for example:\nformula_33\nThus every even perfect number is a pernicious number.\nEvery even perfect number is also a practical number (cf. Related concepts).\nOdd perfect numbers.\n&lt;templatestyles src=\"Unsolved/styles.css\" /&gt;\nUnsolved problem in mathematics\nAre there any odd perfect numbers?\nMore unsolved problems in mathematics\nIt is unknown whether any odd perfect numbers exist, though various results have been obtained. In 1496, Jacques Lef\u00e8vre stated that Euclid's rule gives all perfect numbers, thus implying that no odd perfect number exists, but Euler himself stated: \"Whether\u00a0... there are any odd perfect numbers is a most difficult question\". More recently, Carl Pomerance has presented a heuristic argument suggesting that indeed no odd perfect number should exist. All perfect numbers are also harmonic divisor numbers, and it has been conjectured as well that there are no odd harmonic divisor numbers other than 1.\nAny odd perfect number \"N\" must satisfy the following conditions:\nformula_37\nwhere:\n* \"q\",\u00a0\"p\"1,\u00a0...,\u00a0\"p\"\"k\" are distinct odd primes (Euler).\n* \"q\" \u2261\u00a0\u03b1 \u2261\u00a01 (mod 4) (Euler).\n* The smallest prime factor of \"N\" is at most formula_38\n* formula_39\n* formula_40.\n* formula_41.\n* formula_42.\nFurthermore, several minor results are known about the exponents\n\"e\"1,\u00a0...,\u00a0\"e\"\"k\".\nIn 1888, Sylvester stated:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...\u00a0a prolonged meditation on the subject has satisfied me that the existence of any one such [odd perfect number]\u2014its escape, so to say, from the complex web of conditions which hem it in on all sides\u2014would be little short of a miracle.\nOn the other hand, several odd integers come close to being perfect. Ren\u00e9 Descartes observed that the number \"D\" \n 32 \u22c5 72 \u22c5 112 \u22c5 132 \u22c5 22021 \n (3\u22c51001)2\u2009\u22c5\u2009(22\u22c51001 \u2212 1) \n 198585576189 would be an odd perfect number if only 22021 (\n 192 \u22c5 61) were a prime number. The odd numbers with this property (they would be perfect if one of their composite factors were prime) are the Descartes numbers. Many of the properties proven about odd perfect numbers also apply to Descartes numbers, and Pace Nielsen has suggested that sufficient study of these numbers may lead to a proof that no odd perfect numbers exist.\nMinor results.\nAll even perfect numbers have a very precise form; odd perfect numbers either do not exist or are rare. There are a number of results on perfect numbers that are actually quite easy to prove but nevertheless superficially impressive; some of them also come under Richard Guy's strong law of small numbers:\nRelated concepts.\nThe sum of proper divisors gives various other kinds of numbers. Numbers where the sum is less than the number itself are called deficient, and where it is greater than the number, abundant. These terms, together with \"perfect\" itself, come from Greek numerology. A pair of numbers which are the sum of each other's proper divisors are called amicable, and larger cycles of numbers are called sociable. A positive integer such that every smaller positive integer is a sum of distinct divisors of it is a practical number.\nBy definition, a perfect number is a fixed point of the restricted divisor function \"s\"(\"n\") = \"\u03c3\"(\"n\") \u2212 \"n\", and the aliquot sequence associated with a perfect number is a constant sequence. All perfect numbers are also formula_52-perfect numbers, or Granville numbers.\nA semiperfect number is a natural number that is equal to the sum of all or some of its proper divisors. A semiperfect number that is equal to the sum of all its proper divisors is a perfect number. Most abundant numbers are also semiperfect; abundant numbers which are not semiperfect are called weird numbers.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23672", "revid": "24903194", "url": "https://en.wikipedia.org/wiki?curid=23672", "title": "Parthenon", "text": "Temple on the Athenian Acropolis, Greece\nThe Parthenon (; ; ) is a former temple on the Athenian Acropolis, Greece, that was dedicated to the goddess Athena. Its decorative sculptures are considered some of the high points of classical Greek art, and the Parthenon is considered an enduring symbol of ancient Greece, Western civilization, and democracy. \nThe Parthenon was built in the 5th century BC in thanksgiving for the Greek victory over the Persian invaders during the Greco-Persian Wars. Like most Greek temples, the Parthenon also served as the city treasury. Construction started in 447 BC when the Delian League was at the peak of its power. It was completed in 438 BC; work on the artwork and decorations continued until 432 BC. For a time, it served as the treasury of the Delian League, which later became the Athenian Empire.\nIn the final decade of the 6th century AD, the Parthenon was converted into a Christian church dedicated to the Virgin Mary. After the Ottoman conquest in the mid-15th century, it became a mosque. In the Morean War, a Venetian bomb landed on the Parthenon, which the Ottomans had used as a munitions dump, during the 1687 siege of the Acropolis. The resulting explosion severely damaged the Parthenon. From 1800 to 1803, the 7th Earl of Elgin controversially removed many of the surviving sculptures and subsequently shipped them to England where they are now known as the \"Elgin Marbles\" or Parthenon marbles. Since 1975, numerous large-scale restoration projects have been undertaken to preserve remaining artefacts and ensure its structural integrity.\nEtymology.\nThe word \"Parthenon\" comes from the Greek () 'maiden, girl' as well as 'virgin, unmarried woman'. The Liddell\u2013Scott\u2013Jones \"Greek\u2013English Lexicon\" states that it may have referred to the \"unmarried women's apartments\" in a house, but that in the Parthenon it seems to have been used for a particular room of the temple. There is some debate as to which room that was. The lexicon states that this room was the western cella of the Parthenon. This has also been suggested by J.B. Bury. One theory is that the Parthenon was the room where the \"arrephoroi\", a group of four young girls chosen to serve Athena each year, wove a peplos that was presented to Athena during Panathenaic Festivals. Christopher Pelling asserts that the name \"Parthenon\" means the \"temple of the virgin goddess\", referring to the cult of Athena Parthenos that was associated with the temple. It has also been suggested that the name of the temple alludes to the maidens (), whose supreme sacrifice guaranteed the safety of the city. In that case, the room originally known as the Parthenon could have been a part of the temple known today as the Erechtheion.\nIn 5th-century BC accounts of the building, the structure is simply called (; \"the temple\"). Douglas Frame writes that the name \"Parthenon\" was a nickname related to the statue of Athena Parthenos, and only appeared a century after construction. He contends that \"Athena's temple was never officially called the Parthenon and she herself most likely never had the cult title \"parth\u00e9nos\"\". The ancient architects Iktinos and Callicrates appear to have called the building (; \"the hundred footer\") in their lost treatise on Athenian architecture. Harpocration wrote that some people used to call the Parthenon the \"Hekatompedos\", not due to its size but because of its beauty and fine proportions. The first instance in which Parthenon definitely refers to the entire building comes from the fourth century BC orator Demosthenes. In the 4th century BC and later, the building was referred to as the ' or the as well as the \"Parthenon.\" Plutarch referred to the building during the first century AD as the '.\nA 2020 study by Janric van Rookhuijzen supports the idea that the building known today as the Parthenon was originally called the Hekatompedon. Based on literary and historical research, he proposes that \"the treasury called the Parthenon should be recognized as the west part of the building now conventionally known as the Erechtheion\".\nBecause the Parthenon was dedicated to the Greek goddess Athena, it was sometimes called the Temple of Minerva, the Roman name for Athena, particularly during the 19th century.\n was also applied to the Virgin Mary (\"Parth\u00e9nos Maria\") when the Parthenon was converted to a Christian church dedicated to the Virgin Mary in the final decade of the 6th century.\nFunction.\nAlthough the Parthenon is architecturally a temple and is usually called so, some scholars have argued that it is not really a temple in the conventional sense of the word. A small shrine has been excavated within the building, on the site of an older sanctuary probably dedicated to Athena as a way to get closer to the goddess, but the Parthenon apparently never hosted the official cult of Athena Polias, patron of Athens. The cult image of Athena Polias, which was bathed in the sea and to which was presented the \"peplos\", was an olive-wood \"xoanon\", located in another temple on the northern side of the Acropolis, more closely associated with the Great Altar of Athena. The High Priestess of Athena Polias supervised the city cult of Athena based in the Acropolis, and was the chief of the lesser officials, such as the plyntrides, arrephoroi and kanephoroi.\nThe colossal statue of Athena by Phidias was not specifically related to any cult attested by ancient authors and is not known to have inspired any religious fervour. Preserved ancient sources do not associate it with any priestess, altar or cult name.\nAccording to Thucydides, during the Peloponnesian War when Sparta's forces were first preparing to invade Attica, Pericles, in an address to the Athenian people, said that the statue could be used as a gold reserve if that was necessary to preserve Athens, stressing that it \"contained forty talents of pure gold and it was all removable\", but adding that the gold would afterward have to be restored. The Athenian statesman thus implies that the metal, obtained from contemporary coinage, could be used again if absolutely necessary without any impiety. According to Aristotle, the building also contained golden figures that he described as \"Victories\". The classicist Harris Rackham noted that eight of those figures were melted down for coinage during the Peloponnesian War. Other Greek writers have claimed that treasures such as Persian swords were also stored inside the temple. Some scholars, therefore, argue that the Parthenon should be viewed as a grand setting for a monumental votive statue rather than as a cult site.\nArchaeologist Joan Breton Connelly has argued for the coherency of the Parthenon's sculptural programme in presenting a succession of genealogical narratives that track Athenian identity through the ages: from the birth of Athena, through cosmic and epic battles, to the final great event of the Athenian Bronze Age, the war of Erechtheus and Eumolpos. She argues a pedagogical function for the Parthenon's sculptured decoration, one that establishes and perpetuates Athenian foundation myth, memory, values and identity. While some classicists, including Mary Beard, Peter Green, and Garry Wills have doubted or rejected Connelly's thesis, an increasing number of historians, archaeologists, and classical scholars support her work. They include: J.J. Pollitt, Brunilde Ridgway, Nigel Spivey, Caroline Alexander, and A. E. Stallings.\nOlder Parthenon.\nThe first endeavour to build a sanctuary for on the site of the present Parthenon was begun shortly after the Battle of Marathon (c.\u2009490\u2013488\u00a0BC) upon a solid limestone foundation that extended and levelled the southern part of the Acropolis summit. This building replaced a Hekatompedon temple (\"hundred-footer\") and would have stood beside the archaic temple dedicated to \"Athena Polias\" (\"of the city\"). The Older or Pre-Parthenon, as it is frequently called, was still under construction when the Persians sacked the city in 480\u00a0BC and razed the Acropolis.\nThe existence of both the proto-Parthenon and its destruction were known from Herodotus, and the drums of its columns were visibly built into the curtain wall north of the Erechtheion. Further physical evidence of this structure was revealed with the excavations of Panagiotis Kavvadias of 1885\u20131890. The findings of this dig allowed Wilhelm D\u00f6rpfeld, then director of the German Archaeological Institute, to assert that there existed a distinct substructure to the original Parthenon, called Parthenon I by D\u00f6rpfeld, not immediately below the present edifice as previously assumed. D\u00f6rpfeld's observation was that the three steps of the first Parthenon consisted of two steps of Poros limestone, the same as the foundations, and a top step of Karrha limestone that was covered by the lowest step of the Periclean Parthenon. This platform was smaller and slightly to the north of the final Parthenon, indicating that it was built for a different building, now completely covered over. This picture was somewhat complicated by the publication of the final report on the 1885\u20131890 excavations, indicating that the substructure was contemporary with the Kimonian walls, and implying a later date for the first temple.\nIf the original Parthenon was indeed destroyed in 480, it invites the question of why the site was left as a ruin for thirty-three years. One argument involves the oath sworn by the Greek allies before the Battle of Plataea in 479\u00a0BC declaring that the sanctuaries destroyed by the Persians would not be rebuilt, an oath from which the Athenians were only absolved with the Peace of Callias in 450. The cost of reconstructing Athens after the Persian sack is at least as likely a cause. The excavations of Bert Hodge Hill led him to propose the existence of a second Parthenon, begun in the period of Kimon after 468. Hill claimed that the Karrha limestone step D\u00f6rpfeld thought was the highest of Parthenon I was the lowest of the three steps of Parthenon II, whose stylobate dimensions Hill calculated at .\nOne difficulty in dating the proto-Parthenon is that at the time of the 1885 excavation, the archaeological method of seriation was not fully developed; the careless digging and refilling of the site led to a loss of much valuable information. An attempt to make sense of the potsherds found on the Acropolis came with the two-volume study by Graef and Langlotz published in 1925\u20131933. This inspired American archaeologist William Bell Dinsmoor to give limiting dates for the temple platform and the five walls hidden under the re-terracing of the Acropolis. Dinsmoor concluded that the latest possible date for Parthenon I was no earlier than 495\u00a0BC, contradicting the early date given by D\u00f6rpfeld. He denied that there were two proto-Parthenons, and held that the only pre-Periclean temple was what D\u00f6rpfeld referred to as Parthenon II. Dinsmoor and D\u00f6rpfeld exchanged views in the \"American Journal of Archaeology\" in 1935.\nPresent building.\nIn the mid-5th century BC, when the Athenian Acropolis became the seat of the Delian League, Pericles initiated the building project that lasted the entire second half of the century. The most important buildings visible on the Acropolis today \u2013 the Parthenon, the Propylaia, the Erechtheion and the temple of Athena Nike \u2013 were erected during this period. The Parthenon was built under the general supervision of Phidias, who also had charge of the sculptural decoration. The architects Ictinos and Callicrates began their work in 447, and the building was substantially completed by 432. Work on the decorations continued until at least 431.\nThe Parthenon was built primarily by men who knew how to work marble. These quarrymen had exceptional skills and were able to cut the blocks of marble to very specific measurements. The quarrymen also knew how to avoid the faults, which were numerous in the Pentelic marble. If the marble blocks were not up to standard, the architects would reject them. The marble was worked with iron tools \u2013 picks, points, punches, chisels, and drills. The quarrymen would hold their tools against the marble block and firmly tap the surface of the rock.\nA big project like the Parthenon attracted stonemasons from far and wide who travelled to Athens to assist in the project. Slaves and foreigners worked together with the Athenian citizens in the building of the Parthenon, doing the same jobs for the same pay. Temple building was a specialized craft, and there were not many men in Greece qualified to build temples like the Parthenon, so these men would travel and work where they were needed.\nOther craftsmen were necessary for the building of the Parthenon, specifically carpenters and metalworkers. Unskilled labourers also had key roles in the building of the Parthenon. They loaded and unloaded the marble blocks and moved the blocks from place to place. In order to complete a project like the Parthenon, many different labourers were needed.\nArchitecture.\nSize and exterior.\nThe Parthenon is an octastyle peripteral Doric temple with an Ionic hexastyle amphiprostyle two-chambered cella. It was built on the euthynteria and krepis of its precursor building, the Older Parthenon. In common with other Greek temples, the Parthenon is built using the post and lintel construction, surrounded by columns ('peripteral') carrying an entablature. The gable-end of the Parthenon features eight columns instead of the traditional six found in typical Doric temples. Although octastyle temples were not entirely unknown, their presence in mainland Doric architecture, along with the wider inner masonry structure, the \"cella\", makes the Parthenon unique in its design. There are seventeen columns on the sides. A ratio of 4:9 proportion is found in the elevation and the relationship of the columns to their spacing (the interaxial). There is a double row of columns at both the front and rear. The \"cella\" is divided into two compartments. The \"opisthodomos\" (the back room of the cella) contained the monetary contributions of the Delian League. The hexastyle pronaos replaced the typical distyle in antis porch to the naos. At either end of the building, the gable is finished with a triangular pediment originally occupied by sculpted figures.\nThe choice to design the Parthenon as an octostyle temple likely stemmed from the challenge of scale: creating a larger naos required a proportionally larger overall structure. Maintaining the traditional hexastyle layout would have necessitated wider spacing between columns, which could have compromised structural stability. Moreover, expanding the temple while adhering to the established Doric proportions would have disrupted their harmony. Consequently, the architects made a series of design decisions that ultimately broke with mainland Doric conventions to achieve both the desired size and aesthetic integrity.\nInterior.\nThe Parthenon's interior displays several unusual and innovative features. The northern peristyle contained an archaic naiskos and altar, preserved to maintain religious continuity on the site. The Parthenon's porticos are unusually shallow, and the naos includes a second step. The rear room (opisthodomos) was wider, a Cycladic trait, and held four columns, likely Ionic or Corinthian. Large doors connected the rooms, and the only pronaos had tall windows (about 3 m high and 2.5 m wide), a rare feature in Greek architecture. The north window also served as a landing for a staircase within the thick wall leading to the attic. Other irregularities include varying abacus lengths, deliberate interaxial differences of up to 4.8 cm, and uneven architrave blocks are misaligned and differ by as much as 18 cm. Scholars interpret these changes variously\u2014as adjustments for corner contraction (Dinsmoor), evidence of a mid-construction design change (Wesenberg), or signs of improvisation when an Ionic frieze replaced an intended Doric one (Korres).\nOptical refinements.\nThe close measurement of the Parthenon in the nineteenth century revealed that the temple deviated from strict rectilinearity through several optical refinements. First, the stylobate is curved, bulging upward at the center\u2014by 10.3 cm over 70 m (a 1/700 ratio)\u2014with a corresponding curvature in the entablature, visible as a slight ridge on the capitals. Second, the columns exhibit entasis, i.e., swelling that reduces toward the top, a practice in use by mid-6th century but in Parthenon the effect is subtler with a ratio of 1/550 to 1/600. Third, both the columns and naos walls incline slightly inward. Fourth, the corner columns are slightly displaced.\nScholars have proposed several explanations for the curvatures found in Parthenon. The primary explanation is based on the Optical Correction or Irradiance Theory, proposed in \"An Investigation of the Principles of Athenian Architecture\" by Francis Penrose in 1851. The theory asserts that convex adjustments were made to counteract the concave appearance silhouetted objects can have to the human eye. Others attribute the refinements to structural, such as drainage, or aesthetic considerations rather than perceptual ones. In 1978, John Pennethorne's The Geometry and Optics of Ancient Architecture argued that curvilinear nature of the Parthenon was a deteriberate design choice, supporting Penrose's conclusions and corresponding with Vitruvius' account of the Scamilli impares. Karl B\u00f6tticher believed the deviations resulted from structural settlement, while William Henry Goodyear viewed them as symbolic and aesthetic.\nAnother refinement relates to the Doric order's angle, that is the challenge of spacing columns, metopes, and triglyphs so the frieze ends correctly at the corners, known as contraction problem. At the Parthenon, the architects to resolve this problem varied the length of the metopes, between 1.175 and 1.37 m, and 'overcontracting' the corner incolumnations.\nUnit of measurement.\nThere was no standardized unit of measurement in ancient Greek metrology, as each region\u2014or even individual building site\u2014often employed its own foot (\u03c0\u03bf\u03cd\u03c2). Scholars have proposed several possible units for the Parthenon: the Attic (or Ionic) foot at 294.3 mm, the Common foot at 306.5 mm, and the Doric foot at 327 mm. However, applying any of these to the temple's architectural dimensions, such as the stylobate's length and width or the column height, fails to yield consistent integer values. Attempts to link the design instead to Vitruvius's modular system, based on half the lower column diameter (the width of a triglyph), have been similarly inconclusive. In the Parthenon, the triglyph measures about 858.3 mm, though actual widths vary from roughly 0.84 m to 0.87 m.\nMore recent research by Ernst Berger identifies 858 mm as a recurring unit underlying the building's main dimensions. Dividing this by 2.5 produces a 'Parthenon foot' of 343.04 mm, as proposed by Anne Bulckens. This measure preserves the Parthenon's characteristic 9:4 proportion while also revealing additional ratios that Bulckens suggests may relate to the pentatonic scale.\nProportion.\nNo ancient Greek text on architecture has survived to the present day. The methods and working practices of Greek architects are unknown to us, so attempts to reconstruct the system of proportion used on the Parthenon as a means of uncovering the motivations of its architects have gone hand-in-hand with a desire to explain its purported \u2018perfection\u2019. These systems fall into two broad categories: arithmetic or geometric ratios and systems of modularity.\nPerhaps one of the most common beliefs about the system of proportion used on the Parthenon is that of the Golden Ratio Theory. That the Golden Section, or phi, the ratio of the sum of two values and their larger value, determined the construction of the temple was first articulated by Adolf Zeising in his \"Neue Lehre von den Proportionen des menschlichen K\u00f6rpers\" (1854). Zeising made specific reference to the plan of the Parthenon when he and subsequent scholars made expansive claims that phi was ubiquitous in nature and art and fundamental to human perception of beauty. More recent research has questioned whether ancient architects either had knowledge of phi or made use of it, has pointed out that application of the ratio to the Parthenon was somewhat arbitrary in its construction, and that the basis of the claim was often a geometric figure superimposed on a photograph rather than from measured drawings.\nWhile Zeising\u2019s hypothesis remains unsubstantiated, an alternative observation that the ratio of the length and width of several features of the Parthenon gives simple, commensurable whole number ratios, namely 9:4, has garnered some support. First published in 1863 by William Watkiss-Lloyd, this relationship was detected on the stylobate, the diameter of column to intercolumniation, and the height of the facade including the cornice to the width. That this ratio falls out into integer values, avoids irrationals and is seen on other Greek buildings has led to the popular temptation to see this method of proportioning as the one that motivated the original architects. Lloyd\u2019s approach has, like other numerical ratio-based approaches, been criticised for its arbitrariness, its susceptibility to selective measurement confirmation bias, and the absence of explicit ancient sources validating it.\nOne attempt to describe the Parthenon as a geometric system aligned with Greek mathematical thought was Jay Hambidge\u2019s Dynamic Symmetry theory. Published in the early 20th century it sought to explain harmonious proportions through so-called \u201croot rectangles\u201d and their relationships, inspired by patterns found in nature such as phyllotaxis. Starting from root rectangles whose sides are irrational values, he goes on to construct reciprocal rectangles often in the form of the golden spiral. These recursively generated rectangles, Hambidge claimed, generated a dynamic growth which could be mapped onto the Parthenon, and this demonstrated that Greek design was inherently dynamic and natural rather than static. The theory was influential outside academia on figures such as George Bellows and Le Corbusier, and was responsible for a revival of interest in the Golden Section; it was nevertheless also criticised for its arbitrariness and lack of historical evidence.\nMost recent research has endeavoured to incorporate the idea that the Parthenon\u2019s design reflects Pythagorean musical ratios, such as 3:2 (the perfect fifth) and 4:9. According to this interpretation, the Parthenon\u2019s dimensions (length, width, and height) relate as musical intervals, embedding mathematical harmony into architecture. Anne Bulckens begins with the discovery of a \u2018theoretical triglyph\u2019 width of 857.6 mm, which is the basis for a modular system from which smaller units, \u201cdactyls\u201d, can be derived, and on which basis she observes the presence of 3-4-5 right triangles in the structure. Drawing on the work of Kappraff and McClain, Bucklens shows that all key measurements relate to the musical scale of Pythagoras.\nSculpture.\nThe cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438\u00a0BC. The appearance of this is known from other images. The decorative stonework was originally highly coloured. The temple was dedicated to Athena at that time, though construction continued until almost the beginning of the Peloponnesian War in 432. By the year 438, the Doric metopes on the frieze above the exterior colonnade and the Ionic frieze around the upper portion of the walls of the cella had been completed.\nOnly a small number of the original sculptures remain \"in situ.\" Most of the surviving sculptures are at the Acropolis Museum in Athens and at the British Museum in London (see Elgin Marbles). Additional pieces are at the Louvre, the National Museum of Denmark, and the Kunsthistoriches Museum in Vienna.\nIn March 2022, the Acropolis Museum launched a new website with \"photographs of all the frieze blocks preserved today in the Acropolis Museum, the British Museum and the Louvre\".\nMetopes.\nThe frieze of the Parthenon's entablature contained 92 metopes, 14 each on the east and west sides, 32 each on the north and south sides. They were carved in high relief, a practice employed until then only in treasuries (buildings used to keep votive gifts to the gods). According to the building records, the metope sculptures date to the years 446\u2013440. The metopes of the east side of the Parthenon, above the main entrance, depict the Gigantomachy (the mythical battle between the Olympian gods and the Giants). The metopes of the west end show the Amazonomachy (the mythical battle of the Athenians against the Amazons). The metopes of the south side show the Thessalian Centauromachy (battle of the Lapiths aided by Theseus against the half-man, half-horse Centaurs). Metopes 13\u201321 are missing, but drawings from 1674 attributed to Jaques Carrey indicate a series of humans; these have been variously interpreted as scenes from the Lapith wedding, scenes from the early history of Athens, and various myths. On the north side of the Parthenon, the metopes are poorly preserved, but the subject seems to be the sack of Troy.\nThe mythological figures of the metopes of the East, North, and West sides of the Parthenon had been deliberately mutilated by Christian iconoclasts in late antiquity.\nThe metopes present examples of the Severe Style in the anatomy of the figures' heads, in the limitation of the corporal movements to the contours and not to the muscles, and in the presence of pronounced veins in the figures of the Centauromachy. Several of the metopes still remain on the building, but, with the exception of those on the northern side, they are severely damaged. Some of them are located at the Acropolis Museum, others are in the British Museum, and one is at the Louvre museum.\nIn March 2011, archaeologists announced that they had discovered five metopes of the Parthenon in the south wall of the Acropolis, which had been extended when the Acropolis was used as a fortress. According to \"Eleftherotypia\" daily, the archaeologists claimed the metopes had been placed there in the 18th century when the Acropolis wall was being repaired. The experts discovered the metopes while processing 2,250 photos with modern photographic methods, as the white Pentelic marble they are made of differed from the other stone of the wall. It was previously presumed that the missing metopes were destroyed during the Morosini explosion of the Parthenon in 1687.\nFrieze.\nThe most characteristic feature in the architecture and decoration of the temple is the Ionic frieze running around the exterior of the cella walls. The bas-relief frieze was carved in situ and is dated from c.\u2009443\u2013438.\nOne interpretation is that it depicts an idealized version of the Panathenaic procession from the Dipylon Gate in the Kerameikos to the Acropolis. In this procession held every year, with a special procession taking place every four years, Athenians and foreigners participated in honouring the goddess Athena by offering her sacrifices and a new peplos dress, woven by selected noble Athenian girls called . The procession is more crowded (appearing to slow in pace) as it nears the gods on the eastern side of the temple.\nJoan Breton Connelly offers a mythological interpretation for the frieze, one that is in harmony with the rest of the temple's sculptural programme which shows Athenian genealogy through a series of succession myths set in the remote past. She identifies the central panel above the door of the Parthenon as the pre-battle sacrifice of the daughter of the king Erechtheus, a sacrifice that ensured Athenian victory over Eumolpos and his Thracian army. The great procession marching toward the east end of the Parthenon shows the post-battle thanksgiving sacrifice of cattle and sheep, honey and water, followed by the triumphant army of Erechtheus returning from their victory. This represents the first Panathenaia set in mythical times, the model on which historic Panathenaic processions were based. This interpretation has been rejected by William St Clair, who considers that the frieze shows the celebration of the birth of Ion, who was a descendant of Erechtheus. This interpretation has been rejected by Catharine Titi, who agrees with St Clair that the mood is one of celebration (rather than sacrifice) but argues that the celebration of the birth of Ion requires the presence of an infant but there is no infant on the frieze.\nPediments.\nTwo pediments rise above the portals of the Parthenon, one on the east front, one on the west. The triangular sections once contained massive sculptures that, according to the second-century geographer Pausanias, recounted the birth of Athena and the mythological battle between Athena and Poseidon for control of Athens.\nEast pediment.\nThe east pediment originally contained 10 to 12 sculptures depicting the Birth of Athena. Most of those pieces were removed and lost during renovations in either the eighth or the twelfth century. Only two corners remain today with figures depicting the passage of time over the course of a full day. Tethrippa of Helios is in the left corner and Selene is on the right. The horses of Helios's chariot are shown with livid expressions as they ascend into the sky at the start of the day. Selene's horses struggle to stay on the pediment scene as the day comes to an end.\nWest pediment.\nThe supporters of Athena are extensively illustrated at the back of the left chariot, while the defenders of Poseidon are shown trailing behind the right chariot. It is believed that the corners of the pediment are filled by Athenian water deities, such as the Kephisos river, the Ilissos river, and nymph Kallirhoe. This belief emerges from the fluid character of the sculptures' body position which represents the effort of the artist to give the impression of a flowing river. Next to the left river god, there are the sculptures of the mythical king of Athens (Cecrops or Kekrops) with his daughters (Aglaurus, Pandrosos, Herse). The statue of Poseidon was the largest sculpture in the pediment until it broke into pieces during Francesco Morosini's effort to remove it in 1688. The posterior piece of the torso was found by Lusieri in the groundwork of a Turkish house in 1801 and is currently held in the British Museum. The anterior portion was revealed by Ross in 1835 and is now held in the Acropolis Museum of Athens.\nEvery statue on the west pediment has a fully completed back, which would have been impossible to see when the sculpture was on the temple; this indicates that the sculptors put great effort into accurately portraying the human body.\nAthena Parthenos.\nThe only piece of sculpture from the Parthenon known to be from the hand of Phidias was the statue of Athena housed in the \"naos\". This massive chryselephantine sculpture is now lost and known only from copies, vase painting, gems, literary descriptions, and coins.\nLater history.\nLate antiquity.\nA major fire broke out in the Parthenon shortly after the middle of the third century AD. which destroyed the roof and much of the sanctuary's interior. Heruli pirates sacked Athens in 276, and destroyed most of the public buildings there, including the Parthenon. Repairs were made in the fourth century AD, possibly during the reign of Julian the Apostate. A new wooden roof overlaid with clay tiles was installed to cover the sanctuary. It sloped at a greater angle than the original roof and left the building's wings exposed.\nThe Parthenon survived as a temple dedicated to Athena for nearly 1,000 years until Theodosius II, during the Persecution of pagans in the late Roman Empire, decreed in 435 that all pagan temples in the Eastern Roman Empire be closed. It is debated exactly when during the 5th century that the closure of the Parthenon as a temple was put into practice. It is suggested to have occurred in c.\u2009481\u2013484, on the order of Emperor Zeno, because the temple had been the focus of Pagan Hellenic opposition against Zeno in Athens in support of Illus, who had promised to restore Hellenic rites to the temples that were still standing.\nAt some point in the fifth century, Athena's great cult image was looted by one of the emperors and taken to Constantinople, where it was later destroyed, possibly during the siege and sack of Constantinople during the Fourth Crusade in 1204 AD.\nChristian church.\nThe Parthenon was converted into a Christian church in the final decades of the fifth century to become the Church of the Parthenos Maria (Virgin Mary) or the Church of the Theotokos (Mother of God). The orientation of the building was changed to face towards the east; the main entrance was placed at the building's western end, and the Christian altar and iconostasis were situated towards the building's eastern side adjacent to an apse built where the temple's pronaos was formerly located. A large central portal with surrounding side-doors was made in the wall dividing the cella, which became the church's nave, and from the rear chamber, the church's narthex. The spaces between the columns of the and the peristyle were walled up, though a number of doorways still permitted access. Icons were painted on the walls, and many Christian inscriptions were carved into the Parthenon's columns. These renovations inevitably led to the removal and dispersal of some of the sculptures. Sometime after the Parthenon was converted to a Christian church, the metopes of the north, west and east facades of the Parthenon were defaced by Christians in order to remove images of pagan deities. The damage was so extensive that the images on the affected metopes often can't be confidently identified.\nThe Parthenon became the fourth most important Christian pilgrimage destination in the Eastern Roman Empire after Constantinople, Ephesos, and Thessaloniki. In 1018, the emperor Basil II went on a pilgrimage to Athens after his final victory over the First Bulgarian Empire for the sole purpose of worshipping at the Parthenon. In medieval Greek accounts it is called the Temple of Theotokos Atheniotissa and often indirectly referred to as famous without explaining exactly which temple they were referring to, thus establishing that it was indeed well known.\nAt the time of the Latin occupation, it became for about 250\u00a0years a Latin Catholic church of Our Lady. During this period a tower, used either as a watchtower or bell tower and containing a spiral staircase, was constructed at the southwest corner of the cella, and vaulted tombs were built beneath the Parthenon's floor.\nRediscovery of the Parthenon.\nThe rediscovery of the Parthenon as an ancient monument dates back to the period of Humanism; in 1436 Cyriacus of Ancona was the first after antiquity to describe the Parthenon and to call it by his name, of which he had read many times in ancient texts, including that of Pausanias Periegetes. Thanks to him, Western Europe was able to have the first design of the monument, which Ciriaco called \"temple of the goddess Athena\", unlike previous travellers, who had called it \"church of Virgin Mary\":\n\"...mirabile Palladis Divae marmoreum templum, divum quippe opus Phidiae\" (\"...the wonderful temple of the goddess Athena, a divine work of Phidias\").\nIslamic mosque.\nIn 1456, Ottoman Turkish forces invaded Athens and laid siege to a Florentine army defending the Acropolis until June 1458, when it surrendered to the Turks. The Turks may have briefly restored the Parthenon to the Greek Orthodox Christians for continued use as a church. Some time before the end of the fifteenth century, the Parthenon became a mosque.\nThe precise circumstances under which the Turks appropriated it for use as a mosque are unclear; one account states that Mehmed II ordered its conversion as punishment for an Athenian plot against Ottoman rule. The apse was repurposed into a mihrab, the tower previously constructed during the Roman Catholic occupation of the Parthenon was extended upwards to become a minaret, a minbar was installed, the Christian altar and iconostasis were removed, and the walls were whitewashed to cover icons of Christian saints and other Christian imagery.\nDespite the alterations accompanying the Parthenon's conversion into a church and subsequently a mosque, its structure had remained basically intact. In 1667, the Turkish traveler Evliya \u00c7elebi expressed marvel at the Parthenon's sculptures and figuratively described the building as \"like some impregnable fortress not made by human agency\". He composed a poetic supplication stating that, as \"a work less of human hands than of Heaven itself, [it] should remain standing for all time\". The French artist Jacques Carrey in 1674 visited the Acropolis and sketched the Parthenon's sculptural decorations. Early in 1687, an engineer named Plantier sketched the Parthenon for the Frenchman Graviers d'Orti\u00e8res. These depictions, particularly Carrey's, provide important, and sometimes the only, evidence of the condition of the Parthenon and its various sculptures prior to the devastation it suffered in late 1687 and the subsequent looting of its art objects.\nPartial destruction.\nAs part of the Morean War (1684\u20131699), the Venetians sent an expedition led by Francesco Morosini to attack Athens and capture the Acropolis. The Ottomans fortified the Acropolis and used the Parthenon as a gunpowder magazine \u2013 despite having been forewarned of the dangers of this use by the 1656 explosion that severely damaged the Propylaea \u2013 and as a shelter for members of the local Turkish community.\nOn 26 September 1687 a Venetian mortar round, fired from the Hill of Philopappos, blew up the magazine. The explosion blew out the building's central portion and caused the cella's walls to crumble into rubble. According to Greek architect and archaeologist Kornilia Chatziaslani:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...three of the sanctuary's four walls nearly collapsed and three-fifths of the sculptures from the frieze fell. Nothing of the roof apparently remained in place. Six columns from the south side fell, eight from the north, as well as whatever remained from the eastern porch, except for one column. The columns brought down with them the enormous marble architraves, triglyphs, and metopes.\nAbout three hundred people were killed in the explosion, which showered marble fragments over nearby Turkish defenders and sparked fires that destroyed many homes.\nAccounts written at the time conflict over whether this destruction was deliberate or accidental; one such account, written by the German officer Sobievolski, states that a Turkish deserter revealed to Morosini the use to which the Turks had put the Parthenon; expecting that the Venetians would not target a building of such historic importance. Morosini was said to have responded by directing his artillery to aim at the Parthenon. Subsequently, Morosini sought to loot sculptures from the ruin and caused further damage in the process. Sculptures of Poseidon and Athena's horses fell to the ground and smashed as his soldiers tried to detach them from the building's west pediment.\nIn 1688 the Venetians abandoned Athens to avoid a confrontation with a large force the Turks had assembled at Chalcis; at that time, the Venetians had considered blowing up what remained of the Parthenon along with the rest of the Acropolis to deny its further use as a fortification to the Turks, but that idea was not pursued.\nOnce the Turks had recaptured the Acropolis, they used some of the rubble produced by this explosion to erect a smaller mosque within the shell of the ruined Parthenon. For the next century and a half, parts of the remaining structure were looted for building material and especially valuable objects.\nThe 18th century was a period of Ottoman stagnation\u2014so that many more Europeans found access to Athens, and the picturesque ruins of the Parthenon were much drawn and painted, spurring a rise in philhellenism and helping to arouse sympathy in Britain and France for Greek independence. Amongst those early travellers and archaeologists were James Stuart and Nicholas Revett, who were commissioned by the Society of Dilettanti to survey the ruins of classical Athens. They produced the first measured drawings of the Parthenon, published in 1787 in the second volume of \"Antiquities of Athens Measured and Delineated\".\nFrom 1801 to 1812, agents of Thomas Bruce, 7th Earl of Elgin, removed about half the surviving Parthenon sculptures, sending them to Britain in efforts to establish a private museum. Elgin stated he removed the sculptures with permission of the Ottoman officials who exercised authority in Athens at the time. The legality of Elgin's actions has been disputed.\nWar of Independence.\nDuring the Greek War of Independence (1821\u20131833) which ended the 355-year Ottoman rule of Athens, the Acropolis was besieged twice, first by the Greeks in 1821\u201322 and then by the Ottoman forces in 1826\u201327. During the first siege, the besieged Ottoman forces attempted to melt the lead in the columns of the Parthenon to cast bullets. During the second siege, the Parthenon was significantly damaged by Ottoman artillery fire.\nIndependent Greece.\nWhen independent Greece gained control of Athens in 1832, the visible section of the minaret was demolished; only its base and spiral staircase up to the level of the architrave remain intact. Soon all the medieval and Ottoman buildings on the Acropolis were destroyed. The image of the small mosque within the Parthenon's cella has been preserved in Pierre-Gustave Joly de Lotbini\u00e8re's photograph, published in Lerebours's \"Excursions Daguerriennes\" in 1842: the first photograph of the Acropolis. The area became a historical precinct controlled by the Greek government. In the later 19th century, the Parthenon was widely considered by Americans and Europeans to be the pinnacle of human architectural achievement, and became a popular destination and subject of artists, including Frederic Edwin Church and Sanford Robinson Gifford. Today it attracts millions of tourists every year, who travel up the path at the western end of the Acropolis, through the restored Propylaea, and up the Panathenaic Way to the Parthenon, which is surrounded by a low fence to prevent damage.\nDispute over the marbles.\nThe dispute centres around those of the Parthenon Marbles removed by Elgin, which are in the British Museum. A few sculptures from the Parthenon are also in the Louvre in Paris, in Copenhagen, and elsewhere, while more than half are in the Acropolis Museum in Athens. A few can still be seen on the building itself. In 1983, the Greek government formally asked the UK government to return the sculptures in the British Museum to Greece, and subsequently listed the dispute with UNESCO. The British Museum has consistently refused to return the sculptures, and successive British governments have been unwilling to force the museum to do so (which would require legislation). In 2021, UNESCO called upon the UK government to resolve the issue at the intergovernmental level. Discussions between UK and Greek officials are ongoing.\nFour pieces of the sculptures have been repatriated to Greece: 3 from the Vatican, and 1 from a museum in Sicily.\nRestoration.\nIn 1981, an earthquake caused damage to the east fa\u00e7ade. Air pollution and acid rain have damaged the marble and stonework.\nAn organized effort to preserve and restore buildings on the Acropolis began in 1975, when the Greek government established the Committee for the Conservation of the Acropolis Monuments (ESMA). That group of interdisciplinary specialist scholars oversees the academic understanding of the site to guide restoration efforts. The project later attracted funding and technical assistance from the European Union. An archaeological committee thoroughly documented every artefact remaining on the site, and architects assisted with computer models to determine their original locations. Particularly important and fragile sculptures were transferred to the Acropolis Museum.\nA crane was installed for moving marble blocks; the crane was designed to fold away beneath the roofline when not in use. In some cases, prior re-constructions were found to be incorrect. These were dismantled, and a careful process of restoration began.\nOriginally, various blocks were held together by elongated iron H pins that were completely coated in lead, which protected the iron from corrosion. Stabilizing pins added in the 19th century were not lead-coated, and corroded. Since the corrosion product (rust) is expansive, the expansion caused further damage by cracking the marble.\nThe last remaining slabs from the western section of the Parthenon frieze were removed from the monument in 1993 for fear of further damage. They have now been transported to the new Acropolis Museum. Until cleaning of the remaining sculptures was completed in 2005, black crusts and coatings were present on the marble surface. Between 20 January and the end of March 2008, 4200 items (sculptures, inscriptions small terracotta objects), including some 80 artefacts dismantled from the monuments in recent years, were removed from the old museum on the Acropolis to the new Acropolis Museum.\nIn 2019, Greece's Central Archaeological Council approved a restoration of the interior cella's north wall (along with parts of others). The project will reinstate as many as 360 ancient stones, and install 90 new pieces of Pentelic marble, minimizing the use of new material as much as possible. The eventual result of these restorations will be a partial restoration of some or most of each wall of the interior cella.\nSince the 19th century, the Parthenon's exterior has been encased in various extents of scaffolding until September 2025, when the scaffolding on the western side was removed temporarily. The Greek government says that it intends to remove all scaffolding from the monument altogether by 2026 after the completion of restoration works.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "23673", "revid": "873506", "url": "https://en.wikipedia.org/wiki?curid=23673", "title": "Pachomius the Great", "text": "Egyptian saint\nPachomius (; \"Pakhomios\"; ; c. 292 \u2013 9 May 348 AD), also known as Saint Pachomius the Great, is generally recognized as the founder of Christian cenobitic monasticism. Coptic churches celebrate his feast day on 9 May, and Eastern Orthodox and Catholic churches mark his feast on 15 May or 28 May. In Lutheranism, he is remembered as a renewer of the church, along with his contemporary (and fellow desert saint), Anthony of Egypt on 17 January.\nName.\nThe name \"Pachomius\" is of Coptic origin: \u2ca1\u2c81\u03e7\u2cb1\u2c99 \"pakh\u014dm\" from \u2c81\u03e7\u2cb1\u2c99 \"akh\u014dm\" \"eagle or falcon\" (\u2ca1 \"p\"- at the beginning is the Coptic definite article), from Middle Egyptian \ua725\u1e96m \"falcon\", originally \"divine image\". Into Greek, it was adopted as \u03a0\u03b1\u03c7\u03bf\u03cd\u03bc\u03b9\u03bf\u03c2 and \u03a0\u03b1\u03c7\u03ce\u03bc\u03b9\u03bf\u03c2. By Greek folk etymology, it was sometimes interpreted as \"broad-shouldered\" from \u03c0\u03b1\u03c7\u03cd\u03c2 \"thick, large\" and \u1f66\u03bc\u03bf\u03c2 \"shoulder\".\nLife.\nPachomius was born in c. 292 in Thebaid (near modern-day Luxor, Egypt) to pagan parents. According to his hagiography, at age 21, Pachomius was swept up against his will in a Roman army recruitment drive, a common occurrence during this period of turmoil and civil war. With several other youths, he was put onto a ship that floated down the Nile and arrived at Thebes in the evening. Here he first encountered local Christians, who customarily brought food and comfort daily to the conscripted troops. This made a lasting impression, and Pachomius vowed to investigate Christianity further when he got out. He was able to leave the army without ever having to fight. He moved to the village of Sheneset (Chenoboskion) in Upper Egypt and was converted and baptized in 314.\nPachomius then came into contact with several well known ascetics and decided to pursue that path under the guidance of the hermit named Palaemon (317). One of his devotions, popular at the time, was praying with his arms stretched out in the form of a cross. After studying seven years with Palaemon, Pachomius set out to lead the life of a hermit near St. Anthony of Egypt, whose practices he imitated until Pachomius heard a voice in Tabennisi that told him to build a dwelling for the hermits to come to. An earlier ascetic named Macarius had created a number of proto-monasteries called lavra, or cells, where holy men who were physically or mentally unable to achieve the rigors of Anthony's solitary life would live in a community setting. According to the \"Bohairic Life of Pachomius\" (17), while Pachomius was praying at the deserted village of Tabennisi, he heard a voice calling him, saying, \"Pachomius, Pachomius, struggle, dwell in this place and build a monastery; for many will come to you to become monks with you, and they will profit their souls.\" Later, while praying at night after a day of harvesting reeds with his brother on a small island, Pachomius had another vision of an angel saying to him three times, \"Pachomius, Pachomius, the Lord's will is [for you] to minister to the race of men and to unite them to himself\" (\"Bohairic Life of Pachomius\" 22).\nPachomius established his first monastery between 318 and 323 at Tabennisi, Egypt. His elder brother John joined him, and soon more than 100 monks lived nearby. Pachomius set about organizing these cells into a formal organization. Until then, Christian asceticism had been solitary or \"eremitic\" with male or female monastics living in individual huts or caves and meeting only for occasional worship services. Pachomius created the community or \"cenobitic\" organization, in which male or female monastics lived together and held their property in common under the leadership of an abbot or abbess. Pachomius realized that some men, acquainted only with the eremitical life, might speedily become disgusted if the distracting cares of the cenobitical life were thrust too abruptly upon them. He therefore allowed them to devote their whole time to spiritual exercises, undertaking all the community's administrative tasks himself. The community hailed Pachomius as \"Abba\" (\"father\" in Aramaic), from which \"Abbot\" derives.\nThe monastery at Tabennisi, though enlarged several times, soon became too small and a second was founded at Pbow. This monastery at Pbow would go on to become the center for monasteries springing up along the Nile in Upper Egypt. Both of these are believed to have initially been abandoned villages, which were then repurposed for Pachomius\u2019 vision of his \"Koinonia\" (network of monasteries). After 336, Pachomius spent most of his time at Pbow. Though Pachomius sometimes acted as lector for nearby shepherds, neither he nor any of his monks became priests. St. Athanasius visited and wished to ordain him in 333, but Pachomius fled from him. Athanasius' visit was probably a result of Pachomius' zealous defence of orthodoxy against Arianism. Basil of Caesarea visited, then took many of Pachomius' ideas, which he adapted and implemented in Caesarea. This ascetic rule, or Ascetica, is still used today by the Eastern Orthodox Church, comparable to that of the Rule of St. Benedict in the West.\nRule of St. Pachomius.\nPachomius was the first to set down a written monastic rule. The first rule was composed of prayers generally known and in general use, such as the Lord's Prayer. The monks were to pray them every day. As the community developed, the rules were elaborated with precepts taken from the Bible. He drew up a rule which made things easier for the less proficient, but did not check the most extreme asceticism in the more proficient. The Rule sought to balance prayer with work, the communal life with solitude. The day was organised around the liturgy, with time for manual work and devotional reading.\nFasts and work were apportioned according to the individual's strength. Each monk received the same food and clothing. Common meals were provided, but those who wished to absent themselves from them were encouraged to do so, and bread, salt, and water were placed in their cells. In the Pachomian monasteries it was left very much to the individual taste of each monk to fix the order of life for himself. Thus the hours for meals and the extent of his fasting were settled by him alone, he might eat with the others in common or have bread and salt provided in his own cell every day or every second day.\nHis rule was translated into Latin by Jerome. Honoratus of L\u00e9rins followed the Rule of St. Pachomius. Basil the Great and Benedict of Nursia adapted and incorporated parts of it in their rules.\nDeath and legacy.\nPachomius continued as abbot to the cenobites for some thirty years. During an epidemic (probably plague), Pachomius called the monks, strengthened their faith, and failed to appoint his successor. Pachomius then died on 14 Pashons, 64 AM (9 May 348 AD).\nBy the time Pachomius died, eight monasteries and several hundred monks followed his guidance. Within a generation, cenobic practices spread from Egypt to Palestine and the Judean Desert, Syria, North Africa and eventually Western Europe. The number of monks, rather than the number of monasteries, may have reached 7000.\nHis reputation as a holy man has endured. As mentioned above, several liturgical calendars commemorate Pachomius. Among many miracles attributed to Pachomius, that though he had never learned the Greek or Latin tongues, he sometimes miraculously spoke them. Pachomius is also credited with being the first Christian to use and recommend use of a prayer rope.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23674", "revid": "32589484", "url": "https://en.wikipedia.org/wiki?curid=23674", "title": "Philosophical Investigations", "text": "1953 work by Ludwig Wittgenstein\nPhilosophical Investigations () is a work by the philosopher Ludwig Wittgenstein, published posthumously in 1953.\n\"Philosophical Investigations\" is divided into two parts, consisting of what Wittgenstein calls, in the preface, \"Bemerkungen\", translated by G. E. M. Anscombe as \"remarks\".\nA survey among American university and college teachers ranked the \"Investigations\" as the most important book of 20th-century philosophy.\nRelation to Wittgenstein's body of work.\nIn its preface, Wittgenstein says that \"Philosophical Investigations\" can \"be seen in the right light only by contrast with and against the background of my older way of thinking\". Wittgenstein biographer Ray Monk writes, \"This is partly because of the great differences between his early and late work, but also because of the equally important continuities between the two\". The early work in which Wittgenstein expressed his \"older way of thinking\" is the only book Wittgenstein published in his lifetime, the \"Tractatus Logico-Philosophicus\".\nThe \"Blue and Brown Books\", a set of notes dictated to his class at Cambridge in 1933\u20131934, contain the seeds of Wittgenstein's later thoughts on language and are widely read as a turning point in his philosophy of language\u2014\"as an early prototype for subsequent presentations of Wittgenstein's later philosophy\".\nThe American philosopher Norman Malcolm credits Piero Sraffa with breaking Wittgenstein's hold of the notion that a proposition must literally be a picture of reality by means of a rude gesture from Sraffa, followed by Sraffa asking, \"What is the logical form of that?\" In the Introduction to the book written in 1945, Wittgenstein said Sraffa \"for many years unceasingly practiced on my thoughts. I am indebted to this stimulus for the most consequential ideas in this book\".\nContent.\nThe book begins with a quotation from Johann Nestroy's play \"Der Sch\u00fctzling\": \"The trouble about progress is that it always looks much greater than it really is\" (translation from revised fourth edition).\nLanguage games.\nWittgenstein first mentions games in section 3 of \"Philosophical Investigations\" and then develops this discussion of games into the key notion of a \"language game\". Wittgenstein's use of the term \"language game\" \"is meant to bring into prominence the fact that the speaking of language is part of an activity, or of a life-form\". A central feature of language games is that language is used in context and cannot be understood outside of that context. Wittgenstein lists the following as examples of language games: \"Giving orders, and obeying them\"; \"describing the appearance of an object, or giving its measurements\"; \"constructing an object from a description (a drawing)\"; \"reporting an event\"; \"speculating about an event\". The famous example is the meaning of the word \"game\". We speak of various kinds of games: board games, betting games, sports, and \"war games\". These are all different uses of the word \"games\". Wittgenstein also gives the example of \"Water!\", which can be used as an exclamation, an order, a request, or an answer to a question. The meaning of the word depends on the language game in which it is used. Another way Wittgenstein makes the point is that the word \"water\" has no meaning apart from its use within a language game. One might use the word as an order to have someone else bring you a glass of water. But it can also be used to warn someone that the water has been poisoned.\nWittgenstein applies his concept of language games not only to word meaning but also to sentence meaning. For example, the sentence \"Moses did not exist\" (\u00a7\u00a079) can mean various things. Wittgenstein argues that, independent of use, the sentence does not yet \"say\" anything. It is \"meaningless\" in the sense of not being significant for a particular purpose. It acquires significance only if we use it within a context; the sentence by itself does not determine its meaning but becomes meaningful only when it is used to say something. For instance, it can be used to say that no person or historical figure fits the descriptions attributed to the person who goes by the name of \"Moses\". But it can also mean that the leader of the Israelites was not called Moses. Or that there cannot have been anyone who accomplished all that the Bible relates about Moses, etc. What the sentence means thus depends on its use in a context.\nMeaning as use.\nThe \"Investigations\" deal largely with the difficulties of language and meaning. Wittgenstein viewed the tools of language as being fundamentally simple, and he believed that philosophers had obscured this simplicity by misusing language and by asking meaningless questions. He attempted in the \"Investigations\" to make things clear: \u2014to show the fly the way out of the fly bottle.\nWittgenstein claims that the meaning of a word is based on how the word is understood within the language game. A common summary of his argument is that meaning is use. According to the use theory of meaning, the words are not defined by reference to the objects they designate or by the mental representations one might associate with them, but by how they are used. For example, this means there is no need to postulate that there is something called \"good\" that exists independently of any good deed. Wittgenstein's theory of meaning contrasts with Platonic realism and with Gottlob Frege's notions of sense and reference. This argument has been labelled by some authors as \"anthropological holism\".\nSection 43 in Wittgenstein's \"Philosophical Investigations\" reads: \"For a large class of cases\u2014though not for all\u2014in which we employ the word 'meaning,' it can be defined thus: the meaning of a word is its use in the language.\"\nWittgenstein begins \"Philosophical Investigations\" with a quote from Augustine's \"Confessions\", which represents the view that language serves to point out objects in the world; this is the view that he will be criticising.The individual words in a language name objects\u2014sentences are combinations of such names. In this picture of language, we find the roots of the following idea: Every word has a meaning. This meaning is correlated with the word. It is the object for which the word stands.\nWittgenstein rejects a variety of ways of thinking about what the meaning of a word is or how meanings can be identified. He shows how, in each case, the meaning of the word presupposes our ability to use it. He first asks the reader to perform a thought experiment: come up with a definition of the word \"game\". While this may at first seem like a simple task, he then leads us through the problems with each of the possible definitions of the word \"game\". Any definition that focuses on amusement leaves us unsatisfied, since the feelings experienced by a world-class chess player are very different from those of a circle of children playing duck, duck, goose. Any definition that focuses on competition will fail to explain the game of catch, or the game of solitaire. And a definition of the word \"game\" that focuses on rules will face similar difficulties.\nThe essential point of this exercise is often missed. Wittgenstein's point is not that it is impossible to define \"game\", but that \"even if we don't have a definition, we can still use the word successfully\". Everybody understands what we mean when we talk about playing a game, and we can even clearly identify and correct inaccurate uses of the word, all without reference to any definition that consists of necessary and sufficient conditions for the application of the concept of a game. The German word for \"game\", , has a different sense from the English; the meaning of also extends to the concept of \"play\" and \"playing\". This German sense of the word may help readers better understand Wittgenstein's remarks regarding games.\nWittgenstein argues that definitions emerge from what he termed \"forms of life\", roughly the culture and society in which they are used. Wittgenstein stresses the social aspects of cognition; to see how language works in most cases, we have to see how it functions in a specific social situation. It is this emphasis on becoming attentive to the social backdrop against which language is rendered intelligible that explains Wittgenstein's elliptical comment that \"If a lion could talk, we could not understand him.\" However, in proposing the thought experiment involving the fictional character Robinson Crusoe, a captain shipwrecked on a desolate island with no other inhabitant, Wittgenstein shows that language is not in all cases a social phenomenon (although it is in most cases); instead, the criterion for a language is grounded in a set of interrelated normative activities: teaching, explanations, techniques, and criteria of correctness. In short, it is essential that a language be shareable, but this does not imply that for a language to function, it must be shared.\nWittgenstein rejects the idea that ostensive definitions can provide us with the meaning of a word. For Wittgenstein, the thing that the word stands for does not give the meaning of the word. Wittgenstein argues for this by making a series of moves to show that understanding an ostensive definition presupposes an understanding of the way the word being defined is used. So, for instance, there is no difference between pointing to a piece of paper, to its colour, or to its shape, but understanding the difference is crucial to using the paper in an ostensive definition of a shape or of a colour.\nFamily resemblances.\nWhy is it that we are sure a particular activity (e.g. Olympic target shooting) is a game while a similar activity (e.g. military sharpshooting) is not? Wittgenstein's explanation is tied up with an important analogy. How do we recognise that two people we know are related to one another? We may see similar height, weight, eye colour, hair, nose, mouth, patterns of speech, social or political views, mannerisms, body structure, last names, etc. If we see enough similarities we say we've noticed a family resemblance. This is not always a conscious process\u2014generally we don't catalogue various similarities until we reach a certain threshold; we just intuitively see the resemblances. Wittgenstein suggests that the same is true of language. We are all familiar with enough things that are games and enough things that are not games that we can categorise new activities as either games or not.\nThis brings us back to Wittgenstein's reliance on indirect communication and on thought-experiments. Some philosophical confusions come about because we aren't able to see family resemblances. We've made a mistake in understanding the vague and intuitive rules that language uses and have thereby tied ourselves up in philosophical knots. He suggests that an attempt to untangle these knots requires more than simple deductive arguments pointing out the problems with some particular position. Instead, Wittgenstein's larger goal is to try to divert us from our philosophical problems long enough to become aware of our intuitive ability to see the family resemblances.\nRules and rule-following.\nWittgenstein's discussion of rules and rule-following takes place in \u00a7\u00a7\u00a0138\u2013242. Wittgenstein begins his discussion of rules with the example of one person giving orders to another \"to write down a series of signs according to a certain formation rule\". The series of signs consists of the natural numbers. Wittgenstein draws a distinction between following orders by copying the numbers following instruction and understanding the construction of the series of numbers. One general characteristic of games that Wittgenstein considers in detail is the way in which they consist in following rules. Rules constitute a family, rather than a class that can be explicitly defined. As a consequence, it is not possible to provide a definitive account of what it is to follow a rule. Indeed, he argues that any course of action can be made out to accord with some particular rule, and that therefore a rule cannot be used to explain an action. Rather, that one is following a rule or not is to be decided by looking to see if the actions conform to the expectations in the particular \"form of life\" in which one is involved. Following a rule is a social activity.\nSaul Kripke provides an influential discussion of Wittgenstein's remarks on rules. For Kripke, Wittgenstein's discussion of rules \"may be regarded as a new form of philosophical scepticism\". He starts his discussion of Wittgenstein by quoting what he describes as Wittgenstein's sceptical paradox: \"This was our paradox: no course of action could be determined by a rule, because every course of action can be made out to accord with the rule. The answer was: if everything can be made out to accord with the rule, then it can also be made out to conflict with it. And so there would be neither accord nor conflict here.\" Kripke argues that the implications of Wittgenstein's discussion of rules is that no person can mean something by the language that they use or correctly follow (or fail to follow) a rule. In his 1984 book, \"Wittgenstein on Meaning\", Colin McGinn disputed Kripke's interpretation.\nPrivate language.\nWittgenstein also ponders the possibility of a language that talks about those things that are known only to the user, whose content is inherently private. The usual example is that of a language in which one names one's sensations and other subjective experiences, such that the meaning of the term is decided by the individual alone. For example, the individual names a particular sensation, on some occasion, \"S\", and intends to use that word to refer to that sensation. Such a language Wittgenstein calls a \"private language\".\nWittgenstein presents several perspectives on the topic. One point he makes is that it is incoherent to talk of knowing that one is in some particular mental state. Whereas others can learn of my pain, for example, I simply have my own pain; it follows that one does not know of one's own pain, one simply has a pain. For Wittgenstein, this is a grammatical point, part of the way in which the language game involving the word \"pain\" is played.\nAlthough Wittgenstein certainly argues that the notion of private language is incoherent, because of the way in which the text is presented the exact nature of the argument is disputed. First, he argues that a private language is not really a language at all. This point is intimately connected with a variety of other themes in his later works, especially his investigations of \"meaning\". For Wittgenstein, there is no single, coherent \"sample\" or \"object\" that we can call \"meaning\". Rather, the supposition that there are such things is the source of many philosophical confusions. Meaning is a complicated phenomenon that is woven into the fabric of our lives. A good first approximation of Wittgenstein's point is that meaning is a social event; meaning happens between language users. As a consequence, it makes no sense to talk about a private language, with words that \"mean\" something in the absence of other users of the language.\nWittgenstein also argues that one couldn't possibly use the words of a private language. He invites the reader to consider a case in which someone decides that each time she has a particular sensation she will place a sign \"S\" in a diary. Wittgenstein points out that in such a case one could have no criteria for the correctness of one's use of \"S\". Again, several examples are considered. One is that perhaps using \"S\" involves mentally consulting a table of sensations, to check that one has associated \"S\" correctly; but in this case, how could the mental table be checked for its correctness? It is \"[a]s if someone were to buy several copies of the morning paper to assure himself that what it said was true\", as Wittgenstein puts it. One common interpretation of the argument is that while one may have direct or privileged access to one's current mental states, there is no such infallible access to identifying previous mental states that one had in the past. That is, the only way to check to see if one has applied the symbol \"S\" correctly to a certain mental state is to introspect and determine whether the current sensation is identical to the sensation previously associated with \"S\". And while identifying one's current mental state of remembering may be infallible, whether one remembered correctly is not infallible. Thus, for a language to be used at all it must have some public criterion of identity.\nOften, what is widely regarded as a deep philosophical problem will vanish, argues Wittgenstein, and eventually be seen as a confusion about the significance of the words that philosophers use to frame such problems and questions. It is only in this way that it is interesting to talk about something like a \"private language\"\u00a0\u2013 i.e., it is helpful to see how the \"problem\" results from a misunderstanding.\nIn summary, Wittgenstein asserts that if something is a language, it cannot be logically private; and if something is private, it is not (and cannot be) a language.\nWittgenstein's beetle.\nAnother point that Wittgenstein makes against the possibility of a private language involves the beetle-in-a-box thought experiment. He asks the reader to imagine that each person has a box, inside which is something that everyone intends to refer to with the word \"beetle\". Further, suppose that no one can look inside another's box, and each claims to know what a \"beetle\" is only by examining their own box. Wittgenstein suggests that, in such a situation, the word \"beetle\" could not be the name of a thing, because supposing that each person has something completely different in their boxes (or nothing at all) does not change the meaning of the word; the beetle as a private object \"drops out of consideration as irrelevant\". Thus, Wittgenstein argues, if we can talk about something, then it is not \"private\" in the sense considered. And, contrapositively, if we consider something to be indeed private, it follows that we \"cannot talk about it\".\nMind.\nWittgenstein's investigations of language lead to several issues concerning the mind. His key target of criticism is any form of extreme mentalism that posits mental states that are entirely unconnected to the subject's environment. For Wittgenstein, thought is inevitably tied to language, which is inherently social. Part of Wittgenstein's credo is captured in the following proclamation: \"An 'inner process' stands in need of outward criteria.\" This follows primarily from his conclusions about private languages: a private mental state (a sensation of pain, for example) cannot be adequately discussed without public criteria for identifying it.\nAccording to Wittgenstein, those who insist that consciousness (or any other apparently subjective mental state) is conceptually unconnected to the external world are mistaken. Wittgenstein explicitly criticises so-called conceivability arguments: \"Could one imagine a stone's having consciousness? And if anyone can do so\u2014why should that not merely prove that such image-mongery is of no interest to us?\" He considers and rejects the following reply as well:\nBut if I suppose that someone is in pain, then I am simply supposing that he has just the same as I have so often had.\u00a0\u2013 That gets us no further. It is as if I were to say: \"You surely know what 'It is 5 o'clock here' means; so you also know what 'It's 5 o'clock on the sun' means. It means simply that it is just the same there as it is here when it is 5 o'clock.\"\u00a0\u2013 The explanation by means of identity does not work here.\nThus, according to Wittgenstein, mental states are intimately connected to a subject's environment, especially to his or her linguistic environment, and conceivability or imaginability. Arguments that claim otherwise are misguided.\n\"Seeing that\" vs. \"seeing as\".\nIn addition to ambiguous sentences, Wittgenstein discussed figures that can be seen and understood in two different ways. Often one can see something in a straightforward way\u00a0\u2013 seeing that it is a rabbit, perhaps. But, at other times, one notices a particular aspect\u00a0\u2013 seeing it as something.\nAn example Wittgenstein uses is the \"duck-rabbit\", an ambiguous image that can be \"seen as\" either a duck or a rabbit. When one looks at the duck-rabbit and sees a rabbit, one is not interpreting the picture as a rabbit, but rather reporting what one sees. One just sees the picture as a rabbit. But what occurs when one sees it first as a duck, then as a rabbit? As the gnomic remarks in the \"Investigations\" indicate, Wittgenstein isn't sure. However, he is sure that it could not be the case that the external world stays the same while an \"internal\" cognitive change takes place.\nResponse and influence.\nIn a 1999 poll of philosophers by the journal \"Philosophical Forum\", the \"Philosophical Investigations\" was the most named work in response to the prompt to name the top five most important books in 20th century philosophy.\nBertrand Russell in his book \"My Philosophical Development\", wrote that \"I have not found in Wittgenstein's \"Philosophical Investigations\" anything that seemed to me interesting and I do not understand why a whole school finds important wisdom in its pages.\"\nIn his book \"Words and Things\", Ernest Gellner was fiercely critical of the work of Ludwig Wittgenstein, J. L. Austin, Gilbert Ryle, Antony Flew, P. F. Strawson and many others. Ryle refused to have the book reviewed in the philosophical journal \"Mind\" (which he edited), and Bertrand Russell (who had written an approving foreword) protested in a letter to \"The Times\". A response from Ryle and a lengthy correspondence ensued.\nThe first English translation of Karl Rahner's was named \"Theological Investigations\"; this title choice was by a translator and former student of Wittgenstein's, the theologian Cornelius Ernst, as a homage to the book.\nIn addition to stressing the differences between the \"Investigations\" and the \"Tractatus\", some critical approaches have claimed there to be more continuity and similarity between the two works than many suppose. One of these is the New Wittgenstein approach.\nKripkenstein.\nThe discussion of private languages was revitalised in 1982 with the publication of Kripke's book \"Wittgenstein on Rules and Private Language\". In this work, Kripke uses Wittgenstein's text to develop a particular type of scepticism about rules that stresses the communal nature of language-use as grounding meaning. Critics of Kripke's version of Wittgenstein have facetiously referred to it as \"Kripkenstein\", with scholars such as Gordon Baker, Peter Hacker, Colin McGinn, and John McDowell seeing it as a radical misinterpretation of Wittgenstein's text. Other philosophers\u00a0\u2013 such as Martin Kusch\u00a0\u2013 have defended Kripke's views.\nPopular culture.\nThe album, \"The Rose Has Teeth in the Mouth of a Beast\" and its title track \"Roses and Teeth for Ludwig Wittgenstein\", by electronic duo Matmos is named after a quote from the book. Steve Reich's song, \"You Are (Variations)\", also contain a quote from the book: \"Explanations come to an end somewhere\".\nEditions.\n\"Philosophical Investigations\" was not ready for publication when Wittgenstein died in 1951. G.\u00a0E.\u00a0M. Anscombe translated Wittgenstein's manuscript into English, and it was first published in 1953. There are multiple editions of \"Philosophical Investigations\" with the popular third edition and 50th anniversary edition having been edited by Anscombe:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "23675", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=23675", "title": "Pig latin", "text": ""}
{"id": "23677", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=23677", "title": "Poul Anderson", "text": "American science fiction writer (1926\u20132001)\nPoul William Anderson ( ; November 25, 1926 \u2013 July 31, 2001) was an American fantasy and science fiction author who was active from the 1940s until his death in 2001. Anderson also wrote historical novels. He won the Hugo Award seven times and the Nebula Award three times, and was nominated many more times for awards.\nBiography.\nPoul Anderson was born on November 25, 1926, in Bristol, Pennsylvania to Danish parents. Soon after his birth, his father, Anton Anderson, relocated the family to Texas, where they lived for more than ten years. After Anton Anderson's death, his widow took the children to Denmark. The family returned to the United States after the beginning of World War II, settling eventually on a Minnesota farm.\nWhile he was an undergraduate student at the University of Minnesota, Anderson's first stories were published by editor John W. Campbell in the magazine \"Astounding Science Fiction\": \"Tomorrow's Children\" by Anderson and F. N. Waldrop in March 1947 and a sequel, \"Chain of Logic\" by Anderson alone, in July. He earned his BA in physics with honors but became a freelance writer after he graduated in 1948. His third story was printed in the December \"Astounding\".\nAnderson married Karen Kruse in 1953 and relocated with her to the San Francisco Bay area. Their daughter Astrid (later married to science fiction author Greg Bear) was born in 1954. They made their home in Orinda, California. Over the years Poul gave many readings at The Other Change of Hobbit bookstore in Berkeley; his widow later donated his typewriter and desk to the store.\nIn 1954, he published the fantasy novel \"The Broken Sword\", one of his best-known works.\nIn 1965, Algis Budrys said that Anderson \"has for some time been science fiction's best storyteller\". He was a founding member of the Society for Creative Anachronism (SCA) in 1966 and of the Swordsmen and Sorcerers' Guild of America (SAGA), also during the mid-1960s. The latter was a group of Heroic fantasy authors organized by Lin Carter, originally eight in number, with entry by credentials as a fantasy writer alone. Anderson was the sixth President of the Science Fiction and Fantasy Writers of America, taking office in 1972.\nRobert A. Heinlein dedicated his 1985 novel \"The Cat Who Walks Through Walls\" to Anderson and eight of the other members of the Citizens' Advisory Council on National Space Policy.\nThe Science Fiction Writers of America made Anderson its 16th SFWA Grand Master in 1998. In 2000's fifth class, he was inducted into the Science Fiction and Fantasy Hall of Fame as one of two deceased and two living writers.\nHe died of prostate cancer on July 31, 2001, after a month in the hospital. A few of his novels were first published posthumously.\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
