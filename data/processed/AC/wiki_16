{"id": "21756", "revid": "50719640", "url": "https://en.wikipedia.org/wiki?curid=21756", "title": "Northern Sotho", "text": "Sotho-Tswana language spoken in South Africa\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nSepedi is one of South Africa\u2019s twelve official languages and belongs to the Bantu language family, specifically the Sotho-Tswana group. The language is spoken mainly in Limpopo Province, and to a lesser extent in Gauteng, Mpumalanga, and North West\".\"\n\"Sepedi\" refers to the dialect spoken by the Pedi people. \"Northern Sotho\" is the umbrella term for a group of related dialects. The two terms are often used interchangeably, but technically Sepedi is one dialect of Northern Sotho.\nAs of the 2022 South African Census, approximately 6.2 million people, or 10.0% of the national population, speak Sepedi as their first language. Sepedi ranks as the fifth most spoken first language.\nOfficial language status.\nSepedi vs Northern Sotho.\nAccording to Chapter 1, Section 6 of the South African Constitution, Sepedi is one of South Africa's 12 official languages. There has been significant debate about whether Northern Sotho should be used instead of Pedi. The English version of the South African Constitution lists Sepedi as an official language, while the Sepedi or Northern Sotho version of the Constitution of South Africa lists Sesotho sa Leboa as an official South African language.\nSouth Africa's official language policy.\nSouth Africa's official language policy refers to the twelve official languages of South Africa (i.e., Sepedi, Sesotho, Setswana, siSwati, Tshivenda, Xitsonga, Afrikaans, isiNdebele, isiXhosa, isiZulu, English, and South African Sign Language (SASL)), as specified in the Constitution of the Republic of South Africa.\nName.\nThe Northern Sotho written language was based largely on the Sepedi dialect. Missionaries studied this dialect the most closely and first developed the orthography in 1860 by Alexander Merensky, Grutzner, and Gerlachshoop. This subsequently provided a common writing system for 20 or more varieties of the Sotho-Tswana languages spoken in the former Transvaal, and also helped lead to \"Sepedi\" being used as the umbrella term for the entire language family. However, there are objections to this synecdoche by other Northern Sotho dialect speakers, such as speakers of Modjadji's Lobedu dialect.\nOther varieties of Northern Sotho.\nNorthern Sotho can be subdivided into Highveld-Sotho, which consists of comparatively recent immigrants mostly from the west and southwest parts of South Africa, and Lowveld-Sotho, which consists of a combination of immigrants from the north of South Africa and Sotho inhabitants of longer standing. Like other Sotho-Tswana people, their languages are named after totemic animals and, sometimes, by alternating or combining these with the names of famous chiefs.\nThe Highveld-Sotho.\nThe group consists of the following dialects:\nThe Lowveld-Sotho.\nThe group consists of Lobedu, Narene, Phalaborwa (Malatji), Mogoboya, Kone, Kgaga, Pulana, Pai, Ramafalo, Mohale and Kutswe.\nClassification.\nNorthern Sotho is one of the Sotho languages of the Bantu family. Although Northern Sotho shares the name \"Sotho\" with Southern Sotho, the two groups also have a great deal in common with their sister language Setswana. Northern Sotho is also closely related to Setswana, sheKgalagari and siLozi. It is a standardized variety, amalgamating several distinct varieties or dialects. Northern Sotho is also spoken by the Mohlala people and Malata People.\nMost Khelobedu speakers only learn to speak Sepedi at school, such that Sepedi is only their second or third language. Khelobedu is a written language. Lobedu is spoken by a majority of people in the Greater Tzaneen, Greater Letaba, and BaPhalaborwa municipalities, and a minority in Greater Giyani municipality, as well as in the Limpopo Province and Tembisa township in Gauteng. Its speakers are known as the Balobedu.\nSepulana () exists in unwritten form and forms part of the standard Northern Sotho. Sepulana is spoken in Bushbuckridge area by the MaPulana people.\nWriting system.\nSepedi is written in the Latin alphabet. The letter \"\u0161\" is used to represent the sound [] (\"sh\" is used in the trigraph \"tsh\" to represent an aspirated \"ts\" sound). The circumflex accent can be added to the letters e and o to distinguish their different sounds, but it is mostly used in language reference books. Some word prefixes, especially in verbs, are written separately from the stem.\nPhonology.\nConsonants.\nOther consonant sounds include fricative-combinations and .\nWithin nasal consonant compounds, the first nasal consonant sound is recognized as syllabic. Words such as \"nthu\u0161e\" \"help me\", are pronounced as . /n/ can also be pronounced as following a velar consonant.\nUrban varieties of Northern Sotho, such as Pretoria Sotho (actually a derivative of Tswana), have acquired clicks in an ongoing process of such sounds spreading from Nguni languages.\nTones.\nLike most other Niger\u2013Congo languages, Sesotho is a tonal language, spoken with two basic tones, high (H) and low (L).\nVocabulary.\nSome examples of Northern Sotho words and phrases:\nSample text.\nUniversal Declaration of Human Rights&lt;poem style=\"margin-left: 1em; font-style: italic;\" lang=\"nso\"&gt;\nTemana 1\nBatho ka moka ba belegwe ba lokologile le gona ba na le seriti sa go lekana le ditokelo. Ba filwe monagano le letswalo mme ba swanet\u0161e go swarana ka moya wa bana ba mpa.\nTemana 2\nMang le mang o swanet\u0161e ke ditokelo le ditokologo ka moka t\u0161e go bolet\u0161wego ka t\u0161ona ka mo Boikanong bjo, ntle le kgethollo ya mohuta wo mongwe le wo mongwe bjalo ka morafe, mmala, bong, polelo, bodumedi, dipolitiki goba ka kgopolo, bot\u0161o go ya ka set\u0161haba goba maemo, diphahlo, matswalo goba maemo a mangwe le a mangwe.\nGo feta fao, ga go kgethollo yeo e swanet\u0161ego go dirwa go ya ka maemo a dipolitiki, tokelo ya boahlodi, goba maemo a dit\u0161habat\u0161haba goba lefelo leo motho a dulago go lona, goba ke naga ye e ipu\u0161ago, trasete, naga ya go se ipu\u0161e goba se sengwe le se sengwe seo se ka fokot\u0161ago maemo a go ikemela ga naga ya gabo.\n&lt;/poem&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21757", "revid": "48594613", "url": "https://en.wikipedia.org/wiki?curid=21757", "title": "New York Giants", "text": "National Football League franchise in East Rutherford, New Jersey\nThe New York Giants are a professional American football team based in the New York metropolitan area. The Giants compete in the National Football League (NFL) as a member of the National Football Conference (NFC) East division. The team plays its home games at MetLife Stadium (which it shares with the New York Jets) at the Meadowlands Sports Complex in East Rutherford, New Jersey, west of New York City. The Giants are headquartered and practice at the Quest Diagnostics Training Center, also in the Meadowlands.\nThe Giants were one of five teams that joined the NFL in 1925, and they are the only one of that group still existing, as well as the league's longest-established team in the Northeastern United States. The team ranks third among all NFL franchises with eight NFL championship titles: four in the pre-Super Bowl era (1927, 1934, 1938, 1956) and four since the advent of the Super Bowl (XXI (1986), XXV (1990), XLII (2007), and XLVI (2011)), along with more championship appearances than any other team, with 19 overall appearances. Their championship tally is surpassed only by the Green Bay Packers (13) and the Chicago Bears (9). Throughout their history, the Giants have featured 29 Hall of Fame players, including NFL Most Valuable Player (MVP) award winners Mel Hein, Frank Gifford, Y. A. Tittle, and Lawrence Taylor.\nTo distinguish themselves from the professional baseball team of the same name, the football team was incorporated as the \"New York National League Football Company, Inc.\" in 1929 and then changed to \"New York Football Giants, Inc.\" in 1937. While the baseball team moved to San Francisco after the 1957 season, the football team continues to legally use it as its corporate name, which the team is often referred to by fans and sportscasters alike. The team has also acquired several nicknames, including \"Big Blue\", the \"G-Men\", and the \"Jints\", an intentionally mangled contraction seen frequently in the \"New York Post\" and \"New York Daily News\", originating from the baseball team when they were based in New York. In addition, the team as a whole is occasionally referred to as the \"Big Blue Wrecking Crew\", even though this moniker primarily and originally refers to the Giants defensive unit during the 1980s and early-1990s.\nSince 2011, the team has struggled to find success in recent years, only having three winning seasons in 2012, 2016, and 2022, as well as two playoff appearances and no division titles. As of 2024, the Giants currently hold the longest active division title drought in the NFC, with a total of 13 seasons.\nThe team's heated rivalry with the Philadelphia Eagles is the oldest of the NFC East rivalries, dating back to 1933, and has been called the best rivalry in the NFL in the 21st century.\nHistory.\nMara family era (1925\u20131990).\nOn August 1, 1925, Timothy Mara and Will Gibson were granted a franchise by the NFL for their newly organized team in New York City \u2014 the New York Football Giants. The cost of the franchise was $2,500.\nThe Giants played their first game as an exhibition against All New Britain (which was not an NFL team) in New Britain, Connecticut, on October 4. They defeated New Britain 26\u20130 in front of a crowd of 10,000. \nThe New York Giants' first NFL game was a week later, on October 11, 1925, at the Cycledrome in Providence, Rhode Island, in a 0 to 14 loss to the Providence Steam Roller. The Giants were successful in their first season, finishing in fourth place in a 20-team league with an 8\u20134\u20130 record.\nEarl Potteiger years (1927\u20131928).\nNFL champions (1927).\nIn its third season, the team finished with the best record in the league at 11\u20131\u20131 and was awarded the NFL title. After a disappointing fourth season (1928) owner Tim Mara bought the entire squad of the Detroit Wolverines, principally to acquire star quarterback Benny Friedman, and merged the two teams under the Giants name.\nIn 1930, there were still many who questioned the quality of the professional game, claiming the college \"amateurs\" played with more intensity than professionals. In December 1930, the Giants played a team of Notre Dame All Stars at the Polo Grounds to raise money for the unemployed of New York City. It was also an opportunity to establish the skill and prestige of the pro game. Knute Rockne reassembled his Four Horsemen along with the stars of his 1924 Championship squad and told them to score early, then defend. Rockne, like much of the public, thought little of pro football and expected an easy win. But from the beginning it was a one-way contest, with Friedman running for two Giant touchdowns and Hap Moran passing for another. Notre Dame failed to score. When it was all over, Coach Rockne told his team, \"That was the greatest football machine I ever saw. I am glad none of you got hurt.\" The game raised $100,000 for the homeless, and is often credited with establishing the legitimacy of the professional game for those who were critical. It also was the last game the legendary Rockne ever coached; he was killed in an airplane crash on March 31, 1931.\nSteve Owen years (1931\u20131953).\nIn a 16-year span from 1931 to 1947, the Giants qualified to play in the NFL championship game 8 times, winning twice. During this period the Giants were led by Hall of Fame coach Steve Owen, and Hall of Fame players Mel Hein, Red Badgro, and Tuffy Leemans. In 1933 the Giants faced the Chicago Bears in the championship game and were defeated 23\u201321.\nNFL champions (1934).\nThe famous \"Sneakers Game\" was played in this era where the Giants defeated the Chicago Bears on an icy field in the 1934 NFL Championship Game, while wearing sneakers for better traction. The team would return to the championship game the following year but would fall to the Detroit Lions 26\u20137.\nNFL champions (1938).\nThe Giants captured their third NFL championship in 1938 with a 23\u201317 win over the Green Bay Packers. Both teams returned to the championship game the following year in 1939, with the Packers shutting out the Giants 27\u20130.\nThe period also featured the 1944 Giants, which are ranked as the #1 defensive team in NFL history, \"...a truly awesome unit\". They gave up only 7.5 points per game (a record that still stands) and shut out five of their 10 opponents, though they lost 14\u20137 to the Green Bay Packers in the 1944 NFL Championship Game. The Giants played the Detroit Lions to a scoreless tie on November 7, 1943. To this day, no NFL game played since then has ended in a scoreless tie. The Giants were particularly successful from the latter half of the 1930s until the United States entry into World War II.\nJim Lee Howell years (1954\u20131960).\nNFL champions (1956).\nThe Giants won their next championship in 1956, the first year the team began playing at Yankee Stadium in the Bronx borough of New York City. Aided by a number of future Pro Football Hall of Fame players such as running back Frank Gifford, linebacker Sam Huff, and offensive tackle Roosevelt Brown, as well as all-pro running back Alex Webster. The Giants' 1956 championship team not only included players who would eventually find their way to the Pro Football Hall of Fame, but a Hall of Fame coaching staff, as well. Head coach Jim Lee Howell's staff had Vince Lombardi coaching the offense and Tom Landry coaching the defense. From 1958 to 1963, the Giants played in the NFL Championship Game five times, but failed to win. Most significantly, the Giants played the Colts in the 1958 NFL Championship Game, which is considered a event in the history of the NFL. The game, which the Giants lost in overtime 23\u201317, is often called \"The Greatest Game Ever Played\" and is considered one of the most important events in furthering the NFL's popularity. The following year, they lost the championship to the Colts again, giving up a 9\u20137 fourth-quarter lead en route to a 31\u201316 loss.\nAllie Sherman years (1961\u20131968).\nBoth the 1961 and 1962 championship game matched the Giants up against the Green Bay Packers, with the Giants losing both 37\u20130 and 16\u20137 respectively. In 1963, led by league MVP quarterback Y. A. Tittle, who threw a then-NFL record 36 touchdown passes, the Giants advanced to the NFL Championship Game, where they lost to the Bears 14\u201310 for their third consecutive championship loss, as well as their fifth loss in the title game in 6 years.\nWith players such as Tittle and Gifford approaching their mid 30s, the team declined rapidly, finishing 2\u201310\u20132 in 1964. It was the start of a 15-year stretch with only two winning seasons and no playoff appearances. They rebounded with a 7\u20137 record in 1965, before crumbling to a league-worst 1\u201312\u20131 record, and allowing more than 500 points on defense in 1966. During the 1969 preseason, the Giants lost their first meeting with the New York Jets, 37\u201314, in front of 70,874 fans at the Yale Bowl in New Haven, Connecticut. Following the game, Wellington Mara fired coach Allie Sherman, and replaced him with former Giants fullback Alex Webster.\nIn 1967, the team acquired quarterback Fran Tarkenton from the Minnesota Vikings. Despite having several respectable seasons with Tarkenton at quarterback, including a 7\u20137 finish in 1967 and 9\u20135 in 1970, the Giants traded him back to the Vikings after a 4\u201310 finish in 1971. Tarkenton would go on to lead the Vikings to three Super Bowls and earn a place in the Hall of Fame, while the Giants suffered through one of the worst stretches in their history, winning only 23 games from 1973 to 1979. Before the 1976 season, the Giants tried to revive a weak offense by replacing retired RB Ron Johnson with future Hall of Fame fullback Larry Csonka, but Csonka was often injured and ineffective during his 3 years in New York. The 1977 season featured a roster which included three rookie quarterbacks.\nThe Giants were allowed to play their home games at the Yale Bowl in New Haven, Connecticut in 1973 and 1974, and at Shea Stadium (home of the Mets and Jets) in Queens, New York in 1975, due to the renovation of Yankee Stadium. They finally moved into their own dedicated state-of-the-art stadium in 1976, when they moved into Giants Stadium at the Meadowlands in East Rutherford, New Jersey, located west of New York City. One of the low points during this period was the play known as the \"Miracle at the Meadowlands\", which occurred in 1978. With the Giants trying to kill the clock and secure a win against the Philadelphia Eagles, offensive coordinator, Bob Gibson, chose to call a running play. This resulted in \"The Fumble\" by QB Joe Pisarcik that was returned for a game-winning touchdown by the Eagles' Herman Edwards.\nThe Giants' front office operations were complicated by a long-standing feud between Wellington Mara and his nephew, Tim Mara. Jack Mara had died in 1965, leaving his share of the club to his son Tim. Wellington and Tim's personal styles and their visions for the club clashed, and eventually they stopped talking to each other. NFL Commissioner Pete Rozelle intervened and recommended a neutral general manager, George Young, allowing the club to operate more smoothly. The feud became moot on February 20, 1991, when Tim Mara sold his shares in the club to Preston Robert Tisch.\nIn 1979, the Giants began the steps that would, in time, return them to the pinnacle of the NFL. These included the drafting of quarterback Phil Simms in 1979, and linebacker Lawrence Taylor in 1981. In 1981, Taylor won the NFL's Defensive Rookie of the Year and Defensive Player of the Year awards and the Giants made the playoffs for the first time since 1963. One of the few bright spots during this time was the team's excellent linebackers, who were known as the Crunch Bunch. After the strike-shortened 1982 season, in which they finished 4\u20135, head coach Ray Perkins resigned to succeed the legendary Bear Bryant as head coach at the University of Alabama. In a change that would prove crucial in the coming years, he was replaced by the team's defensive coordinator, Bill Parcells.\nBill Parcells years (1983\u20131990).\nIn 1983, Bill Parcells was promoted to head coach from defensive coordinator. One of his first moves was to change his starting quarterback, sitting the injury-prone and struggling Phil Simms (who had missed the entire 1982 season with an injury) and electing instead to go with Scott Brunner, who had gone 4\u20135 as the starter in place of Simms in the strike-shortened previous season. Parcells went as far as to demote Simms to the third-string position, promoting Jeff Rutledge over Simms to be Brunner's backup. Parcells later said the move was a mistake and one he \"nearly paid for dearly\" as the team finished with a 3\u201312\u20131 record and his job security was called into question.\nIn the off-season the Giants released Brunner and named Simms the starter. The move paid off as the team won nine games and returned to the playoffs. After beating the Los Angeles Rams in the wild-card round, the Giants prepared for a showdown against the top-seeded San Francisco 49ers. The 49ers defeated the Giants 21\u201310 in the divisional round.\nSuper Bowl XXI champions (1986).\nAfter 9\u20137 and 10\u20136 finishes in 1984 and 1985 respectively, the Giants compiled a 14\u20132 record in 1986 led by league MVP and Defensive Player of the Year Lawrence Taylor and the Big Blue Wrecking Crew defense. As of 2023, this is the Giants' best regular season record since the NFL began playing 16-game seasons in 1978. After clinching the top seed in the NFC, the Giants defeated the 49ers 49\u20133 in the divisional round of the NFC playoffs and the Redskins 17\u20130 in the NFC championship game, advancing to their first Super Bowl, Super Bowl XXI, against the Denver Broncos at the Rose Bowl in Pasadena. Led by MVP Simms who completed 22 of 25 passes for a Super Bowl record 88% completion percentage, they defeated the Broncos 39\u201320, to win their first championship since 1956. In addition to Phil Simms and Lawrence Taylor, the team was led during this period by head coach Bill Parcells, tight end Mark Bavaro, running back Joe Morris, and Hall of Fame linebacker Harry Carson.\nThe Giants struggled to a 6\u20139 record in the strike-marred 1987 season, due largely to a decline in the running game, as Morris managed only 658 yards (601.68 m \ud83c\uddea\ud83c\uddfa) behind an injury-riddled offensive line. The early portion of the 1988 season was marred by a scandal involving Lawrence Taylor. Taylor had abused cocaine and was suspended for the first four games of the season for his second violation of the league's substance-abuse policy. Despite the controversy, the Giants finished 10\u20136, and Taylor recorded 15.5 sacks after his return from the suspension; however, the team missed the playoffs in their last game of the season. They surged to a 12\u20134 record in 1989, but lost to the Los Angeles Rams in their opening playoff game when Flipper Anderson caught a 47-yard touchdown pass to give the Rams a 19\u201313 overtime win.\nSuper Bowl XXV champions (1990).\nIn 1990, the Giants went 13\u20133 and, at the time, set an NFL record for fewest turnovers in a season (14). They defeated the San Francisco 49ers, who were attempting to win the Super Bowl for an unprecedented third straight year, 15\u201313 at San Francisco and then defeated the Buffalo Bills 20\u201319 in Super Bowl XXV.\nMara and Tisch era (1991\u2013present).\nFollowing the 1990 season, Parcells resigned as head coach and was replaced by the team's offensive-line coach Ray Handley. Handley served as coach for two disappointing seasons (1991 and 1992), which saw the Giants fall from Super Bowl champions to an 8\u20138 record in 1991 and a 6\u201310 record in 1992. He was fired following the 1992 season, and replaced by former Denver Broncos' coach Dan Reeves. In the early 1990s, Simms and Taylor, two of the stars of the 1980s, played out the last seasons of their careers with steadily declining production. The Giants experienced a resurgent season with Reeves at the helm in 1993 however, and Simms and Taylor ended their careers as members of a playoff team.\nThe Giants initially struggled in the post Simms/Taylor era. After starting 3\u20137 in 1994, the Giants won their final six games to finish 9\u20137 but missed the playoffs. Quarterback Dave Brown received heavy criticism throughout the season. Brown performed poorly the following two seasons, and the Giants struggled to 5\u201311 and 6\u201310 records. Reeves was fired following the 1996 season.\nJim Fassel years (1997\u20132003).\nIn 1997, the Giants named Jim Fassel, who had spent the previous season as offensive coordinator of the Arizona Cardinals, as their 16th head coach. Fassel named Danny Kanell the team's starting quarterback. The Giants finished the 1997 season with a record of 10\u20135\u20131 and qualified for the playoffs for the first time in four years. However, they lost in the wild-card round to the Vikings at home. The following year, the Giants began the season 4\u20138 before rallying to finish the season 8\u20138. One of the notable games of that season was a win over the eventual Super Bowl champion Denver Broncos in week 15, giving the Broncos their first loss of the season after starting 13\u20130.\nBefore the 1999 season, the Giants signed ex-Carolina Panthers quarterback Kerry Collins. Collins was the first-ever draft choice of the expansion Carolina Panthers in 1995 and led the Panthers to the NFC Championship game in his second season. However, problems with alcohol, conflicts with his teammates, and questions about his character led to his release from the Panthers. The Giants finished the season with a 7\u20139 record, Fassel's first losing season as head coach.\nIn 2000, the Giants were looking to make the playoffs for the first time in three seasons. The Giants started the season 7\u20132, but suffered back-to-back home losses to St. Louis and Detroit to make their record 7\u20134 and call their playoff prospects into question. At a press conference following the Giants' loss to Detroit, Fassel guaranteed that \"this team is going to the playoffs\". The Giants responded, winning the rest of their regular season games to finish the season 12\u20134 and clinch the top seed in the NFC. In the divisional round, the Giants beat the Philadelphia Eagles 20\u201310 at home to qualify for the NFC Championship Game, in which they defeated the Minnesota Vikings 41\u20130. They advanced to play the Baltimore Ravens in Super Bowl XXXV. Though the Giants went into halftime down only 10\u20130, the Ravens dominated the second half. Their defense harassed Kerry Collins all game long, resulting in Collins completing only 15 of 39 passes for 112\u00a0yards and 4 interceptions. The Ravens won the game 34\u20137.\nAfter a disappointing 7\u20139 record in 2001, the Giants finished the 2002 season with a record of 10\u20136, qualifying for the playoffs as a wild card. This set up a meeting with the San Francisco 49ers in Candlestick Park in the wild-card round. The Giants built up a sizable lead throughout the game and led 38\u201314 with 4:27 left in the third quarter. However, San Francisco rallied to win the game by one point, with the final score of 39\u201338.\nAfter a dismal 2003 season in which the Giants finished with a 4\u201312 record, Jim Fassel was released by the Giants. His head coaching record with the Giants during this time was 58\u201353\u20131.\nTom Coughlin years (2004\u20132015).\nIn 2004, three years after their last Super Bowl appearance, Fassel was replaced by Tom Coughlin. Although Collins had several solid seasons as the Giants quarterback, he experienced his share of struggles. Also in 2004, the Giants completed a draft day trade for University of Mississippi quarterback Eli Manning. Manning became the team's starting quarterback in the middle of the 2004 season, taking over for Kurt Warner. During the three-year period from 2004 to 2006, Tom Coughlin's Giants compiled a 25\u201323 regular season record and two appearances in the wild-card round \u2014 both losses (to the Carolina Panthers in 2005 and to the Philadelphia Eagles in 2006.) and spawned intense media scrutiny concerning the direction of the team. During this period in their history, standout players included defensive end Michael Strahan, who set the NFL single season record in sacks in 2001, and running back Tiki Barber, who set a team record for rushing yards in a season in 2005. Barber retired at the end of the 2006 season.\nSuper Bowl XLII champions (2007).\nGoing into 2007, the Giants had made the playoffs in back-to-back seasons. In 2007, the Giants became the third NFL franchise to win at least 600 games when they defeated the Atlanta Falcons 31\u201310 on \"Monday Night Football\". For the 2007 season, the NFL scheduled the Giants' road game against the Miami Dolphins on October 28 in London's Wembley Stadium; this was the first NFL regular season game to be played outside of North America. The Giants defeated the Dolphins, 13\u201310. The Giants finished 10\u20136 and became NFC Champions after defeating the Tampa Bay Buccaneers, Dallas Cowboys, and Green Bay Packers in the NFC Playoffs. They set a record for most consecutive road wins in a single season with 10 (a streak which ended with a loss to the Cleveland Browns during week 6 of the 2008 season).\nThe Patriots (18\u20130) entered the Super Bowl undefeated and were 12-point favorites going into game weekend. The Giants defeated the Patriots 17\u201314 in Super Bowl XLII, aided by the famous \"Manning to Tyree\" pass. On this famous play, Manning escaped the grip of several Patriots defensive linemen, stepped up in the pocket, and heaved the ball down the middle of the field to a double-covered David Tyree. With Rodney Harrison, a Patriots defensive back, all over Tyree, David managed to hold on to the ball by holding it on his helmet until he fell to the ground. This catch set up a Manning to Plaxico Burress touchdown pass in the back of the end zone to put the Giants in the lead. It was the third biggest upset by betting line in Super Bowl history (the Baltimore Colts were favored by 19.5-point over the New York Jets in Super Bowl III, and the St. Louis Rams were favored by 14 over the New England Patriots in Super Bowl XXXVI). Co-owner John Mara described it as \"the greatest victory in the history of this franchise, without question\".\nThe Giants began the 2008 season with a record of 11\u20131 but lost three of their last four regular season games partially due to a self-inflicted gunshot wound to wide receiver Plaxico Burress. However, the Giants still won the NFC East with a record of 12\u20134, and clinched the number one seed in the NFC after beating the Carolina Panthers for home-field advantage and a first-round bye. In the divisional round of the playoffs, the Giants lost 23\u201311 to the Philadelphia Eagles at home.\nIn 2009, the Giants opened a new training complex, the Timex Performance Center, also located in the Meadowlands. After starting 5\u20130 in the 2009 season, New York lost to the likewise undefeated New Orleans Saints at the Louisiana Superdome 48\u201327, beginning a four-game losing streak, in which they lost to the Arizona Cardinals 24\u201317, the San Diego Chargers 21\u201320 and the Philadelphia Eagles 40\u201317. The streak was broken with a 34\u201331 overtime victory against the Atlanta Falcons. On Thanksgiving night, they lost to the Denver Broncos 26\u20136. The Giants next beat the division-leading Dallas Cowboys. A week later, with a record of 7\u20135, they lost to the Philadelphia Eagles, 45\u201338. On December 27, the Giants lost to the Carolina Panthers 41\u20139 in their final game at Giants Stadium and were eliminated from playoff eligibility. The Giants finished the season 8\u20138.\nFollowing the season, the Giants fired first-year defensive coordinator Bill Sheridan, and replaced him with the former Buffalo Bills interim head coach, Perry Fewell. The Giants defense finished 13th overall under Sheridan, giving up 324.9 yards (297.09 m \ud83c\uddea\ud83c\uddfa) per game, and the final two losses of the season against Carolina and Minnesota, in which the Giants gave up 85 points, ultimately led to the firing.\nIn 2010, the Giants moved from Giants Stadium into MetLife Stadium, then known as the \"New Meadowlands Stadium\". They won against the Carolina Panthers in the first game at New Meadowlands Stadium but then lost to the Indianapolis Colts in the second \"Manning Bowl\", so-called due to Eli Manning's brother Peyton playing for the Colts. The Giants dropped one game to the Tennessee Titans before going on a five-game winning streak, beating the Chicago Bears, Houston Texans, Detroit Lions, Dallas Cowboys, and Seattle Seahawks. Before long, the Giants were 6\u20132 but lost two straight to division foes: to the Cowboys 33\u201320 at home, and to the Philadelphia Eagles on the road, putting the Giants in second place in the NFC East at 6\u20134. In first place was the Eagles, but at December 19 the two teams tied at 8\u20134, setting up a match for first place. The Giants were at home and led 24\u20133 over the Eagles at halftime. The score was 31\u201310 with 5:40 left in the game, but Michael Vick led the Eagles to three touchdown drives to tie the game up at 31 with 40 seconds left. After a Giants three-and-out, Matt Dodge punted the ball to DeSean Jackson, who returned it for a touchdown, concluding the Giants' epic collapse. The next game, the Giants lost to the eventual Super Bowl champion Green Bay Packers 45\u201317, and at 9\u20136, they faced the Redskins. They had to win and have the Packers lose in order to get into the playoffs. The Giants won 17\u201314, but the Packers beat the Bears 10\u20133, so the Giants missed out on the playoffs again, ending a collapse in which the Giants went 4\u20134 in their last eight games.\nSuper Bowl XLVI champions (2011).\nDuring the 2011 preseason, the Giants lost Kevin Boss, Steve Smith, Rich Seubert, Keith Bulluck, Derek Hagan, and Pro Bowl center Shaun O'Hara to free agency. However, the season also saw the emergence of second-year wide receiver Victor Cruz and second-year tight end Jake Ballard. The Giants opened their season with a 28\u201314 loss to the Washington Redskins at FedEx Field on the 10th anniversary of the September 11th attacks. However, the Giants secured a 6\u20132 record by the midpoint of the season, including road victories over the Philadelphia Eagles and the New England Patriots. The latter victory ended the Patriots' NFL record home-game winning streak, after a touchdown pass from Manning to Jake Ballard with 15 seconds left in the game.\nHowever, the Giants then suffered a four-game losing streak, including road losses against the resurgent San Francisco 49ers and the New Orleans Saints and home losses to the Eagles and the then-undefeated Green Bay Packers, to make their record 6\u20136 entering December. The Giants broke their losing streak with a tightly contested 37\u201334 road victory over the Cowboys on December 11 with Jason Pierre-Paul blocking a last second field goal attempt, but lost at home to the Washington Redskins the following week to make their record 7\u20137 with a Christmas Eve showdown against their crosstown rival New York Jets the following week. The Giants won, 29\u201314, and knocked the Eagles out of playoff contention, to set up a Week 17 home game against the Cowboys in which the winner would clinch the NFC East while the loser would be eliminated from playoff contention. The game was flexed into Sunday Night Football. The Giants defeated the Cowboys, 31\u201314 and clinched the NFC East title and the fourth seed in the playoffs. Wide receiver Victor Cruz finished the regular season with 1,536 receiving yards, breaking the Giants franchise record previously held by Amani Toomer.\nOn January 8, 2012, in the first round of the playoffs, the Giants defeated the Atlanta Falcons 24\u20132. After giving up an early safety in the first half, quarterback Eli Manning threw for three consecutive touchdowns. Running backs Ahmad Bradshaw and Brandon Jacobs combined for 172 yards (157.28 m \ud83c\uddea\ud83c\uddfa) rushing, a season-high for the Giants. With the victory, the Giants advanced to the second round against the top-ranked Green Bay Packers 37\u201320.\nOn January 15, 2012, the Giants defeated the Green Bay Packers 37\u201320. Eli Manning threw for 330 yards (301.75 m \ud83c\uddea\ud83c\uddfa) and 3 touchdowns, two of which to wide receiver Hakeem Nicks. This earned the Giants a spot in the NFC Championship Game on January 22, 2012, against the San Francisco 49ers. They won this game 20\u201317, in overtime, with Tynes scoring the winning field goal as he did four years earlier in the same game against the Packers.\nThe New York Giants won Super Bowl XLVI against the New England Patriots with a score of 21\u201317. The winning touchdown was preceded by a 38-yard reception by receiver Mario Manningham. As in Super Bowl XLII, Eli Manning was Super Bowl MVP, defeating the Patriots for a second time in the Super Bowl.\nAhmad Bradshaw scored the game-winning touchdown by falling into the end zone. The Patriots were allowing Bradshaw to get the touchdown so they would get the ball with some time remaining. When Eli Manning handed the ball to Bradshaw, he told him not to score. Bradshaw was about to fall down at the 1-yard line but his momentum carried him in, thus the \"reluctant touchdown.\"\nAs was the case in each of their four previous Super Bowl appearances, the Giants trailed at halftime. They are the only team in NFL history to have more than two second half, come-from-behind, Super Bowl victories (4). The Pittsburgh Steelers, who accomplished the feat in Super Bowl X and Super Bowl XIV, are the only other team to do it more than once.\nThe Giants began the 2012 season with a home loss to the Dallas Cowboys, but rebounded to finish October with a 6\u20132 record and on a four-game winning streak that included a 26\u20133 road victory against the eventual NFC champion San Francisco 49ers. Following the arrival of Hurricane Sandy in the Northeastern United States, the Giants lost back-to-back games against the Pittsburgh Steelers and the Cincinnati Bengals to fall to 6\u20134. Despite impressive blowout home victories over the Green Bay Packers, New Orleans Saints and Philadelphia Eagles, the Giants finished the season 9\u20137 and out of the playoffs. Quarterback Eli Manning, defensive end Jason Pierre-Paul, wide receiver Victor Cruz, and guard Chris Snee represented the Giants at the Pro Bowl.\nThe 2013 season began with hope that the Giants could become the first team to play in the Super Bowl in their home stadium, as MetLife Stadium was scheduled to host Super Bowl XLVIII that February. However, the Giants' playoff hopes took a massive hit when they lost the first six games of the season. They rebounded to win the next four games in a row to improve to 4\u20136, but lost a critical home game to the Dallas Cowboys on a last-minute field goal. They finished the season 7\u20139 and with a losing record for the first time since 2004. The Giants drafted rookie wide receiver Odell Beckham Jr. in the 2014 NFL draft, who would later go on to win the AP Offensive Rookie of the Year award. However, the Giants missed the playoffs for a third straight season, finishing with a 6\u201310 record. The 2015 season was another disappointing campaign, as the Giants showcased a struggling defense and several late-game collapses. The Giants finished the season with a 6\u201310 record and missed the playoffs.\n2016\u2013present.\nOn January 14, 2016, the Giants announced that Ben McAdoo would become the team's head coach. He replaced Tom Coughlin, who had resigned the previous week. The Giants turned it around in 2016 with an 11\u20135 record, ending their five-year playoff drought. The Giants later lost to the Green Bay Packers 38\u201313 in the wild-card round.\nAfter having high expectations due to their 11\u20135 record in 2016, the Giants had an unexpected 0\u20135 start to the 2017 season, before pulling a massive upset versus the Denver Broncos at Sports Authority Field at Mile High for their first win of the season. However, during the Week 5 game against the Los Angeles Chargers, Odell Beckham Jr. fractured his ankle, an injury that ended his season. During the same game, the Giants also lost wide receivers Brandon Marshall and Dwayne Harris to season-ending injuries. The season was also marred by the suspensions of Dominique Rodgers-Cromartie and Janoris Jenkins. The Giants finished the 2017 season with a 3\u201313 record, the second-worst in the league. This was also the first time since 1983 in which the Giants finished the regular season with three or less wins, and their worst record since the 16-game season was adopted in the NFL.\nThe season was also highlighted by the controversial benching of longtime quarterback Eli Manning in Week 13, and the high-profile firings of head coach Ben McAdoo and general manager Jerry Reese, who were the first mid-season staff firings since the 1976 Giants' season. Manning was eventually renamed the starter in Week 14. Subsequently, the disastrous season led to the team being awarded the second overall pick in the 2018 NFL draft, which they utilized to select Saquon Barkley from Penn State. Despite Barkley's selection, several questions pertained into the following season around the team's offensive line and long-term future at quarterback.\nThe 2018 season began with Pat Shurmur being hired as the new head coach. Despite starting 1\u20137 for the second consecutive year, the Giants managed to marginally improve on their 3\u201313 campaign by finishing the season 5\u201311 in a 30\u201327 overtime win against the Chicago Bears. After defeating the Washington Redskins in Week 14, the Giants became the first team in NFL history to win 100 regular season games against an opponent. However, this ensured last place in the NFC East for the second straight year, marking the first time they were division rock bottom in back-to-back years since 1977 and 1978. The season was also highlighted by blown fourth-quarter leads which was similar to their 2015 team, where the Giants were in 12 one-possession games, and lost 8 of those by 7 points or less. Following the season's end, the team was placed to select sixth overall in the 2019 NFL draft. Barkley impressed in his rookie season, breaking several NFL and Giants team records for a rookie, including having the most receptions by a running back (91), most rushing touchdowns (11), most rushing yards (1,307), and most touchdowns in a season (15). He was also selected to the 2019 Pro Bowl, alongside teammates Olivier Vernon, Landon Collins, and Aldrick Rosas in addition to winning offensive rookie of the year honors.\nThe team used their sixth overall pick in the 2019 NFL draft on Duke quarterback Daniel Jones. The Giants went 4\u201312 in the 2019 season. After the 2019 season, the Giants' longtime quarterback, Eli Manning, retired after spending 16 seasons with the organization, while the team finished the season with a 4\u201312 record. Following the season, Shurmur was fired as head coach. Prior to the 2020 season, the Giants hired Joe Judge as head coach. At the start of the 2020 season, Daniel Jones took over as starting quarterback as the Giants finished 6\u201310, while tight end Evan Engram and cornerback James Bradberry were named to the 2021 Pro Bowl as reserves.\nIn the 2021 season the Giants failed to improve on their 2020 record by finishing 4\u201313. Then after the season, general manager Dave Gettleman retired and head coach Joe Judge was fired. During the season the squad's starting quarterback Daniel Jones sprained his neck and was temporarily replaced in the lineup by Mike Glennon and then Jake Fromm.\nOn January 21, 2022, the team hired Joe Schoen as the team's general manager, and on January 28, hired Brian Daboll as the team's new head coach. The 2022 season showed a much improved record, with the Giants finishing 9\u20137\u20131. On January 1, 2023, the Giants clinched a playoff berth for the first time since the 2016 season. On January 15, the Giants defeated the Minnesota Vikings 31\u201324 in the wild-card round, winning their first postseason game since their victory at Super Bowl XLVI in 2012. The Giants proceeded to lose the next game to the eventual NFC champion Philadelphia Eagles 38\u20137 in the divisional round.\nIn the 2023 season, the Giants regressed on their successful 2022 season, finishing 6\u201311. Daniel Jones was injured several times in the season and was placed on injured reserve after tearing his ACL in week 9. He was temporarily replaced in the lineup by Tyrod Taylor and Tommy DeVito.\nThe team further regressed in the 2024 season to a 3\u201314 record. After their week 10 loss, the Giants were 2\u20138 and benched Daniel Jones in favor of Tommy DeVito due to poor play. A few days later, on November 22, Jones requested to be released which the Giants granted. The season was plagued by poor quarterback play, numerous key injuries, and poor pass protection similar to the previous season. The Giants also had a 10-game losing streak and were swept by all of their division rivals, both for the first time in franchise history. There were a few bright spots for the Giants, such as their rookie class, which was led by wide receiver Malik Nabers. Nabers broke the Giants franchise record and NFL rookie record for receptions in a season.\nThe Giants had the 3rd pick in the 2025 draft which they used on edge rusher Abdul Carter, widely considered one of the best, if not the best, prospect in the draft.\nIn the midst of the Giants' 2025 season, and after falling to a frustrating 2-8 record after a loss to the Chicago Bears during another blown 4th quarter lead, head coach Brian Daboll was fired on November 10th, and offensive coordinator Mike Kafka took the interim head coaching role. That same season, after a loss to the Detroit Lions in the 5th blown fourth quarter lead of the season and falling to 2-10, Kafka fired defensive coordinator Shane Bowen on November 24th.\nChampionships.\nThe Giants have won a total of eight league championships: 1927, 1934, 1938, 1956, 1986, 1990, 2007 and 2011. The first four of those championships came in the pre-Super Bowl era. New York's eight championships put them third among all active and defunct NFL teams, trailing only the Green Bay Packers (13) and the Chicago Bears (9).\nNFL championships (pre-Super Bowl era).\nBefore the Super Bowl was instituted, the Giants won four officially recognized NFL championships.\nSuper Bowl championships.\nThe Giants have won four Super Bowls, tied with Green Bay and Kansas City for the fifth most behind Dallas, San Francisco (both with 5), and New England and Pittsburgh (6 each).\nNFC championships.\nThe Giants have won five NFC Championship Games, including two in overtime in 2007 and 2011.\nLogos and uniforms.\nWith over 100 years of team history, the Giants have used numerous uniforms and logos, while maintaining a consistent identity. The Giants' logos include several incarnations of a giant quarterback preparing to throw a football, a lowercase \"ny\", and stylized versions of the team nickname.\nGiants' jerseys are traditionally blue or red (or white with blue or red accents), and their pants alternate between white and gray. Currently, the Giants wear home jerseys that are solid blue with white block numbering, white pants with five thin blue/gray/red/gray/blue stripes on the pant legs, and solid blue socks. For this they gained their most renowned nickname, \"Big Blue\". For road uniforms, they wear a white jersey with red block numbering and red \"Northwestern\" stripes on the sleeves, gray pants with three thin non-contiguous red/blue/red stripes on the pant legs, and solid red socks. The Giants' current helmet is metallic blue with white block numbers, which are frontally mounted and base mounted on either side of a red stripe running down the center or frontally mounted and base mounted on the red center stripe itself. The Giants, along with the Pittsburgh Steelers, are one of only two teams in the NFL to have the players' uniform numbers on both the front and back of the helmets. The helmet is adorned on both sides with the stylized white lower case \"ny\" logo and features a gray facemask. The home uniforms are generally similar to the design used from 1966 to 1974, but with some slight elements from the 1956\u20131961 uniforms. The road uniforms are essentially a modernization of the design used from 1956 to 1961. Additionally, the Giants had a third jersey until the 2009 season, which recalled the Giants' solid red home jerseys from the early 1950s: a solid red alternate with white block numbers. These jerseys were used a total of four times, but have since been retired. They were used once in 2004 against the Philadelphia Eagles and in three consecutive years \u2013 2005, 2006, and 2007 \u2013 against the Dallas Cowboys.\nOwnerships, financial history and fan base.\nThe Giants have had a long and, at times, turbulent financial history. The team was founded by Tim Mara with an investment of US$500 in 1925 and became one of the first teams in the then five-year-old NFL. To differentiate themselves from the baseball team of the same name, they took the name \"New York Football Giants\", which they still use as their legal corporate name.\nAlthough the Giants were successful on the field in their initial seasons, their financial status was a different story. Overshadowed by baseball, boxing, and college football, professional football was not a popular sport in 1925. The Giants were in dire financial straits until the 11th game of the season when Red Grange and the Chicago Bears came to town, attracting over 73,000 fans. This gave the Giants a much needed influx of revenue, and perhaps altered the history of the franchise. The following year, Grange and his agent formed a rival league and stationed a competing team, led by Grange, in New York. Though the Giants lost $50,000 that season, the rival league folded and was subsumed into the NFL. Following the 1930 season, Mara transferred ownership of the team over to his two sons to insulate the team from creditors, and by 1946, he had given over complete control of the team to them. Jack, the older son, controlled the business aspects, while Wellington controlled the on-field operations. After their initial struggles the Giants financial status stabilized, and they led the league in attendance several times in the 1930s and 1940s.\nBy the early 1960s, the Giants had firmly established themselves as one of the league's biggest attractions. However, rather than continuing to receive their higher share of the league television revenue, the Mara sons pushed for equal sharing of revenue for the benefit of the entire league. Revenue sharing is still practiced in the NFL today, and is credited with strengthening the league. After their struggles in the latter half of the 1960s and the entire 1970s, the Giants hired an outsider, George Young, to run the football operations for the first time in franchise history. The Giants' on-field product and business aspects improved rapidly following the move.\nIn 1991, Tim Mara, grandson of the founder, was struggling with cancer and sold his half of the team to Bob Tisch for a reported $80 million. This marked the first time in franchise history the team had not been solely owned by the Mara family. In 2005, Wellington Mara, who had been with the team since its inception in 1925 when he worked as a ball boy, died at the age of 89. His death was followed two weeks later by the death of Tisch. In 2015, Wellington's widow and Giants co-owner Ann died due to complications from a head injury suffered in a fall. She was 85 years old.\nIn 2010, MetLife Stadium opened, replacing Giants Stadium. The new stadium is a 50/50 partnership between the Giants and Jets, and while the stadium is owned by the New Jersey Sports and Exposition Authority on paper, the two teams jointly built the stadium using private funds, and administer it jointly through New Meadowlands Stadium Corporation. The Giants had previously planned a $300 million renovation to the Meadowlands, before deciding in favor of the new stadium which was originally estimated to cost approximately $600 million, before rising to an estimated cost of one billion dollars. One advantage gained by owning the stadium is that the teams saved considerable money in tax payments. The teams leased the land from the state at a cost of $6.3 million per year. The state paid for all utilities, including the $30 million needed to install them.\nThe Giants are owned and operated by John Mara and Steve Tisch. \"Forbes\" magazine estimated the value of the team in 2012 to be $1.3 billion. This ranks the New York Giants as the fourth most valuable franchise in the NFL and the ninth most valuable professional sports franchise in the world. The value has steadily increased from $288 million in 1998, to their current value. The magazine estimated their revenue in 2006 at $182 million, of which $46 million came from gate receipts. Operating income was $26.9 million, and player salary was $102 million. Current major sponsors include Gatorade, Anheuser Busch, Toyota, and Verizon Wireless. Recent former sponsors include Miller Brewing and North Fork Bank. Luxury suites, retail and game day concessions at the new stadium are provisioned and operated by global hospitality giant Delaware North. The team's average ticket price is $72.\nThe Giants draw their fans from the New York metropolitan area. Since their move to New Jersey in 1976, fans from each state have claimed the team as their own. In January 1987, shortly before the team won Super Bowl XXI, then New York City mayor Ed Koch labeled the team \"foreigners\" and said they were not entitled to a ticker-tape parade in New York City. On February 5, 2008, the city, under mayor Michael Bloomberg, threw a ticker tape parade in honor of the Giants' Super Bowl XLII victory at the Canyon of Heroes in lower Manhattan. New York City held another ticker tape parade on February 7, 2012, in honor of the Giants' Super Bowl XLVI victory. According to a team spokesman, in 2001, 52 percent of the Giants' season ticket-holders lived in New Jersey. Most of the remaining ticket holders lived in New York State with some coming from other states. The Giants also draw fans from the Canadian province of Quebec mostly due to the province sharing a significant international border with New York State \u2014 New York City is only five to six hours away from Montreal by car.\nThrough the lean years of the 1960s and 1970s the Giants, in spite of a 17-year-long playoff drought, still accumulated a 20-year-long waiting list for season tickets. It has been estimated that the Giants have a waiting list of 135,000 people, the largest of any North American professional sports franchise.\nIn September 2025, Julia Koch and her family agreed to acquire a 10% minority stake in the Giants, valuing the franchise at over $10 billion. The transaction is subject to approval by NFL owners.\nRivalries.\nDivisional.\nPhiladelphia Eagles.\nThe rivalry between the New York Giants and the Philadelphia Eagles is one of the oldest in the NFL, dating back to 1933. The two teams have frequently fought for playoff contention, NFC East titles, and respect. While the Giants had the edge this rivalry early on in its history, the series began to even after the 1980s, with the Eagles going 22\u201321 against New York through the 1990s and 2000s. Philadelphia then dominated New York in the 2010s with a 16\u20134 record to claim their first lead in the series. Two key games or moments look at is the \"Miracle of the Meadowlands,\" and the \"The Hit.\" These two games helped create the intense rivalry still to this day even though these moments happened over 50 years ago. The Eagles lead the all-time series 94\u201389\u20132 as of the 2023 season. The two teams have met five times in the postseason, with the Giants winning two games to the Eagles three. Three of those four playoff meetings were held in the 2000s decade. New York City and Philadelphia have a strong geographic rivalry, as seen in other professional sports such as the Mets\u2013Phillies rivalry in Major League Baseball, and the Flyers\u2013Rangers, Flyers\u2013Islanders and Devils\u2013Flyers rivalries in the National Hockey League.\nWashington Commanders.\nThe Giants have an old and storied rivalry with the Washington Commanders, dating back to 1932. While this rivalry is typically given less significance than the rivalries with the Eagles and Cowboys, there have been periods of great competition between the two. In the 1980s the Giants and Redskins, as they were then known, clashed as both struggled against each other for division titles and even Super Bowl Championships. Most notable among these is the 1986 NFC Championship game in which the Giants defeated the Redskins 17\u20130 to earn their first ever trip to the Super Bowl. Wellington Mara always felt the Redskins were the Giants' oldest and truest rival, and after dying in 2005, the Giants honored their longtime owner by defeating the Redskins 36\u20130 at home. The Giants lead this series 107\u201370\u20134 as of the 2023 season. The Giants' 107 wins against the Washington Commanders are the most wins for one team against one opponent in NFL history.\nDallas Cowboys.\nThe Giants have maintained a fierce divisional rivalry with the Dallas Cowboys since the Cowboys first began play in 1960. The two teams have a combined nine Super Bowl victories between them, and have played many games in which the NFC East title was at stake. The rivalry is unique among professional sports as it is the only divisional rivalry between sports teams from New York City and Dallas, partially due to the large distance between the two cities. The Cowboys lead the regular season series 75\u201347\u20132, while the Giants hold the lone playoff victory between the two teams, held at the conclusion of the 2007 season.\nConference.\nSan Francisco 49ers.\nDespite never being in the same division, the Giants and San Francisco 49ers have developed a heated rivalry over the years. The two teams have met eight times in the playoffs (including two NFC Championship Games, both won by New York) since 1982, which is the most of any two teams in that span. In the overall series the 49ers lead 22\u201321, while the postseason series are also tied 4\u20134. Five of the eight times the Giants and 49ers have played in the postseason, the winner of their game has gone on to win the Super Bowl.\nInterconference.\nNew York Jets.\nThe Giants and New York Jets for many years had the only intracity rivalry in the NFL, made even more unusual by sharing a stadium. They have met annually in the preseason since 1969. Since 2011, this meeting has been known as the \"MetLife Bowl\", after the naming sponsor of the teams' stadium. Regular season matchups between the teams occur once every four years, as they follow the NFL scheduling formula for interconference games. Since the two teams play each other so infrequently in the regular season, some, including players on both teams, have questioned whether the Giants and Jets have a real rivalry. A memorable regular season game was in 1988, when the Giants faced off against the Jets in the last game of the season, needing a victory to make the playoffs. The Jets played spoiler, however, beating the Giants 27\u201321 and ruining the latter's playoff hopes. A different scenario unfolded during the penultimate regular season game of 2011 as the \"visiting\" Giants defeated the Jets 29\u201314. The victory simultaneously helped eliminate the Jets from playoff contention and propel the Giants to their own playoff run and eventual win in Super Bowl XLVI. The Giants lead the overall regular season series 8\u20137.\nNew England Patriots.\nThe Giants and New England Patriots rarely played each other given they were on opposite conferences, but the rivalry gained notoriety in the late 2000s thanks to some close contests and memorable moments between Tom Brady and Eli Manning. In the 2007 season, the Patriots defeated the Giants 38\u201335 to clinch a perfect 16\u20130 regular season, but could not finish a perfect 19\u20130 season in Super Bowl XLII following a 17\u201314 defeat. That game featured the now-iconic Helmet Catch from David Tyree. The Giants also defeated the Patriots in Super Bowl XLVI, a 21\u201317 victory. As of the 2023 season, the all-time series is tied 7\u20137.\nHistoric.\nChicago Bears.\nThe Giants and Chicago Bears squared off in six NFL championship games, more than any common matchup in either the NFL championship game or Super Bowl. Though the Bears won four of the six championship games, one of the Giants' two championship victories included the Sneakers Game that took place in the 1934 NFL Championship Game. The two teams also met in the 1985 and 1990 playoffs, splitting each meeting en route to a Super Bowl championship (Bears in Super Bowl XX, Giants in Super Bowl XXV). The Bears lead the all-time series 36\u201324\u20132, including a 5\u20133 postseason record.\nGreen Bay Packers.\nThe Giants\u2013Packers rivalry is a National Football League (NFL) rivalry between the New York Giants and the Green Bay Packers. The two teams have played since 1970 in the National Football Conference, and they play each other in the regular season either every three years or depending on its NFC division placement, and in the postseason, The Packers lead the all-time series 34\u201328\u20132 and postseason series 5\u20133.\nPlayers.\nPro Football Hall of Famers.\nIn the Pro Football Hall of Fame, the Giants boast the second-most enshrined members with 29. Tim Mara, Mel Hein, Pete Henry, Cal Hubbard and Jim Thorpe were a part of the original class of inductees in 1963, while defensive end Michael Strahan, the most recent Giant inducted, was a part of the Class of 2014. Numerous members, including Larry Csonka, Ray Flaherty, Joe Guyon, Pete Henry, Arnie Herber, Cal Hubbard, Tom Landry, Don Maynard, Hugh McElhenny, Jim Thorpe, and Kurt Warner were at one time associated with the New York Giants, but they were inducted largely based on their careers with other teams.\nRing of Honor.\nThe New York Giants unveiled their own Ring of Honor on October 3, 2010, during halftime of their . John Mara had long wished to create a Giants Ring of Honor and Hall of Fame to honor Giants who helped the franchise achieve each of their championships, and the building of MetLife Stadium resulted in the realization of that ambition. The organization had an inaugural induction class of 30 including players, coaches, owners and executives that have had a great impact on the organization. While the entire list of inductees was not revealed until the actual induction, the organization did confirm about a week before the ceremony that Phil Simms, Bill Parcells, Michael Strahan, Tiki Barber, Frank Gifford and Pete Gogolak would all be inducted.\nTop 100 greatest Giants of all-time.\nIn celebration of the Giants 100th season, the team announced the top 100 players in franchise history.\nNFL MVP award winners.\nThe Giants have had six players win NFL MVP in franchise history.\nSuper Bowl MVP award winners.\nThe Giants have had three players win Super Bowl MVP in franchise history.\nFirst-round draft picks.\nThe Giants have had the number one overall pick in the NFL draft two times in their history.\nCoaches.\nThe Giants have had 22 head coaches serve in the capacity.\nStat leaders.\nBold denotes still active with team\n\"Italics\" denote still active but not with team\nThese lists are accurate through the 2024 regular season.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nMedia, radio and television.\nAs of 2010, the Giants' flagship radio station is WFAN, with games simulcast on WFAN-FM as of November 2012. Since WFAN also has the rights to carry baseball, as they currently are the flagship station for the New York Yankees and previously served the same role for the New York Mets, early season Giants games come into conflict; since 2019, WFAN has split the coverage across both of its dial positions, with the Giants carried on 660 AM and the Yankees on 101.9 FM. Prior to that, the Giants' games would air on one of WFAN's designated overflow stations.\nBob Papa on play-by-play and Carl Banks on color commentary are the Giants' radio broadcast team, with Howard Cross as the sideline reporter. When Papa is unavailable to call games Chris Carrino, WFAN's lead broadcaster for the Brooklyn Nets, substitutes for him. Games are carried over the New York Giants Radio Network over various stations in New York, Pennsylvania, and Connecticut.\nPreseason telecasts not seen nationally air in the area on WNBC, with WWOR-TV serving as an overflow station for when WNBC is airing other programming such as the Summer Olympic Games. Papa and Banks call these games on television, with studio host Paul Dottino as Papa's substitute.\nWPIX-TV or WABC-TV will also air any Giants broadcast that is carried exclusively by ESPN, as per the local carriage rules (WABC-TV's corporate parent, The Walt Disney Company, holds an 80% majority ownership stake in ESPN, and has a right of first refusal for these telecasts). Thursday Night Football broadcasts, which are streamed on Amazon Prime Video, are simulcast on WNYW.\nThe Giants' public address announcer at MetLife Stadium is Gordon Deal. Deal replaced Jim Hall, who for years was Bob Sheppard's substitute at Yankee Stadium due to their very similar voices. Hall took over the Giants PA job after Sheppard elected to leave the position in 2005 to focus solely on his Yankee Stadium duties.\nPast.\nWFAN has produced the Giants' radio broadcasts since 1995, but has not always aired them on the station. For 1995, then-Giants flagship WOR continued to carry the games as they had for the previous two seasons. In 1996 the games were simulcast on WFAN and WOR, which caused some conflict as at the time, WFAN was the radio flagship of the New York Jets as well. To remedy the situation, beginning the next year WFAN moved the Giants' radio broadcasts to the FM dial and sister station WNEW-FM, where they remained until the end of the 1999 season. In 2000 WFAN lost the Jets' radio contract to WABC and the Giants moved back to WFAN where they have been ever since.\nThe Giants' longtime radio home was WNEW, where games aired from the mid-1950s until 1993 when the station was bought by Bloomberg L.P. and changed its format. Marty Glickman teamed with Al DeRogatis for a long stretch beginning in the early 1960s on WNEW. Chip Cipolla and later Sam Huff joined Glickman after DeRogatis left to join Curt Gowdy on NBC. After the WNEW split, games began airing on WOR. Glickman moved to the crosstown Jets in 1973 and was succeeded by Marv Albert. Jim Gordon succeeded Albert in 1977, beginning an 18-year tenure as the Giants' play-by-play voice. Meanwhile, Dick Lynch took over as color analyst in 1976 and continued in that role through 2007, with his last game being Super Bowl XLII, and retired following the season due to his advancing leukemia, which took his life in September 2008.\nEventually Gordon and Lynch were joined by Karl Nelson, a former lineman for the Giants. Gordon and Nelson were fired after the 1994 season, after which Papa took over the play-by-play (after being studio host) and led a two-man booth with Lynch. Dave Jennings joined the broadcast team in 2002 following his firing by the Jets, with whom he had worked since his 1987 retirement from the NFL. Jennings was moved to the pregame show after the 2006 season and was replaced by Carl Banks, leaving broadcasting altogether in 2008 due to his ongoing battle with Parkinson's disease that he lost in 2013.\nAfter WFAN began airing games Richard Neer served as pregame and postgame host. He was replaced by Sid Rosenberg, who was in turn fired by the station due to troubles and replaced by Chris Carlin. Carlin left in 2008 to focus full-time on his duties as SNY studio host and Rutgers athletics radio voice and was replaced by WWOR sports reporter and former WFAN host Russ Salzberg, who cohosted with Roman Oben after Jennings left. WEPN Giants beat reporter Paul Dottino was hired by WFAN to host the pregame show for 2009 and continues to be a part of the program. As of the 2020 season, Lance Medow is the host for the pregame show as well as halftime and postgame, with former Giants punter Jeff Feagles as analyst.\nThe Giants were carried on the DuMont Network, then CBS in the early TV days of the NFL, when home games were blacked out within a 75-mile (120.7 km \ud83c\uddea\ud83c\uddfa) radius of New York City. Chris Schenkel was their play-by-play announcer in that early era when each team was assigned its own network voice on its regional telecasts. At the time, there were few if any true national telecasts until the NFL championship game, which was carried by NBC. Schenkel was joined by Jim McKay, later Johnny Lujack through the 1950s and the early 1960s. As Giants players retired to the broadcast booth in the early and 1960s, first Pat Summerall, then Frank Gifford took the color analyst slot next to Schenkel. As the 1970 merger of the NFL and AFL approached, CBS moved to a more generic announcer approach and Schenkel was off the broadcasts.\nGiants regular season Sunday telecasts moved to Fox when that network took over NFC telecasts in 1994 and are carried locally by WNYW.\nWCBS-TV and WPIX were previously home to Giants preseason telecasts in the 1990s, with WPIX serving as the Giants' (and Jets') long-time preseason home. After the NFC rights were lost by CBS, the Giants followed the conference's broadcast rights to WNYW. WWOR became the Giants' flagship TV station in the late 1990s, and stayed so up until WNBC took over rights in 2005.\nWhen the Giants first moved to WNYW, Mike Breen was their preseason play-by-play man. Sam Rosen was the television voice for some time afterward, except for two years when Curt Menefee (then of WNYW) was the voice. When the games moved to WWOR, Rosen regained the position and held it until 2004. Former Giant receiver Phil McConkey became the early season analyst after his retirement and stayed in the booth for many years.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nrole=\"presentation\" class=\"wikitable succession-box noprint\" style=\"margin:0.5em auto; font-size:small;clear:both;\""}
{"id": "21758", "revid": "1896838", "url": "https://en.wikipedia.org/wiki?curid=21758", "title": "November 6", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 6 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21759", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=21759", "title": "November 8", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 8 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21760", "revid": "22986354", "url": "https://en.wikipedia.org/wiki?curid=21760", "title": "November 10", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 10 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21761", "revid": "754658", "url": "https://en.wikipedia.org/wiki?curid=21761", "title": "November 13", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 13 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21762", "revid": "39508241", "url": "https://en.wikipedia.org/wiki?curid=21762", "title": "November 14", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 14 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21763", "revid": "718057", "url": "https://en.wikipedia.org/wiki?curid=21763", "title": "November 15", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 15 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21764", "revid": "44935001", "url": "https://en.wikipedia.org/wiki?curid=21764", "title": "November 3", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 3 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21765", "revid": "38004052", "url": "https://en.wikipedia.org/wiki?curid=21765", "title": "New Malden", "text": "Suburb of London\nNew Malden is a suburban area in southwest London, England. It is within the Royal Borough of Kingston upon Thames and the London Borough of Merton, and is from Charing Cross. Neighbouring localities include Kingston, Kingston Vale, Roehampton, Norbiton, Raynes Park, Coombe, Tolworth, Motspur Park, Old Malden, and Worcester Park. Before the creation of Greater London in 1965, New Malden was in the administrative county of Surrey.\nHistory.\nNew Malden was established as a result of the arrival of the railway. What is now New Malden railway station was opened on 1 December 1846 on the main line from London Waterloo.\nBuilding started slowly in the area just to the north of the station, gathering pace in the late nineteenth and early twentieth centuries with two- and three-bedroom terraced houses. Further out towards Coombe Hill are larger detached and semi-detached houses built in the 1930s. The name of the road which leads up the hill to Coombe, Traps Lane, is thought to derive from a farm owned by a Mrs Trap. Following the opening of the Kingston bypass in 1927, the farms to its south were progressively developed for housing.\nTwo miles (3\u00a0km) to the south is the former village of Old Malden the origins of which are Anglo-Saxon, the name being Old English for \"M\u00e6l\" + \"duna\" = \"the cross on the hill\".\nUnder the District Councils Act 1895, The Maldens &amp; Coombe Urban District Council was created (the plural relating to Old Malden and New Malden). In 1936 Malden and Coombe was granted full Borough status, with its own Mayor, and had the rare distinction of a civic mace bearing the royal insignia of King Edward VIII.\nNew Malden suffered damage from German bombing during the Second World War. The first attack took place on 16 August 1940, killing about 50 people and damaging about 1,300 homes. After dropping about 150 bombs, German aircraft reportedly flew over the railway station at low altitude and machine-gunned passengers as they disembarked from a train. Unexploded munitions from this period are still found on occasion.\nIn 1965, the London Government Act 1963 came into force merging the boroughs of Malden &amp; Coombe and Surbiton with Kingston upon Thames to form the Royal Borough of Kingston upon Thames.\nNew Malden contains offices of several large organisations, including Northrop Grumman in Burlington Road. Nestl\u00e9 Purina Pet Foods (before 1997 Spillers Pet Foods) was located in New Malden until 2012 when Nestl\u00e9 moved its UK headquarters to Gatwick.\nDescription.\nNew Malden is bounded to the north by the affluent Coombe and to the south and east by Raynes Park, Tolworth and Worcester Park. New Malden includes Motspur Park, home to the training ground of Fulham FC, and also the King's College London sports ground, home to the training ground of AFC Wimbledon.\nThe busy A3 trunk road runs through part of New Malden. A minor tributary of the River Thames, Beverley Brook, flows through the east of the town, while its western boundary is along the Hogsmill, another Thames tributary.\nThe first parking meters were made in New Malden at Venners Ltd.\nPolitics.\nNew Malden is mostly part of the Kingston and Surbiton constituency for elections to the House of Commons of the United Kingdom.\nNew Malden is part of the New Malden Village ward for elections to Kingston upon Thames London Borough Council.\nDemographics.\nKorean community.\nThe Royal Borough of Kingston upon Thames has a large expatriate communities of South Koreans in Europe. According to different sources, as of 2014 there were about 10,000 ethnic Koreans in New Malden proper, and as of the same year the Korean population in the area around New Malden is around 20,000, including about 600 originating from North Korea, giving it the largest group of North Koreans in Europe. In the 2001 census, some small areas of New Malden had \"Other Asian\" (i.e., not Indian, Pakistani, Bangladeshi, or Chinese) populations of \"over 25%\", though no whole ward reached over 20%. Many of the Koreans living in New Malden work for Korean companies, and they are either permanently settled and formerly expatriate, or they are still expatriates. According to some journalists, it is often referred as 'Korea Town' or 'Little Korea'.\nThe New Malden area has Korean language churches and nursery schools as well as restaurants and shops with Korean clientele. New Malden functions as the shopping and cultural centre for a Korean population spread more widely across South-West London and the neighbouring counties. The area has Korean supermarkets, about 20 Korean restaurants and cafes, including those serving bulgogi. It also has a noraebang (Karaoke bar), and many other shops. The Korean language is visible on several shop signs. The original Embassy of South Korea was in New Malden, before moving to 60 Buckingham Gate in Westminster.\nSome factors cited in \"The Daily Telegraph\" as reasons why the Korean community formed in New Malden included a 1950s joint venture partnership between a chaebol and Racal Avionics (formerly Decca), Lord Chancellor's Walk in Coombe Lane West previously serving as the residence of the Ambassador of South Korea to the United Kingdom, and Samsung Electronics having its UK offices in New Malden until they moved to their current location in Chertsey, Surrey in 2005. Many Koreans settled in New Malden in the 1970s due to the ambassador's location.\nOther.\nThere is a Hindu temple in the eastern part of Burlington Road with a notable community of predominantly Sri Lankan Tamils living in the area. In 2016 New Malden gained twin city status with Jaffna, Sri Lanka and a permanent plaque was erected to celebrate this.\nAmenities.\nNew Malden has its own sports centre, the Malden Centre, which includes a swimming pool, gym and community facilities. It also runs several adult learning courses.\nTudor Williams Ltd, established in 1913 but closed in 2019, was a family run department store in the High Street. The company also has shops in Cobham and Dorking and expanded by acquiring department stores Elphicks of Farnham in October 2004, and Knights of Reigate in September 2006. A branch of Waitrose is one of a number of other well known stores in the High Street.\nPubs in New Malden include The Glasshouse (formerly The Railway), adjacent to the train station; The Royal Oak, north of the station on Coombe Road; Woodies Freehouse on Thetford Road, and The Watchman, located at the roundabout in a building constructed in the 1890s which was originally a police station. The Fountain pub, once located at the roundabout, closed in 2018 to make way for affordable housing.\nThe local newspapers are the \"Surrey Comet\" which has been in print since 1854, \"Coombe Monthly\", and the \"Kingston Guardian\". A monthly publication, \"The Village Voice\", covers local history, news, topical articles and advertisements for businesses serving the community.\nThere is an annual Malden Fortnight, which includes a parade showcasing all the local schools and community groups and various other activities.\nEach Christmas the High Street is festooned with Christmas lights with its own switching-on ceremony.\nNew Malden has a youth theatre, the Green Theatre Company, established in 1986 in a converted cricket pavilion at Barton Green.\nThe area's last surviving cinema, the Odeon at Shannon Corner on the A3 was replaced by a large retail area including several large stores. The other cinema in the High Street (corner of Sussex Road) burnt down on Boxing Day 1936. There was also a silent cinema on Coombe Road by the station, which became the New Malden Gentlemen's Club in 1923; this closed in August 2010, and is now a Korean karaoke and pool bar.\nNew Malden also has its own \"Dino-Golf\" course, 18 holes of dinosaur themed crazy golf overlooking the A3, as well as a floodlit golf driving range.\nLarge B&amp;Q, Currys and Tesco stores are situated away from the High Street, which focuses more on smaller, more upmarket shops and restaurants.\nNew Malden is home to the playing fields of both King's College London and the London School of Economics, which are available for hire when not in use by university teams.\nEducation and schools.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nTransport.\nRail.\nNew Malden railway station has services provided by South Western Railway to London Waterloo, Hampton Court, Kingston, Richmond and Shepperton. It is in London Zone 4. The Old Malden area is well served by trains from Malden Manor railway station, travelling north to London Waterloo and south to Chessington. Motspur Park railway station on the New Malden/Raynes Park borders also has rail connections to Chessington South, Epsom, Leatherhead and Dorking.\nBus.\nThere are many routes of London Buses going through New Malden, including route 213 route going from Kingston towards Sutton, routes 131 and N87 going through Kingston Town Centre and Tooting Broadway (and Aldwych for the night bus) along with the SL7 express bus to Croydon and Heathrow Airport, route 152 from New Malden towards Pollards Hill and route 265 towards Tolworth, Roehampton and Putney. The town also has a series of local bus routes, including K1 which goes to Kingston and New Malden station and K5 to Ham and Morden.\nNotable residents.\nNotable former or current residents include:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21766", "revid": "17216044", "url": "https://en.wikipedia.org/wiki?curid=21766", "title": "Modern liberalism", "text": "Modern liberalism may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "21767", "revid": "2300502", "url": "https://en.wikipedia.org/wiki?curid=21767", "title": "NAVSPASUR", "text": ""}
{"id": "21768", "revid": "1175960", "url": "https://en.wikipedia.org/wiki?curid=21768", "title": "Air Force Space Surveillance System", "text": "U.S. government radar system to detect orbital objects\nThe AN/FPS-133 Air Force Space Surveillance System, colloquially known as the Space Fence, was a U.S. government multistatic radar system built to detect orbital objects passing over the United States. It was a component of the U.S. Space Surveillance Network, and according to the U.S. Navy was able to detect basketball sized () objects at heights up to .\nThe system ceased operation in September 2013. Plans for a new Space Fence began with sites at the Kwajalein Atoll in the Marshall Islands, along with an option for another radar site in Western Australia. It became operational on March 28, 2020.\nThe operation's headquarters were at Dahlgren, Virginia, and radar stations were spread out across the continental United States at roughly the level of the 33rd parallel north.\nDescription.\nThere were three transmitter sites in the system:\nThe master transmitter at Lake Kickapoo was said to be the most powerful continuous wave (CW) station in the world, at 768\u00a0kW radiated power on 216.97927\u00a0MHz.\nWhen the system became operational in 1961, the original frequency was 108.50\u00a0MHz (just above the FM broadcast band). In 1965, the \"Fence\" system was modernized with the operating frequency doubled to 216.98\u00a0MHz (just above Channel 13 in the VHF TV broadcast band) to obtain higher resolution and to locate smaller objects. This frequency was used until the Fence was decommissioned in 2013. Fill-in transmitter sites at Gila River and Jordan Lake used offset frequencies listed above from the early 1990s to 2013 to help better detect which transmitter \"illuminated\" an object in space, as multiple transmitters could have illuminated the same object at the same time. Overhead imagery (see coordinates given above) of the Gila River and Jordan Lake sites shows the original design at the lower frequency.\nThere were six receiving stations:\nThe following receiving stations were placed in cold storage in April 2013:\nThe receiving stations at Elephant Butte and Hawkinsville were considered to be \"High Altitude\" stations with longer and more complex antenna systems that are designed to see targets at higher altitudes than the other four receiving stations.\nHistory.\nAuthor Curtis Peebles notes that the original \"Space Fence\" or Space Surveillance System began operations in 1959. The system predated the formation of NORAD and was known as the U.S. Navy Space Surveillance System (or SPASUR or NAVSPASUR). From 1960 until the early 1990s the system was used in conjunction with a network of Baker-Nunn cameras that could see \"an object the size of a basketball at \".\nThe system was formerly operated by the U.S. Navy for NORAD from 1961 until October 2004. Initially independent as NAVSPASUR, it was run by Naval Space Command from 1993, and finally by Naval Network and Space Operations Command from 2002 until command was passed to the U.S. Air Force 20th Space Control Squadron on 1 October 2004.\nIn 2009, the operations and maintenance contract for the day-to-day management and operation of the Fence was awarded to Five Rivers Services, LLC, based in Colorado Springs, Colorado. On 30 September 2011, Five Rivers Services was awarded a US$7,022,503 firm fixed price with cost reimbursable line items contract modification to manage, operate, maintain, and logistically support the nine Air Force Space Surveillance System field stations, presumably for Fiscal Year 2012.\nPlans for system upgrade: 2009 \u2013 2012.\nThe 850th Electronic Systems Group, Electronic Systems Center awarded 3 US$30-million contracts to Lockheed Martin, Northrop Grumman and Raytheon Technologies on 11 June 2009.\nA new Space Fence is envisioned to be a system of two or three S-band ground-based radars designed to perform uncued detection, tracking and accurate measurement of orbiting space objects. The Space Fence is intended to replace the Air Force Space Surveillance System, or VHF Fence, that was transferred from the U.S. Navy to the U.S. Air Force in 2004. The shorter wavelength of the S-band Space Fence allows for detection of much smaller satellites and debris.\nThe 10 February 2009, collision of a U.S. Iridium communications satellite (Iridium 33) and a Russian Cosmos 2251 communications satellite, which added hundreds more pieces of debris to the atmosphere, highlighted the need for more precise tracking of space objects.\nData collected from a new Space Fence's sensors would potentially feed into the Joint Space Operations Center Mission System, which is used to track objects orbiting the Earth, monitor space weather and assess foreign launches. Used by operators at the 614th Air and Space Operations Center at Vandenberg Air Force Base, California, the 614 AOC's 24-hour-a-day, seven-day-a-week support provides vigilance of global and theater operations and equips the Joint Functional Component Command for space operations with the tools to conduct command and control of space forces.\nPlans to award the final contract had been stalled by U.S. budget sequestration in early 2013 and the AFSSS system was scheduled to be discontinued in October 2013 due to budget cuts.\n2013 Shutdown.\nOn 1 August 2013, General William L. Shelton, commander of Air Force Space Command, directed that the Air Force Space Surveillance System (AFSSS) be closed and all sites vacated effective 1 October 2013. The main advantage of the system was its ability to provide uncued data on new objects as opposed to tracking objects based on existing information. However, the system was also said to be inherently inaccurate due to its dated design. Alternate operating modes for radars at Cavalier Space Force Station and Eglin AFB were devised to fulfill the mission to provide uncued data for new objects. Shelton also noted the confusion between the planned new S-band space fence and the old UHF AFSSS, which was commonly called the \"space fence\". The AFSSS was turned off September first. \"It appears they pulled the plug at 00:00 UTC (6 a.m. Local MDT) on September 1st\", reports engineer Stan Nelson, who was monitoring the radar using an antenna in Roswell. The radar's final echoes came from a Russian satellite and a sporadic meteor\". The shutdown only affects the original Space Fence, not the new one contracted to be built by Lockheed Martin for deployment in Australia and the Marshall Islands.\nNew space fence.\nA new space fence at Kwajalein Atoll in the Marshall Islands was declared operational on March 27, 2020. In 2014 Lockheed Martin won the contract to build the new S band space fence system at Kwajelein with an option for another radar site in Western Australia.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21770", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=21770", "title": "National Film Preservation Board", "text": "American government board\nThe United States National Film Preservation Board (NFPB) is the board selecting films for preservation in the Library of Congress' National Film Registry. It was established by the National Film Preservation Act of 1988. The National Film Registry is meant to preserve up to 25 \"culturally, historically or aesthetically significant films\" each year; to be eligible, films must be at least 10 years old. Members of the Board also advise the Librarian of Congress on ongoing development and implementation of the national film preservation plan.\nThe NFPB is a federal agency located within the Library of Congress. The NFPB was established by the National Film Preservation Act of 1988, and reauthorized in 1992, 1996 and 2005. The 1996 reauthorization also created the non-profit National Film Preservation Foundation, which is loosely affiliated with the National Film Preservation Board, but the private-sector Foundation (NFPF) and federal Board (NFPB) are separate, legally distinct entities.\nOrganization.\nThe board is appointed by the Librarian of Congress and is composed of representatives from professional organizations representing the film industry, archives, scholars, filmmakers and others who comprise the diverse American motion picture community. Explicitly it is composed of up to 5 \"at-large\" members (with 5 alternates) and 17 member/alternate pairs from the following 18 organizations:\nRelationship with National Film Preservation Foundation.\nThe National Film Preservation Foundation was created by the U.S. Congress in 1996, at the recommendation of the Library of Congress, following four years of hearings and research conducted by the National Film Preservation Board. The National Film Preservation Act of 1996 (Public Law 104\u2013285, Title II), signed into law on October 11, 1996 by President Bill Clinton, charged the NFPF to \"encourage, accept, and administer private gifts to promote and ensure the preservation and public accessibility of the nation's film heritage\" and authorized federal funds to advance this work. The NFPF started operations a year later in 1997 as an independent federally chartered grant-giving public charity and the nonprofit charitable affiliate of the Library of Congress's National Film Preservation Board. Since 1996 Congress has increased the NFPF's authorization twice, in 2005 via the \"Family Entertainment and Copyright Act of 2005\" (Public Law 109-9) and in 2008 via the \"Library of Congress Sound Recording and Film Preservation Programs Reauthorization Act of 2008\" (Public Law 110-336). Funding received through the NFPF's authorization is secured through the Library of Congress and goes directly to the field for film preservation projects.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21774", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=21774", "title": "Nominative case", "text": "Grammatical case\nIn grammar, the nominative case (abbreviated NOM), subjective case, straight case, or upright case is one of the grammatical cases of a noun or other part of speech, which generally marks the subject of a verb, or (in Latin and formal variants of English) a predicative nominal or adjective, as opposed to its object, or other verb arguments. Generally, the noun \"that is doing something\" is in the nominative, and the nominative is often the form listed in dictionaries.\nEtymology.\nThe English word \"nominative\" comes from Latin \"c\u0101sus nomin\u0101t\u012bvus\" \"case for naming\", which was translated from Ancient Greek \u1f40\u03bd\u03bf\u03bc\u03b1\u03c3\u03c4\u03b9\u03ba\u1f74 \u03c0\u03c4\u1ff6\u03c3\u03b9\u03c2, \"onomastik\u1e17 pt\u00f4sis\" \"inflection for naming\", from \"onom\u00e1z\u014d\" \"call by name\", from \"\u00f3noma\" \"name\". Dionysius Thrax in his The Art of Grammar refers to it as \"orth\u0113\u0301\" or \"euthe\u00eea\" \"straight\", in contrast to the oblique or \"bent\" cases.\nCharacteristics.\nThe reference form (more technically, the \"least marked\") of certain parts of speech is normally in the nominative case, but that is often not a complete specification of the reference form, as the number and the gender may need to be specified. Thus, the reference or least marked form of an adjective might be the nominative masculine singular.\nThe parts of speech that are often declined and therefore may have a nominative case are nouns, adjectives, pronouns and (less frequently) numerals and participles. The nominative case often indicates the subject of a verb but sometimes does not indicate any particular relationship with the other parts of a sentence. In some languages, the nominative case is unmarked, and it may then be said to be marked by a null morpheme. Moreover, in most languages with a nominative case, the nominative form is the lemma; that is, it is the reference form used to cite a word, to list it as a dictionary entry etc.\nNominative cases are found in Albanian, Arabic, Estonian, Sanskrit, Slovak, Ukrainian, Hungarian, Lithuanian, Georgian, German, Latin, Greek, Icelandic, Old English, Old French, Polish, Serbian, Czech, Romanian, Russian and Pashto, among other languages. English still retains some nominative pronouns, which are contrasted with the accusative (comparable to the oblique or disjunctive in some other languages): \"I\" (having the accusative \"me\"), \"we\" (having the accusative \"us\"), \"he\" (having the accusative \"him\"), \"she\" (having the accusative \"her\"), \"they\" (having the accusative \"them\") and \"who\" (having the accusative \"whom\"). A usage that is archaic in most current English dialects is the singular second-person pronoun \"thou\" (accusative \"thee\"). A special case is the word \"you\": originally, \"ye\" was its nominative form and \"you\" the accusative, but over time, \"you\" has come to be used for the nominative as well.\nThe term \"nominative case\" is most properly used in the discussion of nominative\u2013accusative languages, such as Latin, Greek and most modern Western European languages.\nIn active\u2013stative languages, there is a case, sometimes called nominative, that is the most marked case and is used for the subject of a transitive verb or a voluntary subject of an intransitive verb but not for an involuntary subject of an intransitive verb. Since such languages are a relatively new field of study, there is no standard name for this case.\nSubjective case.\nEnglish is now often described as having a subjective case, instead of a nominative, to draw attention to the differences between the \"standard\" generic nominative and the way that it is used in English. The term objective case is then used for the oblique case, which covers the roles of accusative, dative and objects of a preposition. The genitive case is then usually called the \"possessive\" form, rather than a noun case \"per se\". English is then said to have two cases: the subjective and the objective.\nExamples.\nSubject.\nThe nominative case marks the subject of a verb. When the verb is active, the nominative is the person or thing doing the action (agent); when the verb is passive, the nominative is the person or thing receiving the action.\nPredicate noun or adjective.\nIn copular sentences, the nominative is used for both subject and predicate.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21775", "revid": "40123752", "url": "https://en.wikipedia.org/wiki?curid=21775", "title": "Nobel prize", "text": ""}
{"id": "21777", "revid": "44920675", "url": "https://en.wikipedia.org/wiki?curid=21777", "title": "Neapolitan sauce", "text": "Tomato-based sauce derived from Italian cuisine\nNeapolitan sauce is the collective name given (outside Italy) to various basic tomato-based sauces derived from Italian cuisine, often served over or alongside pasta.\nIn Naples, Neapolitan sauce is simply referred to as salsa, which literally translates to 'sauce'. Basil, bay leaf, thyme, oregano, peppercorns, cloves, olives, and mushrooms may be included depending on taste preferences. Some variants include carrots and celery. Outside Italy, the basic sauce is vegetarian, although meat such as minced beef or sausage can be added. By contrast, in Italy, the sauce dish carrying Naples in its name is a sauce called Neapolitan rag\u00f9.\nOrigin.\nHistorically, the first Italian cookbook to include a tomato based sauce, \"Lo Scalco alla Moderna\" (\"The Modern Steward\"), was written by Italian chef Antonio Latini and was published in two volumes in 1692 and 1694. Latini served as the Steward of the First Minister to the Spanish Viceroy of Naples.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21780", "revid": "34477066", "url": "https://en.wikipedia.org/wiki?curid=21780", "title": "NBC", "text": "American broadcast television network\nThe National Broadcasting Company (NBC) is an American commercial broadcast television and radio network, serving as the flagship property of NBC Entertainment, a division of NBCUniversal, which is a subsidiary of Comcast. It is one of NBCUniversal's two main flagship subsidiaries, alongside Universal Studios. It is the first and oldest major broadcast programming network in the United States.\nNBC's headquarters are located in New York City at Rockefeller Center's Comcast Building, the network's longtime home. The network's predecessor parent companies were integral to the center's construction. NBC also notably has offices at the NBC Tower in Chicago, Illinois, and at 10 Universal City Plaza in Los Angeles, California.\nFounded in 1926 by the Radio Corporation of America, later formally owned by General Electric (GE), Westinghouse, AT&amp;T Corporation, and United Fruit Company, NBC is the oldest of the traditional \"Big Three\" American television networks in the 21st century (along with ABC and CBS) and is sometimes often referred to as the Peacock Network in reference to its stylized peacock logo, which was introduced in 1956 to promote the company's innovations in early color broadcasting.\nNBC has twelve owned-and-operated stations and has affiliates in almost every TV market in the United States. Some of the stations are also available in Mexico, the Caribbean, and Canada, via pay-television providers or in border areas over the air.\nThe Peacock Network's corporate name was changed from National Broadcasting Company Inc., to NBC Universal Inc. along with the merger with the Vivendi Universal's entertainment division, and in 1986, NBC's controlled power was passed on to General Electric's $4.6 billion purchase with RCA.\nHistory.\nNBC was formed in 1926 by the Radio Corporation of America (RCA) as the first and oldest major broadcast network in the United States. NBC was then owned by General Electric (GE), Westinghouse, AT&amp;T Corporation and United Fruit Company. In 1932, the U.S. government forced GE to sell RCA and NBC due to antitrust violations. In late 1986, GE regained control of RCA through its $6.4 billion purchase of the company. Although it retained NBC, GE immediately closed or sold off most of RCA's other divisions and assets.\nIn 2004, French media company Vivendi merged its entertainment assets with GE, forming NBCUniversal. Comcast purchased a controlling interest in NBCUniversal in 2011 and acquired GE's remaining stake in 2013.\nNBC is the home broadcaster of some of the longest continuously running American television series, including the news program \"Meet the Press\" (debuted 1947); \"Today\" (debuted 1952); \"The Tonight Show\" (debuted nationally 1954); and \"Saturday Night Live\" (debuted 1975). The drama series \"\", which debuted in 1999, began its 26th season in October 2024 and is currently the longest-running live-action series in American prime-time television history.\nProgramming.\nAs of 2025[ [update]], NBC provides 87 hours of regularly scheduled network programming each week. The network provides 22 hours of prime-time programming to affiliated stations Monday through Saturdays from 8:00\u00a0p.m. to 11:00\u00a0p.m. Eastern and Pacific Time (7:00\u00a0p.m.\u201310:00\u00a0p.m. in all other U.S. time zones) and Sundays from 7:00\u00a0p.m. to 11:00\u00a0p.m. Eastern and Pacific Time (6:00\u00a0p.m.\u201310:00\u00a0p.m. in all other time zones).\nDaytime NBC News programming includes the morning news/interview program \"Today\" from 7:00\u00a0a.m. to 11:00\u00a0a.m. weekdays, 7:00\u00a0a.m.\u20138:30\u00a0a.m. / 8:00\u00a0a.m. - 9:30\u00a0a.m. on Saturdays and 7:00\u00a0a.m.\u20138:00\u00a0a.m. / 8:00\u00a0a.m. -9:00\u00a0a.m. on Sundays, it also airs \"NBC News Daily\" at 12:00\u00a0p.m.\u20131:00\u00a0p.m. on weekdays, it includes nightly editions of \"NBC Nightly News\", the Sunday political talk show \"Meet the Press\", weekday early-morning news program \"Early Today\" and primetime newsmagazine \"Dateline NBC on Friday nights\". Late nights feature the weeknight talk shows \"The Tonight Show Starring Jimmy Fallon\", \"Late Night with Seth Meyers\", and an overnight replay of \"Today with Jenna &amp; Friends.\" NBC affiliates carrying it in syndication also have the option to substitute a same-day encore of \"The Kelly Clarkson Show\" on weekdays. On Saturdays, the LXTV-produced \"1st Look\" and \"Open House NYC\" air after \"Saturday Night Live\" (replays of the previous week's \"1st Look\" also air on Friday late nights on most stations), with a \"Meet the Press\" encore a part of its Sunday overnight schedule.\nThe network's weekend morning children's programming time slot is programmed by Litton Entertainment under a time-lease agreement. The three-hour block of programming designed mainly for 14-16-year-old teenage viewers is under the umbrella branding of \"The More You Know\", based on the network's long-time strand of internally-produced public service announcements of the same name. It premiered on October 8, 2016, giving Litton control of all but Fox's Weekend morning E/I programming among the five major broadcast networks.\nLive sports programming is also provided on weekends at any time between 7:00\u00a0a.m. and 1:30\u00a0a.m. Eastern Time, but most commonly between 12 p.m. and 6 p.m. Eastern (and from 7 and 11 p.m. on some Saturdays and all Sundays since Fall 2025 as part of its Sunday Year round sports programming block). Due to the unpredictable length of sporting events, NBC will occasionally pre-empt scheduled programs (more common with the weekend editions of \"NBC Nightly News\", and local and syndicated programs carried by its owned-and-operated stations and affiliates). NBC has also held the American broadcasting rights to the Summer Olympic Games since the 1988 games and the rights to the Winter Olympic Games since the 2002 games. Coverage of the Olympics on NBC has included pre-empting regularly scheduled programs during daytime, prime time, and late night. In July 2022, NBC announced that the Olympic Channel will be shut down on September 30. NBC stated they will be announcing the plans for Olympic content in the fall of 2022.\nNBC News.\nNews coverage has long been an important part of NBC's operations and public image, dating to the network's radio days. Notable NBC News productions past and present include \"Today\", \"NBC Nightly News\" (and its immediate predecessor, \"The Huntley\u2013Brinkley Report\"), \"Meet the Press\" (which has the distinction of the longest continuously running program in the history of American television), \"Dateline NBC\", \"Early Today\", \"NBC News at Sunrise\", \"NBC Nightside\" and \"Rock Center with Brian Williams\".\nIn 1989, the news division began its expansion to cable with the launch of the business news channel CNBC. The company eventually formed other cable news services including MSNBC (created in 1996 originally as a joint venture with Microsoft, which now features a mix of general news and political discussion programs with a liberal stance), and the 2008 acquisition of The Weather Channel in conjunction with Blackstone Group and Bain Capital. In addition, NBCSN (operated as part of the NBC Sports Group, which became an NBC property through Comcast's acquisition of NBCUniversal) carries sports news content alongside sports event telecasts. Key anchors from NBC News are also used during NBC Sports coverage of the Olympic Games.\nFormer Daytime programming block.\nWhile NBC has aired a variety of soap operas on its daytime schedule over its history, \"Days of Our Lives\" (1965\u20132022) was the last soap opera on the network when it was taken off the air in 2022 (and moved to the Peacock streaming service). Currently the network only offers \"NBC News Daily\" on its afternoon schedule, with affiliates using the rest of the afternoon for syndicated or local programming.\nLong-running daytime dramas seen on NBC in the past include \"The Doctors\" (1963\u20131982), \"Another World\" (1964\u20131999), \"Santa Barbara\" (1984\u20131993), and \"Passions\" (1999\u20132007). NBC also aired the final 4&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 years of \"Search for Tomorrow\" (1982\u20131986) after that series was initially cancelled by CBS, although many NBC affiliates did not clear the show during its tenure on the network. NBC has also aired numerous short-lived soap operas, including \"Generations\" (1989\u20131991), \"Sunset Beach\" (1997\u20131999), and the two \"Another World\" spin-offs, \"Somerset\" (1970\u20131976) and \"Texas\" (1980\u20131982).\nNotable daytime game shows that once aired on NBC include \"The Price Is Right\" (1956\u20131963), \"Concentration\" (1958\u20131973; and 1987\u20131991 as \"Classic Concentration\"), \"The Match Game\" (1962\u20131969), \"Let's Make a Deal\" (1963\u20131968 and 1990\u20131991, as well as a short-lived prime-time revival in 2003), \"Jeopardy!\" (1964\u20131975 and 1978\u20131979), \"The Hollywood Squares\" (1966\u20131980), \"Wheel of Fortune\" (1975\u20131989 and 1991), \"Password Plus/Super Password\" (1979\u20131982 and 1984\u20131989), \"Sale of the Century\" (1969\u20131973 and 1983\u20131989) and \"Scrabble\" (1984\u20131990 and 1993). The last game show ever to air as part of NBC's daytime schedule was the short-lived \"Caesars Challenge\", which ended in January 1994.\nNotable past daytime talk shows that have aired on NBC have included \"Home\" (1954\u20131957), \"The Ernie Kovacs Show\" (1955\u20131956), \"The Merv Griffin Show\" (1962\u20131963), \"Leeza\" (1994\u20131999) and \"Later Today\" (1999\u20132000).\nChildren's programming.\nChildren's programming has played a part in NBC's programming since its initial roots in television. NBC's first major children's series, \"Howdy Doody\", debuted in 1947 and was one of the era's first breakthrough television shows. From the mid-1960s until 1992, the bulk of NBC's children's programming was composed of mainly animated programming including classic \"Looney Tunes\" and \"Woody Woodpecker\" shorts; reruns of prime time animated sitcoms such as \"The Flintstones\" and \"The Jetsons\"; foreign acquisitions like \"Astro Boy\" and \"Kimba the White Lion\"; animated adaptions of \"Punky Brewster\", \"ALF\" and \"Star Trek\" as well as animated vehicles for Gary Coleman and Mr. T; live-action programs like \"The Banana Splits\", \"The Bugaloos\" and \"H.R. Pufnstuf\"; and the original broadcasts of \"Gumby\", \"The Rocky and Bullwinkle Show\", \"Underdog\", \"The Smurfs\", \"Alvin and the Chipmunks\" and \"Disney's Adventures of the Gummi Bears\". From 1984 to 1989, the network aired a series of public service announcements called \"One to Grow On\", which aired after the end credits of every program or every other children's program.\nIn 1989, NBC premiered \"Saved by the Bell\", a live-action teen sitcom which originated on The Disney Channel the previous year as \"Good Morning, Miss Bliss\" (which served as a starring vehicle for Hayley Mills; four cast members from that show were cast in the NBC series as the characters they originally played on \"Miss Bliss\"). \"Saved by the Bell\", despite being given bad reviews from television critics, would become one of the most popular teen series in television history as well as the top-rated series on Saturday mornings, dethroning ABC's \"The Bugs Bunny and Tweety Show\" in its first season.\nThe success of \"Saved by the Bell\" led NBC to remove animated series from its Saturday morning lineup in August 1992 in favor of additional live-action series as part of a new block called TNBC, along with the debut of a Saturday edition of \"Today\". Most of the series featured on the TNBC lineup were executive produced by Peter Engel (such as \"City Guys\", \"Hang Time\", \"California Dreams\", \"One World\" and the \"Saved by the Bell\" sequel, \"\"), with the lineup being designed from the start to meet the earliest form of the FCC's educational programming guidelines under the Children's Television Act. \"NBA Inside Stuff\", an analysis and interview program aimed at teens that was hosted for most of its run by Ahmad Rashad, was also a part of the TNBC lineup during the NBA season until 2002 (when the program moved to ABC as a result of that network taking the NBA rights from NBC).\nIn 2002, NBC entered into an agreement with Discovery Communications to carry educational children's programs from the Discovery Kids cable channel. Debuting that September, the Discovery Kids on NBC block originally consisted exclusively of live-action series, including reality series \"Trading Spaces: Boys vs. Girls\" (a kid-themed version of the TLC series \"Trading Spaces\"); the Emmy-nominated reality game show \"Endurance\", hosted and produced by J. D. Roth (whose production company, 3-Ball Productions, would also produce reality series \"The Biggest Loser\" for NBC beginning in 2003); and scripted series such as \"Strange Days at Blake Holsey High\" and \"Scout's Safari\". The block later expanded to include some animated series such as \"Kenny the Shark\", \"Tutenstein\" and \"Time Warp Trio\".\nIn May 2006, NBC announced plans to launch a new Saturday morning children's block under the Qubo brand in September 2006. An endeavor originally operated as a joint venture between NBCUniversal, Ion Media Networks, Scholastic Press, Classic Media and Corus Entertainment's Nelvana unit (Ion acquired the other partners' shares in 2013), the Qubo venture also encompassed weekly blocks on Telemundo and Ion Television, a 24-hour digital multicast network on Ion's owned-and-operated and affiliated stations, as well as video on demand services and a branded website. Qubo launched on NBC on September 9, 2006, with six programs (\"VeggieTales\", \"Dragon\", \"VeggieTales Presents: 3-2-1 Penguins!\", \"Babar\", \"Jane and the Dragon\" and \"Jacob Two-Two\").\nOn March 28, 2012, it was announced that NBC would launch a new Saturday morning preschool block programmed by Sprout (originally jointly owned by NBCUniversal, PBS, Sesame Workshop and Apax Partners, with the former acquiring the other's interests later that year). The block, NBC Kids, premiered on July 7, 2012, replacing the \"Qubo on NBC\" block.\nOn February 24, 2016, it was announced that NBC would launch a new Saturday morning block programmed by Litton Entertainment under the Children's Television Act. It's called The More You Know, inspired by the name of brand extension of \"The More You Know\"\u2014a series of public service campaigns first launched by NBC in 1989. The block premiered on October 8, 2016, replacing NBC Kids block (originally October 1, 2016, but postponed due to the NBC network coverage of the 2016 Ryder Cup).\nSpecials.\nNBC holds the broadcast rights to several annual specials and award show telecasts, including the Golden Globe Awards and the Primetime Emmy Awards (which are rotated across all four major networks each year). Since 1953, NBC has served as the official American broadcaster of the Macy's Thanksgiving Day Parade. CBS also carries unauthorized coverage of the Macy's parade as part of \"The Thanksgiving Day Parade on CBS\"; however, as NBC holds rights to the parade, it has exclusivity over the broadcast of Broadway and music performances appearing in the parade (CBS airs live performances separate from those seen in the parade as a result), and Macy's chose to reroute the parade in 2012 out of the view of CBS' cameras, although it continues to cover the parade. NBC began airing a same-day rebroadcast of the parade telecast in 2009 (replacing its annual Thanksgiving afternoon airing of \"Miracle on 34th Street\"). In 2007, NBC acquired the rights to the National Dog Show, which airs following the Macy's Thanksgiving Day Parade each year.\nThe network also broadcasts several live-action and animated specials during the Christmas holiday season, including the 2014 debuts \"How Murray Saved Christmas\" (an animated musical adaptation of the children's book of the same name) and \"\" (a stop-motion animated special based on the 2003 live-action film \"Elf\").\nSince 2013, the network has aired live musical adaptations with major stars in lead roles. Originally dismissed as a gimmick, they have proven to be rating successes, as well as a nostalgic tribute to the early days of television. Past adaptations include:\nFrom 2003 to 2014, NBC also held rights to two of the three pageants organized by the Miss Universe Organization: the Miss Universe and Miss USA pageants (NBC also held rights to the Miss Teen USA pageant from 2003, when NBC also assumed rights to the Miss USA and Miss Universe pageants as part of a deal brokered by Miss Universe Organization owner Donald Trump that gave the network half-ownership of the pageants, until 2007, when NBC declined to renew its contract to carry Miss Teen USA, effectively discontinuing televised broadcasts of that event until 2023). NBCUniversal relinquished the rights to Miss Universe and Miss USA on June 29, 2015, as part of its decision to cut business ties with Donald Trump and the Miss Universe Organization (which was half-owned by corporate parent NBCUniversal) in response to controversial remarks about Mexican immigrants made by Trump during the launch of his 2016 campaign for the Republican presidential nomination.\nProgramming library.\nThrough the years, NBC has produced many in-house programs, in addition to airing content from other producers such as Revue Studios and its successor Universal Television, along with CBS Studios and Paramount Television Studios, both an unit of Paramount Skydance which currently owns the rights to the pre-1973 NBC's in-house programming library. Notable in-house productions by NBC have included \"Bonanza\", \"Destination X\", \"Little House on the Prairie\", \"Yes, Chef!\" \"Las Vegas\", \"Crossing Jordan\", \"Transplant\", the \"Law &amp; Order\" franchise (begun independently by Universal Television, and became in-house programming after the NBCUniversal deal), \"The Office\", \"Deal or No Deal Island\" and the \"Chicago\" franchise.\nStations.\nNBC has twelve owned-and-operated stations and current and pending affiliation agreements with 222 additional television stations encompassing 50 states, the District of Columbia, six U.S. possessions and two non-U.S. territories (Aruba and Bermuda). The network has a national reach of 88.91% of all households in the United States (or 277,821,345 Americans with at least one television set). From January 24, 2022, when CBS affiliate WBKB-TV in Alpena, Michigan affiliated its DT2 subchannel with NBC, to December 31, 2024, when KXGN-TV in Glendive, Montana dropped NBC from its DT2 subchannel, NBC was the only major network with an in-market affiliate in every designated market area in the United States.\nCurrently, New Jersey and Delaware are the only U.S. states where NBC does not have a locally licensed affiliate. New Jersey is served by New York City O&amp;O WNBC-TV and Philadelphia O&amp;O WCAU; New Jersey formerly had an in-state affiliate in Atlantic City-based WMGM-TV, which was affiliated with the network from 1966 to 2014. Delaware is served by Salisbury affiliate WRDE-LD and Philadelphia-based WCAU. NBC maintains affiliations with low-power stations in a few smaller markets, such as Binghamton, New York (WBGH-CD), Jackson, Tennessee (WNBJ-LD) and Juneau, Alaska (KATH-LD), that do not have enough full-power stations to support a standalone affiliate. In some markets, these stations also maintain digital simulcasts on a subchannel of a co-owned/co-managed full-power television station.\nSouthern New Hampshire receives NBC programming via network-owned WBTS-CD, licensed to serve Nashua; while nominally licensed as a low-power class A station, it transmits a full-power signal under a channel share with the WGBH Educational Foundation and its secondary Boston station WGBX-TV from Needham, Massachusetts, and serves as the NBC station for the entire Boston market. Until 2019, NBC operated a low-powered station in Boston, WBTS-LD (now WYCN-LD), which aimed to serve as its station in that market while using a network of additional full-power stations to cover the market in full (including Merrimack, New Hampshire-licensed Telemundo station WNEU, which transmitted WBTS on a second subchannel); NBC purchased the Nashua station (formerly WYCN-CD) in early 2018 after the FCC spectrum auction, and in 2019 relocated WYCN-LD to Providence, Rhode Island to serve as a Telemundo station for that market.\nTegna Media is the largest operator of NBC stations in terms of overall market reach, owning or providing services to 20 NBC affiliates (including those in larger markets such as Atlanta, Denver, St. Louis, Seattle and Cleveland); Gray Television is the largest owner and operator of NBC stations by quantity and numerical total, owning 64 NBC-affiliated stations.\nRelated services.\nVideo-on-demand services.\nNBC provides video on demand access for delayed viewing of the network's programming through various means, including via its website at NBC.com, a traditional VOD service called NBC on Demand available on most traditional cable and IPTV providers, and through content deals with Hulu and Netflix (the latter of which carries only cataloged episodes of NBC programs, after losing the right to carry newer episodes of its programs during their current seasons in July 2011). From 2007 to 2025, NBCUniversal was a part-owner of Hulu (along with majority owner The Walt Disney Company, owner of ABC), and has offered full-length episodes of most of NBC's programming through the streaming service (which are available for viewing on Hulu's website and mobile app) since Hulu launched in private beta testing on October 29, 2007.\nThe most recent episodes of the network's shows are usually made available on NBC.com and Hulu the day after their original broadcast. In addition, NBC.com and certain other partner websites (including Hulu) provide complete back catalogs of most of its current series as well as a limited selection of episodes of classic series from the NBCUniversal Television Distribution program library \u2013 including shows not broadcast by NBC during their original runs (including the complete or partial episode catalogs of shows like \"30 Rock\", \"The A-Team\", \"Charles in Charge\", \"Emergency!\", \"Knight Rider\" (both the original series and the short-lived 2008 reboot), \"Kojak\", \"Miami Vice\", \"The Office\", \"Quantum Leap\" and \"Simon &amp; Simon\").\nOn February 18, 2015, NBC began providing live programming streams of local NBC stations in select markets, which are only available to authenticated subscribers of participating pay television providers. All eleven NBC-owned-and-operated stations owned by NBCUniversal Owned Television Stations' were the first stations to offer streams of their programming on NBC's website and mobile app, and new affiliation agreements have made a majority of the network's affiliates available through the network's website and app based on a viewer's location. The network's NFL game telecasts were not permitted to be streamed on the service for several years until a change to the league's mobile rights agreement in the 2018 season allowed games to be streamed through network websites and apps.\nNBC HD.\nNBC's master feed is transmitted in 1080i high definition, the native resolution format for NBCUniversal's television properties. However, 19 of its affiliates transmit the network's programming in 720p HD, while four others carry the network feed in 480i standard definition either due to technical considerations for affiliates of other major networks that carry NBC programming on a digital subchannel or because a primary feed NBC affiliate has not yet upgraded their transmission equipment to allow content to be presented in HD.\nNBC's master feed has not fully converted to 1080p or 2160p ultra-high-definition television (UHD). However, some NBC stations have already begun broadcasting at 1080p via ATSC 3.0 multiplex stations. One notable example is WRAL-TV in Raleigh, North Carolina (a station that re-joined NBC in February 2016), which is currently also broadcasting at 1080p via WNGT-CD, which is also serving as an ATSC 3.0 multiplex for the Raleigh area. While the equipment would allow the transmission of 2160p UHD, this was previously done through a secondary experimental station (WRAL-EX) where it transmitted limited NBC programming in UHD. The experimental station went off-air in 2018 as part of the FCC's repacking process.\n\"Meet the Press\" was the first regular series on a major television network to produce a high-definition broadcast on February 2, 1997, which aired in the format over WHD-TV in Washington, D.C., an experimental television station owned by a consortium of industry groups and stations which launched to allow testing of HD broadcasts and operated until 2002 (the program itself continued to be transmitted in 480i standard definition over the NBC network until May 2, 2010, when it became the last NBC News program to convert to HD). NBC officially began its conversion to high definition with the launch of its simulcast feed, NBC HD, on April 26, 1999, when\n\"The Tonight Show\" became the first HD program to air on the NBC network as well as the first regularly scheduled American network program to be produced and transmitted in high definition. NBC gradually converted much of its existing programming from standard-definition to high definition beginning with the 2002\u201303 season, with select shows among that season's slate of freshmen scripted series being broadcast in HD from their debuts.\nNBC completed its conversion to high definition in September 2012, with the launch of NBC Kids, a new Saturday morning children's block programmed by new partial sister network PBS Kids Sprout, which also became the second Saturday morning children's block with an entirely HD schedule (after the ABC-syndicated \"Litton's Weekend Adventure\"). All the network's programming has been presented in full HD since then (except for certain holiday specials produced prior to 2005 \u2013 such as its annual broadcast of \"It's a Wonderful Life\" \u2013 which continues to be presented in 4:3 SD, although some have been remastered for HD broadcast).\nThe network's high-definition programming is broadcast in 5.1 surround sound.\nNBCi.\nIn 1999, NBC launched NBCi (briefly changing its web address to \"www.nbci.com\"), a heavily advertised online venture serving as an attempt to launch a web portal. This move saw NBC partner with Xoom.com (not to be confused with the current money transfer service), e-mail.com, AllBusiness.com, and Snap.com (eventually acquiring all four companies outright; not to be confused with the current-day parent of Snapchat) to launch a multi-faceted internet portal with e-mail, web hosting, community, chat and personalization capabilities, and news content. Subsequently, in April 2000, NBC purchased GlobalBrain, a company specializing in search engines that learned from searches initiated by its users, for $32\u00a0million.\nThe experiment lasted roughly one season; after its failure, NBCi's operations were folded back into NBC. The NBC Television portion of the website reverted to NBC.com. However, the NBCi website continued in operation as a portal for NBC-branded content (NBCi.com would be redirected to NBCi.msnbc.com), using a co-branded version of InfoSpace to deliver minimal portal content. In mid-2007, NBCi.com began to mirror the main NBC.com website; NBCi.com was eventually redirected to the NBC.com domain in 2010. Only one legacy of this direction remains in the website of then-O&amp;O WCMH-TV in Columbus, Ohio (now owned by Nexstar), which continues to use the URL \"nbc4i.com\".\nLogo history.\nNBC has used a number of logos throughout its history; early logos used by the television and radio networks were similar to the logo of its then-parent company, RCA. Logos used later in NBC's existence incorporated stylized peacock designs, including the current version that has been in use since 1986.\nInternational broadcasts.\nCanada.\nNBC network programs can be received throughout most of Canada on cable, satellite and IPTV providers through certain U.S.-based affiliates of the network (such as WBTS-CD in Boston, KING-TV in Seattle, KBJR-TV in Duluth, Minnesota, WGRZ in Buffalo, New York and WHEC-TV in Rochester, New York). Some programs carried on these stations are subject to simultaneous substitutions, a practice imposed by the Canadian Radio-television and Telecommunications Commission in which a pay television provider supplants an American station's signal with a feed from a Canadian station/network airing a particular program in the same time slot to protect domestic advertising revenue. Some of these affiliates are also receivable over the air in southern areas of the country located near the Canada\u2013United States border (signal coverage was somewhat reduced after the digital television transition in 2009 due to the lower radiated power required to transmit digital signals).\nEurope and the Middle East.\nNBC no longer exists outside the Americas as a channel in its own right. However, NBC News and MSNBC programs are broadcast for a few hours a day on OSN News, formerly known as Orbit News in Africa and the Middle East. Sister network CNBC Europe also broadcasts occasional breaking news coverage from MSNBC as well as \"The Tonight Show Starring Jimmy Fallon\". CNBC Europe also broadcast daily airings of \"NBC Nightly News\" at 00:30 CET Monday to Fridays.\nNBC Super Channel becomes NBC Europe.\nIn 1993, then-NBC parent General Electric acquired Super Channel, relaunching the Pan-European cable network as NBC Super Channel. In 1996, the channel was renamed NBC Europe, but was, from then on, almost always referred to on-air as simply \"NBC\".\nMost of NBC Europe's prime time programming was produced in Europe due to rights restrictions associated with U.S. prime time shows; the channel's weekday late-night schedule after 11:00\u00a0p.m. Central European Time, however, featured \"The Tonight Show\", \"Late Night with Conan O'Brien\" and \"Later\", which the channel's slogan \"Where the Stars Come Out at Night\" was based around. Many NBC News programs were broadcast on NBC Europe, including \"Dateline NBC\", \"Meet the Press\" and \"NBC Nightly News\", the latter of which was broadcast simultaneously with the initial U.S. telecast. \"Today\" was also initially aired live in the afternoons, but was later broadcast instead the following morning on a more than half-day delay.\nIn 1999, NBC Europe ceased broadcasting in most of Europe outside of Germany; the network was concurrently relaunched as a German-language technology channel aimed at a younger demographic, with the new series \"NBC GIGA\" as its flagship program. In 2005, the channel was relaunched again as the free-to-air movie channel Das Vierte which eventually shut down end of 2013 (acquired by Disney, which replaced it with a German version of Disney Channel). GIGA Television was subsequently spun off as a separate digital channel, available on satellite and cable providers in Germany, Austria and Switzerland, which shut down as a TV station in the end of 2009.\nLatin America.\nMexico.\nNBC programming is available in Mexico through free-to-air affiliates in markets located within proximity to the Mexico\u2013United States border (such as KYMA-DT/Yuma, Arizona; KGNS-TV/Laredo, Texas; KTSM/El Paso, Texas; KVEO/Brownsville, Texas; and KNSD/San Diego), whose signals are readily receivable over-the-air in border areas of northern Mexico. Some U.S.-based border affiliates are also available on subscription television providers throughout the country, including in the Mexico City area.\nColombia.\nIn Colombia, many subscription providers carry either select U.S.-based NBC and Telemundo affiliated stations or the main network feed from NBCUniversal or Telemundo. Some stations distributing NBC and Telemundo network programming in Colombia include WTVJ and WSCV in Miami, and WNBC and WNJU in New York City.\nIn early 2017, NBC affiliates stopped being distributed in Colombia. This decision coincided with other U.S. affiliated stations from ABC and CBS also being pulled off from the air in the country. This was due to concerns expressed by the broadcasters on broadcasting rights outside their original local coverage area.\nVenezuela.\nIn Venezuela, many subscription providers carry either select U.S.-based NBC and Telemundo affiliated stations or the main network feed from NBCUniversal and Telemundo. Some stations distributing NBC and Telemundo network programming in Venezuela include WTVJ and WSCV in Miami, and WNBC and WNJU in New York City.\nNicaragua and the rest of Central America.\nIn Nicaragua and the rest of Central America, many subscription providers carry either select U.S.-based NBC and Telemundo affiliated stations or the main network feed from NBCUniversal or Telemundo. Some stations distributing NBC and Telemundo network programming in Nicaragua include WTVJ and WSCV in Miami, and WNBC and WNJU in New York City.\nIn late 2017, NBC affiliates stopped being distributed in Nicaragua and the rest of Central America. This decision coincided with other U.S. affiliated stations from ABC and CBS also being pulled off from the air in the region. This was due to concerns expressed by the broadcasters on broadcasting rights outside their original local coverage area.\nEcuador.\nIn Ecuador, many subscription providers carry either select U.S.-based NBC and Telemundo affiliated stations or the main network feed from NBCUniversal or Telemundo. Some stations distributing NBC and Telemundo network programming in Ecuador include WTVJ and WSCV in Miami, and WNBC and WNJU in New York City.\nPeru.\nIn Peru, many subscription providers carry either select U.S.-based NBC and Telemundo affiliated stations or the main network feed from NBCUniversal or Telemundo. Some stations distributing NBC and Telemundo network programming in Peru include WTVJ and WSCV in Miami, and WNBC and WNJU in New York City.\nCanal de Noticias.\nIn 1993, NBC launched a 24-hour Spanish-language news channel serving Latin America (the second news channel serving that region overall, after Noticias ECO, and the first to broadcast 24 hours a day), Canal de Noticias NBC, which based its news schedule around the \"wheel\" format conceived at CNN. The channel, which was headquartered in the offices of the NBC News Channel affiliate news service in Charlotte, North Carolina, employed over 50 journalists to produce, write, anchor and provide technical services. Canal de Noticias NBC shut down in 1999 due to the channel's inability to generate sustainable advertising revenue.\nCaribbean.\nIn the Caribbean, many subscription providers carry either select U.S.-based NBC-affiliated stations or the main network feed from NBC and Telemundo O&amp;Os WNBC and WNJU in New York City or WTVJ and WSCV in Miami. In addition, the network's programming has been available in the U.S. Virgin Islands since 2004 on WVGN-LD in Charlotte Amalie (owned by LKK Group), while Telemundo owned-and-operated station WKAQ-TV in San Juan, Puerto Rico carries the WNBC feed on a digital subchannel.\nBahamas.\nIn the Bahamas, NBC programming is available via U.S.-based affiliate stations on domestic cable providers.\nNetherlands Antilles.\nUntil its bankruptcy and cessation of operations in 2016, NBC maintained an affiliation in Aruba with Oranjestad station PJA-TV (which branded on-air as \"15 ATV\"). Aruba currently receives NBC service from network flagship WNBC via cable.\nPuerto Rico.\nIn Puerto Rico, Telemundo O&amp;O WKAQ-TV carries \"NBC Puerto Rico\" over their third subchannel, which is effectively a simulcast of WNBC with some local advertising and station identification.\nBermuda.\nUntil it ended operations in 2014, NBC's entire program lineup was carried by VSB-TV, using the Eastern Time Zone feed, though an hour ahead due to its location in the Atlantic Time Zone. Bermuda currently receives NBC service from WTVJ Miami via cable.\nPacific.\nGuam.\nIn Guam, the entire NBC programming lineup is carried by Hag\u00e5t\u00f1a affiliate KUAM-TV (which has been an NBC affiliate since 1956) via the network's East Coast satellite feed. Entertainment and news programming is broadcast day and date on a one-day tape delay as Guam is on the west side of the International Date Line (for example, the network's Thursday prime time lineup airs Friday evenings on KUAM, and is advertised by the station as airing on the latter night in on-air promotions). Live programming, including breaking news and sporting events, airs as scheduled; because of the time difference with the six U.S. time zones, live sports coverage often airs on the station early in the morning. KUAM's programming is relayed to the Northern Mariana Islands via satellite station WSZE in Saipan.\nAmerican Samoa.\nIn American Samoa, NBC was affiliated with KKHJ-LP in Pago Pago from 2005 to 2012. Cable television providers on the islands carry the network's programming via Seattle affiliate KING-TV.\nFederated States of Micronesia.\nIn the Federated States of Micronesia, NBC programming is available on domestic cable providers via Honolulu affiliate KHNL.\nAsia.\nNBC Asia and CNBC Asia.\nNBC Asia launched in 1994, distributed to India, Japan, Malaysia, South Korea, Taiwan, Thailand, Pakistan and the Philippines. Like NBC Europe, NBC Asia featured most of NBC's news programs as well as \"The Tonight Show\", \"Late Night\" and \"Saturday Night Live\". Like its European counterpart, it was not allowed to broadcast American-produced prime time shows due to existing broadcast agreements with other domestic broadcasters. NBC Asia produced a regional evening news program that aired each weeknight, and occasionally simulcast some programs from CNBC Asia and MSNBC. NBC also operated NBC Super Sports, a 24-hour channel devoted to televising sporting events.\nIn July 1998, NBC Asia was replaced by a regional version of the National Geographic Channel.\nRegional partners.\nThrough regional partners, NBC-produced programs are seen in some countries on the continent. In the Philippines, Jack TV (owned by Solar Entertainment) airs \"Will &amp; Grace\" and \"Saturday Night Live\", while TalkTV airs \"The Tonight Show\" and NBC News programs including the weekday and weekend editions of \"Today\", \"Early Today\", \"Dateline NBC\" and \"NBC Nightly News\". Solar TV formerly broadcast \"The Jay Leno Show\" from 2009 to 2010. In Hong Kong, the English language free-to-air channel TVB Pearl (operated by TVB) airs live broadcasts of \"NBC Nightly News\", as well as other select NBC programs.\nAustralia.\nIn Australia, the Seven Network has maintained close ties with NBC and has used a majority of the U.S. network's image campaigns and slogans since the 1970s (conversely, in 2009, NBC and Seven both used the Guy Sebastian single \"Like it Like That\" in image promos for their respective summer schedules). The network's \"Seven News\" division has used John Williams-composed \"The Mission\" (the proprietary theme music for NBC News' flagship programs since 1985) as the theme music for its local and national news programs since the mid-1980s, though re-composed domestically to meet their own branding image. Local newscasts were also titled \"Seven Nightly News\" from the mid-1980s until c.\u20092000. NBC News and Seven News often share news resources, with the former division using Seven's reporters for breaking news coverage and select taped story packages relating to Australian stories and the latter sometimes incorporating NBC News reports into its national bulletins.\nSeven also rebroadcasts some of NBC's news and current affairs programming during the early morning hours (usually from 3:00 to 5:00\u00a0a.m. local time), including the weekday and weekend editions of \"Today\" (which it brands as \"NBC Today\" to differentiate it from the unrelated morning program of the same title on the Nine Network), \"Dateline NBC\" and \"Meet the Press\".\nCriticism and controversies.\nDuring the Gulf War NBC received criticism of its reporting of the conflict.\nIn March and April 2019, the \"Huffington Post\" and \"Wired\" reported that NBC had paid a firm to improve its reputation by lobbying for changes to the Wikipedia articles on NBC, Nextdoor and several others.\nThe NBC television network has been accused of tolerating a culture of sexism and sexual harassment among its employees (especially within upper management and among senior anchors such as Matt Lauer) and also of covering up indiscretions committed by prominent figures in the company through intimidation campaigns against victims that include widespread use of non-disclosure agreements. This may have exposed the company to pressure from Harvey Weinstein to delay or terminate reporting on Weinstein's criminal abuse of many women.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21781", "revid": "5846", "url": "https://en.wikipedia.org/wiki?curid=21781", "title": "Nociception", "text": "How an organism receives and responds to painful stimuli\nIn physiology, nociception , also nocioception; from la \" nocere\"\u00a0'to harm/hurt') is the sensory nervous system's process of encoding noxious stimuli. It deals with a series of events and processes required for an organism to receive a painful stimulus, convert it to a molecular signal, and recognize and characterize the signal to trigger an appropriate defensive response. \nIn nociception, intense chemical (e.g., capsaicin present in chili pepper or cayenne pepper), mechanical (e.g., cutting, crushing), or thermal (heat and cold) stimulation of sensory neurons called nociceptors produces a signal that travels along a chain of nerve fibers to the brain. Nociception triggers a variety of physiological and behavioral responses to protect the organism against an aggression, and usually results in a subjective experience, or perception, of pain in sentient beings.\nDetection of noxious stimuli.\nPotentially damaging mechanical, thermal, and chemical stimuli are detected by nerve endings called nociceptors, which are found in the skin, on internal surfaces such as the periosteum, joint surfaces, and in some internal organs. Some nociceptors are unspecialized free nerve endings that have their cell bodies outside the spinal column in dorsal root ganglia. Others are specialised structures in the skin such as nociceptive Schwann cells. Nociceptors are categorized according to the axons which travel from the receptors to the spinal cord or brain. After nerve injury, it is possible for touch fibers that normally carry non-noxious stimuli to be perceived as noxious.\nNociceptive pain consists of an adaptive alarm system. Nociceptors have a certain threshold; that is, they require a minimum intensity of stimulation before they trigger a signal. Once this threshold is reached, a signal is passed along the neuron's axon into the spinal cord.\nNociceptive threshold testing deliberately applies a noxious stimulus to a human or animal subject to study pain. In animals, the technique is often used to study the efficacy of analgesic drugs and to establish dosing levels and periods of effect. After establishing a baseline, the drug under test is given, and the elevation in threshold is recorded at specified times. The threshold should return to the baseline (pretreatment) value when the drug wears off. In some conditions, the excitation of pain fibers increases as the pain stimulus continues, leading to a condition called hyperalgesia.\nConsequences.\nNociception can also cause generalized autonomic responses before or without reaching consciousness to cause pallor, sweating, tachycardia, hypertension, lightheadedness, nausea, and fainting.\nSystem overview.\nThis overview discusses proprioception, thermoception, chemoception, and nociception, as they are all integrally connected.\nMechanical.\nProprioception is determined by using standard mechanoreceptors (especially ruffini corpuscles (stretch) and transient receptor potential channels (TRP channels). Proprioception is completely covered within the somatosensory system, as the brain processes them together.\nThermoception refers to stimuli of moderate temperatures , as anything beyond that range is considered pain and moderated by nociceptors. TRP and potassium channels [TRPM (1-8), TRPV (1-6), TRAAK, and TREK] each respond to different temperatures (among other stimuli), which create action potentials in nerves that join the mechano (touch) system in the posterolateral tract. Thermoception, like proprioception, is then covered by the somatosensory system.\nTRP channels that detect noxious stimuli (mechanical, thermal, and chemical pain) relay that information to nociceptors that generate an action potential. Mechanical TRP channels react to depression of their cells (like touch), thermal TRPs change shape in different temperatures, and chemical TRPs act like taste buds, signalling if their receptors bond to certain elements/chemicals.\nIn non-mammals.\nNociception has been documented in other animals, including fish and a wide range of invertebrates, including leeches, nematode worms, sea slugs, and fruit flies. As in mammals, nociceptive neurons in these species are typically characterized by responding preferentially to high temperature ( or more), low pH, capsaicin, and tissue damage.\nHistory of term.\nThe term \"nociception\" was coined by Charles Scott Sherrington to distinguish the physiological process (nervous activity) from pain (a subjective experience). It is derived from the Latin verb \"noc\u0113re\", which means \"to harm\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21782", "revid": "1201040", "url": "https://en.wikipedia.org/wiki?curid=21782", "title": "Noun case", "text": ""}
{"id": "21784", "revid": "1934512", "url": "https://en.wikipedia.org/wiki?curid=21784", "title": "Nova", "text": "Nuclear explosion in a white dwarf star\nA nova is a transient astronomical event that causes the sudden appearance of a bright, apparently \"new\" star (hence the name \"nova\", Latin for \"new\") that slowly fades over weeks or months. All observed novae involve white dwarfs in close binary systems, but causes of the dramatic appearance of a nova vary, depending on the circumstances of the two progenitor stars. The main sub-classes of novae are classical novae, recurrent novae (RNe), and dwarf novae. They are all considered to be cataclysmic variable stars.\nClassical nova eruptions are the most common type. This type is usually created in a close binary star system consisting of a white dwarf and either a main sequence, subgiant, or red giant star. If the orbital period of the system is a few days or less, the white dwarf is close enough to its companion star to draw accreted matter onto its surface, creating a dense but shallow atmosphere. This atmosphere, mostly consisting of hydrogen, is heated by the hot white dwarf and eventually reaches a critical temperature, causing ignition of rapid runaway fusion. The sudden increase in energy expels the atmosphere into interstellar space, creating the envelope seen as visible light during the nova event. In past centuries such an event was thought to be a new star. A few novae produce short-lived nova remnants, lasting for perhaps several centuries.\nA recurrent nova involves the same processes as a classical nova, except that the nova event repeats in cycles of a few decades or less as the companion star again feeds the dense atmosphere of the white dwarf after each ignition, as in the star T Coronae Borealis.\nUnder certain conditions, mass accretion can eventually trigger runaway fusion that destroys the white dwarf rather than merely expelling its atmosphere. In this case, the event is usually classified as a Type Ia supernova.\nNovae most often occur in the sky along the path of the Milky Way, especially near the observed Galactic Center in Sagittarius; however, they can appear anywhere in the sky. They occur far more frequently than galactic supernovae, averaging about ten per year in the Milky Way. Most are found telescopically, perhaps only one every 12\u201318 months reaching naked-eye visibility. Novae reaching first or second magnitude occur only a few times per century. The last bright nova was V1369 Centauri, which reached 3.3 magnitude on 14 December 2013.\nEtymology.\nDuring the sixteenth century, astronomer Tycho Brahe observed the supernova SN 1572 in the constellation Cassiopeia. He described it in his book \"De nova stella\" (Latin for \"concerning the new star\"), giving rise to the adoption of the name \"nova\". In this work he argued that a nearby object should be seen to move relative to the fixed stars, and thus the nova had to be very far away. Although SN 1572 was later found to be a supernova and not a nova, the terms were considered interchangeable until the 1930s. After this, novae were called \"classical novae\" to distinguish them from supernovae, as their causes and energies were thought to be different, based solely on the observational evidence.\nAlthough the term \"stella nova\" means \"new star\", novae most often take place on white dwarfs, which are remnants of extremely old stars.\nStellar evolution of novae.\nEvolution of potential novae begins with two main sequence stars in a binary system. One of the two evolves into a red giant, leaving its remnant white dwarf core in orbit with the remaining star. The second star\u2014which may be either a main-sequence star or an aging giant\u2014begins to shed its envelope onto its white dwarf companion when it overflows its Roche lobe. As a result, the white dwarf steadily captures matter from the companion's outer atmosphere in an accretion disk, and in turn, the accreted matter falls into the atmosphere. As the white dwarf consists of degenerate matter, the accreted hydrogen is unable to expand even though its temperature increases. Runaway fusion occurs when the temperature of this atmospheric layer reaches ~20\u00a0million K, initiating nuclear burning via the CNO cycle.\nIf the accretion rate is just right, hydrogen fusion may occur in a stable manner on the surface of the white dwarf, giving rise to a super soft X-ray source, but for most binary system parameters, the hydrogen burning is thermally unstable and rapidly converts a large amount of the hydrogen into other, heavier chemical elements in a runaway reaction, liberating an enormous amount of energy. This blows the remaining gases away from the surface of the white dwarf and produces an extremely bright outburst of light.\nThe rise to peak brightness may be very rapid, or gradual; after the peak, the brightness declines steadily. The time taken for a nova to decay by 2 or 3\u00a0magnitudes from maximum optical brightness is used for grouping novae into speed classes. Fast novae typically will take less than 25\u00a0days to decay by 2\u00a0magnitudes, while slow novae will take more than 80\u00a0days.\nDespite its violence, usually the amount of material ejected in a nova is only about &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u204410,000 of a solar mass, quite small relative to the mass of the white dwarf. Furthermore, only five percent of the accreted mass is fused during the power outburst. Nonetheless, this is enough energy to accelerate nova ejecta to velocities as high as several thousand kilometers per second\u2014higher for fast novae than slow ones\u2014with a concurrent rise in luminosity from a few times solar to 50,000\u2013100,000\u00a0times solar. In 2010 scientists using NASA's Fermi Gamma-ray Space Telescope discovered that a nova also can emit gamma rays (&gt;100\u00a0MeV).\nPotentially, a white dwarf can generate multiple novae over time as additional hydrogen continues to accrete onto its surface from its companion star. Where this repeated flaring is observed, the object is called a recurrent nova. An example is RS Ophiuchi, which is known to have flared seven times (in 1898, 1933, 1958, 1967, 1985, 2006, and 2021). Eventually, the white dwarf can explode as a Type\u00a0Ia supernova if it approaches the Chandrasekhar limit.\nOccasionally, novae are bright enough and close enough to Earth to be conspicuous to the unaided eye. The brightest recent example was Nova Cygni 1975. This nova appeared on 29\u00a0August 1975, in the constellation Cygnus about 5 degrees north of Deneb, and reached magnitude\u00a02.0 (nearly as bright as Deneb). The most recent were V1280 Scorpii, which reached magnitude\u00a03.7 on 17\u00a0February 2007, and Nova Delphini 2013. Nova Centauri 2013 was discovered 2\u00a0December 2013 and so far is the brightest nova of this millennium, reaching magnitude\u00a03.3.\nHelium novae.\nA helium nova (undergoing a helium flash) is a proposed category of nova event that lacks hydrogen lines in its spectrum. The absence of hydrogen lines may be caused by the explosion of a helium shell on a white dwarf. The theory was first proposed in 1989, and the first candidate helium nova to be observed was V445 Puppis, in 2000. Since then, four other novae have been proposed as helium novae.\nOccurrence rate and astrophysical significance.\nAstronomers have estimated that the Milky Way experiences roughly 25 to 75 novae per year. The number of novae actually observed in the Milky Way each year is much lower, about 10, probably because distant novae are obscured by gas and dust absorption. As of 2019, 407 probable novae had been recorded in the Milky Way. In the Andromeda Galaxy, roughly 25 novae brighter than about 20th magnitude are discovered each year, and smaller numbers are seen in other nearby galaxies.\nSpectroscopic observation of nova ejecta nebulae has shown that they are enriched in elements such as helium, carbon, nitrogen, oxygen, neon, and magnesium. Classical nova explosions are galactic producers of the element lithium. The contribution of novae to the interstellar medium is not great; novae supply only &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u204450 as much material to the galaxy as do supernovae, and only &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044200 as much as red giant and supergiant stars.\nObserved recurrent novae such as RS Ophiuchi (those with periods on the order of decades) are rare. Astronomers theorize, however, that most, if not all, novae recur, albeit on time scales ranging from 1,000 to 100,000 years. The recurrence interval for a nova is less dependent on the accretion rate of the white dwarf than on its mass; with their powerful gravity, massive white dwarfs require less accretion to fuel an eruption than lower-mass ones. Consequently, the interval is shorter for high-mass white dwarfs.\nV Sagittae is unusual in that the time of its next eruption can be predicted fairly accurately; it is expected to recur in approximately 2083, plus or minus about 11 years.\nSubtypes.\nNovae are classified according to the light curve decay speed, referred to as either type A, B, C and R, or using the prefix \"N\": \nRemnants.\nSome novae leave behind visible nebulosity, material expelled in the nova explosion or in multiple explosions.\nNovae as distance indicators.\nNovae have some promise for use as standard candle measurements of distances. For instance, the distribution of their absolute magnitude is bimodal, with a main peak at magnitude \u22128.8, and a lesser one at \u22127.5. Novae also have roughly the same absolute magnitude 15 days after their peak (\u22125.5). Nova-based distance estimates to various nearby galaxies and galaxy clusters have been shown to be of comparable accuracy to those measured with Cepheid variable stars.\nRecurrent novae.\nA recurrent nova (RN) is an object that has been seen to experience repeated nova eruptions. The recurrent nova typically brightens by about 9 magnitudes, whereas a classical nova may brighten by more than 12 magnitudes.\nAlthough it is estimated that as many as a quarter of nova systems experience multiple eruptions, only ten recurrent novae (listed below) have been observed in the Milky Way.\nSeveral extragalactic recurrent novae have been observed in the Andromeda Galaxy (M31) and the Large Magellanic Cloud. One of these extragalactic novae, M31N 2008-12a, erupts as frequently as once every 12 months.\nOn 20 April 2016, the \"Sky &amp; Telescope\" website reported a sustained brightening of T Coronae Borealis from magnitude 10.5 to about 9.2 starting in February 2015. A similar event had been reported in 1938, followed by another outburst in 1946. By June 2018, the star had dimmed slightly but still remained at an unusually high level of activity. In March or April 2023, it dimmed to magnitude 12.3. A similar dimming occurred in the year before the 1945 outburst, indicating that it would likely erupt between March and September 2024. As of 18, 2025, this predicted outburst has not yet occurred.\nExtragalactic novae.\nNovae are relatively common in the Andromeda Galaxy (M31); including recurrent ones. several dozen novae (brighter than apparent magnitude +20) are discovered in M31 each year. The Central Bureau for Astronomical Telegrams (CBAT) has tracked novae in M31, M33, and M81.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21785", "revid": "1934512", "url": "https://en.wikipedia.org/wiki?curid=21785", "title": "Nuclear weapon", "text": "A nuclear weapon is an explosive device that derives its destructive force from nuclear reactions, either nuclear fission (fission or atomic bomb) or a combination of fission and nuclear fusion reactions (thermonuclear weapon), producing a nuclear explosion. Both bomb types release large quantities of energy from relatively small amounts of matter.\nNuclear weapons have had yields between 10 tons (the W54) and 50 megatons for the Tsar Bomba (see TNT equivalent). Yields in the low kilotons can devastate cities. A thermonuclear weapon weighing as little as can release energy equal to more than 1.2 megatons of TNT (5.0 PJ). Apart from the blast, effects of nuclear weapons include extreme heat and ionizing radiation, firestorms, radioactive nuclear fallout, an electromagnetic pulse, and a radar blackout.\nThe first nuclear weapons were developed by the United States in collaboration with the United Kingdom and Canada during World War II in the Manhattan Project. Production requires a large scientific and industrial complex, primarily for the production of fissile material, either from nuclear reactors with reprocessing plants or from uranium enrichment facilities. Nuclear weapons have been used twice in war, in the 1945 atomic bombings of Hiroshima and Nagasaki that killed between 150,000 and 246,000 people. Nuclear deterrence, including mutually assured destruction, aims to prevent nuclear warfare via the threat of unacceptable damage and the danger of escalation to nuclear holocaust. A nuclear arms race for weapons and their delivery systems was a defining component of the Cold War.\nStrategic nuclear weapons are targeted against civilian, industrial, and military infrastructure, while tactical nuclear weapons are intended for battlefield use. Strategic weapons led to the development of dedicated intercontinental ballistic missiles, submarine-launched ballistic missile, and nuclear strategic bombers, collectively known as the nuclear triad. Tactical weapons options have included shorter-range ground-, air-, and sea-launched missiles, nuclear artillery, atomic demolition munitions, nuclear torpedoes, and nuclear depth charges, but they have become less salient since the end of the Cold War.\nAs of 2025[ [update]], there are nine countries on the list of states with nuclear weapons, and six more agree to nuclear sharing. Nuclear weapons are weapons of mass destruction, and their control is a focus of international security through measures to prevent nuclear proliferation, arms control, or nuclear disarmament. The total from all stockpiles peaked at over 64,000 weapons in 1986, and is around 9,600 today. Key international agreements and organizations include the Treaty on the Non-Proliferation of Nuclear Weapons, the Comprehensive Nuclear-Test-Ban Treaty and Comprehensive Nuclear-Test-Ban Treaty Organization, the International Atomic Energy Agency, the Treaty on the Prohibition of Nuclear Weapons, and nuclear-weapon-free zones.\nTesting and deployment.\nNuclear weapons have only twice been used in warfare, both times by the United States against Japan at the end of World War II. On August 6, 1945, the United States Army Air Forces (USAAF) detonated a uranium gun-type fission bomb nicknamed \"Little Boy\" over the Japanese city of Hiroshima; three days later, on August 9, the USAAF detonated a plutonium implosion-type fission bomb nicknamed \"Fat Man\" over the Japanese city of Nagasaki. These bombings caused injuries that resulted in the deaths of approximately 200,000 civilians and military personnel. The ethics of these bombings and their role in Japan's surrender are to this day, still subjects of debate.\nSince the atomic bombings of Hiroshima and Nagasaki, nuclear weapons have been detonated over 2,000 times for testing and demonstration. Only a few nations possess such weapons or are suspected of seeking them. The only countries known to have detonated nuclear weapons\u2014and acknowledge possessing them\u2014are (chronologically by date of first test) the United States, the Soviet Union (succeeded as a nuclear power by Russia), the United Kingdom, France, China, India, Pakistan, and North Korea. Israel is believed to possess nuclear weapons, though, in a policy of deliberate ambiguity, it does not acknowledge having them. Germany, Italy, Turkey, Belgium, the Netherlands, and Belarus are nuclear weapons sharing states. South Africa is the only country to have independently developed and then renounced and dismantled its nuclear weapons.\nOn the October 30, 2025, US President Donald Trump called for renewed nuclear weapons testing in order to keep pace with other nuclear armed states such as Russia and China however it was not specified whether he referred to nuclear explosive testing or testing of delivery systems for nuclear warheads.\nTypes.\nThere are two basic types of nuclear weapons: those that derive the majority of their energy from nuclear fission reactions alone, and those that use fission reactions to begin nuclear fusion reactions that produce a large amount of the total energy output.\nFission weapons.\nAll existing nuclear weapons derive some of their explosive energy from nuclear fission reactions. Weapons whose explosive output is exclusively from fission reactions are commonly referred to as atomic bombs or atom bombs (abbreviated as A-bombs). This has long been noted as something of a misnomer, as their energy comes from the nucleus of the atom, just as it does with fusion weapons.\nIn fission weapons, a mass of fissile material (enriched uranium or plutonium) is forced into supercriticality\u2014allowing an exponential growth of nuclear chain reactions\u2014either by shooting one piece of sub-critical material into another (the \"gun\" method) or by compression of a sub-critical sphere or cylinder of fissile material using chemically fueled explosive lenses. The latter approach, the \"implosion\" method, is more sophisticated and more efficient (smaller, less massive, and requiring less of the expensive fissile fuel) than the former.\nA major challenge in all nuclear weapon designs is to ensure that a significant fraction of the fuel is consumed before the weapon destroys itself. The amount of energy released by fission bombs can range from the equivalent of just under a ton to upwards of 500,000 tons (500 kilotons) of TNT ().\nAll fission reactions generate fission products, the remains of the split atomic nuclei. Many fission products are either highly radioactive (but short-lived) or moderately radioactive (but long-lived), and as such, they are a serious form of radioactive contamination. Fission products are the principal radioactive component of nuclear fallout. Another source of radioactivity is the burst of free neutrons produced by the weapon. When they collide with other nuclei in the surrounding material, the neutrons transmute those nuclei into other isotopes, altering their stability and making them radioactive.\nThe most commonly used fissile materials for nuclear weapons applications have been uranium-235 and plutonium-239. Less commonly used has been uranium-233. Neptunium-237 and some isotopes of americium may be usable for nuclear explosives as well, but it is not clear that this has ever been implemented, and their plausible use in nuclear weapons is a matter of dispute.\nFusion weapons.\nThe other basic type of nuclear weapon produces a large proportion of its energy in nuclear fusion reactions. Such fusion weapons are generally referred to as thermonuclear weapons or more colloquially as hydrogen bombs (abbreviated as H-bombs), as they rely on fusion reactions between isotopes of hydrogen (deuterium and tritium). All such weapons derive a significant portion of their energy from fission reactions used to \"trigger\" fusion reactions, and fusion reactions can themselves trigger additional fission reactions.\nOnly six countries\u2014the United States, Russia, the United Kingdom, China, France, and India\u2014have conducted thermonuclear weapon tests. Whether India has detonated a \"true\" multi-staged thermonuclear weapon is controversial. North Korea claims to have tested a fusion weapon as of January\u00a02016[ [update]], though this claim is disputed. Thermonuclear weapons are considered much more difficult to successfully design and execute than primitive fission weapons. Almost all of the nuclear weapons deployed today use the thermonuclear design because it results in an explosion hundreds of times stronger than that of a fission bomb of similar weight.\nThermonuclear bombs work by using the energy of a fission bomb to compress and heat fusion fuel. In the Teller-Ulam design, which accounts for all multi-megaton yield hydrogen bombs, this is accomplished by placing a fission bomb and fusion fuel (tritium, deuterium, or lithium deuteride) in proximity within a special, radiation-reflecting container. When the fission bomb is detonated, gamma rays and X-rays emitted first compress the fusion fuel, then heat it to thermonuclear temperatures. The ensuing fusion reaction creates enormous numbers of high-speed neutrons, which can then induce fission in materials not normally prone to it, such as depleted uranium. Each of these components is known as a \"stage\", with the fission bomb as the \"primary\" and the fusion capsule as the \"secondary\". In large, megaton-range hydrogen bombs, about half of the yield comes from the final fissioning of depleted uranium.\nVirtually all thermonuclear weapons deployed today use this \"two-stage\" design, but it is possible to add additional fusion stages\u2014each stage igniting a larger amount of fusion fuel in the next stage. This technique can be used to construct thermonuclear weapons of arbitrarily large yield. This is in contrast to fission bombs, which are limited in their explosive power due to criticality danger (premature nuclear chain reaction caused by too-large amounts of pre-assembled fissile fuel). The largest nuclear weapon ever detonated, the Tsar Bomba of the USSR, which released an energy equivalent of over , was a three-stage weapon. Most thermonuclear weapons are considerably smaller than this, due to practical constraints from missile warhead space and weight requirements. In the early 1950s the Livermore Laboratory in the United States had plans for the testing of two massive bombs, Gnomon and Sundial, 1 gigaton of TNT and 10 gigatons of TNT respectively.\nFusion reactions do not create fission products, and thus contribute far less to the creation of nuclear fallout than fission reactions, but because all thermonuclear weapons contain at least one fission stage, and many high-yield thermonuclear devices have a final fission stage, thermonuclear weapons can generate at least as much nuclear fallout as fission-only weapons. Furthermore, high yield thermonuclear explosions (most dangerously ground bursts) have the force to lift radioactive debris upwards past the tropopause into the stratosphere, where the calm non-turbulent winds permit the debris to travel great distances from the burst, eventually settling and unpredictably contaminating areas far removed from the target of the explosion.\nOther types.\nThere are other types of nuclear weapons as well. For example, a boosted fission weapon is a fission bomb that increases its explosive yield through a small number of fusion reactions, but it is not a fusion bomb. In the boosted bomb, the neutrons produced by the fusion reactions serve primarily to increase the efficiency of the fission bomb. There are two types of boosted fission bomb: internally boosted, in which a deuterium-tritium mixture is injected into the bomb core, and externally boosted, in which concentric shells of lithium-deuteride and depleted uranium are layered on the outside of the fission bomb core. The external method of boosting enabled the USSR to field the first partially thermonuclear weapons, but it is now obsolete because it demands a spherical bomb geometry, which was adequate during the 1950s arms race when bomber aircraft were the only available delivery vehicles.\nThe detonation of any nuclear weapon is accompanied by a blast of neutron radiation. Surrounding a nuclear weapon with suitable materials (such as cobalt or gold) creates a weapon known as a salted bomb. This device can produce exceptionally large quantities of long-lived radioactive contamination. It has been conjectured that such a device could serve as a \"doomsday weapon\" because such a large quantity of radioactivities with half-lives of decades, lifted into the stratosphere where winds would distribute it around the globe, would make all life on the planet extinct.\nIn connection with the Strategic Defense Initiative, research into the nuclear pumped laser was conducted under the DOD program Project Excalibur but this did not result in a working weapon. The concept involves the tapping of the energy of an exploding nuclear bomb to power a single-shot laser that is directed at a distant target.\nDuring the Starfish Prime high-altitude nuclear test in 1962, an unexpected effect was produced which is called a nuclear electromagnetic pulse. This is an intense flash of electromagnetic energy produced by a rain of high-energy electrons which in turn are produced by a nuclear bomb's gamma rays. This flash of energy can permanently destroy or disrupt electronic equipment if insufficiently shielded. It has been proposed to use this effect to disable an enemy's military and civilian infrastructure as an adjunct to other nuclear or conventional military operations. By itself it could as well be useful to terrorists for crippling a nation's economic electronics-based infrastructure. Because the effect is most effectively produced by high altitude nuclear detonations (by military weapons delivered by air, though ground bursts also produce EMP effects over a localized area), it can produce damage to electronics over a wide, even continental, geographical area.\nResearch has been done into the possibility of pure fusion bombs: nuclear weapons that consist of fusion reactions without requiring a fission bomb to initiate them. Such a device might provide a simpler path to thermonuclear weapons than one that required the development of fission weapons first, and pure fusion weapons would create significantly less nuclear fallout than other thermonuclear weapons because they would not disperse fission products. In 1998, the United States Department of Energy divulged that the United States had, \"...made a substantial investment\" in the past to develop pure fusion weapons, but that, \"The U.S. does not have and is not developing a pure fusion weapon\", and that, \"No credible design for a pure fusion weapon resulted from the DOE investment\".\nNuclear isomers provide a possible pathway to fissionless fusion bombs. These are naturally occurring isotopes (178m2Hf being a prominent example) which exist in an elevated energy state. Mechanisms to release this energy as bursts of gamma radiation (as in the hafnium controversy) have been proposed as possible triggers for conventional thermonuclear reactions.\nAntimatter, which consists of particles resembling ordinary matter particles in most of their properties but having opposite electric charge, has been considered as a trigger mechanism for nuclear weapons. A major obstacle is the difficulty of producing antimatter in large enough quantities, and there is no evidence that it is feasible beyond the military domain. However, the US Air Force funded studies of the physics of antimatter in the Cold War, and began considering its possible use in weapons, not just as a trigger, but as the explosive itself. A fourth generation nuclear weapon design is related to, and relies upon, the same principle as antimatter-catalyzed nuclear pulse propulsion.\nMost variation in nuclear weapon design is for the purpose of achieving different yields for different situations, and in manipulating design elements to attempt to minimize weapon size, radiation hardness or requirements for special materials, especially fissile fuel or tritium.\nTactical nuclear weapons.\nSome nuclear weapons are designed for special purposes; most of these are for non-strategic (decisively war-winning) purposes and are referred to as tactical nuclear weapons.\nThe neutron bomb purportedly conceived by Sam Cohen is a thermonuclear weapon that yields a relatively small explosion but a relatively large amount of neutron radiation. Such a weapon could, according to tacticians, be used to cause massive biological casualties while leaving inanimate infrastructure mostly intact and creating minimal fallout. Because high energy neutrons are capable of penetrating dense matter, such as tank armor, neutron warheads were procured in the 1980s (though not deployed in Europe) for use as tactical payloads for US Army artillery shells (200\u00a0mm W79 and 155\u00a0mm W82) and short range missile forces. Soviet authorities announced similar intentions for neutron warhead deployment in Europe; indeed, they claimed to have originally invented the neutron bomb, but their deployment on USSR tactical nuclear forces is unverifiable.\nA type of nuclear explosive most suitable for use by ground special forces was the Special Atomic Demolition Munition, or SADM, sometimes popularly known as a suitcase nuke. This is a nuclear bomb that is man-portable, or at least truck-portable, and though of a relatively small yield (one or two kilotons) is sufficient to destroy important tactical targets such as bridges, dams, tunnels, important military or commercial installations, etc. either behind enemy lines or pre-emptively on friendly territory soon to be overtaken by invading enemy forces. These weapons require plutonium fuel and are particularly \"dirty\". They also demand especially stringent security precautions in their storage and deployment.\nSmall \"tactical\" nuclear weapons were deployed for use as antiaircraft weapons. Examples include the USAF AIR-2 Genie, the AIM-26 Falcon and US Army Nike Hercules. Missile interceptors such as the Sprint and the Spartan also used small nuclear warheads (optimized to produce neutron or X-ray flux) but were for use against enemy strategic warheads.\nOther small, or tactical, nuclear weapons were deployed by naval forces for use primarily as antisubmarine weapons. These included nuclear depth bombs or nuclear armed torpedoes. Nuclear mines for use on land or at sea are also possibilities.\nWeapons delivery.\nThe system used to deliver a nuclear weapon to its target is an important factor affecting both nuclear weapon design and nuclear strategy. The design, development, and maintenance of delivery systems are among the most expensive parts of a nuclear weapons program; they account, for example, for 57% of the financial resources spent by the United States on nuclear weapons projects since 1940.\nThe simplest method for delivering a nuclear weapon is a gravity bomb dropped from aircraft; this was the method used by the United States against Japan in 1945. This method places few restrictions on the size of the weapon. It does, however, limit attack range, response time to an impending attack, and the number of weapons that a country can field at the same time. With miniaturization, nuclear bombs can be delivered by both strategic bombers and tactical fighter-bombers. This method is the primary means of nuclear weapons delivery; the majority of US nuclear warheads, for example, are free-fall gravity bombs, namely the B61, which is being improved upon to this day.\nPreferable from a strategic point of view is a nuclear weapon mounted on a missile, which can use a ballistic trajectory to deliver the warhead over the horizon. Although even short-range missiles allow for a faster and less vulnerable attack, the development of long-range intercontinental ballistic missiles (ICBMs) and submarine-launched ballistic missiles (SLBMs) has given some nations the ability to plausibly deliver missiles anywhere on the globe with a high likelihood of success.\nMore advanced systems, such as multiple independently targetable reentry vehicles (MIRVs), can launch multiple warheads at different targets from one missile, reducing the chance of a successful missile defense. Today, missiles are most common among systems designed for delivery of nuclear weapons. Making a warhead small enough to fit onto a missile, though, can be difficult.\nTactical weapons have involved the most variety of delivery types, including not only gravity bombs and missiles but also artillery shells, land mines, and nuclear depth charges and torpedoes for anti-submarine warfare. An atomic mortar has been tested by the United States. Small, two-man portable tactical weapons (somewhat misleadingly referred to as suitcase bombs), such as the Special Atomic Demolition Munition, have been developed, although the difficulty of combining sufficient yield with portability limits their military utility.\nNuclear strategy.\nNuclear warfare strategy is a set of policies that deal with preventing or fighting a nuclear war. The policy of trying to prevent an attack by a nuclear weapon from another country by threatening nuclear retaliation is known as the strategy of nuclear deterrence. The goal in deterrence is to always maintain a second strike capability (the ability of a country to respond to a nuclear attack with one of its own) and potentially to strive for first strike status (the ability to destroy an enemy's nuclear forces before they could retaliate). During the Cold War, policy and military theorists considered the sorts of policies that might prevent a nuclear attack, and they developed game theory models that could lead to stable deterrence conditions.\nDifferent forms of nuclear weapons delivery (see above) allow for different types of nuclear strategies. The goals of any strategy are generally to make it difficult for an enemy to launch a pre-emptive strike against the weapon system and difficult to defend against the delivery of the weapon during a potential conflict. This can mean keeping weapon locations hidden, such as deploying them on submarines or land mobile transporter erector launchers whose locations are difficult to track, or it can mean protecting weapons by burying them in hardened missile silo bunkers. Other components of nuclear strategies included using missile defenses to destroy the missiles before they land or implementing civil defense measures using early-warning systems to evacuate citizens to safe areas before an attack.\nWeapons designed to threaten large populations or to deter attacks are known as \"strategic weapons.\" Nuclear weapons for use on a battlefield in military situations are called \"tactical weapons.\"\nCritics of nuclear war strategy often suggest that a nuclear war between two nations would result in mutual annihilation. From this point of view, the significance of nuclear weapons is to deter war because any nuclear war would escalate out of mutual distrust and fear, resulting in mutually assured destruction. This threat of national, if not global, destruction has been a strong motivation for anti-nuclear weapons activism.\nCritics from the peace movement and within the military establishment have questioned the usefulness of such weapons in the current military climate. According to an advisory opinion issued by the International Court of Justice in 1996, the use of (or threat of use of) such weapons would generally be contrary to the rules of international law applicable in armed conflict, but the court did not reach an opinion as to whether or not the threat or use would be lawful in specific extreme circumstances such as if the survival of the state were at stake.\nAnother deterrence position is that nuclear proliferation can be desirable. In this case, it is argued that, unlike conventional weapons, nuclear weapons deter all-out war between states, and they succeeded in doing this during the Cold War between the US and the Soviet Union. In the late 1950s and early 1960s, Gen. Pierre Marie Gallois of France, an adviser to Charles de Gaulle, argued in books like \"The Balance of Terror: Strategy for the Nuclear Age\" (1961) that mere possession of a nuclear arsenal was enough to ensure deterrence, and thus concluded that the spread of nuclear weapons could increase international stability. Some prominent neo-realist scholars, such as Kenneth Waltz and John Mearsheimer, have argued, along the lines of Gallois, that some forms of nuclear proliferation would decrease the likelihood of total war, especially in troubled regions of the world where there exists a single nuclear-weapon state. Aside from the public opinion that opposes proliferation in any form, there are two schools of thought on the matter: those, like Mearsheimer, who favored selective proliferation, and Waltz, who was somewhat more non-interventionist. Interest in proliferation and the stability-instability paradox that it generates continues to this day, with ongoing debate about indigenous Japanese and South Korean nuclear deterrent against North Korea.\nThe threat of potentially suicidal terrorists possessing nuclear weapons (a form of nuclear terrorism) complicates the decision process. The prospect of mutually assured destruction might not deter an enemy who expects to die in the confrontation. Further, if the initial act is from a stateless terrorist instead of a sovereign nation, there might not be a nation or specific target to retaliate against. It has been argued, especially after the September 11, 2001, attacks, that this complication calls for a new nuclear strategy, one that is distinct from that which gave relative stability during the Cold War. Since 1996, the United States has had a policy of allowing the targeting of its nuclear weapons at terrorists armed with weapons of mass destruction.\nRobert Gallucci argues that although traditional deterrence is not an effective approach toward terrorist groups bent on causing a nuclear catastrophe, Gallucci believes that \"the United States should instead consider a policy of expanded deterrence, which focuses not solely on the would-be nuclear terrorists but on those states that may deliberately transfer or inadvertently leak nuclear weapons and materials to them. By threatening retaliation against those states, the United States may be able to deter that which it cannot physically prevent.\"\nGraham Allison makes a similar case, arguing that the key to expanded deterrence is coming up with ways of tracing nuclear material to the country that forged the fissile material. \"After a nuclear bomb detonates, nuclear forensics cops would collect debris samples and send them to a laboratory for radiological analysis. By identifying unique attributes of the fissile material, including its impurities and contaminants, one could trace the path back to its origin.\" The process is analogous to identifying a criminal by fingerprints. \"The goal would be twofold: first, to deter leaders of nuclear states from selling weapons to terrorists by holding them accountable for any use of their weapons; second, to give leaders every incentive to tightly secure their nuclear weapons and materials.\"\nAccording to the Pentagon's June 2019 \"Doctrine for Joint Nuclear Operations\" of the Joint Chiefs of Staffs website Publication, \"Integration of nuclear weapons employment with conventional and special operations forces is essential to the success of any mission or operation.\"\nGovernance, control, and law.\nBecause they are weapons of mass destruction, the proliferation and possible use of nuclear weapons are important issues in international relations and diplomacy. In most countries, the use of nuclear force can only be authorized by the head of government or head of state. Despite controls and regulations governing nuclear weapons, there is an inherent danger of \"accidents, mistakes, false alarms, blackmail, theft, and sabotage\".\nIn the late 1940s, lack of mutual trust prevented the United States and the Soviet Union from making progress on arms control agreements. The Russell\u2013Einstein Manifesto was issued in London on July 9, 1955, by Bertrand Russell in the midst of the Cold War. It highlighted the dangers posed by nuclear weapons and called for world leaders to seek peaceful resolutions to international conflict. The signatories included eleven pre-eminent intellectuals and scientists, including Albert Einstein, who signed it just days before his death on April 18, 1955. A few days after the release, philanthropist Cyrus S. Eaton offered to sponsor a conference\u2014called for in the manifesto\u2014in Pugwash, Nova Scotia, Eaton's birthplace. This conference was to be the first of the Pugwash Conferences on Science and World Affairs, held in July 1957.\nBy the 1960s, steps were taken to limit both the proliferation of nuclear weapons to other countries and the environmental effects of nuclear testing. The Partial Nuclear Test Ban Treaty (1963) restricted all nuclear testing to underground nuclear testing, to prevent contamination from nuclear fallout, whereas the Treaty on the Non-Proliferation of Nuclear Weapons (1968) attempted to place restrictions on the types of activities signatories could participate in, with the goal of allowing the transference of non-military nuclear technology to member countries without fear of proliferation.\nIn 1957, the International Atomic Energy Agency (IAEA) was established under the mandate of the United Nations to encourage development of peaceful applications of nuclear technology, provide international safeguards against its misuse, and facilitate the application of safety measures in its use. In 1996, many nations signed the Comprehensive Nuclear-Test-Ban Treaty, which prohibits all testing of nuclear weapons. A testing ban imposes a significant hindrance to nuclear arms development by any complying country. The Treaty requires the ratification by 44 specific states before it can go into force; as of 2012[ [update]], the ratification of eight of these states is still required.\nAdditional treaties and agreements have governed nuclear weapons stockpiles between the countries with the two largest stockpiles, the United States and the Soviet Union, and later between the United States and Russia. These include treaties such as SALT II (never ratified), START I (expired), INF, START II (never in effect), SORT, and New START, as well as non-binding agreements such as SALT I and the Presidential Nuclear Initiatives of 1991. Even when they did not enter into force, these agreements helped limit and later reduce the numbers and types of nuclear weapons between the United States and the Soviet Union/Russia.\nNuclear weapons have also been opposed by agreements between countries. Many nations have been declared Nuclear-Weapon-Free Zones, areas where nuclear weapons production and deployment are prohibited, through the use of treaties. The Treaty of Tlatelolco (1967) prohibited any production or deployment of nuclear weapons in Latin America and the Caribbean, and the Treaty of Pelindaba (1964) prohibits nuclear weapons in many African countries. As recently as 2006 a Central Asian Nuclear Weapon Free Zone was established among the former Soviet republics of Central Asia prohibiting nuclear weapons. \nIn 1996, the International Court of Justice, the highest court of the United Nations, issued an Advisory Opinion concerned with the \"Legality of the Threat or Use of Nuclear Weapons\". The court ruled that the use or threat of use of nuclear weapons would violate various articles of international law, including the Geneva Conventions, the Hague Conventions, the UN Charter, and the Universal Declaration of Human Rights. Given the unique, destructive characteristics of nuclear weapons, the International Committee of the Red Cross calls on States to ensure that these weapons are never used, irrespective of whether they consider them lawful or not.\nAdditionally, there have been other, specific actions meant to discourage countries from developing nuclear arms. In the wake of the tests by India and Pakistan in 1998, economic sanctions were (temporarily) levied against both countries, though neither were signatories with the Nuclear Non-Proliferation Treaty. One of the stated \"casus belli\" for the initiation of the 2003 Iraq War was an accusation by the United States that Iraq was actively pursuing nuclear arms (though this was soon discovered not to be the case as the program had been discontinued). In 1981, Israel had bombed a nuclear reactor being constructed in Osirak, Iraq, in what it called an attempt to halt Iraq's previous nuclear arms ambitions; in 2007, Israel bombed another reactor being constructed in Syria.\nIn 2013, Mark Diesendorf said that governments of France, India, North Korea, Pakistan, UK, and South Africa have used nuclear power or research reactors to assist nuclear weapons development or to contribute to their supplies of nuclear explosives from military reactors. In 2017, 122 countries mainly in the Global South voted in favor of adopting the Treaty on the Prohibition of Nuclear Weapons, which eventually entered into force in 2021.\nThe Doomsday Clock measures the likelihood of a human-made global catastrophe and is published annually by the Bulletin of the Atomic Scientists. The Doomsday Clock is set a certain time from midnight, midnight being the time of global catastrophe. The two years with the highest likelihood had previously been 1953, when the Clock was set to two minutes until midnight after the US and the Soviet Union began testing hydrogen bombs, and 2018, following the failure of world leaders to address tensions relating to nuclear weapons and climate change issues. In 2023, following the escalation of nuclear threats during the Russian invasion of Ukraine, the doomsday clock was set to 90 seconds, the highest likelihood of global catastrophe since the existence of the Doomsday Clock. Given the lack of progress towards peace in Ukraine, the Doomsday Clock was moved to 89 Seconds to midnight in 2025.\nAs of 2024, Russia has intensified nuclear threats in Ukraine and is reportedly planning to place nuclear weapons in orbit, breaching the 1967 Outer Space Treaty. China is significantly expanding its nuclear arsenal, with projections of over 1,000 warheads by 2030 and up to 1,500 by 2035. North Korea is progressing in intercontinental ballistic missile tests and has a mutual-defense treaty with Russia, exchanging artillery for possible missile technology. Iran is currently viewed as a nuclear \"threshold\" state.\nDisarmament.\nNuclear disarmament refers to both the act of reducing or eliminating nuclear weapons and to the end state of a nuclear-free world, in which nuclear weapons are eliminated.\nBeginning with the 1963 Partial Test Ban Treaty and continuing through the 1996 Comprehensive Nuclear-Test-Ban Treaty, there have been many treaties to limit or reduce nuclear weapons testing and stockpiles. The 1968 Nuclear Non-Proliferation Treaty has as one of its explicit conditions that all signatories must \"pursue negotiations in good faith\" towards the long-term goal of \"complete disarmament\". The nuclear-weapon states have largely treated that aspect of the agreement as \"decorative\" and without force.\nOnly one country\u2014South Africa\u2014has ever fully renounced nuclear weapons they had independently developed. The former Soviet republics of Belarus, Kazakhstan, and Ukraine returned Soviet nuclear arms stationed in their countries to Russia after the collapse of the USSR.\nProponents of nuclear disarmament say that it would lessen the probability of nuclear war, especially accidentally. Critics of nuclear disarmament say that it would undermine the present nuclear peace and deterrence and would lead to increased global instability. Various American elder statesmen, who were in office during the Cold War period, have been advocating the elimination of nuclear weapons. These officials include Henry Kissinger, George Shultz, Sam Nunn, and William Perry. In January 2010, Lawrence M. Krauss stated that \"no issue carries more importance to the long-term health and security of humanity than the effort to reduce, and perhaps one day, rid the world of nuclear weapons\".\nIn January 1986, Soviet leader Mikhail Gorbachev publicly proposed a three-stage program for abolishing the world's nuclear weapons by the end of the 20th century. In the years after the end of the Cold War, there have been numerous campaigns to urge the abolition of nuclear weapons, such as that organized by the Global Zero movement, and the goal of a \"world without nuclear weapons\" was advocated by United States President Barack Obama in an April 2009 speech in Prague. A CNN poll from April 2010 indicated that the American public was nearly evenly split on the issue.\nSome analysts have argued that nuclear weapons have made the world relatively safer, with peace through deterrence and through the stability\u2013instability paradox, including in south Asia. Kenneth Waltz has argued that nuclear weapons have helped keep an uneasy peace, and further nuclear weapon proliferation might even help avoid the large scale conventional wars that were so common before their invention at the end of World War II. But former Secretary Henry Kissinger said in 2010 that there is a new danger, which cannot be addressed by deterrence: \"The classical notion of deterrence was that there was some consequences before which aggressors and evildoers would recoil. In a world of suicide bombers, that calculation doesn't operate in any comparable way\". George Shultz has said, \"If you think of the people who are doing suicide attacks, and people like that get a nuclear weapon, they are almost by definition not deterrable\".\nAs of early 2019, more than 90% of world's 13,865 nuclear weapons were owned by Russia and the United States.\nUnited Nations.\nThe UN Office for Disarmament Affairs (UNODA) is a department of the United Nations Secretariat established in January 1998 as part of the United Nations Secretary-General Kofi Annan's plan to reform the UN as presented in his report to the General Assembly in July 1997.\nIts goal is to promote nuclear disarmament and non-proliferation and the strengthening of the disarmament regimes in respect to other weapons of mass destruction, chemical and biological weapons. It also promotes disarmament efforts in the area of conventional weapons, especially land mines and small arms, which are often the weapons of choice in contemporary conflicts.\nControversy.\nEthics.\nEven before the first nuclear weapons had been developed, scientists involved with the Manhattan Project were divided over the use of the weapon. The role of the two atomic bombings of the country in Japan's surrender and the US's ethical justification for them has been the subject of scholarly and popular debate for decades. The question of whether nations should have nuclear weapons, or test them, has been continually and nearly universally controversial.\nNotable nuclear weapons accidents.\nThe production and deployment of nuclear weapons has involved many accidents that resulted in either some radiation casualties, or the near-miss possibility of the unauthorized and unintended detonation of a nuclear weapon. These include:\nGovernmental nuclear agencies frequently point to their procedures and technical safeguards as what has prevented accidental nuclear explosions. But it has also been argued by historians, political scientists, and historical participants that in many instances, the avoidance of an unintended nuclear explosions was due less to correct implementation of controls, but instead can be attributed to disobedience, technical failures, or other factors beyond strict human control. Reliance on non-controlled factors is sometimes referred to as \"luck\". General George Lee Butler, commander of the US Strategic Air Command (1991\u20131992), argued in 1999 that \"...we escaped the Cold War without a nuclear holocaust by some combination of skill, luck, and divine intervention, and I suspect the latter in greatest proportion.\" Dean Acheson, US Secretary of State during the Cuban Missile Crisis, concluded similarly in 1969 that it was ultimately \"plain dumb luck\" that resolved that crisis peacefully.\nNuclear testing and fallout.\nOver 500 atmospheric nuclear weapons tests were conducted at various sites around the world from 1945 to 1980. Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when the Castle Bravo hydrogen bomb test at the Pacific Proving Grounds contaminated the crew and catch of the Japanese fishing boat \"Lucky Dragon\". One of the fishermen died in Japan seven months later, and the fear of contaminated tuna led to a temporary boycotting of the popular staple in Japan. The incident caused widespread concern around the world, especially regarding the effects of nuclear fallout and atmospheric nuclear testing, and \"provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries\".\nAs public awareness and concern mounted over the possible health hazards associated with exposure to the nuclear fallout, various studies were done to assess the extent of the hazard. A Centers for Disease Control and Prevention/ National Cancer Institute study claims that fallout from atmospheric nuclear tests would lead to perhaps 11,000 excess deaths among people alive during atmospheric testing in the United States from all forms of cancer, including leukemia, from 1951 to well into the 21st century.\nAs of March\u00a02009[ [update]], the US is the only nation that compensates nuclear test victims. Since the Radiation Exposure Compensation Act of 1990, more than $1.38\u00a0billion in compensation has been approved. The money is going to people who took part in the tests, notably at the Nevada Test Site, and to others exposed to the radiation.\nIn addition, leakage of byproducts of nuclear weapon production into groundwater has been an ongoing issue, particularly at the Hanford site.\nEffects of nuclear explosions.\nEffects of nuclear explosions on human health.\nSome scientists estimate that a nuclear war with 100 Hiroshima-size nuclear explosions on cities could cost the lives of tens of millions of people from long-term climatic effects alone. The climatology hypothesis is that \"if\" each city firestorms, a great deal of soot could be thrown up into the atmosphere which could blanket the earth, cutting out sunlight for years on end, causing the disruption of food chains, in what is termed a nuclear winter.\nPeople near the Hiroshima explosion and who managed to survive the explosion subsequently suffered a variety of horrible medical effects. Some of these effects are still present to this day:\nFallout exposure\u2014depending on if further afield individuals shelter in place or evacuate perpendicular to the direction of the wind, and therefore avoid contact with the fallout plume, and stay there for the days and weeks after the nuclear explosion, their exposure to fallout, and therefore their total dose, will vary. With those who do shelter in place, and or evacuate, experiencing a total dose that would be negligible in comparison to someone who just went about their life as normal.\nStaying indoors until after the most hazardous fallout isotope, I-131 decays away to 0.1% of its initial quantity after ten half-lifes\u2014which is represented by 80 days in I-131s case, would make the difference between likely contracting Thyroid cancer or escaping completely from this substance depending on the actions of the individual.\nEffects of nuclear war.\nNuclear war could yield unprecedented human death tolls and habitat destruction. Detonating large numbers of nuclear weapons would have an immediate, short term and long-term effects on the climate, potentially causing cold weather known as a \"nuclear winter\". In 1982, Brian Martin estimated that a US\u2013Soviet nuclear exchange might kill 400\u2013450\u00a0million directly, mostly in the United States, Europe and Russia, and maybe several hundred million more through follow-up consequences in those same areas. Many scholars have posited that a global thermonuclear war with Cold War-era stockpiles, or even with the current smaller stockpiles, may lead to the extinction of the human race. The \"International Physicians for the Prevention of Nuclear War\" believe that nuclear war could indirectly contribute to human extinction via secondary effects, including environmental consequences, societal breakdown, and economic collapse. It has been estimated that a relatively small-scale nuclear exchange between India and Pakistan involving 100 Hiroshima yield (15 kilotons) weapons, could cause a nuclear winter and kill more than a billion people.\nAccording to a peer-reviewed study published in the journal \"Nature Food\" in August 2022, a full-scale nuclear war between the US and Russia would directly kill 360 million people and more than 5 billion people would die from starvation. More than 2 billion people could die from a smaller-scale nuclear war between India and Pakistan.\nPublic opposition.\nPeace movements emerged in Japan and in 1954 they converged to form a unified \"Japan Council against Atomic and Hydrogen Bombs.\" Japanese opposition to nuclear weapons tests in the Pacific Ocean was widespread, and \"an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons\".\nIn the United Kingdom, the Aldermaston Marches organised by the Campaign for Nuclear Disarmament (CND) took place at Easter 1958, when, according to the CND, several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.\nIn 1959, a letter in the \"Bulletin of the Atomic Scientists\" was the start of a successful campaign to stop the Atomic Energy Commission dumping radioactive waste in the sea 19 kilometers from Boston. In 1962, Linus Pauling won the Nobel Peace Prize for his work to stop the atmospheric testing of nuclear weapons, and the \"Ban the Bomb\" movement spread.\nIn 1963, many countries ratified the Partial Test Ban Treaty prohibiting atmospheric nuclear testing. Radioactive fallout became less of an issue and the anti-nuclear weapons movement went into decline for some years. A resurgence of interest occurred amid European and American fears of nuclear war in the 1980s.\nCosts and technology spin-offs.\nAccording to an audit by the Brookings Institution, between 1940 and 1996, the US spent $ in present-day terms on nuclear weapons programs, 57% of which was spent on building nuclear weapons delivery systems. Six-point-three percent of the total amount, $ in present-day terms, was spent on environmental remediation and nuclear waste management\u2014for example, cleaning up the Hanford site\u2014and 7% of the total $820 billion was spent on making nuclear weapons themselves.\nNon-weapons uses.\nPeaceful nuclear explosions (PNEs) are nuclear explosions conducted for non-military purposes, such as activities related to economic development including the creation of canals. During the 1960s and 1970s, both the United States and the Soviet Union conducted a number of PNEs. The United States created plans for several uses of PNEs, including Operation Plowshare. Six of the explosions by the Soviet Union are considered to have been of an applied nature, not just tests.\nThe United States and the Soviet Union later halted their programs. Definitions and limits are covered in the Peaceful Nuclear Explosions Treaty of 1976. The stalled Comprehensive Nuclear-Test-Ban Treaty of 1996 would prohibit all nuclear explosions, regardless of whether they are for peaceful purposes or not.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21787", "revid": "1777127", "url": "https://en.wikipedia.org/wiki?curid=21787", "title": "Nathaniel Hawthorne", "text": "American author (1804\u20131864)\nNathaniel Hawthorne (n\u00e9 Hathorne; July 4, 1804 \u2013 May 19, 1864) was an American novelist and short story writer. His works often focus on history, morality, and religion.\nHe was born in 1804 in Salem, Massachusetts, from a family long associated with that town. Hawthorne entered Bowdoin College in 1821, was elected to Phi Beta Kappa in 1824, and graduated in 1825. He published his first work in 1828, the novel \"Fanshawe\"; he later tried to suppress it, feeling that it was not equal to the standard of his later work. He published several short stories in periodicals, which he collected in 1837 as \"Twice-Told Tales\". The following year, he became engaged to Sophia Peabody. He worked at the Boston Custom House and joined Brook Farm, a transcendentalist community, before marrying Peabody in 1842. The couple moved to The Old Manse in Concord, Massachusetts, later moving to Salem, the Berkshires, then to The Wayside in Concord. \"The Scarlet Letter\" was published in 1850, followed by a succession of other novels. A political appointment as consul took Hawthorne and family to Europe before their return to Concord in 1860. Hawthorne died on May 19, 1864.\nMuch of Hawthorne's writing centers on New England, and many works feature moral metaphors with an anti-Puritan inspiration. His fiction works are considered part of the Romantic movement and, more specifically, dark romanticism. His themes often center on the inherent evil and sin of humanity, and his works often have moral messages and deep psychological complexity. His published works include novels, short stories, and a biography of his college friend Franklin Pierce, written for his 1852 campaign for President of the United States, which Pierce won, becoming the 14th president.\nBiography.\nEarly life.\nNathaniel Hathorne, as his name was originally spelled, was born on July 4, 1804, in Salem, Massachusetts; his birthplace is preserved and open to the public. His great-great-great-grandfather, William Hathorne, was a Puritan and the first of the family to emigrate from England. He settled in Dorchester, Massachusetts, before moving to Salem. There he became an important member of the Massachusetts Bay Colony and held many political positions, including magistrate and judge, becoming infamous for his harsh sentencing. William's son, Hawthorne's great-great-grandfather John Hathorne, was one of the judges who oversaw the Salem witch trials. Hawthorne probably added the \"w\" to his surname in his early twenties, shortly after graduating from college, in an effort to disassociate himself from his notorious forebears. Hawthorne's father Nathaniel Hathorne Sr. was a sea captain who died in 1808 of yellow fever in Dutch Suriname; he had been a member of the East India Marine Society. After his death, his widow moved with young Nathaniel, his older sister Elizabeth, and their younger sister Louisa to live with relatives named the Mannings in Salem, where they lived for 10 years. Young Hawthorne was hit on the leg while playing \"bat and ball\" on November 10, 1813, and he became lame and bedridden for a year, though several physicians could find nothing wrong with him.\nIn the summer of 1816, the family lived as boarders with farmers before moving to a home recently built specifically for them by Hawthorne's uncles Richard and Robert Manning in Raymond, Maine, near Sebago Lake. Years later, Hawthorne looked back at his time in Maine fondly: \"Those were delightful days, for that part of the country was wild then, with only scattered clearings, and nine tenths of it primeval woods.\" In 1819, he was sent back to Salem for school and soon complained of homesickness and being too far from his mother and sisters. He distributed seven issues of \"The Spectator\" to his family in August and September 1820 for fun. The homemade newspaper was written by hand and included essays, poems, and news featuring the young author's adolescent humor.\nHawthorne's uncle Robert Manning insisted that the boy attend college, despite Hawthorne's protests. With the financial support of his uncle, Hawthorne was sent to Bowdoin College in 1821, partly because of family connections in the area, and also because of its relatively inexpensive tuition rate. Hawthorne met future president Franklin Pierce on the way to Bowdoin, at the stage stop in Portland, and the two became fast friends. Once at the school, he also met future poet Henry Wadsworth Longfellow, future congressman Jonathan Cilley, and future naval reformer Horatio Bridge. He graduated with the class of 1825, and later described his college experience to Richard Henry Stoddard: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I was educated (as the phrase is) at Bowdoin College. I was an idle student, negligent of college rules and the Procrustean details of academic life, rather choosing to nurse my own fancies than to dig into Greek roots and be numbered among the learned Thebans.\nEarly career.\nHawthorne's first published work, \"Fanshawe: A Tale\", based on his experiences at Bowdoin College, appeared anonymously in October 1828, printed at the author's own expense of $100. Although it received generally positive reviews, it did not sell well. He published several minor pieces in the \"Salem Gazette\".\nIn 1836, Hawthorne served as the editor of the \"American Magazine of Useful and Entertaining Knowledge\". At the time, he boarded with poet Thomas Green Fessenden on Hancock Street in Beacon Hill in Boston. He was offered an appointment as weigher and gauger at the Boston Custom House at a salary of $1,500 a year, which he accepted on January 17, 1839. During his time there, he rented a room from George Stillman Hillard, business partner of Charles Sumner. Hawthorne wrote in the comparative obscurity of what he called his \"owl's nest\" in the family home. As he looked back on this period of his life, he wrote: \"I have not lived, but only dreamed about living.\" He contributed short stories to various magazines and annuals, including \"Young Goodman Brown\" and \"The Minister's Black Veil\", though none drew major attention to him. Horatio Bridge offered to cover the risk of collecting these stories in the spring of 1837 into the volume \"Twice-Told Tales\", which made Hawthorne known locally.\nMarriage and family.\nWhile at Bowdoin, Hawthorne wagered a bottle of Madeira wine with his friend Jonathan Cilley that Cilley would get married before Hawthorne did. By 1836, he had won the bet, but he did not remain a bachelor for life. He had public flirtations with Mary Silsbee and Elizabeth Peabody, then he began pursuing Peabody's sister, the illustrator and transcendentalist Sophia Peabody. He joined the transcendentalist Utopian community at Brook Farm in 1841, not because he agreed with the experiment but because it helped him save money to marry Sophia. He paid a $1,000 deposit and was put in charge of shoveling the hill of manure referred to as \"the Gold Mine\". He left later that year, though his Brook Farm adventure became an inspiration for his novel \"The Blithedale Romance\". Hawthorne married Sophia Peabody on July 9, 1842, at a ceremony in the Peabody parlor on West Street in Boston. The couple moved to The Old Manse in Concord, Massachusetts, where they lived for three years. His neighbor Ralph Waldo Emerson invited him into his social circle, but Hawthorne was almost pathologically shy and stayed silent at gatherings. At the Old Manse, Hawthorne wrote most of the tales collected in \"Mosses from an Old Manse\".\nLike Hawthorne, Sophia was a reclusive person. Throughout her early life, she had frequent migraines and underwent several experimental medical treatments. She was mostly bedridden until her sister introduced her to Hawthorne, after which her headaches seem to have abated. The Hawthornes enjoyed a long and happy marriage. He referred to her as his \"Dove\" and wrote that she \"is, in the strictest sense, my sole companion; and I need no other\u2014there is no vacancy in my mind, any more than in my heart ... Thank God that I suffice for her boundless heart!\" Sophia greatly admired her husband's work. She wrote in one of her journals:\nI am always so dazzled and bewildered with the richness, the depth, the\u00a0... jewels of beauty in his productions that I am always looking forward to a second reading where I can ponder and muse and fully take in the miraculous wealth of thoughts.\nPoet Ellery Channing came to the Old Manse for help on the first anniversary of the Hawthornes' marriage. A local teenager named Martha Hunt had drowned herself in the river and Hawthorne's boat \"Pond Lily\" was needed to find her body. Hawthorne helped recover the corpse, which he described as \"a spectacle of such perfect horror\u00a0... She was the very image of death-agony\". The incident later inspired a scene in his novel \"The Blithedale Romance\".\nThe Hawthornes had three children. Their first was daughter Una, born March 3, 1844; her name was a reference to \"The Faerie Queene\", to the displeasure of family members. Hawthorne wrote to a friend, \"I find it a very sober and serious kind of happiness that springs from the birth of a child\u00a0... There is no escaping it any longer. I have business on earth now, and must look about me for the means of doing it.\" In October 1845, the Hawthornes moved to Salem. In 1846, their son Julian was born. Hawthorne wrote to his sister Louisa on June 22, 1846: \"A small troglodyte made his appearance here at ten minutes to six o'clock this morning, who claimed to be your nephew.\" Daughter Rose was born in May 1851, and Hawthorne called her his \"autumnal flower\".\nMiddle years.\nIn April 1846, Hawthorne was officially appointed the Surveyor for the District of Salem and Beverly and Inspector of the Revenue for the Port of Salem at an annual salary of $1,200. He had difficulty writing during this period, as he admitted to Longfellow:\nI am trying to resume my pen\u00a0... Whenever I sit alone, or walk alone, I find myself dreaming about stories, as of old; but these forenoons in the Custom House undo all that the afternoons and evenings have done. I should be happier if I could write.\nThis employment, like his earlier appointment to the custom house in Boston, was vulnerable to the politics of the spoils system. Hawthorne was a Democrat and lost this job due to the change of administration in Washington after the presidential election of 1848. He wrote a letter of protest to the \"Boston Daily Advertiser\", which was attacked by the Whigs and supported by the Democrats, making Hawthorne's dismissal a much-talked about event in New England. He was deeply affected by the death of his mother in late July, calling it \"the darkest hour I ever lived\". He was appointed the corresponding secretary of the Salem Lyceum in 1848. Guests who came to speak that season included Emerson, Thoreau, Louis Agassiz, and Theodore Parker.\nHawthorne returned to writing and published \"The Scarlet Letter\" in mid-March 1850, including a preface that refers to his three-year tenure in the Custom House and makes several allusions to local politicians\u2014who did not appreciate their treatment. It was one of the first mass-produced books in America, selling 2,500 volumes within ten days and earning Hawthorne $1,500 over 14 years. The book became a best-seller in the United States and initiated his most lucrative period as a writer. Hawthorne's friend Edwin Percy Whipple objected to the novel's \"morbid intensity\" and its dense psychological details, writing that the book \"is therefore apt to become, like Hawthorne, too painfully anatomical in his exhibition of them\", while 20th-century writer D.\u00a0H. Lawrence said that there could be no more perfect work of the American imagination than \"The Scarlet Letter\".\nHawthorne and his family moved to a small red farmhouse near Lenox, Massachusetts, at the end of March 1850. He became friends with Herman Melville beginning on August 5, 1850, when the authors met at a picnic hosted by a mutual friend. Melville had just read Hawthorne's short story collection \"Mosses from an Old Manse\", and his unsigned review of the collection was printed in \"The Literary World\" on August 17 and August 24 titled \"Hawthorne and His Mosses\". Melville wrote that these stories revealed a dark side to Hawthorne, \"shrouded in blackness, ten times black\". He was composing his novel \"Moby-Dick\" at the time, and dedicated the work in 1851 to Hawthorne: \"In token of my admiration for his genius, this book is inscribed to Nathaniel Hawthorne.\"\nHawthorne's time in the Berkshires was very productive. While there, he wrote \"The House of the Seven Gables\" (1851), which poet and critic James Russell Lowell said was better than \"The Scarlet Letter\" and called \"the most valuable contribution to New England history that has been made.\" He also wrote \"The Blithedale Romance\" (1852), his only work written in the first person. He also published \"A Wonder-Book for Girls and Boys\" in 1851, a collection of short stories retelling myths that he had been thinking about writing since 1846. Nevertheless, poet Ellery Channing reported that Hawthorne \"has suffered much living in this place\". The family enjoyed the scenery of the Berkshires, although Hawthorne did not enjoy the winters in their small house. They left on November 21, 1851. Hawthorne noted, \"I am sick to death of Berkshire\u00a0... I have felt languid and dispirited, during almost my whole residence.\"\nThe Wayside and Europe.\nIn May 1852, the Hawthornes returned to Concord where they lived until July 1853. In February, they bought The Hillside, a home previously inhabited by Amos Bronson Alcott and his family, and renamed it The Wayside. Their neighbors in Concord included Emerson and Henry David Thoreau. In July 1852, his younger sister, Maria Louisa, drowned in the disaster of the burning of the steamboat \"Henry Clay\".\nHawthorne completed \"The Life of Franklin Pierce\", the campaign biography of his friend, which depicted him as \"a man of peaceful pursuits\". Horace Mann said, \"If he makes out Pierce to be a great man or a brave man, it will be the greatest work of fiction he ever wrote.\" In the biography, Hawthorne depicts Pierce as a statesman and soldier who had accomplished no great feats because of his need to make \"little noise\" and so \"withdrew into the background\". He also left out Pierce's drinking habits, despite rumors of his alcoholism, and emphasized Pierce's belief that slavery could not \"be remedied by human contrivances\" but would, over time, \"vanish like a dream\".\nWith Pierce's election as President, Hawthorne was rewarded in 1853 with the position of United States consul in Liverpool shortly after the publication of \"Tanglewood Tales\". The role was considered the most lucrative foreign service position at the time, described by Hawthorne's wife as \"second in dignity to the Embassy in London\". During this period he and his family lived in the Rock Park estate in Rock Ferry in one of the houses directly adjacent to Tranmere Beach on the Wirral shore of the River Mersey. As his journal attests, to attend his place of employment at the United States consulate in Liverpool, Hawthorne was a regular passenger on the steamboat operating between Rock Ferry and Liverpool, which departed from the Rock Ferry Slipway at the end of Bedford Road. His appointment ended in 1857 at the close of the Pierce administration. The Hawthorne family toured France and Italy until 1860. During his time in Italy, the previously clean-shaven Hawthorne grew a bushy mustache.\nThe family returned to The Wayside in 1860, and that year saw the publication of \"The Marble Faun\", his first new book in seven years. Hawthorne admitted that he had aged considerably, referring to himself as \"wrinkled with time and trouble\".\nLater years and death.\nAt the outset of the American Civil War, Hawthorne traveled with William D. Ticknor to Washington, D.C., where he met Abraham Lincoln and other notable figures. He wrote about his experiences in the essay \"Chiefly About War Matters\" in 1862.\nFailing health prevented him from completing several more romance novels. Hawthorne was suffering from pain in his stomach and insisted on a recuperative trip with his friend Franklin Pierce, though his neighbor Bronson Alcott was concerned that Hawthorne was too ill. While on a tour of the White Mountains, he died in his sleep on May 19, 1864, in Plymouth, New Hampshire. Pierce sent a telegram to Elizabeth Peabody asking her to inform Mrs. Hawthorne in person. Mrs. Hawthorne was too saddened by the news to handle the funeral arrangements herself. Hawthorne's son Julian, a freshman at Harvard College, learned of his father's death the next day; coincidentally, he was initiated into the Delta Kappa Epsilon fraternity on the same day by being blindfolded and placed in a coffin. Longfellow wrote a tribute poem to Hawthorne published in 1866 called \"\". Hawthorne was buried on what is now known as \"Authors' Ridge\" in Sleepy Hollow Cemetery, Concord, Massachusetts. Pallbearers included Longfellow, Emerson, Alcott, Oliver Wendell Holmes Sr., James T. Fields, and Edwin Percy Whipple. Emerson wrote of the funeral: \"I thought there was a tragic element in the event, that might be more fully rendered\u2014in the painful solitude of the man, which, I suppose, could no longer be endured, &amp; he died of it.\"\nHis wife Sophia and daughter Una were originally buried in England. However, in June 2006, they were reinterred in plots adjacent to Hawthorne.\nWritings.\nHawthorne had a particularly close relationship with his publishers William Ticknor and James T. Fields. Hawthorne once told Fields, \"I care more for your good opinion than for that of a host of critics.\" In fact, it was Fields who convinced Hawthorne to turn \"The Scarlet Letter\" into a novel rather than a short story. Ticknor handled many of Hawthorne's personal matters, including the purchase of cigars, overseeing financial accounts, and even purchasing clothes. Ticknor died with Hawthorne at his side in Philadelphia in 1864; according to a friend, Hawthorne was left \"apparently dazed\".\nLiterary style and themes.\nHawthorne's works belong to romanticism or, more specifically, dark romanticism, cautionary tales that suggest that guilt, sin, and evil are the most inherent natural qualities of humanity. Many of his works are inspired by Puritan New England, combining historical romance loaded with symbolism and deep psychological themes, bordering on surrealism. His depictions of the past are a version of historical fiction used only as a vehicle to express common themes of ancestral sin, guilt and retribution. His later writings also reflect his negative view of the Transcendentalism movement.\nHawthorne was predominantly a short story writer in his early career. Upon publishing \"Twice-Told Tales\", however, he noted, \"I do not think much of them,\" and he expected little response from the public. His four major romances were written between 1850 and 1860: \"The Scarlet Letter\" (1850), \"The House of the Seven Gables\" (1851), \"The Blithedale Romance\" (1852) and \"The Marble Faun\" (1860). Another novel-length romance, \"Fanshawe\", was published anonymously in 1828. Hawthorne defined a romance as being radically different from a novel by not being concerned with the possible or probable course of ordinary experience. In the preface to \"The House of the Seven Gables\", Hawthorne describes his romance-writing as using \"atmospherical medium as to bring out or mellow the lights and deepen and enrich the shadows of the picture\". The picture, Daniel Hoffman found, was one of \"the primitive energies of fecundity and creation.\"\nCritics have applied feminist perspectives and historicist approaches to Hawthorne's depictions of women. Feminist scholars are interested particularly in Hester Prynne: they recognize that while she herself could not be the \"destined prophetess\" of the future, the \"angel and apostle of the coming revelation\" must nevertheless \"be a woman.\" Camille Paglia saw Hester as mystical, \"a wandering goddess still bearing the mark of her Asiatic origins\u00a0... moving serenely in the magic circle of her sexual nature\". Lauren Berlant termed Hester \"the citizen as woman [personifying] love as a quality of the body that contains the purest light of nature,\" her resulting \"traitorous political theory\" a \"Female Symbolic\" literalization of futile Puritan metaphors. Historicists view Hester as a protofeminist and avatar of the self-reliance and responsibility that led to women's suffrage and sometime-reproductive emancipation. Anthony Splendora found her literary genealogy among other archetypally fallen but redeemed women, both historic and mythic. As examples, he offers Psyche of ancient legend; Heloise of twelfth-century France's tragedy involving world-renowned philosopher Peter Abelard; Anne Hutchinson (America's first heretic, circa 1636), and Hawthorne family friend Margaret Fuller. In Hester's first appearance, Hawthorne likens her, \"infant at her bosom\", to Mary, Mother of Jesus, \"the image of Divine Maternity\". In her study of Victorian literature, in which such \"galvanic outcasts\" as Hester feature prominently, Nina Auerbach went so far as to name Hester's fall and subsequent redemption, \"the novel's one unequivocally religious activity\". Regarding Hester as a deity figure, Meredith A. Powers found in Hester's characterization \"the earliest in American fiction that the archetypal Goddess appears quite graphically,\" like a Goddess \"not the wife of traditional marriage, permanently subject to a male overlord\"; Powers noted \"her syncretism, her flexibility, her inherent ability to alter and so avoid the defeat of secondary status in a goal-oriented civilization\".\nAside from Hester Prynne, the model women of Hawthorne's other novels\u2014from Ellen Langton of \"Fanshawe\" to Zenobia and Priscilla of \"The Blithedale Romance,\" Hilda and Miriam of \"The Marble Faun\" and Phoebe and Hepzibah of \"The House of the Seven Gables\"\u2014are more fully realized than his male characters, who merely orbit them. This observation is equally true of his short-stories, in which central females serve as allegorical figures: Rappaccini's beautiful but life-altering, garden-bound, daughter; almost-perfect Georgiana of \"The Birth-Mark\"; the sinned-against (abandoned) Ester of \"Ethan Brand\"; and goodwife Faith Brown, linchpin of Young Goodman Brown's very belief in God. \"My Faith is gone!\" Brown exclaims in despair upon seeing his wife at the Witches' Sabbath. Perhaps the most sweeping statement of Hawthorne's impetus comes from Mark Van Doren: \"Somewhere, if not in the New England of his time, Hawthorne unearthed the image of a goddess supreme in beauty and power.\"\nHawthorne also wrote nonfiction. In 2008, the Library of America selected Hawthorne's \"A show of wax-figures\" for inclusion in its two-century retrospective of American True Crime.\nCritical reception.\nHawthorne's writings were well received at the time. Contemporary response praised his sentimentality and moral purity while more modern evaluations focus on the dark psychological complexity. Herman Melville wrote a passionate review of \"Mosses from an Old Manse\", titled \"Hawthorne and His Mosses\", arguing that Hawthorne \"is one of the new, and far better generation of your writers.\" Melville describes an affinity for Hawthorne that would only increase: \"I feel that this Hawthorne has dropped germinous seeds into my soul. He expands and deepens down, the more I contemplate him; and further, and further, shoots his strong New-England roots into the hot soil of my Southern soul.\" Edgar Allan Poe wrote important reviews of both \"Twice-Told Tales\" and \"Mosses from an Old Manse\". Poe's assessment was partly informed by his contempt for allegory and moral tales, and his chronic accusations of plagiarism, though he admitted:\nThe style of Mr. Hawthorne is purity itself. His tone is singularly effective\u2014wild, plaintive, thoughtful, and in full accordance with his themes\u00a0... We look upon him as one of the few men of indisputable genius to whom our country has as yet given birth. John Neal's magazine \"The Yankee\" published the first substantial public praise of Hawthorne, saying in 1828 that the author of \"Fanshawe\" has a \"fair prospect of future success.\" Ralph Waldo Emerson wrote, \"Nathaniel Hawthorne's reputation as a writer is a very pleasing fact, because his writing is not good for anything, and this is a tribute to the man.\" Henry James praised Hawthorne, saying, \"The fine thing in Hawthorne is that he cared for the deeper psychology, and that, in his way, he tried to become familiar with it.\" Poet John Greenleaf Whittier wrote that he admired the \"weird and subtle beauty\" in Hawthorne's tales. Evert Augustus Duyckinck said of Hawthorne, \"Of the American writers destined to live, he is the most original, the one least indebted to foreign models or literary precedents of any kind.\"\nBeginning in the 1950s, critics have focused on symbolism and didacticism.\nThe critic Harold Bloom wrote that only Henry James and William Faulkner challenge Hawthorne's position as the greatest American novelist, although he admitted that he favored James as the greatest American novelist. Bloom saw Hawthorne's greatest works to be principally \"The Scarlet Letter\", followed by \"The Marble Faun\" and certain short stories, including \"My Kinsman, Major Molineux\", \"Young Goodman Brown\", \"Wakefield\", and \"Feathertop\".\nSelected works.\nAccording to Hawthorne scholar Rita K. Gollin, the \"definitive edition\" of Hawthorne's works is \"The Centenary Edition of the Works of Nathaniel Hawthorne\", edited by William Charvat and others, published by The Ohio State University Press in twenty-three volumes between 1962 and 1997. \"Tales and Sketches\" (1982) was the second volume to be published in the Library of America, \"Collected Novels\" (1983) the tenth.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nWorks"}
{"id": "21788", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=21788", "title": "Neo-paganism", "text": ""}
{"id": "21790", "revid": "47114563", "url": "https://en.wikipedia.org/wiki?curid=21790", "title": "Nagasaki", "text": "Core city in Kyushu, Japan\n, officially , is the capital and the largest city of Nagasaki Prefecture on the island of Kyushu in Japan.\nFounded by the Portuguese, the port of Nagasaki became the sole port used for trade with the Portuguese and Dutch during the 16th through 19th centuries. The Hidden Christian Sites in the Nagasaki Region have been recognized and included in the UNESCO World Heritage Sites list. Part of Nagasaki was home to a major Imperial Japanese Navy base during the First Sino-Japanese War and Russo-Japanese War. Near the end of World War II, the American atomic bombings of Hiroshima and Nagasaki made Nagasaki the second city in the world to experience a nuclear attack. The city was later rebuilt.\nAs of \u00a0, 2024[ [update]], Nagasaki has an estimated population of 392,281, and a population density of 966 people per km2. The total area is .\nHistory.\nNagasaki as a Jesuit port of call.\nThe first recorded contact between Portuguese explorers and Japan occurred in 1543, when a Portuguese ship, possibly a Chinese junk carrying Portuguese sailors, was blown off course and landed on Tanegashima, an island south of Ky\u016bsh\u016b. This event marked the beginning of direct contact between Japan and Europe.\nTwo Portuguese traders, Ant\u00f3nio Mota and Francisco Zeimoto, were among the crew members. They introduced the Japanese to firearms, specifically the Portuguese matchlock guns known as harquebuses. The local lord, Tanegashima Tokitaka, purchased two of these firearms and had local blacksmiths replicate them, leading to the development of the \"tanegashima\" guns in Japan.\nFern\u00e3o Mendes Pinto, a Portuguese adventurer and writer, claimed in his memoirs, \"Peregrina\u00e7\u00e3o\", that he was part of the first landing party in 1543. However, his account is considered unreliable, and historians generally agree that he was not among the first Europeans to reach Japan.\nThe introduction of firearms had a significant impact on Japanese warfare, contributing to the unification of Japan during the Sengoku period. The Portuguese also introduced other goods and ideas, including Christianity, which further influenced Japanese society.\nToday, the arrival of the Portuguese in 1543 is commemorated in Tanegashima with the annual Tepp\u014d Matsuri (Firearm Festival), celebrating the island's historical connection to the introduction of firearms in Japan.\nSoon after, Portuguese ships started sailing to Japan as regular trade freighters, thus increasing the contact and trade relations between Japan and the rest of the world, and particularly with mainland China, with whom Japan had previously severed its commercial and political ties, mainly due to a number of incidents involving wokou piracy in the South China Sea, with the Portuguese now serving as intermediaries between the two East Asian neighbors.\nDespite the mutual advantages derived from these trading contacts, which would soon be acknowledged by all parties involved, the lack of a proper seaport in Ky\u016bsh\u016b for the purpose of harboring foreign ships posed a major problem for both merchants and the Kyushu \"daimy\u014ds\" (feudal lords) who expected to collect great advantages from the trade with the Portuguese.\nIn the meantime, Spanish Jesuit missionary St. Francis Xavier arrived in Kagoshima, South Ky\u016bsh\u016b, in 1549. After a somewhat fruitful two-year sojourn in Japan, he left for China in 1552 but died soon afterwards. His followers who remained behind converted a number of \"daimy\u014ds\". The most notable among them was \u014cmura Sumitada. In 1569, \u014cmura granted a permit for the establishment of a port with the purpose of harboring Portuguese ships in Nagasaki, which was set up in 1571, under the supervision of the Jesuit missionary Gaspar Vilela and Portuguese Captain-Major Trist\u00e3o Vaz de Veiga, with \u014cmura's personal assistance.\nThe little harbor village quickly grew into a diverse port city, and Portuguese products imported through Nagasaki (such as tobacco, bread, textiles and a Portuguese sponge-cake called \"castellas\") were assimilated into popular Japanese culture. Tempura derived from a popular Portuguese recipe originally known as \"peixinhos da horta\", and takes its name from the Portuguese word, 'tempero,' seasoning, and refers to the tempora quadragesima, forty days of Lent during which eating meat was forbidden, another example of the enduring effects of this cultural exchange. The Portuguese also brought with them many goods from other Asian countries, such as China. The value of Portuguese exports from Nagasaki during the 16th century were estimated to ascend to over 1,000,000 \"cruzados\", reaching as many as 3,000,000 in 1637.\nDue to the instability during the Sengoku period, Sumitada and Jesuit leader Alexandro Valignano conceived a plan to pass administrative control over to the Society of Jesus rather than see the Catholic city taken over by a non-Catholic \"daimy\u014d\". Thus, for a brief period after 1580, the city of Nagasaki was a Jesuit colony, under their administrative and military control. It became a refuge for Christians escaping maltreatment in other regions of Japan. In 1587, however, Toyotomi Hideyoshi's campaign to unify the country arrived in Ky\u016bsh\u016b. Concerned with the large Christian influence in Ky\u016bsh\u016b, Hideyoshi ordered the expulsion of all missionaries, and placed the city under his direct control. However, the expulsion order went largely unenforced, and the fact remained that most of Nagasaki's population remained openly practicing Catholic.\nIn 1596, the Spanish ship \"San Felipe\" was wrecked off the coast of Shikoku, and Hideyoshi learned from its pilot that the Spanish Franciscans were the vanguard of an Iberian invasion of Japan. In response, Hideyoshi ordered the crucifixions of twenty-six Catholics in Nagasaki on February 5 of the next year (i.e. the \"Twenty-six Martyrs of Japan\"). Portuguese traders were not ostracized, however, and so the city continued to thrive.\nIn 1602, Augustinian missionaries also arrived in Japan, and when Tokugawa Ieyasu took power in 1603, Catholicism was still tolerated. Many Catholic \"daimy\u014ds\" had been critical allies at the Battle of Sekigahara, and the Tokugawa position was not strong enough to move against them. Once Osaka Castle had been taken and Toyotomi Hideyoshi's offspring killed, though, the Tokugawa dominance was assured. In addition, the Dutch and English presence allowed trade without religious strings attached. Thus, in 1614, Catholicism was officially banned and all missionaries ordered to leave. Most Catholic daimyo apostatized, and forced their subjects to do so, although a few would not renounce the religion and left the country for Macau, Luzon and Japantowns in Southeast Asia. A brutal campaign of persecution followed, with thousands of converts across Ky\u016bsh\u016b and other parts of Japan killed, tortured, or forced to renounce their religion. Many Japanese and foreign Christians were executed by public crucifixion and burning at the stake in Nagasaki. They became known as the Martyrs of Japan and were later venerated by several Popes.\nCatholicism's last gasp as an open religion and the last major military action in Japan until the Meiji Restoration was the Shimabara Rebellion of 1637. While there is no evidence that Europeans directly incited the rebellion, Shimabara Domain had been a Christian \"han\" for several decades, and the rebels adopted many Portuguese motifs and Christian icons. Consequently, in Tokugawa society the word \"Shimabara\" solidified the connection between Christianity and disloyalty. The Shimabara Rebellion also convinced many policy-makers that foreign influences were more trouble than they were worth, leading to the national isolation policy. The Portuguese were expelled from the archipelago altogether. They had previously been living on a specially constructed artificial island in Nagasaki harbour that served as a trading post, called Dejima. The Dutch were then moved from their base at Hirado onto the artificial island.\nSeclusion era.\nThe Great Fire of Nagasaki destroyed much of the city in 1663, including the Mazu shrine at the Kofuku Temple patronized by the Chinese sailors and merchants visiting the port.\nIn 1720, the ban on Dutch books was lifted, causing hundreds of scholars to flood into Nagasaki to study European science and art. Consequently, Nagasaki became a major center of what was called \"rangaku\", or \"Dutch learning\". During the Edo period, the Tokugawa shogunate governed the city, appointing a , the \"Nagasaki bugy\u014d\", as its chief administrator. During this period, Nagasaki was designated a \"shogunal city\". The number of such cities rose from three to eleven under the Tokugawa administration.\nConsensus among historians was once that Nagasaki was Japan's only window on the world during its time as a closed country in the Tokugawa era. However, nowadays, it is generally accepted that this was not the case, since Japan interacted and traded with the Ry\u016bky\u016b Kingdom, Korea and Russia through Satsuma, Tsushima and Matsumae respectively. Nevertheless, Nagasaki was depicted in contemporary art and literature as a cosmopolitan port brimming with exotic curiosities from the Western world.\nIn 1808, during the Napoleonic Wars, the Royal Navy frigate HMS \"Phaeton\" entered Nagasaki Harbor in search of Dutch trading ships. The local magistrate was unable to resist the crew\u2019s demand for food, fuel, and water, later committing \"seppuku\" as a result. Laws were passed in the wake of this incident strengthening coastal defenses, threatening death to intruding foreigners, and prompting the training of English and Russian translators.\nThe \"T\u014djinyashiki\" (\u5510\u4eba\u5c4b\u6577) or Chinese Factory in Nagasaki was also an important conduit for Chinese goods and information for the Japanese market. Various Chinese merchants and artists sailed between the Chinese mainland and Nagasaki. Some actually combined the roles of merchant and artist such as 18th century Yi Hai. It is believed that as much as one-third of the population of Nagasaki at this time may have been Chinese. The Chinese traders at Nagasaki were confined to a walled compound () which was located in the same vicinity as Dejima island, and the activities of the Chinese, though less strictly controlled than the Dutch, were closely monitored by the Nagasaki bugy\u014d.\nMeiji Japan.\nWith the Meiji Restoration, Japan opened its doors once again to foreign trade and diplomatic relations. Nagasaki became a treaty port in 1859 and modernization began in earnest in 1868. Nagasaki was officially proclaimed a city on April 1, 1889. With Christianity legalized and the Kakure Kirishitan coming out of hiding, Nagasaki regained its earlier role as a center for Roman Catholicism in Japan.\nDuring the Meiji period, Nagasaki became a center of heavy industry. Its main industry was ship-building, with the dockyards under control of Mitsubishi Heavy Industries becoming one of the prime contractors for the Imperial Japanese Navy, and with Nagasaki harbor used as an anchorage under the control of nearby Sasebo Naval District. During World War II, at the time of the nuclear attack, Nagasaki was an important industrial city, containing both plants of the Mitsubishi Steel and Arms Works, the Akunoura Engine Works, Mitsubishi Arms Plant, Mitsubishi Electric Shipyards, Mitsubishi Steel and Arms Works, Mitsubishi-Urakami Ordnance Works, several other small factories, and most of the ports storage and trans-shipment facilities, which employed about 90% of the city's labor force, and accounted for 90% of the city's industry. These connections with the Japanese war effort made Nagasaki a major target for strategic bombing by the Allies during the war.\nAtomic bombing of Nagasaki during World War II.\nIn the 12 months prior to the nuclear attack, Nagasaki had experienced five small-scale air attacks by an aggregate of 136 U.S. planes which dropped a total of 270 tons of high explosives, 53 tons of incendiaries, and 20 tons of fragmentation bombs. Of these, a raid of August 1, 1945, was the most effective, with a few of the bombs hitting the shipyards and dock areas in the southwest portion of the city, several hitting the Mitsubishi Steel and Arms Works, and six bombs landing at the Nagasaki Medical School and Hospital, with three direct hits on buildings there. While the damage from these few bombs was relatively small, it created considerable concern in Nagasaki and a number of people, principally school children, were evacuated to rural areas for safety, consequently reducing the population in the city at the time of the atomic attack.\nOn the day of the nuclear strike (August 9, 1945) the population in Nagasaki was estimated to be 263,000, which consisted of 240,000 Japanese residents, 10,000 Korean residents, 2,500 conscripted Korean workers, 9,000 Japanese soldiers, 600 conscripted Chinese workers, and 400 Allied POWs. That day, the Boeing B-29 Superfortress \"Bockscar\", commanded by Major Charles Sweeney, departed from Tinian's North Field just before dawn, this time carrying a plutonium bomb, code named \"Fat Man\". The primary target for the bomb was Kokura, with the secondary target being Nagasaki, if the primary target was too cloudy to make a visual sighting. When the plane reached Kokura at 9:44\u00a0a.m. (10:44\u00a0am. Tinian Time), the city was obscured by clouds and smoke, as the nearby city of Yahata had been firebombed on the previous day \u2013 the steel plant in Yahata had also instructed their workforce to intentionally set fire to containers of coal tar, to produce target-obscuring black smoke. Unable to make a bombing attack 'on visual' because of the clouds and smoke, and with limited fuel, the plane left the city at 10:30\u00a0a.m. for the secondary target. After 20 minutes, the plane arrived at 10:50\u00a0a.m. over Nagasaki, but the city was also concealed by clouds. Desperately short of fuel and after making a couple of bombing runs without obtaining any visual target, the crew was forced to use radar to drop the bomb. At the last minute, the opening of the clouds allowed them to make visual contact with a racetrack in Nagasaki, and they dropped the bomb on the city's Urakami Valley, midway between the Mitsubishi Steel and Arms Works in the south, and the Mitsubishi-Urakami Ordnance Works in the north. The bomb exploded 47 seconds after its release, at 11:02\u00a0a.m. at an approximate altitude of 1,800 feet.\nLess than a second after the detonation, the north of the city was destroyed and more than 10% of the city's population were killed. Among the 35,000 deaths were 150 Japanese soldiers, 6,200 out of the 7,500 employees of the Mitsubishi Munitions plant, and 24,000 others (including 2,000 Koreans). The industrial damage in Nagasaki was high, leaving 68\u200d\u2013\u200d80% of the non-dock industrial production destroyed. It was the second and, to date, the last use of a nuclear weapon in combat, and also the second detonation of a plutonium bomb. The first combat use of a nuclear weapon was the \"Little Boy\" bomb, which was dropped on the Japanese city of Hiroshima on August 6, 1945. The first plutonium bomb was tested in central New Mexico, United States, on July 16, 1945. The Fat Man bomb was more powerful than the one dropped over Hiroshima, but because of Nagasaki's more uneven terrain, there was less damage.\nContemporary era.\nThe city was rebuilt after the war, albeit dramatically changed. The pace of reconstruction was slow. The first simple emergency dwellings were not provided until 1946. The focus of redevelopment was the replacement of war industries with foreign trade, shipbuilding and fishing. This was formally declared when the Nagasaki International Culture City Reconstruction Law was passed in May 1949. New temples were built, as well as new churches, owing to an increase in the presence of Christianity. Some of the rubble was left as a memorial, such as a one-legged \"torii\" at Sann\u014d Shrine and an arch near ground zero. New structures were also raised as memorials, such as the Atomic Bomb Museum. Nagasaki remains primarily a port city, supporting a rich shipbuilding industry.\nOn January 4, 2005, the towns of I\u014djima, K\u014dyagi, Nomozaki, Sanwa, Sotome and Takashima (all from Nishisonogi District) were officially merged into Nagasaki along with the town of Kinkai the following year.\nGeography.\nNagasaki and Nishisonogi Peninsulas are located within the city limits. The city is surrounded by the cities of Isahaya and Saikai, and the towns of Togitsu and Nagayo in Nishisonogi District.\nNagasaki lies at the head of a long bay that forms the best natural harbor on the island of Ky\u016bsh\u016b. The main commercial and residential area of the city lies on a small plain near the end of the bay. Two rivers divided by a mountain spur form the two main valleys in which the city lies. The heavily built-up area of the city is confined by the terrain to less than .\nClimate.\nNagasaki has the typical humid subtropical climate of Ky\u016bsh\u016b and Honsh\u016b, characterized by mild winters and long, hot, and humid summers. Apart from Kanazawa and Shizuoka, it is the wettest sizeable city in Japan. In the summer, the combination of persistent heat and high humidity results in unpleasant conditions, with wet-bulb temperatures sometimes reaching . In the winter, however, Nagasaki is drier and sunnier than Got\u014d to the west, and temperatures are slightly milder than further inland in Ky\u016bsh\u016b. Since records began in 1878, the wettest month has been July 1982, with including in a single day, whilst the driest month has been September 1967, with . Precipitation occurs year-round, though winter is the driest season; rainfall peaks sharply in June and July. August is the warmest month of the year. On January 24, 2016, a snowfall of was recorded.\nTransportation.\nThe nearest airport is Nagasaki Airport in the nearby city of \u014cmura. The Kyushu Railway Company (JR Kyushu) provides rail transportation on the Nishi Kyushu Shinkansen and Nagasaki Main Line, whose terminal is at Nagasaki Station. In addition, the Nagasaki Electric Tramway operates five routes in the city. The Nagasaki Expressway serves vehicular traffic with interchanges at Nagasaki and Susukizuka. In addition, six national highways crisscross the city: Route 34, 202, 206, 251, 324, and 499.\nDemographics.\nOn August 9, 1945, the population was estimated to be 263,000. As of February 1, 2024, the city had a population of 392,281 and a population density of 1,629 people per km2.\nSports.\nNagasaki is represented in the J.League of football with its local club, V-Varen Nagasaki.\nEvents.\nThe Nagasaki Lantern Festival is celebrated annually over the first 15 days of Chinese New Year and is the largest of its kind in all of Japan.\nKunchi, the most famous festival in Nagasaki, is held from October 7\u20139.\nThe Prince Takamatsu Cup Nishinippon Round-Ky\u016bsh\u016b Ekiden, the world's longest relay race, begins in Nagasaki each November.\nSister cities.\nThe city of Nagasaki maintains sister cities or friendship relations with other cities worldwide.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21791", "revid": "50976914", "url": "https://en.wikipedia.org/wiki?curid=21791", "title": "Nanjing", "text": "Capital of Jiangsu, China\nNanjing is the capital of Jiangsu, a province in East China. The city, which is located in the southwestern corner of the province, has 11 districts, an administrative area of , and as of 2021[ [update]] a population of 9,423,400.\nSituated in the Yangtze River Delta, Nanjing has a prominent place in Chinese history and culture, having served as the capital of various Chinese dynasties, kingdoms and republican governments dating from the 3rd century to 1949, and has thus long been a major center of culture, education, research, politics, economy, transport networks and tourism, being the home to one of the world's largest inland ports. The city is also one of the fifteen sub-provincial cities in the People's Republic of China's administrative structure, enjoying jurisdictional and economic autonomy only slightly less than that of a province. It has also been awarded the title of 2008 Habitat Scroll of Honor of China, Special UN Habitat Scroll of Honor Award and National Civilized City. Nanjing is also considered a Beta (global second-tier) city classification, together with Chongqing, Hangzhou and Tianjin by the Globalization and World Cities Research Network, and ranked as one of the world's top 100 cities in the Global Financial Centres Index.\nAs of 2021, Nanjing has 68 institutions of higher learning, including 13 double-first-class universities, ten 111-plan universities, eight 211 universities, and 97 academies. Nanjing University, which has a long history, is among the world's top 10 universities ranked by the Nature Index. The ratio of college students to the total population ranks No.1 among large cities nationwide. Nanjing has the fifth-largest scientific research output of any city in the world. As of 2024, it has been ranked as the world's second most prolific scientific research center in earth and environmental sciences and the world's third most prolific scientific research center in chemistry and physical sciences, according to the Nature Index.\nNanjing, one of the nation's most important cities for over a thousand years, is recognized as one of the Four Great Ancient Capitals of China. It has been one of the world's largest cities, enjoying peace and prosperity despite various wars and disasters. Nanjing served as the capital of Eastern Wu (229\u2013280), one of the three major states in the Three Kingdoms period; the Eastern Jin and each of the Southern dynasties (Liu Song, Southern Qi, Liang and Chen), which successively ruled southern China from 317 to 589; the Southern Tang (937\u201375), one of the Ten Kingdoms; the Ming dynasty when, for the first time, all of China was ruled from a city, one city (1368\u20131421); and the Republic of China under the nationalist Kuomintang (1927\u201337, 1946\u201349) before its flight to Taiwan by Chiang Kai-Shek during the Chinese Civil War. The city also served as the seat of the rebel Taiping Heavenly Kingdom (1853\u201364) and the Japanese puppet regime of Wang Jingwei (1940\u201345) during the Second Sino-Japanese War. It suffered many notable devastating atrocities in both conflicts, most notably the Nanjing Massacre from late 1937 to early 1938.\nNanjing became the capital city of Jiangsu province in 1952, after serving as a Direct-administered Municipality from 1949 to 1952 following the establishment of the People's Republic of China. It has many important heritage sites, including the Presidential Palace, Sun Yat-sen Mausoleum and Ming Xiaoling Mausoleum. Nanjing is famous for human historical landscapes, mountains and waters such as Fuzimiao, Ming Palace, Chaotian Palace, Porcelain Tower, Drum Tower, Stone City, City Wall, Qinhuai River, Xuanwu Lake and Purple Mountain. Key cultural facilities include Nanjing Library, Nanjing Museum and Jiangsu Art Museum.\nNames.\nThe name Nanjing (\"Southern Capital\") was originally informal, appearing in Xiao Zixian's 6th-century reply to Xiao Tong during the Northern and Southern Dynasties era of Chinese history. Synonyms were also used, like Nandu (, ). Under the Hongwu Emperor who founded the Ming dynasty, after the abandonment of plans for a third capital at Fengyang, a distinction began to be made between his northern capital at Kaifeng and the southern one at Yingtian (, , \"Following Heaven\"). This distinction was continued and eventually formalized after his son the Yongle Emperor relocated his court to Shuntian or Beijing (\"Northern Capital\"). The continuation of the dual arrangement was required to respect the wishes of his father, whose Ancestral Injunctions had insisted Nanjing should remain a permanent imperial capital. The Nanjing form of Lower Yangtze Mandarin remained a prestige dialect and the imperial lingua franca for centuries, producing formerly common romanizations of the name as Nanqim, Nankin, and Nanking. The less common Wade-Giles form of Nan-ching was an earlier attempt to represent its pronunciation in the Beijing form of Mandarin, now represented in pinyin as .\nThe city has a number of other names, and some historical names are now used as names of districts of the city.\nDuring the Warring States Era, settlements within modern Nanjing were known as Yuecheng (, , \"Yue City\") and Jinlingyi (, , \"City of the Golden\" or \"Precious Burial Mound\") or Jinling (, ), from which Nanjing is sometimes known as Jincheng (, , \"Golden City\"). Under the Qin, Jinling was renamed Moling (, , \"Fodder Mound\").\nJianye (, , \"Establishing Merit\") was adopted as the name of the Wu capital during the Three Kingdoms Era. The city first became an imperial Chinese capital under the Sima Jin dynasty under the name Jiankang, a change adopted to avoid the naming taboo occasioned by the elevation of Emperor Min, whose personal name was Sima Ye. Under the Tang dynasty, it was known as Shengzhou (, , \"Ascending Prefecture\").\nDuring the Qing dynasty, the city resumed official use of its Northern Song name of Jiangning (, , \"Pacified area of the Yangtze\"), romanized at the time as Kiangning. The Chinese abbreviation of \"ji\u0101ng\" () for Jiangning formed the first syllable of a compound (with \"s\u016b\" from Suzhou) that was the source of the provincial name Jiangsu. As the capital of the Taiping Heavenly Kingdom from 1851 to 1864, Nanjing was known as Tianjing (, , \"Heavenly Capital\" or \"Capital of Heaven\"). With the fall of the Qing Empire in 1911, the city was renamed Nanjing in 1912 and was the capital of the Provisional Government of the new Republic of China. However, the capital returned to Beijing by October of the same year, though the name change was retained. With the success of Kuomintang's Northern Expedition in 1927, Nanjing again became the capital of the Republic of China, and until the fall of the republic in 1949, the Chinese abbreviation (, \"capital\") was used for the city; the same abbreviation is now used for Beijing.\nHistory.\nPrehistory.\nThe 1993 discovery of \"Nanjing Man\" in Hulu Cave in Jiangning District established that reached eastern China around 600,000 years ago, hundreds of thousands of years earlier than previously thought. Following the advent of in China and the end of the Last Glacial Period, the area around Nanjing was home to Neolithic settlements intermediate between societies along the Yellow River such as the Dawenkou culture and those around Lake Tai and Hangzhou Bay such as the Majiabang and Songze cultures. Agriculture was being practiced in Qixia District by 5000\u00a0BC, and the local Beiyinyangying culture (, ) possessed , a kind of rice wine vessel, by about 3000\u00a0BC.\nAbout 2000\u00a0BC, the Qinhuai River Basin was the home of the dense Bronze Age settlements of the Hushu culture (, ). The earliest cities in Nanjing were formed around these settlements. Connecting the development of these ruins, Zhou-era burial mounds, and Chinese legends concerning the Zhou ancestors, some Chinese archaeologists have argued for Nanjing as the site of Taibo's original settlement of Wu as the Shang and Zhou encroached southward from the Central Plains around the 12th century\u00a0BC.\nAncient history.\nIn 571\u00a0BC, the state of Chu established Tangyi in Liuhe. This is the oldest extant administrative establishment in Nanjing. In 541\u00a0BC, Wu\u2014by then centered on Suzhou\u2014built Laizhu Town in Gaochun or Gucheng. The Wu king Fuchai fortified Yecheng in Nanjing in 495BC.\nWu was conquered by Yue in 473\u00a0BC, and the city was rebuilt at the mouth of the Qinhuai River the following year. Later Yuecheng was established on the outskirts of the present-day Zhonghua Gate, which was the beginning of the construction of the main city of Nanjing. In 333\u00a0BC, Chu defeated Yue and built Jinlingyi in the western part of Nanjing. It was the earliest administrative construction in the main city of Nanjing.\nIn 210\u00a0BC, the First Emperor of Qin visited the east and changed Jinling City to Moling. The area was successively part of the Kuaiji, Zhang, and Danyang prefectures under the Qin and Han dynasties. It was part of the Yangzhou region which was established by Han Wudi in Yuanfeng 5 (106\u00a0BC). Nanjing was later made the seat of Danyang Prefecture and served as the chief city in the Yangzhou region for about 400 years from the late Han to the early Tang.\nCapital of the Six Dynasties.\nThe Six Dynasties is a collective term for six Chinese dynasties that all maintained national capitals at Jiankang. The six dynasties were the Eastern Wu (AD222\u2013280), the Eastern Jin (317\u2013420), and the four Southern Dynasties of the Liu Song, Southern Qi, Liang, and Chen (420\u2013589).\nAt the end of the Eastern Han dynasty, the warlord Sun Quan, who ruled Jiangdong, moved his ruling office to Moling in 211. The following year, he built the Stone City at the site of Jinlingyi, and renamed Moling to Jianye. After Sun Quan proclaimed himself emperor in 229, Jianye served as the capital of his Eastern Wu dynasty through the Three Kingdoms period. By the time Wu was conquered by the Western Jin dynasty in 280, Jianye and its neighboring areas had been well cultivated, developing into one of the commercial, cultural, and political centers of China.\nNot long after the unification of China, the Western Jin collapsed under the weight of the War of the Eight Princes and rebellions from the so-called \"Five Barbarians\" in the north. Jianye, renamed Jiankang in 313 to avoid Emperor Min's taboo personal name, was safely isolated from the chaos and became a popular refuge for the northern nobles and wealthy families. In 318, the ruling prince in Jiankang, Sima Rui, proclaimed himself the new emperor and reestablished the dynasty as the Eastern Jin dynasty. This marked the first time a Chinese dynastic capital was moved from the north to southern China, as the north came under the rule of the Sixteen Kingdoms.\nJiankang was the center of administration in the south for more than two and a half centuries, even as China entered the Northern and Southern dynasties period. After the Eastern Jin fell in 420, it continued to serve as the capital for the Southern dynasties of Liu Song, Southern Qi, Liang and Chen. During this time, Jiankang was the international hub of East Asia. Based on historical documents, the city had 280,000 registered households. Assuming an average Nanjing household consisted of about 5.1 people, the city had more than 1.4\u00a0million residents. The Hou Jing Disturbance of 548\u2013552, however, ended with a major systematic massacre of the city's people.\nA number of spirit ways of that era, erected at the tombs of royals and other dignitaries, have survived in various degrees of preservation in Nanjing's northeastern and eastern suburbs, primarily in Qixia and Jiangning District. Possibly the best preserved of them is the ensemble of the Tomb of Xiao Xiu (475\u2013518), a brother of Emperor Wu of Liang.\nDestruction and revival.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n The phoenix birds once frolicked on Phoenix Terrace, The birds are gone, the Terrace empty, and the river flows on. Flourishing flowers of Wu Palace are buried beneath dark trails; Caps and gowns of Jin times all lie in ancient mounds. The Three-peaked Mountain lies half visible under the blue sky, The two-forked stream is separated by the White-Egret Isle in the middle. Clouds always block the sun, Chang'an cannot be seen and I grieve.\n \u2014 About the former opulent capital Jinling (present-day Nanjing) in the poem \"Climbing Phoenix Terrace at Jinling\" by Li Bai of the Tang dynasty\nThe period of division ended when the Sui dynasty reunified China and almost destroyed the entire city, turning it into a small town. The city was razed after the Sui took it over. It was renamed Shengzhou under the Tang dynasty and revived during the late Tang.\nIt was chosen as the capital and again called Jinling under the Southern Tang (937\u2013976), which succeeded the state of Yang Wu. It was renamed Jiangning in the Northern Song and again renamed Jiankang in the Southern Song. Jiankang's textile industry burgeoned and thrived during the Song despite the constant threat of invasions from the north by the Jurchen-led Jin dynasty. The court of Da Chu, a short-lived puppet state established by the Jurchens, and the court of Song were once in the city. In Jianyan 3 (1129), Jiankang became the temporary capital (, ) of the Song, being set as Eastern Capital (, ). Although people like Yue Fei argued for maintaining the imperial court being in the city, in Shaoxing 8 (1139) it withdrew from Jiankang to Lin'an (present Hangzhou) and Jiankang relegated to the \"preserving capital\" (, ).\nThe Southern Song were eventually destroyed by the Mongols. During the Mongols' rule as the Yuan dynasty, the city's status as a hub of the textile industry was further consolidated. According to Odoric of Pordenone, the prefectural capital of Jinling (\"Chilenfu\") had 360 stone bridges, which were finer than anywhere else in the world. It was well populated and had a large craft industry. In 1239, Jiankang was renamed Jiqing (\u96c6\u5e86). \nSouthern capital of the Ming dynasty.\nIn 1365\uff0cZhu Yuanzhang captured Ziqing Circuit and changed its name to Yingtian Prefecture (\u61c9\u5929\u5e9c). In 1364, he was enthroned as the King of Wu and established at Yingtian the capital. He rebuilt and expand the city of Jiankang. In 1368, Zhu Yuanzhang overthrew the Yuan and enthroned as the Hongwu Emperor of the Ming dynasty, he renamed the city Nanjing and made it the southern capital in 1368 (a central capital or \"Zhongdu\" was being planned in Zhu Yuanzhang's hometown Fengyang). In 1379, the Zhongdu project was abandoned and Nanjing became the capital of the Ming dynasty, called Jingshi (\u4eac\u5e08 \"The Capital\"). \nThe Hongwu Emperor constructed a long city wall around Nanjing, as well as a new Ming Palace complex, and government halls. It took 200,000 laborers 21\u00a0years to finish the wall, which was intended to defend the city and its surrounding region from coastal pirates. The present-day City Wall of Nanjing was mainly built during that time and today it remains in good condition and has been well preserved. It is among the longest surviving city walls in China. The Jianwen Emperor ruled from Yingtian from 1398 to 1402. It is believed that Nanjing was the largest city in the world from 1358 to 1425 with a population of 487,000 in 1400.\nHaving usurped power from his nephew and uncertain of the loyalty of the region's officials, the Yongle Emperor relocated the capital in 1421 to Beiping, where he had long served as the regional governor as the Prince of Yan. Because the new status of Yingtian was included in the Hongwu Emperor's \"ancestral injunctions\" for his dynasty, however, the Yongle Emperor was obliged to preserve its special status, at least in name. The \"northern capital\" came to be known as Beijing and the 'southern capital' as Nanjing. Both controlled territories that were \"directly administered\" by the emperor and his staff, Beizhili in the north and Nanzhili in the south.\nThe Hongxi Emperor wanted to restore Nanjing as the sole imperial capital and undertook preparations to do so. On February 24, 1425, he appointed Admiral Zheng He as the defender of Nanjing and ordered him to continue his command over the Ming treasure fleet for the city's defense. Zheng He governed the city with three eunuchs for internal matters and two military noblemen for external matters, awaiting the Hongxi Emperor's return along with the military establishment from the north. The emperor died on May 29, 1425, before this could have taken place.\nThe succeeding Xuande Emperor preferred to remain in Beijing, leaving it the primary and \"de facto\" capital and Nanjing as permanent secondary or reserve capital. Owing to the continuing importance of the ancestral injunctions, however, Nanjing was designated in official documents as the actual capital and Beijing as a temporary capital from 1425 to 1441. In 1441, the Yingzong Emperor ordered the \"provisional\" or \"temporary\" (, ) prefix removed from Beijing's government seals and further ordered that the southern imperial administration would henceforth be required to prefix \"Nanjing\" to their own seals to distinguish them.\nBesides the city wall, other Ming-era structures in the city included the famous Ming Xiaoling Mausoleum and Porcelain Tower, although the latter was destroyed by the Taipings in the 19th century either to prevent a hostile faction from using it to observe and shell the city or from superstitious fear of its geomantic properties. A gigantic stele, cut on the orders of the Yongle Emperor, lies abandoned in the Yangshan Quarry some east of the walled city.\nAs the center of the empire, early-Ming Nanjing had worldwide connections. It was home of the admiral Zheng He, who went to sail the Pacific and Indian Oceans, and it was visited by foreign dignitaries, such as a king from Borneo who died during his visit to China in 1408. The Tomb of the King of Boni, with a spirit way and a tortoise stele, was discovered south of the walled city in Yuhuatai District in 1958 and has been restored.\nNanjing briefly again became the capital of the Southern Ming emperor from 1644 to 1645. After the fall of Beijing to Li Zicheng's rebel forces and then to the Manchu-led Qing dynasty in the spring of 1644, the Ming prince Zhu Yousong was enthroned in Nanjing in June 1644 as the Hongguang Emperor. His short reign was described by later historians as the first reign of the so-called Southern Ming dynasty.\nBeset by factional conflicts, his regime could not offer effective resistance to Qing forces, when the Qing army, led by the Manchu prince Dodo approached Jiangnan the next spring. Days after Yangzhou fell to the Manchus in late May 1645, the Hongguang Emperor fled Nanjing, and the imperial Ming Palace was looted by local residents. On June 6, Dodo's troops approached Nanjing, and the commander of the city's garrison, Zhao the Earl of Xincheng, promptly surrendered the city to them. The Manchus soon ordered all male residents of the city to shave their heads in the Manchu queue way. They requisitioned a large section of the city for the bannermen's cantonment, and occupied the former imperial Ming Palace, but otherwise the city was spared the mass murders and destruction that befell Yangzhou.\nQing dynasty and Taiping Rebellion.\nUnder the Qing dynasty from 1645 to 1911, Nanjing returned to its previous name Jiangning although it continued to be referenced as Nanking in foreign sources. At first, it continued to administer the territory of Nanzhili under the name Jiangnan (\"Area South of the Yangtze\") but this administration was soon broken up into \"Right\" and \"Left\" governments based in Suzhou and Jiangning respectively. After a series of reorganizations, at some point under the Qianlong Emperor, Jiangnan was fully divided into the present provinces of Anhui and Jiangsu. Separately, however, these provinces were reunited under the supervision of a new Viceroy of Liangjiang after 1723, whose seat was based in Jiangning. It was the site of a Qing Army garrison. It had been visited by the Kangxi and Qianlong emperors a number of times on their tours of the southern provinces. The 1842 Treaty of Nanking, ending the First Opium War, was signed in the city harbor on Royal Navy warships.\nThe Taiping Rebellion secured the city in the mid-19th century, taking it as their capital under the name Tianjing. The rebellion destroyed most of the former Ming imperial buildings in the city, including the Porcelain Tower, considered up to that time as one of the wonders of the world. Both the Qing viceroy and the Taiping king resided in buildings that would later be known as the Presidential Palace. When Qing forces led by Zeng Guofan retook the city in 1864, a massive slaughter occurred in the city with over 100,000 estimated to have committed suicide or fought to the death. Since the Taiping Rebellion began, Qing forces allowed no rebels speaking its dialect to surrender and systematically slaughtered civilians within the city.\nThe New York Methodist Mission Society's superintendent Virgil Hart arrived in Nanjing in 1881. After some time, he succeeded in buying land near the city's Southern Gate and Confucian Temple to build the city's first Methodist church, Western hospital and boys' school. The hospital would later be unified with the Drum Tower Hospital and the boys' school would be expanded by later missionaries to become the University of Nanking and Medical School. The old mission property became the No. 13 Middle School, the oldest continually-used school grounds in the city.\nCapital of the Republic and Nanjing Massacre.\nThe Xinhai Revolution led to the founding of the Republic of China in January 1912 with Sun Yat-sen as the first provisional president and Nanjing was selected as its new capital. However, the Qing Empire controlled large regions to the north, so the revolutionaries asked Yuan Shikai to replace Sun as president in exchange for the abdication of Puyi, the last emperor. Yuan demanded the capital be moved to Beijing (closer to his power base).\nIn 1927, the Kuomintang (KMT; Nationalist Party) under Generalissimo Chiang Kai-shek again established Nanjing as the capital of the Republic of China, and this became internationally recognized once KMT forces took Beijing in 1928. The following decade is known as the Nanjing decade. During this decade, Nanjing was of symbolic and strategic importance. The Ming dynasty had made Nanjing a capital, the republic had been established there in 1912, and Sun Yat-sen's provisional government had been there. Sun's body was brought and placed in a grand mausoleum to cement Chiang's legitimacy. Chiang was born in the neighboring province of Zhejiang and the general area had strong popular support for him.\nIn 1927, the Nationalist government proposed a comprehensive proposal, the Capital Plan (), to reconstruct the war-torn city of Nanjing into a modern capital. It was a decade of extraordinary growth with an enormous amount of construction. A lot of government buildings, residential houses, and modern public infrastructures were built. During this boom, Nanjing reputedly became one of the most modern cities in China.\nIn 1937, the Empire of Japan started a full-scale invasion of China after invading Manchuria in 1931, beginning the Second Sino-Japanese War (often considered a theater of World War II). Their troops occupied Nanjing in December and carried out the systematic and brutal Nanjing Massacre (the \"Rape of Nanjing\"). The total death toll, including estimates made by the International Military Tribunal for the Far East and the Nanjing War Crimes Tribunal after the atomic bombings, was between 300,000 and 350,000. The city itself was also severely damaged during the massacre. The Nanjing Massacre Memorial Hall was built in 1985 to commemorate this event.\nA few days before the fall of the city, the National Government of China was relocated to the southwestern city Chongqing and resumed Chinese resistance. In 1940, a Japanese-collaborationist government known as the \"Nanjing Regime\" or \"Reorganized National Government of China\" led by Wang Jingwei was established in Nanjing as a rival to Chiang Kai-shek's government in Chongqing. In 1946, after the Surrender of Japan, the KMT relocated its central government back to Nanjing.\nPeople's Republic.\nIn April 1949, Communist forces crossed the Yangtze River and the Communist People's Liberation Army (PLA) captured Nanjing. The KMT government retreated to Canton (Guangzhou) until October 15, Chongqing until November 25, and then Chengdu before retreating to the island of Taiwan on December 10 where Taipei was proclaimed the temporary capital of the Republic of China. By late 1949, the PLA was pursuing remnants of KMT forces southwards in southern China, and only Tibet and Hainan Island were left.\nAfter the establishment of the People's Republic of China in October 1949, Nanjing was initially a province-level municipality, but it was soon merged into Jiangsu and again became the provincial capital by replacing Zhenjiang which was transferred in 1928, and retains that status to this day.\nGeography.\nNanjing, with a total land area of , is situated in the heartland of the drainage area of the lower reaches of the Yangtze River, and in the Yangtze River Delta, one of the largest economic zones of China. The Yangtze River flows past the west side and then the north side of Nanjing City, while the Ningzheng Ridge surrounds the north, east and south sides of the city. The city is southeast of Luoyang, south-southeast of Beijing, west-northwest of Shanghai, and east-northeast of Chongqing. The Yangtze flows downstream from Jiujiang, Jiangxi, through Anhui and Jiangsu to the East China Sea. The northern part of the lower Yangtze drainage basin is the Huai River basin and the southern part is the Zhe River basin. They are connected by the Grand Canal east of Nanjing. The area around Nanjing is called Xiajiang (, Downstream River) region, with Jianghuai dominant in the northern part and Jiangzhe dominant in the southern part. The region is also well known as Dongnan (, South East, the Southeast) and Jiangnan (, and River South, South of Yangtze).\nNanjing borders Yangzhou to the northeast; Zhenjiang to the east; and Changzhou to the southeast. On its western boundary is Anhui, where Nanjing borders five prefecture-level cities: Chuzhou to the northwest, Wuhu, Chaohu and Ma'anshan to the west and Xuancheng to the southwest.\nClimate and environment.\nNanjing has a humid subtropical climate (K\u00f6ppen \"Cfa\") and is influenced by the East Asian monsoon. The four seasons are distinct, with damp conditions seen throughout the year, very hot and muggy summers, cold, damp winters, and in between, spring and autumn are of reasonable length. Along with Chongqing and Wuhan, Nanjing is traditionally referred to as one of the \"Three Furnaces\" along the Yangtze River for the perennially high temperatures in the summertime. However, the time from mid-June to the end of July is the plum blossom blooming season in which the \"meiyu\" (rainy season of East Asia; literally \"plum rain\") occurs, during which the city experiences a period of mild rain as well as dampness. The northeast wind prevails in winter.\nDespite being called one of the \"Three Furnaces\", Nanjing has a cold climate for its latitude. Nanjing's winter temperatures are similar to, or lower than, those of London, and Nanjing's January is 2\u00a0\u00b0C colder to London's January, despite the fact that Nanjing is on 32\u00b0N and London is on 51\u00b0N. Comparing to the East coast of USA which has the same humid subtropical climate, Nanjing's winter is 6\u00a0\u00b0C colder than the winter of Savannah, Georgia. The average temperatures of Nanjing are also 3\u00a0\u00b0C lower than those of Savannah (notice that Savannah is in the same 32\u00b0N latitude as Nanjing).\nThe average temperature in January is , and the extreme daily minimum temperature is , which occurred on January 6, 1955. The southeast wind prevails in summer, with an average temperature of in July and an extreme daily maximum temperature of , which occurred on July 13, 1934. The number of precipitation days greater than 0.1\u00a0mm was 113 days, and the extreme maximum annual precipitation days were 160 days in 1957. The average annual precipitation is .\nTyphoons are uncommon but possible in the late stages of summer and early part of autumn. The annual mean temperature is around , with the monthly 24-hour average temperature ranging from in January to in July. Extremes since 1951 have ranged from on January 6, 1955, to on August 22, 1959. On average precipitation falls 113\u00a0days out of the year, and the average annual rainfall is . With monthly percent possible sunshine ranging from 37\u00a0percent in June to 48\u00a0percent in August and October, the city receives 1,932 hours of bright sunshine annually.\nNanjing is endowed with rich natural resources, which include more than 40 kinds of minerals. Among them, iron and sulfur reserves make up 40\u00a0percent of those of Jiangsu province. Its reserves of strontium rank first in East Asia and the Southeast Asia region. Nanjing also possesses abundant water resources, both from the Yangtze River and groundwater. In addition, it has several natural hot springs such as Tangshan Hot Spring in Jiangning and Tangquan Hot Spring in Pukou.\nXuanwu Lake and Mochou Lake are located in the center of the city and are easily accessible to the public, while Purple Mountain is covered with deciduous and coniferous forests preserving various historical and cultural sites. Meanwhile, a Yangtze River deep-water channel is under construction to enable Nanjing to handle the navigation of 50,000 DWT vessels from the East China Sea.\nEnvironmental issues.\nA dense wave of smog began in the central and east parts of China on December 2, 2013, across a distance of around , including Tianjin, Hebei, Shandong, Jiangsu, Anhui, Shanghai and Zhejiang. A lack of cold air flow, combined with slow-moving air masses carrying industrial emissions, collected airborne pollutants to form a thick layer of smog over the region. The heavy smog heavily polluted central and southern Jiangsu Province, especially in and around Nanjing, with its AQI pollution Index at \"severely polluted\" for five straight days and \"heavily polluted\" for nine. Officials blamed the dense pollution on lack of wind, automobile exhaust emissions under low air pressure, and coal-powered district heating system in north China. Prevailing winds blew low-hanging air masses of factory emissions (mostly SO2) towards China's east coast.\nSoil.\nThere are mainly two types of soil in Nanjing: zonal soil and cultivated soil. The zonal soil is yellow-brown soil in the northern and central areas of Nanjing, and red soil in the southern part of the border with Anhui. The cultivated soil formed by human-made farming is mainly paddy soil, and there are some yellow Gang soil and vegetable garden soil. The distribution of soil presents a certain law with the undulation of topography and hydrological conditions, which can be divided into three categories: low mountain and hilly area, hilly area and plain area. According to the second national soil survey from 1980 to 1987, the soil in Nanjing is divided into 7 soil types, 13 subtypes, 30 soil genera and 66 soil species, with a total area of 416,300 hectares.\nWater.\nNanjing is located at the lower reaches of the Yangtze River. The Yangtze River runs diagonally across the city from southwest to northeast. It is about 93 kilometers long and more than 300 kilometers away from the sea entrance. The Qinhuai River rushes from south to north, passes through the main urban area, and joins the Yangtze River. It is known as the mother river of Nanjing. Xuanwu Lake and Mochou Lake are like two pearls embedded in the main city. The water area of the city now accounts for about 11%. The river and lake water system mainly belongs to the Yangtze River system, and only the rivers that flow into Gaoyou Lake and Baoying Lake in the northern part of Liuhe District belong to the Huai River system. The Yangtze River system includes the Qinhuai River system in the south of the Yangtze River, the Chuhe River system in the north of the Yangtze River, the riverside system formed by small rivers that flow into the river on both sides of the river, the two lakes system composed of Shijiu Lake and Gucheng Lake, and the West Taihu Lake system in the east of Gaochun. The groundwater resources are abundant and the water quality is excellent, and the Pukou Pearl Spring is particularly famous. Jiangning Tangshan and Pukou Tangquan are hot spring areas with a long history.\nThe Port of Nanjing is the largest inland port in China, with annual cargo tonnage reached 191,970,000\u00a0t in 2012. The port area is in length and has 64 berths including 16 berths for ships with a tonnage of more than 10,000. Nanjing is also the biggest container port along the Yangtze River; in March 2004, the one million container-capacity base, Longtan Containers Port Area opened, further consolidating Nanjing as the leading port in the region. As of 2010[ [update]], it operated six public ports and three industrial ports. The Yangtze River's 12.5-meter-deep waterway enables 50,000-ton-class ocean ships directly arrive at the Nanjing Port, and the ocean ships with the capacities of 100,000 tons or above can also reach the port after load reduction in the Yangtze River's high-tide period. CSC Jinling has a large shipyard.\nAnimal and plant resources.\nNanjing is one of the regions with abundant plant resources and a wide variety of plants in China. The vegetation types are complex, including 7 types of natural vegetation including coniferous forest, deciduous broad-leaved forest, mixed deciduous and evergreen broad-leaved forest, bamboo forest, shrub, grass and aquatic vegetation. Cultivated vegetation includes field crops, vegetable crops, and economic forests, orchards and green belts. Plant species, there are 1061 species of vascular plants, accounting for 64.7% of the total in Jiangsu Province. Seven species such as Sphaerocarpus sinensis, Chinese Allium chinense, Ming Codonopsis, and Pterocarpus sinensis are national key protected rare and endangered plants. The city's forest coverage rate is 27.1%. Among wild animals, there are 795 species of insects belonging to 125 families of 11 orders. There are 99 species of fish belonging to 22 families and 12 orders. There are 327 species of terrestrial wild vertebrates, belonging to 29 orders and 90 families. 243 species of birds belong to 56 families of 17 orders. 47 species of mammals belong to 8 orders and 22 families. Among all animal species, 9 species of wild animals under national first-level protection, such as the Oriental White Crane and White Shoulder Eagle, 65 species of wild animals under the second-level protection, such as the little swan, Chinese tiger and swallowtail, and finless porpoise, and 125 key protected animals in Jiangsu Province Species, 35 species of endangered animals.\nYangtze River crossings.\nIn the 1960s, the first Nanjing Yangtze River Bridge was completed, and served as the only bridge crossing over the Lower Yangtze in eastern China at that time. The bridge was a source of pride and an important symbol of modern China, having been built and designed by the Chinese themselves following failed surveys by other nations and the reliance on and then rejection of Soviet expertise. Begun in 1960 and opened to traffic in 1968, the bridge is a two-tiered road and rail design spanning on the upper deck, with approximately spanning the river itself. Since then four more bridges and four tunnels have been built. Going in the downstream direction, the Yangtze crossings in Nanjing are: Dashengguan Bridge, Third Bridge, Fifth Nanjing Yangtze River Bridge, Nanjing Yangtze River Tunnel (), Line 10 Metro Tunnel, Nanjing Yangtze Tunnel (), First Bridge, Yanziji Yangtze River Tunnel, Nanjing Baguazhou Yangtze River Bridge and Nanjing Qixiashan Yangtze River Bridge.\nMineral resources.\nNanjing is rich in mineral resources. The discovered minerals mainly include 41 types of iron, copper, lead, zinc, strontium, ferrosulfide, dolomite, limestone, gypsum, and clay, among which 23 are of proven reserves and 20 are of industrial mining value. There are more than 10 kinds being mined. The quality and reserves of strontium ore (celestite) rank first in the country. The reserves of copper and lead-zinc ore account for more than 90% of the province, iron ore accounts for 89% of the province, and limestone, dolomite, and attapulgite clay mines are in the whole province. Province occupies an important position. Nanjing's minerals are mainly concentrated in 4 metallogenic belts, namely Jiangpu-Liuhe iron and copper metallogenic belt, Ningzhen iron, copper, and sulfur polymetallic metallogenic belt, Ningwu iron, copper.\nGovernment.\nAt present, the full name of the government of Nanjing is \"Nanjing Municipal People's Government\" and the municipality is under the one-party rule of the Chinese Communist Party, with the Party Secretary of Nanjing as the \"de facto\" governor of the municipality and the mayor as the executive head of the government working under the secretary.\nAdministrative divisions.\nThe sub-provincial city of Nanjing is divided into 11 districts.\nDemographics.\nAt the time of the 2010 census, the total population of the City of Nanjing was 8.005\u00a0million. The OECD estimated the encompassing metropolitan area at the time as 11.7\u00a0million. Official statistics in 2011 estimated the city's population to be 8.11\u00a0million. The birth rate was 8.86\u00a0percent and the death rate was 6.88\u00a0percent. The urban area had a population of 6.47\u00a0million people. The sex ratio of the city population was 107.31 males to 100 females.\nAs in most of eastern China, the official ethnic makeup of Nanjing is predominantly Han nationality (98.56\u00a0percent), with 50 other official ethnic groups. In 1999, 77,394 residents belonged to officially defined minorities, among which the vast majority (64,832) were Hui, contributing 83.76\u00a0percent to the minority population. The second and third largest minority groups were Manchu (2,311) and Zhuang (533). Most of the minority nationalities resided in Jianye District, comprising 9.13\u00a0percent of the district's population.\nLanguages.\nNanjing Mandarin is spoken in most parts of Nanjing, while Wu Chinese is spoken in most of the Gaochun District and the southern part of Lishui District. Nanjing dialect has been the official language of China for a long time in history. Jinling Yayan was established as the standard pronunciation of Chinese as an orthodox traditional Chinese dialect in the ancient Central Plains.\nIn July 2017, the Ministry of Education and the National Language Commission held a press conference, and the penetration rate of Mandarin has reached 73%.\nReligion.\nNanjing has four major religions: Buddhism, Taoism, Christianity, and Islam. Nanjing is one of the earliest areas in China to spread Buddhist culture. The \"480 Temples in the Southern Dynasties\" has become the center of Chinese Buddhist culture and the ancestral home of the Sanlunzong, Niutouzong, Fayanzong, and other Buddhist sects. Nanjing is also the place for the revival of modern Chinese Buddhist culture. The Jinling Carved Scriptures integrates Buddhist publishing, dissemination, and research. It is still the world's unparalleled Chinese Buddhist scripture publishing and circulation center. The engraving and printing skills are included in the world's intangible cultural heritage of humanity. Ancient famous temples such as Jianchu Temple, Qixia Temple, Waguan Temple, Qingliang Temple, Jiming Temple, Dabaoen Temple, etc. were revived. Nanjing Taoism has a long history and occupies an important position in the history of Chinese Taoism.\nThe spread of Catholicism in Nanjing began more than 400 years ago and was started by the scientist and missionary Matteo Ricci. The Shigu Road Catholic Church is the cathedral of the Catholic Diocese of Nanjing. The Nanjing Diocese with Nanjing as its center covers a vast area. As one of the national centers of Christianity in China, Nanjing has two seminaries, Jinling Theological Seminary and Jiangsu Theological Seminary. The Christian social service organization, Amity Foundation and the world's largest Bible printing company, Amity Printing Company are both in Nanjing.\nNanjing is the birthplace of the Islamic \"Renaissance\" and has an important influence on the development of Chinese Islamic culture.\nEconomy.\nThe current economy of the city, is dominated by the service industries, accounting for about 60 percent of the GDP of the city, and financial industry, culture industry and tourism industry are the top three. Industries of information technology, energy saving and environmental protection, new energy, smart power grid and intelligent equipment manufacturing have become the pillar of the industries. Big civilian-run enterprise include Suning Commerce, Yurun, Sanpower, Fuzhong, Hiteker, 5stars, Jinpu, Tiandi, CTTQ Pharmaceutical, Nanjing Iron and Steel Company and Simcere Pharmaceutical. Big state-owned firms include Panda Electronics, Yangzi Petrochemical, Jinling Petrochemical, Nanjing Chemical, Jincheng Motors, Jinling Pharmaceutical, Chenguang and NARI. The city has also attracted foreign investments. Multinational firms such as Siemens, Ericsson, Volkswagen, Iveco, A.O. Smith, and Sharp have established their offices, and a number of multinationals such as Ford, IBM, Lucent, Samsung and SAP have established research center here. Many China-based leading firms such as Huawei, ZTE and Lenovo have key R&amp;D institutes in the city. Nanjing is an industrial technology research and development hub, hosting many R&amp;D centers and institutions, especially in areas of electronics technology, information technology, computer software, biotechnology and pharmaceutical technology and new material technology.\nIn recent years, Nanjing has been developing its economy, commerce, industry, as well as city construction. \nIn 2013 the city's GDP was RMB 801\u00a0billion (3rd in Jiangsu), and GDP per capita (current price) was RMB 98,174(US$16041), an 11\u00a0percent increase from 2012. The average urban resident's disposable income was RMB 36,200, while the average rural resident's net income was RMB 14,513. The registered urban unemployment rate was 3.02\u00a0percent, lower than the national average (4.3\u00a0percent). Nanjing's Gross Domestic Product ranked 12th in 2013 in China, and its overall competence ranked 6th in mainland and 8th including Taiwan and Hong Kong in 2009.\nIn 2004, Nanjing ranked sixth in China's Economic Center Positioning Index, after Beijing, Shanghai, Guangzhou, Shenzhen, and Tianjin. In 2008, the Headquarters Economy Development Capacity ranked the city fifth in China, behind Beijing, Shanghai, Guangzhou, and Shenzhen. In 2014 China's regional central cities (excluding Beijing and Shanghai) competitiveness evaluation, Nanjing was second only to Shenzhen and Guangzhou. In 2015, Nanjing ranked fifth in China's investment attractive cities, closely following Beijing, Shanghai, Guangzhou, and Shenzhen. In August 2020, Nanjing ranked among China's top ten GDP in the first half of the year.\nIn 2019, Nanjing's GDP was 1403,015 billion yuan, ranking 11th in the country, an increase of 7.8% over the previous year. The per capita GDP is 152,886 yuan, ranking second in China's municipalities, sub-provincial cities and provincial capitals, second only to Shenzhen, and the provincial capital ranking first. In 2021, Nanjing's GDP reached 1,6355.32 billion yuan.\nPrimary industry.\nNanjing is one of China's important agricultural and commercial grain bases. The main cash crops are rice, cotton, silkworm cocoons, hemp, tea, bamboo, fruits, medicinal materials, etc. Due to the fertile water quality on both sides of the Yangtze River, it is also one of China's important freshwater fishery bases.\nIn 2019, the total output value of Nanjing's agriculture, forestry, animal husbandry, and fishery was 47.250 billion yuan, an increase of 4.8% over the previous year. Among them, the agricultural output value was 24.077 billion yuan, the forestry output value was 2.017 billion yuan, the animal husbandry output value was 2.435 billion yuan, the fishery output value was 15.389 billion yuan, and the agricultural, forestry, animal husbandry and fishery service industry output value was 3.333 billion yuan.\nSecondary industry.\nIn 2019, Nanjing's total industrial added value was 421.577 billion yuan, an increase of 6.9%. The added value of industrial enterprises above the designated size was 309.226 billion yuan, an increase of 7.0%. Among the industries above designated size, the added value of state-owned and state-holding enterprises fell by 0.2%, private enterprises increased by 20.3%, and foreign companies, Hong Kong, Macao, and Taiwan enterprises increased by 7.0%. Large and medium-sized enterprises increased by 3.9%, and small and micro enterprises increased by 18.2%. Among the 37 major industries in the system, 22 industries have achieved growth in added value. Among the top ten industries ranked by cumulative value-added, six industries including electronics, electrical machinery, steel, medicine, general equipment, and non-metal products increased by 20.2%,\nTertiary industry.\nNanjing is an important regional financial and business center positioned by the National Development and Reform Commission. The financial industry is an important strategic pillar industry in Nanjing. The total financial volume and financial resources account for 25% of Jiangsu Province, and in the Financial Center index, the city ranks sixth in the country. In the 2018 China Financial Center Index evaluation, Nanjing's financial industry performance ranked fourth in China, after Beijing, Shanghai, and Shenzhen. In 2018, Nanjing's financial industry achieved an added value of 147.332 billion yuan, and the balance of domestic and foreign currency deposits in financial institutions was 3452.486 billion yuan.\nNanjing is China's service outsourcing base and national software export innovation base. It is China's only pilot city for comprehensive reform of the national science and technology system. The software industry is the number one leading industry and pillar industry that Nanjing strives to cultivate. At the end of 2019, Nanjing achieved a total execution value of 17.33 billion US dollars in service outsourcing, ranking first among Chinese cities. In 2018, the software and information service industry had a revenue of 450 billion yuan, ranking fourth in China and first in Jiangsu after Beijing, Shenzhen, and Shanghai, accounting for 7.1% of the country's total and 50.8% of Jiangsu's. There are 12 unicorn companies in Nanjing in 2019, ranking seventh in global cities and fifth in China.\nThe convention and exhibition industry is an important industry in Nanjing. In the \"World 2013 City Conference Industry Development Ranking\" issued by the International Conference and Convention Association (ICCA), Nanjing has become the city with the most international conferences in China after Beijing and Shanghai. In 2019, Beichen Convention and Exhibition Research Institute released the \"China Exhibition Index Report 2019\", and Nanjing ranked seventh in China in the comprehensive index of domestic urban exhibition industry development. According to the \"2017 China Exhibition Statistics Report\" released in 2018, Nanjing ranked third in the number of exhibitions held in all cities in China, and ranked fifth in the exhibition area in all cities in China.\nTransport.\nNanjing is the transport hub in eastern China and the downstream Yangtze River area. Different means of transport constitute a three-dimensional transport system that includes land, water and air. As in most other Chinese cities, public transport is the dominant mode of travel for the majority of citizens. As of October 2014, Nanjing had four bridges and two tunnels over the Yangtze River, linking districts north of the river with the city center on the south bank.\nRail.\nNanjing is an important railway hub in eastern China. It serves as rail junction for the Beijing-Shanghai (Jinghu) (which is itself composed of the old Jinpu and Huning Railways), Nanjing\u2013Tongling Railway (Ningtong), Nanjing\u2013Qidong (Ningqi), and the Nanjing-Xi'an (Ningxi) which encompasses the Hefei\u2013Nanjing Railway.\nNanjing is connected to the national high-speed railway network by Beijing\u2013Shanghai High-Speed Railway and Shanghai\u2013Wuhan\u2013Chengdu Passenger Dedicated Line, with several more high-speed rail lines under construction. The main stations in Nanjing are Nanjing Station, Nanjing South Station, Jiangning Station, Lishui Station, Xianlin Station, Jiangning West Station, Nanjing East Station, Nanjing Passenger and Technical Station, as well as the new Nanjing North Station and Lukou Air-Rail Intermodal Transport Hub Station planning in. Among them, Nanjing Railway Station is the national railway hub station and China's top ten railway hubs, Nanjing South Railway Station is the national railway hub station and Asia's largest high-speed railway station, and Nanjing East Railway Station is the largest marshalling station in East China and the country's 15th largest railway network marshalling station. Nanjing Passenger Technology Station is a train technology station\nAmong all 17 railway stations in Nanjing, passenger rail service is mainly provided by Nanjing Railway Station and Nanjing South Railway Station, while other stations like Nanjing West Railway Station, Zhonghuamen Railway Station and Xianlin Railway Station serve minor roles. Nanjing Railway Station was first built in 1968. On November 12, 1999, the station was burnt in a serious fire. Reconstruction of the station was finished on September 1, 2005. Nanjing South Railway Station, which is one of the five hub stations on Beijing\u2013Shanghai High-Speed Railway, has officially been claimed as the largest railway station in Asia and the second largest in the world in terms of GFA (Gross Floor Area). Construction of Nanjing South Station began on January 10, 2008. The station was opened for public service in 2011.\nAviation.\nNanjing's airport, Lukou International Airport (NKG), serves both national and international destinations. In 2013, Nanjing airport handled 15,011,792 passengers and 255,788.6 tonnes of freight. The airport currently has 85 routes to national and international destinations, which include Japan, Korea, Thailand, Malaysia, Singapore, United States and Germany. The airport is connected by a highway directly to the city center, and is also linked to various intercity highways, making it accessible to the passengers from the surrounding cities. A railway Ninggao Intercity Line has been built to link the airport with Nanjing South Railway Station. Lukou Airport was opened on June 28, 1997, replacing Nanjing Dajiaochang Airport as the main airport serving Nanjing. Dajiaochang Airport is still used as a military air base. Nanjing has another airport \u2013 Nanjing Ma'an International Airport which temporarily serves as a dual-use military and civil airport.\nShipping.\nContemporary Nanjing Port is an important hub port in China and a first-class port open to the outside world. It is a multifunctional river-sea port in East China and the Yangtze River Basin for reloading, land and water transfer, cargo distribution and opening to the outside world. It is the only container railway and waterway in the Yangtze River Delta. A seamless port. The completion of the 12.5-meter deep-water channel project on the Yangtze River in Nanjing has made Nanjing Port the deepest inland international deep-water seaport, and it is also a comprehensive hub for China's global river-to-sea transshipment.\nRoad.\nAs an important regional hub in the Yangtze River Delta, Nanjing is well-connected by over 60 state and provincial highways to all parts of China.\nHighways such as Hu\u2013Ning, Ning\u2013He, Ning\u2013Hang enable commuters to travel to Shanghai, Hefei, Hangzhou, and other important cities quickly and conveniently. Inside the city of Nanjing, there are of highways, with a highway coverage density of 3.38\u00a0kilometers per hundred square\u00a0kilometers (5.44\u00a0mi/100\u00a0sq\u00a0mi). The total road coverage density of the city is 112.56\u00a0kilometers per hundred square\u00a0kilometers (181.15\u00a0mi/100\u00a0sq\u00a0mi). The two artery roads in Nanjing are Zhongshan Road and Hanzhong Road are also the two main roads which cross each other in the city center, Xinjiekou.\nExpressways {G+XXxx (National Express, ), S+XX ()}:\nNational Highway\nNanjing is a national comprehensive transportation hub, and its highway network density ranks among the top central cities in the country. As of 2019, the total mileage of Nanjing highways opened to traffic has reached 630 kilometers, and the highway network density has reached 9.56 kilometers per 100 square kilometers, ranking first in the country. With Nanjing as the center, Ninghu, Ninggao, Ningzhen, Ningyang, Ningchu, Ninglian, Ningtong, Ningchao, Ninghe, Ningluo, Ningma, Ningxuan, Ningyan, Ninghuai, Ningmu, Ningchang, Ninghang and other high-grade highways lead to Jiang surrounding provinces and cities in a radial pattern.\nMain long-distance bus terminals: Nanjing Bus Station, Nanjing South Bus Station, Nanjing North Bus Station, Nanjing East Bus Station, Jiangning Bus Station, Lishui Bus Station, Gaochun Bus Station, Nanjing Getang Bus Station.\nPublic transport.\nThe city has an efficient public transport network, which mainly consists of bus, taxi and metro systems. The bus network, which is currently run by three companies since 2011, provides more than 370 routes covering all parts of the city and suburban areas. At present, the Nanjing Metro system has a grand total of of route and 208 stations across 12 lines. They are Line 1, Line 2, Line 3, Line 4, Line 7, Line 10, Line S1, Line S3, Line S6, Line S7, Line S8 and Line S9. The city is planning to complete a 17-line Metro and light-rail system by 2030. The expansion of the Metro network will greatly facilitate intracity transport and reduce the currently heavy traffic congestion.\nNanjing's first subway officially opened on September 3, 2005. It is the sixth city in mainland China to open a subway. As of 2019, Nanjing subway has 12 lines and 208 stations, with a total length of 449 kilometers and an average daily passenger flow. With more than 3.4 million passengers, the length of subway lines ranks seventh in China and eighth in the world.\nAs of the end of 2018, Nanjing had 6,909 buses, operating 468 bus lines, with a total length of , an average daily mileage of , and an average daily passenger volume of 2,182 million. At present, Nanjing has eliminated buses below the National III standard and non-air-conditioned buses, and the number of pure electric buses ranks second in the world.\nAs of the end of 2019, there were more than 12,000 real-name certified taxis in Nanjing. The appearance of the taxis was mostly uniform yellow and black, and the royal blue luxury taxis were a minority.\nAs of July 2019, there are six online ride-hailing platforms in Nanjing, namely Meituan Taxi, Didi Chuxing, First Taxi-hailing, Cao Cao Special Car, Shenzhou Special Car, T3 Travel, and the current car qualification rate of each platform is 70% the above. At present, there are about 13,000 online car-hailing vehicles legally applying for \"car permits\" in Nanjing.\nAs of 2019, there are two lines of Nanjing trams. Nanjing Hexi Tram was officially put into operation on August 1, 2014. It is the world's first inter-area contactless tram, and China's first tram to be charged at a station. The line is about 7.76 kilometers long and has 13 stations., Including 4 subway transfer stations. The Nanjing Kylin Tram was officially put into operation on October 31, 2017. The line is about 8.95 kilometers long and has 15 stations, including 1 subway transfer station.\nCulture and art.\nBeing one of the four ancient capitals of China, Nanjing has always been a cultural center attracting intellectuals from all over the country. In the Tang and Song dynasties, Nanjing was a place where poets gathered and composed poems reminiscent of its luxurious past; during the Ming and Qing dynasties, the city was the official imperial examination center (Jiangnan Examination Hall) for the Jiangnan region, again acting as a hub where different thoughts and opinions converged and thrived.\nToday, with a long cultural tradition and strong support from local educational institutions, Nanjing is commonly viewed as a \"city of culture\" and one of the more pleasant cities to live in China.\nArt.\nSome of the leading art groups of China are based in Nanjing; they include the Qianxian Dance Company, Nanjing Dance Company, Nanjing Little Red Flower Art Troupe, Jiangsu Peking Opera Institute and Nanjing Xiaohonghua Art Company among others.\nJiangsu Art Gallery is the largest gallery in Jiangsu Province, presenting some of the best traditional and contemporary art pieces of China like the historical Master Ho-Kan; many other smaller-scale galleries, such as Red Chamber Art Garden and Jinling Stone Gallery, also have their own special exhibitions. As of 2019, Nanjing has 14 cultural centers, 100 cultural stations, 15 public libraries (excluding libraries for education systems and enterprises and institutions), 132 movie theaters, and 2 large-scale convention and exhibition centers. They are Nanjing International Exhibition Center and Nanjing International Expo Center, 87 various museums, including 77 state-owned museums and 10 non-state-owned museums. As of the end of August 2020, there are 137 calligraphy and painting academies, art museums, and art galleries in Nanjing.\nNanjing is an important town of Chinese painting and calligraphy. In the Six Dynasties, there were painting and calligraphy masters such as Wang Xizhi, Wang Xianzhi, Zhang Sengyou, Lu Tanwei, and Gu Kaizhi. The earliest extant painting theory work \"Paintings\" has a profound impact on later generations. The Nantang Art Academy brought together outstanding calligraphy and painting masters at a time. Dongyuan and Juran pioneered the Southern School of Landscape and became a generation of masters. Xu Xi's flower and bird paintings, Zhou Wenju, and Gu Hongzhong's figure paintings continue to pass. \"Han Xizai's Night Banquet\" is a masterpiece of ancient Chinese meticulous brushwork. The system of Nantang Painting Academy was also inherited by later generations. The Painting Book of Ten Bamboo Studios in the Ming dynasty reproduced the paintings with the pinnacle of three-dimensional color printing techniques. The Painting Book of Mustard Seed Garden in the early Qing dynasty was regarded as a must-read for learning Chinese painting. The \"Eight Masters of Nanjing\" headed by Gong Xian were active in Nanjing in the early Qing dynasty and created the Jinling School of Painting. In the 1930s, celebrities in painting circles such as Lv Fengzi, Xu Beihong, Zhang Daqian, Yan Wenliang, Lu Sibai, Chen Zhifo, Gao Jianfu, Pan Yuliang, and Pang Xunqin gathered in Nanjing. Among them, Xu Beihong, Zhang Shuqi, and Liu Zigu were hailed as the \"Three Masters of Jinling\". Contemporary \"New Jinling Painting School\" represented by Fu Baoshi, Qian Songyan, Song Wenzhi, Wei Zixi, Yaming,\nFestivals.\nMany traditional festivals and customs were observed in the old times, which included climbing the City Wall on January 16, bathing in Qing Xi on March 3, hill hiking on September 9 and others (the dates are in Chinese lunar calendar). Almost none of them, however, are still celebrated by modern Nanjingese.\nInstead, Nanjing, as a tourist destination, hosts a series of government-organized events throughout the year. The annual International Plum Blossom Festival held in Plum Blossom Hill, the largest plum collection in China, attracts thousands of tourists both domestically and internationally. Other events include Nanjing Baima Peach Blossom and Kite Festival, Jiangxin Zhou Fruit Festival and Linggu Temple Sweet Osmanthus Festival.\nLibraries.\nNanjing Library, founded in 1907, houses more than 10 million volumes of printed materials and is the third largest library in China, after the National Library in Beijing and Shanghai Library. Other libraries, such as city-owned Jinling Library and various district libraries, also provide considerable amount of information to citizens. Nanjing University Library is the second largest university libraries in China after Peking University Library, and the fifth largest nationwide, especially in the number of precious collections.\nMuseums.\nNanjing has some of the oldest and finest museums in China. Nanjing Museum, formerly known as National Central Museum during ROC period, is the first modern museum and remains as one of the leading museums in China having 400,000 items in its permanent collection. The museum is notable for enormous collections of Ming and Qing imperial porcelain, which is among the largest in the world.\nOther museums include the City Museum of Nanjing in the Chaotian Palace, the Oriental Metropolitan Museum, the China Modern History Museum in the Presidential Palace, the Nanjing Massacre Memorial Hall, the Taiping Kingdom History Museum, Jiangning Imperial Silk Manufacturing Museum, Nanjing Yunjin Museum, Nanjing City Wall Cultural Museum, Nanjing Customs Museum in Ganxi House, Nanjing Astronomical History Museum, Nanjing Paleontological Museum, Nanjing Geological Museum, Nanjing Riverstones Museum, and other museums and memorials such Zheng He Memorial Jinling Four Modern Calligraphers Memorial.\nTheater.\nJiangsu Province Kun Opera is one of the best theaters for Kunqu, China's oldest stage art.\nIt is considered a conservative and traditional troupe. Nanjing also has professional opera troupes for the Yang, Yue (shaoxing), Xi and Jing (Chinese opera varieties) as well as Suzhou pingtan, spoken theater and puppet theater.\nMost of Nanjing's major theaters are multi-purpose, used as convention halls, cinemas, musical halls and theaters on different occasions. The major theaters include the People's Convention Hall and the Nanjing Arts and Culture Center. The Capital Theater well known in the past is now a museum in theater/film.\nXiqu is a traditional Chinese drama. After a long period of development and evolution, it has gradually formed the Chinese Opera Garden with the five major Chinese opera types of \"Peking Opera, Yue Opera, Huangmei Opera, Ping Opera, and Henan Opera\" as the core. Peking opera has a long history in Nanjing: the famous Peking opera master Mei Baojiu has a deep connection with Nanjing. As the honorary president of the \"Nanjing Meilanfang Jingkun Art Research Association\", Master Mei Jiubao made a special trip to Nanjing as the \"Research Association\" \"Unveiled, and led his disciples to perform the Meipai famous play\" The Return of the Phoenix \" Zheng Ziru, the famous Peking opera artist, performed \"The Flower Spear\" in Nanjing.\nKunqu Opera is one of the oldest operas in traditional Chinese opera, and it is also a treasure of traditional Chinese culture and art, especially opera art. It is called an \"orchid\" in the Hundred Gardens. In Nanjing, famous professional Kunban classes such as \"Xinghua Ministry\", \"Hualin Ministry\", \"Li Yujia Ban\", and \"Cao Yinjia Ban\" appeared in Nanjing, and the style of singing songs by the voiceless section and literati also continued.\nDrama is a form of Western drama introduced in the 20th century. In recent years, Nanjing's annual drama box office has continued to rise. The drama \"Mrs of the Sea\" staged in Nanjing in 2017, \"Broken Gold\", \"Treasure Island Village\" in 2018, and \"Hamlet\" in 2019 have the highest box office in the country. All fell in Nanjing. Not only that, the box office and attendance rate of some plays such as \"White Deer Plain\" in Nanjing are also far ahead in the Yangtze River Delta region.\nQuyi is the collective name of the various \"rap art\" of the Chinese nation. It is a unique art form formed by the long-term development and evolution of folk oral literature and singing art. The local folk arts in Nanjing include Southern Crosstalk, Nanjing Baiju, Nanjing Vernacular, Nanjing Pinghua, Gaochun Yangqiang Mulian Opera, Liuhe Hongshan Opera, etc.\nNight life.\nTraditionally Nanjing's nightlife was mostly centered around Nanjing Fuzimiao (Confucius Temple) area along the Qinhuai River, where night markets, restaurants and pubs thrived. Boating at night in the river was a main attraction of the city. Thus, one can see the statues of the famous teachers and educators of the past not too far from those of the courtesans who educated the young men in the other arts.\nIn the past 20\u00a0years, several commercial streets have been developed, hence the nightlife has become more diverse: there are shopping malls opening late in the Xinjiekou CBD, as well as in and around major residential areas throughout the city. The well-established \"Nanjing 1912\" district hosts a wide variety of recreational facilities ranging from traditional restaurants and western pubs to dance clubs, in both its downtown location and beside Baijia Lake in Jiangning District. In recent years, many night-life options have opened up in Catherine Park as well as in shopping malls such as IST in Xinjiekou and Kingmo near Baijai Lake metro station. Other, more student-oriented places are to be found near to Nanjing University and Nanjing Normal University.\nFood.\nThe local cuisine in Nanjing is called Jinling cuisine (). It is one important part of Jiangsu cuisine (\u6c5f\u82cf\u83dc). Jinling cuisine is famous for its meticulous process, emphasizing no added preservatives and its seasonality. Its duck and goose dishes are well known among Chinese for centuries. It also employs many different style of cooking methods, such as slow cooking, Chinese oven cooking, etc. Its dishes tend to be light and fresh, suitable for all.\nMany of the city's local favorite dishes are based on ducks, including Nanjing salted duck, duck blood and vermicelli soup, and duck oil pancake. The flavor snacks of Jinling Tea House have become an integral part of Qinhuai culture. In addition, Jiangning, Liuhe and Gaochun each have their own local flavors. \"Suiyuan Food List\", \"Baimen Recipe\", \"Yecheng Vegetable Book\" are the crystallization of Nanjing food culture.\nThe radish is considered typically representative of the people of Nanjing, an association commonly known throughout China. Nanjing people like to eat wild vegetables during the Qingming Festival, and they named the eight most eaten spring vegetables and wild vegetables as the \"Eight Dry Seasons\". The phrase \"eight fresh sweet-scented osmanthus fragrance\" refers to eight kinds of aquatic fruits and vegetables associated with the Mid-Autumn Festival.\nSports.\nNanjing is the birthplace of modern Chinese sports. In 1910, the first National Games in Chinese history was held. In 1924, the predecessor of the Chinese Olympic Committee (All-China Sports Association) was established in Nanjing. China's first Olympic delegation trained, assembled, and set off in Nanjing. Nanjing is the birthplace of China's Olympic dream and one of the cities that contributed the most to China's participation in the Olympics. Nanjing has an irreplaceable position in the history of the Chinese Olympics.\nNanjing's planned 20,000 seat Youth Olympic Sports Park Gymnasium will be one of the venues for the 2019 FIBA Basketball World Cup.\nAs a major Chinese city, Nanjing is home to many professional sports teams. 2020 Chinese Super League champions Jiangsu Football Club, owned by Suning Appliance Group, was a tenant of Nanjing Olympic Sports Center from 2007 until the club's dissolution in 2021. Jiangsu Nangang Basketball Club is a competitive team which has long been one of the major clubs fighting for the title in China top-level league, CBA. Jiangsu Volleyball men and women teams are also traditionally considered as at top level in China volleyball league.\nThere are two major sports centers in Nanjing, Wutaishan Sports Center and Nanjing Olympic Sports Center. Both of these two are comprehensive sports centers, including stadium, gymnasium, natatorium, tennis court, etc. Wutaishan Sports Center was established in 1952 and it was one of the oldest and most advanced stadiums in early time of People's Republic of China.\nNanjing hosted the 10th National Games of PRC in 2005 and hosted the 2nd summer Youth Olympic Games in 2014.\nIn 2005, to host The 10th National Game of People's Republic of China, there was a new stadium, Nanjing Olympic Sports Center, constructed in Nanjing. Compared to Wutaishan Sports Center, which the major stadium's capacity is 18,500, Nanjing Olympic Sports Center has a more advanced stadium which is big enough to seat 60,000 spectators. Its gymnasium has capacity of 13,000, and natatorium of capacity 3,000.\nOn February 10, 2010, the 122nd IOC session at Vancouver announced Nanjing as the host city for the 2nd Summer Youth Olympic Games. The Nanjing 2014 Youth Olympic Games featured all 28 sports on the Olympic program and were held from August 16 to 28. It is the first time that China has hosted the Youth Olympic Games and the second time that China has hosted an Olympic event.\nMain venues: Nanjing Olympic Sports Center, Wutaishan Sports Center, Youth Olympic Sports Park, Nanjing Institute of Physical Education (Central Stadium), Nanjing Longjiang Stadium, Nanjing National Fitness Center, Jiangning Sports Center, Lishui Sports Center, Gaochun Sports Center, etc.\nMain teams: Jiangsu Football Club (dissolved), Nanjing Monkey Kings, Jiangsu Dragons (a.k.a. Jiangsu Nangang), etc.\nArchitecture.\nThe city is renowned for its wide variety of architectures which mainly contain buildings from multiple dynasties, the Republic of China, and the present.\nFolklore.\nThe main folklore activities in Nanjing include Chinese New Year greetings for the Spring Festival, hanging Spring Festival couplets at the city gate, eating rice cakes, welcoming the God of Wealth on the fifth day of the first lunar month, climbing the city on the 16th day of the first lunar month, sweeping the tomb on Qingming Festival, dragon boat races on the Dragon Boat Festival, eating rice dumplings, and begging for gifts on Qixi Festival, Liqiu gnawing autumn, Mid-Autumn reunion, eating moon cakes, enjoy the moon and go to the melon rack in the field and pick melon beans under the bean shed, Chongyang ascends, Chongyang cake inserted Chongyang flag, Laba food porridge, sent stove on the 24th lunar month, New Year's Eve reunion and ancestor worship.\nLiterature.\nThe first \"Literature Museum\" in Chinese history, the first literary theory and criticism monograph \"Wen Xin Diao Long\", the earliest existing collection of poetry and essays \"Selected Works of Zhaoming\", China's first poetic theory and criticism monograph \"Shi Pin\" \", the first collection of zhiren novel,\" Shi Shuo Xin Yu, \"and the first children's enlightenment book \"Thousand Characters \"were all born in Nanjing. Masterpieces such as \"A Dream of Red Mansions\" and \"The Scholars\" are inseparable from Nanjing.\nModern literary giants such as Lu Xun, Ba Jin, Zhu Ziqing, Yu Pingbo, Zhang Henshui, Zhang Ailing have inextricably linked with Nanjing, and the masterpiece \"The Earth\" by the American writer Pearl Buck who won the Nobel Prize for Literature was created in Nanjing. Famous contemporary literary writers in Nanjing include Su Tong, Bi Feiyu and Ye Zhaoyan.\nFilm and television.\nIn 1950, 1,800 projectionists from around the country traveled to Nanjing for a training program.71 These projectionists replicated the training program in their own home provinces to develop more projectionists.71 Nanjing was later termed a \"Cradle of People's Cinema.\"71\nNanjing, as the ancient capital of the Six Dynasties and a famous scenic spot, has become the \"best location\" favored by directors. Among them, the 93 edition of \"Legend of the New White Lady\" was shot at Jiming Temple in Nanjing; \"Deep Love and Rain\" shot at Nanjing Pukou Railway Station; \"The Founding of the People's Republic\" shot at Sun Yat-sen Mausoleum, Meiling Palace, Southeast University Auditorium, etc. .; and more movies and TV series \"Jinling Thirteen Hairpins\", \"To Our Dying Youth\", \"Tuina\", etc. were all shot in Nanjing.\nMusic and dance.\nJinling Qin School is an important genre of Chinese Guqin art that originated in Nanjing. It has a great influence on many later generations of Qin Schools. It originated from the Royal Music Officials of the Ming dynasty and has been listed as a World Intangible Cultural Heritage Project. The folk song \"Jasmine Flower\" originated from the \"Flower Tune\" sung by Liuhe folks for a century, and is world-famous. Xishanqiao folk song performances have repeatedly appeared on CCTV. In addition, there are Gaochun folk songs \"Caihongling\", \"Planting Seedlings in May\", Liuhe folk songs \"Flower Tune\", \"Liuzuo Blow Music\" etc.\nIn 2016, the Nanjing Forest Music Carnival, sponsored by the Propaganda Department of the Jiangsu Provincial Party Committee and the Nanjing Municipal People's Government, has been held 5 times. Since 2014, Jiangsu Music Broadcasting will hold the Midou Music Festival in Nanjing every year. The 7th Midou Music Festival; and the popular Nanjing University Student Music Festival in recent years.\nTraditional folk dances in Nanjing include Luoshan Dragon, Dongba Dama Lantern, Sparrow Jump, Jiangpu Hand Lion, Gaochun Dance Wuban, Wanbei Xiaoma Lantern Dance, Qixia Dragon Dance, Changlu Carrying Dragon, Tongshan Gaotai Lion Dance, Dongba Peiqiao stilts, Longyin Che, Zhetang Shahuo, Dangdang, Luohan, Zhuzhen stilts are all intangible cultural heritages.\nCreated by the Nanjing Dancers Association, the original local drama \"The Place Closest to Dream\", with students from the Department of Music of the School of Aeronautics and Astronautics as the performance team, shows youthful demeanor with the theme of youth entrepreneurship; performed by Nanjing folk performing artists \"Drum and Dragon Celebrating the New Year\" is a classic of Nanjing folk dance in recent years; the \"Nanjing City Intangible Cultural Heritage Scene Demonstration\" Jinling Season \"hosted by Nanjing Cultural Bureau and undertaken by Nanjing Art Museum is a work of high artistic level.\nIn Nanjing, we have the first professional children's art school in the country that integrates cultural education, art education and stage performances, Nanjing Art Primary School, referred to as Nanjing Xiaohonghua Art Troupe. The school implements small-class education in an all-round way, and promotes both culture and art. It has been rated as a meritorious unit in Nanjing many times, and twice was awarded the honorary title of \"National Children's Cultural Work Advanced Group\" by the Central Ministry of Culture.\nPhotography.\nNanjing has many excellent photography works, as well as large-scale photography exhibitions, photography conferences, etc. Zhao Ran's \"Quadette of Enchanting Hair\", Ben Daochun's \"Tianjiang Cruise\", Tian Ming's \"Shanghai White-collar Early Class Subway Life\", Yu Xianyun's \"In the Name of the Country\" won 21st, 22nd, 23rd, The 25th National Photographic Art Exhibition Gold Award; Liu Jun's \"Fisher Songs and Moon\" won the 21st Austria Trembler Super Photo Tour Competition Gold Award; Sun Chonglin's \"Little Wangmu\" Gold Award in the second PSAChina International Photography Competition.\nThe Nanjing Photographic Association successfully held the third city photography conference in Nanjing; held photography exhibitions such as \"World Historical and Cultural Cities\", \"Hong Kong in the Eyes of Nanjing People\", \"Nanjing in the Eyes of College Students\"; in Italy, Japan, Singapore, and other countries held \"Splendid Nanjing\" and \"Ancient Capital Nanjing\" photography exhibitions in Italy, Japan, Singapore, and other countries; held \"Harmonious Nanjing\", \"I Love Nanjing\", \"Nanjing City Walls\", \"Four Seasons Jinling\" and other photography competitions; edited and published \"Nanjing New Look\", \"Nanjing\", \"Splendid Nanjing\", \"Brilliant Nanjing\", \"Nanjing City Wall\" and other large-scale picture albums.\nIn 2022, the photography competition, \"A Decade of Nanjing\", organised by Nanjing People's Association for Friendship with Foreign Countries (NPAFFC), sought to chart the changes in Nanjing through the eyes of foreigners living in the city. Almost half a million online votes were cast to decide the final winners.\nFolk crafts.\nThere are many kinds of folk crafts in Nanjing, including brocade, paper-cutting, lantern color, gold leaf, folding fan, velvet flower, carved velvet, wood carving, bamboo carving, etc.\nAs of 2019, Nanjing has 4 world human intangible cultural heritage projects (guqin art, Nanjing cloud brocade weaving, Chinese engraving, and printing techniques, Chinese paper-cutting), 11 national intangible cultural heritage projects, 64 Jiangsu Province and 70 Nanjing City intangible cultural heritage project.\nEducation.\nBy 2021, Nanjing has 68 institutions of higher learning, including ten 111-plan universities, eight 211 universities, and 97 academicians. As the educational center of southern China for more than 1,700 years, Nanjing has many highly ranked educational institutions, with the number of universities (13) listed in 147 Double First-Class Universities ranking third (after Beijing and Shanghai). The ratio of college students to the total population ranks No.1 among large cities nationwide. Nanjing was ranked 69th globally by the QS Best Student City in 2025. \nNanjing has the fifth-largest scientific research output of any city in the world. When compared to other countries in the region, Nanjing ranked higher than South Korea, securing third place in Asia and Oceania after China and Japan, according to the Nature Index for 2025. For instance, Nanjing's share of the 2024 Nature Index is 2,135.61, with a count of 4,282, while South Korea's share is 2,017.95, with 3,431 counts. Since 2022, it has been ranked as the world's top second scientific research center in earth &amp; environmental sciences after Beijing and the world's top third scientific research center in chemistry, physical sciences, and natural sciences after Beijing and Shanghai, according to the Nature Index. \nNanjing University is considered one of the top national universities nationwide, and it is ranked among the world's top 10 universities by Nature Index. As of 2025, Nanjing University ranked 7th in China, 14th in Asia and 65th globally by Times Higher Education World University Rankings. Southeast University is also among the top universities in China, ranking 101-150 globally. It is considered one of the best universities for Architecture and Engineering in China. Many universities in Nanjing have satellite campuses or have moved their main campus to Xianlin University City in the eastern suburb. Some of the other most prominent national universities in Nanjing are:\nSome of the other most prominent national universities in Nanjing are:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nPrivate universities and colleges, such as Communication University of China, Nanjing and Hopkins-Nanjing Center are also located in the city.\nSome notable high schools in Nanjing are: Jiangpu Senior High School, Jinling High School, Liuhe First School, Nanjing Foreign Language School, The Second Yuying Foreign Languages School of Nanjing, High School Affiliated to Nanjing Normal University, Nanjing No.1 High School, Nanjing Zhonghua High School, Caulfield Grammar School (Nanjing Campus), Nanjing No.29 High School, Yuhuatai Senior High School.\nSister cities and twin towns.\nNanjing is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNanjing's sister-city relationship with Nagoya, Japan, was suspended on February 21, 2012, following public comments by Nagoya mayor Takashi Kawamura denying the Nanjing Massacre. Non-governmental relations have been subsequently restored.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21792", "revid": "124925", "url": "https://en.wikipedia.org/wiki?curid=21792", "title": "Nostratic", "text": ""}
{"id": "21793", "revid": "29913457", "url": "https://en.wikipedia.org/wiki?curid=21793", "title": "Ninth Fort", "text": "Historic fort in \u0160ilainiai elderate, Kaunas, Lithuania\nThe Ninth Fort () is a stronghold in the northern part of \u0160ilainiai elderate, Kaunas, Lithuania. It is a part of the Kaunas Fortress, built in the late 19th century. During the Soviet occupation, the fort was used as a prison and way-station for prisoners being transported to labour camps. After the occupation of Lithuania by Nazi Germany, the fort was used as a place of execution for Jews, captured Soviets, and others.\nHistory.\nAt the end of the 19th century the city of Kaunas was fortified and by 1890 was encircled by eight forts and nine gun batteries. Construction of the Ninth Fort (its numerical designation having become its name) began in 1902 and was completed on the eve of World War I. From 1924 on, the Ninth Fort was used as the Kaunas Prison.\nDuring the Soviet occupation in 1940\u20131941, the Ninth Fort was used by the NKVD to house political prisoners pending transfer to Gulag forced labor camps.\nDuring Nazi occupation, the Ninth Fort was a place of mass murder and 45,000 to 50,000 Jews, most from Kaunas and largely the Kovno Ghetto, were transported to the Ninth Fort and murdered by Nazis and Lithuanian collaborators in what became known as the Kaunas massacre.\nNotable among the victims was Rabbi Elchonon Wasserman of Baranovitch. In addition, Jews from as far as France, Austria and Germany were brought to Kaunas during the Nazi occupation, and executed in the Ninth Fort. In 1943, the Germans operated special Jewish squads to dig mass graves and burn the remaining corpses. One squad of 64 people managed to escape the fortress on the eve of 1944. That year, as the Soviets moved in, the Germans liquidated the ghetto and what had by then come to be known as the \"Fort of Death\". The prisoners were dispersed to other camps. After World War II, the Soviets again used the Ninth Fort as a prison for several years. From 1948 to 1958, farm organizations were managed from the Ninth Fort.\nIn 1958, a museum was established in the Ninth Fort. In 1959, an exhibition was prepared in four cells, telling of the Nazi war crimes carried out in Lithuania. In 1960, the discovery, cataloging, and forensic investigation of local mass murder sites began.\nMuseum.\nThe Ninth Fort museum contains collections of historical artifacts related both to Soviet atrocities and the Nazi genocide, as well as materials related to the earlier history of Kaunas and Ninth Fort. Most exhibits are labelled in English.\nMemorial.\nThe memorial to the victims of Nazism at the Ninth Fort in Kaunas, Lithuania, was designed by sculptor A. Ambraziunas. Erected in 1984, the monument is 105 feet (32 m) high. The mass burial place of the victims of the massacres carried out in the fort is a grass field, marked by a simple yet frankly worded memorial written in several languages. It reads, \"This is the place where Nazis and their assistants killed more than 30,000 Jews from Lithuania and other European countries.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "21794", "revid": "47822643", "url": "https://en.wikipedia.org/wiki?curid=21794", "title": "Nostratic languages", "text": "Proposed superfamily of Eurasian and African languages\nNostratic is a hypothetical language macrofamily including many of the language families of northern Eurasia first proposed in 1903. Though the Nostratic hypothesis once had a measure of support among mainstream linguists, it is now generally considered a fringe theory with very low support. The exact composition of languages families included in Nostrastic varies based on proponent; it typically includes the Kartvelian, Indo-European, and the controversial Ural-Altaic family, as well as the Afroasiatic languages, and the hypothetical Elamo-Dravidian languages.\nThe Nostratic hypothesis originates with Holger Pedersen in the early 20th century. The name \"Nostratic\" is due to Pedersen (1903), derived from the Latin \"nostrates\" \"fellow countrymen\". The hypothesis was significantly expanded in the 1960s by Soviet linguists, notably Vladislav Illich-Svitych and Aharon Dolgopolsky.\nThe hypothesis has fallen out of favour since the latter half of the 20th century and has limited degrees of acceptance, predominantly among a minority of Russian linguists. Linguists worldwide mostly reject Nostratic and many other macrofamily hypotheses with the exception of Den\u00e9\u2013Yeniseian languages, which has been met with some degree of acceptance. In Russia, it is endorsed by a minority of linguists, such as Vladimir Dybo, but is not a generally accepted hypothesis. Some linguists take an agnostic view. Eurasiatic, a similar grouping, was proposed by Joseph Greenberg (2000) and endorsed by Merritt Ruhlen.\nHistory of research.\nOrigin of the Nostratic hypothesis.\nThe last quarter of the 19th century saw various linguists putting forward proposals linking the Indo-European languages to other language families, such as Finno-Ugric and Altaic.\nThese proposals were taken much further in 1903 when Holger Pedersen proposed \"Nostratic\", a common ancestor for the Indo-European, Finno-Ugric, Samoyed, Turkish, Mongolian, Manchu, Yukaghir, Eskimo, Semitic, and Hamitic languages, with the door left open to the eventual inclusion of others.\nThe name \"Nostratic\" derives from the Latin word \"nostr\u0101s\", meaning 'our fellow-countryman' (plural: \"nostrates\") and has been defined, since Pedersen, as consisting of those language families that are related to Indo-European. Merritt Ruhlen notes that this definition is not properly taxonomic but amorphous, since there are broader and narrower degrees of relatedness, and moreover, some linguists who broadly accept the concept (such as Greenberg and Ruhlen himself) have criticised the name as reflecting the ethnocentrism frequent among Europeans at the time. Martin Bernal has described the term as distasteful because it implies that speakers of other language families are excluded from academic discussion. However, some people like Pedersen's older contemporary Henry Sweet attributed some of the resistance by Indo-European specialists to hypotheses of wider genetic relationships as \"prejudice against dethroning [Indo-European] from its proud isolation and affiliating it to the languages of yellow races\". Proposed alternative names such as \"Mitian\", formed from the characteristic Nostratic first- and second-person pronouns \"mi\" 'I' and \"ti\" 'you' (more accurately 'thee'), have not attained the same currency.\nAn early supporter was the French linguist Albert Cuny\u2014better known for his role in the development of the laryngeal theory\u2014who published his \"Recherches sur le vocalisme, le consonantisme et la formation des racines en \u00ab nostratique \u00bb, anc\u00eatre de l'indo-europ\u00e9en et du chamito-s\u00e9mitique\" ('Researches on the Vocalism, Consonantism, and Formation of Roots in \"Nostratic\", Ancestor of Indo-European and Hamito-Semitic') in 1943. Although Cuny enjoyed a high reputation as a linguist, the work was coldly received.\nMoscow School of Comparative Linguistics.\nWhile Pedersen's Nostratic hypothesis did not make much headway in the West, it became quite popular in the Soviet Union. Working independently at first, Vladislav Illich-Svitych and Aharon Dolgopolsky elaborated the first version of the contemporary form of the hypothesis during the 1960s. They expanded it to include additional language families. Illich-Svitych also prepared the first dictionary of the hypothetical language. Dolgopolsky's most recent \"Nostratic Dictionary\" was published in 2008, and is considered the most up-to-date attempt at a Nostratic lexicon.\nA principal source for the items in Illich-Svitych's dictionary was the earlier work of Alfredo Trombetti (1866\u20131929), an Italian linguist who had developed a classification scheme for all the world's languages, widely reviled at the time and subsequently ignored by almost all linguists. In Trombetti's time, a widely held view on classifying languages was that similarity in inflections is the surest proof of genetic relationship. In the interim, the view had taken hold that the comparative method\u2014previously used as a means of studying languages already known to be related and without any thought of classification\u2014is the most effective means to establish genetic relationship, eventually hardening into the conviction that it is the only legitimate means to do so. This view was basic to the outlook of the new Nostraticists. Although Illich-Svitych adopted many of Trombetti's etymologies, he sought to validate them by a systematic comparison of the sound systems of the languages concerned.\nConstituent language families.\nThe language families proposed for inclusion in Nostratic vary, but all Nostraticists agree on a common core of language families, with differences of opinion appearing over the inclusion of additional families.\nThe three groups universally accepted among Nostraticists are Indo-European, Uralic, and Altaic. While the validity of Altaic itself generally rejected by linguists, is taken for granted by Nostraticists. Nearly all also include the Kartvelian and Dravidian language families.\nFollowing Pedersen, Illich-Svitych, and Dolgopolsky, most advocates of the theory have included Afroasiatic, though criticisms by Joseph Greenberg and others from the late 1980s onward suggested a reassessment of this position.\nThe Sumerian and Etruscan languages, regarded as language isolates by linguists, are thought by some to be Nostratic languages as well. Others, however, consider one or both to be members of another macrofamily called Den\u00e9\u2013Caucasian. Another notional isolate, the Elamite language, also figures in a number of Nostratic classifications.\nIn 1987 Joseph Greenberg proposed a similar macrofamily which he called Eurasiatic. It included the same \"Euraltaic\" core (Indo-European, Uralic, and Altaic), but excluded some of the above-listed families, most notably Afroasiatic. At about this time Russian Nostraticists, notably Sergei Starostin, constructed a revised version of Nostratic which was slightly broader than Greenberg's grouping but which similarly left out Afroasiatic.\nBeginning in the early 2000s, a consensus emerged among proponents of the Nostratic hypothesis. Greenberg basically agreed with the Nostratic concept, though he stressed a deep internal division between its northern 'tier' (his Eurasiatic) and a southern 'tier' (principally Afroasiatic and Dravidian). Georgiy Starostin (2002) arrives at a tripartite overall grouping: he considers Afroasiatic, Nostratic and Elamite to be roughly equidistant and more closely related to each other than to anything else. Sergei Starostin's school has now re-included Afroasiatic in a broadly defined Nostratic, while reserving the term Eurasiatic to designate the narrower subgrouping which comprises the rest of the macrofamily. Recent proposals thus differ mainly on the precise placement of Kartvelian and Dravidian.\nAccording to Greenberg, Eurasiatic and Amerind form a genetic node, being more closely related to each other than either is to \"the other families of the Old World\". There are a number of hypotheses incorporating Nostratic into an even broader linguistic 'mega-phylum', sometimes called Borean, which would also include at least the Den\u00e9\u2013Caucasian and perhaps the Amerind and Austric superfamilies. The term SCAN has been used for a group that would include Sino-Caucasian, Amerind, and Nostratic. None of these proposed links have found wider acceptance outside of Nostraticists.\nThe following table summarizes the constituent language families of Nostratic, as described by Holger Pedersen, Vladislav Illich-Svitych, Sergei Starostin, and Aharon Dolgopolsky.\nProposed features of Proto-Nostratic.\nAccording to Dolgopolsky, the Proto-Nostratic language had analytic structure, which he argues by diverging of post- and prepositions of auxiliary words in descendant languages.\nDolgopolsky states three lexical categories to be in the Proto-Nostratic language:\nWord order was subject\u2013object\u2013verb when the subject was a noun, and object\u2013verb\u2013subject when it was a pronoun. Attributive (expressed by a lexical word) preceded its head. Pronominal attributive ('my', 'this') might follow the noun. Auxiliary words are considered to be postpositions.\nStatus within comparative linguistics.\nThe Nostratic hypothesis is not endorsed by the mainstream of comparative linguistics.\nNostraticists tend to refuse to include in their schema language families for which no proto-language has yet been reconstructed. This approach was criticized by Joseph Greenberg on the ground that genetic classification is necessarily prior to linguistic reconstruction, but this criticism has so far had no effect on Nostraticist theory and practice.\nCertain critiques have pointed out that the data from individual, established language families that is cited in Nostratic comparisons often involves a high degree of errors; Campbell (1998) demonstrates this for Uralic data. Defenders of the Nostratic theory argue that were this to be true, it would remain that in classifying languages genetically, positives count for vastly more than negatives (Ruhlen 1994). The reason for this is that, above a certain threshold, resemblances in sound/meaning correspondences are highly improbable mathematically.\nPedersen's original Nostratic proposal synthesized earlier macrofamilies, some of which, including Indo-Uralic, involved extensive comparison of inflections. It is true the Russian Nostraticists initially emphasized lexical comparisons. Critics argue that were one to collect all the words from the various known Indo-European languages and dialects which have at least one of any four meanings, one could easily form a list that would cover any conceivable combination of two consonants and a vowel (of which there are only about 20\u00d720\u00d75\u00a0=\u00a02000). Nostraticists respond that they do not compare isolated lexical items but reconstructed proto-languages. To include a word for a proto-language it must be found in a number of languages and the forms must be relatable by regular sound changes. In addition, many languages have restrictions on root structure, reducing the number of possible root-forms far below its mathematical maximum. These languages include, among others, Indo-European, Uralic, and Altaic\u2014all the core languages of the Nostratic hypothesis. For a highly critical assessment of the work of the Moscow School, especially the work of Illich-Svitych, cf. Campbell and Poser 2008:243-264.\nCampbell and Poser argue that Nostratic, as reconstructed by Illich-Svitych and others, is \"typologically flawed\". For instance, they point out that, surprisingly, very few Nostratic roots contain two voiceless stops, which are less marked and should therefore occur more frequently, and where such roots do occur, in almost all cases the second stop occurs after a sonorant. In summary, Campbell and Poser reject the Nostratic hypothesis and, as a parting shot, state that they \"seriously doubt that further research will result in any significant support for this hypothesized macro-family.\"\nProto-Indo-European \"*b[h]ars-\" seems to be a cultural loanword from Semitic (though several reputable Indo-Europeanists dispute this and consider it to be a native IE word). Much of the IE agricultural lexicon is not shared among all branches and seems to have been borrowed, thus supporting the view that the expansion of IE languages was post-Neolithic rather than a Neolithic one as postulated by Renfrew's theory.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n\u00f7==External links=="}
{"id": "21796", "revid": "50912557", "url": "https://en.wikipedia.org/wiki?curid=21796", "title": "Namespace", "text": "Container for a set of identifiers\nIn computing, a namespace is a set of signs (\"names\") that are used to identify and refer to objects of various kinds. A namespace ensures that all of a given set of objects have unique names so that they can be easily identified.\nNamespaces are commonly structured as hierarchies to allow reuse of names in different contexts. As an analogy, consider a system of naming of people where each person has a given name, as well as a family name shared with their relatives. If the first names of family members are unique only within each family, then each person can be uniquely identified by the combination of first name and family name; there is only one Jane Doe, though there may be many Janes. Within the namespace of the Doe family, just \"Jane\" suffices to unambiguously designate this person, while within the \"global\" namespace of all people, the full name must be used.\nProminent examples for namespaces include file systems, which assign names to files.\nSome programming languages organize their variables and subroutines in namespaces.\nComputer networks and distributed systems assign names to resources, such as computers, printers, websites, and remote files. Operating systems can partition kernel resources by isolated namespaces to support virtualization containers.\nSimilarly, hierarchical file systems organize files in directories. Each directory is a separate namespace, so that the directories \"letters\" and \"invoices\" may both contain a file \"to_jane\".\nIn computer programming, namespaces are typically employed for the purpose of grouping symbols and identifiers around a particular functionality and to avoid name collisions between multiple identifiers that share the same name.\nIn networking, the Domain Name System organizes websites (and other resources) into hierarchical namespaces.\nName conflicts.\nElement names are defined by the developer. This often results in a conflict when trying to mix XML documents from different XML applications.\nThis XML carries HTML table information:\n&lt;table&gt;\n &lt;tr&gt;\n &lt;td&gt;Apples&lt;/td&gt;\n &lt;td&gt;Oranges&lt;/td&gt;\n &lt;/tr&gt;\n&lt;/table&gt;\nThis XML carries information about a table (i.e. a piece of furniture):\n&lt;table&gt;\n &lt;name&gt;Mahogany Coffee Table&lt;/name&gt;\n &lt;width&gt;80&lt;/width&gt;\n &lt;length&gt;120&lt;/length&gt;\n&lt;/table&gt;\nIf these XML fragments were added together, there would be a name conflict. Both contain a codice_1 element, but the elements have different content and meaning.\nAn XML parser will not know how to handle these differences.\nSolution via prefix.\nName conflicts in XML can easily be avoided using a name prefix.\nThe following XML distinguishes between information about the HTML table and furniture by prefixing \"h\" and \"f\" at the beginning of the elements.\n&lt;h:table&gt;\n &lt;h:tr&gt;\n &lt;h:td&gt;Apples&lt;/h:td&gt;\n &lt;h:td&gt;Oranges&lt;/h:td&gt;\n &lt;/h:tr&gt;\n&lt;/h:table&gt;\n&lt;f:table&gt;\n &lt;f:name&gt;Mahogany Coffee Table&lt;/f:name&gt;\n &lt;f:width&gt;80&lt;/f:width&gt;\n &lt;f:length&gt;120&lt;/f:length&gt;\n&lt;/f:table&gt;\nNaming system.\nA name in a namespace consists of a namespace name and a local name. The namespace name is usually applied as a prefix to the local name.\nIn augmented Backus\u2013Naur form:\nname = &lt;namespace name&gt; separator &lt;local name&gt;\nWhen local names are used by themselves, name resolution is used to decide which (if any) particular name is alluded to by some particular local name.\nDelegation.\nDelegation of responsibilities between parties is important in real-world applications, such as the structure of the World Wide Web. Namespaces allow delegation of identifier assignment to multiple name issuing organisations whilst retaining global uniqueness. A central Registration authority registers the assigned namespace names allocated. Each namespace name is allocated to an organisation which is subsequently responsible for the assignment of names in their allocated namespace. This organisation may be a name issuing organisation that assign the names themselves, or another Registration authority which further delegates parts of their namespace to different organisations.\nHierarchy.\nA naming scheme that allows subdelegation of namespaces to third parties is a hierarchical namespace.\nA hierarchy is recursive if the syntax for the namespace names is the same for each subdelegation. An example of a recursive hierarchy is the Domain name system.\nAn example of a non-recursive hierarchy are Uniform Resource Name representing an Internet Assigned Numbers Authority (IANA) number.\nNamespace versus scope.\nA namespace name may provide context (scope in computer science) to a name, and the terms are sometimes used interchangeably. However, the context of a name may also be provided by other factors, such as the location where it occurs or the syntax of the name.\nIn programming languages.\nFor many programming languages, namespace is a context for their identifiers. In an operating system, an example of namespace is a directory. Each name in a directory uniquely identifies one file or subdirectory.\nAs a rule, names in a namespace cannot have more than one meaning; that is, different meanings cannot share the same name in the same namespace. A namespace is also called a context, because the same name in different namespaces can have different meanings, each one appropriate for its namespace.\nFollowing are other characteristics of namespaces:\nAs well as its abstract language technical usage as described above, some languages have a specific keyword used for explicit namespace control, amongst other uses. Below is an example of a namespace in C++:\nimport std;\n// This is how one brings a name into the current scope. In this case, it's\n// bringing them into global scope.\nusing std::println;\nnamespace box1 {\n constexpr int BOX_SIDE = 4;\nnamespace box2 {\n constexpr int BOX_SIDE = 12;\nint main() {\n constexpr int BOX_SIDE = 42;\n println(\"{}\", box1::BOX_SIDE); // Outputs 4.\n println(\"{}\", box2::BOX_SIDE); // Outputs 12.\n println(\"{}\", BOX_SIDE); // Outputs 42.\nComputer-science considerations.\nA namespace in computer science (sometimes also called a name scope) is an abstract container or environment created to hold a logical grouping of unique identifiers or symbols (i.e. names). An identifier defined in a namespace is associated only with that namespace. The same identifier can be independently defined in multiple namespaces. That is, an identifier defined in one namespace may or may not have the same meaning as the same identifier defined in another namespace. Languages that support namespaces specify the rules that determine to which namespace an identifier (not its definition) belongs.\nThis concept can be illustrated with an analogy. Imagine that two companies, X and Y, each assign ID numbers to their employees. X should not have two employees with the same ID number, and likewise for Y; but it is not a problem for the same ID number to be used at both companies. For example, if Bill works for company X and Jane works for company Y, then it is not a problem for each of them to be employee #123. In this analogy, the ID number is the identifier, and the company serves as the namespace. It does not cause problems for the same identifier to identify a different person in each namespace.\nIn large computer programs or documents it is common to have hundreds or thousands of identifiers. Namespaces (or a similar technique, see Emulating namespaces) provide a mechanism for hiding local identifiers. They provide a means of grouping logically related identifiers into corresponding namespaces, thereby making the system more modular.\nData storage devices and many modern programming languages support namespaces. Storage devices use directories (or folders) as namespaces. This allows two files with the same name to be stored on the device so long as they are stored in different directories. In some programming languages (e.g. C++, Python), the identifiers naming namespaces are themselves associated with an enclosing namespace. Thus, in these languages namespaces can nest, forming a namespace tree. At the root of this tree is the unnamed global namespace.\nUse in common languages.\nC.\nIt is possible to use anonymous structs as namespaces in C since C99.\nMath.h:\nconst struct {\n double PI;\n double (*sin)(double);\n} Math;\nMath.c:\nstatic double _sin(double arg) {\n return sin(arg);\nconst struct {\n double PI;\n double (*sin)(double);\n} Math = { M_PI, _sin };\nMain.c:\nint main() {\n printf(\"sin(0) = %d\\n\", Math.sin(0));\n printf(\"pi is %f\\n\", Math.PI);\nC++.\nIn C++, a namespace is defined with a namespace block.\nnamespace abc {\n int bar;\nWithin this block, identifiers can be used exactly as they are declared. Outside of this block, the namespace specifier must be prefixed. For example, outside of codice_2, codice_3 must be written codice_4 to be accessed. C++ includes another construct that makes this verbosity unnecessary. By adding the line\nusing namespace abc;\nto a piece of code, the prefix codice_5 is no longer needed.\nIdentifiers that are not explicitly declared within a namespace are considered to be in the global namespace.\nint foo;\nThese identifiers can be used exactly as they are declared, or, since the global namespace is unnamed, the namespace specifier codice_6 can be prefixed. For example, codice_7 can also be written codice_8.\nNamespace resolution in C++ is hierarchical. This means that within the hypothetical namespace codice_9, the identifier codice_10 refers to codice_11. If codice_11 doesn't exist, it then refers to codice_13. If neither codice_11 nor codice_13 exist, codice_10 refers to codice_17, an identifier in the global namespace.\nNamespaces in C++ are most often used to avoid naming collisions. Although namespaces are used extensively in recent C++ code, most older code does not use this facility because it did not exist in early versions of the language. For example, the entire C++ Standard Library is defined within codice_18, but before standardization many components were originally in the global namespace. The codice_19 statement can be used to import a symbol into the current scope.\nThe use of the codice_19 statements in headers for reasons other than backwards compatibility (e.g., convenience) is considered to be against good code practices, as those codice_19 statements propagate into all translation units that include the header. However, modules do not export codice_19 statements unless explicitly marked codice_23, making codice_19 statements safer to use. For instance, one can import a module codice_25 with matching namespace codice_26 and then use codice_19 statements on symbols from that namespace to simplify verbose namespaces. Note that unlike other languages like Java or Rust, C++ modules, namespaces and source file structure do not necessarily match, though it is convention to match them for clarity (for example, module codice_28 matches namespaced class codice_29 and resides in file codice_30).\ncodice_19 should be used to simplify verbose nested namespaces when modular translation units are used.\nexport module org.wikipedia.project.App;\nimport std;\nimport org.wikipedia.project.fs;\nimport org.wikipedia.project.util;\nusing org::wikipedia::project::fs::File;\nusing org::wikipedia::project::util::ConfigLoader;\nusing org::wikipedia::project::util::logging::Logger;\nusing org::wikipedia::project::util::logging::LoggerFactory;\nexport namespace org::wikipedia::project {\nclass App {\nprivate:\n Logger logger;\n // private fields and methods\npublic:\n App():\n logger{LoggerFactory::getLogger(\"Main\")} {\n ConfigLoader cl(File(\"config/config_file.txt\"));\n logger.log(\"Application starting...\");\n // rest of code\n};\nC++11 introduces \"inline namespaces\", which is such that its members are treated as if they are also members of the enclosing namespace. It is declared by writing codice_32. It is akin to an implicit codice_33 statement, meaning qualifying symbols in it is optional.\nThe inline property is transitive. If a namespace codice_34 contains an inline namespace codice_35, which in turn contains another inline namespace codice_36, then members of codice_36 can be accessed as if they were members of codice_34 or codice_35.\nA primary use case for inline namespaces is ABI compatibility and versioning. By placing different versions of an API within distinct inline namespaces (e.g., codice_40, codice_41), and then making the currently desired version inline, library developers can manage ABI compatibility. When a new version is released, the inline keyword can be moved to the new version's namespace, allowing users to automatically link against the new version while still enabling access to older versions through explicit qualification.\nnamespace mylib::utils {\n namespace v1 {\n void func() {\n // Old implementation\n // v2 is the currently active version\n inline namespace v2 {\n void func() {\n // New implementation\nint main() {\n // Calls mylib::utils::v2::func() implicitly\n mylib::utils::func();\n // Calls mylib::utils::v2::func() explicitly\n mylib::utils::v2::func();\n // Calls mylib::utils::v1::func() explicitly\n mylib::utils::v1::func();\n return 0;\nC#.\nNamespaces are heavily used in C# language. All .NET Framework classes are organized in namespaces, to be used more clearly and to avoid chaos. Furthermore, custom namespaces are extensively used by programmers, both to organize their work and to avoid naming collisions.\nWhen referencing a class, one should specify either its fully qualified name, which means namespace followed by the class name:\nSystem.Console.WriteLine(\"Hello World!\");\nint i = System.Convert.ToInt32(\"123\");\nor add a codice_19 statement. This eliminates the need to mention the complete name of all classes in that namespace.\nusing System;\nConsole.WriteLine(\"Hello World!\");\nint i = Convert.ToInt32(\"123\");\nIn the above examples, codice_43 is a namespace, and codice_44 and codice_45 are classes defined within codice_43.\nUnlike C++, codice_19 can only import all symbols in a namespace (much like codice_33 from C++, codice_49 in Rust, or codice_50 in Java). It cannot be used to import individual symbols and classes like it is used in Java.\nnamespace Wikipedia.Project;\nusing System;\nusing System.IO;\nusing Microsoft.Extensions.Logging;\nusing Wikipedia.Project.Utility;\nclass App\n private static ILogger&lt;Program&gt; logger;\n public App()\n ConfigLoader cl = new ConfigLoader(Path.Combine(\"config\", \"config_file.txt\"));\n LoggerFactory loggerFactory = LoggerFactory.Create(builder =&gt; \n builder.AddConsole();\n logger = loggerFactory.CreateLogger&lt;Program&gt;();\n logger.LogInformation(\"Application starting...\");\n // rest of code\nUnlike C++, C# namespaces do not allow relative referencing of symbols. For example, the class codice_51 cannot be referred to as codice_52 even if referred to from within namespace codice_53: either the namespace codice_54 must be imported to refer to class codice_55, or codice_51 must be fully qualified.\nJava.\nIn Java, the idea of a namespace is embodied in Java packages. All code belongs to a package, although that package need not be explicitly named. Code from other packages is accessed by prefixing the package name before the appropriate identifier, for example codice_57 in codice_58 can be referred to as codice_59 (this is known as the fully qualified class name). Like C++, Java offers a construct that makes it unnecessary to type the package name (codice_60). However, certain features (such as reflection) require the programmer to use the fully qualified name.\nUnlike C++, namespaces in Java are not hierarchical as far as the syntax of the language is concerned. However, packages are named in a hierarchical manner. For example, all packages beginning with codice_61 are a part of the Java platform\u2014the package contains classes core to the language, and contains core classes specifically relating to reflection.\nIn Java (and Ada, C#, and others), namespaces/packages express semantic categories of code. For example, in C#, codice_62 contains code provided by the system (the .NET Framework). How specific these categories are and how deep the hierarchies go differ from language to language.\nFunction and class scopes can be viewed as implicit namespaces that are inextricably linked with visibility, accessibility, and object lifetime.\nIn Java, packages cannot be partially qualified like they can in C++. For instance, it is not possible to import the codice_61 namespace and then refer to codice_64 as codice_65. Symbols must either be fully qualified or imported completely into scope. codice_60 statements are not transitive nor can they be deliberately marked codice_23 like in C++. All codice_60 statements must appear at the beginning of the file, and cannot be written at any other scope. This is in contrast to C++, where the codice_69 namespace can be imported by writing using namespace std;, and then codice_70 can be referred to as codice_71.\npackage org.wikipedia.project;\nimport java.nio.file.Paths;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport org.wikipdia.project.util.ConfigLoader;\npublic class App {\n private static final Logger logger = Logger.getLogger(Main.class.getName());\n public App() {\n ConfigLoader cl = new ConfigLoader(Paths.get(\"config/config_file.txt\"));\n logger.log(Level.INFO, \"Application starting...\");\n // rest of code\nBecause Java does not support independent functions outside of classes, static class methods and so-called \"utility classes\" (classes with private constructors and all methods and fields static) are the equivalent to C++-style namespaces. Some examples are codice_72, which contains the constants like codice_73 and methods like codice_74.\ncodice_60 statements can be used to import all symbols in a package, called a glob import, which is similar to codice_33 in C++. For instance, writing import java.util.*; imports all classes in the codice_77 package. This however, can cause symbol pollution within a file. Furthermore, using glob import statements on packages that have classes with the same name can cause ambiguity, and will fail to compile. However, codice_78 is implicitly imported into all Java source files by default.\nimport java.sql.*; // Imports all classes in java.sql, including java.sql.Date\nimport java.util.*; // Imports all classes in java.util, including java.util.Date\nDate d = new Date(); // Ambiguous Date reference resulting in compilation error\n// Instead, the fully-qualified names must be used:\njava.sql.Date sqlDate = new java.sql.Date(System.currentTimeMillis());\njava.util.Date utilDate = new java.util.Date();\nPHP.\nNamespaces were introduced into PHP from version 5.3 onwards. Naming collision of classes, functions and variables can be avoided.\nIn PHP, a namespace is defined with a namespace block.\nnamespace phpstar;\nclass FooBar\n public function foo(): void\n echo 'Hello world, from function foo';\n public function bar(): void\n echo 'Hello world, from function bar';\nWe can reference a PHP namespace with the following different ways:\ninclude \"phpstar/foobar.php\";\n$obj_foobar = new \\phpstar\\FooBar();\nuse phpstar\\FooBar;\n$obj_foobar = new FooBar();\nuse phpstar\\FooBar as FB;\n$obj_foobar = new FB();\n$obj_foobar-&gt;foo();\n$obj_foobar-&gt;bar();\nPython.\nIn Python, namespaces are defined by the individual modules, and since modules can be contained in hierarchical packages, then namespaces are hierarchical too.\nIn general when a module is imported then the names defined in the module are defined via that module's namespace, and are accessed in from the calling modules by using the fully qualified name.\nimport ModuleA\nModuleA.func1()\nModuleA.func2()\na: ModuleA.Class1 = Modulea.Class1()\nThe codice_79 statement can be used to insert the relevant names directly into the calling module's namespace, and those names can be accessed from the calling module without the qualified name:\nfrom ModuleA import func1\nfunc1()\nfunc2() # this will fail as an undefined name, as will the full name ModuleA.func2()\na: Class1 = Class1() # this will fail as an undefined name, as will the full name ModuleA.Class1()\nSince this directly imports names (without qualification) it can overwrite existing names with no warnings.\nA special form of the statement is codice_80 which imports all names defined in the named package directly in the calling module's namespace. Use of this form of import, although supported within the language, is generally discouraged as it pollutes the namespace of the calling module and will cause already defined names to be overwritten in the case of name clashes, though using codice_81 in Python can simplify verbose namespaces, such as nested namespaces.\nfrom selenium.webdriver import Firefox\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.remote.webelement import WebElement\nif __name__ == \"__main__\":\n driver: Firefox = Firefox()\n element: WebElement = driver.find_element(By.ID, \"myInputField\")\n element.send_keys(f\"Hello World{Keys.ENTER}\")\n action: ActionChains = ActionChains(driver)\n action.key_down(Keys.CONTROL).send_keys('a').key_up(Keys.CONTROL).perform()\nPython also supports codice_82 as a way of providing an alias or alternative name for use by the calling module:\nimport numpy as np\nfrom numpy.typing import NDArray, float32\na: NDArray[float32] = np.arange(1000)\nRust.\nIn Rust, a namespace is called a \"module\" and declared using codice_83. Symbols inside the module are by default private, and cannot be accessed externally, unless declared with the codice_84 keyword which exposes them. Modules can have sub-modules inside of them, allowing for nested namespaces.\nSimilar to the codice_19 keyword in C++, Rust has the codice_86 keyword to import symbols into the current scope.\nmod my_module {\n pub trait Greet {\n fn greet(&amp;self);\n pub struct Person {\n pub name: String,\n impl Greet for Person {\n fn greet(&amp;self) {\n println!(\"Hello, {}!\", self.name);\nfn main() {\n use my_module::{Person, Greet};\n let person = Person { name: String::from(\"Alice\") };\n person.greet();\nWriting mod util; indicates to the compiler to find either a file named codice_87 or codice_88. codice_89 in a codice_86 statement refers to the root of the current \"crate\" (project), while codice_91 can be used to refer to the parent module.\nmod util;\nuse std::fs::File;\nuse crate::util::ConfigLoader;\nuse crate::util::logging::{Logger, LoggerFactory};\npub struct App {\n config_loader: ConfigLoader;\nimpl App {\n pub fn new() -&gt; Self {\n config_loader = ConfigLoader::new(File::open(\"config/config_file.txt\"));\n config_loader.load();\n let logger: Logger = LoggerFactory::get_logger(\"Main\");\n logger.log(\"Application starting...\");\n // rest of code\nThe codice_86 keyword in Rust is more versatile than its counterpart codice_19 in C++. In addition to importing single symbols, symbol aliasing with codice_94 and glob imports, codice_86 can import multiple symbols on the same line using braces (which may be nested), import individual namespaces, and do all of the above in a single statement. This is an example of using all of the above:\nuse std::{\n fmt::*, // imports all symbols in std::fmt\n fs::{File, Metadata}, // imports std::fs::File and std::fs::Metadata\n io::{prelude::*, BufReader, BufWriter} // imports all symbols in std::io::prelude::*, std::io::BufReader, and std::io::BufWriter\n process, // imports the std::process namespace (for example std::process::Command can be referred to as process::Command)\n time // imports the std::time namespace\nXML namespace.\nIn XML, the XML namespace specification enables the names of elements and attributes in an XML document to be unique, similar to the role of namespaces in programming languages. Using XML namespaces, XML documents may contain element or attribute names from more than one XML vocabulary.\nSAP Namespace.\nIn SAP systems (especially ABAP environments), namespaces are used to prevent naming collisions between standard SAP-delivered objects and customer or partner developments.\nA namespace identifier is delimited with \u201c/\u201d (for example `/MYNS/`) and is reserved via SAP\u2019s namespace registration process. Once reserved, objects created under that namespace are uniquely identifiable and protected from unintended overwrite by SAP upgrades or imports.\nIn modern SAP landscapes (such as ABAP in the cloud and HDI containers), namespaces are also used to semantically group development artifacts or bundles.\nEmulating namespaces.\nIn programming languages lacking language support for namespaces, namespaces can be emulated to some extent by using an identifier naming convention. For example, C libraries such as libpng often use a fixed prefix for all functions and variables that are part of their exposed interface. Libpng exposes identifiers such as:\n png_create_write_struct\n png_get_signature\n png_read_row\n png_set_invalid\nThis naming convention provides reasonable assurance that the identifiers are unique and can therefore be used in larger programs without naming collisions. Likewise, many packages originally written in Fortran (e.g., BLAS, LAPACK) reserve the first few letters of a function's name to indicate the group to which the function belongs.\nThis technique has several drawbacks:\nIt also has a few advantages:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21797", "revid": "48035214", "url": "https://en.wikipedia.org/wiki?curid=21797", "title": "Nahum", "text": "Minor prophet in the Bible\nNahum ( or ; \"Na\u1e25\u016bm\") was a minor prophet whose prophecy is recorded in the \"Tanakh\", also called the Hebrew Bible and the Old Testament. His book comes in chronological order between Micah and Habakkuk in the Bible. He wrote about the end of the Assyrian Empire, and its capital city, Nineveh, in a vivid poetic style.\nLife.\nLittle is known about Nahum's personal history. His name means \"comfort\", and is derived from the same root as the Hebrew verb meaning \"to comfort\". He came from the town of Alqosh (Nahum 1:1), which scholars have attempted to identify with several cities, including the modern Alqosh in northern Iraq and Capernaum of northern Galilee. He was a very nationalistic Hebrew, however, and lived amongst the Elkoshites in peace. Nahum, called \"the Elkoshite\", is the seventh in order of the minor prophets. According to Jerome, Nahum's Elkosh was a little town in Galilee. This identification could explain how the famous New Testament city of Capernaum got its name. \nScholars with a preference for Hebrew manuscripts place Nahum's prophecy after the Assyrian king Ashurbanipal's Sack of Thebes in 663 B.C. This view is the current majority opinion because the city of Thebes is referred to in the past tense in the Masoretic Text of Nahum 3:8-10. However, both the Septuagint and Vulgate refer to the city in the present tense, and the former opinion held by scholars was that Nahum lived about a century earlier, before both the captivity of the ten lost tribes and the Sack of Thebes. The first-century Jewish historian Flavius Josephus places Nahum's life during the reign of Jotham. This view was also held by the Catholic scholar Thomas Worthington in his notes for the original Douay-Rheims Bible, writing: \"Nahum prophesied about 50 years after Jonah ... 135 before the destruction of Niniveh.\" In this view, rather than Ashurbanipal, Nahum's prophecy would have been directed at Tiglath-Pileser III, who revitalized the Neo-Assyrian Empire into a world power again and conquered most of the Levant, defeating and subjugating previously influential kingdoms, including Aram-Damascus. Tiglath-Pileser was contemporary with the reign of Jotham.\nWorks.\nNahum's writings could be taken as prophecy or as history. One account suggests that his writings are a prophecy written in about 615 BCE, just before the downfall of Assyria, while another account suggests that he wrote this passage as liturgy just after its downfall in 612 BCE.\nThe book was introduced in Reformation theologian Calvin's Commentary as a complete and finished poem:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;No one of the minor Prophets seems to equal the sublimity, the vehemence and the boldness of Nahum: besides, his Prophecy is a complete and finished poem; his exordium is magnificent, and indeed majestic; the preparation for the destruction of Nineveh, and the description of its ruin, and its greatness, are expressed in most vivid colors, and possess admirable perspicuity and fulness.\u2014\u200a\nThere are indications that an acrostic underlies the present text. Thus 1:2 begins with the first letter of the alphabet (\u05d0), verse 3b (\u2018in whirlwind\u2019) with the second letter (\u05d1), verse 4 with the third (\u05d2), and so on until from ten to sixteen of the twenty two letters have appeared. In places the scheme breaks down: in the process of transmission, what was once an alphabetic poem has now been seriously corrupted, rearranged, and supplemented.\nNahum, taking words from Moses himself, has shown in a general way what sort of \"Being God is\". Calvin argued that Nahum painted God by which his nature must be seen, and \"it is from that most memorable vision, when God appeared to Moses after the breaking of the tablets.\"\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Although all three chapters fall below the standards set by the developed Judaeo-Christian tradition concerning the nature of God and man\u2019s relation with his brother man\u2026 it is one of the world\u2019s classic rebukes of militarism\u2026. All tyrants are doomed. They make enemies of those whom they attack and oppress; they become corrupt, dissolute, drunken, effeminate; they are lulled into false security\u2026\nTomb.\nThe tomb of Nahum is supposedly inside the synagogue at Alqosh, although there are other places outside Iraq which also lay claim to being the original \"Elkosh\" from which Nahum hailed. Alqosh was emptied of its Jewish population in 1948 when they were expelled after Israel was recognized as a Jewish nation, and the synagogue that houses the tomb is now in a poor structural state, to the extent that the tomb itself is in danger of destruction. The tomb underwent basic repairs in 1796. When all Jews were forced to flee Alqosh in 1948, the iron keys to the tomb were handed to an Assyrian man, Sami Jajouhana. Few Jews visit the historic site, yet Jajouhana continues to keep the promise he made with his Jewish friends, and looks after the tomb.\nAs of early 2017, the tomb was in significant disrepair and was threatened by the rise of ISIS in Iraq. A team of engineers conducted a survey of the tomb and determined that the tomb was in danger of imminent collapse and might not survive another winter. A team led by the U.S.-based non-profit http:// (\"ARCH\") raised the funds necessary to stabilize the site. After raising the necessary funds, ARCH partnered with the Prague-based https://, experts in historic preservation and reconstruction to do the immediate stabilization work. Following coordination with local partners, the initial stabilization work was completed in January 2018. The stabilization work is expected to prevent further deterioration of the structure for between two and three years. With the tomb and its surrounding structure stabilized, ARCH is planning on raising the funding necessary to fully restore the site. On 26 April 2019, the United States government announced that it would contribute $500,000 to restore the tomb.\nTwo other possible burial sites mentioned in historical accounts are Elkesi, near Rameh in the Galilee and Elcesei in the West Bank.\nLiturgical commemoration.\nThe prophet Nahum is venerated as a saint in Eastern Christianity. On the Eastern Orthodox liturgical calendar, his feast day is December 1 (for those churches which follow the traditional Julian Calendar, December 1 currently falls on December 14 of the modern Gregorian Calendar). He is commemorated with the other minor prophets in the calendar of saints of the Armenian Apostolic Church on July 31.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21798", "revid": "50893216", "url": "https://en.wikipedia.org/wiki?curid=21798", "title": "November 17", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 17 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21799", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=21799", "title": "National Flags", "text": ""}
{"id": "21800", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=21800", "title": "Nazi Reich", "text": ""}
{"id": "21801", "revid": "13560851", "url": "https://en.wikipedia.org/wiki?curid=21801", "title": "New York state", "text": ""}
{"id": "21803", "revid": "44888193", "url": "https://en.wikipedia.org/wiki?curid=21803", "title": "Newfoundland English", "text": "Several accents and dialects of Atlantic Canadian English\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nNewfoundland English refers to several accents and dialects of Atlantic Canadian English found in the province of Newfoundland and Labrador. Most of these differ significantly from the English commonly spoken elsewhere in Canada and North America, reflecting the province's history and geography. Newfoundland was one of the first areas settled by England in North America, beginning in small numbers in the early 17th century and peaking in the early 19th century. After the 1783 independence of the colonies that formed the United States of America, Newfoundland remained part of British North America, becoming a Dominion within the British Empire in 1907. It joined Canada in 1949 as the last province to join the confederation. \nThe dialects of Newfoundland English developed in relative isolation due to the province\u2019s geography. Newfoundland is an island in the North Atlantic Ocean, separated from Labrador by the Strait of Belle Isle. This isolation allowed the dialects to develop independently of those on the North American mainland. Historically, Newfoundland English was first recognized as a distinct dialect in the late 18th century when George Cartwright published a glossary of Newfoundland words. Today, some words from Newfoundland English, such as \"b'y\" (a form of address), have gained recognition through popular culture in other parts of Canada, particularly in Ontario and eastward. \nNewfoundland English shares some linguistic features with Bermudian English, likely due to historical connections between the two regions. Both were part of the See of Nova Scotia until 1839, after which they became part of the Diocese of Newfoundland and Bermuda. The shared ecclesiastical and cultural ties, along with movement between the regions, may have contributed to similarities such as the use of \"b'y\" in Newfoundland and \"bye\" in Bermuda. \nNewfoundland English is often referred to as \"Newfinese\" (also spelled \"Newfunese\"). The term \"Newfie\" is also used but can be considered pejorative.\nInfluences.\nMuch of Newfoundland\u2019s English has been influenced by the languages and dialects of European settlers of the past, such as those who were British, Irish, or French. Also, Indigenous languages prevailed on the island, with some of their influence remains today.\nBritish.\nWhile there was an early dominance of merchants and migrants from Devon, they accounted for only around 30 percent of the English population in places like St. John's and Conception Bay in Newfoundland. Most of the coast, except the Avalon Peninsula, was settled by migrants from Dorset, Somerset, and Hampshire, which Handcock refers to as \"Wessex.\"\nA major reason for the migration pattern is that Poole, Dorset became a major port for the Newfoundland fish trade in the mid-1700s, which resulted in settlements that were densely clustered and mainly derived from Devon, Dorset, Somerset, and Hampshire, as these were located near major ports in the West Country of England. That group of migrants accounts for almost 80 percent of all English settlers on Newfoundland.\nUltimately, that has allowed for the preservation of speech patterns derived from the West Country in Newfoundland English. Paddock illustrates how the speech pattern survived in 72 coastal communities in Newfoundland. Specifically, the use of \"dark\" or \"velar\" allophone in the communities are phonological features of the West Country. There are regional differences in phonological features. Another preserved phonological feature is the Irish-like fronting for all vowels, which is found in communities on the southern shores of the Avalon Peninsula.\nAnother speech pattern that is adopted is the conservative paradigm for the verbs \"have\" and \"do\" found in the West Country. The verbs \"have\" and \"do\" are dependent on their function as auxiliaries or lexical verbs. As auxiliaries, the vernacular paradigm remains uninflected: \"he haven't seen her\". In contrast, when used as lexical verbs, the \"-s\" inflection appears throughout the paradigm, as in \"they haves/has no business being here\" or \"we doos [du:z]/does that all the time.\"\nOther forms of preservation are specific terms in vocabulary like \"moreish,\" meaning a particular food of which one cannot help having more, and are still used in Newfoundland.\nNewfoundland was a British colony for nearly two centuries until 1949, when it became a province of Canada. That makes Newfoundland English have features similar to those found in the English of the West Country. They include the use of certain vocabulary, grammar, and pronunciation. Newfoundland English has also developed its own distinct features over time, particularly by the influence of Irish and French migrants and its isolation from the rest of Canada.\nIrish.\nIrish people in Newfoundland fisheries can be traced back to 1675. Approximately half of the population of most settlements on the shores of the Avalon Peninsula was Irish by 1750. The first significant influx of immigrants occurred mainly during the first thirty years of the 19th century. The number of immigrants on the island had grown to 38,000 by 1836, which constituted half of the total population of Newfoundland.\nApproximately 85% of Irish immigrants originated from the counties of Kilkenny, Wexford, Waterford, Tipperary, and Carlow, in south-eastern Ireland. The remainder came from western counties, like Cork and Kerry.\nIrish migrants inhabited relatively limited areas of the province, primarily in the southern parts of the Avalon Peninsula. Irish and English migrants were divided by their different religious affiliations of Catholicism and Protestantism. There was intermingling of local economics, but those interactions were limited. The geography of the island reinforced the religious division resulted in distinctive and resistant dialects of English in Newfoundland and thus preserved the south-eastern speech patterns of Ireland in Newfoundland.\nThe speech pattern of using the \"after\" form of the perfect aspect of the verb has been widely adopted in Newfoundland English. That particular construction, as in \"look what I'm after doin' now!\", has quickly spread throughout the region, despite the existence of several other alternatives such as \"I've done,\" \"I've adone,\" and \"I bin done,\" which come from the West Country.\nAnother speech pattern that is preserved is the slit fricative [t] variant, a well-known feature of Irish English. The postvocalic /t/ contexts are prevalent in pre-pause positions except before consonants and are commonly used in Newfoundland's Avalon Peninsula. On the other hand, that characteristic is not shared by the Newfoundland settlements from the West Country. In addition, the monophthongal /e/ and postvocalic /l/ pronunciations are inherited from the Irish and are mainly used today by older Irish ethno-religious people.\nFrench.\nFrench settlement influences are prevalent in the Bay d\u2019Espoir and Port au Port Peninsula on the west coast of the island. Newfoundland French was deliberately discouraged by the government of Newfoundland in the public schools during the mid-20th century, and only a small handful of people, who are mainly elderly, still fluently speak the French-Newfoundland dialect. In the last couple of decades, many parents in the region have demanded and obtained Canadian French education for their children, but that would be Standard French education and does not represent a continuation of the old dialect per se.\nAlso, some people living in the Codroy Valley, on the southwestern tip of Newfoundland, have ancestors who were francophone but represent Acadian settlers from Canada's Maritime Provinces. They arrived during the 19th century and have lost the French language as well.\nIndigenous.\nMost of the Indigenous influence within Newfoundland English has been assimilated and forgotten under colonialism. The Beothuk, the Indigenous people of the island, whose language and people were eradicated in the 19th century, have had bits and pieces of their vocabulary poorly transcribed. None of it is used in today\u2019s vernacular.\nA scarce number of Indigenous terms are still used in Newfoundland\u2019s lexis and are influenced by the Innu, Mi\u2019kmaq, and Inuit peoples. For example, the term \"tabanask\", a term from the Innu language, refers to a toboggan. Also, the term \"babbish\" refers to stretched animal hide used in snowshoes. \"Sina\" refers to the edge of a floating ice field and is from the Inuit language.\nPhonology.\nConsonants.\nTh-stopping.\nThe is used to represent the voiced \"th\" sound , and a to represent the voiceless one . For example, \"that thing over there\" becomes \"dat ting over dere\" and is derived from Hiberno-English. The stopping of the interdental /\u00f0/ is present in the speech of those in Petty Harbour, a region south of the capital, St. John\u2019s. Research has shown that men tend to have /\u00f0/ stopping more often than women within this region, but that is not the case with function words like \"this, them, that, these.\" Middle-aged women were found to start /\u00f0/ stopping when they say function words, which would thus change to \"dis, dem, dat, dese.\"\nSlit fricative t.\nThe phoneme at the end of words or between vowels is pronounced as in Hiberno-English. The most common pronunciation is as a voiceless alveolar non-sibilant fricative, also known as a \"slit fricative.\" The phoneme does not have a separate symbol in IPA and can be transcribed as (a retracted voiceless dental fricative). Thus, \"hitting\" is distinguished from \"hissing\" only by the fricative in the latter word being pronounced with clenched teeth (see sibilant consonant) and being laminal, rather than the apical sound of the slit fricative in \"hitting\". As the \"th\" sounds are stopped in Newfoundland, there is no confusion between the slit and the sound. As a result, it is very common to hear \"thing\" being pronounced as \"ting,\" as is mentioned above. The slit fricative /t/, which replaces the usual Canadian /\u03b8/, acts as a marker of Newfoundlanders' identity.\nVoiced fricatives.\nThe modification of initial voiceless fricatives to voiced fricatives can be heard by those in the West Country region (the Northeast, South, and West Coasts and Labrador). Voiceless fricatives, such as /f/ and /s/, are often modified to their voiced fricative counterparts, /v/ and /z/ respectively. Terms like \"salt\" and \"fir\" thus change to \"zalt\" and \"vir\" as a result of that shift. Those speech patterns are less prominent today but survive in pockets in the West Country regions.\nSimplified consonant clusters.\nThe West Country is known to remove the last consonant of clusters in their speech. Terms like \"loft\", \"bald\", and \"almost\" are simplified to like \"lof\", \"bal,\" and \"almos\".\nH-dropping.\nBoth h-dropping and h-insertion occur in the West Country, and in many varieties of Newfoundland English. For example, Holyrood becomes Olyrood, and Avondale becomes Havondale.\nRhoticity.\nNewfoundland is mainly rhotic, like the rest of North America, and in Ireland and the West Country. Some non-rhoticity is found in some regions.\nL-darkness.\nSome speakers of Newfoundland English pronounce as unvelarized and so the phrase \"sell it later\" is pronounced (cf. General American ). That may be from Irish-influenced varieties of English since they have light variants in both coda and onset positions.\nPulmonic ingressive.\nNewfoundland English often pronounces the affirmative \"yeah\" with an inhalation, rather than an exhalation, in the older generations. That is an example of a rare pulmonic ingressive phone.\nVowels.\nIn much of Newfoundland, the words \"fear\" and \"fair\" are homophones. A similar merger is found in the Norfolk dialect of East Anglia, England, and in New Zealand English.\nNewfoundland English traditionally lacked Canadian raising, but that has changed to some extent in the generations since Newfoundland's 1949 joining Canada. People in the Avalon Peninsula, which underwent Irish settlement, display obvious Canadian raising pattern for /\u0251\u026a/ but not typically for the /\u0251\u028a/ diphthong. The latter feature has long existed in Newfoundland English but is not very common except in the rural South Coast community of Newhook. There, it exists in the speech patterns of more women than men.\nMany speakers of Newfoundland English have a complete merger of and (a \"kit\"\u2013\"dress\" merger), usually realized with (in words like \"bit\" and \"bet\") but with before (in words like \"beer\" and \"bear\"). The merger is common in Irish-settled parts of Newfoundland and is thought to be a relic of the former Irish pronunciation.\nTempo.\nSpeakers of Newfoundland English may seem to speak faster than other Canadian English speakers. The perceived tempo difference may be a coupling of obvious pronunciation differences with Newfoundland's unusual sayings and is a contributing factor to the difficulty that outsiders sometimes experience with understanding the dialect.\nGrammar.\n\"After\" past.\nIn a move that was almost certainly taken from Hiberno-English and influenced by the Irish language, Newfoundland English avoids using the verb \"to have\" in past participles and prefers formulations with \"after\" such as \"I'm after telling him to stop\" instead of \"I have told him to stop.\" That is because Irish has no verb \"to have\" but more particularly has a construction using the words \"Tar \u00e9is\" (meaning \"after\") to convey the sense of having just done something: \"T\u00e1im tar \u00e9is \u00e9 a dh\u00e9anamh\" means \"I am just after doing it\" or \"I have just done it.\" Possession in Irish would be indicated by \"Ta ... agam\", literally \"... is at me.\"\nNorthern Subject Rule.\nNewfoundland English often follows the Northern Subject Rule, a legacy of settlement from southeastern Ireland, which in turn was influenced by the Anglo-Irish settlement from Northern England into Ireland. For example, the verb \"to fly\" is conjugated for third-person plural as \"the birds flies.\" According to a 2011 study by Philip Comeau, that feature of Newfoundland English differs from the rule of dialects in Northern England because Newfoundland uses it as a marker of habitual aspect or verb stativity.\nArchaic pronouns.\n\"Ye\" is the plural form of \"you\" (singular) instead of \"you\" (plural), similar to how \"you guys\" is often used to replace \"you\" (plural) in Standard Canadian English. For example, when addressing two or more people, or when addressing one person but referring to everyone accompanying a person is, Newfoundland English uses \"What do ye think?\" instead of \"What do you guys think?\" Alternately, \"What do you think?\" is used to refer to a single person. That avoids the confusion of other English dialects in which a group of people would not know whether the speaker is inquiring about only the opinion of the person who is being speaking or the various opinions of the entire group. In most areas of Newfoundland that use the pronoun, such as the Avalon Peninsula outside St. John's, \"ye\" mirrors the same variant in Hiberno-English in which \"you\" (singular), \"you\" (plural), and \"they\" correspond to \"you,\" \"ye,\" and \"dey.\" The last arises simply from a change in pronunciation and so it is written \"they,\" but the other words are pronounced as in Standard English. Variants of \"ye\" are also used such as \"yeer\" (your), \"yeers\" (yours), and \"yeerselves\" (yourselves). In some communities on the Northeast Coast, \"you\" (singular), \"you\" (plural), and \"they\" correspond to \"ye,\" \"dee,\" and \"dey,\" respectively.\nHabitual aspect using \"be\".\nThe word \"bes\" is sometimes used in place of the normally-conjugated forms of \"to be\" to describe continual actions or states of being, as in \"that rock usually bes under water\" for \"that rock is usually under water,\" but the normal conjugation of \"to be\" is used in all other cases.\n\"Does be\" is a calque of Irish grammar into English. Since there is no habitual aspect in English, Irish speakers learning English would say \"does be\" as a literal translation of \"b\u00edonn m\u00e9\" \"I (habitually) am\".\n\"Me\" for \"my\" and \"mine\".\nUse or ownership in Newfoundland English is characterized by pronouncing \"my\" as \"me,\" which is common also in Ireland, Scotland, Northern and Western England, and some dialects in Australia. Before the Great Vowel Shift, \"my\" was pronounced , \"mine\" as , and \"me\" as . As with all other sound shifts, not all possible words have been changed in the other dialects that were noted. An example in Newfoundland is \"Where's me hat?\" for \"Where's my hat?\"\nUse of \"to\" for location.\nThe use of \"to\" to denote location is common in Newfoundland English by using \"where's that to?\" Replacing the standard \"where's that?\" is a usage comes from the West Country and is still common in southwestern England, particularly in Bristol.\nExpressions.\nArchaic adverbial intensifiers are preserved in Newfoundland such as \"that play was right boring\" and \"that play was some boring\" for \"that play was very boring\". They have been retained also in Northern England such as in the Yorkshire dialect and in Geordie and are sometimes heard in the Maritime Provinces of Canada.\nNewfoundland English is not homogeneous and varies markedly from community to community and from region to region, which reflects both ethnic origin and relative isolation. For many decades, Newfoundland had very few roads connecting its many communities. Fishing villages, in particular, remained very isolated.\nIn Newfoundland English, it is typical for a response to a metaphorical question like \"How's she cuttin'?\" with a dry literal response. A proper response to the foresaid question would be \"Like a knife.\" Or perhaps \"How ya gettin' on?\" To which the response might be \"Same way I gets off!\" The question/greeting is a phrase still current in the Irish Midlands and North but is rarely, if ever, responded to with such a literal answer there.\nIn recent years, the most commonly-noted Newfoundland English expression might be \"Whadd'ya at?\" (\"What are you at?\"), loosely translated to \"How's it going?\" or \"What are you doing?\" Coming in a close second might be \"You're stunned as me arse, b'y;\" it implies incredible stupidity or foolishness in the person being spoken to.\nOther local expressions include:\nAlso of note is the widespread use of the term \"b'y\" as a common form of address. It is shorthand for \"boy\", (and is a turn of phrase particularly pronounced with the Waterford dialect of Hiberno-Irish) but is used variably to address members of either sex. Another term of endearment, often spoken by older generations, is \"me ducky\", used when addressing a female in an informal manner, and usually placed at the end of a sentence which is often a question (Example: \"How's she goin', me ducky?\") \u2013 a phrase also found in East Midlands British English. Also pervasive as a sentence ending is \"right\" used in the same manner as the Canadian \"eh\" or the American \"huh\" or \"y'know\". Even if the sentence would otherwise be a non-question, the pronunciation of \"right\" can sometimes make it seem like affirmation is being requested.\nCertain words have also gained prominence amongst the speakers of Newfoundland English. For instance, a large body of water that may be referred to as a \"lake\" elsewhere may often (but not uniformly) be referred to as a \"pond.\" In addition, a large landmass that rises high out of the ground, regardless of elevation, is referred to unwaveringly as a \"hill,\" but there is a difference between a hill and a big hill.\nAnother major characteristic of some variants of Newfoundland English is adding the letter 'h' to words that begin with vowel sounds or removing 'h' from words that begin with it. In some districts, the term house commonly is referred to as the \"ouse,\" for example, and \"even\" might be said \"h'even.\" The idiom \"'E drops 'is h in 'Olyrood and picks en up in H'Avondal.\" is often used to describe that by using the neighbouring eastern towns Holyrood and Avondale as examples. There are many different variations of the Newfoundland dialect depending on geographical location within the province. It is also important to note that Labrador has a very distinct culture and dialect within its region.\nOther.\nAlthough it is referred to as \"Newfoundland English\" or \"Newfinese\", the island of Newfoundland is not the only place which uses the dialect. Some southerly areas of Labrador and an area near the Labrador border, the mostly English-speaking Basse-C\u00f4te-Nord of Quebec, also use it. Younger generations of the area have adapted the way of speaking, and created some of their own expressions. Some older generations speak Newfoundland English, but it is more commonly used by the younger generations. \"B'y\" is one of the most common terms used in the area.\nIt is also common to hear Newfoundland English in Yellowknife; Southern Alberta; and Fort McMurray, Alberta, where many Newfoundlanders have moved or commute regularly for employment. Newfoundland English is also used frequently in the city of Cambridge, Ontario because of the high population of Newfoundlanders there, most of whom are from Bell Island.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21804", "revid": "40040918", "url": "https://en.wikipedia.org/wiki?curid=21804", "title": "National flag", "text": "Flag of a country or nation\nA national flag is a flag that represents and symbolizes a given nation. It is flown by the government of that nation, but can also be flown by its citizens. A national flag is typically designed with specific meanings for its colors and symbols, which may also be used separately from the flag as a symbol of the nation. The design of a national flag is sometimes altered after the occurrence of important historical events.\nHistory.\nHistorically, flags originated as military standards, used as field signs. Throughout history, various examples of such proto-flags exist: the white cloth banners of the Zhou dynasty's armies in the 11th century BC, the \"vexillum\" standards flown by the armies of the Roman Empire, the Black Standard famously carried by Muhammad which later became the flag of the Abbasid Caliphate, and the various \"Raven banners\" flown by Viking chieftains. Angelino Dulcert published a series of comprehensive Portolan charts in the 14th century AD, which famously showcased the flags of several polities depicted \u2013 although these are not uniformly \"national flags\", as some were likely the personal standards of the respective nation's rulers.\nThe practice of flying flags indicating the country of origin \"outside\" of the context of warfare became common with the maritime flag. During the 13th century, the republics of Genoa and Venice both used maritime flags; William Gordon Perrin wrote that the republic of Genoa was \"one of the earliest states to adopt a national flag\".\nThe current design of the flag of the Netherlands originates as a variant of the late 16th century orange-white-blue \"Prinsenvlag\" (\"Prince's Flag\"), that was used in the Dutch War of Independence (1568\u20131648), evolving in the early 17th century as the red-white-blue \"Statenvlag\" (\"States Flag\"), the naval flag of the States-General of the Dutch Republic, making the Dutch flag perhaps the oldest tricolour flag in continuous use, although standardisation of the exact colours is of a much later date.\nDuring the Age of Sail in the early 17th century, the Union Jack finds its origins, when James VI of Scotland inherited the English and Irish thrones (as James I). On 12 April 1606, the new flag representing this regal union between England and Scotland was specified in a royal decree, according to which the flag of England (a red cross on a white background, known as St George's Cross), and the flag of Scotland (a white saltire on a blue background, known as the Saltire or St Andrew's Cross), would be joined, forming the flag of Great Britain and first Union Flag - but then without the red Cross of St. Patrick. It continued in use until 1 January 1801, the effective date of the legislative union of Great Britain and Ireland, when the Cross of St. Patrick (a red diagonal cross on white) was incorporated into the flag, giving the Union Jack its current design.\nWith the emergence of nationalist sentiment from the late 18th century national flags began to be displayed in civilian contexts as well. Notable early examples include the US flag, which was first adopted as a naval ensign in 1777 but began to be displayed as a generic symbol of the United States after the American Revolution, and the French Tricolor, which became a symbol of the Republic in the 1790s.\nMost countries of Europe standardised and codified the designs of their maritime flags as national flags, in the course of the 19th and early 20th centuries. The specifications of the flag of Denmark, based on a flag that was in continuous use since the 14th-century, were codified in 1748, as a rectangular flag with certain proportions, replacing the variant with a \"split\" (swallow-tail). The flag of Switzerland was introduced in 1889, also based on medieval war flags.\nIn Europe, the red-white-blue tricolour design of the flag of the Kingdom of the Netherlands became popular, since it was associated with a republican form of government through that country's long war of independence against the Spanish Crown. That association was greatly reinforced after the French Revolution (1789), when France used the same colours, but with vertical instead of horizontal stripes. Other countries in Europe (like Ireland, Italy, Romania and Estonia) and in South and Central America selected tricolours of their own to express their adherence to the principles of liberty, equality, and fraternity as embodied in the French flag, although some adopted a monarchical form of government with a constitution instead of a republican government.\nThe Ottoman flag (now the flag of Turkey) was adopted in 1844. Other non-European powers followed the trend in the late 19th century, the flag of Great Qing being introduced in 1862, that of Japan being introduced in 1870. \nAlso in the 19th century, most countries of South America introduced a flag as they became independent (Peru in 1820, Bolivia in 1851, Colombia in 1860, Brazil in 1822, etc.)\nCurrently, there are 193 national flags in the world flown by sovereign states that are members of the United Nations.\nProcess of adoption.\nThe national flag is often mentioned or described in a country's constitution, but its detailed description may be delegated to a flag law passed by the legislature, or even secondary legislation or in monarchies a decree.\nThus, the national flag is mentioned briefly in the Basic Law for the Federal Republic of Germany of 1949 \"the federal flag is black-red-gold\" (art. 22.2 \"Die Bundesflagge ist schwarz-rot-gold\"), but its proportions were regulated in a document passed by the government in the following year. The Flag of the United States is not defined in the constitution but rather in a separate Flag Resolution passed in 1777.\nMinor design changes of national flags are often passed on a legislative or executive level, while substantial changes have constitutional character. The design of the flag of Serbia omitting the communist star of the flag of Yugoslavia was a decision made in the 1992 Serbian constitutional referendum, but the adoption of a coat of arms within the flag was based on a government \"recommendation\" in 2003, adopted legislatively in 2009 and again subject to a minor design change in 2010. The flag of the United States underwent numerous changes because the number of stars represents the number of states, proactively defined in a Flag Act of 1818 to the effect that \"on the admission of every new state into the Union, one star be added to the union of the flag\"; it was changed most recently in 1960 with the accession of Hawaii.\nOn 17 March 1861, there was the proclamation of the Kingdom of Italy, a formal act that sanctioned, with a normative act of the Kingdom of Piedmont-Sardinia, the birth of the unified Kingdom of Italy. On 15 April 1861, the flag of the Kingdom of Piedmont-Sardinia, in the form of a green, white and red tricolour, was declared the flag of the newly formed Kingdom of Italy. The tricolour therefore continued to be the national flag also of the new State, although not officially recognised by a specific law, but regulated with regard to the shape of military banners. With the royal decree n\u00ba 2072 of 24 September 1923 and subsequently with the law n\u00ba2264 of 24 December 1925, the Italian tricolour officially became the national flag of the Kingdom of Italy. On 13 June 1946, the Italian Republic was officially founded and the last king of Italy Umberto II, who succeeded his father Victor Emmanuel III on 9 May 1946, left the country on 13 June into exile. On the same day, the tricolour with the Savoy coat of arms in the centre was lowered from the Quirinal Palace. The Italian flag was modified with the decree of the president of the Council of Ministers No. 1 of 19 June 1946. Compared to the monarchic banner, the Savoy coat of arms was eliminated. This decision was later confirmed in the session of 24 March 1947 by the Constituent Assembly, which decreed the insertion of article 12 of the Italian Constitution, subsequently ratified by the Italian Parliament.\nA change in national flag is often due to a change of regime, especially following a civil war or revolution. In such cases, the military origins of the national flag and its connection to political ideology (form of government, monarchy vs. republic vs. theocracy, etc.) remains visible. In such cases national flags acquire the status of a political symbol.\nThe flag of Germany, for instance, was a tricolour of black-white-red under the German Empire, inherited from the North German Confederation (1866). The Weimar Republic that followed adopted a black-red-gold tricolour. Nazi Germany went back to black-white-red in 1933, and black-red-gold was reinstituted by the two successor states, West Germany and East Germany, with East Germany's flag being defaced with Communist symbols, following World War II. Similarly the flag of Libya introduced with the creation of the Kingdom of Libya in 1951 was abandoned in 1969 with the coup d'\u00e9tat led by Muammar Gaddafi. It was used again by National Transitional Council and by anti-Gaddafi forces during the Libyan Civil War in 2011 and officially adopted by the Libyan interim Constitutional Declaration. \nIn Ba'athist Syria, this was replaced by the flag of the United Arab Republic with red, white and black tribands with either two or three green stars or charged with the national coat of arms.\nDuring the Syrian civil war, the Syrian opposition, represented by the Syrian National Council, then by the National Coalition of Syrian Revolutionary and Opposition Forces (commonly named the Syrian National Coalition) used the independence flag first used in 1932. The flag began to be used as a universal display of the protesting opposition in late 2011.\nFollowing the fall of the Assad regime on 8 December 2024, the revolution flag, a modified version of the independence flag, began to be used within the country by the Syrian parliament and the Syrian caretaker government, and at Syrian embassies abroad. On 13 March 2025, an interim constitution made the independence flag the primary flag. However, the final text ultimately retained the revolution flag.\nUsage.\nThere are three distinct types of national flag for use on land, and three for use at sea, though many countries use identical designs for several (and sometimes all) of these types of flag.\nOn land.\nOn land, there is a distinction between civil flags (FIAV symbol ), state flags (), and war or military flags (). Civil flags may be flown by anyone regardless of whether they are linked to government, whereas state flags are those used officially by government agencies. War flags (also called military flags) are used by military organizations such as Armies, Marine Corps, or Air Forces.\nIn practice, many countries (such as the United States and the United Kingdom) have identical flags for these three purposes; national flag is sometimes used as a vexillological term to refer to such a three-purpose flag (). In a number of countries, however, and notably those in Latin America, there is a distinction between civil and state flags. In most cases, the civil flag is a simplified version of the state flag, with the difference often being the presence of a coat of arms on the state flag that is absent from the civil flag.\nVery few countries use a war flag that differs from the state flag. Taiwan, Japan, and China are notable examples of this. Swallow-tailed flags are used as war flags and naval ensigns in Nordic countries and charged versions as presidential or royal standards. The Philippines does not have a distinctive war flag in this usual sense, but the flag of the Philippines is legally unique in that it is flown with the red stripe on top when the country is in a state of war, rather than the conventional blue.\nAt sea.\nThe flag that indicates nationality on a ship is called an ensign. As with the national flags, there are three varieties: the civil ensign (), flown by private vessels; state ensigns (also called government ensigns; ), flown by government ships; and war ensigns (also called naval ensigns; ), flown by naval vessels. The ensign is flown from an ensign-staff at the stern of the ship, or from a gaff when underway. Both these positions are superior to any other on the ship, even though the masthead is higher. In the absence of a gaff the ensign may be flown from the yardarm. (See Maritime flags.) National flags may also be flown by aircraft and the land vehicles of important officials. In the case of aircraft, those flags are usually painted on, and those are usually to be painted on in the position as if they were blowing in the wind.\nIn some countries, such as the United States and Canada (except for the Royal Canadian Navy's Ensign), the national ensign is identical to the national flag, while in others, such as the United Kingdom, India, Italy, Japan and Thailand, there are specific ensigns for maritime use. Most countries do not have a separate state ensign, although the United Kingdom is a rare exception, in having a red ensign for civil use, a white ensign as its naval ensign, and a blue ensign for government non-military vessels. Italian naval ensign bears the arms of the Italian Navy: a shield, surmounted by a turreted and rostrum crown, which brings together in four parts the arms of four ancient maritime republics (Republic of Venice, Republic of Genoa, Republic of Pisa and Republic of Amalfi).\nProtocol.\nThere is a great deal of protocol involved in the proper display of national flags. A general rule is that the national flag should be flown in the position of honour, and not in an inferior position to any other flag (although some countries make an exception for royal standards). The following rules are typical of the conventions when flags are flown on land:\nHanging a flag vertically.\nMost flags are hung vertically by rotating the flag pole. However, some countries have specific protocols for this purpose or even have special flags for vertical hanging; usually rotating some elements of the flag \u2014 such as the coat of arms \u2014 so that they are seen in an upright position.\nExamples of countries that have special protocol for vertical hanging are: Canada, Czech Republic, Greece, Israel, the Philippines, South Africa, and the United States (reverse always showing); and the United Kingdom (obverse always showing).\nExamples of countries that have special designs for vertical hanging are: Austria, Cambodia (coat of arms must be rotated 90\u00b0 and blue strips are narrowed), Dominica (coat of arms must be rotated and reverse always showing), Germany, Hungary, Liechtenstein (crown must be rotated 90\u00b0), Mexico, Montenegro (coat of arms must be rotated 90\u00b0 to normal position), Nepal, Slovakia (coat of arms must be rotated 90\u00b0 to normal position), and Saudi Arabia (shahada must be rotated 90\u00b0). A vertical banner is used instead of the horizontal flag for Malaysia.\nDesign.\nThe art and practice of designing flags is known as vexillography. The design of national flags has seen a number of customs become apparent.\nMost national flags are rectangular, or have a rectangular common variant, with the most notable exception being the flag of Nepal. The ratios of height to width vary among national flags, but none is taller than it is wide, again except for the flag of Nepal. The flags of Switzerland and the Vatican City are the only national flags which are exact squares.\nThe obverse and reverse of all national flags are either identical or mirrored, except for the flag of Paraguay and the partially recognized Sahrawi Arab Democratic Republic. See Flags whose reverse differs from the obverse for a list of exceptions including non-national flags.\nAs of 2011 all national flags consist of at least two different colours. In many cases, the different colours are presented in either horizontal or vertical bands. It is particularly common for colours to be presented in bands of three.\nIt is common for many flags to feature national symbols, such as coats of arms. National patterns are present in some flags. Variations in design within a national flag can be common in the flag's upper left quarter, or canton. A third of the world's 196 countries currently have national flags that include religious symbols. This has led to controversy in some secular states in regard to the separation of church and state, when the national symbol is officially sanctioned by a government.\nColours.\nThe most common colours in national flags are red, white, green, dark blue, yellow, light blue, and black. The only national flag not to include the colors red, white, or blue is Jamaica's. The occurrence of each colour in all the flags is listed in detail in the table below. The table shows that the colours light brown, dark brown and grey are only present in very small quantities. To be more precise these colours are currently only present in some of the symbols found within a few flags, such as in the case of the Spanish flag.\nSimilarities.\nAlthough the national flag is meant to be a unique symbol representing a nation, many countries have highly similar flags. Examples include the flags of Monaco and Indonesia, which differ only slightly in proportion and the tint of red; the flags of the Netherlands and Luxembourg, which differ in proportion as well as in the tint of blue used; the flags of Romania and Chad, which differ only in the tint of blue, and the flags of Cuba and Puerto Rico, which differ only in proportion, placement and tint of colors.\nThe flags of Ireland and Ivory Coast and the flags of Mali and Guinea are (aside from shade or ratio differences) vertically mirrored versions from each other. This means that the reverse of one flag matches the obverse of the other. Unlike horizontally mirrored flags (like Poland and Indonesia) the direction in which these flags fly is crucial to identify them.\nThere are three colour combinations that are used on several flags in certain regions. Blue, white, and red is a common combination in Slavic countries such as the Czech Republic, Slovakia, Russia, Serbia, Slovenia, and Croatia as well as among Western nations including Australia, France, Iceland, the Netherlands, New Zealand, Norway, the United Kingdom, and the United States. Many African nations use the Pan-African colours of red, yellow, and green, including Cameroon, Ethiopia, Ghana, Guinea, Mali and Senegal. Flags containing red, white, and black (a subset of the Pan-Arab colours) can be found particularly among the Arab nations such as Egypt, Iraq, Sudan and Yemen.\nDue to the common arrangement of the same colours, at first sight, it seems that the only difference between the Italian and the Mexican flag is only the coat of arms of Mexico present in the latter; in reality the Italian tricolour uses lighter shades of green and red, and has different proportions than the Mexican flag\u2014those of the Italian flag are equal to 2:3, while the proportions of the Mexican flag are 4:7. The similarity between the two flags posed a serious problem in maritime transport, given that originally the Mexican mercantile flag was devoid of arms and therefore was consequently identical to the Italian Republican tricolour of 1946; to obviate the inconvenience, at the request of the International Maritime Organization, both Italy and Mexico adopted naval flags with different crests.\nMany other similarities may be found among current national flags, particularly if inversions of colour schemes are considered, e.g., compare the flag of Senegal to that of Cameroon and Indonesia to Poland and Monaco. Also the flag of Italy and the flag of Hungary use the same colours, although the order and direction differ (the Italian flag is vertical green-white-red and the Hungarian flag is horizontal red-white-green). The same goes for the flag of France and the flag of the Netherlands (the French flag is vertical blue-white-red and the Dutch flag is horizontal red-white-blue). \nFlag families.\nWhile some similarities are coincidental, others are part of a flag family, flags rooted in shared histories. For example, the flags of Colombia, of Ecuador, and of Venezuela all use variants of the flag of Gran Colombia, the country they composed upon their independence from Spain, created by the Venezuelan independence hero Francisco de Miranda; and the flags of Kuwait, of Jordan, and of Palestine are all highly similar variants of the flag of the Arab revolt of 1916\u20131918. The flags of Romania and Moldova are virtually the same, because of the common history and heritage. Moldova adopted the Romanian flag during the declaration of independence from the USSR in 1991 (and was used in various demonstrations and revolts by the population) and later the Moldovan coat of arms (which is part of the Romanian coat of arms) was placed in the centre of the flag. All Nordic countries, with the exception of Greenland, use the Nordic Cross design (Iceland, Denmark, Norway, Sweden, Finland, in addition to the autonomous regions of the Faroe Islands and \u00c5land), a horizontal cross shifted to the left on a single-coloured background. The United States and United Kingdom both have red, white, and blue. This similarity is due to the fact that the first 13 states of the U.S. were formerly colonies of the United Kingdom. Some similarities to the United States flag with the red and white stripes are noted as well such as the flag of Malaysia and the flag of Liberia, the latter of which was an American resettlement colony. Also, several former colonies of the United Kingdom, such as Australia, Fiji and New Zealand include the Union Jack in the top left corner.\nSee also.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21805", "revid": "39039209", "url": "https://en.wikipedia.org/wiki?curid=21805", "title": "November 4", "text": "Day of the yearNovember 4 is the day of the year in the Gregorian calendar\n&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21806", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=21806", "title": "November 23", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearNovember 23 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21808", "revid": "33625371", "url": "https://en.wikipedia.org/wiki?curid=21808", "title": "NHL", "text": ""}
{"id": "21809", "revid": "589223", "url": "https://en.wikipedia.org/wiki?curid=21809", "title": "National Hockey League", "text": "North American professional ice hockey league\nThe National Hockey League (NHL; , \"LNH\") is a professional ice hockey league in North America composed of 32 teams, 25 in the United States and 7 in Canada. The NHL is one of the major professional sports leagues in the United States and Canada and is considered the premier professional ice hockey league in the world. The Stanley Cup, the oldest professional sports trophy in North America, is awarded annually to the league playoff champion at the end of each season. The International Ice Hockey Federation (IIHF) views the Stanley Cup as one of the \"most important championships available to the sport\". The NHL is headquartered in Midtown Manhattan.\nThe NHL was organized at the Windsor Hotel in Montreal on November 26, 1917, after the suspension of operations of its predecessor organization, the National Hockey Association (NHA), which had been founded in 1909 at Renfrew, Ontario. The NHL immediately took the NHA's place as one of the leagues that contested for the Stanley Cup in an annual interleague competition before a series of league mergers and foldings left the NHL as the only league competing for the Stanley Cup in 1926.\nAt its inception, the NHL had four teams, all in Canada, thus the adjective \"National\" in the league's name. The league expanded to the United States in 1924, when the Boston Bruins joined, and has since consisted of both American and Canadian teams. From 1942 to 1967, the NHL had only six teams, collectively nicknamed the \"Original Six\". The league added six new teams to double its size as a result of the 1967 NHL expansion, then increased to 18 teams by 1974, and to 21 teams due to the 1979 NHL expansion. Between 1991 and 2000, the NHL further expanded to 30 teams. It added its 31st and 32nd teams in 2017 and 2021, respectively. Salt Lake City was awarded an expansion franchise in 2024; it acquired the hockey assets of the Arizona Coyotes, which were deactivated, and established the Utah Hockey Club (now the Utah Mammoth), thus maintaining the total number of teams at 32.\nThe NHL is the fifth-highest grossing professional sports league in the world by revenue, after the National Football League (NFL), Major League Baseball (MLB), the National Basketball Association (NBA), and the Premier League. The league's headquarters have been in Manhattan since 1989, when the head office moved from Montreal. There have been four league-wide work stoppages in NHL history, all occurring after 1992. As of the \u00a0season[ [update]], the NHL had players from 17 countries.\nThe league's regular season is typically held from October to April, with each team playing 82 games. Following the conclusion of the regular season, 16 teams advance to the Stanley Cup playoffs, a four-round tournament that runs into June to determine the league champion. Since the league's founding in 1917, the Montreal Canadiens have won the most NHL titles with 25, winning three NHL championship series before the league took full exclusivity of the Stanley Cup in 1926, and 22 Stanley Cups afterwards. The reigning league champions are the Florida Panthers, who defeated the Edmonton Oilers in the 2025 Stanley Cup Final.\nHistory.\nEarly years.\nThe National Hockey League (NHL) was established in 1917 as the successor to the National Hockey Association (NHA). Founded in 1909, the NHA began play in 1910 with seven teams in Ontario and Quebec, and was one of the first major leagues in professional ice hockey. However, by its eighth season, a series of disputes with Toronto Blueshirts owner Eddie Livingstone led team owners of the Montreal Canadiens, the Montreal Wanderers, the Ottawa Senators, and the Quebec Bulldogs to hold a meeting to discuss the league's future. Realizing the NHA constitution left them unable to force Livingstone out, the four teams voted instead to suspend the NHA, and, on November 26, 1917, formed the National Hockey League. Frank Calder was chosen as the NHL's first president, serving until his death in 1943.\nThe Bulldogs were unable to play in the NHL, and the remaining owners founded the Toronto Arenas to compete with the Canadiens, Wanderers and Senators. The first games were played on December 19, 1917. The Montreal Arena burned down in January 1918, causing the Wanderers to cease operations, and the NHL continued on as a three-team league until the Bulldogs returned in 1919.\nThe NHL replaced the NHA as one of the leagues that competed for the Stanley Cup, an interleague competition at the time. Toronto won the first NHL title, and then defeated the Vancouver Millionaires of the Pacific Coast Hockey Association (PCHA) for the 1918 Stanley Cup. The Canadiens won the league title in 1919, but the series in the Stanley Cup Final against the PCHA's Seattle Metropolitans was abandoned due to the Spanish Flu epidemic. In 1924, Montreal won their first Stanley Cup as a member of the NHL. The Hamilton Tigers won the regular season title in 1924\u201325, but refused to play in the championship series unless they were given a C$200 bonus. The league refused and declared the Canadiens the league champion after they defeated the Toronto St. Patricks (formerly the Arenas) in the two-game, total-goals NHL championship series. Montreal was then defeated by the Victoria Cougars of the Western Canada Hockey League (WCHL) in 1925. It was the last time a non-NHL team won the trophy, as the Stanley Cup became the \"de facto\" NHL championship in 1926, after the WCHL ceased operation.\nThe NHL embarked on a rapid expansion in the 1920s, adding the Montreal Maroons and the Boston Bruins in 1924, the latter being the first American team to join the league. The New York Americans began play in 1925 after purchasing the assets of the Hamilton Tigers, and they were joined by the Pittsburgh Pirates. The New York Rangers were added in 1926, and the Chicago Black Hawks (later changed to Blackhawks) and Detroit Cougars (later known as the Red Wings) were added after the league purchased the assets of the defunct WCHL. A group purchased the Toronto St. Patricks in 1927 and renamed them the Toronto Maple Leafs.\nIn 1926, Native American Taffy Abel became the first non-white player in the NHL and broke the league's colour barrier by playing for the New York Rangers.\nIn 1934, the first NHL All-Star Game was held, to benefit Ace Bailey, whose career ended on a vicious hit by Eddie Shore. The second was held in 1937, in support of Howie Morenz's family when he died of a coronary embolism after breaking his leg during a game.\nOriginal Six era.\nThe Great Depression and the onset of World War II took a toll on the league. The Pirates became the Philadelphia Quakers in 1930, then folded a year later. The Senators likewise became the St. Louis Eagles in 1934, also lasting only a year. The Maroons did not survive, as they suspended operations in 1938. The Americans were suspended in 1942 due to a lack of available players, and they were never reactivated.\nFor the 1942\u201343 season, the NHL was reduced to six teams: the Boston Bruins, the Chicago Black Hawks, the Detroit Red Wings, the Montreal Canadiens, the New York Rangers, and the Toronto Maple Leafs, a line-up, often referred to as the \"Original Six\", that would remain constant for the next 25 years. In 1947, the league reached an agreement with the Stanley Cup trustees to take full control of the trophy, allowing it to reject challenges from other leagues that wished to play for the Cup.\nIn 1945, Maurice \"Rocket\" Richard became the first player to score 50 goals, doing so in a 50-game season. Richard later led the Canadiens to five consecutive titles between 1956 and 1960, a record no team has matched.\nIn 1948, Asian Canadian Larry Kwong became the first Asian player in the NHL by playing for the New York Rangers. In 1958, Willie O'Ree became the first black player in the league's history when he made his debut with the Boston Bruins.\nExpansion era.\nBy the mid-1960s, the desire for a network television contract in the United States, coupled with concerns that the Western Hockey League was planning to declare itself a major league and challenge for the Stanley Cup, spurred the NHL to undertake its first expansion since the 1920s. The league doubled in size to 12 teams for the 1967\u201368 season, adding the Los Angeles Kings, the Minnesota North Stars, the Philadelphia Flyers, the Pittsburgh Penguins, the California Seals, and the St. Louis Blues. However, Canadian fans were outraged that all six teams were placed in the United States, so the league responded by adding the Vancouver Canucks in 1970, along with the Buffalo Sabres, both located along the Canada\u2013United States border. Two years later, the emergence of the newly founded World Hockey Association (WHA) led the league to add the New York Islanders and the Atlanta Flames to keep the rival league out of those markets. In 1974, the Washington Capitals and the Kansas City Scouts were added, bringing the league up to 18 teams.\nThe NHL fought the WHA for players, losing 67 to the new league in its first season of 1972\u201373, including the Chicago Black Hawks' Bobby Hull, who signed a 10-year, $2.5\u00a0million contract with the Winnipeg Jets, then the largest in hockey history. The league attempted to block the defections in court, but a counter-suit by the WHA led to a Philadelphia judge ruling the NHL's reserve clause to be illegal, thus eliminating the elder league's monopoly over the players. Wayne Gretzky played one season in the WHA for the Indianapolis Racers (eight games) and the Edmonton Oilers (72 games) before the Oilers joined the NHL for the 1979\u201380 season. Gretzky went on to lead the Oilers to win four Stanley Cup championships in 1984, 1985, 1987 and 1988, and set single-season records for goals (92 in 1981\u201382), assists (163 in 1985\u201386) and points (215 in 1985\u201386), as well as career records for goals (894), assists (1,963) and points (2,857). In 1988, he was traded to the Los Angeles Kings in a deal that dramatically improved the league's popularity in the United States. By the turn of the century, nine more teams were added to the NHL: the San Jose Sharks, the Tampa Bay Lightning, the Ottawa Senators, the Mighty Ducks of Anaheim, the Florida Panthers, the Nashville Predators, the Atlanta Thrashers, and, in 2000, the Minnesota Wild and the Columbus Blue Jackets. Also, in the mid to late 1990s, the Quebec Nordiques, original Winnipeg Jets, Hartford Whalers, and Minnesota North Stars relocated to Denver, Phoenix, Raleigh, and Dallas, respectively. In 2011, the Atlanta Thrashers relocated to Winnipeg, and the Winnipeg Jets were revived. On July 21, 2015, the NHL confirmed that it had received applications from prospective ownership groups in Quebec City and Las Vegas for possible expansion teams, and on June 22, 2016, NHL Commissioner Gary Bettman announced the addition of a 31st franchise, based in Las Vegas and later named the Vegas Golden Knights, into the NHL for the 2017\u201318 season. On December 4, 2018, the league announced a 32nd franchise in Seattle, later named the Seattle Kraken, which joined in the 2021\u201322 season. On April 18, 2024, the Arizona Coyotes suspended operations and sold their hockey assets, including players and other personnel, to a new team in Salt Lake City, Utah. Two months after Utah's foundation, the Coyotes ceased their efforts to re-activate within the five-year window granted to do so, bringing the NHL back to 32 franchises.\nLabour issues.\nThere have been four league-wide work stoppages in NHL history, all occurring after 1992. The first was an April 1992 strike by the National Hockey League Players' Association, which lasted for ten days but was settled quickly with all affected games rescheduled.\nA lockout at the start of the 1994\u201395 season forced the league to reduce the schedule from 84 games to 48, with the teams playing only intra-conference games during the reduced season. The resulting collective bargaining agreement (CBA) was set for renegotiation in 1998, and extended to September 15, 2004.\nWith no new agreement in hand when the contract expired, league commissioner Gary Bettman announced a lockout of the players union and closed the league's head office for the 2004\u201305 season. The league vowed to install what it dubbed \"cost certainty\" for its teams, but the Players' Association countered that the move was little more than a euphemism for a salary cap, which the union initially said it would not accept. The lockout shut down the league for 310 days, making it the longest in sports history, as the NHL became the first professional sports league to lose an entire season. A new collective bargaining agreement was eventually ratified in July 2005, including a salary cap. The agreement had a term of six years with an option of extending the collective bargaining agreement for an additional year at the end of the term, allowing the league to resume as of the 2005\u201306 season.\nOn October 5, 2005, the first post-lockout season took to the ice with all 30 teams. The NHL received record attendance in the 2005\u201306 season, with an average of 16,955 per game. However, its television audience was slower to rebound due to American cable broadcaster ESPN's decision to drop its NHL coverage. The league's post-lockout agreement with NBC gave the league a share of revenue from each game's advertising sales, rather than the usual lump sum paid up front for game rights. The league's annual revenues were estimated at $2.27\u00a0billion.\nOn September 16, 2012, the labour pact expired, and the league again locked out the players. The owners proposed reducing the players' share of hockey-related revenues from 57 percent to 47 percent. All games were cancelled up to January 14, 2013, along with the 2013 NHL Winter Classic and the 2013 NHL All-Star Weekend. On January 6, a tentative agreement was reached on a 10-year deal. On January 12, the league and the Players' Association signed a memorandum of understanding on the new deal, allowing teams to begin their training camps the next day, with a shortened 48-game season schedule that began on January 19.\nPlayer safety issues.\nPlayer safety has become a major issue in the NHL, with concussions resulting from a hard hit to the head being the primary concern. Recent studies have shown how the consequences of concussions can last beyond player retirement. This has significant effects on the league, as elite players have suffered from the aftereffects of concussions (such as Sidney Crosby being sidelined for approximately ten and a half months), which adversely affects the league's marketability. In December 2009, Brendan Shanahan was hired to replace Colin Campbell, and was given the role of senior vice-president of player safety. Shanahan began to hand out suspensions on high-profile perpetrators responsible for dangerous hits, such as Raffi Torres receiving 25 games for his hit on Marian Hossa.\nTo aid with removing high-speed collisions on icing, which had led to several potential career-ending injuries, such as to Hurricanes' defenceman Joni Pitkanen, the league mandated hybrid no-touch icing for the 2013\u201314 NHL season.\nOn November 25, 2013, ten former NHL players (Gary Leeman, Rick Vaive, Brad Aitken, Darren Banks, Curt Bennett, Richie Dunn, Warren Holmes, Bob Manno, Blair Stewart, and Morris Titanic) sued the league for negligence in protecting players from concussions. The suit came three months after the National Football League agreed to pay former players US$765\u00a0million due to a player safety lawsuit.\nWomen in the NHL.\nFrom 1952 to 1955, Marguerite Norris served as president of the Detroit Red Wings, being the first female NHL executive and the first woman to have her name engraved on the Stanley Cup. In 1992, Manon Rh\u00e9aume became the first woman to play a game in any of the major professional North American sports leagues, as a goaltender for the Tampa Bay Lightning in a preseason game against the St. Louis Blues, stopping seven of nine shots. In 2016, Dawn Braid was hired as the Arizona Coyotes' skating coach, making her the first female full-time coach in the NHL. The first female referees in the NHL were hired in a test-run during the league's preseason prospect tournaments in September 2019.\nIn 2016, the NHL hosted the 2016 Outdoor Women's Classic, an exhibition game between the Boston Pride of the National Women's Hockey League and Les Canadiennes of the Canadian Women's Hockey League, as part of the 2016 NHL Winter Classic weekend festivities. In 2019, the NHL invited four women from the US and Canadian Olympic teams to demonstrate the events in All-Star skills competition before the All-Star Game. Due to Nathan MacKinnon choosing not to participate following a bruised ankle, Team USA's Kendall Coyne Schofield competed in the Fastest Skater competition in his place, becoming the first woman to officially compete in the NHL's All-Star festivities. The attention led the NHL to include a 3-on-3 women's game before the 2020 All-Star Game. Rheaume returned to perform as a goaltender for the 2022 NHL All-Star Game's Breakaway Challenge.\nTeams.\nBruins \nSabres \nRed&lt;br&gt;Wings \nPanthers \nCanadiens \nSenators \nLightning \nMaple&lt;br&gt;Leafs \nHurricanes \nBlue&lt;br&gt;Jackets \nDevils \nIslanders \nRangers \nFlyers \nPenguins \nCapitals \nBlackhawks \nAvalanche \nStars \nWild \nPredators \nBlues \nMammoth \nJets \nDucks \nFlames \nOilers \nKings \nSharks \nKraken \nCanucks \nGolden Knights \nThe NHL consists of 32 teams\u201425 based in the United States and 7 in Canada. The teams are divided evenly between the Eastern and Western conferences. Each conference is split into two divisions, with 16 teams per conference and 8 per division. The Eastern Conference consists of the Atlantic and Metropolitan divisions, while the Western Conference consists of the Central and Pacific divisions.\nThe number of teams held constant at 30 teams from the 2000\u201301 season, when the Minnesota Wild and the Columbus Blue Jackets joined the league as expansion teams, until 2017. That expansion capped a period in the 1990s of rapid expansion and relocation, when the NHL added nine teams to grow from 21 to 30 teams, and relocated four teams mostly from smaller, northern cities to larger, more southern metropolitan areas (Minneapolis\u2013Saint Paul to Dallas, Quebec City to Denver, Winnipeg to Phoenix, and Hartford to Raleigh). The league has not contracted any teams since the Cleveland Barons were merged into the Minnesota North Stars in 1978. The league expanded for the first time in 17 years to 31 teams with the addition of the Vegas Golden Knights in 2017, then to 32 with the addition of the Seattle Kraken in 2021. In April 2024, a new expansion team in Utah was created, after Alex Meruelo sold the hockey assets of the Arizona Coyotes to Ryan Smith, owner of the Utah Jazz. Meruelo was granted until 2029 to secure an arena in Arizona in order to re-activate the team, bringing the total number of franchises up to 33; however, these efforts were abandoned two months later, leaving the NHL at 32 franchises once again.\nAccording to \"Forbes\", in 2024, the top five most valuable teams were four of the \"Original Six\" teams and the Los Angeles Kings:\nThe remaining members of the Original Six, the Chicago Blackhawks and the Detroit Red Wings, respectively ranked seventh at US$2.45 billion and 10th at US$2.125 billion. In 2023, the Maple Leafs surpassed the Rangers as the most valuable NHL team, and Los Angeles overtook both Chicago and Boston, making its way into the top five.\nList of teams.\nNotes:\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nOrganizational structure.\nBoard of Governors.\nThe Board of Governors is the ruling and governing body of the NHL. In this context, each team is a member of the league, and each member appoints a Governor (usually the owner of the club), and two alternates to the Board. The current chairman of the Board is Boston Bruins owner Jeremy Jacobs. The Board of Governors exists to establish the policies of the league and to uphold its constitution. Some of the responsibilities of the Board of Governors include:\nThe Board of Governors meets twice per year, in the months of June and December, with the exact date and place to be fixed by the Commissioner.\nExecutives.\nThe chief executive of the league is commissioner Gary Bettman. Some other senior executives include deputy commissioner and chief legal officer Bill Daly, director of hockey operations Colin Campbell, and senior vice president of player safety George Parros. A committee led by Bettman and chairman Jeremy Jacobs is responsible for vetting new ownership applications, collective bargaining, and league expansion. Other members include Mark Chipman, N. Murray Edwards, Craig Leipold, Ted Leonsis, Geoff Molson, Henry Samueli, Larry Tanenbaum, Jeff Vinik, and David Blitzer.\nRule differences with international hockey.\nThe NHL's rules are one of the two standard sets of professional ice hockey rules in the world, the other being the rules of the International Ice Hockey Federation (IIHF), as used in tournaments such as the Olympics. The IIHF rules are derived from the Canadian amateur ice hockey rules of the early 20th century, while the NHL rules evolved directly from the first organized indoor ice hockey game in Montreal in 1875, updated by subsequent leagues up to 1917, when the NHL adopted the existing NHA set of rules. The NHL's rules are the basis for rules governing most professional and major junior ice hockey leagues in North America.\nThe NHL hockey rink is , approximately the same length but much narrower than IIHF standards. A trapezoidal area appears behind each goal net. The goaltender can play the puck only within the trapezoid or in front of the goal line; if the goaltender plays the puck behind the goal line and outside the trapezoidal area, a two-minute minor penalty for delay of game is assessed. The rule is unofficially nicknamed the \"Martin Brodeur rule\"; Brodeur at the time was one of the best goaltenders at getting behind the net to handle the puck. Since the 2013\u201314 season, the league trimmed the goal frames by on each side and reduced the size of the goalies' leg pads.\nThe league has regularly modified its rules to counter perceived imperfections in the game. The penalty shot was adopted from the Pacific Coast Hockey Association to ensure players were not being blocked from opportunities to score. For the 2005\u201306 season, the league changed some of the rules regarding being offside. First, the league removed the \"offside pass\" or \"two-line pass\" rule, which required a stoppage in play if a pass originating from inside a team's defending zone was completed on the offensive side of the centre line, unless the puck crossed the line before the player. Furthermore, the league reinstated the \"tag-up offside\" which allows an attacking player a chance to get back onside by returning to the neutral zone. The changes to the offside rule were among several rule changes intended to increase overall scoring, which had been in decline since the expansion years of the mid-nineties and the increased prevalence of the neutral zone trap. Since 2005, when a team is guilty of icing the puck they are not allowed to make a line change or skater substitution of any sort before the following face-off (except to replace an injured player or re-install a pulled goaltender). Since 2013, the league has used \"hybrid icing\", where a linesman stops play due to icing if a defending player (other than the goaltender) crosses the imaginary line that connects the two face-off dots in their defensive zone before an attacking player is able to. This was done to counter a trend of player injury in races to the puck.\nFighting in the NHL leads to \"major penalties\" while IIHF rules, and most amateur rules, call for the ejection of fighting players. Usually, a penalized team cannot replace a player that is penalized on the ice and is thus short-handed for the duration of the penalty, but if the penalties are coincidental, for example when two players fight, both teams remain at full strength. Also, unlike minor penalties, major penalties must be served to their full completion, regardless of number of goals scored during the power play.\nThe league also imposes a conduct policy on its players. Players are banned from gambling and criminal activities have led to the suspension of players. The league and the Players' Association agreed to a stringent anti-doping policy in the 2005 collective bargaining agreement. The policy provides for a twenty-game suspension for a first positive test, a sixty-game suspension for a second positive test, and a lifetime suspension for a third positive test.\nAt the end of regulation time, the team with the most goals wins the game. If a game is tied after regulation time, overtime ensues. During the regular season, overtime is a five-minute, three-on-three sudden-death period, in which whoever scores a goal first wins the game. If the game is still tied at the end of overtime, the game enters a shootout. Three players for each team in turn take a penalty shot. The team with the most goals during the three-round shootout wins the game. If the game is still tied after the three shootout rounds, the shootout continues but becomes sudden-death. Whichever team ultimately wins the shootout is awarded a goal in the game score and thus awarded two points in the standings. The losing team in overtime or shootout is awarded one point. Shootout goals and saves are not tracked in hockey statistics; shootout statistics are tracked separately.\nThere are no shootouts during the playoffs. Instead, multiple sudden-death, 20-minute five-on-five periods are played until one team scores. Two games have reached six overtime periods, but none have gone beyond six. During playoff overtime periods, the only break is to clean the loose ice at the first stoppage after the period is halfway finished.\nSeason structure.\nThe National Hockey League season is divided into a preseason (September and early October), a regular season (from early October through early to mid-April) and a postseason (the Stanley Cup playoffs) that runs until June.\nTeams usually hold a summer showcase for prospects in July and participate in prospect tournaments, full games that do not feature any veterans, in September. Full training camps begin in mid-to-late September, including a preseason consisting of six to eight exhibition games. Split squad games, in which parts of a team's regular season roster play separate games on the same day, are occasionally played during the preseason.\nDuring the regular season, clubs play each other in a predefined schedule. Since 2021, in the regular season, all teams play 82 games: 41 games each of home and road, playing 26 games in their own geographic division\u2014four against five of their seven other divisional opponents, plus three against two others; 24 games against the eight remaining non-divisional intra-conference opponents\u2014three games against every team in the other division of its conference; and 32 against every team in the other conference twice\u2014home and road.\nThe league's regular season standings are based on a point system. Two points are awarded for a win, one point for losing in overtime or a shootout, and zero points for a loss in regulation. At the end of the regular season, the team that finishes with the most points in each division is crowned the division champion, and the league's overall leader is awarded the Presidents' Trophy.\nThe Stanley Cup playoffs, which go from April to the beginning of June, are an elimination tournament where two teams play against each other to win a best-of-seven series in order to advance to the next round. The final remaining team is crowned the Stanley Cup champion. Eight teams from each conference qualify for the playoffs: the top three teams in each division plus the two conference teams with the next highest number of points. The two conference champions proceed to the Stanley Cup Final. In all rounds, the higher-ranked team is awarded home-ice advantage, with four of the seven games played at this team's home venue. In the Stanley Cup Final, the team with the most points during the regular season has home-ice advantage.\nEntry draft.\nThe annual NHL entry draft consists of a seven-round off-season draft held in June on a date of the commissioner's choosing. Early NHL drafts took place at the Queen Elizabeth (currently Fairmont) Hotel in Montreal. Amateur players from junior, collegiate, or European leagues are eligible to enter the entry draft. The selection order is determined by a combination of the standings at the end of the regular season, playoff results, and a draft lottery. The 16 teams that did not qualify for the playoffs are entered in a weighted lottery to determine the initial draft picks in the first round, with the last place team having the best chance of winning the lottery. Once the lottery determines the initial draft picks, the order for the remaining non-playoff teams is determined by the standings at the end of the regular season. For those teams that did qualify for the playoffs, the draft order is then determined by total regular season points for non-division winners that are eliminated in the first two rounds of the playoffs, then any division winners that failed to reach the Conference Finals. Conference finalists receive the 29th and 30th picks depending on total points, with the Stanley Cup runner-up given the 31st pick and the Stanley Cup champions the final pick.\nTrophies and awards.\nTeams.\nThe most prestigious team award is the Stanley Cup, which is awarded to the league champion at the end of the Stanley Cup playoffs. The team that has the most points in the regular season is awarded the Presidents' Trophy.\nThe Montreal Canadiens are the most successful franchise in the league. Since the formation of the league in 1917, they have 25 NHL championships (three between 1917 and 1925 when the Stanley Cup was still contested in an interleague competition, twenty-two since 1926 after the Stanley Cup became the NHL's championship trophy). They also lead all teams with 24 Stanley Cup championships (one as an NHA team, twenty-three as an NHL team). Of the four major professional sports leagues in North America, the Montreal Canadiens are surpassed in the number of championships only by the New York Yankees of Major League Baseball, who have three more.\nThe longest streak of winning the Stanley Cup in consecutive years is five, held by the Montreal Canadiens from 1955\u201356 to 1959\u201360. The 1977 edition of the Montreal Canadiens, the second of four straight Stanley Cup champions, was named by ESPN as the second greatest sports team of all time.\nThe next most successful NHL franchise is the Toronto Maple Leafs with 13 Stanley Cup championships, most recently in 1967. The Detroit Red Wings, with 11 Stanley Cup championships, are the most successful American franchise.\nThe same trophy is reused every year for each of its awards. The Stanley Cup, much like its counterpart in the Canadian Football League (CFL), is unique in this aspect, as opposed to the Vince Lombardi Trophy, Larry O'Brien Trophy, and Commissioner's Trophy, which have new ones made every year for that year's champion. Despite only one trophy being used, the names of the teams winning and the players are engraved every year on the Stanley Cup. The same can also be said for the other trophies reissued every year.\nDivision titles.\nApart from the NHL-sanctioned trophies, which teams often recognize by putting up banners in the rafters of their arenas, many teams also claim titles which are not represented by trophies, often also by putting up banners in their rafters. One example is the division title or division championship. The term unambiguously refers to the team that received the most points in its division at the end of the regular season, but in some previous seasons, for example, from 1926\u201327 to 1927\u201328 and from 1981\u201382 to 1992\u201393, when the playoffs where organized along divisions, the term without qualification could also refer to the team which won the corresponding playoff series. The NHL has made clear in the past that it only allows teams to recognize regular season division titles.\nPlayers.\nThere are numerous trophies that are awarded to players based on their statistics during the regular season; they include, among others, the Art Ross Trophy for the league scoring champion (goals and assists), the Maurice \"Rocket\" Richard Trophy for the goal-scoring leader, and the William M. Jennings Trophy for the goaltender(s) for the team with the fewest goals against them.\nThe other player trophies are voted on by the Professional Hockey Writers' Association or the team general managers. These individual awards are presented at a formal ceremony held in late June after the playoffs have concluded. The most prestigious individual award is the Hart Memorial Trophy which is awarded annually to the Most Valuable Player; the voting is conducted by members of the Professional Hockey Writers Association to judge the player who is the most valuable to his team during the regular season. The Vezina Trophy is awarded annually to the person deemed the best goaltender as voted on by the general managers of the teams in the NHL. The James Norris Memorial Trophy is awarded annually to the National Hockey League's top defenceman, the Calder Memorial Trophy is awarded annually to the top rookie, and the Lady Byng Memorial Trophy is awarded to the player deemed to combine the highest degree of skill and sportsmanship; all three of these awards are voted on by members of the Professional Hockey Writers Association.\nIn addition to the regular season awards, the Conn Smythe Trophy is awarded annually to the most valuable player during the NHL's Stanley Cup playoffs. Furthermore, the top coach in the league wins the Jack Adams Award, as selected by a poll of the National Hockey League Broadcasters Association. The National Hockey League publishes the names of the top three vote getters for all awards, and then names the award winner during the NHL Awards Ceremony.\nPlayers, coaches, officials, and team builders who have had notable careers are eligible to be voted into the Hockey Hall of Fame. Players cannot enter until three years have passed since their last professional game, currently tied with the Naismith Memorial Basketball Hall of Fame for the shortest such time period of any major sport. One unique consequence has been Hall of Fame members (specifically, Gordie Howe, Guy Lafleur, and Mario Lemieux) coming out of retirement to play once more. If a player was deemed significant enough, the three-year wait would be waived; only ten individuals have been honoured in this manner. In 1999, Wayne Gretzky joined the Hall and became the last player to have the three-year restriction waived. After his induction, the Hall of Fame announced that Gretzky would be the last to have the waiting period waived.\nOrigin of players.\nIn addition to Canadian- and American-born and trained players, who have historically composed a large majority of NHL rosters, the NHL also draws players from an expanding pool of other nations where organized and professional hockey is played. Since the collapse of the Soviet Bloc, political/ideological restrictions on the movement of hockey players from this region have disappeared, leading to a large influx of players mostly from the Czech Republic, Slovakia, and Russia into the NHL. Swedes, Finns, and Western European players, who were always free to move to North America, came to the league in greater numbers than before.\nMany of the league's top players in recent years have come from these European countries including Daniel Alfredsson, Erik Karlsson, Henrik Sedin, Daniel Sedin, Henrik Lundqvist, Jaromir Jagr, Patrik Elias, Nikita Kucherov, Zdeno Chara, Pavel Datsyuk, Evgeni Malkin, Nicklas Lidstrom, and Alexander Ovechkin. European players were drafted and signed by NHL teams in an effort to bring in more \"skilled offensive players\", although as of 2008 there has been a decline in European players as more American players enter the league. The addition of European players changed the style of play in the NHL and European style hockey has been integrated into the NHL game.\nAs of the 2017\u201318 season, the NHL has players from 17 countries, with 46.0% coming from Canada and 26.0% from the United States, while players from a further 15 countries make up 26.4% of NHL rosters. The following table shows the seven countries that make up the vast majority of NHL players. The table follows the Hockey Hall of Fame convention of classifying players by the currently existing countries in which their birthplaces are located, without regard to their citizenship or where they were trained.\nCorporate sponsors.\nThe NHL lists its several official corporate partners into three categories: North American Partners, USA Partners and Canada Partners. Discover Card is the league's official credit card in the United States, while competitor Visa is an official sponsor in Canada. Likewise, Tim Hortons is the league's official coffee and doughnuts chain in Canada, while Dunkin' Donuts is the NHL's sponsor in the United States.\nAmong its North American corporate sponsors, Kraft Heinz sponsors \"Kraft Hockeyville\", an annual competition in which communities compete to demonstrate their commitment to the sport of ice hockey. The winning community gets a cash prize dedicated to upgrading their local home arena, as well as the opportunity to host an NHL pre-season game. Two contests are held, one for communities across Canada and a separate competition for communities in the US.\nAt least two of the North American corporate sponsors have ties to NHL franchise owners: the Molson family, founders of Molson Brewery, has owned the Montreal Canadiens for years, while SAP was co-founded by Hasso Plattner, the current majority owner of the San Jose Sharks.\nMany of these same corporate partners become the title sponsors for the league's All-Star and outdoor games.\nBeginning in the 2020\u201321 NHL season, the league allowed for advertising on its gameday uniforms for the first time, starting with helmet ads. The NHL has had advertising on the front of team jerseys starting from the 2022\u201323 season.\nOn May 14, 2021, NHL and the sports-betting company Betway announced a multi-year partnership in which Betway became the official sports betting partner to the NHL in North America.\nMedia coverage.\nCanada.\nBroadcasting rights in Canada have historically included the CBC's \"Hockey Night in Canada\" (\"HNIC\"), a Canadian tradition dating to 1952, and even prior to that on radio since the 1920s.\nThe current national television and digital rightsholder is Rogers Communications, under a 12-year deal valued at C$5.2\u00a0billion which began in the 2014\u201315 season, as the national broadcast and cable television rightsholders. National English-language coverage of the NHL is carried primarily by Rogers' Sportsnet group of specialty channels; Sportsnet holds national windows on Wednesday and Sunday nights. \"Hockey Night in Canada\" was maintained and expanded under the deal, airing up to seven games nationally on Saturday nights throughout the regular season. CBC maintains Rogers-produced NHL coverage during the regular season and playoffs. Sportsnet's networks also air occasional games involving all-U.S. matchups.\nQuebecor Media holds national French-language rights to the NHL, with all coverage airing on its specialty channel TVA Sports.\nGames that are not broadcast as part of the national rights deal are broadcast by Sportsnet's regional feeds, TSN's regional feeds, and RDS. Regional games are subject to blackout for viewers outside of each team's designated market.\nUnited States.\nHistorically, the NHL has never fared well on American television in comparison to the other American professional leagues. The league's American broadcast partners had been in flux for decades prior to 1995. Hockey broadcasting on a national scale was particularly spotty prior to 1981; NBC, CBS, and ABC held rights at various times during that period but with limited schedules during the second half of the regular season and the playoffs, along with some (but not all) of the Stanley Cup Final. The NHL primarily was then only available on cable television after 1981, airing on the USA Network, SportsChannel America, and ESPN at various times. Since 1995, national coverage has been split between broadcast and cable, first with Fox and ESPN from 1995 to 1999, then followed by ABC and ESPN from 1999 to 2004. The U.S. national rights were then held by NBC and OLN (later renamed Versus, then NBCSN) between the 2004\u201305 NHL lockout and 2021.\nThe 2021\u201322 season marks the first year of seven-year agreements with ESPN and TNT (formerly Turner) Sports. ESPN's deal includes 25 regular season games on ABC or ESPN, and 75 exclusive games streamed on ESPN+ and Hulu. Turner Sports' coverage includes up to 72 regular season games on TNT, with early round playoff coverage split between TNT and TBS. The playoffs will be split between ESPN and TNT, with ABC televising the Stanley Cup Final during even years and TNT (simulcast with TBS and TruTV) televising the championship series during odd years.\nAs in Canada, games not broadcast nationally are aired regionally within a team's home market and are subject to blackout outside of them. These broadcasters include regional sports network chains. Certain national telecasts are non-exclusive, and may also air in tandem with telecasts of the game by local broadcasters. However, national telecasts of these games are blacked out in the participating teams' markets to protect the local broadcaster.\nNHL Network.\nThe league co-owns the NHL Network, a television specialty channel devoted to the NHL. Its signature show is \"NHL Tonight\". The NHL Network also airs live games, but primarily simulcasts of one of the team's regional broadcasters.\nOut-of-market packages.\nNHL Centre Ice in Canada and NHL Center Ice in the United States are the league's subscription-based, out-of-market sports packages that offer access to out-of-market feeds of games through a cable or satellite television provider.\nThe league originally launched \"NHL GameCenter Live\" in 2008, allowing the streaming of out-of-market games over the internet. MLB Advanced Media then took over of its day-to-day operations in 2016, renaming it \"NHL.tv\". Under its contract, Rogers Communications distributes the service in Canada as \"NHL Live\"; it will be incorporated into Sportsnet Now Premium for the 2022\u201323 season. Under ESPN's contract, the league's out-of-market streaming package was incorporated into ESPN+ for those viewers in the United States in 2021.\nInternational.\nOutside of Canada and the United States, NHL games are broadcast across Europe, in the Middle East, in Australia, and in the Americas across Mexico, Central America, Dominican Republic, Caribbean, South America and Brazil, among others.\n\"NHL.tv\" is also available for people in most countries to watch games online, but blackout restrictions may still apply if a game is being televised in the user's country. For those in selected international markets where ESPN also holds the streaming rights, they must instead access games on the ESPN platform used in that particular country: ESPNPlayer, ESPN Play, the ESPN App, or Disney+ (previously Star+). And those in Denmark, Estonia, Finland, Iceland, Latvia, Lithuania, Norway, Poland, Sweden and the United Kingdom must use Viaplay.\nInternational competitions.\nClub participation.\nNHL teams have occasionally participated in international club competitions. Most of these competitions were arranged by the NHL or NHLPA. The first international club competition was held in 1976, with eight NHL teams playing against the Soviet Championship League's HC CSKA Moscow, and Krylya Sovetov Moscow. Between 1976 and 1991, the NHL, and the Soviet Championship League would hold several exhibition games between the two leagues known as the Super Series. No NHL club had played a Soviet or Russian-based club from the end of the Super Series in 1991 to 2008 when the New York Rangers faced Metallurg Magnitogorsk in the 2008 Victoria Cup.\nIn addition to the Russian clubs, NHL clubs had participated in several international club exhibitions and competitions with various European-based clubs. The first exhibition game to feature an NHL team against a European-based team (aside from clubs based in the former Soviet Union) was in December 1977, when the New York Rangers faced Poldi Kladno of the Czechoslovak First Ice Hockey League. In the 2000s, the NHL organized four NHL Challenge series between NHL and European clubs. The NHL continued to organize exhibition games between NHL and European teams before the beginning of the NHL season; those games were known as the NHL Premiere from 2007 to 2011 and as the NHL Global Series since 2017. The last exhibition game between an NHL and European club occurred during the 2024 NHL Global Series. \nNHL clubs have also participated in IIHF-organized club tournaments. The most recent IIHF-organized event including an NHL club was the 2009 Victoria Cup, between the Swiss National League A's ZSC Lions and the Chicago Blackhawks.\nPermittance of NHL players in international competitions.\nThe NHL has also permitted its players to participate in international competitions among national teams. The annual Ice Hockey World Championships is held every May, at the same time as the Stanley Cup playoffs. Because of its timing, NHL players generally only join their respective country's team in the World Championships if their respective NHL team has been eliminated from Stanley Cup contention. \nFrom 1998 to 2014, during the year of the quadrennial Winter Olympics, the NHL suspended its all-star game and expanded the traditional all-star break to allow NHL players to participate in the Olympic ice hockey tournament. In 2018, the NHL did not schedule an Olympic break, resulting in their players not participating in that year's Olympic tournament. An Olympic break was also not scheduled in 2022, with the NHL opting to not permit its players to participate due to a shortened NHL season that year, and concerns about the COVID-19 pandemic. The NHL, NHLPA, and IIHF have agreed to permit NHL players participate in the 2026 and 2030 Winter Olympics. The NHL and the NHLPA also organize the World Cup of Hockey. Unlike the Ice Hockey World Championships and the Olympic tournament, the World Cup of Hockey is played under NHL rules and not those of the IIHF.\nIn 2007, the International Ice Hockey Federation (IIHF) formalized the \"Triple Gold Club\", the group of players and coaches who have won an Olympic gold medal, a World Championship gold medal and the Stanley Cup. The term had first entered popular use following the 2002 Winter Olympics, which saw the addition of the first Canadian members.\nPopularity.\nThe NHL is considered one of the four major professional sports leagues in North America, along with Major League Baseball, the National Football League, and the National Basketball Association. The league is very prominent in Canada, where it is the most popular of these four leagues. Overall, hockey has the smallest total fan base of the four leagues and receives the smallest annual revenue; the league earns the least from the television rights sale and has the lowest sponsorship.\nThe NHL had been the sport holding the most affluent fan base of the top four, but it slid behind the MLB and leveled off with the NFL in recent years. A study done by the Stanford Graduate School of Business in 2004, found that NHL fans in the United States were the most educated of the four major leagues. Further, it noted that season-ticket sales were more prominent in the NHL than the other three because of the financial ability of the NHL fan to purchase them. The NHL has the most white-based audience among the four. According to Reuters, in 2010, the largest demographic of NHL fans was males aged 18\u201334.\nThe NHL estimates that half of its fan base roots for teams in outside markets. So, beginning in 2008, the NHL started to shift toward using digital technology to market to fans to capitalize on this.\nThe debut of the Winter Classic, an outdoor regular season NHL game held on New Year's Day in 2008, was a significant success for the league. The game has since become an annual staple of the NHL schedule. Coverage of \"Hockey Day in America\", later rebranded as Hockey Weekend Across America with TNT, allowed for multiple games to be broadcast in the United States on the national rights holder. These improvements led NBC and the cable channel Versus to sign a 10-year broadcast deal, paying US$200 million per year for both American cable and broadcast rights; the deal will lead to further increases in television coverage on the NBC channels.\nThis television contract has boosted viewership metrics for the NHL. The 2010 Stanley Cup playoffs saw the largest audience in the sport's history \"after a regular season that saw record-breaking business success, propelled largely by the NHL's strategy of engaging fans through big events and robust digital offerings.\" This success has resulted in a 66 percent rise in NHL advertising and sponsorship revenue. Merchandise sales were up 22 percent, and the number of unique visitors on the NHL.com website was up 17 percent during the playoffs after rising 29 percent in the regular season.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21810", "revid": "217538", "url": "https://en.wikipedia.org/wiki?curid=21810", "title": "Northern Michigan University", "text": "Public university in Marquette, Michigan, US\nNorthern Michigan University (Northern Michigan, Northern or NMU) is a public university in Marquette, Michigan, United States. It was established in 1899 by the Michigan Legislature as Northern State Normal School. In 1963, the state designated the school a university and gave it the current name. The university comprises five academic divisions, offering some 180 programs at the undergraduate and graduate levels. NMU's athletic teams are nicknamed the Wildcats and compete primarily in the NCAA Division II Great Lakes Intercollegiate Athletic Conference.\nHistory.\nNorthern Michigan University was established in 1899 by the Michigan Legislature as Northern State Normal School to offer teacher preparation programs in Michigan's then-wild and sparsely populated Upper Peninsula. When it opened in 1899, NMU enrolled thirty-two students who were taught by six faculty members in rented rooms in Marquette city hall. The original campus site at the corner of Presque Isle and Kaye Avenues was on land donated by local businessman and philanthropist John M. Longyear, whose namesake academic building, Longyear Hall, opened in 1900.\nThroughout the school's first half-century, education and teacher training was the school's primary focus. During this time, the school built the native sandstone buildings Kaye and Peter White Halls, as well as a manual training school next to the campus buildings, J.D. Pierce School.The institution has undergone several name changes:\nIn 1963, through the adoption of a new state constitution in Michigan, Northern Michigan was designated a comprehensive university serving the diverse educational needs of Upper Michigan. Graduate education began in March 1935 when courses at the master's degree level were offered in cooperation with the University of Michigan.\nAcademics.\nAdmissions.\nNMU is considered \"selective\" by \"U.S. News &amp; World Report\". For the Class of 2025 (enrolling Fall 2021), NMU received 6,553 applications and accepted 4,670 (71.3%), with 1,496 enrolling.\nThe enrolled first-year class of 2023 had the following standardized test scores: the middle 50% range (25th percentile-75th percentile) of SAT scores was 980-1180, while the middle 50% range of ACT scores was 20-26.\nAcademic divisions.\n180 Undergraduate and graduate degree programs are offered at NMU.\nNMU has five academic divisions:\nNorthern's most popular undergraduate majors, by 2021 graduates, were Registered Nursing/Registered Nurse (105), Biology/Biological Sciences (95), and Art/Art Studies (67).\nAccreditation.\nNorthern Michigan University is accredited by the Higher Learning Commission.\nAll education programs are accredited by the Teacher Education Accreditation Council (TEAC). Other accreditations include the Accreditation Board for Engineering and Technology; American Alliance for Health, Physical Education, Recreation and Dance; American Chemical Society; American Society of Cytology; Commission on Accreditation of Allied Health Education Professionals (Surgical Technology); Committee on Accreditation for Respiratory Care of the Commission on Accreditation of Allied Health Education Programs; Council on Social Work Education; Department of Transportation Federal Aviation Administration Certification; International Association of Counseling Services, Inc.; Joint Review Committee on Education in Radiologic Technology; Michigan Department of Licensing and Regulation, State Board of Nursing; National Accrediting Agency for Clinical Laboratory Sciences; and the National Association of Schools of Music.\nIn addition, the nursing programs (practical nursing, baccalaureate, and master's degrees) are fully approved by the Michigan Department of Licensing and Regulation, State Board of Nursing and the baccalaureate and master's degrees are fully accredited by the Commission on Collegiate Nursing Education (CCNE).\nCampus.\nNMU is a tobacco-free campus.\nTen buildings where classes are held having at least 210 instructional spaces. There are 3 distance learning facilities, the largest of which is Mead Auditorium which seats 100.\nNoteworthy buildings on campus include:\nGovernance.\nNorthern Michigan University's eight-member governing board, the Board of Trustees, is appointed by the Governor of Michigan and confirmed by the Michigan Senate for an eight-year term. The Board of Trustees has general supervision of the institution, the control and direction of all expenditures from the institution's funds, and such other powers and duties as prescribed by law. It also has the authority to hire and evaluate the university president, who reports directly to the board. Members of the Board of Trustees serve without compensation, but are reimbursed by the University for expenses related to Board duties.\nAthletics.\nNMU's Wildcats compete in the NCAA's Division II Great Lakes Intercollegiate Athletic Conference in basketball, football, golf, cross country, soccer, volleyball, track &amp; field, and swimming/diving. The hockey program competes in Division I as a member of the Central Collegiate Hockey Association. The Nordic ski team competes in the Central Collegiate Ski Association. The Division II football team plays in the world's largest wooden dome, the Superior Dome. Lloyd Carr, former head coach at the University of Michigan, former NFL coach Jerry Glanville, and Steve Mariucci, former head coach of the Detroit Lions and San Francisco 49ers and Robert Saleh, defensive coordinator for the San Francisco 49ers, played football for NMU, and Michigan State coach Tom Izzo played basketball at NMU. Northern Michigan's rivals in sports action are the two other major schools in the Upper Peninsula: Michigan Technological University, and Lake Superior State University.\nOlympic Training Site.\nWith more than 70 resident athletes and coaches, the NMU-OTS is the second-largest Olympic training center in the United States, in terms of residents, behind Colorado Springs. The USOEC has more residential athletes than the Lake Placid and Chula Vista sites combined. Over the years, it has grown into a major contributor to the U.S. Olympic movement.\nNMU-OTS athletes attend NMU while training in their respective sports, and are officially recognized as NMU varsity athletes. The student athletes receive free or reduced room and board, access to training facilities as well as sports medicine and sports science services, academic tutoring, and a waiver of out-of-state tuition fees by NMU. Although athletes are responsible for tuition at the in-state rate, they may receive the B.J. Stupak Scholarship to help cover expenses.\nThe NMU-OTS also offers a variety of short-term training camps; regional, national, and international competitions; coaches and officials education clinics; and an educational program for retired Olympians.\nStudent life.\nGroups and activities.\nArmy ROTC.\nNMU hosts the United States Army Cadet Command's \"Wildcat Battalion\".\n\"The North Wind\".\n\"The North Wind\" began in 1972 as Northern Michigan University's second independent, student newspaper. The university's first newspaper was The Northern News, which was shut down due to published articles throughout the 1960s that painted the school in an unflattering manner. In 2015, a controversy arose between the school's administration and members of the North Wind staff, which reached federal court on claims of first amendment violations before the case was dismissed. The weekly paper covers news from the university and community alike and prints on most Wednesdays during the school year.\nWUPX.\nWUPX is Northern Michigan University's non-commercial, student run, radio station broadcasting at 91.5 FM. WUPX provides NMU Students and the Marquette area with a wide variety of music, event announcements, and activities.\nNotable alumni.\nSee List of Northern Michigan University people\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21811", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=21811", "title": "Nemo", "text": "Nemo may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "21813", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=21813", "title": "Naked News", "text": "Canadian news and entertainment website\nNaked News is a Canadian news and entertainment program owned by Naked Broadcasting Network. It features nude female news presenters reading news bulletins derived from news wires. The show's production studio is located in Toronto. There are six daily news programs a week and they are approximately 20 minutes long. The female cast members either read the news fully nude, or disrobe as they present their various segments, including entertainment, sports, movies, food, sex, and relationships. \"Naked News TV!\" is an offshoot of the web program and is broadcast on pay TV in various countries around the world. The show recruits women from around the world to appear on a regular basis or as guest reporters, and their auditions are included in the program. Another segment of the show is \"Naked in the Streets\" in which a reporter appears topless in the street and asks the public about various topics.\nHistory.\n\"Naked News\" was conceived by Fernando Pereira and Kirby Stasyna and debuted in December 1999 as a web-based news service featuring an all-female cast. \nIt began with only one anchor, Victoria Sinclair, who worked for the program until 2015. As the show grew, the number of female anchors increased. Roxanne West joined Sinclair as a lead anchor, and other cast members included Holly Weston, April Torres, Lily Kwan, Sandrine Renard, Erin Sherwood, Athena King, Brooke Roberts, Michelle Pantoliano, Erica Stevens, Samantha Page, Christine Kerr and Valentina Taylor, plus guest anchors. \nThe website was popularized entirely by word of mouth, and quickly became a popular web destination. During the height of its popularity, the website was receiving over 6 million unique visitors per month. In the site's early days, the entire newscast could be viewed for free online. The site was initially supported by advertising, but this changed after the collapse of Internet advertising that occurred with the dot-com crash. By 2002, only one news segment could be viewed for free, and by 2004, no free content remained on the website. Beginning in 2005, a nudity-free version of \"Naked News\" was available to non-subscribers. Beginning in June 2008, two news segments could be viewed for free. However, this ended in December 2009.\nIn 2001, following the success of \"The Naked Truth\", a similar show on Russian television, the \"Naked News\" website launched \"Naked News TV!\", a 45-minute show initially broadcast on the pay-per-view cable television channel Viewers Choice in Canada. It was broadcast in the United States a few months later by the iN DEMAND cable TV service on its Too Much for TV pay-per-view network that also included \"Girls Gone Wild\". In 2002, it was broadcast in Australia on The Comedy Channel via cable and satellite television platforms Foxtel and Austar. The British channel Sumo TV briefly showed episodes of \"Naked News\", while the free-to-view Playboy One broadcast the show at 9:30pm Mondays-Fridays until its closure in 2008.\nA male version of the show ran from 2001 until 2007. It was created to parallel the female version, but ceased production as it did not enjoy the female version's popularity and fame. Although it was originally targeted towards female viewers (at one point said to be 30% of the website's audience), the male show later promoted itself as news from a gay perspective.\nIn August 2004, in Britain, Naked News began to be shown at 21:30 GMT every night on the \"Get Lucky TV\" channel, accessed on Sky Digital.\nBy 2008, \"Naked News TV!\" was available on pay-per-view in the US, Europe, Australia, Asia and Canada, while the \"Naked News\" website had viewers in more than 172 nations \nIn the media.\nIn the 2000s, \"Naked News\" was the subject of a UK-funded documentary called \"Naked News - Backstage\".\nIn 2013, \"Naked News\" was the subject of an eight-part documentary series called \"Naked News Uncovered\", which was broadcast on Super Channel in Canada.\nThe female announcers have been featured on \"CBS Sunday Morning\", \"The Today Show\", \"The View\", \"Sally Jessy Rapha\u00ebl\", and numerous appearances on \"Entertainment Tonight\" and \"ET Insider\", newspapers and magazines (\"TV Guide\", \"Playboy\"), and as guests on several radio shows, including Howard Stern.\nOn-screen performers with concurrent music careers, for example, have used a second stage name, such as \"Roxanne O\u2019Neill\".&lt;ref name=\"idolfeatures/singers-side-hustles\"&gt;&lt;/ref&gt;\nSimilar shows.\nIn the late 1990s, British cable television channel L!VE TV broadcast \"Tiffani's Big City Tips\", in which model Tiffani Banister gave the financial news while stripping to her underwear.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21814", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=21814", "title": "Nitrogen Oxide Protocol", "text": "1988 environmental treaty\nProtocol to the 1979 Convention on Long-Range Transboundary Air Pollution Concerning the Control of Emissions of Nitrogen Oxides or Their Transboundary Fluxes, opened for signature on 31 October 1988 and entered into force on 14 February 1991, was to provide for the control or reduction of nitrogen oxides and their transboundary fluxes. It was concluded in Sofia, Bulgaria.\nParties (as of February 2020): (36) Albania, Austria, Belarus, Belgium, Bulgaria, Canada, Croatia, Cyprus, Czech Republic, Denmark, Estonia, European Union, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Liechtenstein, Lithuania, Luxembourg, Netherlands, North Macedonia, Norway, Poland, Russia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Ukraine, United Kingdom, United States."}
{"id": "21815", "revid": "50274877", "url": "https://en.wikipedia.org/wiki?curid=21815", "title": "Noble Eightfold Path", "text": "Buddhist practices leading to liberation from sa\u1e43s\u0101ra\nThe Noble Eightfold Path () or Eight Right Paths () is an early summary of the path of Buddhist practices leading to liberation from samsara, the painful cycle of rebirth, in the form of nirvana.\nThe Eightfold Path consists of eight practices: right view, right resolve, right speech, right conduct, right livelihood, right effort, right mindfulness, and right ('meditative absorption or union'; alternatively, equanimous meditative awareness).\nIn early Buddhism, these practices started with understanding that the body-mind works in a corrupted way (right view), followed by entering the Buddhist path of self-observance, self-restraint, and cultivating kindness and compassion; and culminating in or , which reinforces these practices for the development of the body-mind. In later Buddhism, insight () became the central soteriological instrument, leading to a different concept and structure of the path, in which the \"goal\" of the Buddhist path came to be specified as ending ignorance and rebirth.\nThe Noble Eightfold Path is one of the principal summaries of the Buddhist teachings, taught to lead to \"Arhatship\". In the Theravada tradition, this path is also summarized as (morality), (meditation) and (insight). In Mahayana Buddhism, this path is contrasted with the Bodhisattva path, which is believed to go beyond \"Arhatship\" to full Buddhahood.\nIn Buddhist symbolism, the Noble Eightfold Path is often represented by means of the dharma wheel (), in which its eight spokes represent the eight elements of the path.\nEtymology and nomenclature.\nThe Pali term (Sanskrit: ) is typically translated in English as 'Noble Eightfold Path'. This translation is a convention started by the early translators of Buddhist texts into English, just like is translated as 'Four Noble Truths'. However, the phrase does not mean the path is noble, rather that the path is of the noble people (Pali: , meaning 'enlightened, noble, precious people'). The term (Sanskrit: ) means 'path', while (Sanskrit: ) means 'eightfold'. Thus, an alternate rendering of is 'eightfold path of the noble ones', or 'Eightfold Ariya Path'.\nAll eight elements of the Path begin with the word (in Sanskrit) or (in P\u0101li) which means 'right, proper, as it ought to be, best'. The Buddhist texts contrast with its opposite, .\nThe Noble Eightfold Path, in the Buddhist traditions, is the direct means to nirvana and brings a release from the cycle of life and death in the realms of samsara.\nThe eight divisions.\nOrigins: the Middle Way.\nAccording to Indologist Tilmann Vetter, the description of the Buddhist path may initially have been as simple as the term \"the Middle Way\". In time, this short description was elaborated, resulting in the description of the Eightfold Path. Tilmann Vetter and historian Rod Bucknell both note that longer descriptions of \"the path\" can be found in the early texts, which can be condensed into the Eightfold Path.\nTenfold path.\nIn the \"Mah\u0101catt\u0101r\u012bsaka Sutta\" which appears in the Chinese and Pali canons, the Buddha explains that cultivation of the noble eightfold path of a learner leads to the development of two further paths of the Arahants, which are right knowledge, or insight (\"samm\u0101-\u00f1\u0101\u1e47a\"), and right liberation, or release (\"samm\u0101-vimutti\"). These two factors fall under the category of wisdom (\"pa\u00f1\u00f1\u0101\").\nShort description of the eight divisions.\nThe eight Buddhist practices in the Noble Eightfold Path are:\nRight view.\nThe purpose of \"right view\" (' / ') or \"right understanding\" is to clear one's path from confusion, misunderstanding, and deluded thinking. It is a means to gain right understanding of reality.\nSequences in the suttas.\nThe Pali canon and the Agamas contain various \"definitions\" or descriptions of \"right view.\" The \"Mahasatipatthana Sutta\" (Digha Nikaya 22), compiled from elements from other suttas possibly as late as 20 BCE, defines right view summarily as the Four Noble Truths: \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And what is right view? Knowing about suffering, the origin of suffering, the cessation of suffering, and the practice that leads to the cessation of suffering. This is called right view.\nIn this, right view explicitly includes \"karma\" and rebirth, and the importance of the Four Noble Truths. This view of \"right view\" gained importance when \"insight\" became central to Buddhist soteriology, and still plays an essential role in Theravada Buddhism.\nOther suttas give a more extensive overview, stating that our actions have consequences, that death is not the end, that our actions and beliefs also have consequences after death, and that the Buddha followed and taught a successful path out of this world and the other world (heaven and underworld or hell). The \"Mah\u0101catt\u0101r\u012bsaka Sutta\" (\"The Great Forty,\" Majjhima Nikaya 117) gives an extensive overview, describing the first seven practices as requisites of right \"samadhi\" c.q. \"dhyana\". It makes a distinction between mundane right view (\"karma\", \"rebirth\") and noble right view as a path-factor, relating noble right view to \"dhamma vicaya\" (\"investigation of principles), one of the \"bojjhanga\", the \"seven factors of awakening\" which give an alternate account of right effort and \"dhyana\".\nAlternatively, right view (together with right resolve) is expressed in the stock phrase of \"dhammalsaddhalpabbajja\": \"A layman hears a Buddha teach the Dhamma, comes to have faith in him, and decides to take ordination as a monk.\"\nLikewise, the Samm\u0101di\u1e6d\u1e6dhi Sutta (Majjhima Nikaya 9), and its parallel in the \"Samyukta-\u0101gama\", refer to faith in the Buddha and understanding (\"dhamma vicaya\") the path-factors of wholesome bodily actions, verbal actions and mental actions.\nTheravada.\nRight View can be further subdivided, states translator Bhikkhu Bodhi, into mundane right view and superior or supramundane right view:\nAccording to Theravada Buddhism, mundane right view is a teaching that is suitable for lay followers, while supramundane right view, which requires a deeper understanding, is suitable for monastics. Mundane and supramundane right view involve accepting the following doctrines of Buddhism:\nA-ditthi.\nGombrich notes that there is a tension in the suttas between \"right view\" and 'no view', release by not clinging to any view at all. According to Chryssides and Wilkins, \"right view is ultimately non-view: though the Enlightened One sees things as they really are, 'he has a \"critical awareness\" of the impossibility of giving full and final expression to his conviction in fixed conceptual terms'. One therefore cannot cling to any particular formulation in a rigid and dogmatic manner.\"\nRight resolve.\nRight Resolve (\"samyak-sa\u1e43kalpa\" / \"samm\u0101-sa\u1e45kappa\") can also be known as \"right thought\", \"right aspiration\", or \"right motivation\". In section III.248, the Majjhima Nikaya states,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nLike right view, this factor has two levels. At the mundane level, the resolve includes being harmless (ahimsa) and refraining from ill will (\"avyapadha\") to any being, as this accrues karma and leads to rebirth. At the supramundane level, the factor includes a resolve to consider everything and everyone as impermanent, a source of suffering and without a Self.\nRight speech.\nRight speech (\"samyag-v\u0101c\" / \"samm\u0101-v\u0101c\u0101\") in most Buddhist texts is presented as four abstentions, such as in the Pali Canon thus:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And what is right speech? Abstaining from lying, from divisive speech, from abusive speech, and from idle chatter: This is called right speech.\nInstead of the usual \"abstention and refraining from wrong\" terminology, a few texts such as the Sama\u00f1\u00f1aphala Sutta and Kevata Sutta in \"Digha Nikaya\" explain this virtue in an active sense, after stating it in the form of an abstention. For example, Sama\u00f1\u00f1aphala Sutta states that a part of a monk's virtue is that \"he abstains from false speech. He speaks the truth, holds to the truth, is firm, reliable, no deceiver of the world.\" Similarly, the virtue of abstaining from divisive speech is explained as delighting in creating concord. The virtue of abstaining from abusive speech is explained in this Sutta to include affectionate and polite speech that is pleasing to people. The virtue of abstaining from idle chatter is explained as speaking what is connected with the Dhamma goal of his liberation.\nIn the \"Abhaya-raja-kumara Sutta\", the Buddha explains the virtue of right speech in different scenarios, based on its truth value, utility value and emotive content. The \"Tathagata\", states Abhaya Sutta, never speaks anything that is unfactual or factual, untrue or true, disagreeable or agreeable, if that is unbeneficial and unconnected to his goals. Further, adds Abhaya Sutta, the \"Tathagata\" speaks the factual, the true, if in case it is disagreeable and unendearing, only if it is beneficial to his goals, but with a sense of proper time. Additionally, adds Abhaya Sutta, the \"Tathagata\", only speaks with a sense of proper time even when what he speaks is the factual, the true, the agreeable, the endearing and what is beneficial to his goals.\nThe Buddha thus explains right speech in the Pali Canon, according to Ganeri, as never speaking something that is not beneficial; and, only speaking what is true and beneficial, \"when the circumstances are right, whether they are welcome or not\".\nRight action.\nRight action (\"samyak-karm\u0101nta\" / \"samm\u0101-kammanta\") is like right speech, expressed as abstentions but in terms of bodily action. In the Pali Canon, this path factor is stated as:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe prohibition on killing precept in Buddhist scriptures applies to all living beings, states Christopher Gowans, not just human beings. Bhikkhu Bodhi agrees, clarifying that the more accurate rendering of the Pali canon is a prohibition on \"taking life of any sentient being\", which includes human beings, animals, birds, insects but excludes plants because they are not considered sentient beings. Further, adds Bodhi, this precept refers to \"intentional\" killing, as well as any form of intentional harming or torturing any sentient being. This moral virtue in early Buddhist texts, both in context of harm or killing of animals and human beings, is similar to \"ahimsa\" precepts found in the texts particularly of Jainism as well as of Hinduism, and has been a subject of significant debate in various Buddhist traditions.\nThe prohibition on stealing in the Pali Canon is an abstention from intentionally taking what is not voluntarily offered by the person to whom that property belongs. This includes taking by stealth, by force, by fraud or by deceit. Both the intention and the act matters, as this precept is grounded on the impact on one's karma.\nThe prohibition on sexual misconduct in the Noble Eightfold Path refers to \"not performing sexual acts\". This virtue is more generically explained in the \"Cunda Kammaraputta Sutta\", which teaches that one must abstain from all sensual misconduct, including getting sexually involved with someone unmarried (anyone protected by parents or by guardians or by siblings), and someone married (protected by husband), and someone betrothed to another person, and female convicts or by \"dhamma\".\nFor monastics, the abstention from sensual misconduct means strict celibacy while for lay Buddhists this prohibits adultery as well as other forms of sensual misconduct. Later Buddhist texts state that the prohibition on sexual conduct for lay Buddhists includes any sexual involvement with someone married, a girl or woman protected by her parents or relatives, and someone prohibited by \"dhamma\" conventions (such as relatives, nuns and others).\nRight livelihood.\nRight livelihood (\"samyag-\u0101j\u012bva\" / \"samm\u0101-\u0101j\u012bva\") precept is mentioned in many early Buddhist texts, such as the \"Mah\u0101catt\u0101r\u012bsaka Sutta\" in \"Majjhima Nikaya\" as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And what is right livelihood? Right livelihood, I tell you, is of two sorts: There is right livelihood with effluents, siding with merit, resulting in acquisitions; there is right livelihood that is noble, without effluents, transcendent, a factor of the path.\nAnd what is the right livelihood with effluents, siding with merit, resulting in acquisitions? There is the case where a disciple of the noble ones abandons wrong livelihood and maintains his life with right livelihood. This is the right livelihood with effluents, siding with merit, resulting in acquisitions.\nAnd what is the right livelihood that is noble, without effluents, transcendent, a factor of the path? The abstaining, desisting, abstinence, avoidance of wrong livelihood in one developing the noble path whose mind is noble, whose mind is without effluents, who is fully possessed of the noble path. (...)\nThe early canonical texts state right livelihood as avoiding and abstaining from wrong livelihood. This virtue is further explained in Buddhist texts, states Vetter, as \"living from begging, but not accepting everything and not possessing more than is strictly necessary\". For lay Buddhists, this precept requires that the livelihood avoid causing suffering to sentient beings by cheating them, or harming or killing them in any way.\nThe Anguttara Nikaya III.208 asserts that the right livelihood does not trade in weapons, living beings, meat, alcoholic drink or poison. The same text, in section V.177, asserts that this applies to lay Buddhists. This has meant, states Harvey, that raising and trading cattle livestock for slaughter is a breach of \"right livelihood\" precept in the Buddhist tradition, and Buddhist countries lack the mass slaughter houses found in Western countries.\nRight effort.\nRight effort (\"samyag-vy\u0101y\u0101ma\" / \"samm\u0101-v\u0101y\u0101ma\") is preventing the arising of unwholesome states, and the generation of wholesome states. This includes \"indriya-samvara\", \"guarding the sense-doors\", restraint of the sense faculties. Right effort is presented in the Pali Canon, such as the \"Sacca-vibhanga Sutta\", as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And what is right effort?\nHere the monk arouses his will, puts forth effort, generates energy, exerts his mind, and strives to prevent the arising of evil and unwholesome mental states that have not yet arisen.\nHe arouses his will... and strives to eliminate evil and unwholesome mental states that have already arisen. He arouses his will... and strives to generate wholesome mental states that have not yet arisen.\nHe arouses his will, puts forth effort, generates energy, exerts his mind, and strives to maintain wholesome mental states that have already arisen, to keep them free of delusion, to develop, increase, cultivate, and perfect them.\nThis is called right effort.\nThe unwholesome states (\"akusala\") are described in the Buddhist texts are related to thoughts, emotions, intentions. These include the \"pancanivarana\" (five hindrances), that is, sensual thoughts, doubts about the path, restlessness, drowsiness, and ill will of any kind. Of these, the Buddhist traditions consider sensual thoughts and ill will needing more right effort. Sensual desire that must be eliminated by effort includes anything related to sights, sounds, smells, tastes and touch. This is to be done by restraint of the sense faculties (\"indriya-samvara\"). Ill will that must be eliminated by effort includes any form of aversion including hatred, anger, resentment towards anything or anyone.\nRight mindfulness.\nWhile originally, in Yogic practice, \"sati\" may have meant to remember the meditation object, to cultivate a deeply absorbed, secluded state of mind, in the oldest Buddhism it has the meaning of \"retention\", being mindful of the \"dhammas\" (both wholesome states of mind, and teachings and practices that remind of those wholesome states of mind) that are beneficial to the Buddhist path. According to Gethin, \"sati\" is a quality that guards or watches over the mind; the stronger it becomes, the weaker unwholesome states of mind become, weakening their power \"to take over and dominate thought, word and deed.\" According to Frauwallner, mindfulness was a means to prevent the arising of craving, which resulted simply from contact between the senses and their objects. According to Frauwallner this may have been the Buddha's original idea. According to Trainor, mindfulness aids one not to crave and cling to any transitory state or thing, by complete and constant awareness of phenomena as impermanent, suffering and without self. Gethin refers to the \"Milindapanha\", which states that \"sati\" brings to mind the \"dhammas\" and their beneficial or unbeneficial qualities, aiding the removal of unbeneficial dhammas and the strengthening of beneficial dhammas. Gethin further notes that \"sati\" makes one aware of the \"full range and extent of \"dhammas\"\", that is, the relation between things, broadening one's view and understanding.\nThe \"Satipatthana Sutta\" describes the contemplation of four domains, namely body, feelings, mind and phenomena. The \"Satipatthana Sutta\" is regarded by the vipassana movement as the quintessential text on Buddhist meditation, taking cues from it on \"bare attention\" and the contemplation on the observed phenomena as \"dukkha\", \"anatta\" and \"anicca\". According to Grzegorz Polak, the four \"upassan\u0101\" have been misunderstood by the developing Buddhist tradition, including Theravada, to refer to four different foundations. According to Polak, the four \"upassan\u0101\" do not refer to four different foundations of which one should be aware, but are an alternate description of the \"jhanas\", describing how the \"samskharas\" are tranquilized:\nIn the vipassana movement, mindfulness (\"\" / \"samm\u0101-sati\") is interpreted as \"bare attention\": never be absent minded, being conscious of what one is doing. Rupert Gethin notes that the contemporary vipassana movement interprets the \"Satipatthana Sutta\" as \"describing a pure form of insight (\"vipassan\u0101\") meditation\" for which \"samatha\" (calm) and \"dhy\u0101na\" are not necessary. Yet, in pre-sectarian Buddhism, the establishment of mindfulness was placed before the practice of the \"Dhy\u0101na\", and associated with the abandonment of the five hindrances and the entry into the first \"Dhy\u0101na\".\nThe \"dhy\u0101na\"-scheme describes mindfulness also as appearing in the third and fourth \"dhy\u0101na\", after initial concentration of the mind. Gombrich and Wynne note that, while the second \"dhy\u0101na\" denotes a state of absorption, in the third and fourth \"dhy\u0101na\" one comes out of this absorption, being mindfully aware of objects while being indifferent to them. According to Gombrich, \"the later tradition has falsified the jhana by classifying them as the quintessence of the concentrated, calming kind of meditation, ignoring the other \u2013 and indeed higher \u2013 element\".\nRight samadhi (unification of mind).\n\"Samadhi\".\n\"Samadhi\" (\"samyak-sam\u0101dhi\" / \"samm\u0101-sam\u0101dhi\") is a common practice or goal in Indian religions. The term \"samadhi\" derives from the root sam-a-dha, which means 'to collect' or 'bring together', and thus it is often translated as 'concentration' or 'unification of mind'. In the early Buddhist texts, samadhi is also associated with the term \"samatha\" (calm abiding).\nDhyana.\nBronkhorst notes that neither the Four Noble Truths nor the Noble Eightfold Path discourse provide details of right \"samadhi\". Several \"Suttas\", such as the following in \"Saccavibhanga Sutta\", equate it with \"dhyana\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And what is right concentration?\n[i] Here, the monk, detached from sense-desires, detached from unwholesome states, enters and remains in the first \"jhana\" (level of concentration, Sanskrit: \"dhy\u0101na\"), in which there is applied and sustained thinking, together with joy and pleasure born of detachment;\n[ii] And through the subsiding of applied and sustained thinking, with the gaining of inner stillness and oneness of mind, he enters and remains in the second \"jhana\", which is without applied and sustained thinking, and in which there are joy and pleasure born of concentration;\n[iii] And through the fading of joy, he remains equanimous, mindful and aware, and he experiences in his body the pleasure of which the Noble Ones say: \"equanimous, mindful and dwelling in pleasure\", and thus he enters and remains in the third \"jhana\";\n[iv] And through the giving up of pleasure and pain, and through the previous disappearance of happiness and sadness, he enters and remains in the fourth \"jhana\", which is without pleasure and pain, and in which there is pure equanimity and mindfulness.\nThis is called right concentration.\nBronkhorst has questioned the historicity and chronology of the description of the four \"jhanas\". Bronkhorst states that this path may be similar to what the Buddha taught, but the details and the form of the description of the \"jhanas\" in particular, and possibly other factors, is likely the work of later scholasticism. Bronkhorst notes that description of the third \"jhana\" cannot have been formulated by the Buddha, since it includes the phrase \"Noble Ones say\", quoting earlier Buddhists, indicating it was formulated by later Buddhists. It is likely that later Buddhist scholars incorporated this, then attributed the details and the path, particularly the insights at the time of liberation, to have been discovered by the Buddha.\nConcentration.\nIn the Theravada tradition, \"samadhi\" is interpreted as concentration on a meditation object. Buddhagosa defines samadhi as \"the centering of consciousness and consciousness concomitants evenly and rightly on a single object...the state in virtue of which consciousness and its concomitants remain evenly and rightly on a single object, undistracted and unscattered.\"\nAccording to Henepola Gunaratana, in the suttas samadhi is defined as one-pointedness of mind (\"Cittass'ekaggat\u0101\"). According to Bhikkhu Bodhi, the right concentration factor is reaching a one-pointedness of mind and unifying all mental factors, but it is not the same as \"a gourmet sitting down to a meal, or a soldier on the battlefield\" who also experience one-pointed concentration. The difference is that the latter have a one-pointed object in focus with complete awareness directed to that object\u00a0\u2013 the meal or the target, respectively. In contrast, right concentration meditative factor in Buddhism is a state of awareness without any object or subject, and ultimately unto no-thingness and emptiness, as articulated in apophatic discourse. \nDevelopment into equanimity.\nAlthough often translated as \"concentration\", as in the limiting of the attention of the mind on one object, in the fourth \"dhyana\" \"equanimity and mindfulness remain\", and the practice of concentration-meditation may well have been incorporated from non-Buddhist traditions. Vetter notes that \"samadhi\" consists of the four stages of awakening, but\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...to put it more accurately, the first dhyana seems to provide, after some time, a state of strong concentration, from which the other stages come forth; the second stage is called samadhija.\nGombrich and Wynne note that, while the second \"jhana\" denotes a state of absorption, in the third and fourth \"jhana\" one comes out of this absorption, being mindfully awareness of objects while being indifferent to it. According to Gombrich, \"the later tradition has falsified the jhana by classifying them as the quintessence of the concentrated, calming kind of meditation, ignoring the other \u2013 and indeed higher \u2013 element.\"\nLiberation.\nFollowing the Noble Eightfold Path leads to liberation in the form of nirvana:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;And what is that ancient path, that ancient road, traveled by the Rightly Self-awakened Ones of former times? Just this noble eightfold path: right view, right aspiration, right speech, right action, right livelihood, right effort, right mindfulness, right concentration. That is the ancient path, the ancient road, traveled by the Rightly Self-awakened Ones of former times. I followed that path. Following it, I came to direct knowledge of aging &amp; death, direct knowledge of the origination of aging &amp; death, direct knowledge of the cessation of aging &amp; death, direct knowledge of the path leading to the cessation of aging &amp; death. I followed that path. Following it, I came to direct knowledge of birth... becoming... clinging... craving... feeling... contact... the six sense media... name-&amp;-form... consciousness, direct knowledge of the origination of consciousness, direct knowledge of the cessation of consciousness, direct knowledge of the path leading to the cessation of consciousness. I followed that path.\u2014\u200a\nPractice.\nOrder of practice.\nVetter notes that originally the path culminated in the practice of \"dhyana/samadhi\" as the core soteriological practice. According to the Pali and Chinese canon, the \"samadhi\" state (right concentration) is dependent on the development of preceding path factors:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Blessed One said: \"Now what, monks, is noble right concentration with its supports and requisite conditions? Any singleness of mind equipped with these seven factors\u00a0\u2013 right view, right resolve, right speech, right action, right livelihood, right effort, and right mindfulness\u00a0\u2013 is called noble right concentration with its supports and requisite conditions.\u2014\u200a\nAccording to the discourses, right view, right resolve, right speech, right action, right livelihood, right effort, and right mindfulness are used as the support and requisite conditions for the practice of right concentration. Understanding of the right view is the preliminary role, and is also the forerunner of the entire Noble Eightfold Path.\nAccording to the modern Theravada monk and scholar Walpola Rahula, the divisions of the noble eightfold path \"are to be developed more or less simultaneously, as far as possible according to the capacity of each individual. They are all linked together and each helps the cultivation of the others.\" Bhikkhu Bodhi explains that these factors are not sequential, but components, and \"with a certain degree of progress all eight factors can be present simultaneously, each supporting the others. However, until that point is reached, some sequence in the unfolding of the path is inevitable.\"\nThe stage in the Path where there is no more learning in Yogachara Abhidharma, state Buswell and Gimello, is identical to \"Nirvana\" or \"Buddhahood\", the ultimate goal in Buddhism.\nSila-samadhi-prajna.\nThe Noble Eightfold Path is sometimes divided into three basic divisions, with right view and right resolve concluding the sequence:\nThis order is a later development, when discriminating insight (\"prajna\") became central to Buddhist soteriology, and came to be regarded as the culmination of the Buddhist path. Yet, Majjhima Nikaya 117, \"Mah\u0101catt\u0101r\u012bsaka Sutta\", describes the first seven practices as requisites for right samadhi. According to Vetter, this may have been the original soteriological practice in early Buddhism.\nThe \"moral virtues\" (Sanskrit: \"\u015b\u012bla\", P\u0101li: \"s\u012bla\") group consists of three paths: right speech, right action and right livelihood. The word \"s\u012bla\", though translated by English writers as linked to \"morals or ethics\", states Bhikkhu Bodhi, is in ancient and medieval Buddhist commentary tradition closer to the concept of discipline and disposition that \"leads to harmony at several levels\u00a0\u2013 social, psychological, karmic and contemplative\". Such harmony creates an environment to pursue the meditative steps in the Noble Eightfold Path by reducing social disorder, preventing inner conflict that result from transgressions, favoring future karma-triggered movement through better rebirths, and purifying the mind.\nThe meditation group (\"samadhi\") of the path progresses from moral restraints to training the mind. Right effort and mindfulness calm the mind-body complex, releasing unwholesome states and habitual patterns and encouraging the development of wholesome states and non-automatic responses, the \"bojjha\u1e45ga\" (seven factors of awakening). The practice of \"dhy\u0101na\" reinforces these developments, leading to \"upekkh\u0101\" (equanimity) and mindfulness. According to the Theravada commentarial tradition and the contemporary vipassana movement, the goal in this group of the Noble Eightfold Path is to develop clarity and insight into the nature of reality\u00a0\u2013 \"dukkha\", \"anicca\" and \"anatta\", discard negative states and dispel \"avidya\" (ignorance), ultimately attaining \"nirvana\".\nIn the threefold division, \"prajna\" (insight, wisdom) is presented as the culmination of the path, whereas in the eightfold division the path starts with correct knowledge or insight, which is needed to understand why this path should be followed.\nSchools of Buddhism and their views of the Eightfold Path.\nTheravada presentations of the path.\nTheravada Buddhism is a diverse tradition and thus includes different explanations of the path to awakening. However, the teachings of the Buddha are often encapsulated by Theravadins in the basic framework of the Four Noble Truths and the Eighthfold Path.\nSome Theravada Buddhists also follow the presentation of the path laid out in Buddhaghosa's Visuddhimagga. This presentation is known as the \"Seven Purifications\" (\"satta-visuddhi\"). This schema and its accompanying outline of \"insight knowledges\" (\"vipassan\u0101-\u00f1\u0101\u1e47a\") is used by modern influential Theravadin scholars, such Mahasi Sayadaw (in his \"The Progress of Insight\") and Nyanatiloka Thera (in \"The Buddha's Path to Deliverance\").\nMahayana presentations of the path.\nMah\u0101y\u0101na Buddhism is based principally upon the path of a Bodhisattva. A \"Bodhisattva\" refers to one who is on the path to buddhahood. The term \"Mah\u0101y\u0101na\" was originally a synonym for \"Bodhisattvay\u0101na\" or \"Bodhisattva Vehicle\".\nIn the earliest texts of Mah\u0101y\u0101na Buddhism, the path of a bodhisattva was to awaken the \"bodhicitta\". Between the 1st and 3rd century CE, this tradition introduced the \"Ten Bhumi\" doctrine, which means ten levels or stages of awakening. This development was followed by the acceptance that it is impossible to achieve Buddhahood in one (current) lifetime, and the best goal is not nirvana for oneself, but Buddhahood after climbing through the ten levels during multiple rebirths. Mah\u0101y\u0101na scholars then outlined an elaborate path, for monks and laypeople, and the path includes the vow to help teach Buddhist knowledge to other beings, so as to help them cross samsara and liberate themselves, once one reaches the Buddhahood in a future rebirth. One part of this path are the \"p\u0101ramit\u0101\" (perfections, to cross over), derived from the \"Jatakas\" tales of Buddha's numerous rebirths.\nThe doctrine of the bodhisattva bh\u016bmis was also eventually merged with the Sarv\u0101stiv\u0101da Vaibh\u0101\u1e63ika schema of the \"five paths\" by the Yogacara school\".\" This Mah\u0101y\u0101na \"five paths\" presentation can be seen in Asanga's \"Mah\u0101y\u0101nasa\u1e43graha\".\nThe Mah\u0101y\u0101na texts are inconsistent in their discussion of the \"p\u0101ramit\u0101s\", and some texts include lists of two, others four, six, ten and fifty-two. The six paramitas have been most studied, and these are:\nIn Mah\u0101y\u0101na Sutras that include ten \"p\u0101ramit\u0101\", the additional four perfections are \"skillful means, vow, power and knowledge\". The most discussed \"p\u0101ramit\u0101\" and the highest rated perfection in Mahayana texts is the \"Prajna-paramita\", or the \"perfection of insight\". This insight in the Mah\u0101y\u0101na tradition, states Sh\u014dhei Ichimura, has been the \"insight of non-duality or the absence of reality in all things\".\nEast Asian Buddhism.\nEast Asian Buddhism is influenced by both the classic Indian Buddhist presentations of the path such as the Eightfold Path as well as classic Indian Mah\u0101y\u0101na presentations such as that found in the Da zhidu lun.\nThere are many different presentations of soteriology, including numerous paths and vehicles (\"yanas\") in the different traditions of East Asian Buddhism. There is no single dominant presentation. In Zen Buddhism for example, one can find outlines of the path such as the Two Entrances and Four Practices\",\" The Five ranks, The Ten Ox-Herding Pictures and The Three mysterious Gates of Linji.\nIndo-Tibetan Buddhism.\nIn Indo-Tibetan Buddhism, the path to liberation is outlined in the genre known as Lamrim (\"Stages of the Path\"). All the various Tibetan schools have their own Lamrim presentations. This genre can be traced to Ati\u015ba's 11th-century \"A Lamp for the Path to Enlightenment\" (\"Bodhipathaprad\u012bpa\").\nCognitive psychology.\nThe noble eightfold path has been compared to cognitive psychology; Gil Fronsdal says the right view factor can be interpreted to mean how one's mind views the world, and how that leads to patterns of thought, intention and actions. Peter Randall states that it is the seventh factor or right mindfulness that may be thought in terms of cognitive psychology, wherein the change in thought and behavior are linked.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nSecondary sources.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21816", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=21816", "title": "Neapolitan Ice Cream", "text": ""}
{"id": "21818", "revid": "14650386", "url": "https://en.wikipedia.org/wiki?curid=21818", "title": "National park", "text": "Park for conservation of nature and usually also for visitors\nA national park is a nature park designated for conservation purposes because of unparalleled national natural, historic, or cultural significance. It is an area of natural, semi-natural, or developed land that is protected and owned by a government. Although governments hold different standards for national park designation, the conservation of 'wild nature' for posterity and as a symbol of national pride is a common motivation for the continued protection of all national parks around the world. National parks are almost always accessible to the public. Usually national parks are developed, owned and managed by national governments, though in some countries with federal or devolved forms of government, \"national parks\" may be the responsibility of subnational, regional, or local authorities.\nThe United States established Yellowstone National Park, the first \"public park or pleasuring-ground for the benefit and enjoyment of the people,\" in 1872. Although Yellowstone was not officially termed a \"national park\" at the time, in practice it is widely held to be the first and oldest national park in the world. The Tobago Main Ridge Forest Reserve (in what is now Trinidad and Tobago; established in 1776) and the area surrounding Bogd Khan Uul Mountain (Mongolia, 1778), which were restricted from cultivation to protect surrounding farmland, are considered the oldest legally protected areas. Parks Canada, established on May 19, 1911, is the world's oldest national park service. \nThe International Union for Conservation of Nature (IUCN) and its World Commission on Protected Areas (WCPA) have defined \"National Park\" as its \"Category\u00a0II\" type of protected areas. According to the IUCN, 6,555 national parks worldwide met its criteria in 2006. IUCN is still discussing the parameters of defining a national park.\nThe largest national park in the world meeting the IUCN definition is the Northeast Greenland National Park, which was established in 1974 and is in area.\nDefinitions.\nIn 1969, the IUCN declared a national park to be a relatively large area with the following defining characteristics:\nIn 1971, these criteria were further expanded upon leading to more clear and defined benchmarks to evaluate a national park. These include:\nWhile the term national park is now defined by the IUCN, many protected areas in many countries are called national park even when they correspond to other categories of the IUCN Protected Area Management Definition, for example:\nWhile national parks are generally understood to be administered by national governments (hence the name), in Australia, with the exception of six national parks, national parks are run by state governments and predate the Federation of Australia; similarly, national parks in the Netherlands are administered by the provinces. In Canada, there are both national parks operated by the federal government and provincial or territorial parks operated by the provincial and territorial governments, although nearly all are still national parks by the IUCN definition.\nIn many countries, including Indonesia, the Netherlands, and the United Kingdom, national parks do not adhere to the IUCN definition, while some areas which adhere to the IUCN definition are not designated as national parks.\nTerminology.\nAs many countries do not adhere to the IUCN definition, the term \"national park\" may be used loosely. In the United Kingdom, and in some other countries such as Taiwan, a \"national park\" simply describes a general area that is relatively undeveloped, scenic, and attracts tourists, with some form of planning restrictions to ensure it maintains those characteristics. There may be substantial human settlements within the bounds of a national park. \nConversely, parks that meet the criteria may be not be referred to as \"national parks\". Terms like \"preserve\" or \"reserve\" may be used instead.\nHistory.\nEarly references.\nStarting in 1735 the Naples government undertook laws to protect Natural areas, which could be used as a game reserve by the royal family; Procida was the first protected site; the difference between the many previous royal hunting preserves and this one, which is considered to be closer to a Park rather than a hunting preserve, is that Neapolitan government already considered the division into the present-day wilderness areas and non-strict nature reserves.\nIn 1810, the English poet William Wordsworth described the Lake District as a \"sort of national property, in which every man has a right and interest who has an eye to perceive and a heart to enjoy.\" The painter George Catlin, in his travels through the American West, wrote during the 1830s that Native Americans in the United States might be preserved \"(by some great protecting policy of government) ... in a \"magnificent park\" ... A \"nation's Park\", containing man and beast, in all the wild and freshness of their nature's beauty!\"\nFirst efforts: Hot Springs, Arkansas and Yosemite Valley.\nThe first effort by the U.S. Federal government to set aside such protected lands was on 20 April 1832, when President Andrew Jackson signed legislation that the 22nd United States Congress had enacted to set aside four sections of land around what is now Hot Springs, Arkansas, to protect the natural, thermal springs and adjoining mountainsides for the future disposal of the U.S.\u00a0government. It was known as Hot Springs Reservation, but no legal authority was established. Federal control of the area was not clearly established until 1877. The work of important leaders who fought for animal and land conservation were essential in the development of legal action. Some of these leaders include President Abraham Lincoln, Laurance Rockefeller, President Theodore Roosevelt, John Muir, and First Lady Lady Bird Johnson to name a few.\nJohn Muir is today referred to as the \"Father of the National Parks\" due to his work in Yosemite. He published two influential articles in The Century Magazine, which formed the base for the subsequent legislation.\nPresident Abraham Lincoln signed an Act of Congress on 1 July 1864, ceding the Yosemite Valley and the Mariposa Grove of giant sequoias (later becoming Yosemite National Park) to the state of California. According to this bill, private ownership of the land in this area was no longer possible. The state of California was designated to manage the park for \"public use, resort, and recreation\". Leases were permitted for up to ten years and the proceeds were to be used for conservation and improvement. A public discussion followed this first legislation of its kind and there was a heated debate over whether the government had the right to create parks. The perceived mismanagement of Yosemite by the Californian state was the reason why Yellowstone was put under national control at its establishment six years later.\nFirst national park: Yellowstone.\nIn 1872, Yellowstone National Park was established as the United States' first national park, being also the world's first national park. In some European and Asian countries, however, national protection and nature reserves already existed - though typically as game reserves and recreational grounds set aside for royalty, such as a part of the Forest of Fontainebleau (France, 1861).\nYellowstone was part of a federally governed territory. With no state government that could assume stewardship of the land, the federal government took on direct responsibility for the park, the official first national park of the United States. The combined effort and interest of conservationists, politicians and the Northern Pacific Railroad ensured the passage of enabling legislation by the United States Congress to create Yellowstone National Park. Theodore Roosevelt and his group of conservationists, the Boone and Crockett Club, were active campaigners and were highly influential in convincing fellow Republicans and big business to back the bill. Yellowstone National Park soon played a pivotal role in the conservation of these national treasures, as it was suffering at the hands of poachers and others who stood at the ready to pillage what they could from the area. Theodore Roosevelt and his newly formed Boone and Crockett Club successfully took the lead in protecting Yellowstone National Park from this plight, resulting in laws designed to conserve the natural resources in Yellowstone and other parks under the Government's purview.\nAmerican Pulitzer Prize-winning author Wallace Stegner wrote: \"National parks are the best idea we ever had. Absolutely American, absolutely democratic, they reflect us at our best rather than our worst.\"\nInternational growth of national parks.\nThe first area to use \"national park\" in its creation legislation was the U.S.'s Mackinac National Park, in 1875. (The area was later transferred to the state's authority in 1895, thus losing its official \"national park\" status.)\nFollowing the idea established in Yellowstone and Mackinac, there soon followed parks in other nations. In Australia, what is now Royal National Park was established just south of Sydney, Colony of New South Wales, on 26 April 1879, becoming the world's second official national park. Since Mackinac lost its national park status, the Royal National Park is, by some considerations, the second oldest national park now in existence.\nBanff National Park became Canada's first national park in 1885. New Zealand established Tongariro National Park in 1887. Argentina became the third country in the Americas to create a national park system, with the creation of the Nahuel Huapi National Park in 1934, through the initiative of Francisco Moreno.\nIn Europe, the first national parks were a set of nine in Sweden in 1909, following the passing of a Riksdag law on national parks that year. Switzerland became the second European nation with the founding of the Swiss National Park in 1914. In 1971, Lahemaa National Park in Estonian SSR became the first area to be designated a national park in the former Soviet Union.\nAfrica's first national park was established in 1925 when king Albert\u00a0I of Belgium designated an area in the east of what was then his personal domain of Congo Free State, now Democratic Republic of Congo as the Albert National Park, later renamed Virunga National Park. In 1926, the government of South Africa designated Kruger National Park as the nation's first national park, although it was an expansion and reorganization of the earlier government protected Sabie Game Reserve, established in 1898 by President Paul Kruger of the old South African Republic.\nAfter World War\u00a0II, national parks were founded all over the world. The United Kingdom designated its first national park, Peak District National Park, in 1951. This followed perhaps 70 years of pressure for greater public access to the landscape. By the end of the decade a further nine national parks had been designated in the UK. \nEurope has some 359 national parks as of 2010. The Vanoise National Park in the Alps was the first French national park, created in 1963 after public mobilization against a touristic project.\nIn 1973, Mount Kilimanjaro was classified as a National Park and was opened to public access in 1977.In 1989, the Qomolangma National Nature Preserve (QNNP) was created to protect 3.381\u00a0million hectares on the north slope of Mount Everest in the Tibet Autonomous Region of China. This national park is the first major global park to have no separate warden and protection staff\u2014all of its management consists of existing local authorities, allowing a lower cost basis and a larger geographical coverage (in 1989 when created, it was the largest protected area in Asia). It includes four of the six tallest mountains in the world: Everest, Lhotse, Makalu, and Cho Oyu. The QNNP is contiguous to four Nepali national parks, creating a transnational conservation area equal in size to Switzerland.\nIn 1993, the Blue and John Crow Mountains National Park was established in Jamaica to conserve and protect 41,198 hectares, including tropical montane rainforest and adjacent buffer areas. The site includes Jamaica's tallest peak (Blue Mountain Peak), hiking trails and a visitor center. The Park was also designated a UNESCO World Heritage Site in 2015.\nNational parks services.\nThe world's first national park service was established May 19, 1911, in Canada. The \"Dominion Forest Reserves and Parks Act\" placed the dominion parks under the administration of the Dominion Park Branch (now Parks Canada), within the Department of the Interior. The branch was established to \"protect sites of natural wonder\" to provide a recreational experience, centred on the idea of the natural world providing rest and spiritual renewal from the urban setting. Canada now has the largest protected area in the world with 450,000\u00a0km2 of national park space.\nEven with the creation of Yellowstone, Yosemite, and nearly 37 other national parks and monuments, another 44 years passed before an agency was created in the United States to administer these units in a comprehensive way\u00a0\u2013 the U.S. National Park Service (NPS). The 64th United States Congress passed the National Park Service Organic Act, which President Woodrow Wilson signed into law on 25 August 1916. Of the 433 sites managed by the National Park Service of the United States, only 63 carry the designation of National Park.\nEconomic ramifications.\nCountries with a large ecotourism industry, such as Costa Rica, often experience a huge economic effect on park management as well as the economy of the country as a whole.\nTourism.\nTourism to national parks has increased considerably over time. In Costa Rica for example, a megadiverse country, tourism to parks has increased by 400% from 1985 to 1999. The term \"national park\" is perceived as a brand name that is associated with nature-based tourism and it symbolizes a \"high quality natural environment with a well-designed tourist infrastructure\".\nStaff.\nThe duties of a park ranger are to supervise, manage, and/or perform work in the conservation and use of park resources. This involves functions such as park conservation; natural, historical, and cultural resource management; and the development and operation of interpretive and recreational programs for the benefit of the visiting public. Park rangers also have fire fighting responsibilities and execute search and rescue missions. Activities also include heritage interpretation to disseminate information to visitors of general, historical, or scientific information. Management of resources such as wildlife, lake shores, seashores, forests, historic buildings, battlefields, archaeological properties, and recreation areas are also part of the job of a park ranger. Since the establishment of the National Park Service in the US in 1916, the role of the park ranger has shifted from merely being a custodian of natural resources to include several activities that are associated with law enforcement. They control traffic, manage permits for various uses, and investigate violations, complaints, trespass/encroachment, and accidents.\nConcerns.\nNational parks in former European colonies have come under criticism for allegedly perpetuating colonialism. National parks were created by individuals who felt that pristine, natural sections of nature should be set aside and preserved from urban development. In America, this movement came about during the American frontier and were meant to be monuments to America's true history. Yet, in some instances, the lands that were to be set aside and protected in formerly colonized lands were already being inhabited by native communities, who were then removed off of these lands to create pristine sites for public consumption. Critics claim that the removal of people from national parks enhances the belief that nature can only be protected when humans do not exist within it, and that this leads to perpetuating the dichotomy between nature and humans (also known as the nature\u2013culture divide). They see the creation of national parks as a form of eco-land grabbing. Others claim that traveling to national parks to appreciate nature there leads people to ignore the nature that exists around them every day. Still others argue that tourism can actually negatively impact the areas that are being visited.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21819", "revid": "40671709", "url": "https://en.wikipedia.org/wiki?curid=21819", "title": "Nuncio", "text": "Papal ambassador\nAn apostolic nuncio (; also known as a papal nuncio or simply as a nuncio) is an ecclesiastical diplomat, serving as an envoy or a permanent diplomatic representative of the Holy See to a state or to an international organization. A nuncio is appointed by and represents the Holy See, and is the head of the diplomatic mission, called an apostolic nunciature, which is the equivalent of an embassy. The Holy See is legally distinct from the Vatican City or the Catholic Church. In modern times, a nuncio is usually an Archbishop.\nAn apostolic nuncio is generally equivalent in rank to that of ambassador extraordinary and plenipotentiary, although in Catholic countries the nuncio often ranks above ambassadors in diplomatic protocol. A nuncio performs the same functions as an ambassador and has the same diplomatic privileges. Under the 1961 Vienna Convention on Diplomatic Relations, to which the Holy See is a party, a nuncio is an ambassador like those from any other country. The Vienna Convention allows the host state to grant seniority of precedence to the nuncio over others of ambassadorial rank accredited to the same country, and may grant the deanship of that country's diplomatic corps to the nuncio regardless of seniority. The representative of the Holy See in some situations is called a Delegate or, in the case of the United Nations, Permanent Observer. In the Holy See hierarchy, these usually rank equally to a nuncio, but they do not have formal diplomatic status, though in some countries they have some diplomatic privileges.\nIn addition, the nuncio serves as the liaison between the Holy See and the Church in that particular nation, supervising the diocesan episcopate (usually a national or multinational conference of bishops which has its own chairman, elected by its members). The nuncio has an important role in the selection of bishops.\nTerminology and history.\nThe name \"nuncio\" derived from the ancient Latin word \"nuntius\", meaning \"envoy\" or \"messenger\". Since such envoys are accredited to the Holy See as such and not to the State of Vatican City, the term \"nuncio\" (versus \"ambassador\") emphasizes the unique nature of the diplomatic mission. The 1983 Code of Canon Law claims the \"innate right\" to send and receive delegates independent from interference of non-ecclesiastical civil power. Canon law only recognizes international law limitations on this right.\nArticle 16 of the Vienna Convention on Diplomatic Relations provides:\nIn accordance with this article, many states (even not predominantly Catholic ones such as Germany and Switzerland and including the great majority in central and western Europe and in the Americas) give precedence to the nuncio over other diplomatic representatives, according him the position of Dean of the Diplomatic Corps reserved in other countries for the longest-serving resident ambassador.\nMultilateral.\nHoly See representatives called permanent observers are accredited to several international organisations, including offices or agencies of the United Nations, and other organizations either specialized in their mission or regional or both. A permanent observer of the Holy See is always a cleric, often a titular archbishop with the rank of nuncio, but there has been considerable variation between offices and over time.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21820", "revid": "6196463", "url": "https://en.wikipedia.org/wiki?curid=21820", "title": "Newlyn School", "text": "Art colony around Newlyn in Cornwall\nThe Newlyn School was an art colony of artists based in or near Newlyn, a fishing village adjacent to Penzance, on the south coast of Cornwall, from the 1880s until the early twentieth century. The establishment of the Newlyn School was reminiscent of the Barbizon School in France, where artists fled Paris to paint in a more pure setting emphasising natural light. These schools along with a related California movement were also known as En plein air.\nHistory.\nSome of the first British artists to settle in the area had already travelled in Brittany, but found in Newlyn a comparable English environment with a number of things guaranteed to attract them: fantastic light, cheap living, and the availability of inexpensive models. The artists were fascinated by the fishermen's working life at sea and the everyday life in the harbour and nearby villages. Some paintings showed the hazards and tragedy of the community's life, such as women anxiously looking out to sea as the boats go out, or a young woman crying on hearing news of a disaster. Walter Langley is generally recognised as the pioneer of the Newlyn art colony and Stanhope Forbes, who settled there in 1884, as the father of it. The local newspaper (The Cornishman) was positive about the artists, reporting that the area had much to thank them for \u2033... circulating money, ...\u2033, and \u2033... are ever ready to assist in anything which pertains to the welfare of the town.\u2033\nThe later Forbes School of Painting, founded by Forbes and his wife Elizabeth in 1899, promoted the study of figure painting. A present-day Newlyn School of Art was formed in 2011 with Arts Council funding providing art courses taught by many of the best-known artists working in Cornwall today.\nIn the late nineteenth and early twentieth centuries, Lamorna, a nearby fishing village to the south, became popular with artists of the Newlyn School and is particularly associated with the artist S J \"Lamorna\" Birch who lived there from 1908.\nMember artists.\nNewlyn School painters include:\nFor a full list see: George Bednar. \"Every Corner was a Picture: A checklist compiled for the West Cornwall Art Archive of 50 artists from the early Newlyn School painters through to the present.\" \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21822", "revid": "19089174", "url": "https://en.wikipedia.org/wiki?curid=21822", "title": "Natural Law Party", "text": "The Natural Law Party (NLP) is a transnational party founded in 1992 on \"the principles of Transcendental Meditation\",&lt;ref name=\"Cowan/Bromley\"&gt;&lt;/ref&gt; the laws of nature, and their application to all levels of government. At its peak, it was active in up to 74 countries; it continues in India and at the state level in the United States. The party defines \"natural law\" as the organizing intelligence which governs the natural universe. The Natural Law Party advocates using the Transcendental Meditation technique and the TM-Sidhi program as tools to enliven natural law and reduce or eliminate problems in society.\nProminent candidates included John Hagelin for U.S. president and Doug Henning as representative of Rosedale, Toronto, Canada. George Harrison performed a benefit concert in support of the party in 1992. Electoral success was achieved by the Ajeya Bharat Party in India, which elected a legislator to the state assembly, and the Croatian NLP, which elected a member of their regional assembly in 1993. In 2002, in the USA, its organization was reported to rival that of other \"established third parties\", but most party chapters have since disbanded.\nHistory and platform.\nAccording to the Maharishi Mahesh Yogi, the Natural Law Party (NLP) was first founded in the United Kingdom in March 1992 and was later established in the United States, France, Austria, Germany, Croatia, Israel, Japan, Spain, the Netherlands, Italy, Australia, Norway, Sweden, New Zealand, Chile, Thailand and Canada. The American branch of the party was founded later that year in Fairfield, Iowa U.S.A. by educators, business leaders, lawyers and other supporters of the Transcendental Meditation movement. The party was active in many countries and delegates from 60 countries attended an international convention in Bonn, Germany in 1998. The party became largely inactive in the United States in 2004 and was discontinued in the Netherlands in 2007.\nThe party had its foundation in the principles of Transcendental Meditation and was committed to \"prevention oriented government and conflict free politics\" through holistic health programmes and the practice of the Transcendental Meditation technique. In Scotland and Wales, party advertisements proclaimed that \"natural law which silently governs the whole universe in perfect order and without a problem.\" The Scotland and Wales branch of the party promised reduced pollution, the elimination of genetically modified crops and an increase in sustainable agriculture. They also supported free college education and the use of the Transcendental Meditation technique in the school system. In the UK, NLP candidate Geoffrey Clements advocated the use of Transcendental Meditation and the TM-Sidhi program's yogic flying practice to reduce crime and war deaths. In the U.S.A. its platform included clean energy, labeling of genetically modified foods, a ban on the construction of nuclear energy plants, and an end to political action committees.\nNational branches.\nThe Natural Law Party was reported in 1998 to be active in 74 countries including Australia, Austria, Belgium, Canada, Croatia, Finland, France, Germany, India, Ireland, Israel, Italy, New Zealand, the Netherlands, Trinidad and Tobago, the United Kingdom and the United States.\nAustralia.\nIn 1993, Bevan Morris campaigned for a seat in a district in suburban Adelaide for the Australian House of Representatives on the NLP ticket. The party contested several federal and state elections between 1990 and 1998.\nCanada.\nThe Natural Law Party was active in the Canadian federal elections of 1993, 1997 and 2000 and in provincial elections in Ontario and Quebec during this period; it was deregistered in 2003.\nCroatia.\nIn Croatia, a party member was elected to a regional assembly in 1993.\nFrance.\nBeno\u00eet Frapp\u00e9 of France was the party's candidate for the European Parliament.\nGermany.\nBetween 1992 and 2005, a Natural Law Party existed in Germany. Its name was \"Naturgesetz-Partei, Aufbruch zu neuem Bewusstsein\", shortened to NATURGESETZ or Bewusstsein. In 1995 it had around 2,000 members, 150 thereof in Berlin. Its website was https:// (archived version).\nIndia.\nThe Natural Law Party in India is known as the Ajeya Bharat Party (AJBP) or Invincible India Party. It promotes a Vedic way of life. It was formed in late 1998 as the political wing of the Maharishi Vedic Vishwa Prashasan (MVVP (Maharishi Global Administration Through Natural Law)), which had nominated thirty-four candidates in the February 1998 parliamentary election from Madhya Pradesh. The Maharishi was said to be \"keenly interested\" in building a political base in his native province. The MVVP received 0.28% of the vote in its first election. Mukesh Nayak left the cabinet and the Congress Party to assume the leadership of the Madhya Pradesh MVVP. For the November 1998 election, the Ajeya Bharat had a list of 100 candidates for the Assembly. It received 0.5% of the vote and won one seat in the 320-member state assembly. The following year, that member switched parties, leaving the Ajeya Bharat with no representation. In 2008, Nayak left the party to rejoin the Congress Party. In 2009, the Ajeya Bharat Party president, Ambati Krishnamurthy, filed a complaint against another party for using a flag similar to its own.\nIreland.\nThe Natural Law Party became active in Ireland in 1994 and was based in Dublin. The party leader was John Burns, who was one of nine Natural Law Party candidates in the 1997 general election. In addition, there were four candidates in the European elections of 1999. Burns endorsed the alternative health system of Maharishi Vedic Approach to Health and the five European candidates gained about 0.5% of first-preference votes cast. Burns, who also contested the 1999 Dublin South-Central by-election, spent only \u00a3163 on his campaign. After 1999, the party ceased to field candidates in Ireland. The amount of corporate political donations in 2000 was nil.\nIsrael.\nThe Natural Law Party of Israel (, \"Mifleget Hok HaTeva Shel Yisrael\") was a minor political party in Israel. Its leader was Amihai Rokah. In the 1992 elections the Natural Law Party won 1,734 votes (0.06%), and in the 1999 elections, won 2,924 votes (0.09%), both below the then 1.5% electoral threshold required to enter the Knesset. It has not run in an election since and its website states it has ceased political activity, but as of 2018 it is still registered as a party in Israel.\nItaly.\nThe Natural Law Party in Italy (\"Partito della Legge Naturale\", PLN) participated in several (both general and local) elections in the 1990s. In the 1994 general elections it won 24,897 votes (0.06%) for the Chamber of Deputies and 86,588 votes (0.26%) for the Senate. The list was on ballot in a few constituencies only. In the 1996 general elections the Natural Law Party ran candidates only in the Trentino-Alto Adige/S\u00fcdtirol region, who won 8,298 votes for the Chamber of Deputies and 5,842 for the Senate (about 1% on a regional basis, 0.2% in the whole country).\nNew Zealand.\nThe Natural Law Party contested New Zealand general elections such as the 1996 election. It did not win any representation.\nTrinidad and Tobago.\nThe Natural Law Party in Trinidad and Tobago contested the 1995 general elections. It received 1,590 votes, but failed to win a seat.\nUnited Kingdom.\nThe Natural Law Party was founded in the United Kingdom in March 1992. Geoffrey Clements was its leader.\nThe UK manifesto, as published on its website, listed five key aspects of a successful government including:\nIn the 1992 general election, held on 9 April, the NLP contested 310 seats in the UK, garnering 0.19% of the vote, with every candidate losing their deposit for failing to receive at least 5% of the vote. The group announced that they had budgeted nearly \u00a31 million for the campaign. A significant number of constituencies were contested by nationals of countries outside the UK, including Canada, Australia, New Zealand, and India, as British electoral law allows any member of a Commonwealth country to stand for Parliament. Among them was Canadian-born magician Doug Henning. Despite the \"dismal\" number of votes, an article in \"The Herald\" of Scotland reported that it could be considered a \"reasonable return for a campaign which began only three weeks before polling day.\" In addition the NLP \"notched up\" a \"headline-grabbing record\" when it put forward candidates for all 87 United Kingdom seats in the 1994 European Parliament; the first party to do so.\nGeorge Harrison performed a fund-raising concert at the Royal Albert Hall in London for the NLP on 6 April 1992, his first full concert in the UK since 1969. According to Harrison, a week before the general election, Maharishi Mahesh Yogi suggested to Harrison that he, Paul McCartney and Ringo Starr stand for election as MPs for Liverpool seats as NLP candidates, but they declined.\nIn the 1997 general election, the NLP ran 197 candidates for Parliament in the UK, garnering 0.1% of the vote, with every candidate losing their deposit.\nThe NLP ran 16 candidates in the 20 by-elections held between 1992 and 1997, with every candidate losing their deposit. The NLP ran eight candidates for the 16 by-elections held between 1997 and 2001, averaging 0.10% of the vote, with every candidate losing their deposit. The NLP did not run any candidates for Parliament in the 2001 general election or in the succeeding by-elections. The party, along with its Northern Ireland wing, voluntarily deregistered with the Electoral Commission at the end of 2003.\nNorthern Ireland.\nIt contested its first election in Northern Ireland in the 1994 EU elections. According to the NLP, they prepared a 70-page report in response to the \"1996 Framework Document of the British and Irish governments.\" The report was presented to leaders in Ireland, Northern Ireland and the U.S. Afterwards, NLP representatives participated in the \"special elections to the Northern Ireland Forum\", but withdrew before the election.\nUnited States.\nThe Natural Law Party (United States) ran John Hagelin as its presidential candidate in 1992, 1996, and 2000. He was on ballots in 48 states and received 110,000 votes (0.12%) in 1996. The party also ran congressional and local candidates. In California, psychiatrist Harold H. Bloomfield ran as candidate for governor in 1998. It attempted to merge with the Reform Party in 2000. The NLP in the United States was largely disbanded in 2004. However, some state affiliates, such as Michigan, have kept their ballot positions and allied with other small parties.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21826", "revid": "6727347", "url": "https://en.wikipedia.org/wiki?curid=21826", "title": "Naturalistic fallacy", "text": "Purported fallacy in explaining good reductively\nIn metaethics, the naturalistic fallacy is the claim that it is possible to define good in terms of merely described entities, properties, or processes such as \"pleasant\", \"desirable\", or \"fitness\". The term was introduced by British philosopher G. E. Moore in his 1903 book \"Principia Ethica\".\nMoore's naturalistic fallacy is closely related to the is\u2013ought problem, which comes from David Hume's \"Treatise of Human Nature\" (1738\u201340); however, unlike Hume's view of the is\u2013ought problem, Moore (and other proponents of ethical non-naturalism) did not consider the naturalistic fallacy to be at odds with moral realism.\nCommon uses.\nThe is\u2013ought problem.\nThe term \"naturalistic fallacy\" is sometimes used to label the problematic inference of an \"ought\" from an \"is\" (the is\u2013ought problem). Michael Ridge relevantly elaborates that \"[t]he intuitive idea is that evaluative conclusions require at least one evaluative premise\u2014purely factual premises about the naturalistic features of things do not entail or even support evaluative conclusions.\" This problematic inference usually takes the form of saying that if people generally \"do\" something (e.g., eat three times a day, smoke cigarettes, dress warmly in cold weather), then people \"ought to\" do that thing. The naturalistic fallacy occurs when the is\u2013ought inference (\"People eat three times a day, so it is morally good for people to eat three times a day\") is justified by the claim that whatever practice exists is a natural one (\"because eating three times a day is pleasant and desirable\").\nBentham, in discussing the relations of law and morality, found that when people discuss problems and issues they talk about how they wish it would be, instead of how it actually is. This can be seen in discussions of natural law and positive law. Bentham criticized natural law theory because in his view it was an instance of the naturalistic fallacy, claiming that it described how things are rather than how they ought to be.\nMoore's discussion.\nAccording to G. E. Moore's \"Principia Ethica\", when philosophers try to define \"good\" reductively, in terms of natural properties like \"pleasant\" or \"desirable\", they are committing the naturalistic fallacy.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...the assumption that because some quality or combination of qualities invariably and necessarily accompanies the quality of goodness, or is invariably and necessarily accompanied by it, or both, this quality or combination of qualities is identical with goodness. If, for example, it is believed that whatever is pleasant is and must be good, or that whatever is good is and must be pleasant, or both, it is committing the naturalistic fallacy to infer from this that goodness and pleasantness are one and the same quality. The naturalistic fallacy is the assumption that because the words 'good' and, say, 'pleasant' necessarily describe the same objects, they must attribute the same quality to them.\u2014\u200a\nIn defense of ethical non-naturalism against ethical naturalism, Moore's argument is concerned with the semantic and metaphysical underpinnings of ethics. Moore argues that good, in the sense of intrinsic value, is simply ineffable. It cannot be defined because it is not reducible to other properties, being \"one of those innumerable objects of thought which are themselves incapable of definition, because they are the ultimate terms by reference to which whatever 'is' capable of definition must be defined\". On the other hand, ethical naturalists eschew such principles in favor of a more empirically accessible analysis of what it means to be good: for example, in terms of pleasure in the context of hedonism.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;That \"pleased\" does not mean \"having the sensation of red\", or anything else whatever, does not prevent us from understanding what it does mean. It is enough for us to know that \"pleased\" does mean \"having the sensation of pleasure\", and though pleasure is absolutely indefinable, though pleasure is pleasure and nothing else whatever, yet we feel no difficulty in saying that we are pleased. The reason is, of course, that when I say \"I am pleased\", I do not mean that \"I\" am the same thing as \"having pleasure\". And similarly no difficulty need be found in my saying that \"pleasure is good\" and yet not meaning that \"pleasure\" is the same thing as \"good\", that pleasure \"means\" good, and that good \"means\" pleasure. If I were to imagine that when I said \"I am pleased\", I meant that I was exactly the same thing as \"pleased\", I should not indeed call that a naturalistic fallacy, although it would be the same fallacy as I have called naturalistic with reference to Ethics.\u2014\u200a\nIn \u00a77, Moore argues that a property is either a complex of simple properties, or else it is irreducibly simple. Complex properties can be defined in terms of their constituent parts but a simple property lacks parts. In addition to \"good\" and \"pleasure\", Moore suggests that colour qualia are undefined: if one wants to understand yellow, one must see examples of it. It will do no good to read the dictionary and learn that \"yellow\" names the colour of egg yolks and ripe lemons, or that \"yellow\" names the primary colour between green and orange on the spectrum, or that the perception of yellow is stimulated by electromagnetic radiation with a wavelength of between 570 and 590 nanometers, because yellow is all that and more, by the open question argument.\nAppeal to nature.\nSome people use the phrase, \"naturalistic fallacy\" or \"appeal to nature\", in a different sense, to characterize inferences of the form \"Something is natural; therefore, it is morally acceptable\" or \"This property is unnatural; therefore, this property is undesirable.\" Such inferences are common in discussions of medicine, homosexuality, environmentalism, and veganism.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The naturalistic fallacy is the idea that what is found in nature is good. It was the basis for social Darwinism, the belief that helping the poor and sick would get in the way of evolution, which depends on the survival of the fittest. Today, biologists denounce the naturalistic fallacy because they want to describe the natural world honestly, without people deriving morals about how we ought to behave (as in: If birds and beasts engage in adultery, infanticide, cannibalism, it must be OK).\u2014\u200a\nCriticism.\nBernard Williams called Moore's use of the term \"naturalistic fallacy\" a \"spectacular misnomer\", the matter in question being metaphysical, as opposed to rational.\nSome philosophers reject the naturalistic fallacy or suggest solutions for the proposed is\u2013ought problem.\nBound-up functions.\nRalph McInerny suggests that \"ought\" is already bound up in \"is\", insofar as the very nature of things have ends/goals within them. For example, a clock is a device used to keep time. When one understands the function of a clock, then a standard of evaluation is implicit in the very description of the clock, i.e., because it \"is\" a clock, it \"ought\" to keep the time. Thus, if one cannot pick a good clock from a bad clock, then one does not really know what a clock is. In like manner, if one cannot determine good human action from bad, then one does not really know what the human person is.\nIrrationality of anti-naturalistic fallacy.\nThe belief that naturalistic fallacy is inherently flawed has been criticized as lacking rational bases, and labelled anti-naturalistic fallacy. For instance, Alex Walter wrote:\n\"The naturalistic fallacy and Hume's 'law' are frequently appealed to for the purpose of drawing limits around the scope of scientific inquiry into ethics and morality. These two objections are shown to be without force.\"\nThat is because said beliefs implicitly assert that there is no connection between the facts and the norms (in particular, between the facts and the mental process that led to adoption of the norms). However, philosophers show that these connections are inevitable. \nA very basic example is that if people view rescuing people as morally correct, this would shape their beliefs on what constitutes danger and what situations warrant intervention. For wider-ranging examples, if one believes that a certain ethnic group of humans have a population-level statistical hereditary predisposition to destroy civilization while the other person does not believe that such is the case, that difference in beliefs about factual matters will make the first person conclude that persecution of said ethnic group is an excusable \"necessary evil\" while the second person will conclude that it is a totally unjustifiable evil. \nSimilarly, if two people think it is evil to keep people working extremely hard in extreme poverty, they will draw different conclusions on de facto rights (as opposed to purely semantic rights) of property owners. The latter is dependent on whether they believe property owners are responsible for the aforementioned exploitation. One who accepts this premise would conclude that it is necessary to persecute property owners to mitigate exploitation. The one who does not, on the other hand, would conclude that the persecution is unnecessary and evil.\nInconsistent application.\nSome critics of the assumption that is-ought conclusions are fallacies point at observations of people who purport to consider such conclusions as fallacies do not do so consistently. Examples mentioned are that evolutionary psychologists who gripe about \"the naturalistic fallacy\" do make is-ought conclusions themselves when, for instance, alleging that the notion of the blank slate would lead to totalitarian social engineering or that certain views on sexuality would lead to attempts to convert homosexuals to heterosexuals. Critics point at this as a sign that charges of the naturalistic fallacy are inconsistent rhetorical tactics rather than detection of a fallacy.\nUniversally normative allegations of varied harm.\nA criticism of the concept of the naturalistic fallacy is that while \"descriptive\" statements (used here in the broad sense about statements that purport to be about facts regardless of whether they are true or false, used simply as opposed to normative statements) about specific differences in effects can be inverted depending on values (such as the statement \"people X are predisposed to eating babies\" being normative against group X only in the context of protecting children while the statement \"individual or group X is predisposed to emit greenhouse gases\" is normative against individual/group X only in the context of protecting the environment), the statement \"individual/group X is predisposed to harm whatever values others have\" is universally normative against individual/group X. This refers to individual/group X being \"descriptively\" alleged to detect what other entities capable of valuing are protecting and then destroying it without individual/group X having any values of its own. For example, in the context of one philosophy advocating child protection considering eating babies the worst evil and advocating industries that emit greenhouse gases to finance a safe short term environment for children while another philosophy considers long term damage to the environment the worst evil and advocates eating babies to reduce overpopulation and with it consumption that emits greenhouse gases, such an individual/group X could be alleged to advocate both eating babies and building autonomous industries to maximize greenhouse gas emissions, making the two otherwise enemy philosophies become allies against individual/group X as a \"common enemy\". The principle, that of allegations of an individual or group being predisposed to adapt their harm to damage any values including combined harm of apparently opposite values inevitably making normative implications regardless of which the specific values are, is argued to extend to any other situations with any other values as well due to the allegation being of the individual or group adapting their destruction to different values. This is mentioned as an example of at least one type of \"descriptive\" allegation being bound to make universally normative implications, as well as the allegation not being scientifically self-correcting due to individual or group X being alleged to manipulate others to support their alleged all-destructive agenda which dismisses any scientific criticism of the allegation as \"part of the agenda that destroys everything\", and that the objection that some values may condemn some specific ways to persecute individual/group X is irrelevant since different values would also have various ways to do things against individuals or groups that they would consider acceptable to do. This is pointed out as a falsifying counterexample to the claim that \"no descriptive statement can in itself become normative\".\nNon-synonymous properties.\nIn 1939, William Frankena critiqued G. E. Moore's conception of the naturalistic fallacy, claiming the concept was an instance of a \"definist fallacy\". Frankena stated that, in arguing that \"good\" cannot be defined by natural properties, Moore was trying to avoid a broader confusion caused by attempting to define a term using non-synonymous properties. \nFrankena also argued that \"naturalistic fallacy\" is a complete misnomer because it is neither limited to naturalistic properties nor necessarily a fallacy. On the first word (\"naturalistic\"), he noted that Moore rejected defining \"good\" in non-natural as well as natural terms. Frankena rejected the idea that the second word (\"fallacy\") represented an error in reasoning \u2013 a fallacy as it is usually recognized \u2013 rather than an error in semantics. \nIn Moore's open-question argument, because questions such as \"Is that which is pleasurable good?\" have no definitive answer, then pleasurable is not synonymous with good. Frankena rejected this argument as: the fact that there is always an open question merely reflects the fact that it makes sense to ask whether two things that may be identical in fact are. Thus, even if good \"were\" identical to pleasurable, it makes sense to ask whether it is; the answer may be \"yes\", but the question was legitimate. This seems to contradict Moore's view which accepts that sometimes alternative answers could be dismissed without argument; however, Frankena objects that this would be committing the fallacy of begging the question.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21828", "revid": "38878333", "url": "https://en.wikipedia.org/wiki?curid=21828", "title": "Neapolitan ice cream", "text": "Ice cream composed of vanilla, chocolate, and strawberry flavours\nNeapolitan ice cream, also sometimes referred to as Harlequin ice cream, is an ice cream composed of three flavors (typically vanilla, chocolate, and strawberry) arranged side by side. Although Neapolitan is associated with Naples in Italy, it was first recorded in Prussia in 1839.\nHistory.\nNeapolitan ice cream was the first ice cream recipe to combine three flavors. As of 2020, the origins of the recipe are unclear.\nIn 1839, head chef of the royal Prussian household Louis Ferdinand Jungius published a layered fruit-flavored ice cream recipe named after the Bad Muskau nobleman F\u00fcrst P\u00fcckler, suggesting strawberries, raspberries, Reine Claude greengages, red and black cherries, and apricots, adding that with liqueurs and maraschino it could be incorporated with caramel, rose liqueur and coffee layers In 1862, he suggested apricots, quinces, raspberries and strawberries. In 1903, an illustration shows three layers colored top to bottom white, red and brown, as well as the Kaffee K\u00f6nig original recipe in Bad Muskau with respectively coco with maraschino, strawberries and chocolate flavors, all also containing macaroon pieces with maraschino, in 1920.\nThe English-language name of Neapolitan arose in the late 19th century due to confusion about its origin given Italy's reputation for ice cream or because its colors\u2014originally green (pistachio), white (vanilla) and red (cherry)\u2014matched those of the Italian flag. Early recipes featured a variety of flavors, but the combination of chocolate, vanilla, and strawberry became the standard, likely because these were the most popular flavors in the United States at the time of its introduction.\nQuotes from food historians.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Cosmopolitan slice. A slice of ice-cream cake made with mousse mixture and ordinary ice cream, presented in a small pleated paper case. Neapolitan ice cream consists of three layers, each of a different colour and flavour (chocolate, strawberry, and vanilla), moulded into a block and cut into slices.\nNeapolitan ice-cream makers were famous in Paris at the beginning of the 19th century, especially Tortoni, creator of numerous ice-cream cakes.\u2014\u200a\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;John F. Mariani, \"The Encyclopedia of American Food and Drink\"\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Stuart Berg Flexner, \"I Hear America Talking\"\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...in a dress of pink and white stripes, strongly resembling Neapolitan ice cream.\u2014\u200a\n19th century descriptions.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;You must have a Neapolitan box for this ice and fill it up in three or four layers with different coloured and flavoured ice creams (a water ice may be used with the custards); for instance, lemon, vanilla, chocolate and pistachio. Mould in the patent ice cave for about 1\u00bd to 2 hours, turn it out, cut it in slices, and arrange neatly on the dish, on a napkin or dish-paper.\u2014\u200a\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;These are prepared by putting ices of various kinds and colors into a mold known as a Neapolitan ice box, which, when set and turned out, is cut into slices suitable for serving. However small the pieces, the block should be cut so that each person gets some of each kind. They are generally laid on a lace paper on an ice plate. Four or five kinds are usually put in the mold, though three sorts will do. The following will serve as a guide in arranging: First, vanilla cream, then raspberry or cherry or currant water; coffee or chocolate in the middle; the strawberry cream, with lemon or orange or pineapple water to finish. A cream ice flavored with any liqueur, a brown bread cream flavored with brandy, with a couple of bright-colored water ices, form another agreeable mixture. Tea cream may be introduced into almost any combination unless coffee were used. Banana cream, pistachio, or almond cream with cherry water and damson or strawberry water are other options.\nThe Neapolitan Ice Spoon has a double use; ice bowl is for putting the mixture into the mold, and the handle is for leveling it. The boxes may be made of tin, which is less expensive than pewter. They are generally sold small enough to make single ices, but these are much more troublesome to prepare. After filling the molds, if there is no cave, 'bed' the ice in the usual way.\u2014\u200a\nCake.\nIn Australia, Neapolitan cake or marble cake is made with the same three colours of Neapolitan ice cream swirled through in a marble pattern, usually topped with pink icing.\nSee also.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21830", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=21830", "title": "Nature", "text": "Material world and its phenomena\nNature is an inherent character or constitution, particularly of the ecosphere or the universe as a whole. In this general sense nature refers to the laws, elements and phenomena of the physical world, including life. Although humans are part of nature, human activity or humans as a whole are often described as at times at odds, or outright separate and even superior to nature.\nDuring the advent of modern scientific method in the last several centuries, nature became the passive reality, organized and moved by divine laws. With the Industrial Revolution, nature increasingly became seen as the part of reality deprived from intentional intervention: it was hence considered as sacred by some traditions (Rousseau, American transcendentalism) or a mere decorum for divine providence or human history (Hegel, Marx). However, a vitalist vision of nature, closer to the pre-Socratic one, got reborn at the same time, especially after Charles Darwin.\nWithin the various uses of the word today, \"nature\" often refers to geology and wildlife. Nature can refer to the general realm of living beings, and in some cases to the processes associated with inanimate objects\u2014the way that particular types of things exist and change of their own accord, such as the weather and geology of the Earth. It is often taken to mean the \"natural environment\" or wilderness\u2014wild animals, rocks, forest, and in general those things that have not been substantially altered by human intervention, or which persist despite human intervention. For example, manufactured objects and human interaction generally are not considered part of nature, unless qualified as, for example, \"human nature\" or \"the whole of nature\". This more traditional concept of natural things that can still be found today implies a distinction between the natural and the artificial, with the artificial being understood as that which has been brought into being by a human consciousness or a human mind. Depending on the particular context, the term \"natural\" might also be distinguished from the or the supernatural.\nEtymology.\nThe word \"nature\" is borrowed from the Old French and is derived from the Latin word , or \"essential qualities, innate disposition\", and in ancient times, literally meant \"birth\". In ancient philosophy, is mostly used as the Latin translation of the Greek word (), which originally related to the intrinsic characteristics of plants, animals, and other features of the world to develop of their own accord. The concept of nature as a whole, the physical universe, is one of several expansions of the original notion; it began with certain core applications of the word by pre-Socratic philosophers (though this word had a dynamic dimension then, especially for Heraclitus), and has steadily gained currency ever since.\nEarth.\nEarth is the only planet known to support life, and its natural features are the subject of many fields of scientific research. Within the Solar System, it is third closest to the Sun; it is the largest terrestrial (rocky) planet and the fifth largest overall. Its most prominent climatic features are its two large polar regions, two relatively narrow temperate zones, and a wide equatorial tropical to subtropical region. Precipitation varies widely with location, from several metres of water per year to less than a millimetre. 71 percent of the Earth's surface is covered by salt-water oceans. The remainder consists of continents and islands, with a majority of the inhabited land in the Northern Hemisphere.\nEarth has evolved through geological and biological processes that have left few traces of the original conditions. The outer surface is divided into several gradually migrating tectonic plates. The interior remains active, with a thick layer of plastic mantle and an iron-filled core that generates a magnetic field. This iron core is composed of a solid inner phase, and a fluid outer phase. Convective motion in the outer core generates electric currents through dynamo action, and these, in turn, generate the geomagnetic field.\nThe atmospheric conditions have been significantly altered from the original conditions by the presence of life-forms, which create an ecological balance that stabilizes the surface conditions. Despite the wide regional variations in climate by latitude and other geographic factors, the long-term average global climate is quite stable during interglacial periods, and variations of a degree or two of average global temperature have historically had major effects on the ecological balance, and on the actual geography of the Earth.\nGeology.\nGeology is the science and study of the solid and liquid matter that constitutes the Earth. The field of geology encompasses the study of the composition, structure, physical properties, dynamics, and history of Earth materials, and the processes by which they are formed, moved, and changed. The field is a major academic discipline, and is also important for mineral and hydrocarbon extraction, knowledge about and mitigation of natural hazards, some Geotechnical engineering fields, and understanding past climates and environments.\nGeological evolution.\nThe geology of an area evolves through time as rock units are deposited and inserted and deformational processes change their shapes and locations.\nRock units are first emplaced either by deposition onto the surface or intrude into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows, blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude.\nAfter the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.\nHistorical perspective.\nEarth is estimated to have formed 4.54\u00a0billion years ago from the solar nebula, along with the Sun and other planets. The Moon formed roughly 20\u00a0million years later. Initially molten, the outer layer of the Earth cooled, resulting in the solid crust. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, most or all of which came from ice delivered by comets, produced the oceans and other water sources. The highly energetic chemistry is believed to have produced a self-replicating molecule around 4\u00a0billion years ago.\nContinents formed, then broke up and reformed as the surface of Earth reshaped over hundreds of millions of years, occasionally combining to make a supercontinent. Roughly 750\u00a0million years ago, the earliest known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia which broke apart about 540\u00a0million years ago, then finally Pangaea, which broke apart about 180\u00a0million years ago.\nDuring the Neoproterozoic era, freezing temperatures covered much of the Earth in glaciers and ice sheets. This hypothesis has been termed the \"Snowball Earth\", and it is of particular interest as it precedes the Cambrian explosion in which multicellular life forms began to proliferate about 530\u2013540\u00a0million years ago.\nSince the Cambrian explosion there have been five distinctly identifiable mass extinctions. The last mass extinction occurred some 66 million years ago, when a meteorite collision probably triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared small animals such as mammals. Over the past 66\u00a0million years, mammalian life diversified.\nSeveral million years ago, a species of small African ape gained the ability to stand upright. The subsequent advent of human life, and the development of agriculture and further civilization allowed humans to affect the Earth more rapidly than any previous life form, impacting both the nature and quantity of other organisms as well as global climate. By comparison, the Great Oxygenation Event, produced by the proliferation of algae during the Siderian period, required about 400\u00a0million years to culminate.\nThe present era is classified as part of a mass extinction event, the Holocene extinction event, the fastest ever to have occurred. Some, such as E. O. Wilson of Harvard University, predict that human destruction of the biosphere could cause the extinction of one-half of all species in the next 100\u00a0years. The extent of the current extinction event is still being researched, debated and calculated by biologists.\nAtmosphere, climate, and weather.\nThe Earth's atmosphere is a key factor in sustaining the ecosystem. The thin layer of gases that envelops the Earth is held in place by gravity. Air is mostly nitrogen, oxygen, water vapor, with much smaller amounts of carbon dioxide, argon, etc. The atmospheric pressure and density declines steadily with altitude. The ozone layer plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes.\nTerrestrial weather occurs almost exclusively in the lower part of the atmosphere, and serves as a convective system for redistributing heat. Weather is a chaotic system that is readily modified by small changes to the environment, so accurate weather forecasting is limited to only a few days. Weather is also influenced by the seasons, which result from the Earth's axis being tilted relative to its orbital plane. Thus, at any given time during the summer or winter, one part of the Earth is more directly exposed to the rays of the sun. This exposure alternates as the Earth revolves in its orbit. At any given time, regardless of season, the Northern and Southern Hemispheres experience opposite seasons.\nWeather can have both beneficial and harmful effects. Lightning strikes can cause wildfires, while heavy rain can cause flooding and mud slides. Extremes in weather, such as tornadoes or hurricanes and cyclones, can expend large amounts of energy along their paths, and produce devastation. Surface vegetation has evolved a dependence on the seasonal variation of the weather, and sudden changes lasting only a few years can have a stress effect on the plants. These pose a threat to the animals that depend on its growth for their food.\nClimate is a measure of the long-term trends in the weather. Various factors are known to influence the climate, including ocean currents, surface albedo, greenhouse gases, variations in the solar luminosity, and changes to the Earth's orbit. Based on historical and geological records, the Earth is known to have undergone drastic climate changes in the past, including ice ages. In the present day, two things are happening worldwide: (1) temperature is increasing on the average; and (2) regional climates have been undergoing noticeable changes.\nOcean currents are an important factor in determining climate, particularly the major underwater thermohaline circulation which distributes heat energy from the equatorial oceans to the polar regions. These currents help to moderate the differences in temperature between winter and summer in the temperate zones. Also, without the redistributions of heat energy by the ocean currents and atmosphere, the tropics would be much hotter, and the polar regions much colder.\nThe climate of a region depends on a number of factors, including topology, prevailing winds, proximity to a large body of water, and especially latitude. A latitudinal band of the surface with similar climatic attributes forms a climate region. There are a number of such regions, ranging from the tropical climate at the equator to the polar climate in the northern and southern extremes. The latter regions are typically below the freezing temperature of water for much of the year, which can allow frozen water to accumulate in ice caps and thereby changing the surface albedo.\nWater on Earth.\nWater is a chemical substance that is composed of hydrogen and oxygen (H2O) and is vital for all known forms of life. In typical usage, \"water\" refers only to its liquid form, but it also has a solid state, ice, and a gaseous state, water vapor, or steam. Water covers 71% of the Earth's surface. On Earth, it is found mostly in oceans and other large bodies of water, with 1.6% of water below ground in aquifers and 0.001% in the air as vapor, clouds, and precipitation. Oceans hold 96.5% of surface water; glaciers and polar ice caps, 2.4%; and other land surface water such as rivers, lakes, ponds, underground aquifers, and groundwater, 1%. The smallest freshwater reserve is the 0.1% in the atmosphere. Through subduction processes in the Earth's crust, an equivalent mass of the planet's surface water has been interred in the upper mantle alone.\nOceans.\nAn ocean is a major body of saline water, and a principal component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 361 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. This is a fundamental concept in oceanography: a global-spanning ocean that functions as a continuous body of water with relatively free interchange among its bodies.\nThe major oceanic divisions are determined by the various continents, archipelagos, and other criteria. In descending order of size, they are the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean, and the Arctic Ocean. Smaller regions of the oceans are called seas, gulfs, bays and other names. There are also salt lakes, which are smaller bodies of landlocked saltwater that are not interconnected with the World Ocean. Two notable examples of salt lakes are the Great Salt Lake and the Caspian Sea. No other planet in the Solar System has surface oceans, although there are 15 moons that are suspected of having ice-covered oceans.\nLakes and ponds.\nA lake (from Latin word \"lacus\") is a terrain feature (or physical feature), a body of liquid on the surface of a world that is localized to the bottom of basin (another type of landform or terrain feature; that is, it is not global) and moves slowly if it moves at all. On Earth, a body of water is considered a lake when it is inland, not part of the ocean, is larger and deeper than a pond, and is fed by a river.\nThe only world other than Earth known to harbor lakes is Titan, Saturn's largest moon, which has lakes of ethane, most likely mixed with methane. It is not known if Titan's lakes are fed by rivers, though Titan's surface is carved by numerous river beds. Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins, along the courses of mature rivers, or human-made reservoirs behind dams. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last ice age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.\nSmall bodies of standing water, typically less than , are termed a pond or pool. They can be natural or human-made. A wide variety of human-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams via current speed. While currents in streams are easily observed, ponds possess thermally driven micro-currents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.\nRivers and streams.\nA river is a natural watercourse, usually freshwater, flowing towards an ocean, a lake, a sea or another river. In a few cases, a river simply flows into the ground or dries up completely before reaching another body of water. A river is part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of stored water in natural ice and snowpacks (i.e., from glaciers). Where a river merges with a slow-moving body of water, the deposited sedimentation can build up to form a delta.\nThere is no general rule that defines what can be called a river. Smaller scale water flows with a steady current are termed a stream, creek, brook, rivulet, or rill. These are confined within a stream bed and bank. Many names for small rivers are specific to geographic location; one example is \"Burn\" in Scotland and North-east England. In US naming, sometimes a river is said to be larger than a creek, but this is not always the case, due to vagueness in the language; consequently the US Geographic Names Information System calls all \"linear flowing bodies of water\" \"streams\".\nStreams are important as conduits in the water cycle, instruments in groundwater recharge, and they serve as corridors for fish and wildlife migration. The biological habitat in the immediate vicinity of a stream is called a riparian zone. Given the status of the ongoing Holocene extinction, streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general involves many branches of inter-disciplinary natural science and engineering, including hydrology, fluvial geomorphology, aquatic ecology, fish biology, riparian ecology, and others.\nEcosystems.\nEcosystems are composed of a variety of biotic and abiotic components that function in an interrelated way. The structure and composition is determined by various environmental factors that are interrelated. Variations of these factors will initiate dynamic modifications to the ecosystem. Some of the more important components are soil, atmosphere, radiation from the sun, water, and living organisms.\nCentral to the ecosystem concept is the idea that living organisms interact with every other element in their local environment. Eugene Odum, a founder of ecology, stated: \"Any unit that includes all of the organisms (i.e.: the \"community\") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.\" Within the ecosystem, species are connected and dependent upon one another in the food chain, and exchange energy and matter between themselves as well as with their environment. The human ecosystem concept is based on the human/nature dichotomy and the idea that all species are ecologically dependent on each other, as well as with the abiotic constituents of their biotope.\nA smaller unit of size is called a microecosystem. For example, a microsystem can be a stone and all the life under it. A \"macroecosystem\" might involve a whole ecoregion, with its drainage basin.\nWilderness.\nWilderness is generally defined as areas that have not been significantly modified by human activity. Wilderness areas can be found in preserves, estates, farms, conservation preserves, ranches, , national parks, and even in urban areas along rivers, gulches, or otherwise undeveloped areas. Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, and solitude. Some nature writers believe wilderness areas are vital for the human spirit and creativity, and some ecologists consider wilderness areas to be an integral part of the Earth's self-sustaining natural ecosystem (the biosphere). They may also preserve historic genetic traits and that they provide habitat for wild flora and fauna that may be difficult or impossible to recreate in zoos, arboretums, or laboratories.\nLife.\nAlthough there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli, and reproduction. Life may also be said to be simply the characteristic state of organisms. The latter can then be defined in terms of biochemistry, genetics, or thermodynamics. Properties common to terrestrial organisms (plants, animals, fungi, protists, archaea, and bacteria) are that they are cellular and based on a complex chemical organization. However, not every definition of life considers these properties to be essential. Human-made analogs of life may also be considered to be life.\nPresent day organisms from viruses to humans possess a self-replicating informational molecule (genome), either DNA or RNA (as in some viruses), and such an informational molecule is probably intrinsic to life. It is likely that the earliest forms of life were based on a self-replicating informational molecule (genome), perhaps RNA or a molecule more primitive than RNA or DNA. The specific nucleotide sequence in each organism contains information that functions to promotes survival, reproduction, and the capacity to acquire resources necessary for reproduction; such sequences probably emerged early in the evolution of life. Survival functions present early in the evolution of life likely also included genomic sequences that promote the avoidance of damage to the self-replicating molecule and also the capability to repair such damages that do occur. Repair of some genome damages may have involved using information from another similar molecule by a process of recombination (a primitive form of sexual interaction).\nThe biosphere is the part of Earth's outer shell\u2014including land, surface rocks, water, air and the atmosphere\u2014within which life occurs, and which biotic processes in turn alter or transform. From the broadest geophysiological point of view, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere (rocks), hydrosphere (water), and atmosphere (air). The entire Earth contains over 75\u00a0billion tons (150 \"trillion\" pounds or about ) of biomass (life), which lives within various environments within the biosphere.\nOver nine-tenths of the total biomass on Earth is plant life, on which animal life depends very heavily for its existence. More than 2 million species of plant and animal life have been identified to date, and estimates of the actual number of existing species range from several million to well over 50\u00a0million. The number of individual species of life is constantly in some degree of flux, with new species appearing and others ceasing to exist on a continual basis. The total number of species is in rapid decline.\nEvolution.\nThe origin of life on Earth is not well understood, but it is known to have occurred at least 3.5\u00a0billion years ago, during the hadean or archean eons on a primordial Earth that had a substantially different environment than is found at present. These life forms possessed the basic traits of self-replication and inheritable traits. Once life had appeared, the process of evolution by natural selection resulted in the development of ever-more diverse life forms.\nSpecies that were unable to adapt to the changing environment and competition from other life forms became extinct. However, the fossil record retains evidence of many of these older species. Current fossil and DNA evidence shows that all existing species can trace a continual ancestry back to the first primitive life forms.\nWhen basic forms of plant life developed the process of photosynthesis the sun's energy could be harvested to create conditions which allowed for more complex life forms. The resultant oxygen accumulated in the atmosphere and gave rise to the ozone layer. The incorporation of smaller cells within larger ones resulted in the development of yet more complex cells called eukaryotes. Cells within colonies became increasingly specialized, resulting in true multicellular organisms. With the ozone layer absorbing harmful ultraviolet radiation, life colonized the land surface of Earth.\nMicrobes.\nThe first form of life to develop on the Earth were unicellular, and they remained the only form of life until about a billion years ago when multi-cellular organisms began to appear. Microorganisms or microbes are microscopic, and smaller than the human eye can see. Microorganisms can be single-celled, such as Bacteria, Archaea, many Protista, and a minority of Fungi.\nThese life forms are found in almost every location on the Earth where there is liquid water, including in the Earth's interior.\nTheir reproduction is both rapid and profuse. The combination of a high mutation rate and a horizontal gene transfer ability makes them highly adaptable, and able to survive in new and sometimes very harsh environments, including outer space. They form an essential part of the planetary ecosystem. However, some microorganisms are pathogenic and can post health risk to other organisms.\nViruses are infectious agents, but they are not autonomous life forms, as it is the case for viroids, satellites, DPIs and prions.\nPlants and animals.\nOriginally Aristotle divided all living things between plants, which generally do not move fast enough for humans to notice, and animals. In Linnaeus' system, these became the kingdoms Vegetabilia (later Plantae) and Animalia. Since then, it has become clear that the Plantae as originally defined included several unrelated groups, and the fungi and several groups of algae were removed to new kingdoms. However, these are still often considered plants in many contexts. Bacterial life is sometimes included in flora, and some classifications use the term \"bacterial flora\" separately from \"plant flora\".\nAmong the many ways of classifying plants are by regional floras, which, depending on the purpose of study, can also include \"fossil flora\", remnants of plant life from a previous era, including pollen. People in many regions and countries take great pride in their individual arrays of characteristic flora, which can vary widely across the globe due to differences in climate and terrain.\nRegional floras commonly are divided into categories such as \"native flora\" or \"agricultural and garden flora\". Some types of \"native flora\" actually have been introduced centuries ago by people migrating from one region or continent to another, and become an integral part of the native, or natural flora of the place to which they were introduced. These invasive species are examples of how human interaction with the ecosystem can blur the boundary of what is considered nature.\nAnother category of plant has historically been carved out for \"weeds\". Though the term has fallen into disfavor among botanists as a formal way to categorize \"useless\" plants, the informal use of the word \"weeds\" to describe those plants that are deemed worthy of elimination is illustrative of the general tendency of people and societies to seek to alter or shape the course of nature. Similarly, animals are often categorized in ways such as \"domestic\", \"laboratory\", \"farm animals\", \"wild animals\", \"pests\", etc. according to their relationship to human life.\nAnimals as a category have several characteristics that generally set them apart from other living things. Animals are eukaryotic and usually multicellular, which separates them from bacteria, archaea, and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking cell walls.\nWith a few exceptions\u2014most notably the two phyla consisting of sponges and placozoans\u2014animals have bodies that are differentiated into tissues. These include muscles, which are able to contract and control locomotion, and a nervous system, which sends and processes signals. There is also typically an internal digestive chamber. The eukaryotic cells possessed by all animals are surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules, a framework upon which cells can move about and be reorganized during development and maturation, and which supports the complex anatomy required for mobility.\nHuman interrelationship.\nHuman impact.\nAlthough humans comprise a minuscule proportion of the total living biomass on Earth, the human effect on nature is disproportionately large. Because of the extent of human influence, the boundaries between what humans regard as nature and \"made environments\" is not clear cut except at the extremes. Even at the extremes, the amount of natural environment that is free of discernible human influence is diminishing at an increasingly rapid pace. A 2020 study published in \"Nature\" found that anthropogenic mass (human-made materials) outweighs all living biomass on earth, with plastic alone exceeding the mass of all land and marine animals combined. And according to a 2021 study published in \"Frontiers in Forests and Global Change\", only about 3% of the planet's terrestrial surface is ecologically and faunally intact, with a low human footprint and healthy populations of native animal species. Philip Cafaro, professor of philosophy at the School of Global Environmental Sustainability at Colorado State University, wrote in 2022 that \"the cause of global biodiversity loss is clear: other species are being displaced by a rapidly growing human economy.\"\nThe development of technology by the human race has allowed the greater exploitation of natural resources and has helped to alleviate some of the risk from natural hazards. However, in spite of this progress, the fate of human civilization remains closely linked to changes in the environment. There exists a highly complex feedback loop between the use of advanced technology and changes to the environment. Human-made threats to the Earth's natural environment include pollution, deforestation, and disasters such as oil spills. Humans have contributed to the extinction of many plants and animals, with roughly 1 million species threatened with extinction within decades. The loss of biodiversity and ecosystem functions over the last half century have impacted the extent that nature can contribute to human quality of life, and continued declines could pose a major threat to the existence of human civilization, unless a rapid course correction is made. The value of natural resources to society is often poorly reflected in market prices, because whilst there are extraction costs, natural resources themselves are typically available free of charge. This distorts market pricing of natural resources and at the same time leads to underinvestment in our natural assets. The annual global cost of public subsidies that damage nature is conservatively estimated at $4\u20136 trillion (million million). Institutional protections of these natural goods, such as the oceans and rainforests, are lacking. Governments have not prevented these economic externalities.\nHumans employ nature for both leisure and economic activities. The acquisition of natural resources for industrial use remains a sizable component of the world's economic system. Some activities, such as hunting and fishing, are used for both sustenance and leisure, often by different people. Agriculture was first adopted around the 9th millennium BCE. Ranging from food production to energy, nature influences economic wealth.\nAlthough early humans gathered uncultivated plant materials for food and employed the medicinal properties of vegetation for healing, most modern human use of plants is through agriculture. The clearance of large tracts of land for crop growth has led to a significant reduction in the amount available of forestation and wetlands, resulting in the loss of habitat for many plant and animal species as well as increased erosion.\nAesthetics and beauty.\nBeauty in nature has historically been a prevalent theme in art and books, filling large sections of libraries and bookstores. That nature has been depicted and celebrated by so much art, photography, poetry, and other literature shows the strength with which many people associate nature and beauty. Reasons why this association exists, and what the association consists of, are studied by the branch of philosophy called aesthetics. Beyond certain basic characteristics that many philosophers agree about to explain what is seen as beautiful, the opinions are virtually endless. Nature and wildness have been important subjects in various eras of world history. An early tradition of landscape art began in China during the Tang Dynasty (618\u2013907). The tradition of representing nature \"as it is\" became one of the aims of Chinese painting and was a significant influence in Asian art.\nAlthough natural wonders are celebrated in the Psalms and the Book of Job, in the West, wilderness portrayals in art became more prevalent in the 1800s, especially in the works of the Romantic movement. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Before that, paintings had been primarily of religious scenes or of human beings. William Wordsworth's poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture. This artistic movement also coincided with the Transcendentalist movement in the Western world. A common classical idea of beautiful art involves the word mimesis, the imitation of nature. Also in the realm of ideas about beauty in nature is that the perfect is implied through perfect mathematical forms and more generally by patterns in nature. As David Rothenburg writes, \"The beautiful is the root of science and the goal of art, the highest possibility that humanity can ever hope to see\".\nMatter and energy.\nMatter is defined as a substance that has mass and takes up a volume of space, while energy is a property that can make matter perform work. At the quantum mechanical scale of the very tiny, both matter and energy exibit the property of wave\u2013particle duality, and they are related to each other through mass\u2013energy equivalence. Matter constitutes the observable universe, which is made visible by the radiation of energy waves. The visible components of the universe are now believed to compose only 4.9 percent of the total mass. The remainder is in an unknown form that is believed to consist of 26.8 percent cold dark matter and 68.3 percent dark energy. The exact nature of these unseen components is under intensive investigation by physicists.\nThe behaviour of matter and energy throughout the observable universe appears to follow well-defined physical laws, or laws of nature, which scientists seek to understand. These laws have been employed to produce cosmological models that successfully explain the structure and the evolution of the universe we can observe. The mathematical expressions of the laws of physics employ a set of twenty physical constants that appear to be static across the observable universe. The values of these constants have been carefully measured, but the reason for their specific values remains a mystery. The anthropic principle argues that the physical constants have the observed values precisely because intelligent life is here to observe them.\nBeyond Earth.\nOuter space, also simply called \"space\", refers to the relatively empty regions of the universe outside the atmospheres of celestial bodies. \"Outer\" space is used to distinguish it from airspace (and terrestrial locations). There is no discrete boundary between Earth's atmosphere and space, as the atmosphere gradually attenuates with increasing altitude. Outer space within the Solar System is called interplanetary space, which passes over into interstellar space at what is known as the heliopause.\nOuter space is saturated by blackbody radiation left over from the Big Bang and the origin of the universe. It contains a near-perfect vacuum of predominantly hydrogen and helium plasma, and is permeated by electromagnetic radiation, magnetic fields, and cosmic rays; the latter include various ionized atomic nuclei and subatomic particles. Regions enriched by matter expelled by stars is sparsely filled with dust and numerous types of organic molecules discovered to date by microwave spectroscopy. Near the Earth, there are signs of human life in outer space today, such as material left over from previous crewed and uncrewed launches which are a potential hazard to spacecraft. Some of this debris re-enters the atmosphere periodically.\nAt the largest scale, the visible universe follows the Cosmological principle, appearing uniformly isotropic and homogeneous in all directions. On smaller scales, observable matter is organized in a hierarchy of structures due to the cumulative effect of gravity. Stars are formed in galaxy structures that typically span up to 100,000\u00a0light years in scale. These in turn are organized in larger scale galaxy clusters and groups spanning tens of millions of light years, then superclusters that extend hundreds of millions of light years across. The largest known structures are the galaxy filaments that link together superclusters. In the open regions between these structures are vast, nearly empty voids. Individual galaxies have numerous groupings of stars called clusters. All stars can appear individually or in hierarchical systems of co-orbiting stars. Each star can have orbiting sub-stellar bodies at various scales: brown dwarfs, exoplanets, moons, asteroids and comets, down to meteoroids.\nA major question in astronomy concerns the existence of life elsewhere in the universe. Although Earth is the only body within the Solar System known to support life, evidence suggests that in the distant past the planet Mars possessed bodies of liquid water on the surface. For a brief period in Mars' history, it may have also been capable of forming life. At present though, most of the water remaining on Mars is frozen.\nIf life exists at all on Mars, it is most likely to be located underground where liquid water can still exist. Conditions on the other terrestrial planets, Mercury and Venus, appear to be too harsh to support life as we know it. But it has been conjectured that Europa, the fourth-largest moon of Jupiter, may possess a sub-surface ocean of liquid water and could potentially host life. Astronomers have discovered extrasolar Earth analogs \u2013 planets that lie in the habitable zone of space surrounding a star, and therefore could possibly host life. However the requirements for life are not completely known and astronomical observations provide limited information.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nMedia:\nOrganizations:\nPhilosophy:\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21832", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=21832", "title": "New Moon", "text": ""}
{"id": "21833", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=21833", "title": "New moon", "text": "First lunar phase, the definition varies\nIn astronomy, the new moon is the first lunar phase, when the Moon and Sun have the same ecliptic longitude. At this phase, the lunar disk is not visible to the naked eye, except when it is silhouetted against the Sun during a solar eclipse.\nThe original meaning of the term 'new moon', which is still sometimes used in calendrical, non-astronomical contexts, is the first visible crescent of the Moon after conjunction with the Sun. This thin waxing crescent is briefly and faintly visible as the Moon gets lower in the western sky after sunset, with the smallest arc angle possible between 5\u20137\u00b0. The precise time and even the date of the appearance of the new moon by this definition will be influenced by the geographical location of the observer. The first crescent marks the beginning of the month in the Islamic calendar and in some lunisolar calendars such as the Hebrew calendar. In the Chinese calendar, the beginning of the month is marked by the last visible crescent of a waning Moon.\nThe astronomical new moon occurs by definition at the moment of conjunction in ecliptical longitude with the Sun when the Moon is invisible from the Earth. This moment is unique and does not depend on location, and in certain circumstances, it coincides with a solar eclipse.\nA lunation, or synodic month, is the period from one new moon to the next. At the J2000.0 epoch, the average length of a lunation is 29.53059 days (or 29 days, 12 hours, 44 minutes, and 3 seconds). However, the length of any one synodic month can vary from 29.26 to 29.80 days (12.96 hours) due to the perturbing effects of the Sun's gravity on the Moon's eccentric orbit.\nLunation number.\nThe \"Lunation Number\" or \"Lunation Cycle\" is a number given to each lunation beginning from a specific one in history. Several conventions are in use.\nThe most commonly used was the Brown Lunation Number (BLN), which defines \"lunation 1\" as beginning at the first new moon of 1923, the year when Ernest William Brown's lunar theory was introduced in the American Ephemeris and Nautical Almanac. Lunation 1 occurred at approximately 02:41 UTC, 17 January 1923. With later refinements, the BLN was used in almanacs until 1983.\nA more recent lunation number\u00a0\u2013 called the Lunation Number (LN)\u00a0\u2013 was introduced by Jean Meeus in 1998, and defines lunation 0 as beginning on the first new moon of 2000 (this occurred at approximately 18:14 UTC, 6 January 2000). The formula relating Meeus's Lunation Number to the Brown Lunation Number is BLN = LN + 953.\nThe Goldstine Lunation Number (GLN) refers to the lunation numbering used by Herman Goldstine, with lunation 0 beginning on 11 January 1001 BCE, and can be calculated using GLN = LN + 37105.\nThe Hebrew Lunation Number (HLN) is the count of lunations in the Hebrew calendar with lunation 1 beginning on 6 October 3761 BCE. It can be calculated using HLN = LN + 71234.\nThe Islamic Lunation Number (ILN) is the count of lunations in the Islamic Calendar with lunation 1 as beginning on the first day of the month of Muharram, which occurred in 622\u00a0 CE (15 July, Julian, in the proleptic reckoning). It can be calculated using ILN = LN + 17038.\nThe Thai Lunation Number (TLN) is called \"\u0e21\u0e32\u0e2a\u0e40\u0e01\u0e13\u0e11\u0e4c\" (Maasa-Kendha), defines lunation 0 as the beginning of Burmese era of the Buddhist calendar on Sunday, 22 March 638 CE. It can be calculated using TLN = LN + 16843.\nLunisolar calendars.\nHebrew calendar.\nThe new moon, in Hebrew Rosh Chodesh, signifies the start of every Hebrew month and is considered an important date and minor holiday in the Hebrew calendar. The modern form of the calendar practiced in Judaism is a rule-based lunisolar calendar, akin to the Chinese calendar, measuring months defined in lunar cycles as well as years measured in solar cycles, and distinct from the purely lunar Islamic calendar and the predominantly solar Gregorian calendar. The Jewish months are fixed to the annual seasons by setting the new moon of Aviv, the \"barley ripening\", or \"spring\", as the first moon and head of the year. Since the Babylonian captivity, this month is called Nisan, and it is calculated based on mathematical rules designed to ensure that festivals are observed in their traditional season. Passover always falls in the springtime. This fixed lunisolar calendar follows rules introduced by Hillel II and refined until the ninth century. This calculation makes use of a mean lunation length used by Ptolemy and handed down from Babylonians, which is still very accurate: ca. 29.530594 days vs. a present value (see ) of 29.530589 days. This difference of only 0.000005, or five millionths of a day, adds up to about only four hours since Babylonian times.\nChinese calendar.\nThe new moon is the beginning of the month in the Chinese calendar. Some Buddhist Chinese keep a vegetarian diet on the new moon and full moon each month.\nHindu calendar.\nThe new moon is significant in the lunar Hindu calendar. The first day of the calendar starts the day after the dark moon phase (Amavasya).\nThere are fifteen moon dates for each of the waxing and waning periods. These fifteen dates are divided evenly into five categories: Nanda, Bhadra', Jaya, Rikta, and Purna, which are cycled through in that order.\nNanda dates are considered to be favorable for auspicious works; Bhadra dates for works related to community, social, family, and friends; and Jaya dates for dealing with conflict. Rikta dates are considered beneficial only for works related to cruelty. Purna dates are considered to be favorable for all work.\nLunar calendars.\nIslamic calendar.\nThe lunar Hijri calendar has exactly 12 lunar months in a year of 354 or 355 days. It has retained an observational definition of the new moon, marking the new month when the first crescent moon is seen, and making it impossible to be certain in advance of when a specific month will begin (in particular, the exact date on which the month of Ramadan will begin is not known in advance). In Saudi Arabia, the new King Abdullah Centre for Crescent Observations and Astronomy in Mecca has a clock for addressing this as an international scientific project. In Pakistan, there is a \"Central Ruet-e-Hilal Committee\" whose head is Mufti Muneeb-ur-Rehman, assisted by 150 observatories of the Pakistan Meteorological Department, which announces the sighting of the new moon.\nAn attempt to unify Muslims on a scientifically calculated worldwide calendar was adopted by both the Fiqh Council of North America and the European Council for Fatwa and Research in 2007. The new calculation requires that conjunction must occur before sunset in Mecca, Saudi Arabia, and that, on the same evening, the moonset must take place after sunset. These can be precisely calculated and therefore a unified calendar is possible should it become adopted worldwide.\nSolar calendars holding moveable feasts.\nBah\u00e1\u02bc\u00ed calendar.\nThe Bah\u00e1\u02bc\u00ed calendar is a solar calendar with certain new moons observed as moveable feasts.\nIn the Bah\u00e1\u02bc\u00ed Faith, effective from 2015 onwards, the \"Twin Holy Birthdays\", refer to two successive holy days in the Bah\u00e1\u02bc\u00ed calendar (the birth of the B\u00e1b and the birth of Bah\u00e1'u'll\u00e1h), will be observed on the first and the second day following the occurrence of the eighth new moon after Naw-R\u00faz (Bah\u00e1\u02bc\u00ed New Year), as determined in advance by astronomical tables using Tehran as the point of reference. This will result in the observance of the Twin Birthdays moving, year to year, from mid-October to mid-November according to the Gregorian calendar.\nChristian liturgical calendar.\nEaster, the most important feast in the Christian liturgical calendar, is a movable feast. The date of Easter is determined by reference to the ecclesiastical full moon, which, being historically difficult to determine with precision, is defined as being fourteen days after the (first crescent) new moon.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21834", "revid": "47051508", "url": "https://en.wikipedia.org/wiki?curid=21834", "title": "B-spline", "text": "Spline function\nIn numerical analysis, a B-spline (short for basis spline) is a type of spline function designed to have minimal support (overlap) for a given degree, smoothness, and set of breakpoints (knots that partition its domain), making it a fundamental building block for all spline functions of that degree. A B-spline is defined as a piecewise polynomial of order formula_1, meaning a degree of formula_2. It is built from sections that meet at these knots, where the continuity of the function and its derivatives depends on how often each knot repeats (its multiplicity). Any spline function of a specific degree can be uniquely expressed as a linear combination of B-splines of that degree over the same knots, a property that makes them versatile in mathematical modeling. A special subtype, cardinal B-splines, uses equidistant knots.\nThe concept of B-splines traces back to the 19th century, when Nikolai Lobachevsky explored similar ideas at Kazan University in Russia, though the term \"B-spline\" was coined by Isaac Jacob Schoenberg in 1967, reflecting their role as basis functions.\nB-splines are widely used in fields like computer-aided design (CAD) and computer graphics, where they shape curves and surfaces through a set of control points, as well as in data analysis for tasks like curve fitting and numerical differentiation of experimental data. From designing car bodies to smoothing noisy measurements, B-splines offer a flexible way to represent complex shapes and functions with precision.\nDefinition.\nA B-spline of order formula_3 is a collection of piecewise polynomial functions formula_4 of degree formula_5 in a variable formula_6.\nThe values of formula_6 where the pieces of polynomial meet are known as knots, denoted formula_8 and sorted into nondecreasing order.\nFor a given sequence of knots, there is, up to a scaling factor, a unique spline formula_4 satisfying\nformula_10\nIf we add the additional constraint that \nformula_11 \nfor all formula_6 between the knots formula_13 and formula_14, then the scaling factor of formula_4 becomes fixed. \nThe knots in-between (and not including) formula_13 and formula_14 are called the internal knots.\nB-splines can be constructed by means of the Cox\u2013de Boor recursion formula. \nWe start with the B-splines of degree formula_18, i.e. piecewise constant polynomials.\nformula_19\nThe higher formula_20-degree B-splines are defined by recursion\nformula_21\nProperties.\nA B-spline function is a combination of flexible bands that is controlled by a number of points that are called control points, creating smooth curves. These functions are used to create and manage complex shapes and surfaces using a number of points. B-spline function and B\u00e9zier functions are applied extensively in shape optimization methods.\nA B-spline of order formula_1 is a piecewise polynomial function of degree formula_2 in a variable formula_24. It is defined over formula_25 locations formula_26, called knots or breakpoints, which must be in non-descending order formula_27. The B-spline contributes only in the range between the first and last of these knots and is zero elsewhere. If each knot is separated by the same distance formula_28 (where formula_29) from its predecessor, the knot vector and the corresponding B-splines are called \"uniform\" (see cardinal B-spline below).\nFor each finite knot interval where it is non-zero, a B-spline is a polynomial of degree formula_2. A B-spline is a continuous function at the knots. When all knots belonging to the B-spline are distinct, its derivatives are also continuous up to the derivative of degree formula_31. If the knots are coincident at a given value of formula_24, the continuity of derivative order is reduced by 1 for each additional coincident knot. B-splines may share a subset of their knots, but two B-splines defined over exactly the same knots are identical. In other words, a B-spline is uniquely defined by its knots.\nOne distinguishes internal knots and end points. Internal knots cover the formula_24-domain one is interested in. Since a single B-spline already extends over formula_25 knots, it follows that the internal knots need to be extended with formula_2 endpoints on each side, to give full support to the first and last B-spline, which affect the internal knot intervals. The values of the endpoints do not matter, usually the first or last internal knot is just repeated.\nThe usefulness of B-splines lies in the fact that any spline function of order formula_1 on a given set of knots can be expressed as a linear combination of B-splines:\n formula_37\nB-splines play the role of basis functions for the spline function space, hence the name. This property follows from the fact that all pieces have the same continuity properties, within their individual range of support, at the knots.\nExpressions for the polynomial pieces can be derived by means of the Cox\u2013de Boor recursion formula\nformula_38\nformula_39\nThat is, formula_40 is piecewise constant one or zero indicating which knot span \"x\" is in (zero if knot span \"j\" is repeated). The recursion equation is in two parts: \nformula_41 \nramps from zero to one as \"x\" goes from formula_42 to formula_43, and\nformula_44\nramps from one to zero as \"x\" goes from formula_45 to formula_46. The corresponding \"B\"s are zero outside those respective ranges. For example, formula_47 is a triangular function that is zero below formula_48, ramps to one at formula_49 and back to zero at and beyond formula_50. However, because B-spline basis functions have local support, B-splines are typically computed by algorithms that do not need to evaluate basis functions where they are zero, such as de Boor's algorithm.\nThis relation leads directly to the FORTRAN-coded algorithm BSPLV, which generates values of the B-splines of order \"n\" at \"x\". The following scheme illustrates how each piece of order \"n\" is a linear combination of the pieces of B-splines of order \"n\"\u00a0\u2212\u00a01 to its left.\nformula_51\nApplication of the recursion formula with the knots at formula_52 gives the pieces of the uniform B-spline of order 3 \nformula_53\nThese pieces are shown in the diagram. The continuity property of a quadratic spline function and its first derivative at the internal knots are illustrated, as follows\n formula_54\nThe second derivative of a B-spline of degree 2 is discontinuous at the knots:\n formula_55\nFaster variants of the de Boor algorithm have been proposed, but they suffer from comparatively lower stability.\nCardinal B-spline.\nA cardinal B-spline has a constant separation \"h\" between knots. The cardinal B-splines for a given order \"n\" are just shifted copies of each other. They can be obtained from the simpler definition.\nformula_56\nThe \"placeholder\" notation is used to indicate that the \"n\"-th divided difference of the function formula_57 of the two variables \"t\" and \"x\" is to be taken by fixing \"x\" and considering formula_58 as a function of \"t\" alone.\nA cardinal B-spline has uniformly spaced knots, therefore interpolation between the knots equals convolution with a smoothing kernel.\nExample, if we want to interpolate three values in between B-spline nodes (formula_59), we can write the signal as\n formula_60\nConvolution of the signal formula_61 with a rectangle function formula_62 gives first order interpolated B-spline values. Second-order B-spline interpolation is convolution with a rectangle function twice formula_63; by iterative filtering with a rectangle function, higher-order interpolation is obtained.\nFast B-spline interpolation on a uniform sample domain can be done by iterative mean-filtering. Alternatively, a rectangle function equals sinc in Fourier domain. Therefore, cubic spline interpolation equals multiplying the signal in Fourier domain with sinc4.\nSee Irwin\u2013Hall distribution#Special cases for algebraic expressions for the cardinal B-splines of degree 1\u20134.\nP-spline.\nThe term P-spline stands for \"penalized B-spline\". It refers to using the B-spline representation where the coefficients are determined partly by the data to be fitted, and partly by an additional penalty function that aims to impose smoothness to avoid overfitting.\nTwo- and multidimensional P-spline approximations of data can use the face-splitting product of matrices to the minimization of calculation operations.\nDerivative expressions.\nThe derivative of a B-spline of degree \"k\" is simply a function of B-splines of degree \"k\"\u00a0\u2212\u00a01:\nformula_64\nThis implies that\nformula_65\nwhich shows that there is a simple relationship between the derivative of a spline function and the B-splines of degree one less.\nMoments of univariate B-splines.\nUnivariate B-splines, i.e. B-splines where the knot positions lie in a single dimension, can be used to represent 1-d probability density functions formula_66. An example is a weighted sum of formula_67 B-spline basis functions of order formula_1, which each are area-normalized to unity (i.e. not directly evaluated using the standard de-Boor algorithm)\n formula_69\nand with normalization constant constraint formula_70.\nThe \"k\"-th raw moment formula_71 of a normalized B-spline formula_72 can be written as Carlson's Dirichlet average formula_73, which in turn can be solved exactly via a contour integral and an iterative sum as\n formula_74\nwith\n formula_75\nand formula_76. Here, formula_77 represents a vector with the formula_78 knot positions and formula_79 a vector with the respective knot multiplicities. One can therefore calculate any moment of a probability density function formula_66 represented by a sum of B-spline basis functions exactly, without resorting to numerical techniques.\nRelationship to piecewise/composite B\u00e9zier.\nA B\u00e9zier curve is also a polynomial curve definable using a recursion from lower-degree curves of the same class and encoded in terms of control points, but a key difference is that all terms in the recursion for a B\u00e9zier curve segment have the same domain of definition (usually formula_81), whereas the supports of the two terms in the B-spline recursion are different (the outermost subintervals are not common). This means that a B\u00e9zier curve of degree formula_1 given by formula_83 control points consists of about formula_84 mostly independent segments, whereas the B-spline with the same parameters smoothly transitions from subinterval to subinterval. To get something comparable from a B\u00e9zier curve, one would need to impose a smoothness condition on transitions between segments, resulting in some manner of B\u00e9zier spline (for which many control points would be determined by the smoothness requirement).\nA piecewise/composite B\u00e9zier curve is a series of B\u00e9zier curves joined with at least C0 continuity (the last point of one curve coincides with the starting point of the next curve). Depending on the application, additional smoothness requirements (such as C1 or C2 continuity) may be added. C1 continuous curves have identical tangents at the breakpoint (where the two curves meet). C2 continuous curves have identical curvature at the breakpoint.\nCurve fitting.\nUsually in curve fitting, a set of data points is fitted with a curve defined by some mathematical function. For example, common types of curve fitting use a polynomial or a set of exponential functions. When there is no theoretical basis for choosing a fitting function, the curve may be fitted with a spline function composed of a sum of B-splines, using the method of least squares. Thus, the objective function for least-squares minimization is, for a spline function of degree \"k\",\n formula_85\nwhere \"W\"(\"x\") is a weight, and \"y\"(\"x\") is the datum value at \"x\". The coefficients formula_86 are the parameters to be determined. The knot values may be fixed or treated as parameters.\nThe main difficulty in applying this process is in determining the number of knots to use and where they should be placed. de Boor suggests various strategies to address this problem. For instance, the spacing between knots is decreased in proportion to the curvature (2nd derivative) of the data. A few applications have been published. For instance, the use of B-splines for fitting single Lorentzian and Gaussian curves has been investigated. Optimal spline functions of degrees 3\u20137 inclusive, based on symmetric arrangements of 5, 6, and 7 knots, have been computed and the method was applied for smoothing and differentiation of spectroscopic curves. In a comparable study, the two-dimensional version of the Savitzky\u2013Golay filtering and the spline method produced better results than moving average or Chebyshev filtering.\nComputer-aided design and computer graphics.\nIn computer-aided design and computer graphics applications, a spline curve is sometimes represented as formula_87, a parametric curve of some real parameter formula_6. In this case the curve formula_87 can be treated as two or three separate coordinate functions formula_90, or formula_91. The coordinate functions formula_92, formula_93 and formula_94 are each spline functions, with a common set of knot values formula_95.\nBecause a B-splines form basis functions, each of the coordinate functions can be expressed as a linear sum of B-splines, so we have\nformula_96\nThe weights formula_97, formula_98 and formula_99 can be combined to form points formula_100 in 3-d space. These points formula_101 are commonly known as control points.\nWorking in reverse, a sequence of control points, knot values, and order of the B-spline define a parametric curve. This representation of a curve by control points has several useful properties:\nA less desirable feature is that the parametric curve does not interpolate the control points. Usually the curve does not pass through the control points.\nCubic B-Splines.\nA cubic B-spline curve formula_105 with a normalized parameter formula_106 is defined by four nodes (i.e. \"control points\") formula_107, formula_108, formula_109, and formula_110. It forms a polynomial of degree 3 that can be written as\n formula_111.\nThis corresponds to B-spline polynomials\n formula_112\nand the curve can be evaluated as formula_113. Expanding this, we can write the full polynomial form as below\n formula_114.\nSince this is a cubic polynomial, we can also write it as a cubic B\u00e9zier curve with control points formula_115, formula_116, formula_117, and formula_118, such that\n formula_119\nA piecewise cubic B-spline is formed by a set of nodes and each four consecutive nodes define a cubic piece of the curve with the formulation above.\nNURBS.\nIn computer-aided design, computer-aided manufacturing, and computer graphics, a powerful extension of B-splines is non-uniform rational B-splines (NURBS). NURBS are essentially B-splines in homogeneous coordinates. Like B-splines, they are defined by their order, and a knot vector, and a set of control points, but unlike simple B-splines, the control points each have a weight. When the weight is equal to 1, a NURBS is simply a B-spline and as such NURBS generalizes both B-splines and B\u00e9zier curves and surfaces, the primary difference being the weighting of the control points which makes NURBS curves \"rational\".\nBy evaluating a NURBS at various values of the parameters, the curve can be traced through space; likewise, by evaluating a NURBS surface at various values of the two parameters, the surface can be represented in Cartesian space.\nLike B-splines, NURBS control points determine the shape of the curve. Each point of the curve is computed by taking a weighted sum of a number of control points. The weight of each point varies according to the governing parameter. For a curve of degree \"d\", the influence of any control point is only nonzero in \"d\"+1 intervals (knot spans) of the parameter space. Within those intervals, the weight changes according to a polynomial function (basis functions) of degree \"d\". At the boundaries of the intervals, the basis functions go smoothly to zero, the smoothness being determined by the degree of the polynomial.\nThe knot vector is a sequence of parameter values that determines where and how the control points affect the NURBS curve. The number of knots is always equal to the number of control points plus curve degree plus one. Each time the parameter value enters a new knot span, a new control point becomes active, while an old control point is discarded.\nA NURBS curve takes the following form:\n formula_120\nHere the notation is as follows. \"u\" is the independent variable (instead of \"x\"), \"k\" is the number of control points, \"N\" is a B-spline (used instead of \"B\"), \"n\" is the polynomial degree, \"P\" is a control point and \"w\" is a weight. The denominator is a normalizing factor that evaluates to one if all weights are one.\nIt is customary to write this as\n formula_121\nin which the functions\n formula_122\nare known as the rational basis functions.\nA NURBS surface is obtained as the tensor product of two NURBS curves, thus using two independent parameters \"u\" and \"v\" (with indices \"i\" and \"j\" respectively):\n formula_123\nwith\n formula_124\nas rational basis functions.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited"}
{"id": "21835", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=21835", "title": "Nurbs", "text": ""}
{"id": "21836", "revid": "44106950", "url": "https://en.wikipedia.org/wiki?curid=21836", "title": "North Pole", "text": "Northernmost point on Earth\nThe North Pole, also known as the Geographic North Pole or Terrestrial North Pole, is the point in the Northern Hemisphere where the Earth's axis of rotation meets its surface. It is called the True North Pole to distinguish from the Magnetic North Pole.\nThe North Pole is by definition the northernmost point on the Earth, lying antipodally to the South Pole. It defines geodetic latitude 90\u00b0 North, as well as the direction of true north. At the North Pole all directions point south; all lines of longitude converge there, so its longitude can be defined as any degree value. No time zone has been assigned to the North Pole, so any time can be used as the local time. Along tight latitude circles, counterclockwise is east and clockwise is west. The North Pole is at the center of the Northern Hemisphere. The nearest land is usually said to be Kaffeklubben Island, off the northern coast of Greenland about away, though some perhaps semi-permanent gravel banks lie slightly closer. The nearest permanently inhabited place is Alert on Ellesmere Island, Canada, which is located from the Pole.\nWhile the South Pole lies on a continental land mass, the North Pole is located in the middle of the Arctic Ocean amid waters that are almost permanently covered with constantly shifting sea ice. The sea depth at the North Pole has been measured at by the Russian Mir submersible in 2007 and at by USS \"Nautilus\" in 1958. This makes it impractical to construct a permanent station at the North Pole (unlike the South Pole). However, the Soviet Union, and later Russia, constructed a number of manned drifting stations on a generally annual basis since 1937, some of which have passed over or very close to the Pole. Since 2002, a group of Russians have also annually established a private base, Barneo, close to the Pole. This operates for a few weeks during early spring. Studies in the 2000s predicted that the North Pole may become seasonally ice-free because of Arctic ice shrinkage, with timescales varying from 2016 to the late 21st century or later.\nAttempts to reach the North Pole began in the late 19th century, with the record for \"Farthest North\" being surpassed on numerous occasions. The first undisputed expedition to reach the North Pole was that of the airship \"Norge\", which overflew the area in 1926 with 16 men on board, including expedition leader Roald Amundsen. Three prior expeditions \u2013 led by Frederick Cook (1908, land), Robert Peary (1909, land) and Richard E. Byrd (1926, aerial) \u2013 were once also accepted as having reached the Pole. However, in each case later analysis of expedition data has cast doubt upon the accuracy of their claims. \nThe first verified individuals to reach the North Pole on foot was in 1948 by a 24-man Soviet party, part of Aleksandr Kuznetsov's \"Sever-2\" expedition to the Arctic, who flew near to the Pole first before making the final trek to the Pole on foot. The first complete land expedition to reach the North Pole was in 1968 by Ralph Plaisted, Walt Pederson, Gerry Pitzl and Jean-Luc Bombardier, using snowmobiles and with air support.\nPrecise definition.\nThe Earth's axis of rotation\u00a0\u2013 and hence the position of the North Pole\u00a0\u2013 was commonly believed to be fixed (relative to the surface of the Earth) until, in the 18th century, the mathematician Leonhard Euler predicted that the axis might \"wobble\" slightly. Around the beginning of the 20th century astronomers noticed a small apparent \"variation of latitude\", as determined for a fixed point on Earth from the observation of stars. Part of this variation could be attributed to a wandering of the Pole across the Earth's surface, by a range of a few metres. The wandering has several periodic components and an irregular component. The component with a period of about 435 days is identified with the eight-month wandering predicted by Euler and is now called the Chandler wobble after its discoverer. The exact point of intersection of the Earth's axis and the Earth's surface, at any given moment, is called the \"instantaneous pole\", but because of the \"wobble\" this cannot be used as a definition of a fixed North Pole (or South Pole) when metre-scale precision is required.\nIt is desirable to tie the system of Earth coordinates (latitude, longitude, and elevations or orography) to fixed landforms. However, given plate tectonics and isostasy, there is no system in which all geographic features are fixed. Yet the International Earth Rotation and Reference Systems Service and the International Astronomical Union have defined a framework called the International Terrestrial Reference System.\nExploration.\nPre-1900.\nAs early as the 16th century, many prominent people correctly believed that the North Pole was in a sea, which in the 19th century was called the Polynya or Open Polar Sea. It was therefore hoped that passage could be found through ice floes at favorable times of the year. Several expeditions set out to find the way, generally with whaling ships, already commonly used in the cold northern latitudes.\nOne of the earliest expeditions to set out with the explicit intention of reaching the North Pole was that of British naval officer William Edward Parry, who in 1827 reached latitude 82\u00b045\u2032 North. In 1871, the \"Polaris\" expedition, a U.S. attempt on the Pole led by Charles Francis Hall, ended in disaster. Another British Royal Navy attempt to get to the pole, part of the British Arctic Expedition, by Commander Albert H. Markham reached a then-record 83\u00b020'26\" North in May 1876 before turning back. An 1879\u20131881 expedition commanded by U.S. Navy officer George W. De Long ended tragically when their ship, the , was crushed by ice. Over half the crew, including De\u00a0Long, were lost.\nIn April 1895, the Norwegian explorers Fridtjof Nansen and Hjalmar Johansen struck out for the Pole on skis after leaving Nansen's icebound ship \"Fram\". The pair reached latitude 86\u00b014\u2032 North before they abandoned the attempt and turned southwards, eventually reaching Franz Josef Land.\nIn 1897, Swedish engineer Salomon August Andr\u00e9e and two companions tried to reach the North Pole in the hydrogen balloon \"\u00d6rnen\" (\"Eagle\"), but came down north of Kvit\u00f8ya, the northeasternmost part of the Svalbard archipelago. They trekked to Kvit\u00f8ya but died there three months after their crash. In 1930 the remains of this expedition were found by the Norwegian Bratvaag Expedition.\nThe Italian explorer Luigi Amedeo, Duke of the Abruzzi and Captain Umberto Cagni of the Italian Royal Navy () sailed the converted whaler \"Stella Polare\" (\"Pole Star\") from Norway in 1899. On 11 March 1900, Cagni led a party over the ice and reached latitude 86\u00b0 34\u2019 on 25 April, setting a new record by beating Nansen's result of 1895 by . Cagni barely managed to return to the camp, remaining there until 23 June. On 16 August, the \"Stella Polare\" left Rudolf Island heading south and the expedition returned to Norway.\n1900\u20131940.\nThe U.S. explorer Frederick Cook claimed to have reached the North Pole on 21 April 1908 with two Inuit men, Ahwelah and Etukishook, but he was unable to produce convincing proof and his claim is not widely accepted.\nThe conquest of the North Pole was for many years credited to U.S. Navy engineer Robert Peary, who claimed to have reached the Pole on 6 April 1909, accompanied by Matthew Henson and four Inuit men, Ootah, Seeglo, Egingwah, and Ooqueah. However, Peary's claim remains highly disputed and controversial. Those who accompanied Peary on the final stage of the journey were not trained in navigation, and thus could not independently confirm his navigational work, which some claim to have been particularly sloppy as he approached the Pole.\nThe distances and speeds that Peary claimed to have achieved once the last support party turned back seem incredible to many people, almost three times that which he had accomplished up to that point. Peary's account of a journey to the Pole and back while traveling along the direct line\u00a0\u2013 the only strategy that is consistent with the time constraints that he was facing\u00a0\u2013 is contradicted by Henson's account of tortuous detours to avoid pressure ridges and open leads.\nThe British explorer Wally Herbert, initially a supporter of Peary, researched Peary's records in 1989 and found that there were significant discrepancies in the explorer's navigational records. He concluded that Peary had not reached the Pole. Support for Peary came again in 2005, however, when British explorer Tom Avery and four companions recreated the outward portion of Peary's journey with replica wooden sleds and Canadian Eskimo Dog teams, reaching the North Pole in 36 days, 22 hours\u00a0\u2013 nearly five hours faster than Peary. However, Avery's fastest 5-day march was , significantly short of the claimed by Peary. Avery writes on his web site that \"The admiration and respect which I hold for Robert Peary, Matthew Henson and the four Inuit men who ventured North in 1909, has grown enormously since we set out from Cape Columbia. Having now seen for myself how he travelled across the pack ice, I am more convinced than ever that Peary did indeed discover the North Pole.\"\nThe first claimed flight over the Pole was made on 9 May 1926 by U.S. naval officer Richard E. Byrd and pilot Floyd Bennett in a Fokker tri-motor aircraft. Although verified at the time by a committee of the National Geographic Society, this claim has since been undermined by the 1996 revelation that Byrd's long-hidden diary's solar sextant data (which the NGS never checked) consistently contradict his June 1926 report's parallel data by over . The secret report's alleged en-route solar sextant data were inadvertently so impossibly overprecise that he excised all these alleged raw solar observations out of the version of the report finally sent to geographical societies five months later (while the original version was hidden for 70 years), a realization first published in 2000 by the University of Cambridge after scrupulous refereeing.\nThe first consistent, verified, and scientifically convincing attainment of the Pole was on 12 May 1926, by Norwegian explorer Roald Amundsen and his U.S. sponsor Lincoln Ellsworth from the airship \"Norge\". \"Norge\", though Norwegian-owned, was designed and piloted by the Italian Umberto Nobile. The flight started from Svalbard in Norway, and crossed the Arctic Ocean to Alaska. Nobile, with several scientists and crew from the \"Norge\", overflew the Pole a second time on 24 May 1928, in the airship \"Italia\". The \"Italia\" crashed on its return from the Pole, with the loss of half the crew.\nAnother transpolar flight was accomplished in a Tupolev ANT-25 airplane with a crew of Valery Chkalov, Georgy Baydukov and Alexander Belyakov, who flew over the North Pole on 19 June 1937, during their direct flight from the Soviet Union to the USA without any stopover.\nIce station.\nIn May 1937 the world's first North Pole ice station, North Pole-1, was established by Soviet scientists 20 kilometres (13\u00a0mi) from the North Pole after the ever first landing of four heavy and one light aircraft onto the ice at the North Pole. The expedition members \u2014 oceanographer Pyotr Shirshov, meteorologist Yevgeny Fyodorov, radio operator Ernst Krenkel, and the leader Ivan Papanin \u2014 conducted scientific research at the station for the next nine months. By 19 February 1938, when the group was picked up by the ice breakers \"Taimyr\" and \"Murman\", their station had drifted 2850\u00a0km to the eastern coast of Greenland.\n1940\u20132000.\nIn May 1945 an RAF Lancaster of the \"Aries\" expedition became the first Commonwealth aircraft to overfly the North Geographic and North Magnetic Poles. The plane was piloted by David Cecil McKinley of the Royal Air Force. It carried an 11-man crew, with Kenneth C. Maclure of the Royal Canadian Air Force in charge of all scientific observations. In 2006, Maclure was honoured with a spot in Canada's Aviation Hall of Fame.\nDiscounting Peary's disputed claim, the first men to set foot at the North Pole were a Soviet party including geophysicists Mikhail Ostrekin and Pavel Senko, oceanographers Mikhail Somov and Pavel Gordienko, and other scientists and flight crew (24 people in total) of Aleksandr Kuznetsov's \"Sever-2\" expedition (March\u2013May 1948). It was organized by the Chief Directorate of the Northern Sea Route. The party flew on three planes (pilots Ivan Cherevichnyy, Vitaly Maslennikov and Ilya Kotov) from Kotelny Island to the North Pole and landed there at 4:44pm (Moscow Time, ) on 23 April 1948. They established a temporary camp and for the next two days conducted scientific observations. On 26 April the expedition flew back to the continent.\nNext year, on 9 May 1949 two other Soviet scientists (Vitali Volovich and Andrei Medvedev) became the first people to parachute onto the North Pole. They jumped from a Douglas C-47 Skytrain, registered CCCP H-369.\nOn 3 May 1952, U.S. Air Force Lieutenant Colonel Joseph O. Fletcher and Lieutenant William Pershing Benedict, along with scientist Albert P. Crary, landed a modified Douglas C-47 Skytrain at the North Pole. Some Western sources considered this to be the first landing at the Pole until the Soviet landings became widely known.\nThe United States Navy submarine \"USS Nautilus\" (SSN-571) crossed the North Pole on 3 August 1958. On 17 March 1959 \"USS Skate\" (SSN-578) surfaced at the Pole, breaking through the ice above it, becoming the first naval vessel to do so.\nThe first confirmed surface conquest of the North Pole was accomplished by Ralph Plaisted, Walt Pederson, Gerry Pitzl and Jean Luc Bombardier, who traveled over the ice by snowmobile and arrived on 19 April 1968. The United States Air Force independently confirmed their position.\nOn 6 April 1969 Wally Herbert and companions Allan Gill, Roy Koerner and Kenneth Hedges of the British Trans-Arctic Expedition became the first men to reach the North Pole on foot (albeit with the aid of dog teams and airdrops). They continued on to complete the first surface crossing of the Arctic Ocean\u00a0\u2013 and by its longest axis, Barrow, Alaska, to Svalbard\u00a0\u2013 a feat that has never been repeated. Because of suggestions (later proven false) of Plaisted's use of air transport, some sources classify Herbert's expedition as the first confirmed to reach the North Pole over the ice surface by any means. In the 1980s Plaisted's pilots Weldy Phipps and Ken Lee signed affidavits asserting that no such airlift was provided. It is also said that Herbert was the first person to reach the pole of inaccessibility.\nOn 17 August 1977 the Soviet nuclear-powered icebreaker \"Arktika\" completed the first surface vessel journey to the North Pole.\nIn 1982 Ranulph Fiennes and Charles R. Burton became the first people to cross the Arctic Ocean in a single season. They departed from Cape Crozier, Ellesmere Island, on 17 February 1982 and arrived at the geographic North Pole on 10 April 1982. They travelled on foot and snowmobile. From the Pole, they travelled towards Svalbard but, due to the unstable nature of the ice, ended their crossing at the ice edge after drifting south on an ice floe for 99 days. They were eventually able to walk to their expedition ship \"MV Benjamin Bowring\" and boarded it on 4 August 1982 at position 80:31N 00:59W. As a result of this journey, which formed a section of the three-year Transglobe Expedition 1979\u20131982, Fiennes and Burton became the first people to complete a circumnavigation of the world via both North and South Poles, by surface travel alone. This achievement remains unchallenged to this day. The expedition crew included a Jack Russell Terrier named Bothie who became the first dog to visit both poles.\nIn 1985 Sir Edmund Hillary (the first man to stand on the summit of Mount Everest) and Neil Armstrong (the first man to stand on the moon) landed at the North Pole in a small twin-engined ski plane. Hillary thus became the first man to stand at both poles and on the summit of Everest.\nIn 1986 Will Steger, with seven teammates, became the first to be confirmed as reaching the Pole by dogsled and without resupply.\nUSS \"Gurnard\" (SSN-662) operated in the Arctic Ocean under the polar ice cap from September to November 1984 in company with one of her sister ships, the attack submarine USS \"Pintado\" (SSN-672). On 12 November 1984 \"Gurnard\" and \"Pintado\" became the third pair of submarines to surface together at the North Pole. In March 1990, \"Gurnard\" deployed to the Arctic region during exercise Ice Ex '90 and completed only the fourth winter submerged transit of the Bering and Seas. \"Gurnard\" surfaced at the North Pole on 18 April, in the company of the USS \"Seahorse\" (SSN-669).\nOn 6 May 1986 USS \"Archerfish\" (SSN 678), USS \"Ray\" (SSN 653) and USS \"Hawkbill\" (SSN-666) surfaced at the North Pole, the first tri-submarine surfacing at the North Pole.\nOn 21 April 1987 Shinji Kazama of Japan became the first person to reach the North Pole on a motorcycle.\nOn 18 May 1987 USS \"Billfish\" (SSN 676), USS \"Sea Devil\" (SSN 664) and HMS \"Superb\" (S 109) surfaced at the North Pole, the first international surfacing at the North Pole.\nIn 1988 a team of 13 (9 Soviets, 4 Canadians) skied across the arctic from Siberia to northern Canada. One of the Canadians, Richard Weber, became the first person to reach the Pole from both sides of the Arctic Ocean.\nOn April 16, 1990, a German-Swiss expedition led by a team of the University of Giessen reached the Geographic North Pole for studies on pollution of pack ice, snow and air. Samples taken were analyzed in cooperation with the Geological Survey of Canada and the Alfred Wegener Institute for Polar and Marine Research. Further stops for sample collections were on multi-year sea ice at 86\u00b0N, at Cape Columbia and Ward Hunt Island.\nOn 4 May 1990 B\u00f8rge Ousland and Erling Kagge became the first explorers ever to reach the North Pole unsupported, after a 58-day ski trek from Ellesmere Island in Canada, a distance of 800\u00a0km.\nOn 7 September 1991 the German research vessel \"Polarstern\" and the Swedish icebreaker \"Oden\" reached the North Pole as the first conventional powered vessels. Both scientific parties and crew took oceanographic and geological samples and had a common tug of war and a football game on an ice floe. \"Polarstern\" again reached the pole exactly 10 years later, with the \"Healy\".\nIn 1998, 1999, and 2000, Lada Niva Marshs (special very large wheeled versions made by BRONTO, Lada/Vaz's experimental product division) were driven to the North Pole. The 1998 expedition was dropped by parachute and completed the track to the North Pole. The 2000 expedition departed from a Russian research base around 114\u00a0km from the Pole and claimed an average speed of 20\u201315\u00a0km/h in an average temperature of \u221230\u00a0\u00b0C.\n21st century.\nCommercial airliner flights on the polar routes may pass within viewing distance of the North Pole. For example, a flight from Chicago to Beijing may come close as latitude 89\u00b0 N, though because of prevailing winds return journeys go over the Bering Strait. In recent years journeys to the North Pole by air (landing by helicopter or on a runway prepared on the ice) or by icebreaker have become relatively routine, and are even available to small groups of tourists through adventure holiday companies. Parachute jumps have frequently been made onto the North Pole in recent years. The temporary seasonal Russian camp of Barneo has been established by air a short distance from the Pole annually since 2002, and caters for scientific researchers as well as tourist parties. Trips from the camp to the Pole itself may be arranged overland or by helicopter.\nThe first attempt at underwater exploration of the North Pole was made on 22 April 1998 by Russian firefighter and diver Andrei Rozhkov with the support of the Diving Club of Moscow State University, but ended in fatality. The next attempted dive at the North Pole was organized the next year by the same diving club, and ended in success on 24 April 1999. The divers were Michael Wolff (Austria), Brett Cormick (UK), and Bob Wass (USA).\nIn 2005 the United States Navy submarine USS \"Charlotte\" (SSN-766) surfaced through of ice at the North Pole and spent 18 hours there.\nIn July 2007 British endurance swimmer Lewis Gordon Pugh completed a swim at the North Pole. His feat, undertaken to highlight the effects of global warming, took place in clear water that had opened up between the ice floes. His later attempt to paddle a kayak to the North Pole in late 2008, following the erroneous prediction of clear water to the Pole, was stymied when his expedition found itself stuck in thick ice after only three days. The expedition was then abandoned.\nBy September 2007 the North Pole had been visited 66 times by different surface ships: 54 times by Soviet and Russian icebreakers, 4 times by Swedish \"Oden\", 3 times by German \"Polarstern\", 3 times by USCGC \"Healy\" and USCGC \"Polar Sea\", and once by CCGS \"Louis S. St-Laurent\" and by Swedish \"Vidar Viking\".\n2007 descent to the North Pole seabed.\nOn 2 August 2007 a Russian scientific expedition Arktika 2007 made the first ever manned descent to the ocean floor at the North Pole, to a depth of , as part of the research programme in support of Russia's 2001 extended continental shelf claim to a large swathe of the Arctic Ocean floor. The descent took place in two MIR submersibles and was led by Soviet and Russian polar explorer Artur Chilingarov. In a symbolic act of visitation, the Russian flag was placed on the ocean floor exactly at the Pole.\nThe expedition was the latest in a series of efforts intended to give Russia a dominant influence in the Arctic according to \"The New York Times\".\nMLAE 2009 Expedition.\nIn 2009 the Russian Marine Live-Ice Automobile Expedition (MLAE-2009) with Vasily Elagin as a leader and a team of Afanasy Makovnev, Vladimir Obikhod, Alexey Shkrabkin, Sergey Larin, Alexey Ushakov and Nikolay Nikulshin reached the North Pole on two custom-built 6 x 6 low-pressure-tire ATVs. The vehicles, Yemelya-1 and Yemelya-2, were designed by Vasily Elagin, a Russian mountain climber, explorer and engineer. They reached the North Pole on 26 April 2009, 17:30 (Moscow time). The expedition was partly supported by Russian State Aviation. The Russian Book of Records recognized it as the first successful vehicle trip from land to the Geographical North Pole.\nMLAE 2013 Expedition.\nOn 1 March 2013 the Russian Marine Live-Ice Automobile Expedition (MLAE 2013) with Vasily Elagin as a leader, and a team of Afanasy Makovnev, Vladimir Obikhod, Alexey Shkrabkin, Andrey Vankov, Sergey Isayev and Nikolay Kozlov on two custom-built 6 x 6 low-pressure-tire ATVs\u2014Yemelya-3 and Yemelya-4\u2014started from Golomyanny Island (the Severnaya Zemlya Archipelago) to the North Pole across drifting ice of the Arctic Ocean. The vehicles reached the Pole on 6 April and then continued to the Canadian coast. The coast was reached on 30 April 2013 (83\u00b008N, 075\u00b059W Ward Hunt Island), and on 5 May 2013 the expedition finished in Resolute Bay, NU. The way between the Russian borderland (Machtovyi Island of the Severnaya Zemlya Archipelago, 80\u00b015N, 097\u00b027E) and the Canadian coast (Ward Hunt Island, 83\u00b008N, 075\u00b059W) took 55 days; it was ~2300\u00a0km across drifting ice and about 4000\u00a0km in total. The expedition was totally self-dependent and used no external supplies. The expedition was supported by the Russian Geographical Society.\nTime and day and night.\nThe sun at the North Pole is continuously above the horizon during the summer and continuously below the horizon during the winter. Sunrise is just before the March equinox (around 20 March); the Sun then takes three months to reach its highest point of near 23\u00bd\u00b0 elevation at the summer solstice (around 21 June), after which time it begins to sink, reaching sunset just after the September equinox (around 23 September). When the Sun is visible in the polar sky, it appears to move in a horizontal circle above the horizon. This circle gradually rises from near the horizon just after the vernal equinox to its maximum elevation (in degrees) above the horizon at summer solstice and then sinks back toward the horizon before sinking below it at the autumnal equinox. Hence the North and South Poles experience the slowest rates of sunrise and sunset on Earth.\nThe twilight period that occurs before sunrise and after sunset has three different definitions:\nThese effects are caused by a combination of the Earth's axial tilt and its revolution around the Sun. The direction of the Earth's axial tilt, as well as its angle relative to the plane of the Earth's orbit around the Sun, remains very nearly constant over the course of a year (both change very slowly over long time periods). At northern midsummer the North Pole is facing towards the Sun to its maximum extent. As the year progresses and the Earth moves around the Sun, the North Pole gradually turns away from the Sun until at midwinter it is facing away from the Sun to its maximum extent. A similar sequence is observed at the South Pole, with a six-month time difference.\nSince longitude is undefined at the north pole, the exact time is a matter of convention. Polar expeditions use whatever time is most convenient, such as Greenwich Mean Time or the time zone of their origin.\nClimate, sea ice at North Pole.\nThe North Pole is substantially warmer than the South Pole because it lies at sea level in the middle of an ocean (which acts as a reservoir of heat), rather than at altitude on a continental land mass. Despite being an ice cap, the northernmost weather station in Greenland has a tundra climate (K\u00f6ppen \"ET\") due to the July and August mean temperatures peaking just above freezing.\nWinter temperatures at the northernmost weather station in Greenland can range from about , averaging around , with the North Pole being slightly colder. However, a freak storm caused the temperature to reach for a time at a World Meteorological Organization buoy, located at 87.45\u00b0N, on 30 December 2015. It was estimated that the temperature at the North Pole was between during the storm. Summer temperatures (June, July, and August) average around the freezing point (). The highest temperature yet recorded is , much warmer than the South Pole's record high of only . A similar spike in temperatures occurred on 15 November 2016 when temperatures hit freezing. Yet again, February 2018 featured a storm so powerful that temperatures at Cape Morris Jesup, the world's northernmost weather station in Greenland, reached and spent 24 straight hours above freezing. Meanwhile, the pole itself was estimated to reach a high temperature of . This same temperature of was also recorded at the Hollywood Burbank Airport in Los Angeles at the very same time.\nThe sea ice at the North Pole is typically around thick, although ice thickness, its spatial extent, and the fraction of open water within the ice pack can vary rapidly and profoundly in response to weather and climate. Studies have shown that the average ice thickness has decreased in recent years. It is likely that global warming has contributed to this, but it is not possible to attribute the recent abrupt decrease in thickness entirely to the observed warming in the Arctic. Reports have also predicted that within a few decades the Arctic Ocean will be entirely free of ice in the summer. This may have significant commercial implications; see \"Territorial claims\", below.\nThe retreat of the Arctic sea ice will accelerate global warming, as less ice cover reflects less solar radiation, and may have serious climate implications by contributing to Arctic cyclone generation.\nFlora and fauna.\nPolar bears are believed to travel rarely beyond about 82\u00b0 North, owing to the scarcity of food, though tracks have been seen in the vicinity of the North Pole, and a 2006 expedition reported sighting a polar bear just from the Pole. The ringed seal has also been seen at the Pole, and Arctic foxes have been observed less than away at 89\u00b040\u2032\u00a0N.\nBirds seen at or very near the Pole include the snow bunting, northern fulmar and black-legged kittiwake, though some bird sightings may be distorted by the tendency of birds to follow ships and expeditions.\nFish have been seen in the waters at the North Pole, but these are probably few in number. A member of the Russian team that descended to the North Pole seabed in August 2007 reported seeing no sea creatures living there. However, it was later reported that a sea anemone had been scooped up from the seabed mud by the Russian team and that video footage from the dive showed unidentified shrimps and amphipods.\nTerritorial claims to the North Pole and Arctic regions.\nCurrently, under international law, no country owns the North Pole or the region of the Arctic Ocean surrounding it. The five surrounding Arctic countries, Russia, Canada, Norway, Denmark (via Greenland), and the United States, are limited to a exclusive economic zone off their coasts, and the area beyond that is administered by the International Seabed Authority.\nUpon ratification of the United Nations Convention on the Law of the Sea, a country has 10 years to make claims to an extended continental shelf beyond its 200-mile exclusive economic zone. If validated, such a claim gives the claimant state rights to what may be on or beneath the sea bottom within the claimed zone. Norway (ratified the convention in 1996), Russia (ratified in 1997), Canada (ratified in 2003) and Denmark (ratified in 2004) have all launched projects to base claims that certain areas of Arctic continental shelves should be subject to their sole sovereign exploitation.\nIn 1907 Canada invoked the \"sector principle\" to claim sovereignty over a sector stretching from its coasts to the North Pole. This claim has not been relinquished, but was not consistently pressed until 2013.\nCultural associations.\nIn some children's Christmas legends and Western folklore, the geographic North Pole is described as the location of Santa Claus' workshop and residence. Canada Post has assigned postal code H0H 0H0 to the North Pole (referring to Santa's traditional exclamation of \"Ho ho ho!\").\nThis association reflects an age-old esoteric mythology of Hyperborea that posits the North Pole, the otherworldly world-axis, as the abode of God and superhuman beings.\nAs Henry Corbin has documented, the North Pole plays a key part in the cultural worldview of Sufism and Iranian mysticism. \"The Orient sought by the mystic, the Orient that cannot be located on our maps, is in the direction of the north, beyond the north.\".\nIn Mandaean cosmology, the North Pole and Polaris are considered to be auspicious, since they are associated with the World of Light. Mandaeans face north when praying, and temples are also oriented towards the north. On the contrary, South is associated with the World of Darkness.\nOwing to its remoteness, the Pole is sometimes identified with a mysterious mountain of ancient Iranian tradition called Mount Qaf (Jabal Qaf), the \"farthest point of the earth\". According to certain authors, the Jabal Qaf of Muslim cosmology is a version of Rupes Nigra, a mountain whose ascent, like Dante's climbing of the Mountain of Purgatory, represents the pilgrim's progress through spiritual states. In Iranian theosophy, the heavenly Pole, the focal point of the spiritual ascent, acts as a magnet to draw beings to its \"palaces ablaze with immaterial matter.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21837", "revid": "45179772", "url": "https://en.wikipedia.org/wiki?curid=21837", "title": "Nanometre", "text": "Unit of length\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nThe nanometre (international spelling as used by the International Bureau of Weights and Measures; SI symbol: nm), or nanometer (American spelling), is a unit of length in the International System of Units (SI), equal to one billionth (short scale) or one thousand million (long scale) of a metre (0.000000001\u00a0m) and to 1000\u00a0picometres. One nanometre can be expressed in scientific notation as 1 \u00d7 10\u22129\u00a0m and as \u00a0m.\nHistory.\nThe nanometre was formerly known as the \"\"millimicrometre\"\u00a0\u2013 or, more commonly, the \"millimicron\"\" for short\u00a0\u2013 since it is of a micrometre. It was often denoted by the symbol \"m\u03bc\" or, more rarely, as \"\u03bc\u03bc\" (however, \"\u03bc\u03bc\" should refer to a \"millionth\" of a micron).\nEtymology.\nThe name combines the SI prefix \"nano-\" (from the Ancient Greek , ', \"dwarf\") with the parent unit name \"metre\" (from Greek , ', \"unit of measurement\").\nUsage.\nNanotechnologies are based on physical processes which occur on a scale of nanometres (see nanoscopic scale).\nThe nanometre is often used to express dimensions on an atomic scale: the diameter of a helium atom, for example, is about 0.06\u00a0nm, and that of a ribosome is about 20\u00a0nm. The nanometre is also commonly used to specify the wavelength of electromagnetic radiation near the visible part of the spectrum: visible light ranges from around 400 to 700\u00a0nm. The \u00e5ngstr\u00f6m, which is equal to 0.1\u00a0nm, was formerly used for these purposes.\nSince the late 1980s, in usages such as the 32\u00a0nm and the 22\u00a0nm semiconductor node, it has also been used to describe typical feature sizes in successive generations of the ITRS Roadmap for miniaturized semiconductor device fabrication in the semiconductor industry.\nUnicode.\nThe CJK Compatibility block in Unicode has the symbol .\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21838", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=21838", "title": "New York city", "text": ""}
{"id": "21840", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=21840", "title": "National Transportation Safety Board", "text": "US government investigative agency for civil transportation accidents\nThe National Transportation Safety Board (NTSB) is an independent U.S. government investigative agency responsible for civil transportation accident investigation. In this role, the NTSB investigates and reports on aviation accidents and incidents, certain types of highway crashes, ship and marine accidents, pipeline incidents, bridge failures, and railroad accidents. The NTSB is also in charge of investigating cases of hazardous materials releases that occur during transportation. The agency is based in Washington, D.C. It has three regional offices, located in Anchorage, Alaska; Aurora, Colorado; and Federal Way, Washington. The agency also operated a national training center at its Ashburn facility.\nHistory.\nThe origin of the NTSB was in the Air Commerce Act of 1926, which assigned the United States Department of Commerce responsibility for investigating domestic aviation accidents. Before the NTSB, the Federal Aviation Administration's (FAA; at the time the CAA/Civil Aviation Authority) independence was questioned as it was investigating itself and would be biased to find external faults, coalescing with the 1931 crash killing Notre Dame coach Knute Rockne and the 1935 crash that killed Senator Bronson Cutting. The United States's first \"independent\" \"Air Safety Board\" was established in 1938: it lasted only fourteen months. In 1940, this authority was transferred to the Civil Aeronautics Board's newly formed Bureau of Aviation Safety.\nOn April 1, 1967, the Congress created a separate cabinet-level Department of Transportation, which among other things, established the Federal Aviation Administration as an agency under the DOT. At the same time, the NTSB was established as an independent agency which absorbed the Bureau of Aviation Safety's responsibilities. However, from 1967 to 1975, the NTSB reported to the DOT for administrative purposes, while conducting investigations into the Federal Aviation Administration, also a DOT agency.\nTo avoid any conflict, the Congress passed the Independent Safety Board Act, and on April 1, 1975, the NTSB became a fully independent agency. As of 2015[ [update]], the NTSB has investigated over 140,000 aviation incidents and several thousand surface transportation incidents.\nOrganization.\nFormally, the \"National Transportation Safety Board\" refers to a five-manager investigative board whose five members are nominated by the President and confirmed by the Senate for five-year terms. Board members may continue to serve until a successor is confirmed and takes office. No more than three of the five members may be from the same political party. One of the five board members is nominated as the chair by the President and then approved by the Senate for a fixed three-year term; another is designated as vice-chair for a fixed three-year term and who becomes acting chair when there is no formal chair. This board is authorized by Congress under Chapter 11, Title 49 of the United States Code to investigate civil aviation, highway, marine, pipeline, and railroad accidents and incidents. This five-member board is authorized to establish and manage separate sub-offices for highway, marine, aviation, railroad, pipeline, and hazardous materials investigations.\nSince its creation, the NTSB's primary mission has been \"to determine the probable cause of transportation accidents and incidents and to formulate safety recommendations to improve transportation safety (in the USA)\". Based on the results of investigations within its jurisdiction, the NTSB issues formal safety recommendations to agencies and institutions with the power to implement those recommendations. The NTSB considers safety recommendations to be its primary tool for preventing future civil transportation accidents. However, the NTSB does not have the authority to enforce its safety recommendations.\nCurrent board members.\nThe board members as of \u00a0, 2025[ [update]] are:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAccident and incident investigations.\nThe NTSB is the lead agency in investigating a civil transportation accident or incident within its sphere. An investigation of a major accident within the United States typically starts with the creation of a \"go team\", composed of specialists in fields relating to the incident who are rapidly deployed to the incident location. The \"go team\" can have as few as three people or as many as a dozen, depending on the nature of the incident. The agency may then hold public hearings on the issue following the investigation. Ultimately, it will publish a final report which may include safety recommendations based on its findings. The NTSB has no legal authority to implement or impose its recommendations. Its recommendations are often implemented by regulators at the federal or state level or by individual transportation companies.\nIf the structure of an aircraft remains largely intact during a crash and does not transmit gravitational forces to occupants that a human cannot tolerate, the NTSB deems it a survivable incident. Humans can generally tolerate 4 to 5 Gs.\nUse of the \"party system\".\nTo conduct its investigations, the NTSB operates under the \"party system\", which utilizes the support and participation of industry and labor representatives with expertise or technical knowledge specifically useful to its investigation. The NTSB may invite these individuals or organizations to become parties to the investigation and participate under the supervision of the NTSB. The NTSB has discretion over which organizations it allows to participate. Only individuals with relevant technical expertise can represent an organization in an investigation, and attorneys and insurance investigators are prohibited by law from participating.\nThe NTSB considers the party system crucial to the investigative process, as it provides the NTSB with access to individuals with specialized expertise or knowledge relevant to a particular investigation. However, the use of the party system is not without controversy. The NTSB invited Boeing to participate as a party to the investigation of the crash of TWA Flight 800, a Boeing 747, in 1996. While the NTSB relied on Boeing's sharing of expertise, it was later determined that Boeing had withheld a study of military versions of the 747 that investigated flammable vapor combustion in the center fuel tank. Boeing had told the NTSB that it had no studies proving or disproving the vapor combustion theory. In response to political pressure after the Boeing incident, the NTSB commissioned the nonprofit Rand Corporation to conduct an independent study of the NTSB's aircraft investigation process.\nIn 2000, Rand published its report, which concluded that the party system is \"a key component of the NTSB investigative process\" and that participant parties \"are uniquely able to provide essential information about aircraft design and manufacture, airline operations, or functioning of [the National Airspace System] that simply cannot be obtained elsewhere\". However, Rand also found conflicts of interest inherent in the party system, \"may, in some instances, threaten the integrity of the NTSB investigative process\". The Rand study recommended that the NTSB reduce its reliance on party representatives and make greater use of independent investigators, including from NASA, the Department of Defense, government research laboratories, and universities. As of 2014[ [update]], the NTSB has not adopted these recommendations and instead continues to rely on the party system.\nSafety recommendations adopted.\nAs of 2014[ [update]], the NTSB has issued about 14,000 safety recommendations in its history, 73 percent of which have been adopted in whole or in part by the entities to which they were directed. From 1990 to 2023, the NTSB annually published a \"Most Wanted List\", which highlighted safety recommendations that the NTSB believed would provide the most significant \u2014 and sometimes immediate \u2014 benefit to the traveling public.\nAmong transportation safety improvements brought about or inspired by NTSB recommendations:\nOther responsibilities.\nA less well-known responsibility of the NTSB is that it serves as a court of appeals for airmen, aircraft mechanics, certificated aviation-related companies, and mariners who have their licenses suspended or revoked by the FAA or the Coast Guard. The NTSB employs administrative law judges who initially hear all appeals, and the administrative law judge's ruling may be appealed to the five-member Board. The Board's determinations may be appealed to the federal court system by the losing party, whether it is the individual or company, on the one hand, or the FAA or the Coast Guard, on the other. However, from \"Ferguson v. NTSB\", the NTSB's determinations are not overturned by the federal courts unless the NTSB abused its discretion or its determination is wholly unsupported by the evidence.\nThe Safety Board maintains a training academy in Ashburn, Virginia, where it conducts courses for its employees and professionals in other government agencies, foreign governments or private companies, in areas such as general accident investigation, specific elements of investigations like survival factors or human performance, or related matters like family affairs or media relations. The facility houses for training purposes the reconstruction of more than 90 feet of the TWA Flight 800 Boeing 747, which was recovered from the Atlantic Ocean after it crashed on July 17, 1996, following a fuel tank explosion.\nOn February 22, 2021, the NTSB announced that the TWA Flight 800 recreation would be decommissioned on July 7, 2021. This decision comes as the lease for the Ashburn training center expires shortly. The NTSB indicated it is moving away from large-scale reconstructions like with TWA Flight 800 and towards using 3D scans to reconstruct accidents. Under an agreement made with the victims' families, when the reconstruction was retained as a training tool, the reconstruction was not allowed to be used as a public exhibit or put on display. For this reason, the NTSB is planning to dismantle and destroy the reconstruction.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21842", "revid": "41833184", "url": "https://en.wikipedia.org/wiki?curid=21842", "title": "NTSB", "text": ""}
{"id": "21843", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=21843", "title": "Nucleosome", "text": "Basic structural unit of DNA packaging in eukaryotes\nA nucleosome is the basic structural unit of DNA packaging in eukaryotes. The structure of a nucleosome consists of a segment of DNA wound around eight histone proteins and resembles thread wrapped around a spool. The nucleosome is the fundamental subunit of chromatin. Each nucleosome is composed of a little less than two turns of DNA wrapped around a set of eight proteins called histones, which are known as a histone octamer. Each histone octamer is composed of two copies each of the histone proteins H2A, H2B, H3, and H4.\nDNA must be compacted into nucleosomes to fit within the cell nucleus. In addition to nucleosome wrapping, eukaryotic chromatin is further compacted by being folded into a series of more complex structures, eventually forming a chromosome. Each human cell contains about 30 million nucleosomes.\nNucleosomes are thought to carry epigenetically inherited information in the form of covalent modifications of their core histones. Nucleosome positions in the genome are not random, and it is important to know where each nucleosome is located because this determines the accessibility of the DNA to regulatory proteins.\nNucleosomes were first observed as particles in the electron microscope by Don and Ada Olins in 1974, and their existence and structure (as histone octamers surrounded by approximately 200 base pairs of DNA) were proposed by Roger Kornberg. The role of the nucleosome as a regulator of transcription was demonstrated by Lorch et al. in vitro in 1987 and by Han and Grunstein and Clark-Adams et al. in vivo in 1988.\nThe nucleosome core particle consists of approximately 146 base pairs (bp) of DNA wrapped in 1.67 left-handed superhelical turns around a histone octamer, consisting of 2 copies each of the core histones H2A, H2B, H3, and H4. Core particles are connected by stretches of linker DNA, which can be up to about 80 bp long. Technically, a nucleosome is defined as the core particle plus one of these linker regions; however the word is often synonymous with the core particle. Genome-wide nucleosome positioning maps are now available for many model organisms and human cells.\nLinker histones such as H1 and its isoforms are involved in chromatin compaction and sit at the base of the nucleosome near the DNA entry and exit binding to the linker region of the DNA. Non-condensed nucleosomes without the linker histone resemble \"beads on a string of DNA\" under an electron microscope.\nIn contrast to most eukaryotic cells, mature sperm cells largely use protamines to package their genomic DNA, most likely to achieve an even higher packaging ratio. Histone equivalents and a simplified chromatin structure have also been found in Archaea, suggesting that eukaryotes are not the only organisms that use nucleosomes.\nStructure.\nStructure of the core particle.\nOverview.\nPioneering structural studies in the 1980s by Aaron Klug's group provided the first evidence that an octamer of histone proteins wraps DNA around itself in about 1.7 turns of a left-handed superhelix. In 1997 the first near atomic resolution crystal structure of the nucleosome was solved by the Richmond group at the ETH Zurich, showing the most important details of the particle. The human alpha satellite palindromic DNA critical to achieving the 1997 nucleosome crystal structure was developed by the Bunick group at Oak Ridge National Laboratory in Tennessee. The structures of over 20 different nucleosome core particles have been solved to date, including those containing histone variants and histones from different species. The structure of the nucleosome core particle is remarkably conserved, and even a change of over 100 residues between frog and yeast histones results in electron density maps with an overall root mean square deviation of only 1.6\u00c5.\nThe nucleosome core particle (NCP).\nThe nucleosome core particle (shown in the figure) consists of about 146 base pair of DNA wrapped in 1.67 left-handed superhelical turns around the histone octamer, consisting of 2 copies each of the core histones H2A, H2B, H3, and H4. Adjacent nucleosomes are joined by a stretch of free DNA termed linker DNA (which varies from 10 - 80 bp in length depending on species and tissue type).The whole structure generates a cylinder of diameter 11\u00a0nm and a height of 5.5\u00a0nm.\nNucleosome core particles are observed when chromatin in interphase is treated to cause the chromatin to unfold partially. The resulting image, via an electron microscope, is \"beads on a string\". The string is the DNA, while each bead in the nucleosome is a core particle. The nucleosome core particle is composed of DNA and histone proteins.\nPartial DNAse digestion of chromatin reveals its nucleosome structure. Because DNA portions of nucleosome core particles are less accessible for DNAse than linking sections, DNA gets digested into fragments of lengths equal to multiplicity of distance between nucleosomes (180, 360, 540 base pairs etc.). Hence a very characteristic pattern similar to a ladder is visible during gel electrophoresis of that DNA. Such digestion can occur also under natural conditions during apoptosis (\"cell suicide\" or programmed cell death), because autodestruction of DNA typically is its role.\nProtein interactions within the nucleosome.\nThe core histone proteins contains a characteristic structural motif termed the \"histone fold\", which consists of three alpha-helices (\u03b11-3) separated by two loops (L1-2). In solution, the histones form H2A-H2B heterodimers and H3-H4 heterotetramers. Histones dimerise about their long \u03b12 helices in an anti-parallel orientation, and, in the case of H3 and H4, two such dimers form a 4-helix bundle stabilised by extensive H3-H3' interaction. The H2A/H2B dimer binds onto the H3/H4 tetramer due to interactions between H4 and H2B, which include the formation of a hydrophobic cluster.\nThe histone octamer is formed by a central H3/H4 tetramer sandwiched between two H2A/H2B dimers. Due to the highly basic charge of all four core histones, the histone octamer is stable only in the presence of DNA or very high salt concentrations.\nHistone - DNA interactions.\nThe nucleosome contains over 120 direct protein-DNA interactions and several hundred water-mediated ones. Direct protein - DNA interactions are not spread evenly about the octamer surface but rather located at discrete sites. These are due to the formation of two types of DNA binding sites within the octamer; the \u03b11\u03b11 site, which uses the \u03b11 helix from two adjacent histones, and the L1L2 site formed by the L1 and L2 loops. Salt links and hydrogen bonding between both side-chain basic and hydroxyl groups and main-chain amides with the DNA backbone phosphates form the bulk of interactions with the DNA. This is important, given that the ubiquitous distribution of nucleosomes along genomes requires it to be a non-sequence-specific DNA-binding factor. Although nucleosomes tend to prefer some DNA sequences over others, they are capable of binding practically to any sequence, which is thought to be due to the flexibility in the formation of these water-mediated interactions. In addition, non-polar interactions are made between protein side-chains and the deoxyribose groups, and an arginine side-chain intercalates into the DNA minor groove at all 14 sites where it faces the octamer surface.\nThe distribution and strength of DNA-binding sites about the octamer surface distorts the DNA within the nucleosome core. The DNA is non-uniformly bent and also contains twist defects. The twist of free B-form DNA in solution is 10.5 bp per turn. However, the overall twist of nucleosomal DNA is only 10.2 bp per turn, varying from a value of 9.4 to 10.9 bp per turn.\nHistone tail domains.\nThe histone tail extensions constitute up to 30% by mass of histones, but are not visible in the crystal structures of nucleosomes due to their high intrinsic flexibility, and have been thought to be largely unstructured. The N-terminal tails of histones H3 and H2B pass through a channel formed by the minor grooves of the two DNA strands, protruding from the DNA every 20 bp. The N-terminal tail of histone H4, on the other hand, has a region of highly basic amino acids (16\u201325), which, in the crystal structure, forms an interaction with the highly acidic surface region of a H2A-H2B dimer of another nucleosome, being potentially relevant for the higher-order structure of nucleosomes. This interaction is thought to occur under physiological conditions also, and suggests that acetylation of the H4 tail distorts the higher-order structure of chromatin.\nHigher order structure.\nThe organization of the DNA that is achieved by the nucleosome cannot fully explain the packaging of DNA observed in the cell nucleus. Further compaction of chromatin into the cell nucleus is necessary, but it is not yet well understood. The current understanding is that repeating nucleosomes with intervening \"linker\" DNA form a \"10-nm-fiber\", described as \"beads on a string\", and have a packing ratio of about five to ten. A chain of nucleosomes can be arranged in a \"30\u00a0nm fiber\", a compacted structure with a packing ratio of ~50 and whose formation is dependent on the presence of the H1 histone.\nA crystal structure of a tetranucleosome has been presented and used to build up a proposed structure of the 30\u00a0nm fiber as a two-start helix.\nThere is still a certain amount of contention regarding this model, as it is incompatible with recent electron microscopy data. Beyond this, the structure of chromatin is poorly understood, but it is classically suggested that the 30\u00a0nm fiber is arranged into loops along a central protein scaffold to form transcriptionally active euchromatin. Further compaction leads to transcriptionally inactive heterochromatin.\nDynamics.\nAlthough the nucleosome is a very stable protein-DNA complex, it is not static and has been shown to undergo a number of different structural re-arrangements including nucleosome sliding and DNA site exposure. Depending on the context, nucleosomes can inhibit or facilitate transcription factor binding. Nucleosome positions are controlled by three major contributions: First, the intrinsic binding affinity of the histone octamer depends on the DNA sequence. Second, the nucleosome can be displaced or recruited by the competitive or cooperative binding of other protein factors. Third, the nucleosome may be actively translocated by ATP-dependent remodeling complexes.\nNucleosome sliding.\nWhen incubated thermally, nucleosomes reconstituted onto the 5S DNA positioning sequence were able to reposition themselves translationally onto adjacent sequences. This repositioning does not require disruption of the histone octamer but is consistent with nucleosomes being able to \"slide\" along the DNA \"in cis\". CTCF binding sites act as nucleosome positioning anchors so that, when used to align various genomic signals, multiple flanking nucleosomes can be readily identified. Although nucleosomes are intrinsically mobile, eukaryotes have evolved a large family of ATP-dependent chromatin remodelling enzymes to alter chromatin structure, many of which do so via nucleosome sliding. Nucleosome sliding is one of the possible mechanism for large scale tissue specific expression of genes. The transcription start site for genes expressed in a particular tissue, are nucleosome depleted while, the same set of genes in other tissue where they are not expressed, are nucleosome bound.\nDNA site exposure.\nNucleosomal DNA is in equilibrium between a wrapped and unwrapped state. DNA within the nucleosome remains fully wrapped for only 250 ms before it is unwrapped for 10-50 ms and then rapidly rewrapped, as measured using time-resolved FRET. This implies that DNA does not need to be actively dissociated from the nucleosome but that there is a significant fraction of time during which it is fully accessible. Introducing a DNA-binding sequence within the nucleosome increases the accessibility of adjacent regions of DNA when bound.\nThis propensity for DNA within the nucleosome to \"breathe\" has important functional consequences for all DNA-binding proteins that operate in a chromatin environment. In particular, the dynamic breathing of nucleosomes plays an important role in restricting the advancement of RNA polymerase II during transcription elongation.\nNucleosome free region.\nPromoters of active genes have nucleosome free regions (NFR). This allows for promoter DNA accessibility to various proteins, such as transcription factors. Nucleosome free region typically spans for 200 nucleotides in \"S. cerevisiae\" Well-positioned nucleosomes form boundaries of NFR. These nucleosomes are called +1-nucleosome and \u22121-nucleosome and are located at canonical distances downstream and upstream, respectively, from transcription start site. +1-nucleosome and several downstream nucleosomes also tend to incorporate H2A.Z histone variant.\nModulating nucleosome structure.\nEukaryotic genomes are ubiquitously associated into chromatin; however, cells must spatially and temporally regulate specific loci independently of bulk chromatin. In order to achieve the high level of control required to co-ordinate nuclear processes such as DNA replication, repair, and transcription, cells have developed a variety of means to locally and specifically modulate chromatin structure and function. This can involve covalent modification of histones, the incorporation of histone variants, and non-covalent remodelling by ATP-dependent remodeling enzymes.\nHistone post-translational modifications.\nSince they were discovered in the mid-1960s, histone modifications have been predicted to affect transcription. The fact that most of the early post-translational modifications found were concentrated within the tail extensions that protrude from the nucleosome core lead to two main theories regarding the mechanism of histone modification. The first of the theories suggested that they may affect electrostatic interactions between the histone tails and DNA to \"loosen\" chromatin structure. Later it was proposed that combinations of these modifications may create binding epitopes with which to recruit other proteins. Recently, given that more modifications have been found in the structured regions of histones, it has been put forward that these modifications may affect histone-DNA and histone-histone interactions within the nucleosome core. Modifications (such as acetylation or phosphorylation) that lower the charge of the globular histone core are predicted to \"loosen\" core-DNA association; the strength of the effect depends on location of the modification within the core.\nSome modifications have been shown to be correlated with gene silencing; others seem to be correlated with gene activation. Common modifications include acetylation, methylation, or ubiquitination of lysine; methylation of arginine; and phosphorylation of serine. The information stored in this way is considered epigenetic, since it is not encoded in the DNA but is still inherited to daughter cells. The maintenance of a repressed or activated status of a gene is often necessary for cellular differentiation.\nHistone variants.\nAlthough histones are remarkably conserved throughout evolution, several variant forms have been identified. This diversification of histone function is restricted to H2A and H3, with H2B and H4 being mostly invariant. H2A can be replaced by H2AZ (which leads to reduced nucleosome stability) or H2AX (which is associated with DNA repair and T cell differentiation), whereas the inactive X chromosomes in mammals are enriched in macroH2A. H3 can be replaced by H3.3 (which correlates with activate genes and regulatory elements) and in centromeres H3 is replaced by CENPA.\nATP-dependent nucleosome remodeling.\nA number of distinct reactions are associated with the term ATP-dependent chromatin remodeling. Remodeling enzymes have been shown to slide nucleosomes along DNA, disrupt histone-DNA contacts to the extent of destabilizing the H2A/H2B dimer and to generate negative superhelical torsion in DNA and chromatin. Recently, the Swr1 remodeling enzyme has been shown to introduce the variant histone H2A.Z into nucleosomes. At present, it is not clear if all of these represent distinct reactions or merely alternative outcomes of a common mechanism. What is shared between all, and indeed the hallmark of ATP-dependent chromatin remodeling, is that they all result in altered DNA accessibility.\nStudies looking at gene activation \"in vivo\" and, more astonishingly, remodeling \"in vitro\" have revealed that chromatin remodeling events and transcription-factor binding are cyclical and periodic in nature. While the consequences of this for the reaction mechanism of chromatin remodeling are not known, the dynamic nature of the system may allow it to respond faster to external stimuli. A recent study indicates that nucleosome positions change significantly during mouse embryonic stem cell development, and these changes are related to binding of developmental transcription factors.\nDynamic nucleosome remodelling across the Yeast genome.\nStudies in 2007 have catalogued nucleosome positions in yeast and shown that nucleosomes are depleted in promoter regions and origins of replication.\nAbout 80% of the yeast genome appears to be covered by nucleosomes and the pattern of nucleosome positioning clearly relates to DNA regions that regulate transcription, regions that are transcribed and regions that initiate DNA replication. Most recently, a new study examined \"dynamic changes\" in nucleosome repositioning during a global transcriptional reprogramming event to elucidate the effects on nucleosome displacement during genome-wide transcriptional changes in yeast (\"Saccharomyces cerevisiae\"). The results suggested that nucleosomes that were localized to promoter regions are displaced in response to stress (like heat shock). In addition, the removal of nucleosomes usually corresponded to transcriptional activation and the replacement of nucleosomes usually corresponded to transcriptional repression, presumably because transcription factor binding sites became more or less accessible, respectively. In general, only one or two nucleosomes were repositioned at the promoter to effect these transcriptional changes. However, even in chromosomal regions that were not associated with transcriptional changes, nucleosome repositioning was observed, suggesting that the covering and uncovering of transcriptional DNA does not necessarily produce a transcriptional event. After transcription, the rDNA region has to protected from any damage, it suggested HMGB proteins play a major role in protecting the nucleosome free region.\nDNA Twist Defects.\nDNA twist defects are when the addition of one or a few base pairs from one DNA segment are transferred to the next segment resulting in a change of the DNA twist. This will not only change the twist of the DNA but it will also change the length. This twist defect eventually moves around the nucleosome through the transferring of the base pair, this means DNA twists can cause nucleosome sliding. Nucleosome crystal structures have shown that superhelix location 2 and 5 on the nucleosome are commonly found to be where DNA twist defects occur as these are common remodeler binding sites. There are a variety of chromatin remodelers but all share the existence of an ATPase motor which facilitates chromatin sliding on DNA through the binding and hydrolysis of ATP. ATPase has an open and closed state. When the ATPase motor is changing from open and closed states, the DNA duplex changes geometry and exhibits base pair tilting. The initiation of the twist defects via the ATPase motor causes tension to accumulate around the remodeler site. The tension is released when the sliding of DNA has been completed throughout the nucleosome via the spread of two twist defects (one on each strand) in opposite directions.\nNucleosome assembly \"in vitro\".\nNucleosomes can be assembled \"in vitro\" by either using purified native or recombinant histones. One standard technique of loading the DNA around the histones involves the use of salt dialysis. A reaction consisting of the histone octamers and a naked DNA template can be incubated together at a salt concentration of 2 M. By steadily decreasing the salt concentration, the DNA will equilibrate to a position where it is wrapped around the histone octamers, forming nucleosomes. In appropriate conditions, this reconstitution process allows for the nucleosome positioning affinity of a given sequence to be mapped experimentally.\nDisulfide crosslinked nucleosome core particles.\nA recent advance in the production of nucleosome core particles with enhanced stability involves site-specific disulfide crosslinks. Two different crosslinks can be introduced into the nucleosome core particle. A first one crosslinks the two copies of H2A via an introduced cysteine (N38C) resulting in histone octamer which is stable against H2A/H2B dimer loss during nucleosome reconstitution. A second crosslink can be introduced between the H3 N-terminal histone tail and the nucleosome DNA ends via an incorporated convertible nucleotide. The DNA-histone octamer crosslink stabilizes the nucleosome core particle against DNA dissociation at very low particle concentrations and at elevated salt concentrations.\nNucleosome assembly \" in vivo \".\nNucleosomes are the basic packing unit of genomic DNA built from histone proteins around which DNA is coiled. They serve as a scaffold for formation of higher order chromatin structure as well as for a layer of regulatory control of gene expression. Nucleosomes are quickly assembled onto newly synthesized DNA behind the replication fork.\nH3 and H4.\nHistones H3 and H4 from disassembled old nucleosomes are kept in the vicinity and randomly distributed on the newly synthesized DNA. They are assembled by the chromatin assembly factor 1 (CAF-1) complex, which consists of three subunits (p150, p60, and p48). Newly synthesized H3 and H4 are assembled by the replication coupling assembly factor (RCAF). RCAF contains the subunit Asf1, which binds to newly synthesized H3 and H4 proteins. The old H3 and H4 proteins retain their chemical modifications which contributes to the passing down of the epigenetic signature. The newly synthesized H3 and H4 proteins are gradually acetylated at different lysine residues as part of the chromatin maturation process. It is also thought that the old H3 and H4 proteins in the new nucleosomes recruit histone modifying enzymes that mark the new histones, contributing to epigenetic memory.\nH2A and H2B.\nIn contrast to old H3 and H4, the old H2A and H2B histone proteins are released and degraded; therefore, newly assembled H2A and H2B proteins are incorporated into new nucleosomes. H2A and H2B are assembled into dimers which are then loaded onto nucleosomes by the nucleosome assembly protein-1 (NAP-1) which also assists with nucleosome sliding. The nucleosomes are also spaced by ATP-dependent nucleosome-remodeling complexes containing enzymes such as Isw1 Ino80, and Chd1, and subsequently assembled into higher order structure.\nGallery.\nThe crystal structure of the nucleosome core particle (PDB: https://\u200b) - different views showing details of histone folding and organization. Histones H2A, H2B, H3, H4 and DNA are coloured.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21847", "revid": "73244", "url": "https://en.wikipedia.org/wiki?curid=21847", "title": "Nordic", "text": "Nordic most commonly refers to:\nNordic may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "21848", "revid": "50997737", "url": "https://en.wikipedia.org/wiki?curid=21848", "title": "Neurosurgery", "text": "Medical specialty of disorders which affect any portion of the nervous system\nNeurosurgery or/and neurological surgery, known in common parlance as brain surgery, is the medical specialty that focuses on the surgical treatment or rehabilitation of disorders which affect any portion of the nervous system including the brain, spinal cord, peripheral nervous system, and cerebrovascular system. Neurosurgery as a medical specialty also includes non-surgical management of some neurological conditions.\nEducation and context.\nIn different countries, there are different requirements for an individual to legally practice neurosurgery, and there are varying methods through which they must be educated. In most countries, neurosurgeon training requires a minimum period of seven years after graduating from medical school.\nCanada.\nIn Canada, neurosurgery residency is overseen by the Royal College of Physicians and Surgeons of Canada (RCPSC). To qualify, candidates must hold a Doctor of Medicine (M.D.) degree and be licensed physicians. The residency program lasts six years, often with one year of mandatory research as in University of Calgary, and it comprises two years of \"Surgical Foundations\" and four years of specialized neurosurgery training. Admission is facilitated through the Canadian Resident Matching Service (CaRMS), which matches candidates to programs based on academic credentials, interviews, and references. Training requirements and certification processes differ slightly in Quebec, where the Coll\u00e8ge des m\u00e9decins du Qu\u00e9bec (CMQ) collaborates with RCPSC, but has french language requirements and different application procedure. Upon completion, residents take the RCPSC examination to earn the Fellowship of the Royal College of Physicians and Surgeons of Canada (FRCSC) designation.\nOn the other hand, to qualify for residency, International Medical Graduates must pass equivalent licensing exams, including the Medical Council of Canada Qualifying Examination Part I (MCCQE Part I) and the National Assessment Collaboration (NAC) Objective Structured Clinical Examination, to be eligible for residency. They apply through CaRMS, where competition is high, and may face additional requirements in Quebec due to french language fluency expectations.\nIndia.\nIn India, neurosurgery training is overseen by the National Medical Commission (NMC) and the qualifying examinations by National Board of Examinations in Medical Sciences (NBEMS). To qualify, candidates must hold a Bachelor of Medicine, Bachelor of Surgery (MBBS) degree with at least 55% aggregate marks from a WHO recognized institution, complete a one-year compulsory rotating internship, and possess a practising medical license. The pathway spans three to six years post MBBS. A three-year residency cum degree of Master of Surgery (M.S.) in neurosurgery is the basic qualification of a neurosurgeon in India. Physicians can opt for super specialization of three years i.e., Master of Chirurgiae (M.Ch.) after completing Master of Surgery (M.S.) in General Surgery or Neurosurgery. Qualifying exams for specialisation (M.S.) are \u2014 NEET (PG) for admission into general medical colleges and INI CET for admission into Institutes of National Importance, such as AIIMS, JIPMER, NIMHANS and PGIMER. Super speciality selection exams are NEET SS and INI SS similarly. Neurosurgery is the one of the most competitive specialities in India with fewer than 200 seats annually. Foreign Medical Graduates (FMG) are required to pass FMGE for registration into postgraduate training.\nUnited Kingdom.\nIn the United Kingdom, students must gain entry into medical school. The MBBS qualification (Bachelor of Medicine, Bachelor of Surgery) takes four to six years depending on the student's route. The newly qualified physician must then complete foundation training lasting two years; this is a paid training program in a hospital or clinical setting covering a range of medical specialties including surgery. Junior doctors then apply to enter the neurosurgical pathway. Unlike most other surgical specialties, it currently has its own independent training pathway which takes around eight years (ST1-8); before being able to sit for consultant exams with sufficient amounts of experience and practice behind them. Neurosurgery remains consistently amongst the most competitive medical specialties in which to obtain entry.\nUnited States.\nIn the United States, a neurosurgeon must generally complete four years of undergraduate education, four years of medical school, and seven years of residency (PGY-1-7). Most, but not all, residency programs have some component of basic science or clinical research. Neurosurgeons may pursue additional training in the form of a fellowship after residency, or, in some cases, as a senior resident in the form of an enfolded fellowship. These fellowships include pediatric neurosurgery, trauma/neurocritical care, functional and stereotactic surgery, surgical neuro-oncology, radiosurgery, neurovascular surgery, skull-base surgery, peripheral nerve and complex spinal surgery. Fellowships typically span one to two years. In the U.S., neurosurgery is a very small, highly competitive specialty, constituting only 0.5 percent of all physicians.\nHistory.\nNeurosurgery, or the premeditated incision into the head for pain relief, has been around for thousands of years, but notable advancements in neurosurgery have only come within the last hundred years.\nAncient.\nDuring the Roman Empire, doctors and surgeons performed neurosurgery on depressed skull fractures. The Incas appear to have practiced a procedure known as trepanation since before European colonization. During the Middle Ages in Al-Andalus from 936 to 1013 AD, Al-Zahrawi performed surgical treatments of head injuries, skull fractures, spinal injuries, hydrocephalus, subdural effusions and headache. Simple forms of neurosurgery were performed on King Henri II in 1559, after a jousting accident with Gabriel Montgomery fatally wounded him. Ambroise Par\u00e9 and Andreas Vesalius, both experts in their field at the time, \nattempted their own methods, to no avail, in curing Henri. In China, Hua Tuo created the first general anaesthesia called mafeisan, which he used on surgical procedures on the brain.\nModern.\nHistory of tumor removal: In 1879, after locating it via neurological signs alone, Scottish surgeon William Macewen (1848\u20131924) performed the first successful brain tumor removal. On November 25, 1884, after English physician Alexander Hughes Bennett (1848\u20131901) used Macewen's technique to locate it, English surgeon Rickman Godlee (1849\u20131925) performed the first primary brain tumor removal, which differs from Macewen's operation in that Bennett operated on the exposed brain, whereas Macewen operated outside of the \"brain proper\" via trepanation. On March 16, 1907, Austrian surgeon Hermann Schloffer became the first to successfully remove a pituitary tumor.\nLobotomy: also known as leucotomy, was a form of psychosurgery, a neurosurgical treatment of mental disorders that involves severing connections in the brain's prefrontal cortex. The originator of the procedure, Portuguese neurologist Ant\u00f3nio Egas Moniz, shared the Nobel Prize for Physiology or Medicine of 1949. Some patients improved in some ways after the operation, but complications and impairments\u00a0\u2013 sometimes severe\u00a0\u2013 were frequent. The procedure was controversial from its initial use, in part due to the balance between benefits and risks. It is mostly rejected as a treatment now and non-compliant with patients' rights.\nHistory of electrodes in the brain: In 1878, Richard Caton discovered that electrical signals transmitted through an animal's brain. In 1950 Jose Delgado invented the first electrode that was implanted in an animal's brain (bull), using it to make it run and change direction. In 1972 the cochlear implant, a neurological prosthetic that allowed deaf people to hear was marketed for commercial use. In 1998 researcher Philip Kennedy implanted the first Brain Computer Interface (BCI) into a human subject.\nA survey done in 2010 on 100 most cited works in neurosurgery shows that the works mainly cover clinical trials evaluating surgical and medical therapies, descriptions of novel techniques in neurosurgery, and descriptions of systems classifying and grading diseases.\nModern surgical instruments.\nThe main advancements in neurosurgery came about as a result of highly crafted tools. Modern neurosurgical tools, or instruments, include chisels, curettes, dissectors, distractors, elevators, forceps, hooks, impactors, probes, suction tubes, power tools, and robots. Most of these modern tools have been in medical practice for a relatively long time. The main difference of these tools in neurosurgery, were the precision in which they were crafted. These tools are crafted with edges that are within a millimeter of desired accuracy. Other tools, such as handheld power saws and robots, have only recently been commonly used inside of a neurological operating room. As an example, the University of Utah developed a device for computer-aided design / computer-aided manufacturing (CAD-CAM) which uses an image-guided system to define a cutting tool path for a robotic cranial drill.\nOrganised neurosurgery.\nThe World Federation of Neurosurgical Societies (WFNS) was founded in 1955 in Switzerland as a professional, scientific, non governmental organization. It is composed of 130 member societies: consisting of 5 Continental Associations (AANS, AASNS, CAANS, EANS and FLANC), 6 Affiliate Societies, and 119 National Neurosurgical Societies, representing some 50,000 neurosurgeons worldwide. It has a consultative status in the United Nations. The official Journal of the Organization is World Neurosurgery. The other global organisations being the World Academy of Neurological Surgery (WANS) and the World Federation of Skull Base Societies (WFSBS).\nMain divisions.\nGeneral neurosurgery involves most neurosurgical conditions including neuro-trauma and other neuro-emergencies such as intracranial hemorrhage. Most level 1 hospitals have this kind of practice.\nSpecialized branches have developed to cater to special and difficult conditions. These specialized branches co-exist with general neurosurgery in more sophisticated hospitals. To practice advanced specialization within neurosurgery, additional higher fellowship training of one to two years is expected from the neurosurgeon.\nSome of these divisions of neurosurgery are:\nCommonly performed surgeries.\nAccording to an analysis by the American College of Surgeons National Surgical Quality Improvement Program (NSQIP), the most common surgeries performed by neurosurgeons in between 2006 and 2014 were the following:\nNeuropathology.\nNeuropathology is a specialty within the study of pathology focused on the disease of the brain, spinal cord, and neural tissue. This includes the central nervous system and the peripheral nervous system. Tissue analysis comes from either surgical biopsies or post mortem autopsies. Common tissue samples include muscle fibers and nervous tissue. Common applications of neuropathology include studying samples of tissue in patients who have Parkinson's disease, Alzheimer's disease, dementia, Huntington's disease, amyotrophic lateral sclerosis, mitochondria disease, and any disorder that has neural deterioration in the brain or spinal cord.\nHistory.\nWhile pathology has been studied for millennia only within the last few hundred years has medicine focused on a tissue- and organ-based approach to tissue disease. In 1810, Thomas Hodgkin started to look at the damaged tissue for the cause. This was conjoined with the emergence of microscopy and started the current understanding of how the tissue of the human body is studied.\nNeuroanesthesia.\nNeuroanesthesia is a field of anesthesiology which focuses on neurosurgery. Anesthesia is not used during the middle of an \"awake\" brain surgery. Awake brain surgery is where the patient is conscious for the middle of the procedure and sedated for the beginning and end. This procedure is used when the tumor does not have clear boundaries and the surgeon wants to know if they are invading on critical regions of the brain which involve functions like talking, cognition, vision, and hearing. It will also be conducted for procedures which the surgeon is trying to combat epileptic seizures.\nHistory.\nThe physician Hippocrates (460\u2013370 BCE) made accounts of using different wines to sedate patients while trepanning. In 60 CE, Dioscorides, a physician, pharmacologist, and botanist, detailed how mandrake, henbane, opium, and alcohol were used to put patients to sleep during trepanning. In 972 CE, two brother surgeons in Paramara, now India, used \"samohine\" to sedate a patient while removing a small tumor, and awoke the patient by pouring onion and vinegar in the patient's mouth. The combination of carbon dioxide, hydrogen, and nitrogen, was a form of neuroanesthesia adopted in the 18th century and introduced by Humphry Davy.\nNeurosurgery methods.\nVarious Imaging methods are used in modern neurosurgery diagnosis and treatment. They include computer assisted imaging computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), magnetoencephalography (MEG), and stereotactic radiosurgery. Some neurosurgery procedures involve the use of intra-operative MRI and functional MRI.\nIn \"conventional neurosurgery\" the neurosurgeon opens the skull, creating a large opening to access the brain. Techniques involving smaller openings with the aid of microscopes and endoscopes are now being used as well. Methods that utilize small craniotomies in conjunction with high-clarity microscopic visualization of neural tissue offer excellent results. However, the open methods are still traditionally used in trauma or emergency situations.\n\"Microsurgery\" is utilized in many aspects of neurological surgery. Microvascular techniques are used in EC-IC bypass surgery and in restoration carotid endarterectomy. The clipping of an aneurysm is performed under microscopic vision. Minimally-invasive spine surgery utilizes microscopes or endoscopes. Procedures such as microdiscectomy, laminectomy, and artificial disc replacement rely on microsurgery.\nUsing \"stereotaxy\" neurosurgeons can approach a minute target in the brain through a minimal opening. This is used in functional neurosurgery where electrodes are implanted or gene therapy is instituted with high level of accuracy as in the case of Parkinson's disease or Alzheimer's disease. Using the combination method of open and stereotactic surgery, intraventricular hemorrhages can potentially be evacuated successfully. Conventional surgery using image guidance technologies is also becoming common and is referred to as surgical navigation, computer-assisted surgery, navigated surgery, stereotactic navigation. Similar to a car or mobile Global Positioning System (GPS), image-guided surgery systems, like Curve Image Guided Surgery and StealthStation, use cameras or electromagnetic fields to capture and relay the patient's anatomy and the surgeon's precise movements in relation to the patient, to computer monitors in the operating room. These sophisticated computerized systems are used before and during surgery to help orient the surgeon with three-dimensional images of the patient's anatomy including the tumor. Real-time functional brain mapping has been employed to identify specific functional regions using electrocorticography (ECoG)\nMinimally invasive \"endoscopic surgery\" is commonly utilized by neurosurgeons when appropriate. Techniques such as endoscopic endonasal surgery are used in pituitary tumors, craniopharyngiomas, chordomas, and the repair of cerebrospinal fluid leaks. Ventricular endoscopy is used in the treatment of intraventricular bleeds, hydrocephalus, colloid cyst and neurocysticercosis. Endonasal endoscopy is at times carried out with neurosurgeons and ENT surgeons working together as a team.\nRepair of craniofacial disorders and disturbance of cerebrospinal fluid circulation is done by neurosurgeons who also occasionally team up with maxillofacial and plastic surgeons. Cranioplasty for craniosynostosis is performed by pediatric neurosurgeons with or without plastic surgeons.\nNeurosurgeons are involved in \"stereotactic radiosurgery\" along with radiation oncologists in tumor and AVM treatment. Radiosurgical methods such as Gamma knife, Cyberknife and Novalis Radiosurgery are used as well.\n\"Endovascular neurosurgery\" utilize endovascular image guided procedures for the treatment of aneurysms, AVMs, carotid stenosis, strokes, and spinal malformations, and vasospasms. Techniques such as angioplasty, stenting, clot retrieval, embolization, and diagnostic angiography are endovascular procedures.\nA common procedure performed in neurosurgery is the placement of ventriculo-peritoneal shunt (VP shunt). In pediatric practice this is often implemented in cases of congenital hydrocephalus. The most common indication for this procedure in adults is normal pressure hydrocephalus (NPH).\n\"Neurosurgery of the spine\" covers the cervical, thoracic and lumbar spine. Some indications for spine surgery include spinal cord compression resulting from trauma, arthritis of the spinal discs, or spondylosis. In cervical cord compression, patients may have difficulty with gait, balance issues, and/or numbness and tingling in the hands or feet. Spondylosis is the condition of spinal disc degeneration and arthritis that may compress the spinal canal. This condition can often result in bone-spurring and disc herniation. Power drills and special instruments are often used to correct any compression problems of the spinal canal. Disc herniations of spinal vertebral discs are removed with special rongeurs. This procedure is known as a \"discectomy\". Generally once a disc is removed it is replaced by an implant which will create a bony fusion between vertebral bodies above and below. Instead, a mobile disc could be implanted into the disc space to maintain mobility. This is commonly used in cervical disc surgery. At times instead of disc removal a Laser discectomy could be used to decompress a nerve root. This method is mainly used for lumbar discs. \"Laminectomy\" is the removal of the lamina of the vertebrae of the spine in order to make room for the compressed nerve tissue.\nSurgery for chronic pain is a sub-branch of functional neurosurgery. Some of the techniques include implantation of deep brain stimulators, spinal cord stimulators, peripheral stimulators and pain pumps.\nSurgery of the peripheral nervous system is also possible, and includes the very common procedures of carpal tunnel decompression and peripheral nerve transposition. Numerous other types of nerve entrapment conditions and other problems with the peripheral nervous system are treated as well.\nConditions.\nConditions treated by neurosurgeons include, but are not limited to:\nRecovery.\nPostoperative pain.\nPain following brain surgery can be significant and may lengthen recovery, increase the amount of time a person stays in the hospital following surgery, and increase the risk of complications following surgery. Severe acute pain following brain surgery may also increase the risk of a person developing a chronic post-craniotomy headache. Approaches to treating pain in adults include treatment with nonsteroidal anti\u2010inflammatory drugs (NSAIDs), which have been shown to reduce pain for up to 24 hours following surgery. Low-quality evidence supports the use of the medications dexmedetomidine, pregabalin or gabapentin to reduce post-operative pain. Low-quality evidence also supports scalp blocks and scalp infiltration to reduce postoperative pain. Gabapentin or pregabalin may also decrease vomiting and nausea following surgery, based on very low-quality medical evidence.\nBioethics in neurosurgery.\nNeurosurgery is a part of practical medicine and the only specialty that involves invasive intervention in the activity of the living brain. The brain ensures the structural and functional integrity of the body and the implementation of all the main life processes of the body. Therefore, neurosurgery faces a wide range of bioethical issues and a significant selection of the latest treatment technologies.\nNeurosurgery has the following applied scientific and ethical problems:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21849", "revid": "51041501", "url": "https://en.wikipedia.org/wiki?curid=21849", "title": "Nintendo 64", "text": "Home video game console\nThe (N64) is a home video game console developed and marketed by Nintendo. It was released in Japan on June 23, 1996, in North America on September 29, 1996, in Europe and Australia on March 1, 1997. As the successor to the Super Nintendo Entertainment System (SNES), the N64 was the last major home console to use ROM cartridges as its primary storage medium. As a fifth-generation console, the Nintendo 64 primarily competed with Sony's PlayStation and the Sega Saturn.\nDevelopment of the N64 began in 1993 in collaboration with Silicon Graphics, initially codenamed Project Reality and later tested as the Ultra 64 arcade platform. The console was named for its 64-bit CPU. Although its design was largely finalized by mid-1995, the console\u2019s release was delayed until 1996 to allow for the completion of the console's launch titles, \"Super Mario 64\", \"Pilotwings 64\", and the Japan-exclusive \"Saiky\u014d Habu Sh\u014dgi.\"\nThe N64's original charcoal-gray console was later joined by several color variants. Certain games required the Expansion Pak to boost system RAM from 4 to 8\u00a0MB, improving both graphics and gameplay functionality. The console supported saved game storage either on cartridges or the optional Controller Pak accessory. The 64DD magnetic disc peripheral offered additional storage for game content and enabled the Randnet online service. However, due to a delayed launch, the 64DD was a commercial failure and was released exclusively in Japan.\nIn 1996, \"Time\" magazine named the N64 its Machine of the Year, and in 2011, \"IGN\" ranked it as the ninth-greatest video game console of all time. Though the N64 sold over 32 million units globally, it was ultimately discontinued worldwide on April 30, 2002 following the release of its successor, the GameCube. While it was critically acclaimed, the N64 faced commercial challenges; its sales lagged behind the PlayStation, and underperformed in both Japan and Europe, despite strong performance in the United States.\nHistory.\nBackground.\nFollowing the video game crash of 1983, Nintendo revitalized the industry with the release of its second home console, the Family Computer (Famicom), launched in Japan in 1983 and later introduced internationally as the Nintendo Entertainment System (NES) in 1985. Both the NES and its successor, the Super Nintendo Entertainment System (SNES), achieved significant commercial success. However, SNES sales declined during the Japanese economic recession. At the same time, competition intensified with the arrival of the Sega Saturn, a 32-bit console, which outpaced the aging 16-bit SNES and highlighted the urgency for Nintendo to upgrade its hardware or risk losing market share. Additional competition came from Atari's 5200, 7800, Lynx, and Jaguar systems.\nIn an effort to extend the SNES\u2019s lifespan, Nintendo explored the development of a CD-ROM peripheral through partnerships with CD-ROM technology pioneers Philips and Sony. Despite the creation of early hardware prototypes, both collaborations ultimately collapsed, and no games were released by Nintendo or its third-party partners. Philips retained limited licensing rights and used them to release original \"Mario\" and \"Legend of Zelda\" games on its competing CD-i device. Meanwhile, Sony leveraged its progress to develop what would become the PlayStation console. During this period, third-party developers also expressed growing dissatisfaction with Nintendo\u2019s strict licensing policies.\nDevelopment.\nSilicon Graphics, Inc. (SGI), a long-established leader in high-performance computing, sought to expand by adapting its supercomputing technology into the higher volume consumer market, starting with the video game industry. To support this shift, SGI redesigned its MIPS R4000 CPU family, reducing power consumption, and aimed to lower unit cost from up to US$ to approximately $. SGI developed a video game chipset prototype and sought an established industry partner. SGI founder Jim Clark first pitched the concept to Tom Kalinske, CEO of Sega of America, who said they were \"quite impressed.\" However, Sega\u2019s Japanese engineers rejected the design, citing technical issues, which SGI later resolved. Nintendo disputes this account, claiming SGI ultimately favored Nintendo because Sega had demanded exclusive rights to the technology, while Nintendo was open to a non-exclusive licensing agreement.\nIn early 1993, Clark met with Nintendo president Hiroshi Yamauchi. By August 23, during Nintendo's annual Shoshinkai trade show, the companies announced a joint development and licensing agreement for what they called \"Project Reality.\" They projected an arcade debut in 1994 and a home release by late 1995, targeting a retail price under $. Michael Slater, publisher of Microprocessor Report highlighted the significance of the partnership saying, \"The mere fact of a business relationship there is significant because of Nintendo's phenomenal ability to drive volume. If it works at all, it could bring MIPS to levels of volume [SGI] never dreamed of.\"\nSGI named the console\u2019s core chipset \"Reality Immersion Technology\", featuring MIPS R4300i CPU and the Reality Coprocessor for graphics, audio, and memory management). NEC, Toshiba, and Sharp would provide manufacturing support. The chipset was a collaborative effort between SGI and its subsidiary, MIPS Technologies. SGI and Nintendo also partnered with Rambus, designing a bus architecture to transfer data at 500 Mb/s using its proprietary RDRAM. Rambus hoped the partnership would encourage RDRAM adoption in PCs.\nTo enable game creation before the hardware was finalized, SGI offered a development platform based on the Onyx supercomputer to simulate expected console performance. The Onyx was priced at up to $. It included a $ RealityEngine2 graphics board and four 150\u00a0MHz R4400 CPUs. Once the chipset was finalized, the supercomputing setup was replaced by a simulation board integrated into low-end SGI Indy workstation in July 1995. SGI's early performance estimates proved largely accurate; LucasArts, for instance, ported a prototype \"Star Wars\" game to the final hardware in just three days.\nOn June 23, 1994, at the Consumer Electronics Show, Nintendo announced that the upcoming console would be named the \"Ultra 64\". The console design was shown, but its controller remained under wraps. The most controversial detail was Nintendo\u2019s decision to use limited-capacity ROM cartridges rather than the increasingly popular CD-ROM format, despite previous development work for a CD-based SNES. Nintendo defended the decision, citing the performance advantages of cartridges. The Ultra 64 was marketed as the world\u2019s first 64-bit console. Though Atari had previously advertised the Jaguar as a 64-bit system, its architecture used two 32-bit coprocessors and a 16/32-bit Motorola 68000 CPU, falling short of Nintendo\u2019s full 64-bit implementation.\nLater in 1994, Nintendo signed a licensing agreement with arcade giant Williams. The company's Midway studio would develop Ultra 64-branded arcade titles, including \"Killer Instinct\" and \"Cruis\u2019n USA\". However, these arcade machines used hardware distinct from the home console: they lacked the Reality Coprocessor, used different MIPS CPUs, and relied on hard drives instead of cartridges to store game data. The expanded storage enabled games like \"Killer Instinct\" to incorporate pre-rendered 3D character sprites and full-motion video backgrounds.\nIn April 1995, it introduced its \"Dream Team\" of developers. Graphic development tools were provided by Alias Research and MultiGen, while Software Creations provided audio tools. Game development studios included Acclaim, Angel Studios, DMA Design, GameTek, Midway, Paradigm, Rare, Sierra On-Line, and Spectrum HoloByte. Despite the initial hype, the Dream Team did not live up to expectations. Some studios like GameTek failed to deliver games, while only a few, including Rare, Acclaim, and Midway, made a significant impact.\nNintendo originally planned to launch the console as the \"Ultra Famicom\" in Japan and \"Nintendo Ultra 64\" internationally. While rumors claimed trademark conflicts with Konami's Ultra Games prompted a name change, Nintendo denied this, citing a desire for a unified global brand. The final name \"Nintendo 64\" was proposed by \"Earthbound\" creator Shigesato Itoi. Still, the original name lived on in the console's model numbering prefix \"NUS-\", widely believed to stand for \"Nintendo Ultra Sixty-four.\"\nAnnouncement.\nThe newly renamed Nintendo 64 console was unveiled to the public in playable form on November 24 at Nintendo's Shoshinkai 1995 trade show. Eager for a preview, \"hordes of Japanese schoolkids huddled in the cold outside ... the electricity of anticipation clearly rippling through their ranks\". \"Game Zero\" magazine disseminated photos of the event two days later. Official coverage by Nintendo followed later via the \"Nintendo Power\" website and print magazine.\nThe console was originally slated for release by Christmas of 1995. In May 1995, Nintendo delayed the release to April 21, 1996. Consumers anticipating a Nintendo release the following year at a lower price than the competition reportedly reduced the sales of competing Sega and Sony consoles during the important Christmas shopping season. \"Electronic Gaming Monthly\" editor Ed Semrad even suggested that Nintendo may have announced the April 21, 1996, release date with this end in mind, knowing in advance that the system would not be ready by that date.\nIn its explanation of the delay, Nintendo claimed it needed more time for Nintendo 64 software to mature, and for third-party developers to produce games. Adrian Sfarti, a former engineer for SGI, attributed the delay to hardware problems; he claimed that the chips underperformed in testing and were being redesigned. In 1996, the Nintendo 64's software development kit was completely redesigned as the Windows-based Partner-N64 system, by Kyoto Microcomputer, Co. Ltd. of Japan.\nThe Nintendo 64's release date was later delayed again, to June 23, 1996. Nintendo said the reason for this delay, and in particular, the cancellation of plans to release the console in all markets worldwide simultaneously, was that the company's marketing studies now indicated that they would not be able to manufacture enough units to meet demand by April 21, 1996, potentially angering retailers in the same way Sega had done with its surprise early launch of the Saturn in North America and Europe.\nTo counteract the possibility that gamers would grow impatient with the wait for the Nintendo 64 and purchase one of the several competing consoles already on the market, Nintendo ran ads for the system well in advance of its announced release dates, with slogans like \"Wait for it...\" and \"Is it worth the wait? Only if you want the best!\"\nRelease.\n\"Popular Electronics\" called the launch a \"much hyped, long-anticipated moment\". Several months before the launch, \"GamePro\" reported that many gamers, including a large percentage of their own editorial staff, were already saying they favored the Nintendo 64 over the Saturn and PlayStation.\nThe console was first released in Japan on June 23, 1996. Though the initial shipment of 300,000 units sold out on the first day, Nintendo successfully avoided a repeat of the Super Famicom launch day pandemonium, in part by using a wider retail network which included convenience stores. The remaining 200,000 units of the first production run shipped on June 26 and 30, with almost all of them reserved ahead of time. In the months between the Japanese and North American launches, the Nintendo 64 saw brisk sales on the American gray market, with import stores charging as much as $699 plus shipping for the system. The Nintendo 64 was first sold in North America on September 26, 1996, though having been advertised for the 29th. It was launched with just two games in the United States, \"Pilotwings 64\" and \"Super Mario 64\"; \"Cruis'n USA\" was pulled from the line-up less than a month before launch because it did not meet Nintendo's quality standards. In 1994, prior to the launch, Nintendo of America chairman Howard Lincoln emphasized the quality of first-party games, saying \"... we're convinced that a few great games at launch are more important than great games mixed in with a lot of dogs\". Its American launch was wildly successful, breaking records - its first day sales were significantly higher than PlayStation's and Saturn's respective launches the year before.\nThe PAL version of the console was released in Europe on March 1, 1997, except for France where it was released on September 1 of the same year. According to Nintendo of America representatives, Nintendo had been planning a simultaneous launch in Japan, North America, and Europe, but market studies indicated that worldwide demand for the system far exceeded the number of units they could have ready by launch, potentially leading to consumer and retailer frustration.\nOriginally intended to be priced at US$, the console was ultimately launched at US$ to make it competitive with Sony and Sega offerings, as both the Saturn and PlayStation had been lowered to $199.99 earlier that summer. Nintendo priced the console as an impulse purchase, a strategy from the toy industry. The price of the console in the United States was further reduced in August 1998.\nPromotion.\nThe Nintendo 64's North American launch was backed with a $54\u00a0million marketing campaign by Leo Burnett Worldwide (meaning over $100 in marketing per North American unit that had been manufactured up to this point). While the competing Saturn and PlayStation both set teenagers and adults as their target audience, the Nintendo 64's target audience was pre-teens.\nTo boost sales during the slow post-Christmas season, Nintendo and General Mills worked together on a promotional campaign that appeared in early 1999. The advertisement by Saatchi &amp; Saatchi, New York began on January 25 and encouraged children to buy Fruit by the Foot snacks for tips to help them with their Nintendo 64 games. Ninety different tips were available, with three variations of thirty tips each.\nNintendo advertised its Funtastic Series of peripherals with a $10\u00a0million print and television campaign from February 28 to April 30, 2000. Leo Burnett Worldwide was in charge again.\nHardware.\nTechnical specifications.\nThe Nintendo 64's architecture is built around the Reality Coprocessor (RCP), which serves as the system\u2019s central hub for processing graphics, audio, and memory management. It works in tandem with the VR4300, a 93.75\u00a0MHz 64-bit CPU fabricated by NEC with a performance of 125 million instructions per second. \"Popular Electronics\" compared its processing power to that of contemporary Pentium desktop processors. Though constrained by a narrower 32-bit system bus, the VR4300 retained the computational capabilities of the more powerful 64-bit MIPS R4300i on which it was based. However, software rarely utilized 64-bit precision, as Nintendo 64 games primarily relied on faster and more compact 32-bit operations.\nThe RCP operates at 62.5\u00a0MHz and contains two critical components: the \"Signal Processor\", responsible for sound and graphics processing, and the \"Display Processor\", which manages pixel drawing. The RCP renders visual data into the graphics frame buffer and controls direct memory access (DMA), transferring video and audio data from memory to a digital-to-analog converter (DAC) for final output.\nA key advantage of the Nintendo 64's architecture is that the CPU and RCP operate in parallel, dividing tasks for better efficiency. While the VR4300 executes the main game logic, the RCP processes graphics and sound independently. This design enables 3D rendering and complex audio effects but also requires careful coordination to avoid performance bottlenecks.\nThe Nintendo 64 was among the first consoles to implement a unified memory architecture, eliminating separate banks of random-access memory (RAM) for CPU, audio, and video operations. It features 4\u00a0MB of RDRAM (Rambus DRAM), expandable to 8\u00a0MB with the Expansion Pak. At the time, RDRAM was a relatively new technology that provided high bandwidth at a lower cost.\nAudio processing is handled by both the CPU and the RCP and is output through a DAC with a sample rate of up to 44.1 kHz with 16-bit depth, matching CD quality. However, this level of fidelity was rarely used due to the high CPU demand and the storage limitations of the ROM cartridges. Most games featured stereo sound, with some supporting Dolby Pro Logic surround sound.\nFor video output, the system supports composite and S-Video output, using the same cables as the Super NES and GameCube. It can display up to 16.8 million colors and resolutions ranging from 256\u00d7224 to 640\u00d7480 pixels. While most games run at 320\u00d7240, some support higher resolutions, often requiring the Expansion Pak. The console also accommodates widescreen formats, with games offering either anamorphic 16:9 or letterboxed display modes.\nController.\nThe Nintendo 64 controller features a distinctive \"M\"-shaped design, with a \"control stick\", making Nintendo the first manufacturer to include a thumbstick as a standard feature in its primary controller. While functionally similar to an analog stick, the control stick is digital, operating on the same principles as a ball mouse.\nThe controller includes a D-pad and ten buttons: a large A and B button, a Start button, four C-buttons (Up, Down, Left, and Right), two shoulder buttons (L and R), and a Z trigger positioned on the back. \"Popular Electronics\" described its shape as \"evocative of some alien spaceship.\" While noting that the three-handle design could be confusing, the magazine praised its versatility, stating \"the separate grips allow different hand positions for various game types\".\nA port on the bottom of the controller allows users to connect various accessories, including the Controller Pak for saving game data, the Rumble Pak for force feedback, and the Transfer Pak, which enabled data transfer between supported Nintendo 64 and Game Boy games.\nThe Nintendo 64 was also one of the first consoles to feature four controller ports. According to Shigeru Miyamoto, Nintendo included four ports because it was the first console powerful enough to handle four-player split-screen gameplay without significant slowdown.\nGame Paks.\nAfter multiple attempts to develop a compact disc-based add-on for the Super NES, many in the industry expected Nintendo\u2019s next console to follow Sony\u2019s PlayStation in adopting the CD format. However, when the first Nintendo 64 prototypes debuted in November 1995, observers were surprised to find that the system once again used ROM cartridges.\nNintendo 64 cartridges range in size from 4 to 64\u00a0MB and often include built-in save functionality.\nNintendo\u2019s selection of the cartridge medium was highly controversial and is frequently cited as a key factor in the company losing its dominant position in the gaming market. While cartridges offered advantages such as faster load times and durability, their limitations\u2014higher production costs, lower storage capacity, and longer manufacturing lead times\u2014posed challenges for developers. Many of the format\u2019s benefits required innovative solutions, which only emerged later in the console\u2019s lifecycle.\nAdvantages.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nThe big strength was the N64 cartridge. We use the cartridge almost like normal RAM and are streaming all level data, textures, animations, music, sound and even program code while the game is running. With the final size of the levels and the amount of textures, the RAM of the N64 never would have been even remotely enough to fit any individual level. So the cartridge technology really saved the day.\nFactor 5, \"Bringing Indy to N64\" at IGN\nNintendo cited several reasons for choosing cartridges. The biggest advantage was their fast load times\u2014unlike CDs, which required lengthy loading screens, cartridges provided near-instant gameplay. This advantage had previously helped Nintendo compete against home computers like the Commodore 64 in the 1980s. Although cartridges are susceptible to long-term environmental damage, they are significantly more durable than compact discs.\nAnother key factor was copyright protection\u2014cartridges were harder to pirate than CDs, reducing widespread software piracy. While unauthorized N64-to-PC devices eventually emerged, they were far less common than the more easily copied PlayStation CDs.\nDisadvantages.\nCartridges also had notable drawbacks. They took longer to manufacture than CDs, requiring at least two weeks per production run. This forced publishers to predict demand ahead of time, risking either overproduction of costly cartridges or weeks-long shortages if demand was underestimated. Additionally, cartridges were significantly more expensive to produce than CDs, leading to higher game prices, typically US$ more than PlayStation titles.\nThird-party developers also complained that they were at an unfair disadvantage. Since Nintendo controlled cartridge manufacturing, it could sell its own first-party games at a lower price, and prioritize their production over those of other companies.\nStorage limitations were another key issue. While Nintendo 64 cartridges maxed out at 64\u00a0MB, CDs could hold 650\u00a0MB. As games became more complex, this restriction forced compromises, including compressed textures, shorter music tracks, and fewer cutscenes. Full-motion video was rarely feasible, and many multiplatform games had to be scaled down for the N64.\nThese cost and storage constraints pushed many third-party developers toward the PlayStation. Square and Enix, which had originally planned to release \"Final Fantasy VII\" and \"Dragon Warrior VII\" on the Nintendo 64, switched to Sony\u2019s console due to storage constraints. Other developers, like Konami, released far fewer N64 titles than PlayStation games. As a result, new N64 releases were less frequent compared to its competitors.\nDespite these challenges, the Nintendo 64 remained competitive, bolstered by strong first-party titles and exclusive hits like \"GoldenEye 007\". Nintendo\u2019s flagship franchises, including Mario and Zelda, retained strong brand appeal, and deals with second-party developers like Rare further strengthened the console\u2019s game library.\nProgramming characteristics.\nProgramming for the Nintendo 64 presented unique challenges alongside notable advantages. \"The Economist\" described development for the system as \"horrendously complex\". Like many game consoles and embedded systems, the Nintendo 64 featured highly specialized hardware optimizations, which were further complicated by design oversights, limitations in 3D technology, and manufacturing constraints.\nAs the console neared the end of its lifecycle, Nintendo\u2019s hardware chief, Genyo Takeda, repeatedly reflected on these difficulties, using the Japanese term , meaning \"reflective regret.\" Looking back, he admitted, \"When we made Nintendo 64, we thought it was logical that if you want to make advanced games, it becomes technically more difficult. We were wrong. We now understand it's the cruising speed that matters, not the momentary flash of peak power.\"\nRegional lockout.\nUnlike the NES and Super NES, which employed region-specific branding and hardware variations, the Nintendo 64 maintained a consistent design and brand worldwide. While Nintendo initially announced the use of regional lockout chips to restrict game compatibility, the platform ultimately enforced region-locking through physical cartridge design, with each market having cartridges with different notches on the back, preventing a cartridge from one region from being inserted into a foreign console.\nColor variants.\nThe Nintendo 64 comes in several colors. The standard Nintendo 64 is charcoal gray, nearly black, and the controller is solid gray (later releases in the U.S., Canada, and Australia included a bonus second controller in Atomic Purple). The console was released in various colors and special editions.\nMost Nintendo 64 game cartridges are gray in color, but some games have a colored cartridge. Fourteen games have black cartridges, and other colors (such as yellow, blue, red, gold, and green) were each used for six or fewer games. Several games, such as \"\", were released both in standard gray and in colored, limited edition versions.\nGames.\nA total of 388 Nintendo 64 games were officially released, with just 85 exclusively sold in Japan. For comparison, the PlayStation received 4,105 games, the Saturn got over 1,000, the SNES got 1,755 games, and the NES got 716 Western releases plus over 1,000 in Japan. The considerably smaller Nintendo 64 game library has been attributed by some to the controversial decision not to adopt the CD-ROM, and programming difficulties for its complex architecture. This trend is also seen as a result of Hiroshi Yamauchi's strategy, announced during his speech at the Nintendo 64's November 1995 unveiling, that Nintendo would be restricting the number of games produced for the Nintendo 64 so that developers would focus on higher quality instead of quantity. The \"Los Angeles Times\" also observed that this was part of Nintendo's \"penchant for perfection [...] while other platforms offer quite a bit of junk, Nintendo routinely orders game developers back to the boards to fix less-than-perfect titles\".\nAlthough having much less third-party support than rival consoles, Nintendo's strong first-party franchises such as \"Mario\" enjoyed wide brand appeal. Second-parties of Nintendo, such as Rare, released groundbreaking titles. Consequently, the Nintendo 64 game library included a high number of critically acclaimed and widely sold games. According to TRSTS reports, three of the top five best-selling games in the U.S. for December 1996 were Nintendo 64 games (both of the remaining two were Super NES games). \"Super Mario 64\" is the best-selling console game of the generation, with 11\u00a0million units sold beating \"Gran Turismo\" for the PlayStation (at 10.85\u00a0million) and \"Final Fantasy VII\" (at 9.72\u00a0million) in sales. The game also received much praise from critics and helped to pioneer three-dimensional control schemes. \"GoldenEye 007\" was important in the evolution of the first-person shooter, and has been named one of the greatest in the genre. \"\" set the standard for future 3D action-adventure games and is considered by many to be one of the greatest games ever made.\nGraphics.\nThe most graphically demanding Nintendo 64 games on larger 32 or 64\u00a0MB cartridges are among the most advanced and detailed of 32- and 64-bit platforms. To maximize the hardware, developers created custom microcode. Nintendo 64 games running on custom microcode benefit from much higher polygon counts and more advanced lighting, animation, physics, and AI routines than its competition. \"Conker's Bad Fur Day\" is arguably the pinnacle of its generation combining multicolored real-time lighting that illuminates each area to real-time shadowing, and detailed texturing replete with a full in-game facial animation system. The Nintendo 64 is capable of executing many more advanced and complex rendering techniques than its competitors. It is the first home console to feature trilinear filtering, to smooth textures. This contrasts with the Saturn and PlayStation, which use nearest-neighbor interpolation and produce more pixelated textures. Overall however the results of the Nintendo cartridge system were mixed.\nThe smaller storage size of ROM cartridges can limit the number of available textures. As a result, many games with much smaller 8 or 12\u00a0MB cartridges are forced to stretch textures over larger surfaces. Compounded by a limit of 4,096 bytes of on-chip texture memory, the result is often a distorted, out-of-proportion appearance. Many games with larger 32 or 64\u00a0MB cartridges avoid this issue entirely, including \"Resident Evil 2\", \"Sin and Punishment: Successor of the Earth\", and \"Conker's Bad Fur Day\", allowing for more detailed graphics with multiple, multi-layered textures across all surfaces.\nEmulation.\nSeveral Nintendo 64 games have been released for the Wii and Wii U Virtual Console (VC) services and are playable with the Classic Controller, GameCube controller, Wii U Pro Controller, or Wii U GamePad. Differences include a higher resolution and a more consistent framerate than the Nintendo 64 originals. Some features, such as Rumble Pak functionality, are not available in the Wii versions. Some features are also changed on the Virtual Console releases. For example, the VC version of \"Pok\u00e9mon Snap\" allows players to send photos through the Wii's message service, and \"Wave Race 64\"'s in-game content was altered due to the expiration of the Kawasaki license. Several games developed by Rare were released on Microsoft's Xbox Live Arcade service, including \"Banjo-Kazooie\", \"Banjo-Tooie\", and \"Perfect Dark\", following Microsoft's acquisition of Rareware in 2002. One exception is \"Donkey Kong 64\", released in April 2015 on the Wii U Virtual Console, as Nintendo retained the rights to the game. Select Nintendo 64 games have been re-released via the Nintendo Classics service as part of the \"Expansion Pack\" tier of the Nintendo Switch Online service. With the launch of the Nintendo Switch 2 on June 5, 2025, the additional features of the Nintendo 64 - Nintendo Classics will offer CRT filter, rewind function and button remapping (one of these features is also available on the Nintendo Switch).\nSeveral unofficial third-party emulators can play Nintendo 64 games on other platforms, such as Windows, Macintosh, and smartphones.\nAccessories.\n64DD.\nNintendo released a peripheral platform called 64DD, where \"DD\" stands for \"Disk Drive\". Connecting to the expansion slot at the bottom of the system, the 64DD turns the Nintendo 64 console into an Internet appliance, a multimedia workstation, and an expanded gaming platform. This large peripheral allows players to play Nintendo 64 disk-based games, capture images from an external video source, and it allowed players to connect to the now-defunct Japanese Randnet online service. Not long after its limited mail-order release, the peripheral was discontinued. Only nine games were released, including the four \"Mario Artist\" games (\"Paint Studio\", \"Talent Studio\", \"Communication Kit\", and \"Polygon Studio\"). Many planned games were eventually released in cartridge format or on other game consoles. The 64DD and the accompanying Randnet online service were released only in Japan.\nTo illustrate the fundamental significance of the 64DD to all game development at Nintendo, lead designer Shigesato Itoi said: \"I came up with a lot of ideas because of the 64DD. All things start with the 64DD. There are so many ideas I wouldn't have been allowed to come up with if we didn't have the 64DD\". Shigeru Miyamoto concluded: \"Almost every new project for the N64 is based on the 64DD. ... we'll make the game on a cartridge first, then add the technology we've cultivated to finish it up as a full-out 64DD game\".\niQue Player.\nThe iQue Player was a handheld TV game Nintendo 64 system that released only in China on November 17, 2003, after China banned video game consoles. The games that were released in the iQue Player's lifetime (from 2003 to 2016) are \"Super Mario 64\", \"\", \"Mario Kart 64\", \"Wave Race 64\", \"Star Fox 64\", \"Yoshi's Story\", \"Paper Mario\", \"Super Smash Bros.\", \"F-Zero X\", \"Dr. Mario 64\", \"Excitebike 64\", \"Sin and Punishment\", \"Custom Robo\" and \"Animal Crossing\".\nReception.\nCritical reception.\nThe Nintendo 64 received acclaim from critics. Reviewers praised the console's advanced 3D graphics and gameplay, while criticizing the lack of games. On G4techTV's \"Filter\", the Nintendo 64 was voted up to No. 1 by registered users.\nIn February 1996, \"Next Generation\" magazine called the Nintendo Ultra 64 the \"best kept secret in videogames\" and the \"world's most powerful game machine\". It called the system's November 24, 1995, unveiling at Shoshinkai \"the most anticipated videogaming event of the 1990s, possibly of all time\". Previewing the Nintendo 64 shortly prior to its launch, \"Time\" magazine praised the realistic movement and gameplay provided by the combination of fast graphics processing, pressure-sensitive controller, and the \"Super Mario 64\" game. The review praised the \"fastest, smoothest game action yet attainable via joystick at the service of equally virtuoso motion\", where \"[f]or once, the movement on the screen feels real\". Asked if consumers should buy a Nintendo 64 at launch, buy it later, or buy a competing system, a panel of six \"GamePro\" editors voted almost unanimously to buy at launch; one editor said consumers who already own a PlayStation and are on a limited budget should buy it later, and all others should buy it at launch.\nAt launch, the \"Los Angeles Times\" called the system \"quite simply, the fastest, most graceful game machine on the market\". Its form factor was described as small, light, and \"built for heavy play by kids\" unlike the \"relatively fragile Sega Saturn\". Showing concern for a major console product launch during a sharp, several-year long, decline in the game console market, the review said that the long-delayed Nintendo 64 was \"worth the wait\" in the company's pursuit of quality. Although the \"Times\" expressed concerns about having only two launch games at retail and twelve expected by Christmas, this was suggested to be part of Nintendo's \"penchant for perfection\", as \"while other platforms offer quite a bit of junk, Nintendo routinely orders game developers back to the boards to fix less-than-perfect titles\". Describing the quality control incentives associated with cartridge-based development, the \"Times\" cited Nintendo's position that cartridge game developers tend to \"place a premium on substance over flash\", and noted that the launch games lack the \"poorly acted live-action sequences or half-baked musical overtures\" which it says tend to be found on CD-ROM games. Praising Nintendo's controversial choice of the cartridge medium with its \"nonexistent\" load times and \"continuous, fast-paced action CD-ROMs simply cannot deliver\", the review concluded that \"the cartridge-based Nintendo 64 delivers blistering speed and tack-sharp graphics that are unheard of on personal computers and make competing 32-bit, disc-based consoles from Sega and Sony seem downright sluggish\".\n\"Time\" named it the 1996 Machine of the Year, saying the machine had \"done to video-gaming what the 707 did to air travel\". The magazine said the console achieved \"the most realistic and compelling three-dimensional experience ever presented by a computer\". \"Time\" credited the Nintendo 64 with revitalizing the video game market, \"rescuing this industry from the dustbin of entertainment history\". The magazine suggested that the Nintendo 64 would play a major role in introducing children to digital technology in the final years of the 20th century. The article concluded by saying the console had already provided \"the first glimpse of a future where immensely powerful computing will be as common and easy to use as our televisions\". The console also won the 1996 Spotlight Award for Best New Technology.\n\"Popular Electronics\" complimented the system's hardware, calling its specifications \"quite impressive\". It found the controller \"comfortable to hold, and the controls to be accurate and responsive\".\nIn a 1997 year-end review, a team of five \"Electronic Gaming Monthly\" editors gave the Nintendo 64 scores of 8.0, 7.0, 7.5, 7.5, and 9.0. They highly praised the power of the hardware and the quality of the first-party games, especially those developed by Rare's and Nintendo's internal studios, but also commented that the third-party output to date had been mediocre and the first-party output was not enough by itself to provide Nintendo 64 owners with a steady stream of good games or a full breadth of genres. \"Next Generation\"'s end of 1997 review expressed similar concern about third party support, while also noting signs that the third party output was improving, and speculated that the Nintendo 64's arrival late in its generation could lead to an early obsolescence when Sony and Sega's successor consoles launched. However, they said that for some, Nintendo's reliably high-quality software would outweigh those drawbacks, and gave the system 3 1/2 out of 5 stars.\nDeveloper Factor 5, which created some of the system's most technologically advanced games along with the system's audio development tools for Nintendo, said, \"[T]he N64 is really sexy because it combines the performance of an SGI machine with a cartridge. We're big arcade fans, and cartridges are still the best for arcade games or perhaps a really fast CD-ROM. But there's no such thing for consoles yet [as of 1998]\".\nSales.\nThe Nintendo 64 was highly successful in the North America region; conversely, sales proved to be underwhelming in the domestic Japanese and in European markets. Nintendo reported that the system's vintage hardware and software sales had ceased by 2004, three years after the GameCube's launch; as of December 31, 2009, the Nintendo 64 had yielded a lifetime total of 5.54\u00a0million system units sold in Japan, 20.63\u00a0million in the Americas, and 6.75\u00a0million in other regions, for a total of 32.93\u00a0million units.\nNorth America.\nThe Nintendo 64 was in heavy demand upon its release. David Cole, industry analyst, said \"You have people fighting to get it from stores\". \"Time\" called the purchasing interest \"that rare and glorious middle-class Cabbage Patch-doll frenzy\". The magazine said celebrities Matthew Perry, Steven Spielberg, and Chicago Bulls players called Nintendo to ask for special treatment to get their hands on the console. In North America and Europe, the console had only two launch games, with \"Super Mario 64\" as its killer app.\nDuring the system's first three days on the market, retailers sold 350,000 of 500,000 available console units. During its first four months, the console yielded 500,000 unit sales in North America. Nintendo successfully outsold Sony and Sega early in 1997 in the United States; by the end of its first full year, the console had sold 3.6\u00a0million units in the United States. \"BusinessWire\" reported that the Nintendo 64 was responsible for Nintendo's sales having increased by 156% by 1997. Five different Nintendo 64 games exceeded 1 million in sales during 1997.\nAfter a strong launch year, the decision to use the cartridge format is said to have contributed to the diminished release pace and higher price of games compared to the competition, and thus Nintendo was unable to maintain its lead in the United States. The console would continue to outsell the Sega Saturn throughout the generation, but would trail behind the PlayStation.\nNintendo's efforts to attain dominance in the key 1997 holiday shopping season were also hurt by game delays. Five high-profile Nintendo games slated for release by Christmas 1997 (\"\", \"Banjo-Kazooie\", \"Conker's Quest\", \"Yoshi's Story\", and \"Major League Baseball Featuring Ken Griffey Jr.\") were delayed until 1998, and \"Diddy Kong Racing\" was announced at the last minute in an effort to somewhat fill the gaps. In an effort to take the edge off of the console's software pricing disadvantage, Nintendo worked to lower manufacturing costs for Nintendo 64 cartridges, and leading into the 1997 holiday shopping season announced a new pricing structure which amounted to a roughly 15% price cut on both first-party and third-party games. Response from third-party publishers was positive, with key third-party publisher Capcom saying the move led them to reconsider their decision not to publish games for the console.\nJapan.\nIn Japan, the console was not as successful, failing to outsell the PlayStation and the Sega Saturn. Benimaru It\u014d, a developer for \"Mother 3\" and friend of Shigeru Miyamoto, speculated in 1997 that the Nintendo 64's lower popularity in Japan was due to the lack of role-playing video games. Nintendo CEO Hiroshi Yamauchi also said the console's lower popularity in Japan was most likely due to lack of role-playing games, and the small number of games being released in general. The higher price of cartridges as opposed to CD-ROM has also been cited as a reason for the system's lackluster third-party support, which led to domestically big titles, such as \"Dragon Quest VII\", moving away from Nintendo's platforms to its rivals.\nShigeru Miyamoto commented at the time that the Nintendo 64's situation in Japan was grim and that it was also tough in Europe, but that these were overcome by its success in America and therefore \"the business has become completely viable\".\nLegacy.\nThe Nintendo 64 is one of the most recognized video game systems in history, Designed in tandem with the controller, \"Super Mario 64\" and \"\" are widely considered by critics and the public to be two of the greatest and most influential games of all time. \"GoldenEye 007\" is one of the most influential games for the shooter genre.\nThe Aleck 64 is a Nintendo 64 design in arcade form, designed by Seta in cooperation with Nintendo, and sold from 1998 to 2003 only in Japan.\nIn 2011, \"IGN\" ranked it as the ninth-greatest video game console of all time.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21850", "revid": "44838462", "url": "https://en.wikipedia.org/wiki?curid=21850", "title": "GNU nano", "text": "Text editor for Unix-like computing systems\nGNU nano is a text editor for Unix-like computing systems or operating environments using a command line interface. It emulates the Pico text editor, part of the Pine email client, and also provides additional functionality. Unlike Pico, nano is licensed under the GNU General Public License (GPL). Released as free software by Chris Allegretta in 1999, nano became part of the GNU Project in 2001. The logo resembles the lowercase form of the Greek letter Eta (\u03b7).\nHistory.\nGNU nano was first created in 1999 with the name \"TIP\" (a recursive acronym for \"TIP Isn't Pico\"), by Chris Allegretta. His motivation was to create a free software replacement for Pico, which was not distributed under a free-software license. The name was changed to nano on January 10, 2000, to avoid a naming conflict with the existing Unix utility \"tip\". The name comes from the system of SI prefixes, in which nano is 1000 times larger than pico. In February 2001, nano became a part of the GNU Project.\nGNU nano implements several features that Pico lacks, including syntax highlighting, line numbers, regular expression search and replace, line-by-line scrolling, multiple buffers, indenting groups of lines, rebindable key support, and the undoing and redoing of edit changes.\nOn 11 August 2003, Chris Allegretta officially handed the source code maintenance of nano to David Lawrence Ramsey. On 20 December 2007, with the release of 2.0.7, Ramsey stepped down as nano's maintainer. The license was also upgraded to GPL-3.0-or-later. The project is currently maintained by Benno Schulenberg.\nIn version 2.6.0, released in June 2016, the principal developer and other active members of the nano project decided by consensus to leave the GNU Project. Their decision was prompted by objections to the Free Software Foundation's policies, including its requirement for copyright assignment, as well as the belief that decentralized copyright ownership does not impede effective enforcement of the GNU General Public License. The step was acknowledged by Debian and Arch Linux, while the GNU Project resisted the move and called it a \"fork\". On 19 August 2016, Chris Allegretta announced the return of the project to the GNU family, following concessions from GNU on copyright assignment for Nano specifically, which happened when version 2.7.0 was released in September 2016.\nControl keys.\nGNU nano, like Pico, is keyboard-oriented, controlled with control keys. For example, saves the current file; goes to the search menu. GNU nano puts a two-line \"shortcut bar\" at the bottom of the screen, listing many of the commands available in the current context. For a complete list, gets the help screen.\nUnlike Pico, nano uses meta keys to toggle its behavior. For example, toggles smooth scrolling mode on and off. Almost all features that can be selected from the command line can be dynamically toggled. On keyboards without the meta key it is often mapped to the escape key, , such that in order to simulate, say, one has to press the key, then release it, and then press the key.\nGNU nano can also use pointing devices, such as a mouse, to activate functions that are on the shortcut bar, as well as position the cursor.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21851", "revid": "47587808", "url": "https://en.wikipedia.org/wiki?curid=21851", "title": "Nieuwe Waterweg", "text": "Ship canal in the Netherlands\nThe Nieuwe Waterweg (\"New Waterway\") is a ship canal in the Netherlands from het Scheur (a branch of the Rhine-Meuse-Scheldt delta) west of the town of Maassluis to the North Sea at Hook of Holland: the Maasmond, where the Nieuwe Waterweg connects to the Maasgeul. It is the artificial mouth of the river Rhine.\nThe Nieuwe Waterweg, which opened in 1872 and has a length of approximately , was constructed to keep the city and port of Rotterdam accessible to seafaring vessels as the natural Meuse-Rhine branches silted up. The Waterway is a busy shipping route since it is the primary access to one of the busiest ports in the world, the Port of Rotterdam. At the entrance to the sea, a flood protection system called Maeslantkering has been installed (completed in 1997). There are no bridges or tunnels across the Nieuwe Waterweg.\nHistory.\nBy the middle of the 19th century, Rotterdam was already one of the largest port cities in the world, mainly because of transshipment of goods from Germany to Great Britain. The increase in shipping traffic created a capacity problem: there were too many branches in the river delta, making the port difficult to reach.\nIn 1863, a law was passed that allowed for the provision of a new canal for large ocean-going ships from Rotterdam to the North Sea. Hydraulic engineer Pieter Caland was commissioned to design a canal cutting through the \"Hook of Holland\u201d and to extend the Mouth of Rhine to the sea. The designs for this were already done back in 1731 by Nicolaas Samuelsz Cruquius but the implementation could no longer be postponed to prevent the decline of the harbour of Rotterdam.\nConstruction began on 31 October 1863. The first phase consisted of the expropriation of farm lands from Rozenburg to Hook of Holland.\nDuring the second phase two dikes were built parallel to each other, which took 2 years. Caland proposed to extend the dikes 2\u00a0km into the sea to disrupt the coastal sea currents and decrease silt deposits in the shipping lane.\nUpon the completion of the dikes, the third phase began by the digging of the actual waterway. This began on 31 October 1866 and was completed three years later. The large amounts of removed soil were in turn used to reinforce other dams and dikes.\nThe last phase consisted of the removal of the dam separating the new waterway from the sea and river. In 1872, the Nieuwe Waterweg was completed and Rotterdam was easily accessible.\nBecause of the currents and erosion, the shipping lane has been widened somewhat. Yet because of the draft of today's supertankers, it needs to be dredged constantly.\nIn 1997, the last part of the Delta Works, the Maeslantkering, was put in operation near the mouth of the Nieuwe Waterweg. This storm surge barrier protects Rotterdam against north westerly Beaufort Force 10 to 12 storms.\nCurrent situation.\nThe Nieuwe Waterweg gives the Port of Rotterdam its deep-water access to the North Sea. From Hook of Holland it stretches for approximately where the waterway continues as the Nieuwe Maas. The very first Nieuwe Waterweg\u2014a breach through the dunes at Hook of Holland\u2014was only long, but in around 1877 the channel was made much larger and wider and the current Nieuwe Waterweg was created. Currently the width of the channel is between and it is dredged to a depth of below Amsterdam Ordnance Datum.\nIt is this channel, together with the dredged channels in the North Sea, Maasgeul and Eurogeul, that allows ships like the MS \"Berge Stahl\" and MV \"Vale Rio de Janeiro\" (both with a draught of 23 meters) to enter Europoort.\nThe Dutch government agency Rijkswaterstaat is responsible for maintaining the channel.\nMaasmond.\nThe point where the Nieuwe Waterweg enters into the North Sea, between Hook of Holland on the north bank and the Maasvlakte to the south, is called the Maasmond. It is marked with two navigation light-towers called the Paddestoelen (\"mushrooms\"). The Nieuwe Waterweg connects, in the North Sea, to the Maasgeul. This dredged channel in the North Sea is being widened to to facilitate the largest container vessels for the new Maasvlakte 2 that opened in 2013.\nSources and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21852", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=21852", "title": "New moon/details", "text": ""}
{"id": "21853", "revid": "45320532", "url": "https://en.wikipedia.org/wiki?curid=21853", "title": "Neijia", "text": "Group of Chinese martial arts\nNeijia (\u5167\u5bb6) is the collective name for the internal Chinese martial arts. It relates to those martial arts occupied with spiritual, mental or \"qi\"-related aspects, as opposed to an \"external\" approach focused on physiological aspects. The distinction dates to the 17th century, but its modern application is due to publications by Sun Lutang, dating to the period of 1915 to 1928. \"Neijin\" is developed by using \"neigong\" or \"internal changes\", contrasted with \"waigong\" (\u5916\u529f; \"w\u00e0ig\u014dng\") or \"external exercises\" .\n\"Wudangquan\" is a more specific grouping of internal martial arts named for their association in popular Chinese legend with the Taoist monasteries of the Wudang Mountains in Hubei province. These styles were enumerated by Sun Lutang as tai chi, \"xingyiquan\" and \"baguazhang\", but most also include \"bajiquan\" and the legendary Wudang Sword.\nSome other Chinese arts, not in the wudangquan group, such as \"qigong\", \"liuhebafa\", \"Bak Mei Pai\", \"ziranmen\" (Nature Boxing), \"Bok Foo Pai\" and \"yiquan\" are frequently classified (or classify themselves) as \"internal\".\nHistory.\nQing China.\nThe term \"neijia\" and the distinction between internal and external martial arts first appears in Huang Zongxi's 1669 \"Epitaph for Wang Zhengnan\". Stanley Henning proposes that the \"Epitaph\"'s identification of the internal martial arts with the Taoism indigenous to China and of the external martial arts with the foreign Buddhism of Shaolin\u2014and the Manchu Qing Dynasty to which Huang Zongxi was opposed\u2014was an act of political defiance rather than one of technical classification.\nIn 1676 Huang Zongxi's son, Huang Baijia, who learned martial arts from Wang Zhengnan, compiled the earliest extant manual of internal martial arts, the \"Neijia Quanfa\".\nRepublic of China.\nBeginning in 1914, Sun Lutang together with Yang Shaohou, Yang Chengfu and Wu Jianquan taught tai chi to the public at the Beijing Physical Education Research Institute. Sun taught there until 1928, a seminal period in the development of modern Yang, Wu and Sun-style tai chi. Sun Lutang also published martial arts texts starting in 1915.\nIn 1928, Kuomintang generals Li Jinglin, Chang Chih-chiang, and Fung Zuziang organized a national martial arts tournament in China; they did so to screen the best martial artists in order to begin building the Central Guoshu Institute. The generals separated the participants of the tournament into Shaolin and Wudang. Wudang participants were recognized as having \"internal\" skills. These participants were generally practitioners of tai chi, \"xingyiquan\" and \"baguazhang\". All other participants competed under the classification of Shaolin. One of the winners in the \"internal\" category was the \"baguazhang\" master Fu Zhensong.\nSun Lutang.\nSun Lutang identified the following as the criteria that distinguish an internal martial art:\nSun Lutang's eponymous style of tai chi fuses principles from all three arts he named as \"neijia\". Similarities applying classical principles between tai chi, \"xingyi\", and \"baquazhang\" include: Loosening (song) the soft tissue, opening shoulder and hip gates or \"gua\", cultivating \"qi\" or intrinsic energy, issuing various \"jin\" or compounded energies. Tai chi is characterized by an ever-present \"peng jin\" or expanding energy. \"Xingyiquan\" is characterized by its solely forward moving pressing \"ji jin\" energy. \"Baguazhang\" is characterized by its \"dragon body\" circular movements. Some Chinese martial arts other than the ones Sun named also teach what are termed internal practices, despite being generally classified as external (e.g. Wing Chun that also is internal ). Some non-Chinese martial arts also claim to be internal, for example Aikido and Kito Ryu. A number of martial artists, especially outside of China, disregard the distinction entirely. Some \"neijia\" schools refer to their arts as \"soft style\" martial arts.\nTraining.\nInternal styles focus on awareness of the spirit, mind, \"qi\" and the use of relaxed ( ) leverage rather than muscular tension. Pushing hands is a training method commonly used in \"neijia\" arts to develop sensitivity and softness.\nMuch time may nevertheless be spent on basic physical training, such as stance training (\"zhan zhuang\"), stretching and strengthening of muscles, as well as on empty hand and weapon forms which can be quite demanding.\nSome forms in internal styles are performed slowly, although some include sudden outbursts of explosive movements (\"fa jin\"), such as those the Chen style of tai chi is famous for teaching earlier than some other styles (e.g. Yang and Wu). The reason for the generally slow pace is to improve coordination and balance by increasing the work load, and to require the student to pay minute attention to their whole body and its weight as they perform a technique. At an advanced level, and in actual fighting, internal styles are performed quickly, but the goal is to learn to involve the entire body in every motion, to stay relaxed, with deep, controlled breathing, and to coordinate the motions of the body and the breathing accurately according to the dictates of the forms while maintaining perfect balance.\nCharacteristics.\nExternal styles are characterized by fast and explosive movements and a focus on physical strength and agility. External styles include both the traditional styles focusing on application and fighting, as well as the modern styles adapted for competition and exercise. Examples of external styles are Shaolin kung fu, with its direct explosive attacks and multiple \"wushu\" forms that have spectacular aerial techniques. External styles begin with a training focus on muscular power, speed and application, and generally integrate their \"qigong\" aspects in advanced training, after their desired \"hard\" physical level has been reached.\nCurrently, some people believe that there is no difference between \"internal\" and \"external\" systems of the Chinese martial arts, while other well known teachers have expressed differing opinions. For example, the tai chi teacher Wu Jianquan:\nThose who practice [Shaolin kung fu] leap about with strength and force; people not proficient at this kind of training soon lose their breath and are exhausted. Tai chi is unlike this. Strive for quiescence of body, mind and intention.\nCurrent practice.\nMany internal schools teach forms that are practised for health benefits only. Thus, tai chi in spite of its roots in martial arts has become similar in scope to \"qigong\", the purely meditative practice based on notions of circulation of \"qi\". As a health practice, tai chi classes have become popular in hospitals, clinics, community and senior centers as the art's reputation as a low-stress exercise for seniors became better known.\nTraditionalists feel that a school not teaching martial aspects somewhere in their syllabus cannot be said to be actually teaching the art itself, that they have accredited themselves prematurely. Traditional teachers also believe that understanding the core theoretical principles of \"neijia\" and the ability to apply them are a necessary gateway to health benefits.\nFiction.\nInternal styles have been associated in legend and in much popular fiction with the Taoist monasteries of the Wudang Mountains in central China.\n\"Neijia\" are a common theme in Chinese \"wuxia\" novels and films, and are usually represented as originating in Wudang or similar mythologies. Often, genuine internal practices are highly exaggerated to the point of making them seem miraculous, as in the novels of Jin Yong and Gu Long. Internal concepts have also been a source of comedy, such as in the films \"Shaolin Soccer\" and \"Kung Fu Hustle\".\nIn the \"Naruto\" series, Neji Hy\u016bga's name and techniques were based on \"neijia\".\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21854", "revid": "50719386", "url": "https://en.wikipedia.org/wiki?curid=21854", "title": "Navigation", "text": "Process of monitoring and controlling the movement of a craft or vehicle\nNavigation is a field of study that focuses on the process of monitoring and controlling the movement of a craft or vehicle from one place to another. The field of navigation includes four general categories: land navigation, marine navigation, aeronautic navigation, and space navigation. It is also the term of art used for the specialized knowledge used by navigators to perform navigation tasks. All navigational techniques involve locating the navigator's position compared to known locations or patterns. Navigation, in a broader sense, can refer to any skill or study that involves the determination of position and direction. In this sense, navigation includes orienteering and pedestrian navigation.\nFor marine navigation, this involves the safe movement of ships, boats and other nautical craft either on or underneath the water using positions from navigation equipment with appropriate nautical charts (electronic and paper). Navigation equipment for ships is mandated under the requirements of the SOLAS Convention, depending on ship size. For land navigation, this involves the movement of persons, animals and vehicles from one place to another by means of navigation equipment (such as a compass or GNSS receivers), maps and visual navigation marks across urban or rural environments. Aeronautic (air) navigation involves piloting an aircraft from one geographic position to another position while monitoring the position as the flight progresses.\nEtymology.\nThe term stems from the 1530s, from Latin \"navigationem\" (nom. \"navigatio\"), from \"navigatus\", pp. of \"navigare\" \"to sail, sail over, go by sea, steer a ship,\" from \"navis\" \"ship\" and the root of \"agere\" \"to drive\".\nHistory.\n \nPolynesian navigation is probably the earliest form of open-ocean navigation; it was based on memory and observation recorded on scientific instruments like the Marshall Islands Stick Charts of Ocean Swells. Early Pacific Polynesians used the motion of stars, weather, the position of certain wildlife species, or the size of waves to find the path from one island to another. Among the first proper navigational instruments was the compass, with one of the oldest Chinese in origin from the Han dynasty (since c. 206 BC). The compass was later adopted for sea navigation by the Song dynasty Chinese during the 11th century. The first usage of a compass recorded in Western Europe and the Islamic world occurred around 1190.\nMaritime navigation using scientific instruments such as the mariner's astrolabe first occurred in the Mediterranean during the Middle Ages. Although land astrolabes were invented in the Hellenistic period and existed in classical antiquity and the Islamic Golden Age, the oldest record of a sea astrolabe is that of Spanish astronomer Ramon Llull dating from 1295. The perfecting of this navigation instrument is attributed to Portuguese navigators during early Portuguese discoveries in the Age of Discovery. The earliest known description of how to make and use a sea astrolabe comes from Spanish cosmographer Mart\u00edn Cort\u00e9s de Albacar's \"Arte de Navegar\" (\"The Art of Navigation\") published in 1551, based on the principle of the archipendulum used in constructing the Egyptian pyramids. However, the first altitude measuring instrument to navigate extensively used at sea was the quadrant. This was reintroduced by Leonardo of Pisa in the 13th century. Its first recorded use was in 1461 by Diogo Gomes. As well as astrolabes and quadrants, the first cross-staff used in navigation was known from the 14th century onwards, believed to have come from early Arab navigators. However, it had many errors and was also difficult to use as it required squinting at the sun. These disadvantages were overcome with the invention of the backstaff in 1595 by John Davis.\nWidespread open-seas navigation using the astrolabe, quadrant, backstaff and the compass started during the Age of Discovery in the 15th century. The Portuguese began systematically exploring the Atlantic coast of Africa from 1418, under the sponsorship of Prince Henry. In 1488 Bartolomeu Dias reached the Indian Ocean by this route. In 1492 the Spanish monarchs funded Christopher Columbus's expedition to sail west to reach the Indies by crossing the Atlantic, which resulted in the Discovery of the Americas. In 1498, a Portuguese expedition commanded by Vasco da Gama reached India by sailing around Africa, opening up direct trade with Asia. Soon, the Portuguese sailed further eastward, to the Spice Islands in 1512, landing in China one year later. The first circumnavigation of the earth was completed in 1522 with the Magellan-Elcano expedition, a Spanish voyage of discovery led by Portuguese explorer Ferdinand Magellan and completed by Spanish navigator Juan Sebasti\u00e1n Elcano after the former's death in the Philippines in 1521.\nFor sailing ships, other developments took place with charting and methods to record courses. One of the oldest surviving marine charts is the Carta Pisana, drawn on a sheepskin, dating to 1275. On land, improvements in the production of maps led to improved navigation by armies, traders and other travellers. For sailing ships, navigation by dead reckoning requires frequent recording of course changes and the ship tacks with the wind. To prevent paper charts, which were expensive and in the early days, rare, from being worn out, other methods were used, including the Traverse board and traverse tables (the oldest traverse tables, dates back to 1428). Quadrants were further developed by inventors such as Robert Hooke, Isaac Newton and John Hadley leading to the invention of the octant. \nDevelopments in mathematics were also important in the history of navigation. These include initially meridional parts, then developments in spherical trigonometry and logarithms enabled navigators from the 1700s onwards to navigate more accurately. On land, mathematical and new instruments led to developments in Surveying and triangulation which further improved maps, as well as the construction of better roads, paths, canals and eventually railways. Development of an accurate marine chronometer under John Harrison and others ensured accurate timekeeping for calculating longitude. Further improvements in ocean navigation led to the first proper sextant in 1757, the parts and usage developed by various inventors including Pierre Vernier and John Campbell. Various methods for calculation with sextant and chronometer evolved over time, beginning with the Duller method (1728) but reached their most accessible with the Douwes method (1821), the Sumner method (1837), modified by Henry Raper (1844) and the Marc St Hilaire or intercept method (1877). Modifications to the magnetic compass and better methods of determining course were also important, include developments in the compass by Matthew Flinders, Lord Kelvin and others.\nThe sextant, together with the chronometer, compass and astronomical calculations became the most widely used methods of maritime navigation until developments in the 20th century with radio-navigation and gyrocompasses. These in turn were superseded with the advent of computers, electronic calculators and later satellite navigation in the 20th century. On land, the development of handheld GPS occurred in the 1980s and with the advent of smartphones, with in-build compassess and satellite receivers, navigation is now widely achieved through technology globally.\nBasic concepts.\nIn terrestrial navigation, the location of a person, ship, plane, etc is defined as a position using a reference point/coordinates (see Cartesian coordinate system). Positions can either be referenced as latitude/longitude or a distance and direction from a fixed reference point (bearing). Lines of position can be derived from a variety of methods and equipment. By determining and monitoring positions it is possible to find and direct a person, ship, plane, etc in a scientific way from one place to another. This often involves the use of maps or charts from which if desired, courses can be calculated or followed depending on the projection or methods used (Rhumb line, Great circle, etc).\nLatitude.\nRoughly, the latitude of a place on Earth is its angular distance north or south of the equator. Latitude is usually expressed in degrees (marked with \u00b0) ranging from 0\u00b0 at the Equator to 90\u00b0 at the North and South poles. The latitude of the North Pole is 90\u00b0 N, and the latitude of the South Pole is 90\u00b0 S. Mariners calculated latitude in the Northern Hemisphere by sighting the pole star (Polaris) with a sextant and using sight reduction tables to correct for height of eye and atmospheric refraction. The height of Polaris in degrees above the horizon is the latitude of the observer, within a degree or so.\nLongitude.\nSimilar to latitude, the longitude of a place on Earth is the angular distance east or west of the prime meridian or Greenwich meridian. Longitude is usually expressed in degrees (marked with \u00b0) ranging from 0\u00b0 at the Greenwich meridian to 180\u00b0 east and west. Sydney, for example, has a longitude of about 151\u00b0 east. New York City has a longitude of 74\u00b0 west. For most of history, mariners struggled to determine longitude. Longitude can be calculated if the precise time of a sighting is known. Lacking that, one can use a sextant to take a lunar distance (also called \"the lunar observation\", or \"lunar\" for short) that, with a nautical almanac, can be used to calculate the time at zero longitude (see Greenwich Mean Time). Reliable marine chronometers were unavailable until the late 18th century and not affordable until the 19th century. For about a hundred years, from about 1767 until about 1850, mariners lacking a chronometer used the method of lunar distances to determine Greenwich time to find their longitude. A mariner with a chronometer could check its reading using a lunar determination of Greenwich time.\nLoxodrome.\nIn navigation, a rhumb line (or loxodrome) is a line crossing all meridians of longitude at the same angle, i.e. a path derived from a defined initial bearing. That is, upon taking an initial bearing, one proceeds along the same bearing, without changing the direction as measured relative to true or magnetic north.\nMethods of navigation.\nMost modern navigation relies primarily on positions determined electronically by receivers collecting information from satellites. Most other modern techniques rely on finding intersecting lines of position or LOP.\nA line of position can refer to two different things, either a line on a chart or a line between the observer and an object in real life. A bearing is a measure of the direction to an object. If the navigator measures the direction in real life, the angle can then be drawn on a nautical chart and the navigator will be somewhere on that bearing line on the chart.\nIn addition to bearings, navigators also often measure distances to objects. On the chart, a distance produces a circle or arc of position. Circles, arcs, and hyperbolae of positions are often referred to as lines of position.\nIf the navigator draws two lines of position, and they intersect he must be at that position. A fix is the intersection of two or more LOPs.\nIf only one line of position is available, this may be evaluated against the dead reckoning position to establish an estimated position.\nLines (or circles) of position can be derived from a variety of sources:\nThere are some methods seldom used today such as the maritime method of \"dipping a light\" to calculate the geographic range from observer to lighthouse, where the height of the lighthouse is known (from a list of lights or from a chart).\nMethods of navigation have changed through history. Each new method has enhanced the mariner's ability to complete his voyage. One of the most important judgments the navigator must make is the best method to use. Some types of navigation are depicted in the table.\nThe practice of navigation usually involves a combination of these different methods.\nMental navigation checks.\nBy mental navigation checks, a pilot or a navigator estimates tracks, distances, and altitudes which will then help the pilot avoid gross navigation errors.\nPiloting.\nPiloting (also called pilotage) involves navigating an aircraft by visual reference to landmarks, or a water vessel in restricted waters and fixing its position as precisely as possible at frequent intervals. More so than in other phases of navigation, proper preparation and attention to detail are important. Procedures vary from vessel to vessel, and between military, commercial, and private vessels. As pilotage takes place in shallow waters, it typically involves following courses to ensure sufficient under keel clearance, ensuring a sufficient depth of water below the hull as well as a consideration for squat. It may also involve navigating a ship within a river, canal or channel in close proximity to land.\nA military navigation team will nearly always consist of several people. A military navigator might have bearing takers stationed at the gyro repeaters on the bridge wings for taking simultaneous bearings, while the civilian navigator on a merchant ship or leisure craft must often take and plot their position themselves, typically with the aid of electronic position fixing. While the military navigator will have a bearing book and someone to record entries for each fix, the civilian navigator will simply pilot the bearings on the chart as they are taken and not record them at all. If the ship is equipped with an ECDIS, it is reasonable for the navigator to simply monitor the progress of the ship along the chosen track, visually ensuring that the ship is proceeding as desired, checking the compass, sounder and other indicators only occasionally. If a pilot is aboard, as is often the case in the most restricted of waters, his judgement can generally be relied upon, further easing the workload. But should the ECDIS fail, the navigator will have to rely on his skill in the manual and time-tested procedures.\nCelestial navigation.\nCelestial navigation systems are based on observation of the positions of the Sun, Moon, planets and navigational stars using a sextant or similar navigation instrument. By knowing which point on the rotating Earth a celestial object is above and measuring its height above the observer's horizon, the navigator can determine his distance from that subpoint using mathematical calculation. A nautical almanac and a source of time, typically a marine chronometer are used to compute the subpoint on Earth a celestial body is over, and a sextant is used to measure the body's angular height above the horizon. That height can then be used to compute distance from the subpoint to create a circular line of position. Alternatively sight reduction tables can be used. A navigator shoots a number of stars in succession to give a series of overlapping lines of position. Where they intersect is the celestial fix. The Moon and Sun may also be used. The Sun can also be used by itself to shoot a succession of lines of position (best done around local noon) to determine a position. Since the advent of GNSS, celestial navigation is less used for marine and air navigation, though it remains useful as a backup or as another method to cross-check the accuracy of electronic systems, particularly in the open ocean.\nMarine chronometer.\nIn order to accurately measure longitude, the precise time is required of a sextant sighting (down to the second, if possible) which is then recorded for subsequent calculation. Each second of error is equivalent to 15 seconds of longitude error, which at the equator is a position error of .25 of a nautical mile, about the accuracy limit of manual celestial navigation. The spring-driven marine chronometer is a precision timepiece used aboard ship to provide accurate time for celestial observations. A chronometer differs from a spring-driven watch principally in that it contains a variable lever device to maintain even pressure on the mainspring, and a special balance designed to compensate for temperature variations. A spring-driven chronometer is set approximately to Greenwich mean time (GMT) and is not reset until the instrument is overhauled and cleaned, usually at three-year intervals. The difference between GMT and chronometer time is carefully determined and applied as a correction to all chronometer readings. Spring-driven chronometers must be wound at about the same time each day.\nQuartz crystal marine chronometers have replaced spring-driven chronometers onboard modern ships because of their greater accuracy. They are maintained on GMT directly from radio time signals. This eliminates chronometer error and watch error corrections. Should the second hand be in error by a readable amount, it can be reset electrically. The basic element for time generation is a quartz crystal oscillator. The quartz crystal is temperature compensated and is hermetically sealed in an evacuated envelope. A calibrated adjustment capability is provided to adjust for the aging of the crystal.\nThe chronometer is typically designed to operate for a minimum of one year on a single set of batteries. Observations may be timed and ship's clocks set with a comparing watch, which is set to chronometer time and taken to the bridge wing for recording sight times. In practice, a wrist watch coordinated to the nearest second with the chronometer will be adequate. A stop watch, either spring wound or digital, may also be used for celestial observations. In this case, the watch is started at a known GMT by chronometer, and the elapsed time of each sight added to this to obtain GMT of the sight. All chronometers and watches should be checked regularly with a radio time signal. Times and frequencies of radio time signals are listed in publications such as Radio Navigational Aids.\nThe marine sextant.\nThe second critical component of celestial navigation is to measure the angle formed at the observer's eye between the celestial body and the sensible horizon. The sextant, an optical instrument, is used to perform this function. The sextant consists of two primary assemblies. The frame is a rigid triangular structure with a pivot at the top and a graduated segment of a circle, referred to as the \"arc\", at the bottom. The second component is the index arm, which is attached to the pivot at the top of the frame. At the bottom is an endless vernier which clamps into teeth on the bottom of the \"arc\". The optical system consists of two mirrors and, generally, a low power telescope. One mirror, referred to as the \"index mirror\" is fixed to the top of the index arm, over the pivot. As the index arm is moved, this mirror rotates, and the graduated scale on the arc indicates the measured angle (\"altitude\"). The second mirror, referred to as the \"horizon glass\", is fixed to the front of the frame. One half of the horizon glass is silvered and the other half is clear. Light from the celestial body strikes the index mirror and is reflected to the silvered portion of the horizon glass, then back to the observer's eye through the telescope. The observer manipulates the index arm so the reflected image of the body in the horizon glass is just resting on the visual horizon, seen through the clear side of the horizon glass.\nThere are three main errors that must be corrected in order to each usage for navigation. The main errors are perpendicular error, side error and index error. Adjustment of the sextant consists of checking and aligning all the optical elements to eliminate the overall \"index error\" (or index correction). Index correction should be checked, using the horizon or more preferably a star, each time the sextant is used. The practice of taking celestial observations from the deck of a rolling ship, often through cloud cover and with a hazy horizon, is by far the most challenging part of celestial navigation.\nBubble octant.\nUntil the widespread usage of technologies such as inertial navigation systems, VHF omnidirectional range and GNSS, air navigators used the Bubble octant or bubble sextant. Using this instrument to take sights, mathematical calculations could then be carried out to determine the past position of the aircraft.\nInertial navigation.\nInertial navigation system (INS) is a dead reckoning type of navigation system that computes its position based on motion sensors. Before actually navigating, the initial latitude and longitude and the INS's physical orientation relative to the Earth (e.g., north and level) are established. After alignment, an INS receives impulses from motion detectors that measure (a) the acceleration along three axes (accelerometers), and (b) rate of rotation about three orthogonal axes (gyroscopes). These enable an INS to continually and accurately calculate its current latitude and longitude (and often velocity).\nAdvantages over other navigation systems are that, once aligned, an INS does not require outside information. An INS is not affected by adverse weather conditions and it cannot be detected or jammed. Its disadvantage is that since the current position is calculated solely from previous positions and motion sensors, its errors are cumulative, increasing at a rate roughly proportional to the time since the initial position was input. Inertial navigation systems must therefore be frequently corrected with a location 'fix' from some other type of navigation system.\nThe first inertial system is considered to be the V-2 guidance system deployed by the Germans in 1942. However, inertial sensors are traced to the early 19th century. The advantages INSs led their use in aircraft, missiles, surface ships and submarines. For example, the U.S. Navy developed the Ships Inertial Navigation System (SINS) during the Polaris missile program to ensure a reliable and accurate navigation system to initial its missile guidance systems. Inertial navigation systems were in wide use until satellite navigation systems (GPS) became available. INSs are still in common use on submarines (since GPS reception or other fix sources are not possible while submerged) and long-range missiles but are not now widely found elsewhere.\nGravity-aided navigation.\nGravity-aided navigation originated in the 1990s and provides a technology to obtain a position fix for navigation. It utilises the concept that an onboard sensor measures elements of the gravitational vector while the platform is in motion and then these measurements are referenced to a map of the Earth's gravitational field to determine a position.\nSpace navigation.\nNot to be confused with satellite navigation, which depends upon satellites to function, space navigation refers to the navigation of spacecraft themselves. This has historically been achieved (during the Apollo program) via a navigational computer, an Inertial navigation system, and via celestial inputs entered by astronauts which were recorded by sextant and telescope. Space rated navigational computers, like those found on Apollo and later missions, are designed to be hardened against possible data corruption from radiation. Navigation in space has three main components: the use of a suitable reference trajectory which describes the planned flight path of the spacecraft, monitoring the actual spacecraft position while the mission is in flight (orbit determination) and creating maneuvers to bring the spacecraft back to the reference trajectory as required (flight path control).\nAnother possibility that has been explored for deep space navigation is Pulsar navigation, which compares the X-ray bursts from a collection of known pulsars in order to determine the position of a spacecraft. This method has been tested by multiple space agencies, such as NASA and ESA.\nElectronic navigation.\nRadar navigation.\nRadars can be used for navigation and marine radars are commonly fitted to ships for navigation at sea. Radar is an effective aid to navigation because it provides ranges and bearings to objects within range of the radar scanner. When a vessel (ship or boat) is within radar range of land or fixed objects (such as special radar aids to navigation and navigation marks) the navigator can take distances and angular bearings to charted objects and use these to establish arcs of position and lines of position on a chart. A fix consisting of only radar information is called a radar fix. Types of radar fixes include \"range and bearing to a single object,\" \"two or more bearings,\" \"tangent bearings,\" and \"two or more ranges.\" Radar can also be used with ECDIS as a means of position fixing with the radar image or distance/bearing overlaid onto an Electronic nautical chart.\nParallel indexing is a technique defined by William Burger in the 1957 book \"The Radar Observer's Handbook\". This technique involves creating a line on the screen that is parallel to the ship's course, but offset to the left or right by some distance. This parallel line allows the navigator to maintain a given distance away from hazards. The line on the radar screen is set to a specific distance and angle, then the ship's position relative to the parallel line is observed. This can provide an immediate reference to the navigator as to whether the ship is on or off its intended course for navigation.\nOther techniques that are less used in general navigation have been developed for special situations. One, known as the \"contour method,\" involves marking a transparent plastic template on the radar screen and moving it to the chart to fix a position. Another special technique, known as the Franklin Continuous Radar Plot Technique, involves drawing the path a radar object should follow on the radar display if the ship stays on its planned course. During the transit, the navigator can check that the ship is on track by checking that the pip lies on the drawn line.\nRadio navigation.\nA radio direction finder or RDF is a device for finding the direction to a radio source. Due to radio's ability to travel very long distances \"over the horizon\", it makes a particularly good navigation system for ships and aircraft that might be flying at a distance from land. RDFs works by rotating a directional antenna and listening for the direction in which the signal from a known station comes through most strongly. This sort of system was widely used in the 1930s and 1940s. RDF antennas are easy to spot on German World War II aircraft, as loops under the rear section of the fuselage, whereas most US aircraft enclosed the antenna in a small teardrop-shaped fairing.\nIn navigational applications, RDF signals are provided in the form of \"radio beacons\", the radio version of a lighthouse. The signal is typically a simple AM broadcast of a morse code series of letters, which the RDF can tune in to see if the beacon is \"on the air\". Most modern detectors can also tune in any commercial radio stations, which is particularly useful due to their high power and location near major cities.\nDecca, OMEGA, and LORAN-C are three similar hyperbolic navigation systems. Decca was a hyperbolic low frequency radio navigation system (also known as multilateration) that was first deployed during World War II when the Allied forces needed a system which could be used to achieve accurate landings. As was the case with Loran C, its primary use was for ship navigation in coastal waters. Fishing vessels were major post-war users, but it was also used on aircraft, including a very early (1949) application of moving-map displays. The system was deployed in the North Sea and was used by helicopters operating to oil platforms.\nThe OMEGA Navigation System was the first truly global radio navigation system for aircraft, operated by the United States in cooperation with six partner nations. OMEGA was developed by the United States Navy for military aviation users. It was approved for development in 1968 and promised a true worldwide oceanic coverage capability with only eight transmitters and the ability to achieve a four-mile (6\u00a0km) accuracy when fixing a position. Initially, the system was to be used for navigating nuclear bombers across the North Pole to Russia. Later, it was found useful for submarines. Due to the success of the Global Positioning System the use of Omega declined during the 1990s, to a point where the cost of operating Omega could no longer be justified. Omega was terminated on September 30, 1997, and all stations ceased operation.\nLORAN is a terrestrial navigation system using low frequency radio transmitters that use the time interval between radio signals received from three or more stations to determine the position of a ship or aircraft. The current version of LORAN in common use is LORAN-C, which operates in the low frequency portion of the EM spectrum from 90 to 110 kHz. Many nations are users of the system, including the United States, Japan, and several European countries. Russia uses a nearly exact system in the same frequency range, called CHAYKA. LORAN use is in steep decline, with GPS being the primary replacement. However, there are attempts to enhance and re-popularize LORAN. LORAN signals are less susceptible to interference and can penetrate better into foliage and buildings than GPS signals.\nSatellite navigation.\nA GNSS allow small electronic receivers to determine their location (longitude, latitude, and altitude) within a few meters using time signals transmitted along a line of sight by radio from satellites. Positions derived can then be used with maps and charts for satellite navigation. Since the first experimental satellite was launched in 1978, GNSS have become an indispensable aid to navigation around the world, and an important tool for map-making and land surveying. GNSS also provides a precise time reference used in many applications including scientific study of earthquakes, and synchronization of telecommunications networks. Global Navigation Satellite System or GNSS is the term for satellite navigation systems that provide positioning with global coverage. The first system, GPS was developed by the United States Department of Defense and officially named NAVSTAR GPS (NAVigation Satellite Timing And Ranging Global Positioning System). The satellite constellation is managed by the United States Air Force 50th Space Wing. The cost of maintaining the system is approximately US$750 million per year, including the replacement of aging satellites, and research and development. Despite this fact, GPS is free for civilian use as a public good.\nWith improvements in technology and developments globally, as of 2024, there are several different operational GNSS now available for navigation by the public. These include the United States NAVSTAR Global Positioning System (GPS), the Russian GLONASS, the European Union's Galileo positioning system and the Beidou navigation system of China. The different global systems have varying differences in accuracy but stated positions are normally in the range of between 1 and 10 metres accuracy depending on system and on that system's satellite coverage. As a result over 100 satellites are in medium Earth orbit, transmitting signals allowing GNSS receivers to determine the receiver's location, speed and direction. There are also several regional GNSS systems available for navigation, including the Indian Regional Navigation Satellite System and the Quasi-Zenith Satellite System. However, not all GNSS receivers are capable of operating with these systems and older GNSS receivers, such as on old ships may not be capable of receiving all of the GNSS now available to users.\nModern smartphones act as personal GNSS navigators for civilians who own them. Overuse of these devices, whether in the vehicle or on foot, can lead to a relative inability to learn about navigated environments, resulting in sub-optimal navigation abilities when and if these devices become unavailable. Typically a compass is also provided to determine direction when not moving.\nAcoustic navigation.\nAcoustic location is a method of navigation by the use of acoustic positioning systems which determine the position of an object by using sound waves. It is primarily used by submarines and ships fitted with sonar and similar transducer based technologies. Underwater acoustic positioning systems are also commonly used by divers and Remotely operated underwater vehicles, specifically the Long baseline acoustic positioning system, the Short baseline acoustic positioning system and the Ultra-short baseline acoustic positioning system.\nNavigation processes.\nPassage planning.\nPassage planning or voyage planning is a procedure to develop a complete description of vessel's voyage from start to finish. The plan includes leaving the dock and harbor area, the en route portion of a voyage, approaching the destination, and mooring. According to international law, a vessel's captain is legally responsible for passage planning, however on larger vessels, the task will be delegated to the ship's navigator.\nStudies show that human error is a factor in 80 percent of navigational accidents and that in many cases the human making the error had access to information that could have prevented the accident. The practice of voyage planning has evolved from penciling lines on nautical charts to a process of risk management.\nPassage planning consists of four stages: appraisal, planning, execution, and monitoring, which are specified in \"International Maritime Organization Resolution A.893(21), Guidelines For Voyage Planning,\" and these guidelines are reflected in the local laws of IMO signatory countries (for example, Title 33 of the U.S. Code of Federal Regulations), and a number of professional books or publications. There are some fifty elements of a comprehensive passage plan depending on the size and type of vessel.\nThe appraisal stage deals with the collection of information relevant to the proposed voyage as well as ascertaining risks and assessing the key features of the voyage. This will involve considering the type of navigation required e.g. Ice navigation, the region the ship will be passing through and the hydrographic information on the route. In the next stage, the written plan is created. The third stage is the execution of the finalised voyage plan, taking into account any special circumstances which may arise such as changes in the weather, which may require the plan to be reviewed or altered. The final stage of passage planning consists of monitoring the vessel's progress in relation to the plan and responding to deviations and unforeseen circumstances.\nIntegrated bridge systems.\nElectronic integrated bridge concepts are driving future navigation system planning. Integrated systems take inputs from various ship sensors, electronically display positioning information, and provide control signals required to maintain a vessel on a preset course. The navigator becomes a system manager, choosing system presets, interpreting system output, and monitoring vessel response.\nShips and similar vessels.\nOne day's work in traditional navigation.\nIn traditional marine navigation, one day's work in navigation is a minimal set of tasks consistent with prudent celestial navigation. The definition and processes vary on military and civilian vessels, and from ship to ship, but the traditional method takes a form resembling:\nNavigation on ships is usually always conducted on the bridge. It may also take place in adjacent space, where chart tables and publications are available. However, increasingly traditional navigation processes have been replaced with technological processes for marine navigation using GNSS and marine radar.\nLand navigation.\nNavigation for cars and other land-based travel typically uses maps, landmarks, and in recent times computer navigation (\"satnav\", short for satellite navigation), as well as any means available on water.\nComputerized navigation commonly relies on GPS for current location information, a navigational map database of roads and navigable routes, and uses algorithms related to the shortest path problem to identify optimal routes.\nPedestrian navigation is involved in orienteering, land navigation (military), and wayfinding.\nUnderwater navigation.\nSubmariners, divers, remotely operated underwater vehicles (ROVs) and other underwater craft carry out underwater navigation by a variety of methods and processes including GNSS, radar navigation and sonar/acoustic position fixing.\nArtificial intelligence.\nArtificial intelligence can be utilised to assist with planning, problem-serving and decision-making processes in navigation. This includes using AI in navigation systems such as GNSS as well as in general computing to assist with position fixing and monitoring from one position to another such as in vehicles, planes and cars.\nStandards, training and organisations.\nProfessional standards for navigation depend on the type of navigation and vary by country. For marine navigation, Merchant Navy deck officers are trained and internationally certified according to the STCW Convention. Leisure and amateur mariners may undertake lessons in navigation at local/regional training schools. Naval officers receive navigation training as part of their naval training.\nIn land navigation, courses and training is often provided to young persons as part of general or extra-curricular education. Land navigation is also an essential part of army training. Additionally, organisations such as the Scouts and DoE programme teach navigation to their students. Orienteering organisations are a type of sports that require navigational skills using a map and compass to navigate from point to point in diverse and usually unfamiliar terrain whilst moving at speed.\nIn aviation, pilots undertake air navigation training as part of learning to fly.\nProfessional organisations also assist to encourage improvements in navigation or bring together navigators in learned environments. The Royal Institute of Navigation (RIN) is a learned society with charitable status, aimed at furthering the development of navigation on land and sea, in the air and in space. It was founded in 1947 as a forum for mariners, pilots, engineers and academics to compare their experiences and exchange information. In the US, the Institute of Navigation (ION) is a non-profit professional organisation advancing the art and science of positioning, navigation and timing.\nPublications.\nNumerous nautical publications are available on navigation, which are published by professional sources all over the world. In the UK, the United Kingdom Hydrographic Office, the Witherby Publishing Group and the Nautical Institute provide numerous navigational publications, including the comprehensive Admiralty Manual of Navigation.\nIn the US, Bowditch's American Practical Navigator is a free available encyclopedia of navigation issued by the US Government.\nNavigation in spatial cognition.\nNavigation is an essential everyday activity that involves a series of abilities that help humans and animals to locate, track, and follow paths in order to arrive at different destinations. Navigation, in spatial cognition, allows for acquiring information about the environment by using the body and landmarks of the environment as frames of references to create mental representations of our environment, also known as a cognitive map. Humans navigate by transitioning between different spaces and coordinating both egocentric and allocentric frames of reference.\nNavigation can be distinguished into two sptial components: locomotion and wayfinding. Locomotion is the process of movement from one place to another, both in humans and in animals. Locomotion helps you understand an environment by moving through a space in order to create a mental representation of it. Wayfinding is defined as an active process of following or deciding upon a path between one place to another through mental representations. It involves processes such as representation, planning and decision which help to avoid obstacles, to stay on course or to regulate pace when approaching particular objects.\nNavigation and wayfinding can be approached in the environmental space. According to Dan Montello\u2019s space classification, there are four levels of space with the third being the environmental space. The environmental space represents a very large space, like a city, and can only be fully explored through movement since all objects and space are not directly visible. Also Barbara Tversky systematized the space, but this time taking into consideration the three dimensions that correspond to the axes of the human body and its extensions: above/below, front/back and left/right. Tversky ultimately proposed a fourfold classification of navigable space: space of the body, space around the body, space of navigation and space of graphics.\nWayfinding.\nThere are two types of wayfinding in navigation: aided and unaided. Aided wayfinding requires a person to use various types of media, such as maps, GPS, directional signage, etc., in their navigation process which generally involves low spatial reasoning and is less cognitively demanding. Unaided wayfinding involves no such devices for the person who is navigating. Unaided wayfinding can be subdivided into a taxonomy of tasks depending on whether it is undirected or directed, which basically makes the distinction of whether there is a precise destination or not: undirected wayfinding means that a person is simply exploring an environment for pleasure without any set destination.\nDirected wayfinding, instead, can be further subdivided into search vs. target approximation. Search means that a person does not know where the destination is located and must find it either in an unfamiliar environment, which is labeled as an uninformed search, or in a familiar environment, labeled as an informed search. In target approximation, on the other hand, the location of the destination is known to the navigator but a further distinction is made based on whether the navigator knows how to arrive or not to the destination. Path following means that the environment, the path, and the destination are all known which means that the navigator simply follows the path they already know and arrive at the destination without much thought. For example, when you are in your city and walking on the same path as you normally take from your house to your job or university. However, path finding means that the navigator knows where the destination is but does not know the route they have to take to arrive at the destination: you know where a specific store is but you do not know how to arrive there or what path to take. If the navigator does not know the environment, it is called path search which means that only the destination is known while neither the path nor the environment is: you are in a new city and need to arrive at the train station but do not know how to get there. Path planning, on the other hand, means that the navigator knows both where the destination is and is familiar with the environment so they only need to plan the route or path that they should take to arrive at their target. For example, if you are in your city and need to get to a specific store that you know the destination of but do not know the specific path you need to take to get there.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21855", "revid": "525927", "url": "https://en.wikipedia.org/wiki?curid=21855", "title": "New Materials", "text": ""}
{"id": "21857", "revid": "50599820", "url": "https://en.wikipedia.org/wiki?curid=21857", "title": "Non-fiction", "text": "Type of genre, true work\nNon-fiction (or nonfiction) is any document or media content that attempts, in good faith, to convey information only about the real world, rather than being grounded in imagination. Non-fiction typically aims to present topics objectively based on historical, scientific, and empirical information. However, some non-fiction ranges into more subjective territory, including sincerely held opinions on real-world topics.\nOften referring specifically to prose writing, non-fiction is one of the two fundamental approaches to story and storytelling, in contrast to narrative fiction, which is largely populated by imaginary characters and events. Non-fiction writers can show the reasons and consequences of events, they can compare, contrast, classify, categorise and summarise information, put the facts in a logical or chronological order, infer and reach conclusions about facts, etc. They can use graphic, structural and printed appearance features such as pictures, graphs or charts, diagrams, flowcharts, summaries, glossaries, sidebars, timelines, table of contents, headings, subheadings, bolded or italicised words, footnotes, maps, indices, labels, captions, etc. to help readers find information. \nWhile specific claims in a non-fiction work may prove inaccurate, the sincere author aims to be truthful at the time of composition. A non-fiction account is an exercise in accurately representing a topic, and remains distinct from any implied endorsement.\nDescriptions.\nThe numerous narrative techniques used within fiction are generally thought inappropriate for use in non-fiction. They are still present particularly in older works, but are often muted so as not to overshadow the information within the work. Simplicity, clarity, and directness are some of the most important considerations when producing non-fiction. Audience is important in any artistic or descriptive endeavour, but it is perhaps most important in non-fiction. In fiction, the writer believes that readers will make an effort to follow and interpret an indirectly or abstractly presented progression of theme, whereas the production of non-fiction has more to do with the direct provision of information. Understanding of the potential readers' use for the work and their existing knowledge of a subject are both fundamental for effective non-fiction. Despite the claim to truth of non-fiction, it is often necessary to persuade the reader to agree with the ideas and so a balanced, coherent, and informed argument is vital. However, the boundaries between fiction and non-fiction are continually blurred and argued upon, especially in the field of biography; as Virginia Woolf said: \"if we think of truth as something of granite-like solidity and of personality as something of rainbow-like intangibility and reflect that the aim of biography is to weld these two into one seamless whole, we shall admit that the problem is a stiff one and that we need not wonder if biographers, for the most part failed to solve it.\"\nIncluding information that the author knows to be untrue within such works is usually regarded as dishonest. Still, certain kinds of written works can legitimately be either fiction or non-fiction, such as journals of self-expression, letters, magazine articles, and other expressions of imagination. Though they are mostly either one or the other, a blend of both is also possible. Some fiction may include non-fictional elements; semi-fiction is fiction implementing a great deal of non-fiction, (such as a fictional description based on a true story). Some non-fiction may include elements of unverified supposition, deduction, or imagination for the purpose of smoothing out a narrative, but the inclusion of open falsehoods would discredit it as a work of non-fiction. The publishing and bookselling businesses sometimes use the term \"creative nonfiction\" to distinguish works with a more literary or intellectual bent, as opposed to the bulk of non-fiction subjects.\nTypes.\nBased on the author's intention or the purpose of the content, the main genres of non-fiction are instructional, explanatory, discussion-based, report-based (non-chronological), opinion-based (persuasive) and relating (chronological recounting) non-fiction. Non-fictional works of these different genres can be created with the help of a range of structures or formats such as: \nCommon literary examples of non-fiction include expository, argumentative, functional, and opinion pieces; essays on art or literature; biographies; memoirs; journalism; and historical, scientific, technical, or economic writings (including electronic ones).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21861", "revid": "7021758", "url": "https://en.wikipedia.org/wiki?curid=21861", "title": "Cryptonomicon", "text": "1999 novel by Neal Stephenson\nCryptonomicon is a 1999 novel by American author Neal Stephenson, set in two different time periods. One group of characters are World War II\u2013era Allied codebreakers and tactical-deception operatives affiliated with the British Government Code and Cypher School at Bletchley Park, and disillusioned Axis military and intelligence figures. The second narrative is set in the late 1990s, with characters that are (in part) descendants of those of the earlier time period, who employ cryptologic, telecom, and computer technology to build an underground data haven in the fictional Sultanate of Kinakuta. Their goal is to facilitate anonymous Internet banking using electronic money and (later) digital gold currency, with a long-term objective to distribute Holocaust Education and Avoidance Pod (HEAP) media for instructing genocide-target populations on defensive warfare.\nGenre and subject matter.\n\"Cryptonomicon\" is closer to the genres of historical fiction and contemporary techno-thriller than to the science fiction of Stephenson's two previous novels, \"Snow Crash\" and \"The Diamond Age\". It features fictionalized characterizations of such historical figures as Alan Turing, Albert Einstein, Douglas MacArthur, Winston Churchill, Isoroku Yamamoto, Karl D\u00f6nitz, Hermann G\u00f6ring, and Ronald Reagan, as well as some highly technical and detailed descriptions of modern cryptography and information security, with discussions of prime numbers, modular arithmetic, and Van Eck phreaking.\nTitle.\nAccording to Stephenson, the title is a play on \"Necronomicon\", the title of a book mentioned in the stories of horror writer H. P. Lovecraft:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I wanted to give it a title a 17th-century book by a scholar would be likely to have. And that's how I came up with \"Cryptonomicon\". I've heard the word \"Necronomicon\" bounced around. I haven't actually read the Lovecraft books, but clearly it's formed by analogy to that.\nThe novel's Cryptonomicon, described as a \"cryptographer's bible\", is a fictional book summarizing America's knowledge of cryptography and cryptanalysis. Begun by John Wilkins (the Cryptonomicon is mentioned in \"Quicksilver\") and amended over time by William Friedman, Lawrence Waterhouse, and others, the Cryptonomicon is described by Katherine Hayles as \"a kind of Kabala created by a Brotherhood of Code that stretches across centuries. To know its contents is to qualify as a Morlock among the Eloi, and the elite among the elite are those gifted enough actually to contribute to it.\"\nPlot.\nThe action takes place in two periods\u2014World War II and the late 1990s, during the Internet boom and the Asian financial crisis.\nIn 1942, Lawrence Pritchard Waterhouse, a young United States Navy code breaker and mathematical savant, is assigned to the newly formed joint British and American Detachment 2702. This ultra-secret unit's role is to hide the fact that Allied intelligence has cracked the German Enigma code. The detachment stages events, often behind enemy lines, that provide alternative explanations for the Allied intelligence successes. United States Marine sergeant Bobby Shaftoe, a veteran of China and Guadalcanal, serves in unit 2702, carrying out Waterhouse's plans. At the same time, Japanese soldiers, including mining engineer Goto Dengo, a \"friendly enemy\" of Shaftoe's, are assigned to build a mysterious bunker in the mountains in the Philippines as part of what turns out to be a literal suicide mission.\nCirca 1997, Randy Waterhouse (Lawrence's grandson) joins his old role-playing game companion Avi Halaby in a new startup, providing Pinoy-grams (inexpensive, non-real-time video messages) to migrant Filipinos via new fiber-optic cables. The Epiphyte Corporation uses this income stream to fund the creation of a data haven in the nearby fictional Sultanate of Kinakuta. Vietnam veteran Doug Shaftoe, the son of Bobby Shaftoe, and his daughter Amy do the undersea surveying for the cables and engineering work on the haven, which is overseen by Goto Furudenendu, heir-apparent to Goto Engineering. Complications arise as figures from the past reappear seeking gold or revenge.\nCharacters.\nWorld War II storyline.\nHistorical figures.\nFictionalized versions of several historical figures appear in the World War II storyline:\n1990s storyline.\nThe precise date of this storyline is not established, but the ages of characters, the technologies described, and certain date-specific references suggest that it is set in the late 1990s, at the time of the internet boom and the Asian financial crisis.\nTechnical content.\nPortions of \"Cryptonomicon\" contain large amounts of exposition. Several pages are spent explaining in detail some of the concepts behind cryptography and data storage security, including a description of Van Eck phreaking.\nCryptography.\nPontifex Cipher.\nIn the book, a playing-card based cipher called Pontifex is used. At Stephenson's request, Bruce Schneier developed such a cipher, calling it Solitaire, and a precise description of Solitaire is included as an appendix. Solitaire was cryptanalyzed in 1999.\nOne-time pad.\nSeveral of the characters in the book communicate with each other through the use of one-time pads. A one-time pad (OTP) is an encryption technique that requires a single-use pre-shared key of at least the same length as the encrypted message.\nThe story posits a variation of the OTP technique wherein there is no pre-shared key - the key is instead generated algorithmically.\nSoftware.\nFinux.\nHe also describes computers using a fictional operating system, Finux. The name is a thinly veiled reference to Linux, a kernel originally written by the Finnish native Linus Torvalds. Stephenson changed the name so as not to be creatively constrained by the technical details of Linux-based operating systems.\nAllusions and references from other works.\nAn excerpt from \"Cryptonomicon\" was originally published in the short story collection \"Disco 2000\", edited by Sarah Champion and published in 1998. Stephenson's subsequent work, a trio of novels dubbed \"The Baroque Cycle\", provides part of the deep backstory to the characters and events featured in \"Cryptonomicon\". Set in the late 17th and early 18th centuries, the novels feature ancestors of several characters in \"Cryptonomicon\", as well as events and objects which affect the action of the later-set book. The subtext implies the existence of secret societies or conspiracies, and familial tendencies and groupings found within those darker worlds.\nThe short story \"Jipi and the Paranoid Chip\" takes place some time after the events of \"Cryptonomicon\". In the story, the construction of the Crypt has triggered economic growth in Manila and Kinakuta, in which Goto Engineering, and Homa/Homer Goto, a Goto family heir, are involved. The IDTRO (\"Black Chamber\") is also mentioned.\nStephenson's 2019 novel, \"Fall; or, Dodge in Hell\", a sequel to \"Reamde\" (2011), reveals that \"Fall\", \"Reamde\", \"Cryptonomicon\" and \"The Baroque Cycle\" are all set in the same fictional universe, with references to the Waterhouse, Shaftoe and Hacklheber families, as well as Societas Eruditorum and Epiphyte Corporation. Two \"Wise\" entities from \"The Baroque Cycle\" also appear in \"Fall,\" including Enoch Root.\nPeter Thiel states in his book \"Zero to One\" that \"Cryptonomicon\" was required reading during the early days of PayPal.\nLiterary significance and criticism.\nAccording to critic Jay Clayton, the book is written for a technical or geek audience. Despite the technical detail, the book drew praise from both Stephenson's science fiction fan base and literary critics and buyers. In Clayton's book \"Charles Dickens in Cyberspace: The Afterlife of the Nineteenth Century in Postmodern Culture\" (2003), he calls Stephenson's book the \"ultimate geek novel\" and draws attention to the \"literary-scientific-engineering-military-industrial-intelligence alliance\" that produced discoveries in two eras separated by fifty years, World War II and the Internet age. In July 2012, io9 included the book on its list of \"10 Science Fiction Novels You Pretend to Have Read\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21862", "revid": "43103191", "url": "https://en.wikipedia.org/wiki?curid=21862", "title": "In the Beginning... Was the Command Line", "text": "Essay by Neal Stephenson\nIn the Beginning... Was the Command Line is an essay by Neal Stephenson which was originally published online in 1999 and later made available in book form (November 1999, ). The essay is a commentary on why the proprietary operating systems business is unlikely to remain profitable in the future because of competition from free software. It also analyzes the corporate/collective culture of the Microsoft, Apple Computer, and free software communities.\nThemes.\nStephenson explores the graphical user interface (GUI) as a metaphor in terms of the increasing interposition of abstractions between humans and the actual workings of devices (in a similar manner to \"Zen and the Art of Motorcycle Maintenance\") and explains the beauty hackers feel in good-quality tools. He does this with a car analogy. He compares four operating systems, Mac OS by Apple Computer to a luxury European car, Windows by Microsoft to a station wagon, Linux to a free tank, and BeOS to a batmobile. Stephenson argues that people continue to buy the station wagon despite free tanks being given away, because people do not want to learn how to operate a tank; they know that the station wagon dealership has a machine shop that they can take their car to when it breaks down. Because of this attitude, Stephenson argues that Microsoft is not really a monopoly, as evidenced by the free availability of other choice OSes, but rather has simply accrued enough mindshare among the people to have them coming back. He compares Microsoft to Disney, in that both are selling a vision to their customers, who in turn \"want to believe\" in that vision.\nStephenson relays his experience with the Debian bug tracking system (http://). He then contrasts it with Microsoft's approach. Debian developers responded from around the world within a day. He was completely frustrated with his initial attempt to achieve the same response from Microsoft, but he concedes that his subsequent experience was satisfactory. The difference he notes is that Debian developers are personally accessible and transparently own up to defects in their OS distribution, while Microsoft pretends errors don't exist.\nLater developments.\nThe essay was written before the advent of Mac OS X. A recurring theme is the full power of the command line compared with easier-to-learn graphical user interfaces (GUIs) which are described as broken mixed metaphors for 'power users'. He then mentions GUIs that have traditional terminals in windows. In a Slashdot interview in 2004, in response to the question:\n... have you embraced the new UNIX based MacOS X as the OS you want to use when you \"Just want to go to Disneyland\"?\nhe replied:\nI embraced OS X as soon as it was available and have never looked back. So a lot of \"In the Beginning...was the Command Line\" is now obsolete. I keep meaning to update it, but if I'm honest with myself, I have to say this is unlikely.\nWith Neal Stephenson's permission, Garrett Birkel responded to \"In the Beginning...was the Command Line\" in 2004, bringing it up to date and critically discussing Stephenson's argument. Birkel's response is interspersed throughout the original text, which remains untouched.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21863", "revid": "55767", "url": "https://en.wikipedia.org/wiki?curid=21863", "title": "Netscape Navigator", "text": "Web browser by Netscape released in 1994\n \nNetscape Navigator was a series of the now-discontinued Netscape line of proprietary web browsers released during the 1990s. It was the flagship product of the Netscape Communications Corporation and was the dominant web browser in terms of usage share in the 1990s, but by around 2003 its user base had all but disappeared. This was partly because Microsoft bundled Internet Explorer with the Windows operating system.\nThe business demise of Netscape was a central premise of Microsoft's antitrust trial, wherein the Court ruled that Microsoft's bundling of Internet Explorer with the Windows operating system was a monopolistic and illegal business practice. The decision came too late for Netscape, however, as Internet Explorer had by then become the dominant web browser in Windows.\nThe Netscape Navigator web browser was succeeded by the Netscape Communicator suite in 1997. Netscape Communicator's 4.x source code was the base for the Netscape-developed Mozilla Application Suite, which was later renamed SeaMonkey. Netscape's Mozilla Suite also served as the base for a browser-only spinoff called Mozilla Firefox.\nThe Netscape Navigator name returned in 2007 when AOL announced version 9 of the Netscape series of browsers, Netscape Navigator 9. On December 28, 2007, AOL canceled its development but continued supporting the web browser with security updates until March 1, 2008. AOL allows downloading of archived versions of the Netscape Navigator web browser family.\nHistory and development.\nOrigin.\nNetscape Navigator was inspired by the success of the Mosaic web browser, which was co-written by Marc Andreessen, a part-time employee of the National Center for Supercomputing Applications at the University of Illinois. After Andreessen graduated in 1993, he moved to California and there met Jim Clark, the recently departed founder of Silicon Graphics. Clark believed that the Mosaic browser had great commercial possibilities and provided the seed money. Soon Mosaic Communications Corporation was in business in Mountain View, California, with Andreessen as a vice-president. Since the University of Illinois was unhappy with the company's use of the Mosaic name, the company changed its name to Netscape Communications (suggested by product manager Greg Sands) and named its flagship web browser Netscape Navigator.\nNetscape announced in its first press release (October 13, 1994) that it would make Navigator available without charge to all non-commercial users, and beta versions of version 1.0 and 1.1 were freely downloadable in November 1994 and March 1995, with the full version 1.0 available in December 1994. However, two months later, the company announced that only educational and non-profit institutions could use version 1.0 at no charge.\nThe reversal was complete with the availability of version 1.1 beta on March 6, 1995, in which a press release states that the final 1.1 release would be available at no cost only for academic and non-profit organizational use.\nThe first few releases of the product were made available in \"commercial\" and \"evaluation\" versions; for example, version \"1.0\" and version \"1.0N\". The \"N\" evaluation versions were identical to the commercial versions; the letter was intended as a reminder to people to pay for the browser once they felt they had tried it long enough and were satisfied with it. This distinction was formally dropped within a year of the initial release, and the full version of the browser continued to be made available for free online, with boxed versions available on floppy disks (and later CDs) in stores along with a period of phone support. During this era, \"Internet Starter Kit\" books were popular, and usually included a floppy disk or CD containing internet software, and this was a popular means of obtaining Netscape's and other browsers. Email support was initially free and remained so for a year or two until the volume of support requests grew too high.\nDuring development, the Netscape browser was known by the code name \"Mozilla\", which became the name of a Godzilla-like cartoon dragon mascot used prominently on the company's website. The Mozilla name was also used as the User-Agent in HTTP requests by the browser. Other web browsers claimed to be compatible with Netscape's extensions to HTML and therefore used the same name in their User-Agent identifiers so that web servers would send them the same pages as were sent to Netscape browsers. Mozilla is now a generic name for matters related to the open source successor to Netscape Communicator and is most identified with the browser Firefox.\nRise.\nWhen the consumer Internet revolution arrived in the mid-1990s, Netscape was well-positioned to take advantage of it and the influx of new users it brought. With a good mix of features and an attractive licensing scheme that allowed free use for non-commercial purposes, the Netscape browser soon became the de facto standard, particularly on the Windows platform. Internet service providers and computer magazine publishers helped make Navigator readily available.\nAn innovation that Netscape introduced in 1994 was the on-the-fly display of web pages, where text and graphics appeared on the screen as the web page downloaded. Earlier web browsers would not display a page until all graphics on it had been loaded over the network connection; this meant a user might have only a blank page for several minutes. With Netscape, people using dial-up connections could begin reading the text of a web page within seconds of entering a web address, even before the rest of the text and graphics had finished downloading. This made the web much more tolerable to the average user.\nThrough the late 1990s, Netscape made sure that Navigator remained the technical leader among web browsers. New features included cookies, frames, proxy auto-config, and JavaScript (in version 2.0). Although those and other innovations eventually became open standards of the W3C and ECMA and were emulated by other browsers, they were often viewed as controversial. Netscape, according to critics, was more interested in bending the web to its own de facto \"standards\" (bypassing standards committees and thus marginalizing the commercial competition) than it was in fixing bugs in its products. Consumer rights advocates were particularly critical of cookies and of commercial websites using them to invade individual privacy.\nIn the marketplace, however, these concerns made little difference. Netscape Navigator remained the market leader with more than 50% usage share. The browser software was available for a wide range of operating systems, including Windows (3.1, 95, 98, NT), Macintosh, Linux, OS/2, and many versions of Unix including OSF/1, Sun Solaris, BSD/OS, IRIX, AIX, and HP-UX, and looked and worked nearly identically on every one of them. Netscape began to experiment with prototypes of a web-based system, known internally as \"Constellation\", which would allow users to access and edit their files anywhere across a network, no matter what computer or operating system they happened to be using.\nIndustry observers forecast the dawn of a new era of connected computing. The underlying operating system, it was believed, would not be an important consideration; future applications would run within a web browser. This was seen by Netscape as a clear opportunity to entrench Navigator at the heart of the next generation of computing, and thus gain the opportunity to expand into all manner of other software and service markets.\nDecline.\nWith the success of Netscape showing the importance of the web (more people were using the Internet due in part to the ease of using Netscape), Internet browsing began to be seen as a potentially profitable market. Following Netscape's lead, Microsoft started a campaign to enter the web browser software market. Like Netscape before them, Microsoft licensed the Mosaic source code from Spyglass, Inc. (which in turn licensed code from University of Illinois). Using this basic code, Microsoft created Internet Explorer (IE).\nThe competition between Microsoft and Netscape dominated the browser wars. Internet Explorer, Version 1.0 (shipped in the Internet Jumpstart Kit in Microsoft Plus! For Windows 95) and IE, Version 2.0 (the first cross-platform version of the web browser, supporting both Windows and Mac OS) were thought by many to be inferior and primitive when compared to contemporary versions of Netscape Navigator. With the release of IE version 3.0 (1996), Microsoft was able to catch up with Netscape competitively, with IE Version 4.0 (1997) further improving in terms of market share. IE 5.0 (1999) improved stability and took significant market share from Netscape Navigator for the first time.\nThere were two versions of Netscape Navigator 3.0, the Standard Edition and the Gold Edition. The latter consisted of the Navigator browser with e-mail, news readers, and a WYSIWYG web page compositor; however, these extra functions enlarged and slowed the software, rendering it prone to crashing.\nThis Gold Edition was renamed Netscape Communicator starting with version 4.0; the name change diluted its name-recognition and confused users. Netscape CEO James L. Barksdale insisted on the name change because Communicator was a general-purpose \"client\" application, which contained the Navigator \"browser\".\nThe aging Netscape Communicator 4.x was slower than Internet Explorer 5.0. Typical web pages had become heavily illustrated, often JavaScript-intensive, and encoded with HTML features designed for specific purposes but now employed as global layout tools (HTML tables, the most obvious example of this, were especially difficult for Communicator to render). The Netscape browser, once a solid product, became crash-prone and buggy; for example, some versions re-downloaded an entire web page to re-render it when the browser window was re-sized (a nuisance to dial-up users), and the browser would usually crash when the page contained simple Cascading Style Sheets, as proper support for CSS never made it into Communicator 4.x. At the time that Communicator 4.0 was being developed, Netscape had a competing technology called JavaScript Style Sheets. Near the end of the development cycle, it became obvious that CSS would prevail, so Netscape quickly implemented a CSS to JSSS converter, which then processed CSS as JSSS (this is why turning JavaScript off also disabled CSS). Moreover, Netscape Communicator's browser interface design appeared dated in comparison to Internet Explorer and interface changes in Microsoft and Apple's operating systems.\nBy the end of the decade, Netscape's web browser had lost dominance over the Windows platform, and the August 1997 Microsoft financial agreement to invest $150 million in Apple Computer required that Apple make Internet Explorer the default web browser in new Mac OS distributions. The latest IE Mac release at that time was Internet Explorer version 3.0 for Macintosh, but Internet Explorer 4 was released later that year.\nMicrosoft succeeded in having ISPs and PC vendors distribute Internet Explorer to their customers instead of Netscape Navigator, mostly due to Microsoft using its leverage from Windows OEM licenses, and partly aided by Microsoft's investment in making IE brandable, such that a customized version of IE could be offered. Also, web developers used proprietary, browser-specific extensions in web pages. Both Microsoft and Netscape did this, having added many proprietary HTML tags to their browsers, which forced users to choose between two competing and almost incompatible web browsers.\nIn March 1998, Netscape released most of the development code base for Netscape Communicator under an open source license. Only pre-alpha versions of Netscape 5 were released before the open source community decided to scrap the Netscape Navigator codebase entirely and build a new web browser around the Gecko layout engine which Netscape had been developing but which had not yet incorporated. The community-developed open source project was named \"Mozilla\", Netscape Navigator's original code name. America Online bought Netscape; Netscape programmers took a pre-beta-quality form of the Mozilla codebase, gave it a new GUI, and released it as Netscape 6. This did nothing to win back users, who continued to migrate to Internet Explorer. After the release of Netscape 7 and a long public beta test, Mozilla 1.0 was released on June 5, 2002. The same code-base, notably the Gecko layout engine, became the basis of independent applications, including Firefox and Thunderbird.\nOn December 28, 2007, the Netscape developers announced that AOL had canceled development of Netscape Navigator, leaving it unsupported as of March 1, 2008. Archived and unsupported versions of the browser remain available for download.\nLegacy.\nNetscape's contributions to the web include JavaScript, which was submitted as a new standard to Ecma International. The resultant ECMAScript specification allowed JavaScript support by multiple web browsers and its use as a cross-browser scripting language, long after Netscape Navigator itself had dropped in popularity. Another example is the FRAME tag, which is widely supported today, and has been incorporated into official web standards such as the \"HTML 4.01 Frameset\" specification.\nIn a 2007 \"PC World\" column, the original Netscape Navigator was considered the \"best tech product of all time\" due to its impact on the Internet.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21865", "revid": "47988270", "url": "https://en.wikipedia.org/wiki?curid=21865", "title": "Neurotransmitter", "text": "Chemical substance that enables neurotransmission\nPostsynaptic density\nVoltage-gated Ca++ channel\nSynaptic vesicle\nNeurotransmitter transporter\nReceptor\nNeurotransmitter\nAxon terminal\nSynaptic cleft\nDendrite\n Structure of a typical chemical synapse\nA neurotransmitter is a signaling molecule secreted by a neuron to affect another cell across a synapse. The cell receiving the signal, or target cell, may be another neuron, but could also be a gland or muscle cell.\nNeurotransmitters are released from synaptic vesicles into the synaptic cleft where they are able to interact with neurotransmitter receptors on the target cell. Some neurotransmitters are also stored in large dense core vesicles. The neurotransmitter's effect on the target cell is determined by the receptor it binds to. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available and often require a small number of biosynthetic steps for conversion.\nNeurotransmitters are essential to the function of complex neural systems. The exact number of unique neurotransmitters in humans is unknown, but more than 100 have been identified. Common neurotransmitters include glutamate, GABA, acetylcholine, glycine, dopamine and norepinephrine.\nMechanism and cycle.\nSynthesis.\nNeurotransmitters are generally synthesized in neurons and are made up of, or derived from, precursor molecules that are found abundantly in the cell. Classes of neurotransmitters include amino acids, monoamines, and peptides. Monoamines are synthesized by altering a single amino acid. For example, the precursor of serotonin is the amino acid tryptophan. Peptide neurotransmitters, or neuropeptides, are protein transmitters which are larger than the classical small-molecule neurotransmitters and are often released together to elicit a modulatory effect. Purine neurotransmitters, like ATP, are derived from nucleic acids. Metabolic products such as nitric oxide and carbon monoxide have also been reported to act like neurotransmitters.\nStorage.\nNeurotransmitters are generally stored in synaptic vesicles, clustered close to the cell membrane at the axon terminal of the presynaptic neuron. However, some neurotransmitters, like the metabolic gases carbon monoxide and nitric oxide, are synthesized and released immediately following an action potential without ever being stored in vesicles.\nRelease.\nGenerally, a neurotransmitter is released via exocytosis at the presynaptic terminal in response to an electrical signal called an action potential in the presynaptic neuron. However, low-level \"baseline\" release also occurs without electrical stimulation. Neurotransmitters are released into and diffuse across the synaptic cleft, where they bind to specific receptors on the membrane of the postsynaptic neuron.\nReceptor interaction.\nAfter being released into the synaptic cleft, neurotransmitters diffuse across the synapse where they are able to interact with receptors on the target cell. The effect of the neurotransmitter is dependent on the identity of the target cell's receptors present at the synapse. Depending on the receptor, binding of neurotransmitters may cause excitation, inhibition, or modulation of the postsynaptic neuron.\nElimination.\nIn order to avoid continuous activation of receptors on the post-synaptic or target cell, neurotransmitters must be removed from the synaptic cleft. Neurotransmitters are removed through one of three mechanisms:\nFor example, acetylcholine is eliminated by having its acetyl group cleaved by the enzyme acetylcholinesterase; the remaining choline is then taken in and recycled by the pre-synaptic neuron to synthesize more acetylcholine. Other neurotransmitters are able to diffuse away from their targeted synaptic junctions and are eliminated from the body via the kidneys, or destroyed in the liver. Each neurotransmitter has very specific degradation pathways at regulatory points, which may be targeted by the body's regulatory system or medication. Cocaine blocks a dopamine transporter responsible for the reuptake of dopamine. Without the transporter, dopamine diffuses much more slowly from the synaptic cleft and continues to activate the dopamine receptors on the target cell.\nDiscovery.\nUntil the early 20th century, scientists assumed that the majority of synaptic communication in the brain was electrical. However, through histological examinations by Ram\u00f3n y Cajal, a 20 to 40\u00a0nm gap between neurons, known today as the synaptic cleft, was discovered. The presence of such a gap suggested communication via chemical messengers traversing the synaptic cleft, and in 1921 German pharmacologist Otto Loewi confirmed that neurons can communicate by releasing chemicals. Through a series of experiments involving the vagus nerves of frogs, Loewi was able to manually slow the heart rate of frogs by controlling the amount of saline solution present around the vagus nerve. Upon completion of this experiment, Loewi asserted that sympathetic regulation of cardiac function can be mediated through changes in chemical concentrations. Furthermore, Otto Loewi is credited with discovering acetylcholine (ACh) \u2013 the first known neurotransmitter.\nIdentification.\nTo identify neurotransmitters, the following criteria are typically considered:\nHowever, given advances in pharmacology, genetics, and chemical neuroanatomy, the term \"neurotransmitter\" can be applied to chemicals that:\nThe anatomical localization of neurotransmitters is typically determined using immunocytochemical techniques, which identify the location of either the transmitter substances themselves or of the enzymes that are involved in their synthesis. Immunocytochemical techniques have also revealed that many transmitters, particularly the neuropeptides, are co-localized, that is, a neuron may release more than one transmitter from its synaptic terminal. Various techniques and experiments such as staining, stimulating, and collecting can be used to identify neurotransmitters throughout the central nervous system.\nActions.\nNeurons communicate with each other through synapses, specialized contact points where neurotransmitters transmit signals. When an action potential reaches the presynaptic terminal, the action potential can trigger the release of neurotransmitters into the synaptic cleft. These neurotransmitters then bind to receptors on the postsynaptic membrane, influencing the receiving neuron in either an inhibitory or excitatory manner. If the overall excitatory influences outweigh the inhibitory influences, the receiving neuron may generate its own action potential, continuing the transmission of information to the next neuron in the network. This process allows for the flow of information and the formation of complex neural networks.\nModulation.\nA neurotransmitter may have an excitatory, inhibitory or modulatory effect on the target cell. The effect is determined by the receptors the neurotransmitter interacts with at the post-synaptic membrane. Neurotransmitter influences trans-membrane ion flow either to increase (excitatory) or to decrease (inhibitory) the probability that the cell with which it comes in contact will produce an action potential. Synapses containing receptors with excitatory effects are called Type I synapses, while Type II synapses contain receptors with inhibitory effects. Thus, despite the wide variety of synapses, they all convey messages of only these two types. The two types are different appearance and are primarily located on different parts of the neurons under its influence. Receptors with modulatory effects are spread throughout all synaptic membranes and binding of neurotransmitters sets in motion signaling cascades that help the cell regulate its function. Binding of neurotransmitters to receptors with modulatory effects can have many results. For example, it may result in an increase or decrease in sensitivity to future stimulus by recruiting more or less receptors to the synaptic membrane.\nType I (excitatory) synapses are typically located on the shafts or the spines of dendrites, whereas type II (inhibitory) synapses are typically located on a cell body. In addition, Type I synapses have round synaptic vesicles, whereas the vesicles of type II synapses are flattened. The material on the presynaptic and post-synaptic membranes is denser in a Type I synapse than it is in a Type II, and the Type I synaptic cleft is wider. Finally, the active zone on a Type I synapse is larger than that on a Type II synapse.\nThe different locations of Type I and Type II synapses divide a neuron into two zones: an excitatory dendritic tree and an inhibitory cell body. From an inhibitory perspective, excitation comes in over the dendrites and spreads to the axon hillock to trigger an action potential. If the message is to be stopped, it is best stopped by applying inhibition on the cell body, close to the axon hillock where the action potential originates. Another way to conceptualize excitatory\u2013inhibitory interaction is to picture excitation overcoming inhibition. If the cell body is normally in an inhibited state, the only way to generate an action potential at the axon hillock is to reduce the cell body's inhibition. In this \"open the gates\" strategy, the excitatory message is like a racehorse ready to run down the track, but first, the inhibitory starting gate must be removed.\nNeurotransmitter actions.\nAs explained above, the only direct action of a neurotransmitter is to activate a receptor. Therefore, the effects of a neurotransmitter system depend on the connections of the neurons that use the transmitter, and the chemical properties of the receptors.\nTypes.\nThere are many different ways to classify neurotransmitters. They are commonly classified into amino acids, monoamines and peptides.\nSome of the major neurotransmitters are:\nIn addition, over 100 neuroactive peptides have been found, and new ones are discovered regularly. Many of these are co-released along with a small-molecule transmitter. Nevertheless, in some cases, a peptide is the primary transmitter at a synapse. Beta-Endorphin is a relatively well-known example of a peptide neurotransmitter because it engages in highly specific interactions with opioid receptors in the central nervous system.\nSingle ions (such as synaptically released zinc) are also considered neurotransmitters by some, as well as some gaseous molecules such as nitric oxide (NO), carbon monoxide (CO), and hydrogen sulfide (H2S). The gases are produced in the neural cytoplasm and are immediately diffused through the cell membrane into the extracellular fluid and into nearby cells to stimulate production of second messengers. Soluble gas neurotransmitters are difficult to study, as they act rapidly and are immediately broken down, existing for only a few seconds.\nThe most prevalent transmitter is glutamate, which is excitatory at well over 90% of the synapses in the human brain. The next most prevalent is gamma-Aminobutyric Acid, or GABA, which is inhibitory at more than 90% of the synapses that do not use glutamate. Although other transmitters are used in fewer synapses, they may be very important functionally: the great majority of psychoactive drugs exert their effects by altering the actions of some neurotransmitter systems, often acting through transmitters other than glutamate or GABA. Addictive drugs such as cocaine and amphetamines exert their effects primarily on the dopamine system. The addictive opiate drugs exert their effects primarily as functional analogs of opioid peptides, which, in turn, regulate dopamine levels.\nNeurotransmitter systems.\nNeurons expressing certain types of neurotransmitters sometimes form distinct systems, where activation of the system affects large volumes of the brain, called volume transmission. Major neurotransmitter systems include the noradrenaline (norepinephrine) system, the dopamine system, the serotonin system, and the cholinergic system, among others. Trace amines have a modulatory effect on neurotransmission in monoamine pathways (i.e., dopamine, norepinephrine, and serotonin pathways) throughout the brain via signaling through trace amine-associated receptor\u00a01. A brief comparison of these systems follows:\nDrug effects.\nUnderstanding the effects of drugs on neurotransmitters comprises a significant portion of research initiatives in the field of neuroscience. Most neuroscientists involved in this field of research believe that such efforts may further advance our understanding of the circuits responsible for various neurological diseases and disorders, as well as ways to effectively treat and someday possibly prevent or cure such illnesses.\nDrugs can influence behavior by altering neurotransmitter activity. For instance, drugs can decrease the rate of synthesis of neurotransmitters by affecting the synthetic enzyme(s) for that neurotransmitter. When neurotransmitter syntheses are blocked, the amount of neurotransmitters available for release becomes substantially lower, resulting in a decrease in neurotransmitter activity. Some drugs block or stimulate the release of specific neurotransmitters. Alternatively, drugs can prevent neurotransmitter storage in synaptic vesicles by causing the synaptic vesicle membranes to leak. Drugs that prevent a neurotransmitter from binding to its receptor are called receptor antagonists. For example, drugs used to treat patients with schizophrenia such as haloperidol, chlorpromazine, and clozapine are antagonists at receptors in the brain for dopamine. Other drugs act by binding to a receptor and mimicking the normal neurotransmitter. Such drugs are called receptor agonists. An example of a receptor agonist is morphine, an opiate that mimics effects of the endogenous neurotransmitter \u03b2-endorphin to relieve pain. Other drugs interfere with the deactivation of a neurotransmitter after it has been released, thereby prolonging the action of a neurotransmitter. This can be accomplished by blocking re-uptake or inhibiting degradative enzymes. Lastly, drugs can also prevent an action potential from occurring, blocking neuronal activity throughout the central and peripheral nervous system. Drugs such as tetrodotoxin that block neural activity are typically lethal.\nDrugs targeting the neurotransmitter of major systems affect the whole system, which can explain the complexity of action of some drugs. Cocaine, for example, blocks the re-uptake of dopamine back into the presynaptic neuron, leaving the neurotransmitter molecules in the synaptic gap for an extended period of time. Since the dopamine remains in the synapse longer, the neurotransmitter continues to bind to the receptors on the postsynaptic neuron, eliciting a pleasurable emotional response. Physical addiction to cocaine may result from prolonged exposure to excess dopamine in the synapses, which leads to the downregulation of some post-synaptic receptors. After the effects of the drug wear off, an individual can become depressed due to decreased probability of the neurotransmitter binding to a receptor. Fluoxetine is a selective serotonin re-uptake inhibitor (SSRI), which blocks re-uptake of serotonin by the presynaptic cell which increases the amount of serotonin present at the synapse and furthermore allows it to remain there longer, providing potential for the effect of naturally released serotonin. AMPT prevents the conversion of tyrosine to L-DOPA, the precursor to dopamine; reserpine prevents dopamine storage within vesicles; and deprenyl inhibits monoamine oxidase (MAO)-B and thus increases dopamine levels.\nAgonists.\nAn agonist is a chemical capable of binding to a receptor, such as a neurotransmitter receptor, and initiating the same reaction typically produced by the binding of the endogenous substance. An agonist of a neurotransmitter will thus initiate the same receptor response as the transmitter. In neurons, an agonist drug may activate neurotransmitter receptors either directly or indirectly. Direct-binding agonists can be further characterized as full agonists, partial agonists, inverse agonists.\nDirect agonists act similar to a neurotransmitter by binding directly to its associated receptor site(s), which may be located on the presynaptic neuron or postsynaptic neuron, or both. Typically, neurotransmitter receptors are located on the postsynaptic neuron, while neurotransmitter autoreceptors are located on the presynaptic neuron, as is the case for monoamine neurotransmitters; in some cases, a neurotransmitter utilizes retrograde neurotransmission, a type of feedback signaling in neurons where the neurotransmitter is released postsynaptically and binds to target receptors located on the presynaptic neuron. Nicotine, a compound found in tobacco, is a direct agonist of most nicotinic acetylcholine receptors, mainly located in cholinergic neurons. Opiates, such as morphine, heroin, hydrocodone, oxycodone, codeine, and methadone, are \u03bc-opioid receptor agonists; this action mediates their euphoriant and pain relieving properties.\nIndirect agonists increase the binding of neurotransmitters at their target receptors by stimulating the release or preventing the reuptake of neurotransmitters. Some indirect agonists trigger neurotransmitter release and prevent neurotransmitter reuptake. Amphetamine, for example, is an indirect agonist of postsynaptic dopamine, norepinephrine, and serotonin receptors in each their respective neurons; it produces both neurotransmitter release into the presynaptic neuron and subsequently the synaptic cleft and prevents their reuptake from the synaptic cleft by activating TAAR1, a presynaptic G protein-coupled receptor, and binding to a site on VMAT2, a type of monoamine transporter located on synaptic vesicles within monoamine neurons.\nAntagonists.\nAn antagonist is a chemical that acts within the body to reduce the physiological activity of another chemical substance (such as an opiate); especially one that opposes the action on the nervous system of a drug or a substance occurring naturally in the body by combining with and blocking its nervous receptor.\nThere are two main types of antagonist: direct-acting Antagonist and indirect-acting Antagonists:\nDrug antagonists.\nAn antagonist drug is one that attaches (or binds) to a site called a receptor without activating that receptor to produce a biological response. It is therefore said to have no intrinsic activity. An antagonist may also be called a receptor \"blocker\" because they block the effect of an agonist at the site. The pharmacological effects of an antagonist, therefore, result in preventing the corresponding receptor site's agonists (e.g., drugs, hormones, neurotransmitters) from binding to and activating it. Antagonists may be \"competitive\" or \"irreversible\".\nA competitive antagonist competes with an agonist for binding to the receptor. As the concentration of antagonist increases, the binding of the agonist is progressively inhibited, resulting in a decrease in the physiological response. High concentration of an antagonist can completely inhibit the response. This inhibition can be reversed, however, by an increase of the concentration of the agonist, since the agonist and antagonist compete for binding to the receptor. Competitive antagonists, therefore, can be characterized as shifting the dose\u2013response relationship for the agonist to the right. In the presence of a competitive antagonist, it takes an increased concentration of the agonist to produce the same response observed in the absence of the antagonist.\nAn irreversible antagonist binds so strongly to the receptor as to render the receptor unavailable for binding to the agonist. Irreversible antagonists may even form covalent chemical bonds with the receptor. In either case, if the concentration of the irreversible antagonist is high enough, the number of unbound receptors remaining for agonist binding may be so low that even high concentrations of the agonist do not produce the maximum biological response.\nDiseases and disorders.\nThe following sections describe how imbalances or dysfunction in specific neurotransmitters\u2014dopamine, serotonin, and glutamate\u2014have been tentatively linked to various mental or neurological disorders.\nDopamine.\nFor example, problems in producing dopamine (mainly in the substantia nigra) can result in Parkinson's disease, a disorder that affects a person's ability to move as they want to, resulting in stiffness, tremors or shaking, and other symptoms. Some studies suggest that having too little or too much dopamine or problems using dopamine in the thinking and feeling regions of the brain may play a role in disorders like schizophrenia or attention deficit hyperactivity disorder (ADHD). Dopamine is also involved in addiction and drug use, as most recreational drugs cause an influx of dopamine in the brain (especially opioid and methamphetamines) that produces a pleasurable feeling, which is why users constantly crave drugs.\nSerotonin.\nSimilarly, after some research suggested that drugs that block the recycling, or reuptake, of serotonin seemed to help some people diagnosed with depression, it was theorized that people with depression might have lower-than-normal serotonin levels. Though widely popularized, this theory was not borne out in subsequent research. Therefore, selective serotonin reuptake inhibitors (SSRIs) are used to increase the amounts of serotonin in synapses.\nGlutamate.\nFurthermore, problems with producing or using glutamate have been suggestively and tentatively linked to many mental disorders, including autism, obsessive\u2013compulsive disorder (OCD), schizophrenia, and depression. Having too much glutamate has been linked to neurological diseases such as Parkinson's disease, multiple sclerosis, Alzheimer's disease, stroke, and ALS (amyotrophic lateral sclerosis).\nNeurotransmitter imbalance.\nGenerally, there are no scientifically established \"norms\" for appropriate levels or \"balances\" of different neurotransmitters. In most cases, it is practically impossible to measure neurotransmitter levels in the brain or body at any given moment. Neurotransmitters regulate each other's release, and weak consistent imbalances in this mutual regulation were linked to temperament in healthy people. However, significant imbalances or disruptions in neurotransmitter systems are associated with various diseases and mental disorders, including Parkinson's disease, depression, insomnia, Attention Deficit Hyperactivity Disorder (ADHD), anxiety, memory loss, dramatic weight changes, and addictions. Some of these conditions are also related to neurotransmitter switching, a phenomenon where neurons change the type of neurotransmitters they release. Chronic physical or emotional stress can be a contributor to neurotransmitter system changes. Genetics also plays a role in neurotransmitter activities.\nApart from recreational use, medications that directly and indirectly interact with one or more transmitter or its receptor are commonly prescribed for psychiatric and psychological issues. Notably, drugs interacting with serotonin and norepinephrine are prescribed to patients with problems such as depression and anxiety\u2014though the notion that there is much solid medical evidence to support such interventions has been widely criticized. Studies shown that dopamine imbalance has an influence on multiple sclerosis and other neurological disorders.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21868", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=21868", "title": "Neutronium", "text": "Hypothetical substance in nuclear physics\nNeutronium (or neutrium, neutrite, or element zero) is a hypothetical substance made purely of neutrons. The word was coined by scientist Andreas von Antropoff in 1926 (before the 1932 discovery of the neutron) for the hypothetical \"element of atomic number zero\" (with no protons in its nucleus) that he placed at the head of the periodic table (denoted by -). However, the meaning of the term has changed over time, and from the last half of the 20th century onward it has been also used to refer to extremely dense substances resembling the neutron-degenerate matter theorized to exist in the cores of neutron stars.\nIn neutron stars.\nNeutronium is used in popular physics literature to refer to the material present in the cores of neutron stars (stars which are too massive to be supported by electron degeneracy pressure and which collapse into a denser phase of matter). In scientific literature the term \"neutron-degenerate matter\" or simply neutron matter is used for this material.\nHypothetical multi-neutrons.\nThe term \"neutronium\" was coined in 1926 by Andreas von Antropoff for a conjectured form of matter made up of neutrons with no protons or electrons, which he placed as the chemical element of atomic number zero at the head of his new version of the periodic table. It was subsequently placed in the middle of several spiral representations of the periodic system for classifying the chemical elements, such as those of Charles Janet (1928), Edgar Emerson (1944), and John D. Clark (1950).\nThe term is not used in the scientific literature either for a condensed form of matter, or as an element, and theoretical analysis expects no bound forms of neutrons without protons. \nScattering resonances with multiple neutrons.\nThe dineutron, containing two neutrons, is not a stable bound particle, but an extremely short-lived resonance state produced by nuclear reactions in the decay of beryllium-16. Evidence reported in 2012 for the resonance was disputed, but new work reportedly clears up the issues.\nThe dineutron hypothesis had been used in theoretical studies of the structure of exotic nuclei. For example 11Li is modeled as a dineutron bound to a 9Li core. A system made up of only two neutrons is not bound, though the attraction between them is very nearly enough to make them so. This has some consequences on nucleosynthesis and the abundance of the chemical elements.\nA trineutron state consisting of three bound neutrons has not been detected, and is not expected to be bound.\nA tetraneutron is a hypothetical particle consisting of four bound neutrons. Reports of its existence have not been replicated.\nCalculations indicate that the hypothetical pentaneutron state, consisting of a cluster of five neutrons, would not be bound.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21869", "revid": "32978398", "url": "https://en.wikipedia.org/wiki?curid=21869", "title": "Neutron star", "text": "Collapsed core of a massive star\nA neutron star is the gravitationally collapsed core of a massive supergiant star. It results from the supernova explosion of a massive star\u2014combined with gravitational collapse\u2014that compresses the core past white dwarf star density to that of atomic nuclei. Surpassed only by black holes, neutron stars are the second smallest and densest known class of stellar objects. Neutron stars have a radius on the order of and a mass of about 1.4\u00a0solar masses (M\u2609). Stars that collapse into neutron stars have a total mass of between or possibly more for those that are especially rich in elements heavier than hydrogen and helium.\nOnce formed, neutron stars no longer actively generate heat and cool over time, but they may still evolve further through collisions or accretion. Most of the basic models for these objects imply that they are composed almost entirely of neutrons, as the extreme pressure causes the electrons and protons present in normal matter to combine into additional neutrons. These stars are partially supported against further collapse by neutron degeneracy pressure, just as white dwarfs are supported against collapse by electron degeneracy pressure. However, this is not by itself sufficient to hold up an object beyond and repulsive nuclear forces increasingly contribute to supporting more massive neutron stars. If the remnant star has a mass exceeding the Tolman\u2013Oppenheimer\u2013Volkoff limit, approximately , the combination of degeneracy pressure and nuclear forces is insufficient to support the neutron star, causing it to collapse and form a black hole. The most massive neutron star detected so far, PSR J0952\u20130607, is estimated to be .\nNewly formed neutron stars may have surface temperatures of ten million kelvin or more. However, since neutron stars generate no new heat through fusion, they inexorably cool down after their formation. Still, surface temperatures will be around one million kelvin after one thousand to one million years, and older, even cooler neutron stars are still easy to discover. For example, the well-studied neutron star, RX J1856.5\u22123754, has an average surface temperature of about . By comparison, the effective surface temperature of the Sun is only .\nNeutron star material is remarkably dense: a normal-sized matchbox containing neutron-star material would have a weight of approximately 3 billion tonnes, the same weight as a 0.5-cubic-kilometer chunk of the Earth (a cube with edges of about 800 meters) from Earth's surface.\nAs a star's core collapses, its rotation rate increases due to conservation of angular momentum, so newly formed neutron stars typically rotate at up to several hundred times per second. Some neutron stars emit beams of electromagnetic radiation that make them detectable as pulsars, and the discovery of pulsars by Jocelyn Bell Burnell and Antony Hewish in 1967 was the first observational suggestion that neutron stars exist. The fastest-spinning neutron star known is PSR J1748\u22122446ad, rotating at a rate of 716 times per second or revolutions per minute, giving a linear (tangential) speed at the surface on the order of 0.24\u200d\"c\" (i.e., nearly a quarter the speed of light).\nThere are thought to be around one billion neutron stars in the Milky Way, and at a minimum several hundred million, a figure obtained by estimating the number of stars that have undergone supernova explosions. However, many of them have existed for a long period of time and have cooled down considerably. These stars radiate very little electromagnetic radiation; most neutron stars that have been detected occur only in certain situations in which they do radiate, such as if they are a pulsar or a part of a binary system. Slow-rotating and non-accreting neutron stars are difficult to detect, due to the absence of electromagnetic radiation; however, since the Hubble Space Telescope's detection of RX J1856.5\u22123754 in the 1990s, a few nearby neutron stars that appear to emit only thermal radiation have been detected.\nNeutron stars in binary systems can undergo accretion, in which case they emit large amounts of X-rays. During this process, matter is deposited on the surface of the stars, forming \"hotspots\" that can be sporadically identified as X-ray pulsar systems. Additionally, such accretions are able to \"recycle\" old pulsars, causing them to gain mass and rotate extremely quickly, forming millisecond pulsars. Furthermore, binary systems such as these continue to evolve, with many companions eventually becoming compact objects such as white dwarfs or neutron stars themselves, though other possibilities include a complete destruction of the companion through ablation or collision.\nThe study of neutron star systems is central to gravitational wave astronomy. The merger of binary neutron stars produces gravitational waves and is associated with kilonovae and short gamma-ray bursts. In 2017, the LIGO and Virgo interferometer sites observed GW170817, the first direct detection of gravitational waves from such an event. Prior to this, indirect evidence for gravitational waves was inferred by studying the gravity radiated from the orbital decay of a different type of (unmerged) binary neutron system, the Hulse\u2013Taylor pulsar.\nFormation.\nAny main-sequence star with an initial mass of greater than (eight times the mass of the Sun) has the potential to become a neutron star. As the star evolves away from the main sequence, stellar nucleosynthesis produces an iron-rich core. When all nuclear fuel in the core has been exhausted, the core must be supported by degeneracy pressure alone. Further deposits of mass from shell burning cause the core to exceed the Chandrasekhar limit. Electron-degeneracy pressure is overcome, and the core collapses further, causing temperatures to rise to over . At these temperatures, photodisintegration (the breakdown of iron nuclei into alpha particles due to high-energy gamma rays) occurs. As the temperature of the core continues to rise, electrons and protons combine to form neutrons via electron capture, releasing a flood of neutrinos. When densities reach a nuclear density of , a combination of strong force repulsion and neutron degeneracy pressure halts the contraction. The contracting outer envelope of the star is halted and rapidly flung outwards by a flux of neutrinos produced in the creation of the neutrons, resulting in a supernova and leaving behind a neutron star. However, if the remnant has a mass greater than about , it instead becomes a black hole.\nAs the core of a massive star is compressed during a Type II supernova or a Type Ib or Type Ic supernova, and collapses into a neutron star, it retains most of its angular momentum. Because it has only a tiny fraction of its parent's radius (sharply reducing its moment of inertia), a neutron star is formed with very high rotation speed and then, over a very long period, it slows. Neutron stars are known that have rotation periods from about 1.4\u00a0ms to 30\u00a0s. The neutron star's density also gives it very high surface gravity, with typical values ranging from to (more than times that of Earth). One measure of such immense gravity is the fact that neutron stars have an escape velocity of over half the speed of light. The neutron star's gravity accelerates infalling matter to tremendous speed, and tidal forces near the surface can cause spaghettification.\nProperties.\nEquation of state.\nThe equation of state of neutron stars is not currently known. This is because neutron stars are the second most dense known object in the universe, only less dense than black holes. The extreme density means there is no way to replicate the material on Earth in laboratories, which is how equations of state for other things like ideal gases are tested. The closest neutron star is many parsecs away, meaning there is no feasible way to study it directly. While it is known neutron stars should be similar to a , it cannot be modeled strictly like one (as white dwarfs are) because of the extreme gravity. General relativity must be considered for the neutron star equation of state because Newtonian gravity is no longer sufficient in those conditions. Effects such as quantum chromodynamics (QCD), superconductivity, and superfluidity must also be considered.\nAt the extraordinarily high densities of neutron stars, ordinary matter is squeezed to nuclear densities. Specifically, the matter ranges from nuclei embedded in a sea of electrons at low densities in the outer crust, to increasingly neutron-rich structures in the inner crust, to the extremely neutron-rich uniform matter in the outer core, and possibly exotic states of matter at high densities in the inner core.\nUnderstanding the nature of the matter present in the various layers of neutron stars, and the phase transitions that occur at the boundaries of the layers is a major unsolved problem in fundamental physics. A presumptive neutron star equation of state would encode information about the structure of a neutron star and would explain how matter behaves at the extreme densities found inside neutron stars. Constraints on the neutron star equation of state would then provide constraints on how the strong interaction of the Standard Model works, which would have profound implications for nuclear and atomic physics. This would make neutron stars natural laboratories for probing fundamental physics.\nFor example, the exotic states that may be found at the cores of neutron stars are types of QCD matter. At the extreme densities at the centers of neutron stars, neutrons become disrupted giving rise to a sea of quarks. This matter's equation of state is governed by the laws of quantum chromodynamics and since QCD matter cannot be produced in any laboratory on Earth, most of the current knowledge about it is only theoretical.\nDifferent equations of state lead to different values of observable quantities. While the equation of state relates directly only to density and pressure, these in turn lead to calculating observables like the speed of sound, mass, radius, and Love numbers. There are many proposed neutron star equations of state, such as FPS, UU, APR, L, and SLy, and it is an active area of research.\nAnother aspect of the equation of state is whether it is a soft or stiff equation of state. This relates to how much pressure there is at a certain energy density, and often corresponds to phase transitions. When the material is about to go through a phase transition, the pressure will tend to increase until it shifts into a more comfortable state of matter. A soft equation of state would have a gently rising pressure versus energy density while a stiff one would have a sharper rise in pressure. In neutron stars, nuclear physicists are still testing whether the equation of state should be stiff or soft, and sometimes it changes within individual equations of state depending on the phase transitions within the model. This is referred to as the equation of state stiffening or softening, depending on the previous behavior. Since it is unknown what neutron stars are made of, there is room for different phases of matter to be explored within the equation of state.\nDensity and pressure.\nNeutron stars have overall densities of to ( to times the density of the Sun), which is comparable to the approximate density of an atomic nucleus of . The density increases with depth, varying from about at the crust to an estimated or deeper inside. Pressure increases accordingly, from about (32\u00a0QPa) at the inner crust to in the center.\nA neutron star is so dense that one teaspoon (5 milliliters) of its material would have a mass over , about 900 times the mass of the Great Pyramid of Giza. The entire mass of the Earth at neutron star density would fit into a sphere 305\u00a0m in diameter, about the size of the Arecibo Telescope.\nIn popular scientific writing, neutron stars are sometimes described as macroscopic atomic nuclei. Indeed, both states are composed of nucleons, and they share a similar density to within an order of magnitude. However, in other respects, neutron stars and atomic nuclei are quite different. A nucleus is held together by the strong interaction, whereas a neutron star is held together by gravity. The density of a nucleus is uniform, while neutron stars are predicted to consist of multiple layers with varying compositions and densities.\nCurrent constraints.\nBecause equations of state for neutron stars lead to different observables, such as different mass-radius relations, there are many astronomical constraints on equations of state. These come mostly from the LIGO gravitational wave observatory and the NICER X-ray telescope.\nNICER's observations of pulsars in binary systems, from which the pulsar mass and radius can be estimated, can constrain the neutron star equation of state. A 2021 measurement of the pulsar PSR J0740+6620 was able to constrain the radius of a neutron star to with 95% confidence. These mass-radius constraints, combined with chiral effective field theory calculations, tighten constraints on the neutron star equation of state.\nEquation of state constraints from LIGO gravitational wave detections start with nuclear and atomic physics researchers, who work to propose theoretical equations of state (such as FPS, UU, APR, L, SLy, and others). The proposed equations of state can then be passed onto astrophysics researchers who run simulations of binary neutron star mergers. From these simulations, researchers can extract gravitational waveforms, thus studying the relationship between the equation of state and gravitational waves emitted by binary neutron star mergers. Using these relations, one can constrain the neutron star equation of state when gravitational waves from binary neutron star mergers are observed. Numerical relativity simulations of binary neutron star mergers have found relationships between the equation of state and frequency dependent peaks of the gravitational wave signal that must conform with LIGO detections. For example, the LIGO detection of the binary neutron star merger GW170817 provided limits on the tidal deformability of neutron star binaries, ruling out whole families equations of state. Future gravitational wave signals with next generation detectors like Cosmic Explorer can impose further constraints.\nWhen nuclear physicists are trying to understand the likelihood of their equation of state, it is good to compare with these constraints to see if it predicts neutron stars of these masses and radii. There is also recent work on constraining the equation of state with the speed of sound through hydrodynamics.\nTolman\u2013Oppenheimer\u2013Volkoff equation.\nThe Tolman\u2013Oppenheimer\u2013Volkoff (TOV) equation can be used to describe a neutron star. The equation is a solution to Einstein's equations from general relativity for a spherically symmetric, time invariant metric. With a given equation of state, solving the equation leads to observables such as the mass and radius. There are many codes that numerically solve the TOV equation for a given equation of state to find the mass-radius relation and other observables for that equation of state.\nThe following differential equations can be solved numerically to find the neutron star observables:\nformula_1\nformula_2\nwhere \"G\" is the gravitational constant, \"p\"(\"r\") is the pressure, \"\u03f5\"(\"r\") is the energy density (found from the equation of state), and \"c\" is the speed of light.\nMass\u2013radius relation.\nUsing the TOV equations and an equation of state, a mass\u2013radius curve can be found. In theory, for a correct equation of state, every neutron star that could possibly exist would lie along that curve. To create these curves, one must solve the TOV equations for different central densities. For each central density, one numerically solve the mass and pressure equations until the pressure goes to zero, which represents the outside of the star. Each solution gives a corresponding mass and radius for that central density.\nMass-radius curves determine what the maximum mass is for a given equation of state. Through most of the mass-radius curve, each radius corresponds to a unique mass value. At a certain point, the curve will reach a maximum and start going back down, leading to repeated mass values for different radii. This maximum point is what is known as the maximum mass. Beyond that mass, the star will no longer be stable, i.e. no longer be able to hold itself up against gravity, and would collapse into a black hole. Since each equation of state leads to a different mass-radius curve, they also lead to a unique maximum mass value. Therefore, astronomical observations of existing neutron stars can eliminate potential equations of state that find a maximum mass lower than the mass of the observed star. For example, Oppenheimer and Volkoff came up with the Tolman\u2013Oppenheimer\u2013Volkoff limit of ~ using a degenerate gas equation of state with the TOV equations. Since the neutron stars that have been observed are more massive than that, that maximum mass was discarded. The most recent massive neutron star that was observed was PSR J0952-0607 which was . Any equation of state with a mass less than that would not predict that star and thus is much less likely to be correct.\nOne phenomenon in this area of astrophysics relating to the maximum mass of neutron stars is what is called the \"mass gap\". The mass gap refers to a range of masses from roughly 2 to 5 solar masses where very few compact objects were observed. This range is based on the current assumed maximum mass of neutron stars (~) and the minimum black hole mass (~). Recently, some objects have been discovered that fall in that mass gap from gravitational wave detections. If the true maximum mass of neutron stars was known, it would help characterize compact objects in that mass range as either neutron stars or black holes.\nI-Love-Q relations.\nThere are three more properties of neutron stars that are dependent on the equation of state but can also be astronomically observed: the moment of inertia, the quadrupole moment, and the Love number. The moment of inertia of a neutron star describes how fast the star can rotate at a fixed spin momentum. The quadrupole moment of a neutron star specifies how much that star is deformed out of its spherical shape. The Love number of the neutron star represents how easy or difficult it is to deform the star due to tidal forces, typically important in binary systems.\nWhile these properties depend on the material of the star and therefore on the equation of state, there is a relation between these three quantities that is independent of the equation of state. This relation assumes slowly and uniformly rotating stars and uses general relativity to derive the relation. While this relation would not be able to add constraints to the equation of state, since it is independent of the equation of state, it does have other applications. If one of these three quantities can be measured for a particular neutron star, this relation can be used to find the other two. In addition, this relation can be used to break the degeneracies in detections by gravitational wave detectors of the quadrupole moment and spin, allowing the average spin to be determined within a certain confidence level.\nTemperature.\nThe temperature inside a newly formed neutron star is around to . However, so much of this energy is carried away by the enormous flux of residual neutrinos that the temperature of an isolated neutron star falls to around within a few years. After reaching this lower temperature, most of the remaining light emitted by the slowly cooling star will be in X-rays.\nSome researchers have proposed a neutron star classification system using Roman numerals (not to be confused with the Yerkes luminosity classes for non-degenerate stars) to sort neutron stars by their mass and cooling rates: type I for neutron stars with low mass and cooling rates, type II for neutron stars with higher mass and cooling rates, and a proposed type III for neutron stars with even higher mass, approaching , and with higher cooling rates and possibly candidates for exotic stars.\nMagnetic field.\nThe magnetic field strength on the surface of neutron stars ranges from c. to \u00a0tesla (T). These are orders of magnitude higher than in any other object: for comparison, a continuous 16\u00a0T field has been achieved in the laboratory and is sufficient to levitate a living frog due to diamagnetic levitation. Variations in magnetic field strengths are most likely the main factor that allows different types of neutron stars to be distinguished by their spectra, and explains the periodicity of pulsars.\nThe subclass of neutron stars known as magnetars have the strongest magnetic fields, in the range of to , and have become the widely accepted hypothesis for neutron star types soft gamma repeaters (SGRs) and anomalous X-ray pulsars (AXPs). The magnetic energy density of a field is extreme, greatly exceeding the mass-energy density of ordinary matter. Fields of this strength are able to polarize the vacuum to the point that the vacuum becomes birefringent. Photons can merge or split in two, and virtual particle\u2013antiparticle pairs are produced. The field changes electron energy levels and atoms are forced into thin cylinders. Unlike in an ordinary pulsar, magnetar spin-down can be directly powered by its magnetic field, and the magnetic field is strong enough to stress the crust to the point of fracture. Fractures of the crust cause starquakes, observed as extremely luminous millisecond hard gamma ray bursts. The fireball is trapped by the magnetic field, and comes in and out of view when the star rotates, which is observed as a periodic soft gamma repeater (SGR) emission with a period of 5\u20138\u00a0seconds and which lasts for a few minutes.\nThe origins of the strong magnetic field are as yet unclear. One hypothesis is that of \"flux freezing\", or conservation of the original magnetic flux during the formation of the neutron star. If an object has a certain magnetic flux over its surface area, and that area shrinks to a smaller area, but the magnetic flux is conserved, then the magnetic field would correspondingly increase. Likewise, a collapsing star begins with a much larger surface area than the resulting neutron star, and conservation of magnetic flux would result in a far stronger magnetic field. However, this simple explanation does not fully explain magnetic field strengths of neutron stars.\nGravity.\nThe gravitational field at a neutron star's surface is about times stronger than on Earth, at around . Such a strong gravitational field acts as a gravitational lens and bends the radiation emitted by the neutron star such that parts of the normally invisible rear surface become visible.\nIf the radius of the neutron star is 3\"GM\"/\"c\"2 or less, then the photons may be trapped in an orbit, thus making the whole surface of that neutron star visible from a single vantage point, along with destabilizing photon orbits at or below the 1 radius distance of the star.\nA fraction of the mass of a star that collapses to form a neutron star is released in the supernova explosion from which it forms (from the law of mass\u2013energy equivalence, \"E\" = \"mc\"2). The energy comes from the gravitational binding energy of a neutron star.\nHence, the gravitational field of a typical neutron star is huge. If an object were to fall from a height of on a neutron star in radius, it would reach the ground at around . However, even before impact, the tidal force would cause spaghettification, breaking any sort of an ordinary object into a stream of material.\nBecause of the enormous gravity, time dilation between a neutron star and Earth is significant. For example, eight years could pass on the surface of a neutron star, yet ten years would have passed on Earth, not including the time-dilation effect of the star's very rapid rotation.\nNeutron star relativistic equations of state describe the relation of radius vs. mass for various models. The most likely radii for a given neutron star mass are bracketed by models AP4 (smallest radius) and MS2 (largest radius). \"E\"B is the gravitational binding energy of the observed neutron star of mass of \"M\" with radius \"R\",\nformula_3\nwhere\nformula_4\nA neutron star of mass 2M\u2609 would not be more compact than radius (AP4 model). Its mass fraction gravitational binding energy \"E\"B/\"Mc\"2 would then be 0.187, \u221218.7% (exothermic). This is not near 0.6/2 = 0.3, \u221230%.\nStructure.\nCurrent understanding of the structure of neutron stars is defined by existing mathematical models, but it might be possible to infer some details through studies of neutron-star oscillations. Asteroseismology, a study applied to ordinary stars, can reveal the inner structure of neutron stars by analyzing observed spectra of stellar oscillations.\nCurrent models indicate that matter at the surface of a neutron star is composed of ordinary atomic nuclei crushed into a solid lattice with a sea of electrons flowing through the gaps between them. It is possible that the nuclei at the surface are iron, due to iron's high binding energy per nucleon. It is also possible that heavy elements, such as iron, simply sink beneath the surface, leaving only light nuclei like helium and hydrogen. If the surface temperature exceeds (as in the case of a young pulsar), the surface should be fluid instead of the solid phase that might exist in cooler neutron stars (temperature &lt;\u00a0).\nThe \"atmosphere\" of a neutron star is hypothesized to be at most several micrometers thick, and its dynamics are fully controlled by the neutron star's magnetic field. Below the atmosphere one encounters a solid \"crust\". This crust is extremely hard and very smooth (with maximum surface irregularities on the order of millimeters or less), due to the extreme gravitational field.\nProceeding inward, one encounters nuclei with ever-increasing numbers of neutrons; such nuclei would decay quickly on Earth, but are kept stable by tremendous pressures. As this process continues at increasing depths, the neutron drip becomes overwhelming, and the concentration of free neutrons increases rapidly.\nAfter a supernova explosion of a supergiant star, neutron stars are born from the remnants. A neutron star is composed mostly of neutrons (neutral particles) and contains a small fraction of protons (positively charged particles) and electrons (negatively charged particles), as well as nuclei. In the extreme density of a neutron star, many neutrons are free neutrons, meaning they are not bound in atomic nuclei and move freely within the star's dense matter, especially in the densest regions of the star\u2014the inner crust and core. Over the star's lifetime, as its density increases, the energy of the electrons also increases, which generates more neutrons.\nIn neutron stars, the neutron drip is the transition point where nuclei become so neutron-rich that they can no longer hold additional neutrons, leading to a sea of free neutrons being formed. The sea of neutrons formed after neutron drip provides additional pressure support, which helps maintain the star's structural integrity and prevents gravitational collapse. The neutron drip takes place within the inner crust of the neutron star and starts when the density becomes so high that nuclei can no longer hold additional neutrons.\nAt the beginning of the neutron drip, the pressure in the star from neutrons, electrons, and the total pressure is roughly equal. As the density of the neutron star increases, the nuclei break down, and the neutron pressure of the star becomes dominant. When the density reaches a point where nuclei touch and subsequently merge, they form a fluid of neutrons with a sprinkle of electrons and protons. This transition marks the neutron drip, where the dominant pressure in the neutron star shifts from degenerate electrons to neutrons.\nAt very high densities, the neutron pressure becomes the primary pressure holding up the star, with neutrons being non-relativistic (moving at a small fraction of the speed of light) and extremely compressed. However, at extremely high densities, neutrons begin to move at relativistic speeds (close to the speed of light). These high speeds significantly increase the star's overall pressure, altering the star's equilibrium state, and potentially leading to the formation of exotic states of matter.\nIn that region, there are nuclei, free electrons, and free neutrons. The nuclei become increasingly small (pressure due to gravity overwhelming the strong force) until the core is reached, by definition the point where mostly neutrons exist. The expected hierarchy of phases of nuclear matter in the inner crust has been characterized as \"nuclear pasta\", with fewer voids and larger structures towards higher pressures.\nThe composition of the superdense matter in the core remains uncertain. One model describes the core as superfluid neutron-degenerate matter (mostly neutrons, with some protons and electrons). More exotic forms of matter are possible, including degenerate strange matter (containing strange quarks in addition to up and down quarks), matter containing high-energy pions and kaons in addition to neutrons, or ultra-dense quark-degenerate matter.\nRadiation.\nPulsars.\nNeutron stars are detected from their electromagnetic radiation. Neutron stars are usually observed to pulse radio waves and other electromagnetic radiation, and neutron stars observed with pulses are called pulsars.\nPulsars' radiation is thought to be caused by particle acceleration near their magnetic poles, which need not be aligned with the rotational axis of the neutron star. It is thought that a large electrostatic field builds up near the magnetic poles, leading to electron emission. These electrons are magnetically accelerated along the field lines, leading to curvature radiation, with the radiation being strongly polarized towards the plane of curvature. In addition, high-energy photons can interact with lower-energy photons and the magnetic field for electron\u2212positron pair production, which through electron\u2013positron annihilation leads to further high-energy photons.\nThe radiation emanating from the magnetic poles of neutron stars can be described as \"magnetospheric radiation\", in reference to the magnetosphere of the neutron star. It is not to be confused with \"magnetic dipole radiation\", which is emitted because the magnetic axis is not aligned with the rotational axis, with a radiation frequency the same as the neutron star's rotational frequency.\nIf the axis of rotation of the neutron star is different from the magnetic axis, external viewers will only see these beams of radiation whenever the magnetic axis point towards them during the neutron star rotation. Therefore, periodic pulses are observed, at the same rate as the rotation of the neutron star.\nIn May 2022, astronomers reported an ultra-long-period radio-emitting neutron star PSR J0901-4046, with spin properties distinct from the known neutron stars. It is unclear how its radio emission is generated, and it challenges the current understanding of how pulsars evolve.\nNon-pulsating neutron stars.\nIn addition to pulsars, non-pulsating neutron stars have also been identified, although they may have minor periodic variation in luminosity. This seems to be a characteristic of the X-ray sources known as Central Compact Objects in supernova remnants (CCOs in SNRs), which are thought to be young, radio-quiet isolated neutron stars.\nSpectra.\nIn addition to radio emissions, neutron stars have also been identified in other parts of the electromagnetic spectrum. This includes visible light, near infrared, ultraviolet, X-rays, and gamma rays. Pulsars observed in X-rays are known as X-ray pulsars if accretion-powered, while those identified in visible light are known as optical pulsars. The majority of neutron stars detected, including those identified in optical, X-ray, and gamma rays, also emit radio waves; the Crab Pulsar produces electromagnetic emissions across the spectrum. However, there exist neutron stars called radio-quiet neutron stars, with no radio emissions detected.\nRotation.\nNeutron stars rotate extremely rapidly after their formation due to the conservation of angular momentum; in analogy to spinning ice skaters pulling in their arms, the slow rotation of the original star's core speeds up as it shrinks. A newborn neutron star can rotate many times a second.\nSpin down.\nOver time, neutron stars slow, as their rotating magnetic fields in effect radiate energy associated with the rotation; older neutron stars may take several seconds for each revolution. This is called \"spin down\". The rate at which a neutron star slows its rotation is usually constant and very small.\nThe periodic time (\"P\") is the rotational period, the time for one rotation of a neutron star. The spin-down rate, the rate of slowing of rotation, is then given the symbol formula_5 (\"P\"-dot), the derivative of \"P\" with respect to time. It is defined as periodic time increase per unit time; it is a dimensionless quantity, but can be given the units of s\u22c5s\u22121 (seconds per second).\nThe spin-down rate (\"P\"-dot) of neutron stars usually falls within the range of to , with the shorter period (or faster rotating) observable neutron stars usually having smaller \"P\"-dot. As a neutron star ages, its rotation slows (as \"P\" increases); eventually, the rate of rotation will become too slow to power the radio-emission mechanism, so radio emission from the neutron star no longer can be detected.\n\"P\" and \"P\"-dot allow minimum magnetic fields of neutron stars to be estimated. \"P\" and \"P\"-dot can be also used to calculate the \"characteristic age\" of a pulsar, but gives an estimate which is somewhat larger than the true age when it is applied to young pulsars.\n\"P\" and \"P\"-dot can also be combined with neutron star's moment of inertia to estimate a quantity called \"spin-down luminosity\", which is given the symbol formula_6 (\"E\"-dot). It is not the measured luminosity, but rather the calculated loss rate of rotational energy that would manifest itself as radiation. For neutron stars where the spin-down luminosity is comparable to the actual luminosity, the neutron stars are said to be \"rotation powered\". The observed luminosity of the Crab Pulsar is comparable to the spin-down luminosity, supporting the model that rotational kinetic energy powers the radiation from it. With neutron stars such as magnetars, where the actual luminosity exceeds the spin-down luminosity by about a factor of one hundred, it is assumed that the luminosity is powered by magnetic dissipation, rather than being rotation powered.\n\"P\" and \"P\"-dot can also be plotted for neutron stars to create a \"P\"\u2013\"P\"-dot diagram. It encodes a tremendous amount of information about the pulsar population and its properties, and has been likened to the Hertzsprung\u2013Russell diagram in its importance for neutron stars.\nSpin up.\nNeutron star rotational speeds can increase, a process known as spin up. Sometimes neutron stars absorb orbiting matter from companion stars, increasing the rotation rate and reshaping the neutron star into an oblate spheroid. This causes an increase in the rate of rotation of the neutron star of over a hundred times per second in the case of millisecond pulsars.\nThe most rapidly rotating neutron star currently known, PSR J1748-2446ad, rotates at 716 revolutions per second. A 2007 paper reported the detection of an X-ray burst oscillation, which provides an indirect measure of spin, of 1122\u00a0Hz from the neutron star XTE J1739-285, suggesting 1122 rotations a second. However, at present, this signal has only been seen once, and should be regarded as tentative until confirmed in another burst from that star.\nGlitches and starquakes.\nSometimes a neutron star will undergo a glitch, a sudden small increase of its rotational speed or spin up. Glitches are thought to be the effect of a starquake\u2014as the rotation of the neutron star slows, its shape becomes more spherical. Due to the stiffness of the \"neutron\" crust, this happens as discrete events when the crust ruptures, creating a starquake similar to earthquakes. After the starquake, the star will have a smaller equatorial radius, and because angular momentum is conserved, its rotational speed has increased.\nStarquakes occurring in magnetars, with a resulting glitch, is the leading hypothesis for the gamma-ray sources known as soft gamma repeaters.\nRecent work, however, suggests that a starquake would not release sufficient energy for a neutron star glitch; it has been suggested that glitches may instead be caused by transitions of vortices in the theoretical superfluid core of the neutron star from one metastable energy state to a lower one, thereby releasing energy that appears as an increase in the rotation rate.\nAnti-glitches.\nAn anti-glitch, a sudden small decrease in rotational speed, or spin down, of a neutron star has also been reported. It occurred in the magnetar 1E 2259+586, that in one case produced an X-ray luminosity increase of a factor of 20, and a significant spin-down rate change. Current neutron star models do not predict this behavior. If the cause were internal this suggests differential rotation of the solid outer crust and the superfluid component of the magnetar's inner structure.\nPopulation and distances.\nAt present, there are about 3,200 known neutron stars in the Milky Way and the Magellanic Clouds, the majority of which have been detected as radio pulsars. Neutron stars are mostly concentrated along the disk of the Milky Way, although the spread perpendicular to the disk is large because the supernova explosion process can impart high translational speeds (400\u00a0km/s) to the newly formed neutron star.\nSome of the closest known neutron stars are RX J1856.5\u22123754, which is about 400 light-years from Earth, and PSR J0108\u22121431 about 424 light-years. RX J1856.5-3754 is a member of a close group of neutron stars called The Magnificent Seven. Another nearby neutron star that was detected transiting the backdrop of the constellation Ursa Minor has been nicknamed Calvera by its Canadian and American discoverers, after the villain in the 1960 film \"The Magnificent Seven\". This rapidly moving object was discovered using the ROSAT Bright Source Catalog.\nNeutron stars are only detectable with modern technology during the earliest stages of their lives (almost always less than 1 million years) and are vastly outnumbered by older neutron stars that would only be detectable through their blackbody radiation and gravitational effects on other stars.\nBinary neutron star systems.\nAbout 5% of all known neutron stars are members of a binary system. The formation and evolution of binary neutron stars and double neutron stars can be a complex process. Neutron stars have been observed in binaries with ordinary main-sequence stars, red giants, white dwarfs, or other neutron stars. According to modern theories of binary evolution, it is expected that neutron stars also exist in binary systems with black hole companions. The merger of binaries containing two neutron stars, or a neutron star and a black hole, has been observed through the emission of gravitational waves.\nX-ray binaries.\nBinary systems containing neutron stars often emit X-rays, which are emitted by hot gas as it falls towards the surface of the neutron star. The source of the gas is the companion star, the outer layers of which can be stripped off by the gravitational field of the neutron star if the two stars are sufficiently close. As the neutron star accretes this gas, its mass can increase; if enough mass is accreted, the neutron star may collapse into a black hole.\nNeutron star binary mergers and nucleosynthesis.\nThe distance between two neutron stars in a close binary system is observed to shrink as gravitational waves are emitted. Ultimately, the neutron stars will come into contact and coalesce. The coalescence of binary neutron stars is one of the leading models for the origin of short gamma-ray bursts. Strong evidence for this model came from the observation of a kilonova associated with the short-duration gamma-ray burst GRB 130603B, and was finally confirmed by detection of gravitational wave GW170817 and short GRB 170817A by LIGO, Virgo, and 70 observatories covering the electromagnetic spectrum observing the event. The light emitted in the kilonova is believed to come from the radioactive decay of material ejected in the merger of the two neutron stars. The merger momentarily creates an environment of such extreme neutron flux that the \"r\"-process can occur; this\u2014as opposed to supernova nucleosynthesis\u2014may be responsible for the production of around half the isotopes in chemical elements beyond iron.\nPlanets.\nNeutron stars can host exoplanets. These can be original, circumbinary, captured, or the result of a second round of planet formation. Pulsars can also strip the atmosphere off from a star, leaving a planetary-mass remnant, which may be understood as a chthonian planet or a stellar object depending on interpretation. For pulsars, such pulsar planets can be detected with the pulsar timing method, which allows for high precision and detection of much smaller planets than with other methods. Two systems have been definitively confirmed. The first exoplanets ever to be detected were the three planets Draugr, Poltergeist and Phobetor around the pulsar Lich, discovered in 1992\u20131994. Of these, Draugr is the smallest exoplanet ever detected, at a mass of twice that of the Moon. Another system is PSR B1620\u221226, where a circumbinary planet orbits a neutron star-white dwarf binary system. Also, there are several unconfirmed candidates. Pulsar planets receive little visible light, but massive amounts of ionizing radiation and high-energy stellar wind, which makes them rather hostile environments to life as presently understood.\nHistory of discoveries.\nAt the meeting of the American Physical Society in December 1933 (the proceedings were published in January 1934), Walter Baade and Fritz Zwicky proposed the existence of neutron stars, less than two years after the discovery of the neutron by James Chadwick. In seeking an explanation for the origin of a supernova, they tentatively proposed that in supernova explosions ordinary stars are turned into stars that consist of extremely closely packed neutrons that they called neutron stars. Baade and Zwicky correctly proposed at that time that the release of the gravitational binding energy of the neutron stars powers the supernova: \"In the supernova process, mass in bulk is annihilated\". Neutron stars were thought to be too faint to be detectable and little work was done on them until November 1967, when Franco Pacini pointed out that if the neutron stars were spinning and had large magnetic fields, then electromagnetic waves would be emitted. Unknown to him, radio astronomer Antony Hewish and his graduate student Jocelyn Bell at Cambridge were shortly to detect radio pulses from stars that are now believed to be highly magnetized, rapidly spinning neutron stars, known as pulsars.\nIn 1965, Antony Hewish and Samuel Okoye discovered \"an unusual source of high radio brightness temperature in the Crab Nebula\". This source turned out to be the Crab Pulsar that resulted from the great supernova of 1054.\nIn 1967, Iosif Shklovsky examined the X-ray and optical observations of Scorpius X-1 and correctly concluded that the radiation comes from a neutron star at the stage of accretion.\nIn 1967, Jocelyn Bell Burnell and Antony Hewish discovered regular radio pulses from PSR B1919+21. This pulsar was later interpreted as an isolated, rotating neutron star. The energy source of the pulsar is the rotational energy of the neutron star. The majority of known neutron stars (about 2000, as of 2010) have been discovered as pulsars, emitting regular radio pulses.\nIn 1968, Richard V. E. Lovelace and collaborators discovered period formula_7 ms of the Crab Pulsar using Arecibo Observatory. After this discovery, scientists concluded that pulsars were rotating neutron stars. Before that, many scientists believed that pulsars were pulsating white dwarfs.\nIn 1971, Riccardo Giacconi, Herbert Gursky, Ed Kellogg, R. Levinson, E. Schreier, and H. Tananbaum discovered 4.8 second pulsations in an X-ray source in the constellation Centaurus, Cen X-3. They interpreted this as resulting from a rotating hot neutron star. The energy source is gravitational and results from a rain of gas falling onto the surface of the neutron star from a companion star or the interstellar medium.\nIn 1974, Antony Hewish was awarded the Nobel Prize in Physics \"for his decisive role in the discovery of pulsars\" without Jocelyn Bell who shared in the discovery.\nIn 1974, Joseph Taylor and Russell Hulse discovered the first binary pulsar, PSR B1913+16, which consists of two neutron stars (one seen as a pulsar) orbiting around their center of mass. Albert Einstein's general theory of relativity predicts that massive objects in short binary orbits should emit gravitational waves, and thus that their orbit should decay with time. This was indeed observed, precisely as general relativity predicts, and in 1993, Taylor and Hulse were awarded the Nobel Prize in Physics for this discovery.\nIn 1982, Don Backer and colleagues discovered the first millisecond pulsar, PSR B1937+21. This object spins 642 times per second, a value that placed fundamental constraints on the mass and radius of neutron stars. Many millisecond pulsars were later discovered, but PSR B1937+21 remained the fastest-spinning known pulsar for 24 years, until PSR J1748-2446ad (which spins ~716 times a second) was discovered.\nIn 2003, Marta Burgay and colleagues discovered the first double neutron star system where both components are detectable as pulsars, PSR J0737\u22123039. The discovery of this system allows a total of 5 different tests of general relativity, some of these with unprecedented precision.\nIn 2010, Paul Demorest and colleagues measured the mass of the millisecond pulsar PSR J1614\u22122230 to be M\u2609, using Shapiro delay. This was substantially higher than any previously measured neutron star mass (1.67\u00a0M\u2609, see PSR J1903+0327), and places strong constraints on the interior composition of neutron stars.\nIn 2013, John Antoniadis and colleagues measured the mass of PSR J0348+0432 to be M\u2609, using white dwarf spectroscopy. This confirmed the existence of such massive stars using a different method. Furthermore, this allowed, for the first time, a test of general relativity using such a massive neutron star.\nIn August 2017, LIGO and Virgo made first detection of gravitational waves produced by colliding neutron stars (GW170817), leading to further discoveries about neutron stars.\nIn October 2018, astronomers reported that GRB 150101B, a gamma-ray burst event detected in 2015, may be directly related to the historic GW170817 and associated with the merger of two neutron stars. The similarities between the two events, in terms of gamma ray, optical and x-ray emissions, as well as to the nature of the associated host galaxies, are \"striking\", suggesting the two separate events may both be the result of the merger of neutron stars, and both may be a kilonova, which may be more common in the universe than previously understood, according to the researchers.\nIn July 2019, astronomers reported that a new method to determine the Hubble constant, and resolve the discrepancy of earlier methods, has been proposed based on the mergers of pairs of neutron stars, following the detection of the neutron star merger of GW170817. Their measurement of the Hubble constant is (km/s)/Mpc.\nA 2020 study by University of Southampton PhD student Fabian Gittins suggested that surface irregularities (\"mountains\") may only be fractions of a millimeter tall (about 0.000003% of the neutron star's diameter), hundreds of times smaller than previously predicted, a result bearing implications for the non-detection of gravitational waves from spinning neutron stars.\nUsing the JWST, astronomers have identified a neutron star within the remnants of the Supernova 1987A stellar explosion after seeking to do so for 37 years, according to a 23 February 2024 \"Science\" article. In a paradigm shift, new JWST data provides the elusive direct confirmation of neutron stars within supernova remnants as well as a deeper understanding of the processes at play within SN 1987A's remnants.\nSubtypes.\nThere are a number of types of object that consist of or contain a neutron star:\nThere are also a number of theorized compact stars with similar properties that are not actually neutron stars.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21871", "revid": "41625025", "url": "https://en.wikipedia.org/wiki?curid=21871", "title": "Nassau, Bahamas", "text": "Capital and largest city of The Bahamas\nNassau ( ) is the capital and largest city of The Bahamas. It is on the island of New Providence, which has a population of 296,522, which is 74.26% of the country's population. Nassau is commonly defined as a primate city, dwarfing all other towns in the country. It is the centre of commerce, education, law, administration, and media of the country.\nLynden Pindling International Airport, the major airport for The Bahamas, is located about west of the city centre of Nassau, and has daily flights to and from major cities in Canada, the Caribbean, the United Kingdom and the United States. Nassau is the site of the House of Assembly and various judicial departments and was considered historically to be a stronghold of pirates. The city was named in honour of William III of England, Prince of Orange-Nassau.\nNassau's modern growth began in the late eighteenth century, with the influx of thousands of Loyalists and their slaves to The Bahamas following the American War of Independence. Many of them settled in Nassau and eventually came to outnumber the original inhabitants.\nAs the population of Nassau grew, so did its populated areas. Today, the city dominates the entire island. However, until the post-Second World War era, the outer suburbs scarcely existed. Most of New Providence was uncultivated bush until Loyalists were resettled there following the American Revolutionary War; they established several plantations, such as Clifton and Tusculum. Slaves were imported as labour.\nAfter the British abolished the international slave trade in 1807, they resettled thousands of Africans liberated from slave ships by the Royal Navy on New Providence (at Adelaide Village and Gambier Village), along with other islands such as Grand Bahama, Exuma, Abaco and Inagua. In addition, slaves freed from American ships, such as the slave ship Creole in November 1841, were allowed to settle there. The largest concentration of Africans historically lived in the \"Over-the-Hill\" suburbs of Grants Town and Bain Town to the south of the city of Nassau, while most of the inhabitants of European descent lived on the island's northern coastal ridges.\nHistory.\nThe town that would be called Nassau was founded in 1670 by British noblemen who brought British settlers with them to New Providence. They built a fort, and named it Charles Town in honour of England's King Charles II. During this time there were frequent wars with the Spanish, and Charles Town was used as a base for privateering against them. In 1684 the town was burned to the ground during the Raid on Charles Town. It was rebuilt in 1695 under Governor Nicholas Trott and renamed Nassau in honour of King William III, who belonged to a branch of the House of Nassau. William was the Dutch Stadtholder (\"stadhouder\" in Dutch), and, from 1689, the King of England, Scotland and Ireland. The name Nassau ultimately derives from the town of Nassau in Germany.\nLacking effective governors after Trott, Nassau fell on hard times. In 1703, Spanish and French allied forces briefly occupied Nassau. More so, Nassau suffered greatly during the War of Spanish Succession and witnessed Spanish incursions during 1703, 1704 and 1706. From 1703 to 1718, there was no legitimate governor in the colony. Thomas Walker was the island's last remaining appointed official and although evidence is scarce, it appears that he was acting in the role of deputy governor upon Benjamin Hornigold's arrival in 1713. By this time, the sparsely settled Bahamas had become a pirate haven known as New Providence. The Governor of Bermuda stated that there were over 1,000 pirates in Nassau and that they outnumbered the mere hundred inhabitants of the town. They proclaimed Nassau a pirate republic, recognising the island's prosperous state in which it offered fresh fruit, meat and water and plenty of protection amid its waterways. Nassau's harbour was tailor-made for defence and it could take around 500 vessels, though it was too shallow to accept large battleships. Benjamin Hornigold, along with his great rival Henry Jennings, became the unofficial overlord of a veritable pirate republic which played host to the self-styled Flying Gang. Other pirates of note that spent time in Nassau included Charles Vane, Thomas Barrow (who declared himself \"Governor of New Providence\"), John Rackham, Anne Bonny, Mary Read, and the infamous Edward Teach, better known as \"Blackbeard\".\nIn 1718, the British government sought to regain control of the islands and appointed Captain Woodes Rogers as Royal governor. He successfully clamped down on the pirates, reformed the civil administration, and restored commerce. Rogers cleaned up Nassau and rebuilt the fort, using his own wealth to try to overcome problems. In 1720, the Spanish attacked Nassau but failed to capture the town and the island.\nDuring the wars in the Thirteen Colonies, Nassau experienced an economic boom. With funds from privateering, a new fort, street lights and over 2300 sumptuous houses were built and Nassau was extended. In addition to this, mosquito breeding swamps were filled.\nIn 1776, the Battle of Nassau resulted in a brief occupation by American Continental Marines during the American War of Independence, where the marines staged their first amphibious raid on Fort Montague after attempting to sneak up on Fort Nassau. In 1778 after an overnight invasion, American raiders led by Captain Rathbun, left with ships, gunpowder and military stores after stopping in Nassau for only two weeks. In 1782, Spain captured Nassau for the last time when Don Juan de Cagigal, governor-general of Cuba, attacked New Providence with 5,000 men. In April 1783, Andrew Deveaux, an American Loyalist who resettled on the island, set forth and recaptured the island for the British Crown with just 220 men and 150 muskets to face a force of 600 trained soldiers.\nLord Dunmore governed the colony from 1787 to 1796. He oversaw the construction of Fort Charlotte and Fort Fincastle in Nassau.\nDuring the American Civil War, Nassau served as a port for blockade runners making their way to and from ports along the southern Atlantic Coast for continued trade with the Confederacy.\nIn the 1920s and 1930s, Nassau profited from Prohibition in the United States.\nDuring the Cuban Revolution of 1959, tourism further benefited due to the restrictions imposed on American citizens visiting Cuba. Today, Nassau's location close to the United States and ties between the U.S. and The Bahamas make it a common tourist destination.\nGeography.\nLocated on New Providence Island, Nassau's harbour has a blend of old world and colonial architecture, and a busy port. The tropical climate and natural environment of The Bahamas have made Nassau an attractive tourist destination.\nNassau developed directly behind the port area. New Providence provides 200\u00a0km2 of relatively flat and low-lying land intersected by low ridges (none of which restricted settlement). In the centre of the island there are several shallow lakes that are tidally connected.\nThe city's proximity to the United States (290\u00a0km (181 miles) east-southeast of Miami, Florida) has contributed to its popularity as a holiday resort, especially after the United States imposed a ban on travel to Cuba in 1963. The Atlantis resort on nearby Paradise Island accounts for more tourist arrivals to the city than any other hotel property of Nassau. The mega-resort employs over 6,000 Bahamians, and is the largest employer outside of the government.\nClimate.\nNassau has a tropical monsoon climate (K\u00f6ppen: \"Am\"), bordering on a tropical savanna climate (K\u00f6ppen: \"Aw\"), with hot wet summers, and mild dry winters. Temperatures are relatively consistent throughout the course of the year. During the wet season from May through October, average daytime high temperatures are , while during the dry season from November through April daytime temperatures are between , rarely falling below .\nUrban development.\nDuring the 19th century, Nassau became urbanized, attracting rural residents. Growth since the 1950s has been outwards from the town. The 1788 heart of Nassau was just a few blocks of buildings between Government House and the harbour, but the town gradually expanded east to Malcolm's Park, south to Wulff Road, and west to Nassau Street. Grants Town and Bain Town south of the city became the main residential areas for those of African descent, and until about 30 years ago was the most populous part of the city.\nThose of European descent built houses along the shore, east as far as Fort Montagu, west as far as Saunders Beach, and along the ridge edging the city. During the 20th century, the city spread east to Village Road and west to Fort Charlotte and Oakes Field. This semicircle of residential development was the main area of settlement until after the Second World War, and marks a distinct phase in the city's expansion, the outer boundary to this zone being the effective limit of the continuous built-up area. The wealthier residents continued to spread east (to East End Point) and West (to Lyford Cay).\nIn the last 40 years, residential development has been quite different. It has consisted mainly of planned middle-income sub-divisions. Since the 1960s, government has sponsored low-cost housing developments at Yellow Elder, Elizabeth Estates, and Pinewood Gardens, in the outer ring.\nCity centre.\nThe city centre is the hub for all activities in Nassau. Thousands of people visit daily, to shop, dine, sightsee and to enjoy the tropical climate of the city. While the busiest part of central city is the Bay Street thoroughfare and the Woodes Rogers Walk, located across the street from the port and parallel to Bay, the area extends for several blocks in each direction. It starts at West Bay, around the Junkanoo Beach area. A few hotels and restaurants are located on West Bay.\nThe next landmark is the British Colonial Hotel, which marks the beginning of Bay Street proper. Pirates of Nassau Museum is just across from the British Colonial Hilton. The next few blocks of Bay Street are wall-to-wall boutiques, with a few restaurants and clubs interspersed throughout the retailers.\nHistorical landmarks are also in the vicinity, including Vendue House, Christ Church Cathedral, and the Nassau Public Library. Although the tourist part of the city centre peters out after about seven blocks, smaller, more local shops are located down Bay Street. At this point, Bay Street becomes East Bay.\nThe Straw Market is a tourist destination in the city centre. A new market was opened in 2011 after a fire in 2001 destroyed the original Fish, Vegetable and Straw Market. The market is open on all sides, and contains a number of Bahamian craft stores.\nCable Beach.\nCable Beach is recognized as the hotel district of Nassau. Five hotels\u2014two of which are all-inclusive\u2014are located on this strip. The area is also known for its dining, with most of the area's restaurants in the hotels or across the street. There is a bit of shopping, most of it in the Wyndham and at Baha Mar. In 2017, the development of Baha Mar, a luxury resort and casino, brought more than 2,000 hotel rooms and the largest gaming and convention facility in the Caribbean to this section of New Providence Island.\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nNassau had a population of 128,420 females and 117,909 males and was home to 70,222 households with an average family size of 3.5 according to the 2010 census. Nassau's large population in relation to the remainder of The Bahamas is the result of waves of immigration from the Family Islands to the capital. Consequently, this has led to the decline in the population of the lesser developed islands and the rapid growth of Nassau.\nPublic safety.\nIn January 2018, the U.S. Department of State issued the latest in a series of travel advisories due to violent crime.\nTransport.\nAir.\nLynden Pindling International Airport (formerly Nassau International Airport) is on the western side of Nassau. New Providence Airport on Paradise Island was closed in 1999 with runway removed and integrated into the resort on the island.\nWater.\nFerries (boats) provide water travel around Nassau to the surrounding islands, namely Paradise Island.\nPrince George Wharf is a seaport, the main port in the city, that serves cruise ships with ports of call in Nassau.\nTransportation and shipping around the Family Islands is primarily through mailboats based at Potters Cay. International shipping is done through the Arawak Port Department on Arawak Cay. High speed excursions to Exuma, Spanish Wells and Harbour Island are available daily.\nRoads.\nPublic jitney buses and taxis provide transport in and around Nassau. Rental cars are also available in the city and at the airport.\nMajor roads in Nassau include:\nThe major road in Nassau is Bay Street for tourists. Bay Street runs the entire length of the Island from East to West. Bay Street also provides beachfront views. The downtown area and the cruise ships are in walking distance.\nThe Bahamas is a left-hand traffic country, but many cars are imported from the US and are left-hand drive.\nCulture.\nUNESCO Creative Cities Network.\nNassau has been recognized as a part of the UNESCO Creative Cities Network as a city of Crafts and Folk Art. It is one of only three Caribbean cities to receive this honour.\nJunkanoo.\nThe city's chief festival is Junkanoo, an energetic, colourful street parade of brightly costumed people dancing to the rhythmic accompaniment of cowbells, drums and whistles. The word 'Junkanoo' is derived from the name of the founder: John Canoe. The celebration occurs on 26 December, 10 July and 1 January, beginning in the early hours of the morning (1:00\u00a0a.m.) and ending around 10 a.m. At the end of the Junkanoo procession, judges award cash prizes for the best music, costumes, and overall group presentation. Participants spend all year preparing their handmade costumes by using coloured crepe paper and cardboard.\nIn popular culture.\nNassau was the main setting for the Starz Network show \"Black Sails\" (2014\u20132017). However, filming was based in South Africa.\nNassau was featured as an important setting in several movies, including the Beatles film \"Help!\" and the James Bond films \"Thunderball\" (1965), \"Never Say Never Again\" (1983), and \"Casino Royale\" (2006). Nassau has also served as a shooting location for film production. In 1981, Nassau was used as a stand-in for Greece in an ocean scene in \"For Your Eyes Only\".\nSeveral other late-20th- and 21st-century movies have been set here, including \"After the Sunset\" (2004), \"Into the Blue\" (2005), and \"Flipper\" (1996).\nIt hosted the Miss Universe 2009 pageant.\nNassau was featured as a primary setting in the 2013 video game \"\" (2013).\nNassau Town is mentioned in \"Sloop John B\", a Bahamian folk song. Since the early 1950s there have been many recordings of the song, the best known being by The Beach Boys on their \"Pet Sounds\" album.\nNassau is the subject of \"Funky Nassau,\" a song written by Ray Munnings and Tyrone Fitzgerald and recorded by the Nassau-based funk band The Beginning of the End in 1971 as the single from their album of the same name. The song reached #7 on the US R&amp;B chart, #15 on the \"Billboard\" Hot 100, and #31 on the UK Singles Chart in 1971.\nTwin towns \u2013 sister cities.\nNassau's sister cities are:\nInfrastructure.\nNassau is home to 6 hospitals or medical clinics:\nNassau is also home to the Nassau Container Port, which is located on Arawak Cay.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21873", "revid": "35751661", "url": "https://en.wikipedia.org/wiki?curid=21873", "title": "Nastassja Kinski", "text": "German actress (born 1961)\nNastassja Aglaia Kinski (; n\u00e9e Nakszynski, ; born 24 January 1961) is a German actress and former model who has appeared in more than 60 films in Europe and the United States. Her worldwide breakthrough was with \"Stay as You Are\" (1978). She then came to global prominence with her Golden Globe Award-winning performance as the title character in the Roman Polanski-directed film \"Tess\" (1979). Other films in which she acted include the Francis Ford Coppola musical romance film \"One from the Heart\" (1982), erotic horror film \"Cat People\" (1982) from Paul Schrader, and the Wim Wenders drama films \"Paris, Texas\" (1984) and \"Faraway, So Close!\" (1993). She also appeared in the biographical drama film \"An American Rhapsody\" (2001). She is the daughter of German actor Klaus Kinski.\nEarly life.\nBorn Nastassja Aglaia Nakszynski in West Berlin. She is the daughter of German actor Klaus Kinski and his second wife, actress Ruth Brigitte Tocki. She is of partial Polish descent, for her grandfather Bruno Nakszynski was a Germanized ethnic Pole. Kinski has two half-siblings: Pola and Nikolai Kinski. Her parents divorced in 1968. After the age of 10, Kinski rarely saw her father. Her young mother struggled financially to support them; they eventually lived in a commune in Munich.\nIn a 1999 interview, Kinski denied that her father had molested her as a child, but said he had abused her \"in other ways\". In 2013, when interviewed about the allegations of sexual abuse made by her half-sister Pola Kinski, she confirmed that he attempted this with her, but did not succeed. She said, \"He was no father. Ninety-nine percent of the time I was terrified of him. He was so unpredictable that the family lived in constant terror.\" When asked what she would say to him now, if she had the chance, she replied, \"I would do anything to put him behind bars for life. I am glad he is no longer alive.\"\nCareer.\nKinski began working as a model as a teenager in Germany. Actress Lisa Kreuzer of the German New Wave helped get her the role of the mute Mignon in Wim Wenders 1975 film \"The Wrong Move\", in which at the age of 14 she was depicted topless. She later played one of the leading roles in Wenders' film \"Paris, Texas\" (1984) and appeared in his film \"Faraway, So Close\" (1993).\nIn 1976, while still a teenager, Kinski had her first two major roles: in Wolfgang Petersen's feature film-length episode \"Reifezeugnis\" of the German TV crime series \"Tatort.\" Next, she appeared in the British horror film \"To the Devil a Daughter\" (1976), produced by Hammer Film Productions, which was released in the UK just 40 days after Kinski's fifteenth birthday, making it a virtual certainty she was only fourteen when her scenes were shot (including full frontal nudity). In regards to her early films, Kinski has stated that she felt exploited by the industry. In an interview with \"W\", she said, \"If I had had somebody to protect me or if I had felt more secure about myself, I would not have accepted certain things. Nudity things. And inside it was just tearing me apart.\"\nIn 1978, Kinski starred in the Italian romance \"Stay as You Are\" (\"Cos\u00ec come sei\") with Marcello Mastroianni, gaining her recognition in the United States after New Line Cinema released it there in December 1979. \"Time\" wrote that she was \"simply ravishing, genuinely sexy and high-spirited without being painfully aggressive about it.\" The film also received a major international release from Columbia Pictures.\nKinski met the director Roman Polanski at a party in 1976. He urged her to study method acting with Lee Strasberg in the United States and she was offered the title role in Polanski's upcoming film, \"Tess\" (1979). In 1978, Kinski underwent extensive preparation for the portrayal of an English peasant girl, which included acquiring a Dorset accent through elocution studies:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I was given the book almost a year prior to read, I then had to transform myself and lose my German accent completely. I worked with a coach from the National Theatre in London, Kate Fleming. It was almost an intellectual voyage. [...] I went to live in the countryside of the deep part of England, on a farm, did everything they did, and learned it. When the time came in Paris to do my test, it was with our director and our producers Claude Berri and Timothy Burrill, I had done a screen test with Roman prior to that, for Dino DeLaurentis, but now this was for \"Tess\". Preparation is an amazing thing. It, somehow, after all the work, carries you if you are fully present, it carries you through like a bird, like big inner and outer wings.\n'Tess' was nominated for six awards, including Best Picture, at the 53rd Academy Awards, and won three.\nOn 14 June 1981, Vogue editor Polly Allen Mellen asked Nastassja Kinski what she liked and Kinski replied,&lt;ref name=\"anothermag/nastassja-boa\"&gt;&lt;/ref&gt; \"snakes,\"&lt;ref name=\"AD/avedon-photos\"&gt;&lt;/ref&gt;&lt;ref name=\"RN/lady-snakes-3\"&gt;&lt;/ref&gt; then a Burmese python was hired for the photoshoot with Richard Avedon, with resulting photograph of Kinski with a Burmese python&lt;ref name=\"\u0152il/kinski-serpent\"&gt;&lt;/ref&gt; coiled&lt;ref name=\"phillips/UK040222/65\"&gt;&lt;/ref&gt; around her nude body, \"Nastassja Kinski and the Serpent, Los Angeles, California, June 14, 1981\".&lt;ref name=\"christies/6466317\"&gt;&lt;/ref&gt;&lt;ref name=\"artland/serpent\"&gt;&lt;/ref&gt; The image, which first appeared in the October 1981 issue of US \"Vogue\", was released as a poster and became a best-seller, further confirming her status as a sex symbol.\nIn 1982, she starred in Francis Ford Coppola's romantic musical \"One from the Heart\", her first film made in the United States. \"Texas Monthly\" described her as acting \"as a Felliniesque circus performer to represent the twinkling evanescence of Eros.\" The film failed at the box office and was a major loss for Coppola's new Zoetrope Studios. That year, she was also in the erotic supernatural horror movie \"Cat People\". On 29 December 1982, Kinski made a puzzling appearance on the program \"Late Night with David Letterman\", seeming somewhat oblivious to the jokes and everything else that was going on around her and appearing with an unusual hair style Letterman described as \"looking like there was an owl perched on top of her head.\" (Letterman's second guest, John Candy, came out with his own hair moussed up in a pile as a spoof of Kinski's hair.)\nDudley Moore's comedy \"Unfaithfully Yours\" and an adaptation of John Irving's \"The Hotel New Hampshire\" followed in 1984.\nKinski reteamed with Wenders for the 1984 film \"Paris, Texas\". One of her most acclaimed films to date, it won the top award at the Cannes Film Festival. Throughout the 1980s, Kinski split her time between Europe and the United States, making \"Moon in the Gutter\" (1983), \"Harem\" (1985) and \"Torrents of Spring\" (1989) in Europe, and \"Exposed\" (1983), \"Maria's Lovers\" (1984), and \"Revolution\" (1985) in the United States.\nDuring the 1990s, Kinski appeared in a number of American films, including the action movie \"Terminal Velocity\" opposite Charlie Sheen, the Mike Figgis 1997 adultery tale \"One Night Stand\", \"Your Friends &amp; Neighbors\" (1998), John Landis's \"Susan's Plan\" (1998), and \"The Lost Son\" (1999).\nHer most recent films include David Lynch's \"Inland Empire\" (2006) and Rotimi Rainwater's \"Sugar\" (2013). In 2016, she competed in the German \"Let's Dance\" show.\nPersonal life.\nIn 1976, when Kinski was aged 15, it was speculated that there had been a romantic relationship with director Roman Polanski, who at the time was 43. Polanski confirmed the relationship in a 1994 interview with Diane Sawyer: \"...what about Nastassja Kinski? She was young and we had a love affair.\" However, in a 1999 interview in \"The Guardian\", Kinski was quoted as saying that there was no affair and that \"there was a flirtation. There could have been a seduction, but there was not. He had respect for me.\"\nKinski has three children from different relationships. Her first child, son Aljosha Nakszynski (born 29 June 1984), was fathered by actor Vincent Spano, her co-star in \"Maria's Lovers\".&lt;ref name=\"atw/nastassjakinsk\"&gt;&lt;/ref&gt; On 10 September 1984, Kinski married Egyptian filmmaker Ibrahim Moussa, with whom she had daughter Sonja Kinski (born 2 March 1986). The marriage was dissolved in July 1992. From 1992 until 1995, Kinski lived with musician Quincy Jones, though she kept her own apartment on Hilgard Avenue, near UCLA, at the time. They had a daughter, Kenya Julia Niambi Sarah Jones (born 9 February 1993), a model known professionally as Kenya Kinski-Jones.\nIn 1997, Kinski dated married producer Jonathan D. Krane during a brief separation from his wife, actress Sally Kellerman. Over the course of her career, Kinski has also been romantically linked with Paul Schrader, Jean-Jacques Beineix, Rob Lowe, Jon Voight, G\u00e9rard Depardieu, Dudley Moore, Milo\u0161 Forman and Wim Wenders. As of 2012, she was dating actor Rick Yune.\nIn 2001, Kinski stated in an interview in \"The Daily Telegraph\" that she was affected by the sleep disorder narcolepsy.\nAwards and nominations.\nThe awards and nominations received by Nastassja Kinski include one Art Film Fest Award, one Bambi Award, two Bravo Ottos (out of three nominations), two Deutscher Filmpreis Awards (also out of three nominations), one Golden Globe (out of two nominations), one Jupiter Award, one Nastro d'Argento Award and one Wine Country Film Festival Award.\nAmong others, her achievements in film industry include also two C\u00e9sar Awards nominations, one Globo d'oro nomination, and one Saturn Award nomination.\nActing awards.\nBambi Awards\nBravo Otto Awards\nC\u00e9sar Awards\nDeutscher Filmpreis Awards\nGlobo d'oro Awards\nGolden Globe Awards\nJupiter Awards\nNastro d'Argento Awards\nSaturn Awards\nWine Country Film Festival Awards\nCareer achievement awards.\nArt Film Fest Awards\nMoscow International Film Festival\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nMetadata"}
{"id": "21875", "revid": "30585864", "url": "https://en.wikipedia.org/wiki?curid=21875", "title": "Nuremberg trials", "text": "Trials of Nazi German leaders\nThe Nuremberg trials were international criminal trials held by France, the Soviet Union, the United Kingdom, and the United States against leaders of the defeated Nazi Germany for plotting and carrying out invasions of several countries across Europe and committing atrocities against their citizens in the Second World War.\nBetween 1939 and 1945, Nazi Germany invaded many countries across Europe, inflicting 27 million deaths in the Soviet Union alone. Proposals for how to punish the defeated Nazi leaders ranged from a show trial (the Soviet Union) to summary executions (the United Kingdom). In mid-1945, France, the Soviet Union, the United Kingdom, and the United States agreed to convene a joint tribunal in Nuremberg, occupied Germany, with the Nuremberg Charter as its legal instrument. Between 20 November 1945 and 1 October 1946, the International Military Tribunal (IMT) tried 22 of the most important surviving leaders of Nazi Germany in the political, military, and economic spheres, as well as six German organizations. The purpose of the trial was not only to try the defendants but also to assemble irrefutable evidence of Nazi war crimes, offer a history lesson to the defeated Germans, and delegitimize the traditional German elite.\nThe IMT verdict followed the prosecution in declaring the crime of plotting and waging aggressive war \"the supreme international crime\" because \"it contains within itself the accumulated evil of the whole\". Most defendants were also charged with war crimes and crimes against humanity, the Holocaust significantly contributing to the trials. Twelve further trials were conducted by the United States against lower-level perpetrators and focused more on the Holocaust. Controversial at the time for their retroactive criminalization of aggression, the trials' innovation of holding individuals responsible for violations of international law is considered \"the true beginning of international criminal law\".\nOrigin.\nBetween 1939 and 1945, Nazi Germany invaded many European countries, including Poland, Denmark, Norway, the Netherlands, Belgium, Luxembourg, France, Yugoslavia, Greece, and the Soviet Union. German aggression was accompanied by immense brutality in occupied areas; war losses in the Soviet Union alone included 27 million dead, mostly civilians, which was one seventh of the prewar population. The legal reckoning was premised on the extraordinary nature of Nazi criminality, particularly the perceived singularity of the systematic murder of millions of Jews.\nIn early 1942, representatives of nine governments-in-exile from German-occupied Europe issued a declaration to demand an international court to try the German crimes committed in occupied countries. The United States and United Kingdom refused to endorse this proposal, citing the failure of war crimes prosecutions following World War I. The London-based United Nations War Crimes Commission\u2014without Soviet participation\u2014first met in October 1943 and became bogged down in the scope of its mandate, with Belgian jurist Marcel de Baer and Czech legal scholar Bohuslav E\u010der arguing for a broader definition of war crimes that would include \"the crime of war\". On 1 November 1943, the Soviet Union, United Kingdom, and United States issued the Moscow Declaration, warning Nazi leadership of the signatories' intent to \"pursue them to the uttermost ends of the earth ... in order that justice may be done\". The declaration stated high-ranking Nazis who had committed crimes in several countries would be dealt with jointly, while others would be tried where they had committed their crimes.\nSoviet jurist Aron Trainin developed the concept of crimes against peace (waging aggressive war) which would later be central to the proceedings at Nuremberg. Trainin's ideas were reprinted in the West and widely adopted. Of all the Allies, the Soviet Union lobbied most intensely for trying the defeated German leaders for aggression in addition to war crimes. The Soviet Union wanted to hold a trial with a predetermined outcome similar to the 1930s Moscow trials, in order to demonstrate the Nazi leaders' guilt and build a case for war reparations to rebuild the Soviet economy, which had been devastated by the war. The United States insisted on a trial that would be seen as legitimate as a means of reforming Germany and demonstrating the superiority of the Western system. The United States Department of War was drawing up plans for an international tribunal in late 1944 and early 1945. The British government still preferred the summary execution of Nazi leaders, citing the failure of trials following World War I and qualms about retroactive criminality. The form that retribution would take was left unresolved at the Yalta Conference in February 1945. On 2 May, at the San Francisco Conference, United States president Harry S. Truman announced the formation of an international military tribunal. On 8 May, Germany surrendered unconditionally, bringing an end to the war in Europe.\nEstablishment.\nNuremberg charter.\nAt the London Conference, held from 26 June to 2 August 1945, representatives of France, the Soviet Union, the United Kingdom, and the United States negotiated the form that the trial would take. Until the end of the negotiations, it was not clear that any trial would be held at all.\nThe offences that would be prosecuted were crimes against peace, crimes against humanity, and war crimes. At the conference, it was debated whether wars of aggression were prohibited in existing customary international law; regardless, before the charter was adopted there was no law providing for criminal responsibility for aggression. Despite misgivings from other Allies, American negotiator and Supreme Court justice Robert H. Jackson threatened the United States' withdrawal if aggression was not prosecuted because it had been the rationale for American entry into World War II. However, Jackson conceded on defining crimes against peace; the other three Allies were opposed because it would undermine the freedom of action of the United Nations Security Council.\nWar crimes already existed in international law as criminal violations of the laws and customs of war, but these did not apply to a government's treatment of its own citizens. Legal experts sought a way to try crimes against German citizens, such as the German Jews. A Soviet proposal for a charge of \"crimes against civilians\" was renamed \"crimes against humanity\" at Jackson's suggestion after previous uses of the term in the post-World War I Commission of Responsibilities and in failed efforts to prosecute the perpetrators of the Armenian genocide. The British proposal to define crimes against humanity was largely accepted, with the final wording being \"murder, extermination, enslavement, deportation, and other inhumane acts committed against any civilian population\". The final version of the charter limited the tribunal's jurisdiction over crimes against humanity to those committed as part of a war of aggression. Both the United States (concerned that its Jim Crow system of racial segregation not be labeled a crime against humanity) and the Soviet Union wanted to avoid giving an international court jurisdiction over a government's treatment of its own citizens.\nThe charter upended the traditional view of international law by holding individuals, rather than states, responsible for breaches. The other three Allies' proposal to limit the definition of the crimes to acts committed by the defeated Axis was rejected by Jackson. Instead, the charter limited the jurisdiction of the court to Germany's actions. Article 7 prevented the defendants from claiming sovereign immunity, and Article 8 meant that the plea of acting under superior orders was not a valid defence, although it might be treated in mitigation. The trial was held under modified common law. The negotiators decided that the tribunal's permanent seat would be in Berlin, while the trial would be held at the Palace of Justice in Nuremberg. Located in the American occupation zone, Nuremberg was a symbolic location as the site of Nazi rallies. The Palace of Justice was relatively intact but needed to be renovated for the trial due to bomb damage; it had an attached prison where the defendants could be held. On 8 August, the Nuremberg Charter was signed in London.\nJudges and prosecutors.\nIn early 1946, there were a thousand employees from the four countries' delegations in Nuremberg, of which about two thirds were from the United States. Besides legal professionals, there were many social-science researchers, psychologists, translators, interpreters, and graphic designers, the last to make the many charts used during the trial. Each state appointed a prosecution team and two judges, one being a deputy without voting rights.\nJackson (whom historian Kim Christian Priemel described as \"a versatile politician and a remarkable orator, if not a great legal thinker\") was appointed the United States' chief prosecutor. The United States prosecution believed Nazism was the product of a German deviation from the West (the \"Sonderweg\" thesis) and sought to correct this deviation with a trial that would serve both retributive and educational purposes. As the largest delegation, it would take on the bulk of the prosecutorial effort. At Jackson's recommendation, the United States appointed judges Francis Biddle and John Parker. The British chief prosecutor was Hartley Shawcross, Attorney General for England and Wales, assisted by his predecessor David Maxwell Fyfe. Although the chief British judge, Sir Geoffrey Lawrence (Lord Justice of Appeal), was the nominal president of the tribunal, in practice Biddle exercised more authority.\nThe French prosecutor, Fran\u00e7ois de Menthon, had just overseen trials of the leaders of Vichy France; he resigned in January 1946 and was replaced by Auguste Champetier de Ribes. The French judges were Henri Donnedieu de Vabres, a professor of criminal law, and deputy Robert Falco, a judge of the Cour de Cassation who had represented France at the London Conference. The French government tried to appoint staff untainted by collaboration with the Vichy regime; some appointments, including Champetier de Ribes, were of those who had been in the French resistance. Expecting a show trial, the Soviet Union initially appointed as chief prosecutor Iona Nikitchenko, who had presided over the Moscow trials, but he was made a judge and replaced by Roman Rudenko, a show trial prosecutor chosen for his skill as an orator. The Soviet judges and prosecutors were not permitted to make any major decisions without consulting a commission in Moscow led by Soviet politician Andrei Vyshinsky; the resulting delays hampered the Soviet effort to set the agenda. The influence of the Soviet delegation was also constrained by limited English proficiency, lack of interpreters, and unfamiliarity with diplomacy and international institutions.\nRequests by Chaim Weizmann, the president of the World Zionist Organization, as well as the Provisional Government of National Unity in Poland, for an active role in the trial justified by their representation of victims of Nazi crimes were rejected. The Soviet Union invited prosecutors from its allies, including Poland, Czechoslovakia, and Yugoslavia; Denmark and Norway also sent a delegation. Although the Polish delegation was not empowered to intervene in the proceedings, it submitted evidence and an indictment, succeeding at drawing some attention to crimes committed against Polish Jews and non-Jews.\nIndictment.\nThe work of drafting the indictment was divided up by the national delegations. The British worked on aggressive war; the other delegations were assigned the task of covering crimes against humanity and war crimes committed on the Western Front (France) and the Eastern Front (the Soviet Union). The United States delegation outlined the overall Nazi conspiracy and criminality of Nazi organizations. The British and American delegations decided to work jointly in drafting the charges of conspiracy to wage aggressive war. On 17 September, the various delegations met to discuss the indictment.\nThe charge of conspiracy, absent from the charter, held together the wide array of charges and defendants and was used to charge the top Nazi leaders, as well as bureaucrats who had never killed anyone or perhaps even directly ordered killing. It was also an end run on the charter's limits on charging crimes committed before the beginning of World War II. Conspiracy charges were central to the cases against propagandists and industrialists: the former were charged with providing the ideological justification for war and other crimes, while the latter were accused of enabling Germany's war effort. The charge, a brainchild of War Department lawyer Murray C. Bernays, and perhaps inspired by his previous work prosecuting securities fraud, was spearheaded by the United States and less popular with the other delegations, particularly France.\nThe problem of translating the indictment and evidence into the three official languages of the tribunal\u2014English, French, and Russian\u2014as well as German was severe due to the scale of the task and difficulty of recruiting interpreters, especially in the Soviet Union. Vyshinsky demanded extensive corrections to the charges of crimes against peace, especially regarding the role of the German\u2013Soviet pact in starting World War II. Jackson also separated out an overall conspiracy charge from the other three charges, aiming that the American prosecution would cover the overall Nazi conspiracy while the other delegations would flesh out the details of Nazi crimes. The division of labor, and the haste with which the indictment was prepared, resulted in duplication, imprecise language, and lack of attribution of specific charges to individual defendants.\nDefendants.\nSome of the most prominent Nazis\u2014Adolf Hitler, Heinrich Himmler, and Joseph Goebbels\u2014had died by suicide and therefore could not be tried. The prosecutors aimed to prosecute key leaders in German politics, business, and the military. Most of the defendants had surrendered to the United States or United Kingdom.\nThe defendants, who were largely unrepentant, included former cabinet ministers: Franz von Papen (who had brought Hitler to power), Joachim von Ribbentrop (foreign minister), Konstantin von Neurath (foreign minister), Wilhelm Frick (interior minister), and Alfred Rosenberg, minister for the occupied eastern territories. Also prosecuted were leaders of the German economy, such as Gustav Krupp of the Krupp AG conglomerate, former Reichsbank president Hjalmar Schacht, and economic planners Albert Speer and Walther Funk, along with Speer's subordinate and head of the forced labor program, Fritz Sauckel. While the British were skeptical of prosecuting economic leaders, the French had a strong interest in highlighting German economic imperialism. The military leaders were Hermann G\u00f6ring\u2014the most infamous surviving Nazi and the main target of the trial\u2014Wilhelm Keitel, Alfred Jodl, Erich Raeder, and Karl D\u00f6nitz. Also on trial were propagandists Julius Streicher and Hans Fritzsche; Rudolf Hess, Hitler's deputy who had flown to Britain in 1941; Hans Frank, governor-general of the General Governorate of Poland; Hitler Youth leader Baldur von Schirach; Arthur Seyss-Inquart, Reich Commissioner for the Netherlands; and Ernst Kaltenbrunner, leader of Himmler's Reich Security Main Office. Observers of the trial found the defendants mediocre and contemptible.\nAlthough the list of defendants was finalized on 29 August, as late as October, Jackson demanded the addition of new names, but was denied. Of the 24 men indicted, Martin Bormann was tried \"in absentia\", as the Allies were unaware of his death; Krupp was too ill to stand trial; and Robert Ley had died by suicide before the start of the trial. Former Nazis were allowed to serve as counsel and by mid-November all defendants had lawyers. The defendants' lawyers jointly appealed to the court, claiming it did not have jurisdiction against the accused, but this motion was rejected. Defense lawyers saw themselves as acting on behalf of their clients and the German nation.\nInitially, the Americans had planned to try fourteen organizations and their leaders, but this was narrowed to six: the Reich Cabinet, the Leadership Corps of the Nazi Party, the Gestapo, the SA, the SS and the SD, and the General Staff and High Command of the German military (Wehrmacht). The aim was to have these organizations declared criminal, so that their members could be tried expeditiously for membership in a criminal organization. Senior American officials believed that convicting organizations was a good way of showing that not just the top German leaders were responsible for crimes, without condemning the entire German people.\nEvidence.\nOver the summer, all of the national delegations struggled to gather evidence for the upcoming trial. The American and British prosecutors focused on documentary evidence and affidavits rather than testimony from survivors. This strategy increased the credibility of their case, since survivor testimony was considered less reliable and more vulnerable to accusations of bias, but reduced public interest in the proceedings. The American prosecution drew on reports of the Office of Strategic Services, an American intelligence agency, and information provided by the YIVO Institute for Jewish Research and the American Jewish Committee, while the French prosecution presented many documents that it had obtained from the Center of Contemporary Jewish Documentation. The prosecution called 37 witnesses compared to the defense's 83, not including 19 defendants who testified on their own behalf. The prosecution examined 110,000 captured German documents and entered 4,600 into evidence, along with of film and 25,000 photographs.\nThe charter allowed the admissibility of any evidence deemed to have probative value, including depositions. Because of the loose evidentiary rules, photographs, charts, maps, and films played an important role in making incredible crimes believable. After the American prosecution submitted many documents at the beginning of the trial, the judges insisted that all of the evidence be read into the record, which slowed the trial. The structure of the charges also caused delays as the same evidence ended up being read out multiple times, when it was relevant to both conspiracy and the other charges.\nCourse of the trial.\nThe International Military Tribunal began trial on 20 November 1945, after postponement requests from the Soviet prosecution, who wanted more time to prepare its case, were rejected. All defendants pleaded not guilty. Jackson made clear that the trial's purpose extended beyond convicting the defendants. Prosecutors wanted to assemble irrefutable evidence of Nazi crimes, establish individual responsibility and the crime of aggression in international law, provide a history lesson to the defeated Germans, delegitimize the traditional German elite, and allow the Allies to distance themselves from appeasement. Jackson maintained that while the United States did \"not seek to convict the whole German people of crime\", neither did the trial \"serve to absolve the whole German people except 21 men in the dock\". Nevertheless, defense lawyers (although not most of the defendants) often argued that the prosecution was trying to promote German collective guilt and forcefully countered this strawman. According to Priemel, the conspiracy charge \"invited apologetic interpretations: narratives of absolute, totalitarian dictatorship, run by society's lunatic fringe, of which the Germans had been the first victims rather than agents, collaborators, and fellow travellers\". In contrast, the evidence presented on the Holocaust convinced some observers that Germans must have been aware of this crime while it was ongoing.\nAmerican and British prosecution.\nOn 21 November, Jackson gave the opening speech for the prosecution. He described the fact that the defeated Nazis received a trial as \"one of the most significant tributes that Power has ever paid to Reason\". Focusing on aggressive war, which he described as the root of the other crimes, Jackson promoted an intentionalist view of the Nazi state and its overall criminal conspiracy. The speech was favorably received by the prosecution, the tribunal, the audience, historians, and even the defendants.\nMuch of the American case focused on the development of the Nazi conspiracy before the outbreak of war. The American prosecution became derailed during attempts to provide evidence on the first act of aggression, against Austria. On 29 November, the prosecution was unprepared to continue presenting on the invasion of Czechoslovakia, and instead screened \"Nazi Concentration and Prison Camps\". The film, compiled from footage of the liberation of Nazi concentration camps, shocked both the defendants and the judges, who adjourned the trial. Indiscriminate selection and disorganized presentation of documentary evidence without tying it to specific defendants hampered the American prosecutors' work on the conspiracy to commit crimes against humanity. The Americans summoned commander Otto Ohlendorf, who testified about the murder of 80,000 people by those under his command, and SS general Erich von dem Bach-Zelewski, who admitted that German anti-partisan warfare was little more than a cover for the mass murder of Jews.\nThe British prosecution covered the charge of crimes against peace, which was largely redundant to the American conspiracy case. On 4 December, Shawcross gave the opening speech, much of which had been written by Cambridge professor Hersch Lauterpacht. Unlike Jackson, Shawcross attempted to minimize the novelty of the aggression charges, elaborating its precursors in the conventions of Hague and Geneva, the League of Nations Covenant, the Locarno Treaty, and the Kellogg\u2013Briand Pact. The British took four days to make their case, with Maxwell Fyfe detailing treaties broken by Germany. In mid-December the Americans switched to presenting the case against the indicted organizations, while in January both the British and Americans presented evidence against individual defendants. Besides the organizations mentioned in the indictment, American, and British prosecutors also mentioned the complicity of the German Foreign Office, army, and navy.\nFrench prosecution.\nFrom 17 January to 7 February 1946, France presented its charges and supporting evidence. In contrast to the other prosecution teams, the French prosecution delved into Germany's development in the nineteenth century, arguing that it had diverged from the West due to pan-Germanism and imperialism. They argued that Nazi ideology, which derived from these earlier ideas, was the \"mens rea\"\u2014criminal intent\u2014of the crimes on trial. The French prosecutors, more than their British or American counterparts, emphasized the complicity of many Germans; they barely mentioned the charge of aggressive war and instead focused on forced labor, economic plunder, and massacres. Prosecutor Edgar Faure grouped together various German policies, such as the annexation of Alsace\u2013Lorraine, under the label of Germanization, which he argued was a crime against humanity. Unlike the British and American prosecution strategies, which focused on using German documents, French prosecutors took the perspective of the victims, submitting postwar police reports. Eleven witnesses, including victims of Nazi persecution, were called; resistance fighter and Auschwitz survivor Marie Claude Vaillant-Couturier testified about crimes she had witnessed. The French charges of war crimes were accepted by the tribunal, except for the execution of hostages. Due to the narrow definition of crimes against humanity in the charter, the only part of the Germanization charges accepted by the judges was the deportation of Jews from France and other parts of Western Europe.\nSoviet prosecution.\nOn 8 February, the Soviet prosecution opened its case with a speech by Rudenko that covered all four prosecution charges, highlighting a wide variety of crimes committed by the German occupiers as part of their destructive and unprovoked invasion. Rudenko tried to emphasize common ground with the other Allies while rejecting any similarity between Nazi and Soviet rule. The next week, the Soviet prosecution produced Friedrich Paulus\u2014a German field marshal captured after the Battle of Stalingrad\u2014as a witness and questioned him about the preparations for the invasion of the Soviet Union. Paulus incriminated his former associates, pointing to Keitel, Jodl, and G\u00f6ring as the defendants most responsible for the war.\nMore so than other delegations, Soviet prosecutors showed the gruesome details of German atrocities, especially the death by starvation of 3 million Soviet prisoners of war and several hundred thousand residents of Leningrad. Although Soviet prosecutors dealt most extensively with the systematic murder of Jews in eastern Europe, at times they blurred the fate of Jews with that of other Soviet nationalities. Although these aspects had already been covered by the American prosecution, Soviet prosecutors introduced new evidence from Extraordinary State Commission reports and interrogations of senior enemy officers. Lev Smirnov presented evidence on the Lidice massacre in Czechoslovakia, adding that German invaders had destroyed thousands of villages and murdered their inhabitants throughout eastern Europe. The Soviet prosecution emphasized the racist aspect of policies such as the deportation of millions of civilians to Germany for forced labor, the murder of children, systematic looting of occupied territories, and theft or destruction of cultural heritage. The Soviet prosecution also attempted to fabricate German responsibility for the Katyn massacre, which had in fact been committed by the NKVD. Although Western prosecutors never publicly rejected the Katyn charge for fear of casting doubt on the entire proceedings, they were skeptical. The defense presented evidence of Soviet responsibility, and Katyn was not mentioned in the verdict.\nInspired by the films shown by the American prosecution, the Soviet Union commissioned three films for the trial: \"The German Fascist Destruction of the Cultural Treasures of the Peoples of the USSR\", \"Atrocities Committed by the German Fascist Invaders in the USSR\", and \"The German Fascist Destruction of Soviet Cities\", using footage from Soviet filmmakers as well as shots from German newsreels. The second included footage of the liberations of Majdanek and Auschwitz and was considered even more disturbing than the American concentration camp film. Soviet witnesses included several survivors of German crimes, including two civilians who lived through the siege of Leningrad, a peasant whose village was destroyed in anti-partisan warfare, a Red Army doctor who endured several prisoner-of-war camps and two Holocaust survivors\u2014Samuel Rajzman, a survivor of Treblinka extermination camp, and poet Abraham Sutzkever, who described the murder of tens of thousands of Jews from Vilna. The Soviet prosecution case was generally well received and presented compelling evidence for the suffering of the Soviet people and the Soviet contributions to victory.\nDefense.\nFrom March to July 1946, the defense presented its counterarguments. Before the prosecution finished, it was clear that their general case was proven, but it remained to determine the individual guilt of each defendant. None of the defendants tried to assert that the Nazis' crimes had not occurred. Some defendants denied involvement in certain crimes or implausibly claimed ignorance of them, especially the Holocaust. A few defense lawyers inverted the arguments of the prosecution to assert that the Germans' authoritarian mindset and obedience to the state exonerated them from any personal guilt. Most rejected that Germany had deviated from Western civilization, arguing that few Germans could have supported Hitler because Germany was a civilized country.\nThe defendants tried to blame their crimes on Hitler, who was mentioned 12,000 times during the trial\u2014more than the top five defendants combined. Other absent and dead men, including Himmler, Reinhard Heydrich, Adolf Eichmann, and Bormann, were also blamed. To counter claims that conservative defendants had enabled the Nazi rise to power, defense lawyers blamed the Social Democratic Party of Germany, trade unions, and other countries that maintained diplomatic relations with Germany. In contrast, most defendants avoided incriminating each other. Most defendants argued their own insignificance within the Nazi system, though G\u00f6ring took the opposite approach, expecting to be executed but vindicated in the eyes of the German people.\nThe charter did not recognize a \"tu quoque\" defense\u2014asking for exoneration on the grounds that the Allies had committed the same crimes with which the defendants were charged. Although defense lawyers repeatedly equated the Nuremberg Laws to legislation found in other countries, Nazi concentration camps to Allied detention facilities, and the deportation of Jews to the expulsion of Germans, the judges rejected their arguments. Alfred Seidl repeatedly tried to disclose the secret protocols of the German\u2013Soviet pact; although he was eventually successful, it was legally irrelevant and the judges rejected his attempt to bring up the Treaty of Versailles. Six defendants were charged with the German invasion of Norway, and their lawyers argued that this invasion was undertaken to prevent a British invasion of that country; a cover-up prevented the defense from capitalizing on this argument. Fleet admiral Chester Nimitz testified that the United States Navy had also used unrestricted submarine warfare against Japan in the Pacific; D\u00f6nitz's counsel successfully argued that this meant that it could not be a crime. The judges barred most evidence on Allied misdeeds from being heard in court.\nMany defense lawyers complained about various aspects of the trial procedure and attempted to discredit the entire proceedings. In order to appease them, the defendants were allowed a free hand with their witnesses and a great deal of irrelevant testimony was heard. The defendants' witnesses sometimes managed to exculpate them, but other witnesses\u2014including Rudolf H\u00f6ss, the former commandant of Auschwitz, and Hans Bernd Gisevius, a member of the German resistance\u2014bolstered the prosecution's case. In the context of the brewing Cold War\u2014for example, in early March 1946, Winston Churchill delivered the Iron Curtain speech\u2014the trial became a means of condemning not only Germany but also the Soviet Union.\nClosing.\nOn 31 August, closing arguments were presented. Over the course of the trial, crimes against humanity and especially against Jews (who were mentioned as victims of Nazi atrocities far more than any other group) came to upstage the aggressive war charge. In contrast to the opening prosecution statements, all eight closing statements highlighted the Holocaust. The French and British prosecutors made this the main charge, as opposed to that of aggression. All prosecutors except the Americans mentioned the concept of genocide, which had been recently invented by the Polish-Jewish jurist Raphael Lemkin. British prosecutor Shawcross quoted from witness testimony about a murdered Jewish family from Dubno, Ukraine. During the closing statements, most defendants disappointed the judges with lies and denials. Speer managed to give the impression of apologizing without assuming personal guilt or naming any victims other than the German people. On 2 September, the court recessed, and the judges retreated into seclusion to decide the verdict and sentences, which had been under discussion since June. The verdict was drafted by British deputy judge Norman Birkett. All eight judges participated in the deliberations, but the deputies could not vote.\nVerdict.\nThe International Military Tribunal agreed with the prosecution that aggression was the gravest charge, stating in its judgment that because \"war is essentially an evil thing\", \"to initiate a war of aggression, therefore, is not only an international crime; it is the supreme international crime differing only from other war crimes in that it contains within itself the accumulated evil of the whole\". The work of the judges was made more difficult due to the broadness of the crimes listed in the Nuremberg Charter. The judges did not attempt to define the crime of aggression and did not mention the retroactivity of the charges in the verdict. Despite the lingering doubts of some of the judges, the official interpretation of the IMT held that all of the charges had a solid basis in customary international law and that the trial was procedurally fair. The judges were aware that both the Allies and the Axis had planned or committed acts of aggression, writing the verdict carefully to avoid discrediting either the Allied governments or the tribunal.\nThe judges ruled that there had been a premeditated conspiracy to commit crimes against peace, whose goals were \"the disruption of the European order\" and \"the creation of a Greater Germany beyond the frontiers of 1914\". Contrary to Jackson's argument that the conspiracy began with the founding of the Nazi Party in 1920, the verdict dated the planning of aggression to the 1937 Hossbach Memorandum. The conspiracy charge caused significant dissent on the bench; Donnedieu de Vabres wanted to scrap it. Through a compromise proposed by the British judges, the charge of conspiracy was narrowed to a conspiracy to wage aggressive war. Only eight defendants were convicted on that charge, all of whom were also found guilty of crimes against peace. All 22 defendants were charged with crimes against peace, and 12 were convicted. The war crimes and crimes against humanity charges held up the best, with only two defendants charged on those grounds being acquitted. The judges determined that crimes against humanity concerning German Jews before 1939 were not under the court's jurisdiction because the prosecution had not proven a connection to aggressive war.\nFour organizations were ruled to be criminal: the Leadership Corps of the Nazi Party, the SS, the Gestapo, and the SD, although some lower ranks and subgroups were excluded. The verdict only allowed for individual criminal responsibility if willing membership and knowledge of the criminal purpose could be proved, complicating denazification efforts. The SA, Reich Cabinet, General Staff and High Command were not ruled to be criminal organizations. Although the Wehrmacht leadership was not considered an organization within the meaning of the charter, misrepresentation of the verdict as an exoneration would become one of the foundations of the clean Wehrmacht myth. The trial had nevertheless resulted in the coverage of its systematic criminality in the German press.\nSentences were debated at length by the judges. Twelve defendants were sentenced to death: G\u00f6ring, Ribbentrop, Keitel, Kaltenbrunner, Rosenberg, Frank, Frick, Streicher, Sauckel, Jodl, Seyss-Inquart, and Bormann. On 16 October, ten were hanged, with G\u00f6ring killing himself the day before. Seven defendants (Hess, Funk, Raeder, D\u00f6nitz, Schirach, Speer, and Neurath) were sent to Spandau Prison to serve their sentences. All three acquittals (Papen, Schacht, and Fritzsche) were based on a deadlock between the judges; these acquittals surprised observers. Despite being accused of the same crimes, Sauckel was sentenced to death, while Speer was given a prison sentence because the judges considered that he could reform. Nikichenko released a dissent approved by Moscow that rejected all the acquittals, called for a death sentence for Hess, and convicted all the organizations.\nSubsequent Nuremberg trials.\nInitially, it was planned to hold a second international tribunal for German industrialists, but this was never held because of differences between the Allies. Twelve military trials were convened solely by the United States in the same courtroom that had hosted the International Military Tribunal. Pursuant to Law No. 10 adopted by the Allied Control Council, United States forces arrested almost 100,000 Germans as war criminals. The Office of Chief Counsel for War Crimes identified 2,500 major war criminals, of whom 177 were tried. Many of the worst offenders were not prosecuted, for logistical or financial reasons.\nOne set of trials focused on the actions of German professionals: the Doctors' trial focused on human experimentation and euthanasia murders, the Judges' trial on the role of the judiciary in Nazi crimes, and the Ministries trial on the culpability of bureaucrats of German government ministries, especially the Foreign Office. Also on trial were industrialists\u2014in the Flick trial, the IG Farben trial, and the Krupp trial\u2014for using forced labor, looting property from Nazi victims, and funding SS atrocities. Members of the SS were tried in the Pohl trial, which focused on members of the SS Main Economic and Administrative Office that oversaw SS economic activity, including the Nazi concentration camps; the RuSHA trial of Nazi racial policies; and the \"Einsatzgruppen\" trial, in which members of the mobile killing squads were tried for the murder of more than one million people behind the Eastern Front. Luftwaffe general Erhard Milch was tried for using slave labor and deporting civilians. In the Hostages case, several generals were tried for executing thousands of hostages and prisoners of war, looting, using forced labor, and deporting civilians in the Balkans. Other generals were tried in the High Command Trial for plotting wars of aggression, issuing criminal orders, deporting civilians, using slave labor, and looting in the Soviet Union.\nThese trials emphasized the crimes committed during the Holocaust. The trials heard 1,300 witnesses, entered more than 30,000 documents into evidence, and generated 132,855 pages of transcripts, with the judgments totaling 3,828 pages. Of 177 defendants, 142 were convicted and 25 sentenced to death; the severity of sentencing was related to the defendant's proximity to mass murder. Legal historian Kevin Jon Heller argues that the trials' greatest achievement was \"their inestimable contribution to the form and substance of international criminal law\", which had been left underdeveloped by the IMT.\nContemporary reactions.\nIn all, 249 journalists were accredited to cover the IMT and 61,854 visitor tickets were issued. In France, the sentence for Rudolf Hess and acquittal of organizations were met with outrage from the media and especially from organizations for deportees and resistance fighters, as they were perceived as too lenient. In the United Kingdom, although a variety of responses were reported, it was difficult to sustain interest in a long trial. Where the prosecution was disappointed by some of the verdicts, the defense could take satisfaction.\nMany Germans at the time of the trials focused on finding food and shelter. Despite this, a majority read press reports about the trial. In a 1946 poll, 78 percent of Germans assessed the trial as fair, but four years later that had fallen to 38 percent, with 30 percent considering it unfair. As time went on, more Germans considered the trials illegitimate victor's justice and an imposition of collective guilt, which they rejected\u2014instead considering themselves victims of the war. As the Cold War began, the rapidly changing political environment began to affect the effectiveness of the trials. The educational purpose of the Nuremberg Military Tribunals was a failure, in part because of the resistance to war crimes trials in German society, but also because of the United States Army's refusal to publish the trial record in German for fear it would undermine the fight against communism.\nThe German churches, both Catholic and Protestant, were vocal proponents of amnesty. The pardon of convicted war criminals also had cross-party support in West Germany, which was established in 1949. The Americans satisfied these wishes to bind West Germany to the Western Bloc, beginning early releases of Nuremberg Military Tribunal convicts in 1949. In 1951, High Commissioner John J. McCloy overturned most of the sentences and the last three prisoners, all convicted at the \"Einsatzgruppen\" trial, were released in 1958. The German public took the early releases as confirmation of what they saw as the illegitimacy of the trials. The IMT defendants required Soviet permission for release; Speer was not successful in obtaining early release, and Hess remained in prison until his death in 1987. By the late 1950s, the West German consensus on release began to erode, due to greater openness in political culture and new revelations of Nazi criminality, including the first trials of Nazi perpetrators in West German courts.\nLegacy.\nThe International Military Tribunal, and its charter, \"marked the true beginning of international criminal law\". The trial has met a mixed reception ranging from glorification to condemnation. The reaction was initially predominantly negative, but has become more positive over time.\nThe selective prosecution exclusively of the defeated Axis and hypocrisy of all four Allied powers has garnered the most persistent criticism. Such actions as the German\u2013Soviet pact, the expulsion of millions of Germans from central and eastern Europe, deportation of civilians for forced labor, and violent suppression of anti-colonial uprisings would have been deemed illegal according to the definitions of international crimes in the Nuremberg charter. Another controversy resulted from trying defendants for acts that were not criminal at the time, particularly crimes against peace. Equally novel but less controversial were crimes against humanity, the conspiracy charge, and criminal penalties on individuals for breaches of international law. Besides these criticisms, the trials have been taken to task for the distortion that comes from fitting historical events into legal categories.\nThe International Military Tribunal for the Far East (Tokyo Trial) borrowed many of its ideas from the IMT, including all four charges, and was intended by the Truman Administration to shore up the IMT's legal legacy. On 11 December 1946, the United Nations General Assembly unanimously passed a resolution affirming \"the principles of international law recognized by the Charter of the Nuremberg Tribunal and the judgment of the Tribunal\". In 1950, the International Law Commission drafted the Nuremberg principles to codify international criminal law, although the Cold War prevented the adoption of these principles until the 1990s. The 1948 Genocide Convention was much more restricted than Lemkin's original concept and its effectiveness was further limited by Cold War politics. In the 1990s, a revival of international criminal law included the establishment of \"ad hoc\" international criminal tribunals for Yugoslavia (ICTY) and Rwanda (ICTR), which were widely viewed as part of the legacy of the Nuremberg and Tokyo trials. A permanent International Criminal Court (ICC), proposed in 1953, was established in 2002.\nThe trials were the first use of simultaneous interpretation, which stimulated technical advances in translation methods. The Palace of Justice houses a museum on the trial and the courtroom became a tourist attraction, drawing 13,138 visitors in 2005. The IMT is one of the most well-studied trials in history, and it has also been the subject of an abundance of books and scholarly publications, along with motion pictures such as \"Judgment at Nuremberg\" (1961), \"The Memory of Justice\" (1976) and \"Nuremberg\" (2025).\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nConsists of footage from German films documenting Nazi personalities and activities interwoven with film shot during the trials \u2014 including testimony and statements from defendants, prosecuting attorneys, judges, and witnesses. It also contains flashbacks of a variety of Nazi crimes against humanity."}
{"id": "21876", "revid": "6056090", "url": "https://en.wikipedia.org/wiki?curid=21876", "title": "Natasha Stott Despoja", "text": "Australian politician\n \nNatasha Jessica Stott Despoja AO (born 9 September 1969) is an Australian diplomat, gender equality advocate, former Australian of the Year nominee, and former politician. Starting her career in student politics, she became an advisor to the Australian Democrats and was appointed to the Australian Senate in 1995 at the age of 26. At the time, she was the youngest woman to serve in Federal Parliament. She went on to become deputy leader of the Democrats in 1997 and then federal leader from 2001 to 2002. She retired from the Senate in 2008 as the longest-serving senator from her party.\nShe has remained active in the public sphere, working with government and non-profit organisations. She was appointed Officer of the Order of Australia in 2019 for her work on gender equality. Stott Despoja was the founding chair of Our Watch, a national foundation to prevent violence against women and children, and served as national Ambassador for Women and Girls from 2013 to 2016. She was also a member of the World Bank Gender Advisory Council from 2015 to 2017. She has served in positions at the United Nations, including on the High Level Working Group on the Health and Human Rights of Women, Children, and Adolescents in 2017, and as a member of the Convention on the Elimination of all forms of Discrimination Against Women (CEDAW) since 2020. She has published several books and writes regularly on current topics.\nEarly life and education.\nStott Despoja was born in Adelaide on 9 September 1969. She is the daughter of Shirley Stott Despoja, an Australian-born journalist and Mario Despoja, who was from Croatia (then part of Yugoslavia). She attended Stradbroke Primary and Pembroke School and later graduated from the University of Adelaide in 1991. She was President of the Students' Association of the University of Adelaide (SAUA) and the South Australian Women's Officer for the National Union of Students. She then went on to work as a political advisor to Senator John Coulter and Senator Cheryl Kernot.\nPolitical career.\nWhen Democrats Senator John Coulter resigned for health reasons in 1995, Stott Despoja was the successful candidate to fill the resulting vacancy. Her rise to prominence began when she won a full term in the 1996 election the following year, becoming the youngest woman ever elected to the federal Parliament at age 26. Late the following year, following the resignation of Cheryl Kernot and the rise of Meg Lees to the leadership of the Democrats, Stott Despoja was elected deputy leader of the Democrats. During this time, she built her image as spokesperson for Employment, Higher Education, Youth, Science and Information Technology, Consumer Affairs, Trade, and the Republic.\nDuring the passage of the Goods and Services Tax (GST) legislation in 1999, Stott Despoja and Andrew Bartlett split with the party's other senators by opposing the package, which had been negotiated by Lees and prime minister John Howard. She said that she refused to break promises made during the election. The party had stated that they would work with whichever party formed government to improve their tax package. The Australian Democrats traditionally permitted parliamentary representatives to cast a conscience vote on any issue but, on this occasion, close numbers in the Senate placed greater pressure than usual on the dissenters.\nLeader of the Democrats.\nThe passage of the GST was not popular among the Democrats membership. Unlike other parties, the members directly elected the party leader and a spill could be called at any time with 100 signatures. Meg Lees had been subject to such a challenge before but was re-elected unopposed as no other senator opted to run. By early 2001, the party's fortunes were declining. The state election in Western Australia in February, where the Democrats lost both their seats in the upper house, was particularly damaging and prompted another spill campaign against Lees. Stott Despoja, who by this time was widely recognised and popular among voters, decided to run and was successful, winning 69% of votes.\nStott Despoja became the leader of her party on 6 April 2001. From the beginning she faced difficulties in working with Lees, who viewed her run for the leadership as a betrayal. Other senators, including new deputy leader Aden Ridgeway, remained sympathetic to Lees. In the 2001 federal election in November, the Democrats recorded a fall in their Senate vote from 8.5% to 7.3% and returned four of the five senators up for election. The party also saw a marginal lift in its primary vote for the lower house from 5.1% to 5.4%. The substantial rise of the Greens vote to 4.9% in the Senate and their election of a second senator ignited further discussion about the fortunes of the Democrats.\nThroughout 2002, Stott Despoja struggled to keep the party together as senators publicly strayed from party positions and privately expressed a lack of confidence in her leadership. After the party bureaucracy opened an investigation into Meg Lees for allegedly damaging party unity, which Lees and her allies saw as part of a campaign by Stott Despoja to silence her, Lees left the party in July 2002. This was followed by a stand-off with Andrew Murray, who threatened to follow. After deciding to stay, Murray proposed a ten-point package to reform party structures and address the issues raised by Lees, designed to shift power from the leader. At a party room meeting on 21 August, all ten measures were passed four votes to three: Murray, Ridgeway, Lyn Allison and John Cherry in favour, with Stott Despoja and her allies Andrew Bartlett and Brian Greig against. Understanding her position to be untenable after this defeat, Stott Despoja announced her resignation to the Senate. She had been leader for 16 and a half months.\nPost-leadership.\nStott Despoja remained active in the Senate and the Democrats after resigning as leader. The party's fortunes continued to decline under new leader Andrew Bartlett. In the 2004 election they failed to elect any senators, leaving only the four elected in 2001: Stott Despoja, Andrew Bartlett, Lyn Allison, and Andrew Murray.\nIn 2004, Stott Despoja took 11 weeks' leave from the Senate following the birth of her first child. She returned to full duties as spokesperson for Higher Education, Science and Biotechnology, Women, Privacy, Territories and Local Government, and Work and Family.\nDuring her career, Stott Despoja introduced 24 private member's bills on issues including paid maternity leave, the Republic, genetic privacy, stem cells, captioning, and same-sex marriage. Stott Despoja regularly attends the Sydney Gay and Lesbian Mardi Gras.\nOn 22 October 2006, after undergoing emergency surgery for an ectopic pregnancy, she announced that she would not contest the 2007 election and would leave office at the expiration of her term on 30 June 2008. She was the Australian Democrats' longest-serving senator. As in 2004, the Democrats elected no senators in 2007, and Stott Despoja's retirement coincided with the end of her party's federal parliamentary representation.\nPost-political career.\nStott Despoja has been a casual host on ABC Radio Adelaide, a guest panellist on Channel 10's \"The Project\" and a columnist for the Australian business news website \"Business Spectator\". She has also been a columnist for \"The Advertiser\" and an honorary visiting research fellow at the University of Adelaide.\nShe was on the board of the Burnet Institute (Australia's largest virology and communicable disease research institute) from 2008 until December 2013. On 21 July 2015, Stott Despoja returned to the Burnet Institute as a patron. She was no longer a patron by May 2023.\nIn 2010, she taught a course at winter school at the University of Adelaide with former foreign minister Alexander Downer, called \"The Practice of Australian Politics\".\nShe was a board member of non-profit organisations the South Australian Museum from 2009 to 2013; the Museum of Australian Democracy from 2010 to 2013; and the Advertising Standards Board from 2008 to 2013. She was a deputy chair at beyondblue, Australia's national depression initiative.\nShe has been an ambassador for Ovarian Cancer Australia, The Orangutan Project; Cancer Australia; secondbite; and the HIV/AIDS anti-stigma campaign, ENUF (along with her husband Ian Smith).\nIn July 2013, Stott Despoja was the founding chair of Our Watch, originally named Foundation to Prevent Violence Against Women and their Children. She left the position in July 2021, and was appointed life patron in August 2022. Our Watch is a joint initiative of the Victorian and Commonwealth Governments, based in Melbourne. It is an independent non-profit organisation that is now jointly funded by all states and territories of Australia, after the New South Wales Government was the last state government to join the organisation in 2019.\nForeign minister Julie Bishop announced the appointment of Stott Despoja as Australia's new Ambassador for Women and Girls in December 2013, a role she held until 2016. This involved visiting some 45 countries to promote women's economic empowerment and leadership and to help reduce violence against women and girls.\nStott Despoja has also been an election observer for the US-based National Democratic Institute in Nigeria (2011); visited Burkina Faso for Oxfam (2012); and went to Laos (2011) and Burma (2013) with The Burnet Institute. She was mentioned in June 2014 as a possible replacement for Kevin Scarce as the next Governor of South Australia, however Hieu Van Le was chosen.\nIn April 2019 Stott Despoja was on the advisory board of the Australian Privacy Foundation. and the Global Women's Institute Leadership Council.\nIn November 2020, she was elected to the UN Committee on the Elimination of Discrimination against Women, becoming the first Australian member in 28 years.\nIn 2022, she delivered the Hugh Stretton Oration at the University of Adelaide. She was also a nominee for South Australia's Australian of the Year.\nIn March 2024, Stott Despoja was appointed as South Australia's Royal Commissioner into Domestic, Family, and Sexual Violence, after the Royal Commission into Domestic, Family and Sexual Violence had been announced by Premier Peter Malinauskas in December 2023. On 1 July 2024, she began her role as royal commissioner. On 19 August 2025 the Royal Commission published its 600-page report, which included 136 recommendations for changes.\nWriting.\nStott Despoja has authored a large number of essays, reports, and non-fiction works on a range of topics, both during and since her political career.\nIn March 2019 she published \"On Violence\", with the publisher's blurb asking \"Why is violence against women endemic, and how do we stop it?\". Stott Despoja posits that violence against women is \"Australia's national emergency\", with one woman dying at the hands of her partner or someone she knows every week. This violence is preventable, and that we need to \"create a new normal\".\nHonours and accolades.\nIn 1999, she was appointed a Global Leader for Tomorrow by the World Economic Forum (WEF).\nDespoja was awarded a Member of the Order of Australia in June 2011 for her \"service to the Parliament of Australia, particularly as a Senator for South Australia, through leadership roles with the Australian Democrats, to education, and as a role model for women\".\nShe is as of April 2019[ [update]] listed as one of the \"Gender Equality Top 100\" by the UK organisation Apolitical.\nIn June 2019 Despoja was appointed as an Officer of the Order of Australia for her \"distinguished service to the global community as an advocate for gender equality, and through roles in a range of organisations\".\nPersonal life.\nStott Despoja was married to former Liberal Party advisor Ian Smith, whom she married in a beachside ceremony in Byron Bay in 2003. The marriage produced two children.\nThe couple announced their separation in 2024.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "21880", "revid": "41195652", "url": "https://en.wikipedia.org/wiki?curid=21880", "title": "Nullum crimen, nulla poena sine praevia lege poenali", "text": ""}
{"id": "21881", "revid": "49679309", "url": "https://en.wikipedia.org/wiki?curid=21881", "title": "Nuremberg Code", "text": "Principles for ethical human research\nThe Nuremberg Code () is a set of ethical research principles for human experimentation created by the court in \"U.S. v Brandt\", one of the Subsequent Nuremberg trials that were held after the Second World War.\nThough it was articulated as part of the court's verdict in the trial, the Code would later become significant beyond its original context; in a review written on the 50th anniversary of the \"Brandt\" verdict, Jay Katz writes that \"a careful reading of the judgment suggests that [the authors] wrote the Code for the practice of human experimentation whenever it is being conducted.\"\nBackground.\nThe origin of the Code began in pre\u2013World War II German politics, particularly during the 1930s and 1940s. Starting in the mid-1920s, German physicians, usually proponents of racial hygiene, were accused by the public and the medical society of unethical medical practices. The use of racial hygiene was supported by the German government in order to promote an Aryan race. Racial hygiene extremists merged with National Socialism to promote the use of biology to accomplish their goals of racial purity, a core concept in the Nationalist ideology. Physicians were attracted to the scientific ideology and aided in the establishment of the National Socialist Physicians' League in 1929 to \"purify the German medical community of 'Jewish Bolshevism'.\" Criticism was becoming prevalent; Alfons Stauder, member of the Reich Health Office, claimed that the \"dubious experiments have no therapeutic purpose\", and Fredrich von Muller, physician and the president of the Deutsche Akademie, joined the criticism.\nIn response to the criticism of unethical human experimentation, the Weimar Republic (Germany's government from 1919 to 1933) issued \"Guidelines for New Therapy and Human Experimentation\". The guidelines were based on beneficence and non-maleficence, but also stressed the legal doctrine of informed consent. The guidelines clearly distinguished the difference between therapeutic and non-therapeutic research. For therapeutic purposes, the guidelines allowed administration without consent only in dire situations, but for non-therapeutic purposes any administration without consent was strictly forbidden. However, the guidelines from Weimar were negated by Adolf Hitler. By 1942, the Nazi party included more than 38,000 German physicians, who helped carry out medical programs such as the Law for the Prevention of Hereditarily Diseased Offspring.\nAfter World War II, a series of trials were held to hold members of the Nazi party responsible for a multitude of war crimes. The trials were approved by President Harry Truman on 2 May 1945, and were led by the United States, Great Britain, and the Soviet Union. They began on 20 November 1945, in Nuremberg, Germany, in what became known as the Nuremberg trials. In the trial of \"USA v. Brandt,\" which became known as the \"Doctors' Trial\", German physicians responsible for conducting unethical medical procedures on humans during the war were tried. They focused on physicians who conducted inhumane and unethical human experiments in concentration camps, in addition to those who were involved in over 3.5 million sterilizations of German citizens.\nSeveral of the accused argued that their experiments differed little from those used before the war, and that there was no law that differentiated between legal and illegal experiments. This worried Andrew Ivy and Leo Alexander, who worked with the prosecution during the trial. In April 1947, Alexander submitted a memorandum to the United States Counsel for War Crimes outlining six points for legitimate medical research.\nAn early version of the Code known as the Memorandum, which stated explicit voluntary consent from patients is required for human experimentation, was drafted on 9 August 1947. On 20 August 1947, the judges delivered their verdict against Karl Brandt and 22 others. The verdict reiterated the Memorandum's points and, in response to expert medical advisers for the prosecution, revised the original six points of the Memorandum to ten points. The ten points became known as the Code, which includes such principles as informed consent and absence of coercion; properly formulated scientific experimentation; and beneficence towards experiment participants. It is thought to have been mainly based on the Hippocratic Oath, which was interpreted as endorsing the experimental approach to medicine while protecting the patient.\nAuthorship 'controversy'.\nThe Code was initially ignored, but gained much greater significance about 20 years after it was written. As a result, there were substantial rival claims for the creation of the Code. Some claimed that Harold Sebring, one of the three U.S. judges who presided over the Doctors' trial, was the author. Leo Alexander, MD and Andrew Ivy, MD, the prosecution's chief medical expert witnesses, were also each identified as authors. In his letter to Maurice Henry Pappworth, an English physician and the author of the 1967 book \"Human Guinea Pigs\", Andrew Ivy claimed sole authorship of the code. Leo Alexander, approximately 30 years after the trial, also claimed sole authorship. However, after careful reading of the transcript of the Doctors' trial, background documents, and the final judgements, it is more accepted that the authorship was shared and the code grew out of the trial itself.\nThe ten points of the Nuremberg Code.\nThe ten points of the code were given in the section of the judges' verdict entitled \"Permissible Medical Experiments\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nImportance.\nThe Code has not been officially accepted as law by any nation or as official ethics guidelines by any association. In fact, the Code's reference to Hippocratic duty to the individual patient and the need to provide information was not initially favored by the American Medical Association. Katz observes that the Western world initially dismissed the Nuremberg Code as a \"code for barbarians, but unnecessary (or superfluous) for ordinary physicians.\" Additionally, the final judgment did not specify whether the Code should be applied to cases such as political prisoners, convicted felons, and healthy volunteers. The lack of clarity, the brutality of the unethical medical experiments, and the uncompromising language of the Code created an image that it was designed for singularly egregious transgressions.\nHowever, the Code is considered by some to be the most important document in the history of clinical research ethics, because of its massive influence on global human rights. In the United States, the Code and the related Declaration of Helsinki influenced the drafting of regulations promulgated by the United States Department of Health and Human Services to ensure ethical treatment of human research subjects, known as the Common Rule, which is now codified in Part 46 of Title 45 of the Code of Federal Regulations. These regulations are enforced by Institutional Review Boards (IRBs). In 1966, the International Covenant on Civil and Political Rights was adopted by the United Nations, and after enough nations had ratified the Covenant, it came into force on 23 March 1976. Article Seven prohibits experiments conducted without the \"free consent to medical or scientific experimentation\" of the subject. As of September 2019, the Covenant has 173 states parties.\nIn his 2014 review, Gaw observes that the Code \"not only entered the legal landscape, but also became the prototype for all future codes of ethical practice across the globe.\" The idea of free or informed consent also served as the basis for International Ethical Guidelines for Biomedical Research Involving Human Subjects proposed by the World Health Organization. Another notable symposium review was published by the Medical University of Vienna in 2017: \"Medical Ethics in the 70 Years after the Nuremberg Code, 1947 to the Present\". President and Rector Markus Muller writes in his introduction that the Code \"constitutes one of the most important milestones in the history of medicine, providing for the first time a proper framework for research on human subjects. This milestone was not a voluntary, precautionary measure, but only came into existence in the aftermath of Nazi atrocities. The Nuremberg Code became a cornerstone of clinical research and bioethics.\"\nIn 1995, United States District Judge Sandra Beckwith ruled that the Nuremberg Code may be applied in criminal and civil litigation in the federal courts of the United States.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "21883", "revid": "29596807", "url": "https://en.wikipedia.org/wiki?curid=21883", "title": "Neural net", "text": ""}
{"id": "21884", "revid": "29596807", "url": "https://en.wikipedia.org/wiki?curid=21884", "title": "Neural nets", "text": ""}
