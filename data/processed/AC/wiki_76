{"id": "27641", "revid": "50588066", "url": "https://en.wikipedia.org/wiki?curid=27641", "title": "Sam Peckinpah", "text": "American film director (1925\u20131984)\nDavid Samuel Peckinpah (; February 21, 1925 \u2013 December 28, 1984) was an American film director and screenwriter. His 1969 Western epic \"The Wild Bunch\" received two Academy Award nominations and was ranked No. 80 on the American Film Institute's top 100 list. His films employed a visually innovative and explicit depiction of action and violence as well as a revisionist approach to the Western genre.\nPeckinpah's films deal with the conflict between values and ideals, as well as the corruption and violence in human society. His characters are often loners or losers who desire to be honorable but are forced to compromise in order to survive in a world of nihilism and brutality. He was given the nickname \"Bloody Sam\" owing to the violence in his films.\nPeckinpah's combative personality, marked by years of alcohol and drug abuse, affected his professional legacy. The production of many of his films included battles with producers and crew members, damaging his reputation and career during his lifetime. Peckinpah's other films include \"Ride the High Country\" (1962), \"Major Dundee\" (1965), \"The Ballad of Cable Hogue\" (1970), \"Straw Dogs\" (1971), \"The Getaway\" (1972), \"Pat Garrett and Billy the Kid\" (1973), \"Bring Me the Head of Alfredo Garcia\" (1974), \"Cross of Iron\" (1977) and \"Convoy\" (1978).\nFamily origins.\nThe Peckinpahs originated from the Frisian Islands in the northwest of Europe. Both sides of Peckinpah's family migrated to the American West by covered wagon in the mid-19th century. Peckinpah and several relatives often claimed Native American ancestry, but this has been denied by surviving family members. Peckinpah's great-grandfather, Rice Peckinpaugh, a merchant and farmer in Indiana, moved to Humboldt County, California, in the 1850s, working in the logging business, and changed the spelling of the family name to \"Peckinpah\".\nPeckinpah Meadow and Peckinpah Creek, where the family ran a lumber mill on a mountain in the High Sierra east of North Fork, California, have been officially named on U.S. geographical maps. Peckinpah's maternal grandfather was Denver S. Church, a cattle rancher, Superior Court judge and United States Congressman of a California district including Fresno County.\nSam Peckinpah's nephew is David Peckinpah, who was a television producer and director, as well as a screenwriter. He was a cousin of former New York Yankees shortstop Roger Peckinpaugh.\nLife.\nDavid Samuel Peckinpah was born February 21, 1925, to David Edward (1895\u20131960) and Fern Louise (\"n\u00e9e\" Church) Peckinpah (1893\u20131983) in Fresno, California, where he attended both grammar school and high school. He had an elder brother, Denver Charles (1916\u20131996). He spent much time skipping classes with his brother to engage in cowboy activities on their grandfather Denver Church's ranch, including trapping, branding, and shooting. During the 1930s and 1940s, Coarsegold and Bass Lake were still populated with descendants of the miners and ranchers of the 19th century. Many of these descendants worked on Church's ranch. At that time, it was a rural area undergoing extreme change, and this exposure is believed to have affected Peckinpah's Western films later in life.\nHe played on the junior varsity football team while at Fresno High School, but frequent fighting and discipline problems caused his parents to enroll him in the San Rafael Military Academy for his senior year.\nIn 1943, he joined the United States Marine Corps. Within two years, his battalion was sent to China with the task of disarming Japanese soldiers and repatriating them following World War II. While his duty did not include combat, he claimed to have witnessed acts of war between Chinese and Japanese soldiers. According to friends, these included several acts of torture and the murder of a laborer by sniper fire. The American Marines were not permitted to intervene. Peckinpah also claimed he was shot during an attack by Communist forces. Also during his final weeks as a Marine, he applied for discharge in Beijing, so he could marry a local woman, but was refused. His experiences in China reportedly deeply affected Peckinpah, and may have influenced his depictions of violence in his films.\nAfter being discharged in Los Angeles, he attended California State University, Fresno, where he studied history. While a student, he met and married his first wife, Marie Selland, in 1947. A drama major, Selland introduced Peckinpah to the theater department and he became interested in directing for the first time. During his senior year, he adapted and directed a one-hour version of Tennessee Williams' \"The Glass Menagerie\".\nAfter graduation in 1948, Peckinpah enrolled in graduate studies in drama at University of Southern California. He spent two seasons as the director in residence at Huntington Park Civic Theatre near Los Angeles before obtaining his master's degree. He was asked to stay another year, but Peckinpah began working as a stagehand at KLAC-TV in the belief that television experience would eventually lead to work in films. Even during this early stage of his career, Peckinpah was developing a combative streak. Reportedly, he was kicked off the set of \"The Liberace Show\" for not wearing a tie, and he refused to cue a car salesman during a live feed because of his attitude towards stagehands.\nIn 1954, Peckinpah was hired as a dialogue coach for the film \"Riot in Cell Block 11\". His job entailed acting as an assistant for the movie's director, Don Siegel. The film was shot on location at Folsom Prison. Reportedly, the warden was reluctant to allow the filmmakers to work at the prison until he was introduced to Peckinpah. The warden knew of his influential family from Fresno and was immediately cooperative. Siegel's location work and his use of actual prisoners as extras in the film made a lasting impression on Peckinpah. He worked as a dialogue coach on four additional Siegel films: \"Private Hell 36\" (1954), \"An Annapolis Story\" (1955, and co-starring L. Q. Jones), \"Invasion of the Body Snatchers\" (1956) and \"Crime in the Streets\" (1956).\n\"Invasion of the Body Snatchers\", in which Peckinpah appeared as Charlie the meter reader, starred Kevin McCarthy and Dana Wynter. It became one of the most critically praised science fiction films of the 1950s. Peckinpah claimed to have done an extensive rewrite on the film's screenplay, a statement which remains controversial.\nThroughout much of his adult life, Peckinpah was affected by alcoholism, and, later, other forms of drug addiction. According to some accounts, he also suffered from mental illness, possibly manic depression or paranoia. It is believed his drinking problems began during his service in the military while stationed in China, when he frequented the saloons of Tianjin and Beijing. After divorcing Selland, the mother of his first four children, in 1960, he married Mexican actress Bego\u00f1a Palacios in 1964. A stormy relationship developed, and over the years they married on three separate occasions. They had one daughter together. His personality reportedly often swung between a sweet, softly-spoken, artistic disposition, and bouts of rage and violence, during which he verbally and physically abused himself and others. An experienced hunter, Peckinpah was fascinated with firearms and was known to shoot the mirrors in his house while abusing alcohol, an image which occurs several times in his films.\nPeckinpah's reputation as a hard-living brute with a taste for violence, inspired by the content in his most popular films and in many ways perpetuated by himself, affected his artistic legacy. His friends and family have claimed this does a disservice to a man who was actually more complex than generally credited. He used such actors as Warren Oates, L. Q. Jones, R. G. Armstrong, James Coburn, Ben Johnson, and Kris Kristofferson, and collaborators (Jerry Fielding, Lucien Ballard, Gordon Dawson, and Martin Baum) in many of his films, and several of his friends and assistants stuck by him to the end of his life.\nPeckinpah spent a great deal of his life in Mexico after his marriage to Palacios, eventually buying property in the country. He was fascinated by the Mexican lifestyle and Mexican culture, and he often portrayed it with an unusual sentimentality and romanticism in his films.\nFrom 1979 until his death, Peckinpah lived at the Murray Hotel in Livingston, Montana. Peckinpah was seriously ill during his final years, as a lifetime of hard living caught up with him. Regardless, he continued to work until his last months. He died of heart failure at age 59 on December 28, 1984, in Inglewood, California. At the time, he was working on the script for \"On the Rocks\", a projected independent film to be shot in San Francisco.\nTelevision career.\nOn the recommendation of Don Siegel, Peckinpah established himself during the late 1950s as a scriptwriter of Western series of the era, selling scripts to \"Gunsmoke\", \"Have Gun \u2013 Will Travel\", \"Broken Arrow\", \"Klondike\", \"The Rifleman\", and \"Dick Powell's Zane Grey Theatre\", the latter Four Star Television productions. He wrote one episode \"The Town\" (December 13, 1957) for the CBS series, \"Trackdown\".\nPeckinpah wrote a screenplay from the novel \"The Authentic Death of Hendry Jones\", a draft that evolved into the 1961 Marlon Brando film \"One-Eyed Jacks\". His writing led to directing, and he directed a 1958 episode of \"Broken Arrow\" (generally credited as his first official directing job) and several 1960 episodes of \"Klondike\", (co-starring James Coburn, L. Q. Jones, Ralph Taeger, Joi Lansing, and Mari Blanchard). He also directed the CBS sitcom \"Mr. Adams and Eve\", starring Howard Duff and Ida Lupino.\nIn 1958, Peckinpah wrote a script for \"Gunsmoke\" that was rejected due to content. He reworked the screenplay, titled \"The Sharpshooter\", and sold it to \"Zane Grey Theater\". The episode received popular response and became the television series \"The Rifleman\", starring Chuck Connors. Peckinpah directed four episodes of the series (with guest stars R. G. Armstrong and Warren Oates), but left after the first year. \"The Rifleman\" ran for five seasons and achieved enduring popularity in syndication.\n\"The Westerner\".\nDuring this time, he also created the television series \"The Westerner\" for Four Star Television, starring Brian Keith and in three episodes also featuring John Dehner. Peckinpah wrote and directed a pilot called \"Trouble at Tres Cruzes\", which was aired in March 1959 before the actual series was made in 1960. Peckinpah acted as producer of the series, having a hand in the writing of each episode and directing five of them. Critically praised, the show ran for only 13 episodes before cancellation mainly due to its gritty content detailing the drifting, laconic cowboy Dave Blassingame (Brian Keith). Especially noteworthy are the episodes \"Jeff\" and \"Hand on the Gun\", extraordinary in their depiction of violence and their imaginative directing, forerunners of his later feature films. Despite its short run, \"The Westerner\" and Peckinpah were nominated by the Producers Guild of America for Best Filmed Series. An episode of the series eventually served as the basis for Tom Gries' 1968 film \"Will Penny\" starring Charlton Heston. \"The Westerner\", which has since achieved cult status, further established Peckinpah as a talent to be reckoned with.\nIn 1962, Peckinpah directed two hour-long episodes for \"The Dick Powell Theater\". In the second of these, \"The Losers\", an updated remake of \"The Westerner\" set in the present day with Lee Marvin as Dave Blassingame and Keenan Wynn as Dehner's character Bergundy Smith, he mixed slow motion, fast motion and stills together to capture violence, a technique famously put to more sophisticated use in 1969s \"The Wild Bunch\".\nEarly film career.\n\"The Deadly Companions\".\nAfter cancellation of \"The Westerner\", Brian Keith was cast as the male lead in the 1961 Western film \"The Deadly Companions\". He suggested Peckinpah as director and the project's producer Charles B. Fitzsimons accepted the idea. By most accounts, the low-budget film shot on location in Arizona was a learning process for Peckinpah, who feuded with Fitzsimons (brother of the film's star Maureen O'Hara) over the screenplay and staging of the scenes. Reportedly, Fitzsimons refused to allow Peckinpah to give direction to O'Hara. Unable to rewrite the screenplay or edit the picture, Peckinpah vowed to never again direct a film unless he had script control. \"The Deadly Companions\" passed largely without notice and is the least known of Peckinpah's films.\n\"Ride the High Country\".\nHis second film, \"Ride the High Country\" (1962), was based on the screenplay \"Guns in the Afternoon\" written by N.B. Stone, Jr. Producer Richard Lyons admired Peckinpah's work on \"The Westerner\" and offered him the directing job. Peckinpah did an extensive rewrite of the screenplay, including personal references from his own childhood growing up on Denver Church's ranch, and even naming one of the mining towns \"Coarsegold.\" He based the character of Steve Judd, a once-famous lawman fallen on hard times, on his own father David Peckinpah. In the screenplay, Judd and old friend Gil Westrum are hired to transport gold from a mining community through dangerous territory. Westrum hopes to talk Judd into taking the gold for themselves. Along the way, following Judd's example, Westrum slowly realizes his own self-respect is far more important than profit. During the final shootout, when Judd and Westrum stand up to a trio of men, Judd is fatally wounded but his death serves as Westrum's salvation, a Catholic tragedy woven from the cloth of the Western genre. This sort of salvation became a major theme in many Peckinpah's later films. Starring aging Western stars Joel McCrea and Randolph Scott in their final major screen roles, the film initially went unnoticed in the United States but was an enormous success in Europe. Beating Federico Fellini's \"8\u00bd\" for first prize at the Belgium Film Festival, the film was hailed by foreign critics as a brilliant reworking of the Western genre. New York critics also discovered Peckinpah's unusual Western, with \"Newsweek\" naming \"Ride the High Country\" the best film of the year and \"Time\" placing it on its ten-best list. By some critics, the film is admired as one of Peckinpah's greatest works.\n\"Major Dundee\".\nPeckinpah's next film, \"Major Dundee\" (1965), was the first of Peckinpah's many unfortunate experiences with the major studios that financed his productions. Based on a screenplay by Harry Julian Fink, the film was to star Charlton Heston. Peckinpah was hired as director after Heston viewed producer Jerry Bresler's private screening of \"Ride the High Country\". Heston liked the film and called Peckinpah, saying, \"I'd like to work with you.\" The sprawling screenplay told the story of Union cavalry officer Major Dundee who commands a New Mexico outpost of Confederate prisoners. When an Apache war chief wipes out a company and kidnaps several children, Dundee throws together a makeshift army, including unwilling Confederate veterans, black Federal soldiers, and traditional Western types, and takes off after the Indians. Dundee becomes obsessed with his quest and heads deep into the wilderness of Mexico with his exhausted men in tow.\nFilming began without a completed screenplay, and Peckinpah chose several remote locations in Mexico, causing the film to go heavily overbudget. Intimidated by the size and scope of the project, Peckinpah reportedly drank heavily each night after shooting. He also fired at least 15 crew members. At one point, Peckinpah's mean streak and abusiveness towards the actors so enraged Heston that the normally even-tempered star threatened to run the director through with his cavalry saber if he did not show more courtesy to the cast. Shooting ended 15 days over schedule and $1.5 million more than budgeted with Peckinpah and producer Bresler no longer on speaking terms. The movie, detailing themes and sequences Peckinpah mastered later in his career, was taken away from him and substantially reedited. An incomplete mess which today exists in a variety of versions, \"Major Dundee\" performed poorly at the box office and was trashed by critics (though its standing has improved over the years). Peckinpah maintained, nonetheless, throughout his life that his original version of \"Major Dundee\" was among his best films, but his reputation was severely damaged.\nPeckinpah was next signed to direct \"The Cincinnati Kid\", a gambling drama about a young prodigy who takes on an old master during a big New Orleans poker match. Before filming started, producer Martin Ransohoff began to receive phone calls about the \"Major Dundee\" ordeal and was told Peckinpah was impossible to work with. Peckinpah decided to shoot in black and white and was hoping to transform the screenplay into a social realist saga about a kid surviving the tough streets of the Great Depression. After four days of filming, which reportedly included some nude scenes, Ransohoff disliked the rushes and immediately fired him. Eventually directed by Norman Jewison and starring Steve McQueen, the film went on to become a 1965 hit.\n\"Noon Wine\".\nPeckinpah caught a lucky break in 1966 when producer Daniel Melnick needed a writer and director to adapt Katherine Anne Porter's short novel \"Noon Wine\" for television. Melnick was a big fan of \"The Westerner\" and \"Ride the High Country\", and had heard Peckinpah had been unfairly fired from \"The Cincinnati Kid\". Against the objections of many within the industry, Melnick hired Peckinpah and gave him free rein. Peckinpah completed the script, which Porter enthusiastically endorsed, and the project became an hour-long presentation for \"ABC Stage 67\".\nTaking place in turn of the century West Texas, \"Noon Wine\" was a dark tragedy about a farmer's act of futile murder which leads to suicide. Starring Jason Robards and Olivia de Havilland, the film was a critical hit, with Peckinpah nominated by the Writers Guild for Best Television Adaptation and the Directors Guild of America for Best Television Direction. Robards kept a personal copy of the film in his private collection for years as he considered the project to be one of his most satisfying professional experiences. A rare film which had no home video release until 2014, \"Noon Wine\" is today considered one of Peckinpah's most intimate works, revealing his dramatic potential and artistic depth.\nInternational fame.\n\"The Wild Bunch\".\nThe surprising success of \"Noon Wine\" laid the groundwork for one of the most explosive comebacks in film history. In 1967, Warner Bros.-Seven Arts producers Kenneth Hyman and Phil Feldman were interested in having Peckinpah rewrite and direct an adventure film, \"The Diamond Story\". An alternative screenplay written by Roy N. Sickner and Walon Green was the Western \"The Wild Bunch\". At the time, William Goldman's screenplay \"Butch Cassidy and the Sundance Kid\" had recently been purchased by 20th Century Studios.\nIt was quickly decided that \"The Wild Bunch\", which had several similarities to Goldman's work, would be produced in order to beat \"Butch Cassidy\" to the theaters. By the fall of 1967, Peckinpah was rewriting the screenplay into what became \"The Wild Bunch\". Filmed on location in Mexico, Peckinpah's epic work was inspired by a number of forces\u2014his hunger to return to films, the violence seen in Arthur Penn's \"Bonnie and Clyde\", America's growing frustration with the Vietnam War, and what he perceived to be the utter lack of reality seen in Westerns up to that time. He set out to make a film which portrayed not only the vicious violence of the period, but the crude men attempting to survive the era. During this period, Peckinpah said that his life was changed by seeing Carlos Saura's \"La Caza\" (1966), which profoundly influenced his subsequent oeuvre.\nThe film detailed a gang of veteran outlaws on the Texas/Mexico border in 1913 trying to survive within a rapidly approaching modern world. \"The Wild Bunch\" is framed by two ferocious and infamous gunfights, beginning with a failed robbery of the railway company office and concluding with the outlaws battling the Mexican army in suicidal vengeance prompted by the brutal torture and murder of one of their members.\nIrreverent and unprecedented in its explicit detail, the 1969 film was an instant success. Multiple scenes attempted in \"Major Dundee\", including slow motion action sequences, characters leaving a village as if in a funeral procession and the use of inexperienced locals as extras, were perfected in \"The Wild Bunch\". Many critics denounced its violence as sadistic and exploitative. Other critics and filmmakers hailed the originality of its unique rapid editing style, created for the first time in this film and ultimately becoming a Peckinpah trademark, and praised the reworking of traditional Western themes. It was the beginning of Peckinpah's international fame, and he and his work remained controversial for the rest of his life. The film was ranked No. 80 on the American Film Institute's top 100 list of the greatest American films ever made and No. 69 as the most thrilling, but the controversy has not diminished.\n\"The Wild Bunch\" was re-released for its 25th anniversary, and received an NC-17 rating from the MPAA. Peckinpah received his only Academy Award nomination (for Best Original Screenplay) for this film.\n\"The Ballad of Cable Hogue\".\nDefying audience expectations, as he often did, Peckinpah immediately followed \"The Wild Bunch\" with the elegiac, funny and mostly non-violent 1970 Western \"The Ballad of Cable Hogue\". Using many of the same cast (L. Q. Jones, Strother Martin) and crew members of \"The Wild Bunch\", the film covered three years in the life of small-time entrepreneur Cable Hogue (Jason Robards) who decides to make his living by remaining in the desert after having miraculously discovered water when he had been abandoned there to die. He opens his business along a stagecoach line, only to see his dreams end with the appearance of the first automobile on the horizon.\nShot on location in the Valley of Fire in Nevada, the film was plagued by poor weather, Peckinpah's renewed drinking and his brusque firing of 36 crew members. The chaotic filming wrapped 19 days over schedule and $3 million over budget, effectively terminating his tenure with Warner Bros.-Seven Arts. In retrospect, it was a damaging career move as \"Deliverance\" and \"Jeremiah Johnson\", critical and enduring box office hits, were in development at the time and Peckinpah was considered the first choice to direct both films.\nLargely ignored upon its initial release, \"The Ballad of Cable Hogue\" has been rediscovered in recent years and is often held up by critics as exemplary of the breadth of Peckinpah's talents. They claim that the film proves Peckinpah's ability to make unconventional and original work without resorting to explicit violence. Over the years, Peckinpah cited the film as one of his favorites.\n\"Straw Dogs\".\nHis alienation from Warner Brothers once again left him with a limited number of directing jobs. Peckinpah traveled to England to direct \"Straw Dogs\" (1971), one of his darkest and most psychologically disturbing films. Produced by Daniel Melnick, who had previously worked with Peckinpah on \"Noon Wine\", the film's screenplay was based on the novel \"The Siege of Trencher's Farm\" by Gordon Williams.\nIt starred Dustin Hoffman as David Sumner, a timid American mathematician who leaves the chaos of college anti-war protests to live with his young wife Amy (Susan George) in her native village in Cornwall, England. Resentment of David's presence by the locals slowly builds to a shocking climax when the mild-mannered academic is forced to violently defend his home. Peckinpah rewrote the existing screenplay, inspired by the books \"African Genesis\" and \"The Territorial Imperative\" by Robert Ardrey, which argued that man was essentially a carnivore who instinctively battled over control of territory.\nThe character of David Sumner, taunted and humiliated by the violent town locals, is eventually cornered within his home where he loses control and kills several of the men during the violent conclusion. \"Straw Dogs\" deeply divided critics, some of whom praised its artistry and its confrontation of human savagery, while others attacked it as a misogynistic and fascistic celebration of violence.\nMuch of the criticism centered on Amy's complicated and lengthy rape scene, which Peckinpah reportedly attempted to base on his own personal fears rooted in past failed marriages. To this day, the scene is attacked by some critics as an ugly male-chauvinist fantasy. The film was for many years banned on video in the UK.\n\"Junior Bonner\".\nDespite his growing alcoholism and controversial reputation, Peckinpah was prolific during this period of his life. In May 1971, weeks after completing \"Straw Dogs\", he returned to the United States to begin work on \"Junior Bonner\". The lyrical screenplay by Jeb Rosenbrook, depicting the changing times of society and binding family ties, appealed to Peckinpah's tastes. He accepted the project, at the time concerned with being typed as a director of violent action. The film was his final attempt to make a low-key, dramatic work in the vein of \"Noon Wine\" and \"The Ballad of Cable Hogue\".\nFilmed on location in Prescott, Arizona, the story covered a week in the life of aging rodeo rider Junior \"JR\" Bonner (Steve McQueen) who returns to his hometown to compete in an annual rodeo competition. Promoted as a Steve McQueen action vehicle, the film's reviews were mixed and the film performed poorly at the box office. Peckinpah remarked, \"I made a film where nobody got shot and nobody went to see it.\" The film's reputation has grown over the years as many critics consider \"Junior Bonner\" to be one of Peckinpah's most sympathetic works, while also noting McQueen's earnest performance.\n\"The Getaway\".\nEager to work with Peckinpah again, Steve McQueen presented him Walter Hill's screenplay to \"The Getaway\". Based on the Jim Thompson novel, the gritty crime thriller detailed lovers on the run following a dangerous robbery. Both Peckinpah and McQueen needed a hit, and they immediately began working on the film in February 1972. Peckinpah had no pretensions about making \"The Getaway\", as his only goal was to create a highly polished thriller to boost his market value. McQueen played Doc McCoy, a convicted robber who colludes with corrupt businessman Jack Beynon (Ben Johnson) to be released from prison and later masterminds a bank heist organized by Beynon.\nA series of double-crosses ensues and Doc and his wife Carol (MacGraw) attempt to flee from their pursuers to Mexico. Replete with explosions, car chases and intense shootouts, the film became Peckinpah's biggest financial success to date earning more than $25 million at the box office. Though strictly a commercial product, Peckinpah's creative touches abound throughout, most notably during the intricately edited opening sequence when McQueen's character is suffering from the pressures of prison life. The film remains popular and was remade in 1994, starring Alec Baldwin and Kim Basinger.\nLater career.\nThe year 1973 marked the beginning of the most difficult period of Peckinpah's life and career. While still filming \"The Getaway\" in El Paso, Texas, Peckinpah sneaked across the border into Juarez in April 1972 and married Joie Gould. He had met Gould in England while filming \"Straw Dogs\", and she had since been his companion and a part-time crew member. Peckinpah's intake of alcohol had increased dramatically while making \"The Getaway\", and he became fond of saying, \"I can't direct when I'm sober.\" He began to have violent mood swings and explosions of rage, at one point assaulting Gould. After four months, she returned to England and filed for divorce. Devastated by the breakup, Peckinpah fell into a self-destructive pattern of almost continuous alcohol consumption, and his health was unstable for the remainder of his life.\n\"Pat Garrett and Billy the Kid\".\nIt was in this state of mind that Peckinpah agreed to make \"Pat Garrett and Billy the Kid\" (1973) for Metro-Goldwyn-Mayer. Based on the screenplay by Rudolph Wurlitzer, who previously penned \"Two-Lane Blacktop\", a film he admired, Peckinpah was convinced that he was about to make his definitive statement on the Western genre. The script offered Peckinpah the opportunity to explore themes that appealed to him: two former partners forced by changing times onto opposite sides of the law, manipulated by corrupt economic interests. Peckinpah rewrote the screenplay, establishing Pat Garrett and Billy the Kid as friends, and attempted to weave an epic tragedy from the historical legend. Filmed on location in the Mexican state of Durango, the film starred James Coburn and Kris Kristofferson in the title roles, with a huge supporting cast including Bob Dylan, who composed the film's music, Jason Robards, R. G. Armstrong, Richard Jaeckel, Jack Elam, Chill Wills, Katy Jurado, Matt Clark, L. Q. Jones, Rutanya Alda, Slim Pickens, and Harry Dean Stanton. From the beginning, Peckinpah clashed with MGM and its president James Aubrey, known for his stifling of creative interests and eventual dismantling of the historic movie company. Numerous production difficulties, including an outbreak of influenza and malfunctioning cameras, combined with Peckinpah's alcoholism, resulted in one of the most troubled productions of his career. Principal photography finished 21 days behind schedule and $1.6 million over budget. Enraged, Aubrey severely cut Peckinpah's film from 124 to 106 minutes, resulting in \"Pat Garrett and Billy the Kid\" being released in a truncated version largely disowned by cast and crew members. Critics complained that the film was incoherent, and the experience soured Peckinpah forever on Hollywood. In 1988, however, Peckinpah's director's cut was released on video and led to a reevaluation, with many critics hailing it as a mistreated classic and one of the era's best films. Filmmakers, including Martin Scorsese, have praised the film as one of the greatest modern Westerns.\n\"Bring Me the Head of Alfredo Garcia\".\nIn the eyes of his admirers, \"Bring Me the Head of Alfredo Garcia\" (1974) was the \"last true Peckinpah film.\" The director himself claimed it was his only film released exactly as he intended it. A project in development for many years and based on an idea by Frank Kowalski, Peckinpah wrote the screenplay with the assistance of Kowalski, Walter Kelley and Gordon Dawson. An alcohol-soaked fever dream involving revenge, greed and murder in the Mexican countryside, the film featured Bennie (Warren Oates) as a thinly disguised self-portrait of Peckinpah, and co-starred a burlap bag containing the severed head of a gigolo being sought by a Mexican patrone for having impregnated his young granddaughter. Bennie is offered a reward of ten thousand dollars for Alfredo's death or proof thereof, and Alfredo's head is demanded as proof the contract has been fulfilled. The macabre drama was part black comedy, action film and tragedy, with a warped edge rarely seen in Peckinpah's works. Most critics were repulsed, and it was listed in the book \"The 50 Worst Films of All Time\" by Harry Medved and Randy Dreyfuss. One of the few critics to praise the film was Roger Ebert, and the film's reputation has grown in recent years, with many noting its uncompromising vision as well as its anticipation of the violent black comedy of David Lynch and Quentin Tarantino. A failure at the box office, the film now has a cult following. In 1991, the UCLA film school's festival of great but forgotten American films included \"Bring Me the Head of Alfredo Garcia\". It is reportedly Takeshi Kitano's favorite film.\n\"The Killer Elite\".\nHis career now suffering from consecutive box office failures, Peckinpah once again was in need of a hit on the level of \"The Getaway\". For his next film, he chose \"The Killer Elite\" (1975), an action-filled espionage thriller starring James Caan and Robert Duvall as rival American agents. Filmed on location in San Francisco, Peckinpah allegedly discovered cocaine for the first time thanks to Caan and his entourage. This led to increased paranoia and his once legendary dedication to detail deteriorated. Producers also refused to allow Peckinpah to rewrite the screenplay for the first time since his debut film \"The Deadly Companions\". Frustrated, the director spent large amounts of time in his on-location trailer, allowing assistants to direct many scenes. At one point he overdosed on cocaine, ending up in a hospital with a second pacemaker. The film was reasonably successful at the box office, although most critics panned it. Today, the film is considered one of Peckinpah's weakest films, and an example of his decline as a major director.\n\"Cross of Iron\".\nStill renowned in 1975, Peckinpah was offered the opportunity to direct the eventual blockbusters \"King Kong\" (1976) and \"Superman\" (1978). He turned down both offers and chose instead the bleak and vivid World War II drama \"Cross of Iron\" (1977). The screenplay was based on a novel about a platoon of German soldiers in 1943 on the verge of utter collapse on the Taman Peninsula on the Eastern Front. The German production was filmed in Yugoslavia. Working with James Hamilton and Walter Kelley, Peckinpah rewrote the screenplay and screened numerous Nazi documentaries in preparation. Almost immediately, Peckinpah realized he was working on a low-budget production, as he had to spend $90,000 of his own money to hire experienced crew members. While not suffering from the cocaine abuse which marked \"The Killer Elite\", Peckinpah continued to drink heavily, causing his direction to become confused and erratic. The production abruptly ran out of funds, and Peckinpah was forced to completely improvise the concluding sequence, filming the scene in one day. Co-starring James Mason, Maximilian Schell, David Warner and Senta Berger, \"Cross of Iron\" was noted for its opening montage utilizing documentary footage as well as the visceral impact of the unusually intense battle sequences. The film was a huge box office success in Europe, inspiring the sequel \"Breakthrough\" starring Richard Burton. \"Cross of Iron\" was reportedly a favorite of Orson Welles, who said that after \"All Quiet on the Western Front\" it was the finest anti-war film he had ever seen. The film performed poorly in the U.S., ultimately eclipsed by \"Star Wars\", though today it is highly regarded and considered the last instance of Peckinpah's once-great talent.\n\"Convoy\".\nHoping to create a blockbuster, Peckinpah decided to take on \"Convoy\" (1978). His associates were perplexed, as they felt his choice to direct such substandard material was a result of his renewed cocaine use and continued alcoholism. Based on the hit song by C. W. McCall, the film was an attempt to capitalize on the huge success of \"Smokey and the Bandit\" (1977). In spite of his addictions, Peckinpah felt compelled to turn the genre exercise into something more significant. Unhappy with the screenplay written by B. W. L. Norton, Peckinpah tried to encourage the actors to re-write, improvise and ad-lib their dialogue. In another departure from the script, Peckinpah attempted to add a new dimension by casting a pair of black actors as members of the convoy, Madge Sinclair as Widow Woman and Franklyn Ajaye as Spider Mike. Filmed in New Mexico and starring Kris Kristofferson, Ali MacGraw and Ernest Borgnine, \"Convoy\" turned out to be yet another troubled Peckinpah production, with the director's health a continuing problem. Friend and actor James Coburn was brought in to serve as second unit director, and he filmed many of the scenes while Peckinpah remained in his on-location trailer. The film wrapped in September 1977, 11 days behind schedule and $5 million over budget. Surprisingly, \"Convoy\" was the highest-grossing picture of Peckinpah's career, notching $46.5 million at the box office, but was panned by many critics, leaving his reputation seriously damaged. For the first time in almost a decade, Peckinpah finished a picture and found himself unemployed.\n2nd unit work on \"Jinxed!\".\nFor the next three years, Peckinpah remained a professional outcast. But during the summer of 1981, his original mentor Don Siegel gave him a chance to return to filmmaking. While shooting \"Jinxed!\", a comedy drama starring Bette Midler and Rip Torn, Siegel asked Peckinpah if he would be interested in directing 12 days of second unit work. Peckinpah immediately accepted, and his earnest collaboration, while uncredited, was noted within the industry. For the final time, Peckinpah found himself back in the directing business.\n\"The Osterman Weekend\".\nBy 1982, Peckinpah's health was poor. Producers Peter S. Davis and William N. Panzer were undaunted, as they felt that having Peckinpah's name attached to \"The Osterman Weekend\" (1983) would lend the suspense thriller an air of respectability. Peckinpah accepted the job but reportedly hated the convoluted screenplay based upon Robert Ludlum's novel, which he also disliked. Multiple actors in Hollywood auditioned for the film, intrigued by the opportunity. Many of those who signed on, including John Hurt, Burt Lancaster and Dennis Hopper, did so for less than their usual salaries for a chance to work with the legendary director. By the time shooting wrapped in January 1983 in Los Angeles, Peckinpah and the producers were hardly speaking. Nevertheless, Peckinpah brought the film in on time and on budget, delivering his director's cut to the producers. Davis and Panzer were unhappy with Peckinpah's version, which included an opening sequence of two characters making love. The producers changed the opening and also deleted other scenes they deemed unnecessary. Peckinpah's final film was critically panned. It grossed $6.5 million in the United States (nearly recouping its budget) and did well in Europe and on the new home-video market.\nJulian Lennon music videos.\nPeckinpah's last work as a filmmaker was undertaken two months before his death. He was hired by producer Martin Lewis to shoot two music videos featuring Julian Lennon\u2014\"Valotte\" and \"Too Late For Goodbyes.\" The critically acclaimed videos led to Lennon's nomination for Best New Video Artist at the 1985 MTV Video Music Awards.\nReception and commentary.\nCritic for \"The New Yorker\", Pauline Kael was a fan of Pekinpah's but gave insight into his career when she wrote: \"With his history of butchered films and films released without publicity or being fired and blacklisted for insubordination, of getting ornerier and ornerier, Pekinpah has lost a lot of blood.\" She goes on to describe his work: \"He doesn't do the expected, and so, scene by scene, he creates his own actor-director's suspense.\"\nFilmography.\nTelevision.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "27643", "revid": "51028733", "url": "https://en.wikipedia.org/wiki?curid=27643", "title": "Shanghai", "text": "Municipality and largest city in China\nShanghai is a direct-administered municipality and the most populous urban area in China. The city is located on the Chinese shoreline on the southern estuary of the Yangtze River, with the Huangpu River flowing through it. The population of the city proper is the second largest in the world with around 24.87 million inhabitants in 2023, while the urban area is the most populous in China, with 29.87 million residents. As of 2022, the Greater Shanghai metropolitan area was estimated to produce a gross metropolitan product (nominal) of nearly 13 trillion RMB ($1.9 trillion). Shanghai is one of the world's major centers for finance, business and economics, research, science and technology, manufacturing, transportation, tourism, and culture. The Port of Shanghai is the world's busiest container port.\nOriginally a fishing village and market town, Shanghai grew to global prominence in the 19th century due to domestic and foreign trade and its favorable port location. The city was one of five treaty ports forced to open to trade with the Europeans after the First Opium War, with the Shanghai International Settlement and French Concession subsequently established. The city became a primary commercial and financial hub of Asia in the 1930s. During the Second World War, it was the site of the Battle of Shanghai. This was followed by the Chinese Civil War with the Communists taking over the city and most of the mainland. During the Cold War, trade was mostly limited to other socialist countries in the Eastern Bloc, causing the city's global influence to decline.\nEconomic reforms supported by Deng Xiaoping led to extensive redevelopment by the 1990s, particularly in the Pudong New Area, spurring the return of finance and foreign investment. The city has re-emerged as a hub for international trade and finance. It is the home of the Shanghai Stock Exchange, the largest stock exchange in the Asia-Pacific by market capitalization and the Shanghai Free-Trade Zone, the first free-trade zone in mainland China. It is ranked eighth globally on the Global Financial Centres Index. Shanghai has been classified as an Alpha+ (global first-tier) city by the Globalization and World Cities Research Network. As of 2024, it is home to 13 companies of the \"Fortune\" Global 500\u2014the fourth-highest number of any city. Shanghai is the world's second largest city by scientific outputs and home to several highly ranked universities, including Fudan, Shanghai Jiao Tong and Tongi. The Shanghai Metro, first opened in 1993, is the largest metro network in the world by route length.\nShanghai has been described as a global finance and innovation hub, and it is one of the ten biggest economic hubs in the world. Featuring several architectural styles such as Art Deco and shikumen, the city contains the Lujiazui skyline, and museums and historic buildings such as the City God Temple, Yu Garden, the China Pavilion and buildings along the Bund. Shanghai is known for its cuisine, local language, and cosmopolitan culture. It ranks sixth in the list of cities with the most skyscrapers.\nEtymology.\n The two Chinese characters in the city's name are (/\"zaon\", \"upon\") and (/\"h\u00e9\", \"sea\"), together meaning \"On the Sea\". The earliest occurrence of this name is the 11th-century Song dynasty, when there was a river confluence and a town with this name in the area. Others contend that the city is referenced in historical records dating back 2150 years, and that its ancient name, \"Hu\", suggests it was a fishing village. In 1280 it was renamed \"Shanghai\", which translates to \"Above the Sea\". The name's interpretation was disputed, but Chinese historians concluded that during the Tang dynasty, the area of modern-day Shanghai was under sea level, so the land appeared to be \"on the sea\".\nShanghai is officially abbreviated (/\"wu\") in Chinese, a contraction of (/\"wu-doq\", \"Harpoon Ditch\"), a 4th- or 5th-century Jin name for the mouth of Suzhou Creek when it was the main conduit into the ocean. This character appears on motor vehicle license plates issued in the municipality.\nAlternative names.\n (\"Sh\u0113n\"/\"s\u00e9n\") or (\"Sh\u0113nch\u00e9ng\"/\"s\u00e9n-zen\", \"Shen City\") was an early name originating from Lord Chunshen, a 3rd-century\u00a0BC nobleman and prime minister of the state of Chu, whose fief included modern Shanghai. (\"Hu\u00e1t\u00edng\"/\"gho-din\") was another early name for Shanghai. In AD\u00a0751, Huating County was established as the first county-level administration within modern-day Shanghai by Zhao Juzhen, the governor of Wu Commandery.\n (\"M\u00f3d\u016b\"/\"m\u00f3-tu\", \"monster/fiend/magical city\"), is a contemporary nickname for Shanghai. The name was first mentioned in \"Mato\" (1924) by Japanese novelist Sh\u014dfu Muramatsu. The city has various English nicknames including the \"New York of China\", in reference to its status as a cosmopolitan megalopolis and financial hub, the \"Pearl of the Orient\", and the \"Paris of the East\".\nHistory.\nAntiquity.\nThe western part of modern-day Shanghai was inhabited 6,000 years ago. During the Spring and Autumn period (approximately 771 to 476 BC), it belonged to the Kingdom of Wu, which was conquered by the Kingdom of Yue, which in turn was conquered by the Kingdom of Chu. During the Warring States period (475 BC), Shanghai was part of the fief of Lord Chunshen of Chu, one of the Four Lords of the Warring States. He ordered the excavation of the Huangpu River. Its former or poetic name, the Chunshen River, gave Shanghai its nickname of \"Sh\u0113n\". Fishermen living in the Shanghai area then created a fish tool called the \"h\u00f9\", which lent its name to the outlet of Suzhou Creek north of the Old City and became a common nickname and abbreviation for the city.\nImperial era.\nDuring the Tang and Song dynasties, Qinglong Town () in modern Qingpu District was a major trading port. Established in 746, it developed into what was historically called a \"giant town of the Southeast\". The port experienced thriving trade with provinces along the Yangtze and the Chinese coast, as well as foreign countries such as Japan and Silla. By the end of the Song dynasty, the center of trading had moved downstream of the Wusong River to Shanghai. Its status was upgraded from a village to a market town in 1074; in 1172, a second sea wall was built to stabilize the ocean coastline, supplementing an earlier dike. From the Yuan dynasty in 1292 until Shanghai officially became a municipality in 1927, central Shanghai was administered as a county under Songjiang Prefecture, which had its seat in the present-day Songjiang District.\nShanghai's first city wall was built in 1554 to protect the town from raids by Japanese pirates. It was high and in circumference. A City God Temple was built in 1602 during the Wanli reign. This honor was usually reserved for prefectural capitals and not normally given to a county seat like Shanghai. Scholars theorized that this reflected the town's economic importance.\nDuring the Qing dynasty, two central government policy changes caused Shanghai to become one of the most important seaports in the Yangtze Delta region. The first was in 1684, when the Kangxi Emperor reversed the 1525 prohibition on oceangoing vessels. In 1732, the Qianlong Emperor moved the customs office for Jiangsu province (\u6c5f\u6d77\u5173; see Customs House, Shanghai) from Songjiang to Shanghai, and gave Shanghai exclusive control over customs collections for Jiangsu's foreign trade. Shanghai became the major trade port for the lower Yangtze region by 1735, despite being at the lowest administrative level in the political hierarchy.\nIn the 19th century, international attention and recognition of its economic and trade potential at the Yangtze grew. British forces occupied the city during the First Opium War. The war ended in 1842 with the Treaty of Nanking, which opened Shanghai as one of the five treaty ports for international trade. The Treaty of the Bogue, the Treaty of Wanghia, and the Treaty of Whampoa, signed between 1843 and 1844, forced Chinese concession to European and American desires for visitation and trade in China. Britain, France, and the United States established a presence outside the walled city of Shanghai, which remained under the direct administration of the Chinese.\nThe Chinese-held Old City of Shanghai fell to rebels from the Small Swords Society in 1853, but was regained by the Qing government in February 1855. In 1854, the Shanghai Municipal Council was created to manage the foreign settlements. Between 1860 and 1862, the Taiping rebels twice attacked Shanghai and destroyed the city's eastern and southern suburbs, but failed to take the city. In 1863, the British settlement south of Suzhou Creek (northern Huangpu District) and the American settlement to the north (southern Hongkou District) joined to form the Shanghai International Settlement. The French opted out of the Shanghai Municipal Council and maintained its own concession at the city's south and southwest. The First Sino-Japanese War concluded with the 1895 Treaty of Shimonoseki, which elevated Japan as another foreign power in Shanghai. Japan built the first factories in Shanghai, which were copied by other foreign powers. This international activity gave Shanghai the nickname \"the Great Athens of China\".\nRepublic era.\nIn 1912, the Old City walls were dismantled as they blocked the city's expansion. In July 1921, the Chinese Communist Party was founded in the Shanghai French Concession. On 30 May 1925, the May Thirtieth Movement broke out when a worker in a Japanese-owned cotton mill was shot and killed by a Japanese foreman. Workers in the city then launched general strikes against imperialism, which became nationwide protests that gave rise to Chinese nationalism.\nThe golden age of Shanghai began with its elevation to municipality after it was separated from Jiangsu on 7 July 1927. This new Chinese municipality was , and included the districts of Baoshan, Yangpu, Zhabei, Nanshi, and Pudong. Headed by a Chinese mayor and municipal council, the city's government implemented the Greater Shanghai Plan to create a new city center in Jiangwan town of Yangpu district, outside the boundaries of the foreign concessions. The city became a commercial and financial hub of the Asia-Pacific region in the 1930s. During the ensuing decades, citizens of many countries immigrated to Shanghai; those who stayed for long periods\u2060\u2060 called themselves \"Shanghailanders\". In the 1920s and 1930s, almost 20,000 White Russians fled the newly established Soviet Union to reside in Shanghai. These Shanghai Russians constituted the second-largest foreign community. By 1932, Shanghai had become the world's fifth-largest city and home to 70,000 foreigners. In the 1930s, approximately 30,000 Jewish refugees from Europe arrived in the city.\nJapanese invasion.\nOn 28 January 1932, Japanese military forces invaded Shanghai. More than 10,000 shops and hundreds of factories and public buildings were destroyed, leaving Zhabei district ruined. About 18,000 civilians were either killed, injured, or declared missing. A ceasefire was brokered on 5 May. In 1937, the Battle of Shanghai resulted in the occupation of the Chinese-administered parts of Shanghai outside of the International Settlement and the French Concession. People who stayed in the occupied city experienced hunger, oppression, or death. The foreign concessions were occupied by the Japanese on 8 December 1941 and remained occupied until Japan's surrender in 1945.\nMany Jewish people arrived in Shanghai during the Japanese occupation period. A vice-consul for Japan in Lithuania, Chiune Sugihara, issued thousands of visas to Jewish refugees escaping the Holocaust, and the Japanese government transferred many of them to Shanghai by November 1941. Other Jewish refugees traveled from Italy. The refugees from Europe were interned in the Shanghai Ghetto in Hongkou District after the Japanese attack on Pearl Harbor. After the surrender of Japan, the Chinese Army liberated the Ghetto, and most of the Jews left over the next few years.\nPeople's Republic era.\nOn 27 May 1949, the People's Liberation Army took control of Shanghai through the Shanghai Campaign. Under the new People's Republic of China (PRC), Shanghai was one of only three municipalities not merged into neighboring provinces (the others being Beijing and Tianjin). Most foreign firms moved their offices from Shanghai to Hong Kong, as part of a foreign divestment due to the PRC's victory.\nAfter the war, Shanghai's economy was restored. From 1949 to 1952, the city's agricultural and industrial output increased by 51.5% and 94.2%, respectively. As the industrial center of China with the most skilled industrial workers, Shanghai became a center for radical leftism during the 1950s and 1960s. During the Cultural Revolution (1966\u20131976), Shanghai's society was severely damaged. The majority of the workers in the Shanghai branch of the People's Bank of China were Red Guards, and they formed a group called the Anti-Economy Liaison Headquarters within the branch.38 The Anti-Economy Liaison Headquarters dismantled economic organizations in Shanghai, investigated bank withdrawals, and disrupted regular bank service in the city.38 Despite the disruptions of the Cultural Revolution, Shanghai maintained economic production with a positive annual growth rate.\nIn 1990, Deng Xiaoping permitted Shanghai to initiate economic reforms, which reintroduced foreign capital to the city and developed the Pudong district, resulting in the birth of Lujiazui. That year, the China's central government designated Shanghai as the \"Dragon Head\" of economic reform. In 2022 Shanghai experienced a large outbreak of COVID-19 cases and the Chinese government locked down the entire city on 5 April. This resulted in widespread food shortages across the city as food-supply chains were severely disrupted. These restrictions were lifted on 1 June.\nGeography.\nShanghai is located on the Yangtze Estuary of China's east coast, with the Yangtze River to the north and Hangzhou Bay to the south, with the East China Sea to the east. The land is formed by the Yangtze's natural deposition and modern land reclamation projects. It has sandy soil, and skyscrapers have to be built with deep concrete piles to avoid sinking into the soft ground. The provincial-level Municipality of Shanghai administers the estuary and many of its surrounding islands. It borders the provinces of Zhejiang to the south and Jiangsu to the west and north. The municipality's northernmost point is on Chongming Island, the second-largest island in mainland China after its expansion during the 20th century.\nShanghai is located on an alluvial plain and the vast majority of its land area is flat, with an average elevation of . Tidal flat ecosystems exist around the estuary, but they have been reclaimed for agricultural purposes. The city's few hills, such as She Shan, lie to the southwest; its highest point is the peak of Dajinshan Island () in Hangzhou Bay. Shanghai has rivers, canals, streams, and lakes, and it is known for its rich water resources as part of the Lake Tai drainage basin.\nDowntown Shanghai is bisected by the Huangpu River, a man-made tributary of the Yangtze created by order of Lord Chunshen during the Warring States period. The historic center of the city was located on the west bank of the Huangpu (Puxi), near the mouth of Suzhou Creek, connecting it with Lake Tai and the Grand Canal. The central financial district, Lujiazui, was established on the east bank of the Huangpu (Pudong). Along Shanghai's eastern shore, the destruction of local wetlands due to the construction of Pudong International Airport has been partially offset by the protection and expansion of a nearby shoal, Jiuduansha, as a nature preserve.\nClimate.\nShanghai has a humid subtropical climate (K\u00f6ppen: \"Cfa\"), with an average annual temperature of for downtown areas and for suburbs. The city experiences four distinct seasons. Winters are temperate to cold and damp\u2014northwesterly winds from Siberia can cause nighttime temperatures to drop below freezing. Each year, there are an average of 4.7 days with snowfall and 1.6 days with snow cover. Summers are hot and humid, and occasional downpours or thunderstorms can be expected. On average, 14.5 days exceed annually. In summer and the beginning of autumn, the city is susceptible to typhoons.\nThe most pleasant seasons are generally spring, although changeable and often rainy, and autumn, which is usually sunny and dry. With monthly percent possible sunshine ranging from 28% in June to 46% in August, the city receives 1,754 hours of bright sunshine annually. According to China's seasonal division standard, from 2001 to 2025, Shanghai enters spring on 9 March, summer on 15 May, autumn on 5 October, and winter on 4 December. The average temperature for the three weeks from 19 July to 8 August is above . Extremes since 1951 have ranged from on 31 January 1977 (unofficial record of was set on 19 January 1893) to on 21 July 2017 and 13 July 2022 at a weather station in Xujiahui. It also has as the highest ever daily minimum temperature at Xujiahui on 2 August 2024.\nCityscape.\nThe Bund, located by the bank of the Huangpu River, is home to a row of early 20th-century architecture, ranging in style from the neoclassical HSBC Building to the Art Deco Sassoon House (now part of the Peace Hotel). The area has been revitalized several times: the first was in 1986, with a new promenade by the Dutch architect Paulus Snoeren. The second was before the 2010 Expo, which includes restoration of the century-old Waibaidu Bridge and reconfiguration of traffic flow.\nShanghai's construction boom during the 1920s and 1930s caused the city to have several Art Deco buildings. L\u00e1szl\u00f3 Hudec, a Hungarian-Slovak who lived in the city between 1918 and 1947, designed Art Deco buildings such as the Park Hotel, the Grand Cinema, and the Paramount. Other prominent Art Deco-style architects are Clement Palmer and Arthur Turner, who designed the Peace Hotel, the Metropole Hotel, and the Broadway Mansions; and Austrian architect C.H. Gonda, who designed the Capitol Theatre. One common architectural element is the shikumen (, \"stone storage door\") residence, typically two- or three-story gray brick houses with the front yard protected by a heavy wooden door in a stylistic stone arch. Each residence is connected and arranged in straight alleys, known as longtang (). Shanghai also has Soviet neoclassical architecture or Stalinist architecture: most were erected between the founding of the People's Republic in 1949 and the Sino-Soviet Split in the late 1960s when Soviet personnel came to China to aid in the development of a communist state. An example of Soviet neoclassical architecture in Shanghai is the Shanghai Exhibition Center.\nShanghai has making it the fifth city in the world with the most skyscrapers. Some of Shanghai's skyscrapers include the Jin Mao Tower, the Shanghai World Financial Center, and the Shanghai Tower, which was completed in 2015 and is currently the tallest building in China and the third tallest in the world. The Oriental Pearl Tower, at , is located nearby at the northern tip of Lujiazui. Many areas in the former foreign concessions are well-preserved. Despite rampant redevelopment, the Old City retains traditional architecture and designs, such as the Yu Garden, an elaborate Jiangnan style garden.\nPolitics.\nStructure.\nLike all governing institutions in mainland China, Shanghai has a parallel party-government system, in which the CCP Committee Secretary, officially termed the Chinese Communist Party Shanghai Municipal Committee Secretary, outranks the Mayor. The CCP committee acts as the top policy-formulation body, and typically composed of 12 members (including the secretary); it has control over the Shanghai Municipal People's Government.\nPolitical power in Shanghai has been a stepping stone to higher positions in the central government. Since Jiang Zemin became the General Secretary of the Chinese Communist Party in June 1989, several former Shanghai party secretaries and deputy party secretaries were elevated to the Politburo Standing Committee, the \"de facto\" highest decision-making body in China. Officials with ties to the Shanghai administration collectively form a powerful faction in the central government known as the Shanghai Clique, which has often been viewed as competing against the rival Youth League Faction over personnel appointments and policy decisions.\nAdministrative divisions.\nShanghai is one of the four municipalities under the direct administration of the Central People's Government, and is divided into 16 districts. These are further divided to 108 subdistricts, 106 towns and 2 townships.\nWhen the Shanghai Municipal People's Government was founded in 1949, the land area governed was , largely located within the present-day Outer Ring Expressway. In 1958, ten counties were reassigned under Shanghai from Jiangsu. District reorganizations saw several counties in the suburbs become districts between 1988 and 2015, and Chongming was the last county to be retitled as a district in 2015.\nShanghai also administers several enclaves in Jiangsu and Anhui provinces. Local residents hold Shanghai \"household registration\" and enjoy benefits identical to Shanghai residents.\nEconomy.\nThe city is a global center for finance and innovation, and a national center for commerce, trade, and transportation, with the world's busiest container port\u2014the Port of Shanghai. As of 2022, the Greater Shanghai metropolitan area, which includes Suzhou, Wuxi, Nantong, Ningbo, Jiaxing, Zhoushan, and Huzhou, was estimated to produce a gross metropolitan product (nominal) of nearly 13 trillion RMB ($1.9 trillion). As of 2020, the economy of Shanghai was estimated to be $1 trillion (PPP), ranking the most productive metro area of China and among the top ten largest metropolitan economies in the world. Shanghai's six largest industries\u2014retail, finance, IT, real estate, machine manufacturing, and automotive manufacturing\u2014comprise about half the city's GDP.\nAs of 2024[ [update]], Shanghai had a GDP of CN\u00a5 ($757 billion in nominal; $1.52 trillion in PPP) that makes up 4% of China's GDP, and a GDP per capita of CN\u00a5 (US$ in nominal; US$ in PPP). In 2022, the average annual disposable income of Shanghai's residents was CN\u00a5 (US$) per capita, while the average annual salary of people employed in urban units in Shanghai was CN\u00a5 (US$), making it one of the wealthiest cities in China, but also the most expensive city in mainland China to live in according to a 2023 study by the Economist Intelligence Unit. According to Julius Baer's Global Wealth and Lifestyle Report, Shanghai was the most expensive city in the world for living a luxurious lifestyle in 2021.\nIn 2023, the city's imports and exports reached CN\u00a57.73 trillion (US$1.07 trillion), accounting for 18.5% of the national total. In 2022, Shanghai was ranked fifth-highest in the number of billionaires by Forbes. Shanghai's nominal GDP was projected to reach US$1.3 trillion in 2035 (ranking first in China), making it one of the world's top 5 major cities in terms of GRP according to a study by Oxford Economics. As of August 2024, Shanghai ranked 4th in the world and 2nd in Greater China (after Beijing) by the largest number of the \"Fortune\" Global 500 companies.\nIn the last two decades, Shanghai has been one of the fastest-developing cities in the world; it has recorded double-digit GDP growth in almost every year between 1992 and 2008, before the 2008 financial crisis.\nFinance.\nShanghai is a global financial center, ranking third in Asia and eighth globally on the Global Financial Centres Index. Shanghai is also a large hub of the Chinese and global technology industry and home to a large startup ecosystem. As of 2021, the city was ranked as the 2nd Fintech powerhouse in the world after New York City.\nAs of 2019[ [update]], the Shanghai Stock Exchange had a market capitalization of US$, making it the largest stock exchange in China and the fourth-largest stock exchange in the world. In 2009, the trading volume of six key commodities\u2014including rubber, copper, and zinc\u2014on the Shanghai Futures Exchange all ranked first globally. By the end of 2017, Shanghai had 1,491 financial institutions, of which 251 were foreign-invested.\nManufacturing.\nAs one of the main industrial centers of China, Shanghai plays a key role in domestic manufacturing and heavy industry. Several industrial zones\u2014including Shanghai Hongqiao Economic and Technological Development Zone, Jinqiao Export Economic Processing Zone, Minhang Economic and Technological Development Zone, and Shanghai Caohejing High-Tech Development Zone\u2014are backbones of Shanghai's secondary sector. Shanghai is home to China's largest steelmaker Baosteel Group, China's largest shipbuilding base Hudong\u2013Zhonghua Shipbuilding Group, and one of China's oldest shipbuilders, the Jiangnan Shipyard. In auto manufacturing, the Shanghai-based SAIC Motor is one of the three largest automotive corporations in China, and has strategic partnerships with Volkswagen and General Motors. The company ranked 84 on the Fortune Global 500 list in 2023.\nTourism.\nIn 2017, the number of domestic tourists to the city increased by 7.5% to 318 million, while the number of overseas tourists increased by 2.2% to 8.73 million. In 2017, Shanghai was the highest earning tourist city in the world. According to the International Congress and Convention Association, Shanghai hosted 82 international meetings in 2018, a 34% increase from 61 in 2017. As of 2023[ [update]], it had 57 five-star hotels, 52 four star hotels, 1,942 travel agencies, 144 rated tourist attractions, and 34 red tourist attractions. In 2023, Shanghai had 3.64 million tourists, a 4.8-fold growth compared to 2022. It generated CN\u00a5177.12 billion (US$24.53 billion) in value, a 98.5% increase from the previous year. The number of foreign tourists reached 2.41 million, with a 5.2-fold increase.\nFree-trade zone.\nIn September 2013, the city launched the Shanghai Free-Trade Zone\u2014the first free-trade zone in mainland China. It introduced several reforms to incentivize foreign investment. \"The Banker\" reported that Shanghai attracted the highest volumes of financial sector foreign direct investment in the Asia-Pacific region in 2013. As of October 2019[ [update]], it is the second largest free-trade zone in mainland China in terms of land area (behind Hainan Free Trade Zone) covering an area of and integrating four existing bonded zones\u2014Waigaoqiao Free Trade Zone, Waigaoqiao Free Trade Logistics Park, Yangshan Free Trade Port Area, and Pudong Airport Comprehensive Free Trade Zone. Commodities entering the zone are exempt from duty and customs clearance.\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nAs of 2023[ [update]], Shanghai had a population of 24,874,500, including 14,801,700 (59.5%) hukou holders (registered locally). As of 2022[ [update]], 89.3% of Shanghai's population lives in urban areas, and 10.7% live in rural areas. Based on the population of its total administrative area, Shanghai is the second largest of the four municipalities of China, behind Chongqing, but is generally considered the largest Chinese city because the urban population of Chongqing is much smaller. According to the OECD, Shanghai's metropolitan area has an estimated population of 34 million.\nAccording to the Shanghai Municipal Statistics Bureau, about 157,900 residents in Shanghai are foreigners, including 28,900 Japanese, 21,900 Americans, and 20,800 Koreans. The actual number of foreign citizens in the city is probably much higher. Shanghai is also a domestic immigration city\u201440.3% (9.8 million) of the city's residents are from other regions of China.\nShanghai has a life expectancy of 83.18 years for the city's registered population, the highest life expectancy of all cities in mainland China. This has also caused the city to experience population aging\u2014in 2021, 17.4% (4.3 million) of the city's registered population was aged 65 or above. In 2017, the Chinese government implemented population controls for Shanghai, resulting in a population decline of 10,000 people by the end of the year.\nReligion.\nDue to its cosmopolitan history, Shanghai has a blend of religious heritage; religious buildings and institutions are scattered around the city. According to a 2012 survey, 13.1% of the city's population belongs to organized religions, including Buddhists with 10.4%, Protestants with 1.9%, Catholics with 0.7%, and other faiths with 0.1%. The remaining 86.9% of the population could be either atheists or involved in worship of nature deities and ancestors or folk religious sects.\n&lt;templatestyles src=\"Pie chart/styles.css\"/&gt;\nBuddhism, in its Chinese varieties, has had a presence in Shanghai since the Three Kingdoms period, during which the Longhua Temple\u2014the largest temple in Shanghai\u2014and the Jing'an Temple were founded. As of 2014[ [update]], Buddhism in Shanghai had 114 temples, 1,182 clergical staff, and 453,300 registered followers. The religion also has its own college, the Shanghai Buddhist College, and its own press, Shanghai Buddhological Press.\nCatholicism was brought into Shanghai in 1608 by Italian missionary Lazzaro Cattaneo. The Apostolic Vicariate of Shanghai was erected in 1933, and was further elevated to the Diocese of Shanghai in 1946. The St. Ignatius Cathedral in Xujiahui is the largest Catholic church in the city. Shanghai has the highest concentration of urban Catholics in China.38 Other forms of Christianity in Shanghai include Eastern Orthodox minorities and, since 1996, registered Christian Protestant churches. The Protestant All Saints Church in Huangpu was built in 1925 and features a Neo-Romanesque tower. \nProminent Jewish families immigrated to Shanghai when the Treaty of Nanking opened the city to Western populations. During World War II, thousands of Jews emigrated to Shanghai to flee Nazi Germany. They lived in a designated area called the Shanghai Ghetto and formed a community centered on the Ohel Moishe Synagogue, (now the Shanghai Jewish Refugees Museum). In 1939, Horace Kadoorie, the head of the powerful philanthropic Sephardic Jewish family in Shanghai, founded the Shanghai Jewish Youth Association to support Jewish refugees through English education so they would be prepared to emigrate from Shanghai.\nIslam came into Shanghai during the Yuan dynasty. The city's first mosque, Songjiang Mosque, was built during the Zhizheng () era under Emperor Huizong (reigned 1333 \u2013 1368). Shanghai's Muslim population increased in the 19th and early 20th centuries (when the city was a treaty port), during which time many mosques\u2014including the Xiaotaoyuan Mosque, the Huxi Mosque, and the Pudong Mosque\u2014were built. The Shanghai Islamic Association is located in the Xiaotaoyuan Mosque in Huangpu. According to the 2010 census of China, there are an estimated 85,000 Muslims in Shanghai.\nShanghai has several folk religious temples, including the City God Temple at the heart of the Old City, the Dajing Ge Pavilion dedicated to the Three Kingdoms general Guan Yu, the Confucian Temple of Shanghai, and a major Taoist center Shanghai White Cloud Temple where the Shanghai Taoist Association locates.\nLanguage.\nThe vernacular language spoken in the city is Shanghainese, part of the Taihu Wu subgroup of the Wu Chinese language family. This is different from the national language, Mandarin, which is mutually unintelligible with Wu Chinese. Modern Shanghainese derives from the indigenous Wu spoken in the former Songjiang prefecture but has been influenced by other dialects of Taihu Wu, most notably Suzhounese, and Ningbonese.\nBefore its expansion, the language spoken in Shanghai was not as prominent as those spoken around Jiaxing and later Suzhou, and was known as \"the local tongue\" (), a name which is now used in suburbs only. In the late 19th century, downtown Shanghainese ( or simply ) appeared, undergoing rapid changes and replacing Suzhounese as the prestige dialect of the Yangtze River Delta region. At the time, most immigration into the city came from the two adjacent provinces, Jiangsu and Zhejiang, the local dialects of which had the greatest influence on Shanghainese. After 1949, Putonghua (Standard Mandarin) also had an impact on Shanghainese because it was promoted by the government. Since the 1990s, many migrants outside of the Wu-speaking region come to Shanghai for education and jobs; they often cannot speak the local language and use Putonghua (Mandarin) as a lingua franca. Because Putonghua and English were more favored, Shanghainese began to decline, and fluency among young speakers weakened. In recent years, there have been movements within the city to promote the local language and protect it from fading out.\nEducation and research.\nShanghai is an international center of research and development and as of 2025, it was ranked second globally (after Beijing) by scientific research outputs, as tracked by the Nature Index. When compared to other countries, Shanghai ranked higher than France and nearly on par with Japan, securing sixth place globally after China, the US, Germany, the United Kingdom, and Japan, according to the Nature Index for 2025. For instance, Shanghai's share of the 2024 Nature Index is 3,153.61, with a count of 6,680, while Japan's share is 3,185.39, with 5,555 counts.\nAs of 2023, Shanghai had 68 universities and colleges, ranking first in East China region as a city with most higher education institutions. The city government's education agency is the Shanghai Municipal Education Commission.\nShanghai has 15 universities listed in 147 Double First-Class Universities, ranking second nationwide among Chinese cities (after Beijing). According to the U.S. News &amp; World Report Best Global University Ranking for 2025\u201326, Shanghai had the third highest concentration of universities among all major cities in the world included in the ranking, totaling 22, with three in the top 125 and six in the global top 500. In the 2025 Academic Ranking of World Universities, Shanghai had two in the top 40, three in the top 150 and nine in the top 500. Some of these universities were selected as \"985 universities\" or \"211 universities\" since the 90s by the Chinese government to build world-class universities.\nShanghai has two members (Fudan University and Shanghai Jiao Tong University) in the C9 League, an alliance of elite Chinese universities offering comprehensive and leading education. These two universities are consistently ranked in the Asia top 10. As of 2025, Fudan University and Shanghai Jiao Tong University were ranked in the global top 40 research comprehensive universities based on aggregate performance from four widely observed university rankings (THE+ARWU+QS+US News).\nThe other two members of Project 985, Tongji University and East China Normal University, are also based in Shanghai and internationally; they were ranked they ranked 150\u2013175th globally by the \"Times Higher Education World Reputation Rankings\" where . Shanghai University of Sport is also based in the city, which consistently ranks the best in China among universities specialized in sports, and as of 2024 ranks #1 in Asia and #29 globally according to the \"Global Ranking of Sport Science Schools and Departments\" released by Shanghai Ranking.\nThe city has many Chinese\u2013foreign joint education institutes, such as the Shanghai University\u2013University of Technology Sydney Business School since 1994, the University of Michigan\u2013Shanghai Jiao Tong University Joint Institute since 2006, and New York University Shanghai\u2014the first China\u2013U.S. joint venture university\u2014since 2012. In 2013, the Shanghai Municipality and the Chinese Academy of Sciences founded the ShanghaiTech University in the Zhangjiang Hi-Tech Park in Pudong. The city is also a seat of the Shanghai Academy of Social Sciences, China's oldest think tank for the humanities and social sciences.\nBy the end of 2023, the city also had a total of 49 institutions for postgraduate education, 900 secondary schools, 70 vocational schools, 664 primary schools, and 31 special education schools. Five years of primary education and four years of junior secondary education are free, with a gross enrollment ratio of over 99.9%. In 2009 and 2012, 15-year-old students from Shanghai ranked first in every subject (math, reading, and science) in the Program for International Student Assessment. The consecutive three-year senior secondary education is priced and uses the Senior High School Entrance Examination (\"Zhongkao\") as a selection process, with a gross enrollment ratio of 98%. Shanghai High School, No. 2 High School Attached to East China Normal University, High School Affiliated to Fudan University, and High School Affiliated to Shanghai Jiao Tong University\u2014are termed \"The Four Schools\" () of Shanghai and highlighted as having the best teaching quality in the city.\nTransport.\nPublic.\nShanghai has a public transportation system comprising metros, buses, ferries, and taxis, which can be accessed using a Shanghai Public Transport Card.\nShanghai's rapid transit system, the Shanghai Metro, incorporates subway and light metro lines and extends to each core urban district as well as neighboring suburban districts. As of 2025[ [update]], there are 19 metro lines (excluding the Shanghai maglev train and Jinshan railway), 508 stations, and of lines in operation, making it the longest network in the world. On 8 March 2019, it set the city's daily metro ridership record with 13.3 million. Opened in 2004, the Shanghai maglev train is the first and the fastest commercial high-speed maglev in the world, with a maximum operation speed of .\nThe first tram line in Shanghai was opened in 1908. By 1925, there were 328 tramcars and 14 routes operated by Chinese, French, and British companies collaboratively, all of which were nationalized in 1949. Since the 1960s, tram lines were either dismantled or replaced by trolleybus or motorbus lines; the last tram line was demolished in 1975. Shanghai reintroduced trams in 2010 with the rubber-tyred Zhangjiang Tram. In 2018, the steel wheeled Songjiang Tram started operating in Songjiang District. \nShanghai has the world's most extensive bus network, including the world's oldest continuously operating trolleybus system, with 1,575 lines covering a total length of by 2019. The system is operated by multiple companies. As of 2024, 30,900 taxis were in operation in Shanghai, which carried 134 million passengers that year.\nRoads and expressways.\nShanghai is a major hub of China's expressway network. Many national expressways pass through or end in Shanghai, including Jinghu Expressway, Hurong Expressway, Shenhai Expressway, Hushaan Expressway, Huyu Expressway, Hukun Expressway, and Shanghai Ring Expressway. There are also numerous municipal expressways prefixed with the letter S. As of 2019, Shanghai has 12 bridges and 14 tunnels crossing the Huangpu River.\nBicycle lanes are common in Shanghai, separating non-motorized traffic from car traffic on most surface streets. However, bicycles and motorcycles are banned on expressways and some main roads. Cycling has increased in popularity due to the emergence of dockless, app-based bicycle-sharing systems, such as Mobike, Hello, and DiDi Bike. As of \u00a02018[ [update]], bicycle-sharing systems had an average of 1.15 million daily riders within the city.\nPrivate car ownership in Shanghai is rapidly increasing: in 2019, there were 3.40 million private cars in the city, a 12.5% increase from 2018. New private cars cannot be driven without a license plate, which are sold in monthly license plate auctions. Around 9,500 license plates are auctioned each month, and the average price was about CN\u00a5 (US$) in 2019. This policy was introduced to limit the growth of automobile traffic and alleviate congestion.\nRailways.\nShanghai has four major railway stations: Shanghai railway station, Shanghai South railway station, Shanghai West railway station, and Shanghai Hongqiao railway station.\nBuilt in 1876, the Woosung railway was the first railway in Shanghai and the first railway in operation in China By 1909, Shanghai\u2013Nanjing railway and Shanghai\u2013Hangzhou railway were in service. As of \u00a02019[ [update]], the two railways have been integrated into two main railways in China: Beijing\u2013Shanghai railway and Shanghai\u2013Kunming railway, respectively.\nShanghai has four high-speed railways (HSRs): Beijing\u2013Shanghai HSR (overlaps with Shanghai\u2013Wuhan\u2013Chengdu passenger railway), Shanghai\u2013Nanjing intercity railway, Shanghai\u2013Kunming HSR, and Shanghai\u2013Nantong railway. One HSR is under construction: Shanghai\u2013Suzhou\u2013Huzhou HSR.\nShanghai also has four commuter railways: Pudong railway (although passenger service was suspended in 2015) and Jinshan railway operated by China Railway, and Line 16 and Line 17 operated by Shanghai Metro. As of \u00a02022[ [update]], four additional lines\u2014Chongming line, Jiamin line, Airport link line and Lianggang Express line\u2014are under construction.\nAir and sea.\nShanghai is one of the largest air transportation hubs in Asia. The city has two commercial airports: Shanghai Pudong International Airport and Shanghai Hongqiao International Airport. Pudong is the primary international airport, while Hongqiao mainly operates domestic flights with limited short-haul international flights. In 2018, Pudong International Airport served 74.0 million passengers and handled 3.8 million tons of cargo, making it the ninth-busiest airport by passenger volume and third-busiest airport by cargo volume. The same year, Hongqiao International Airport served 43.6 million passengers, making it the 19th-busiest airport by passenger volume.\nSince its opening, the Port of Shanghai has become the largest port in China. Yangshan Port was built in 2005 because the river was unsuitable for docking large container ships. The port is connected with the mainland through the long Donghai Bridge. In 2010, it became world's busiest container port with an annual TEU transportation of 42 million in 2018. The Port of Shanghai also handled 259 cruises and 1.89 million passengers in 2019. Although the port is run by the Shanghai International Port Group under the government of Shanghai, it administratively belongs to Shengsi County, Zhejiang. Shanghai is part of the 21st Century Maritime Silk Road that runs from the Chinese coast to the northern Italian hub of Trieste.\nCulture.\nThe culture of Shanghai was formed by a combination of the Wuyue culture and the \"East Meets West\" Haipai culture. Wuyue culture's influence is manifested in Shanghainese language\u2014which comprises dialectal elements from Jiaxing, Suzhou, and Ningbo\u2014and Shanghai cuisine, which was influenced by those of Jiangsu and Zhejiang. Haipai culture emerged after Shanghai became a prosperous port in the early 20th century, with foreigners from Europe, America, Japan, and India moving into the city. The culture fuses elements of Western cultures with the local Wuyue culture, and its influence extends to the city's literature, fashion, architecture, music, and cuisine. The term Haipai was coined by Beijing writers in 1920 to criticize Shanghai scholars for admiring capitalism and Western culture. In the early 21st century, Shanghai has been recognized as a new influence and inspiration for cyberpunk culture. The city is recognized by UNESCO as a \"City of Design\" since February 2010.\nMuseums.\nCultural curation in Shanghai has grown since 2013, with several new museums having been opened in the city. This is in part due to the city's 2018 development plans, which aim to make Shanghai \"an excellent global city\". The Shanghai Museum has one of the largest collections of Chinese artifacts in the world, including a large collection of ancient Chinese bronzes and ceramics. The China Art Museum is one of the largest museums in Asia and displays an animated replica of the 12th century painting Along the River During the Qingming Festival. The Shanghai Natural History Museum and the Shanghai Science and Technology Museum are natural history and science museums. There are numerous smaller, specialist museums housed in archeological and historical sites, such as the Songze Museum, the Site of the First National Congress of the Chinese Communist Party, the site of the former Provisional Government of the Republic of Korea, the Shanghai Jewish Refugees Museum, and the Shanghai Post Office Museum (located in the General Post Office Building).\nCuisine.\nBenbang cuisine () is cooking style that originated in the 1600s, with influences from surrounding provinces. It emphasizes the use of condiments while retaining the original flavors of the raw ingredients. Sugar is an important ingredient in Benbang cuisine, especially in combination with soy sauce. Signature dishes of Benbang cuisine include Xiaolongbao, Red braised pork belly, and Shanghai hairy crab. \nHaipai cuisine is a Western-influenced cooking style that originated in Shanghai. It uses elements from French, British, Russian, German, and Italian cuisines and adapted them for local taste preferences and to incorporate local ingredients. Haipai cuisine dishes include Shanghai-style borscht (, \"Russian soup\"), crispy pork cutlets, and Shanghai salad, derived from Olivier salad. Both Benbang and Haipai cuisine use varoius seafoods including freshwater fish, shrimp, and crab.\nVisual arts.\nThe Songjiang School (), containing the Huating School () founded by Gu Zhengyi, was a small painting school in Shanghai during the Ming and Qing Dynasties. It was represented by Dong Qichang. The school was considered an expansion of the Wu School in Suzhou, the cultural center of the Jiangnan region at the time. The Shanghai School commenced in the 19th century, focusing on the visual content of painting through the use of bright colors, using secular objects like flowers and birds as themes. Western art was introduced to Shanghai in 1847 by Spanish missionary Joannes Ferrer (), and the city's first Western atelier was established in 1864 inside the Tushanwan orphanage (\u571f\u5c71\u6e7e\u5b64\u513f\u9662). During the Republic of China, artists including Zhang Daqian, Liu Haisu, Xu Beihong, Feng Zikai, and Yan Wenliang settled in Shanghai, allowing it to become the art center of China. Art forms such as photography, wood carving, sculpture, comics (Manhua), and Lianhuanhua\u2014thrived. Sanmao was created to dramatize the chaos created by the Second Sino-Japanese War. The most comprehensive art and cultural facility in Shanghai is the China Art Museum, with of exhibition space.\nSince 2001, Shanghai has held Shanghai Fashion Week each April and October. The main venue is in Fuxing Park, and the opening and closing ceremonies are held in the Shanghai Fashion Center. The April session is also part of the one-month Shanghai International Fashion Culture Festival.\nPerformance arts.\nTraditional Chinese opera became a popular source of public entertainment in the late 19th century. In the early 20th century, monologue and burlesque in Shanghainese appeared, absorbing elements from traditional dramas. In the 1920s, Pingtan performance art expanded from Suzhou to Shanghai; commercial radio stations expanded its popularity in the 1930s, with 103 programs every day. A Shanghai-style Beijing Opera was formed in the 1930s, led by Zhou Xinfang and Gai Jiaotian. A small troupe from Shengxian (now Shengzhou) promoted Yue opera on the Shanghainese stage. Shanghai opera was formed when local folksongs were fused with modern operas.\nDrama appeared in missionary schools in Shanghai in the late 19th century, mainly performed in English. \"Scandals in Officialdom\" (), staged in 1899, was one of the earliest-recorded plays. In 1907, \"Uncle Tom's Cabin; or, Life Among the Lowly\" () was performed at the Lyceum Theatre.\nShanghai is the birthplace of Chinese cinema; China's first short film, \"The Difficult Couple\" (1913), and the country's first fictional feature film, \"An Orphan Rescues His Grandfather\" (, 1923) were both produced in the city. Shanghai's film industry grew during the early 1930s, generating stars such as Hu Die, Ruan Lingyu, Zhou Xuan, Jin Yan, and Zhao Dan. The exile of Shanghainese filmmakers and actors during the Second Sino-Japanese War and the Communist revolution contributed to the development of the Hong Kong film industry. Shanghai Television Festival, founded in 1986, is the earliest international TV festival founded in China. The Shanghai International Film Festival was founded in 1993 and is one of the nine major international film festivals in the A category.\nSports.\nShanghai has several football teams, including two in the Chinese Super League: Shanghai Shenhua and Shanghai Port. Shanghai's top-tier basketball team, the Shanghai Sharks of the Chinese Basketball Association, developed Yao Ming before he entered the NBA. Shanghai's baseball team, the Shanghai Golden Eagles, plays in the China Baseball League. Professional athletes from Shanghai include 110 metres hurdles runner Liu Xiang, table tennis player Wang Liqin, and badminton player Wang Yihan.\nThe Shanghai Cricket Club dates back to 1858, when the first recorded cricket match was played between a team of British Naval officers and a Shanghai 11. The Shanghai cricket team played various international matches between 1866 and 1948 as China's de facto China national cricket team. After going dormant in 1949 after the founding of the PRC, the club was re-established in 1994 by expatriates living in the city and has since grown to over 300 members.\nShanghai hosts several international sports events. Since 2004, it has hosted the Chinese Grand Prix, a round of the Formula One World Championship, at the Shanghai International Circuit. The city also hosts the Shanghai Masters tennis tournament, which is part of ATP World Tour Masters 1000, as well as golf tournaments including the BMW Masters and WGC-HSBC Champions. In 2023, Shanghai hosted 118 sports events, with 190,000 participants and 1.29 million spectators, driving a consumption of CN\u00a53.713 billion (US$510.83 million).\nEnvironment.\nParks and resorts.\nShanghai has an extensive public park system; by 2022, the city had 670 parks, of which 281 had free admission, and the per capita park area was . The largest park in Shanghai is Century Park in Pudong.\nThe People's Square park, located in the heart of downtown Shanghai, is known for its proximity to other major landmarks in the city. Fuxing Park, located in the former French Concession, features formal French-style gardens and is surrounded by high-end bars and cafes. Lu Xun Park in Hongkou is named after writer Lu Xun, whose tomb is located within the park. Zhongshan Park, in western central Shanghai, contains a monument of Chopin, the tallest statue dedicated to the composer in the world. The park features sakura and peony gardens and a 150-year-old platanus. \nShanghai Botanical Garden is located southwest of the city center and established in 1978. In 2011, the largest botanical garden in Shanghai\u2014Shanghai Chen Shan Botanical Garden\u2014opened in Songjiang District. The Shanghai Disney Resort opened in 2016, featuring a castle that is the biggest among Disney's resorts.\nAir pollution.\nAir pollution in Shanghai is not as severe as in many other Chinese cities, but is still considered substantial by world standards. During the 2013 Eastern China smog, air pollution rates reached between 23 and 31 times the international standard. On 6 December 2013, levels of PM2.5 particulate matter in Shanghai rose above 600 micrograms per cubic meter and in the surrounding area, above 700 micrograms per cubic meter. Levels of PM2.5 in Putuo District reached 726 micrograms per cubic meter. The following month, Yang Xiong, the mayor of Shanghai, announced three measures to manage the air pollution in Shanghai: implementing the 2013 air-cleaning program, establishing a linkage mechanism with the three surrounding provinces, and improving the city's early-warning systems. That year, China's cabinet announced that a CN\u00a5 (US$) fund will be set up to help companies meet the new environmental standards. From 2013 to 2018, more than 3,000 treatment facilities for industrial waste gases were installed, and the city's annual smoke, nitrogen oxide, and sulfur dioxide emissions decreased by 65%, 54%, and 95%, respectively.\nIn 2023, the Air Quality Index (AQI) of Shanghai reached a rate of 87.7%, a 0.6% increase compared to the previous year. The annual average concentration of inhalable particulate matter (PM10) was 48 microgrammes per cubic meter, while the annual average concentration of fine particulate matter was 28 microgrammes per cubic meter.\nEnvironmental protection.\nA 16-year rehabilitation of Suzhou Creek, which runs through the city, was finished in 2012, clearing the creek of barges and factories and removing 1.3 million cubic meters of sludge. The government has moved almost all the factories within the city center to either the outskirts or other provinces. Shanghai once promoted the usage of liquefied petroleum gas vehicles, such as scooters and taxis, in the early 2000s; due to safety risks and lack of refuelling stations, these vehicles met limited success in the city.\nOn 1 July 2019, Shanghai adopted a new garbage-classification system that sorts waste into categories such as residual, kitchen, recyclable, and hazardous. The wastes are collected by separate vehicles and sent to incineration plants, landfills, recycling centers, and hazardous-waste-disposal facilities, respectively.\nMedia.\nMedia in Shanghai covers newspapers, publishers, broadcast, television, and the Internet, with some media having influence over the country. Concerning foreign publications in Shanghai, Hartmut Walravens of the IFLA Newspapers Section said that when the Japanese controlled Shanghai in the 1940s \"it was very difficult to publish good papers \u2013 one either had to concentrate on emigration problems, or cooperate like the \"Chronicle\".\"\nAs of \u00a02020[ [update]], newspapers publishing in Shanghai include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNewspapers formerly published in Shanghai include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nThe city's main broadcaster is Shanghai Media Group.\nInternational relations.\nThe city is the seat of the New Development Bank, a multilateral development bank established by the BRICS states.\nTwin towns \u2013 sister cities.\nShanghai is twinned with 68 cities from the following 57 countries:\nConsulates and consulates general.\nAs of September 2020, Shanghai hosts 71 consulates general and 5 consulates, excluding Hong Kong and Macao trade offices.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27644", "revid": "140154", "url": "https://en.wikipedia.org/wiki?curid=27644", "title": "Sinai Peninsula", "text": "Peninsula in Egypt\nThe Sinai Peninsula, or simply Sinai ( ; , ), is a peninsula in Egypt, and the only part of the country located in Asia. It is between the Mediterranean Sea to the north and the Red Sea to the south, and is a land bridge between Asia and Africa. Sinai has a land area of about (6 percent of Egypt's total area) and a population of approximately 600,000 people. Administratively, the vast majority of the area of the Sinai Peninsula is divided into two governorates: the South Sinai Governorate and the North Sinai Governorate. Three other governorates span the Suez Canal, crossing into African Egypt: Suez Governorate on the southern end of the Suez Canal, Ismailia Governorate in the center, and Port Said Governorate in the north.\nIn the classical era, the region was known as Arabia Petraea. The peninsula acquired the name \"Sinai\" in modern times due to the assumption that a mountain near Saint Catherine's Monastery is the Biblical Mount Sinai. Mount Sinai is one of the most religiously significant places in the Abrahamic faiths.\nThe Sinai Peninsula has been a part of Egypt from the First Dynasty of ancient Egypt (c.\u20093100 BC). This comes in stark contrast to the region north of it, the Levant (the present-day territories of Syria, Lebanon, Jordan, Israel and Palestine), which, due largely to its strategic geopolitical location and cultural convergences, has historically been the center of conflict between Egypt and various states of Mesopotamia and Asia Minor. In periods of foreign occupation, the Sinai was, like the rest of Egypt, also occupied and controlled by foreign empires, in more recent history the Ottoman Empire (1517\u20131867) and the United Kingdom (1882\u20131956). Israel invaded and occupied Sinai during the Suez Crisis (known in Egypt as the \"Tripartite Aggression\" due to the simultaneous coordinated attack by the UK, France and Israel) of 1956, and during the Six-Day War of 1967. On 6 October 1973, Egypt launched the Yom Kippur War, seizing most of the east bank of the Suez Canal while Israel seized even more area to its west. Israel exchanged the ten kilometers along the east bank and territory seized in 1973 for a 1974 ceasefire. In 1982, as a result of the Egypt\u2013Israel peace treaty of 1979, Israel withdrew from all of the Sinai Peninsula except the contentious territory of Taba, which was returned after a ruling by a commission of arbitration in 1989.\nToday, Sinai has become a tourist destination due to its natural setting, rich coral reefs, and biblical history.\nName.\nBecause the Sinai Peninsula was the main region where mining of turquoise was carried out in Ancient Egypt, it was called \"Biau\" (the \"Mining Country\") and \"Khetiu Mafkat\" (\"Ladders of Turquoise\") by the ancient Egyptians.\nThe origin of the modern name is a source of contention (see Biblical Mount Sinai for a fuller discussion). The name \"Sinai\" (, ) may have been derived from the ancient Mesopotamian moon-god Sin. The moon-deity Sin is associated with the area; the ancient Egyptian moon-god Thoth is also associated with Sin, and his worship was widespread throughout the south tip of the Sinai Peninsula. \"The Jewish Encyclopedia\" (1901-1906) quotes a Rabbinic source, the 8th- or 9th-century Pirke De-Rabbi Eliezer, which derives the name from the biblical Hebrew word \"seneh\" (), a word only known from two occurrences in the Hebrew Bible, in both cases referring to the burning bush. Rabbi Eliezer opines that Mount Horeb only received the name \"Sinai\" after God appeared to Moses in the shape of a burning bush.\nIts modern Arabic name is ' (Egyptian Arabic '; ). The modern Arabic is an adoption of the Biblical name; the 19th-century Arabic designation of Sinai was \"Jebel el-T\u016br\", and the name of the mountain is derived from the town of El Tor (formerly called \"Tur Sinai\"), whose name comes from the Arabic term for the mountain where the prophet Moses received the Tablets of the Law from God, thus this mountain is designated as \"\"Jabal A\u1e6d-\u1e6c\u016br\" (Arabic: \u062c\u0628\u0644 \u0627\u0644\u0637\u0651\u0648\u0631)\", and the town is also the capital of the South Sinai Governorate of Egypt. As another Arabic word for \"mass of very high land going up to a peak \u2013 mountain\" is \"\"\u1e6c\u016br\".\"\nThe English name came from Latin, ultimately from Hebrew \u05e1\u05b4\u05d9\u05e0\u05b7\u05d9\u200e, pronounced /si\u02c8n\u00e1i/ (, in English phonetic spelling). In English, the name is now usually pronounced . An alternative, now dated pronunciation in English was .\nGeography.\nSinai is triangular in shape, with its northern shore lying on the southern Mediterranean Sea, and its southwest and southeast shores on the Gulf of Suez and the Gulf of Aqaba of the Red Sea. It is linked to the African continent by the Isthmus of Suez, wide strip of land, containing the Suez Canal. The eastern isthmus, linking it to the Asian mainland, is around wide. The peninsula is on the African Plate with the Arabian Plate on its eastern boundary.\nThe southernmost tip is the Ras Muhammad National Park.\nMost of the Sinai Peninsula is divided among the two governorates of Egypt: South Sinai (Ganub Sina) and North Sinai (Shamal Sina). Together, they comprise around 60,000 square kilometres (23,000 sq mi) and have a population (January 2013) of 597,000. Three more governates span the Suez Canal, crossing into African Egypt: Suez (el-Sewais) is on the southern end of the Suez Canal, Ismailia (el-Isma'ileyyah) in the centre, and Port Said in the north.\nThe largest city of Sinai is Arish, capital of the North Sinai, with around 160,000 residents. Other larger settlements include Sharm El Sheikh and El-Tor, on the southern coast. Inland Sinai is arid (effectively a desert), mountainous and sparsely populated, the largest settlements being Saint Catherine and Nekhel.\nClimate.\nSinai is one of the coldest provinces in Egypt because of its high altitudes and mountainous topographies. Winter temperatures in some of Sinai's cities and towns reach .\nHistory.\nChalcolithic.\nA cave with paintings of people and animals was discovered about north of Mount Catherine in January 2020, dates back to the Chalcolithic Period, c.\u20095th\u20134th millennium BCE.\nAncient Egypt.\nFrom the time of the First Dynasty or before, the Egyptians mined turquoise in Sinai at two locations, now called by their Egyptian Arabic names Wadi Maghareh and Serabit el-Khadim. The mines were worked intermittently and on a seasonal basis for thousands of years. Modern attempts to exploit the deposits have been unprofitable. These may be the first historically attested mines.\nThe fortress Tjaru in western Sinai was a place of banishment for Egyptian criminals. The Way of Horus connected it across northern Sinai with ancient Canaan.\nAchaemenid Persian Period.\nAt the end of the time of Darius I, the Great (521\u2013486 BCE) Sinai was part of the Persian province of Abar-Nahra, which means 'beyond the river [Euphrates]'.\nCambyses successfully managed the crossing of the hostile Sinai Desert, traditionally Egypt's first and strongest line of defence, and brought the Egyptians under Psamtik III, son and successor of Ahmose, to battle at Pelusium. The Egyptians lost and retired to Memphis; the city fell to the Persian control and the Pharaoh was carried off in captivity to Susa in Persia.\nRoman and Byzantine Periods.\nRhinocorura (Greek for \"Cut-off Noses\") and the eponymous region around it were used by Ptolemaid Egypt as a place of banishment for criminals, today known as Arish.\nAfter the death of the last Nabatean king, Rabbel II Soter, in 106, the Roman emperor Trajan faced practically no resistance and conquered the kingdom on 22 March 106. With this conquest, the Roman Empire went on to control all shores of the Mediterranean Sea. The Sinai Peninsula became part of the Roman province of Arabia Petraea.\nSaint Catherine's Monastery on the foot of Mount Sinai was constructed by order of the Emperor Justinian between 527 and 565. Most of the Sinai Peninsula became part of the province of Palaestina Salutaris in the 6th century.\nAyyubid Period.\nDuring the Crusades it was under the control of Fatimid Caliphate. Later, Sultan Saladin abolished the Fatimid Caliphate in Egypt and took this region under his control too. It was the military route from Cairo to Damascus during the Crusades. And in order to secure this route, he built a citadel on the island of Pharaoh (near present Taba) known by his name 'Saladin's Citadel'.\nMamluk and Ottoman Periods.\nThe peninsula was governed as part of Egypt under the Mamluk Sultanate of Egypt from 1260 until 1517, when the Ottoman Sultan, Selim the Grim, defeated the Egyptians at the Battles of Marj Dabiq and al-Raydaniyya, and incorporated Egypt into the Ottoman Empire. From then until 1906, Sinai was administered by the Ottoman provincial government of the \"Pashalik\" of Egypt, even following the establishment of the Muhammad Ali dynasty's rule over the rest of Egypt in 1805.\nBritish control.\nIn 1906, the Ottoman Porte formally transferred administration of Sinai to the Khedivate of Egypt, which essentially meant that it fell under the control of the British Empire, who had occupied and largely controlled Egypt since the 1882 Anglo-Egyptian War. The border imposed by the British runs in an almost straight line from Rafah on the Mediterranean shore to Taba on the Gulf of Aqaba. This line has served as the de jure eastern border of Egypt ever since.\nIsraeli invasions and occupation.\n1956 war.\nIn 1956, Egypt nationalised the Suez Canal, a waterway marking the boundary between Egyptian territory in Africa and the Sinai Peninsula. Thereafter, Israeli ships were prohibited from using the Canal, owing to the state of war between the two states. Egypt also prohibited ships from using Egyptian territorial waters on the eastern side of the peninsula to travel to and from Israel, effectively imposing a blockade on the Israeli port of Eilat. In October 1956, in what is known in Egypt as the Tripartite Aggression, Israel Defense Forces troops, aided by the United Kingdom and France (which sought to reverse the nationalization and regain control over the Suez Canal), invaded Sinai and occupied much of the peninsula within a few days. In March 1957, Israel withdrew its forces from Sinai, following strong pressure from the United States and the Soviet Union. Thereafter, the United Nations Emergency Force (UNEF) was stationed in Sinai to prevent any further conflict in the Sinai.\n1967 war.\nOn 16 May 1967, Egypt ordered the UNEF out of Sinai and reoccupied it militarily. Secretary-General U Thant eventually complied and ordered the withdrawal without Security Council authorisation. In the course of the Six-Day War that broke out shortly thereafter, Israel occupied the entire Sinai Peninsula, and Gaza Strip from Egypt, the West Bank (including East Jerusalem) from Jordan (which Jordan had controlled since 1949), and the Golan Heights from Syria. The Suez Canal, the east bank of which was now occupied by Israel, was closed. Israel commenced efforts at large scale Israeli settlement in the Sinai Peninsula.\nFollowing the Israeli conquest of Sinai, Egypt launched the War of Attrition (1967\u201370) aimed at forcing Israel to withdraw from the Sinai. The war saw protracted conflict in the Suez Canal Zone, ranging from limited to large-scale combat. Israeli shelling of the cities of Port Said, Ismailia, and Suez on the west bank of the canal, led to high civilian casualties (including the virtual destruction of Suez), and contributed to the flight of 700,000 Egyptian internal refugees. Ultimately, the war concluded in 1970 with no change in the front line.\nOn 6 October 1973, Egypt commenced Operation Badr to retake the Sinai, while Syria launched a simultaneous operation to retake the Golan Heights, thereby beginning the Yom Kippur War (known in Egypt as the \"October War\"). Egyptian engineering forces built pontoon bridges to cross the Suez Canal, and stormed the Bar Lev Line, Israel's defensive line along the Suez Canal's east bank. Though the Egyptians maintained control of most of the east bank of the Suez Canal, in the later stages of the war, the Israeli military crossed the southern section of the Suez Canal, cutting off the Egyptian 3rd Army, and occupied a section of the Suez Canal's west bank. The war ended following a mutually agreed-upon ceasefire. After the war, as part of the subsequent Sinai Disengagement Agreements, Israel withdrew from immediate proximity with the Suez Canal, with Egypt agreeing to permit passage of Israeli ships. The canal was reopened in 1975, with President Anwar Sadat leading the first convoy through the canal aboard an Egyptian Navy destroyer.\n1979\u20131982 Israeli withdrawal.\nIn 1979, Egypt and Israel signed a peace treaty in which Israel agreed to withdraw from the entirety of the Sinai Peninsula. Israel subsequently withdrew in several stages, ending in 1982. The Israeli pull-out involved dismantling almost all Israeli settlements, including the settlement of Yamit in north-eastern Sinai. The exception was that the coastal city of Sharm el-Sheikh (which the Israelis had founded as Ofira during their occupation of the Sinai Peninsula) was not dismantled. The Treaty allows monitoring of Sinai by the Multinational Force and Observers, and limits the number of Egyptian military forces in the peninsula.\nSinai peacekeeping zones.\nArticle 2 of Annex I of the Peace Treaty called for the Sinai Peninsula to be divided into zones. Within these zones, Egypt and Israel were permitted varying degrees of military buildup:\nEarly 21st century security issues.\nSince the early 2000s, Sinai has been the site of several terror attacks against tourists, the majority of whom are Egyptian. Investigations have shown that these were mainly motivated by a resentment of the poverty faced by many Bedouin in the area. Attacking the tourist industry was viewed as a method of damaging the industry so that the government would pay more attention to their situation. (See 2004 Sinai bombings, 2005 Sharm El Sheikh bombings and 2006 Dahab bombings). Since the 2011 Egyptian revolution, unrest has become more prevalent in the area including the August 2012 Sinai attack in which 16 Egyptian soldiers were killed by militants. (See Sinai insurgency.)\nAlso on the rise are kidnappings of refugees. According to Meron Estifanos, Eritrean refugees are often kidnapped by Bedouin in the northern Sinai, tortured, raped, and only released after paying a large ransom.\nUnder President el-Sisi, Egypt has implemented a rigorous policy of controlling the border to the Gaza Strip, including the dismantling of tunnels between Gaza and Sinai.\nDemographics.\nThe two governorates of North Sinai and South Sinai have a total population of 597,000 (January 2013). This figure rises to 1,400,000 by including Western Sinai, the parts of the Port Said, Ismailia and Suez Governorates lying east of the Suez Canal. Port Said alone has a population of roughly 500,000 people (January 2013). Portions of the populations of Ismailia and Suez live in west Sinai, while the rest live on the western side of the Suez Canal.\nThe population of Sinai has largely consisted of desert-dwelling Bedouins with their colourful traditional costumes and significant culture. Large numbers of Egyptians from the Nile Valley and Delta moved to the area to work in tourism, but development adversely affected the native Bedouin population. In order to help alleviate their problems, various NGOs began to operate in the region, including the Makhad Trust, a UK charity that assists the Bedouin in developing a sustainable income while protecting Sinai's natural environment, heritage and culture.\nEconomy.\nSinai's scenic spots (including coral reefs offshore) and religious structures have become important to the tourism industry. The most popular tourist destination in Sinai are Mount Sinai (\"Jabal Musa\") and St Catherine's Monastery, which is considered to be the oldest working Christian monastery in the world, and the beach resorts of Sharm el-Sheikh, Dahab, Nuweiba and Taba. Most tourists arrive at Sharm El Sheikh International Airport, through Eilat, Israel and the Taba Border Crossing, by road from Cairo or by ferry from Aqaba in Jordan.\nCacti \u2013 especially cactus pears \u2013 are grown in Sinai. They are a crop of the Columbian Exchange. Cactus hedges \u2013 both intentionally planted and wild garden escapes \u2013 formed an important part of defensible positions during the Sinai and Palestine campaign of World War I. Some unfamiliar soldiers even tried eating them, to negative result.\nDromedary herding is important here. \"Trypanosoma evansi\" is a constant concern and is transmitted by several vectors. Although ticks have not been proven to be among them, Mahmoud and Gray 1980 and El-Kady 1998 experimentally demonstrate survival of \"T. evansi\" in camel ticks of the \"Hyalomma\" for several hours in the real bio-climatic conditions of Sinai.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27646", "revid": "42649728", "url": "https://en.wikipedia.org/wiki?curid=27646", "title": "Spy fiction", "text": "Fiction genre involving espionage\nSpy fiction is a genre of literature involving espionage as an important context or plot device. It emerged in the early twentieth century, inspired by rivalries and intrigues between the major powers, and the establishment of modern intelligence agencies. It was given new impetus by the development of communism and fascism in the lead-up to World War II, continued to develop during the Cold War, and received a fresh impetus from the emergence of rogue states, international criminal organizations, global terrorist networks, maritime piracy and technological sabotage and espionage as potent threats to Western societies. As a genre, spy fiction is thematically related to the novel of adventure (\"The Prisoner of Zenda\", 1894, \"The Scarlet Pimpernel\", 1905), the thriller (such as the works of Edgar Wallace) and the politico-military thriller (\"The Schirmer Inheritance\", 1953, \"The Quiet American\", 1955).\nHistory.\nCommentator William Bendler noted that \"Chapter 2 of the Hebrew Bible's Book of Joshua might count as the first Spy Story in world literature. (...) Three thousand years before James Bond seduced Pussy Galore and turned her into his ally against Goldfinger, the spies sent by General Joshua into the city of Jericho did much the same with Rahab the Harlot.\"\nNineteenth century.\nSpy fiction as a genre started to emerge during the 19th Century. Early examples of the espionage novel are \"The Spy\" (1821) and \"The Bravo\" (1831), by American novelist James Fenimore Cooper. \"The Bravo\" attacks European anti-republicanism, by depicting Venice as a city-state where a ruthless oligarchy wears the mask of the \"serene republic\".\nIn nineteenth-century France, the Dreyfus Affair (1894\u201399) contributed much to public interest in espionage. For some twelve years (ca. 1894\u20131906), the Affair, which involved elements of international espionage, treason, and antisemitism, dominated French politics. The details were reported by the world press: an Imperial German penetration agent betraying to Germany the secrets of the General Staff of the French Army; the French counter-intelligence riposte of sending a charwoman to rifle the trash in the German Embassy in Paris, were news that inspired successful spy fiction.\nAt least two Sherlock Holmes stories have clear espionage themes. In \"The Adventure of the Naval Treaty\", Holmes recovers the text of a secret Naval Treaty between Britain and Italy, stolen by a daring spy. In \"His Last Bow\", Holmes himself acts as a double agent, providing Germany with a lot of false information on the eve of WWI.\nTwentieth century.\nThe major themes of a spy in the lead-up to the First World War were the continuing rivalry between the European colonial powers for dominance in Asia, the growing threat of conflict in Europe, the domestic threat of revolutionaries and anarchists, and historical romance.\n\"Kim\" (1901) by Rudyard Kipling concerns the Anglo\u2013Russian \"Great Game\", which consisted of a geopolitical rivalry and strategic warfare for supremacy in Central Asia, usually in Afghanistan. \"The Secret Agent\" (1907) by Joseph Conrad examines the psychology and ideology motivating the socially marginal men and women of a revolutionary cell. A diplomat from an unnamed (but clearly Russian) embassy forces a double-agent, Verloc, to organise a failed attempt to bomb the Greenwich Observatory in the hope that the revolutionaries will be blamed. Conrad's next novel, \"Under Western Eyes\" (1911), follows a reluctant spy sent by the Russian Empire to infiltrate a group of revolutionaries based in Geneva. G. K. Chesterton's \"The Man Who Was Thursday\" (1908) is a metaphysical thriller ostensibly based on the infiltration of an anarchist organisation by detectives, but the story is actually a vehicle for exploring society's power structures and the nature of suffering.\nThe fictional detective Sherlock Holmes, created by Arthur Conan Doyle, served as a spy hunter for the British government in the stories \"The Adventure of the Second Stain\" (1904), and \"The Adventure of the Bruce-Partington Plans\" (1912). In \"His Last Bow\" (1917), he served Crown and country as a double agent, transmitting false intelligence to Imperial Germany on the eve of the Great War.\n\"The Scarlet Pimpernel\" (1905) by Baroness Orczy chronicled an English aristocrat's derring-do in rescuing French aristocrats from the Reign of Terror of the French Revolution (1789\u201399).\nBut the term \"spy novel\" was defined by \"The Riddle of the Sands\" (1903) by Irish author Erskine Childers. \"The Riddle of the Sands\" described a British yachtsman and his friend cruising off the North Sea coast of Germany who turned amateur spies when they discover a secret German plan to invade Britain. Its success created a market for the invasion literature subgenre, which was flooded by imitators. William Le Queux and E. Phillips Oppenheim became the most widely read and most successful British writers of spy fiction, especially of invasion literature. Their prosaic style and formulaic stories, produced voluminously from 1900 to 1914, proved of low literary merit.\nDuring the First World War.\nDuring the War, John Buchan became the pre-eminent British spy novelist. His well-written stories portray the Great War as a \"clash of civilisations\" between Western civilization and barbarism. His notable novels are \"The Thirty-nine Steps\" (1915), \"Greenmantle\" (1916) and sequels, all featuring the heroic Scotsman Richard Hannay. In France Gaston Leroux published the spy thriller \"Rouletabille chez Krupp\" (1917), in which a detective, Joseph Rouletabille, engages in espionage.\nInter-war period.\nAfter the Russian Revolution (1917), the quality of spy fiction declined, perhaps because the Bolshevik enemy won the Russian Civil War (1917\u201323). Thus, the inter-war spy story usually concerns combating the Red Menace, which was perceived as another \"clash of civilizations\".\nSpy fiction was dominated by British authors during this period, initially former intelligence officers and agents writing from inside the trade. Examples include \"\" (1928) by W. Somerset Maugham, which accurately portrays spying in the First World War, and \"The Mystery of Tunnel 51\" (1928) by Alexander Wilson whose novels convey an uncanny portrait of the first head of the Secret Intelligence Service, Mansfield Smith-Cumming, the original 'C'.\nIn the book \"Literary Agents\" (1987), Anthony Masters wrote: \"Ashenden's adventures come nearest to the real-life experiences of his creator\"'. John Le Carr\u00e9 described Ashenden stories as a major influence on his novels as praised Maugham as \"the first person to write anything about espionage in a mood of disenchantment and almost prosaic reality\".\nAt a more popular level, Leslie Charteris' popular and long-running \"Saint\" series began, featuring Simon Templar, with \"Meet the Tiger\" (1928). \"Water on the Brain\" (1933) by former intelligence officer Compton Mackenzie was the first successful spy novel satire. Prolific author Dennis Wheatley also wrote his first spy novel, \"The Eunuch of Stamboul\" (1935) during this period.\nIn the sham state of Manchukuo, spies often featured in stories published in its government-sponsored magazines as villains threatening Manchukuo. Manchukuo had been presented since its founding in 1931 as an idealistic Pan-Asian experiment, where the officially designated \"five races\" of the Japanese, Han Chinese, Manchus, Koreans and Mongols had come together to build a utopian society. Manchukuo also had a substantial Russian minority who initially been considered as the \"sixth race\", but had been excluded. The spy stories of Manchukuo such as \"A Mixed Race Woman\" by the writer Ding Na often linked the willingness to serve as spies with having a mixed Russian-Han heritage; the implication being that people of \"pure\" descent from one of the \"five races\" of Manchukuo would not betray it. In \"A Mixed Race Woman\", the villain initially appears to Mali, the eponymous character who has a Russian father and a Han mother, but she ultimately is revealed to be blackmailed by the story's true villain, the foreign spy Baoerdun, and she proves to be loyal to Manchukuo after all as she forces the gun out of Baoerdun's hand at the story's climax. However, Ding's story also states that Baoerdun would not dare to have attempted his blackmail scheme against a Han woman and that he targeted Mali because she was racially mixed and hence \"weak\".\nWhen Japan invaded China in 1937 and even more so in 1941, the level of repression and propaganda in Manchukuo was increased as the state launched a \"total war\" campaign to mobilise society for the war. As part of the \"total war\" campaign, the state warned people to be vigilant at all times for spies; alongside this campaign went a mania for spy stories, which likewise warned people to be vigilant against spies. Novels and films with a counterespionage theme became ubiquitous in Manchukuo from 1937 onward. Despite the intensely patriarchal values of Manchukuo, the counter-spy campaign targeted women who were encouraged to report anyone suspicious to the police with one slogan saying, \"Women defend inside and men defend outside\". The spy stories of Manchukuo such as \"A Mixed Race Woman\" often had female protagonists. In \"A Mixed Race Woman\", it is two ordinary women who break up the spy ring instead of the Manchukuo police as might be expected. The South Korean scholar Bong InYoung noted stories such as \"A Mixed Race Woman\" were part of the state's campaign to take over \"...the governance of private and family life, relying on the power of propaganda literature and the nationwide mobilization of the social discourse of counterespionage\". At the same time, she noted \"A Mixed Race Woman\" with its intelligent female protagonists seemed to challenge the patriarchal values of Manchukuo which portrayed women as the weaker sex in need of male protection and guidance. However, Bong noted that the true heroine of \"A Mixed Race Woman\", Shulan is presented as superior to Mali as she is Han and the story is one \"...of female disempowerment in that Mali is completely subordinate to the racial order Shulan sets\".\nSecond World War.\nThe growing support of fascism in Germany, Italy and Spain, and the imminence of war, attracted quality writers back to spy fiction.\nBritish author Eric Ambler brought a new realism to spy fiction. \"The Dark Frontier\" (1936), \"Epitaph for a Spy\" (1938), \"The Mask of Dimitrios\" (US: \"A Coffin for Dimitrios\", 1939), and \"Journey into Fear\" (1940) feature amateurs entangled in espionage. The politics and ideology are secondary to the personal story that involved the hero or heroine. Ambler's Popular Front\u2013period \"\u0153uvre\" has a left-wing perspective about the personal consequences of \"big picture\" politics and ideology, which was notable, given spy fiction's usual right-wing tilt in defence of establishment attitudes. Ambler's early novels \"Uncommon Danger\" (1937) and \"Cause for Alarm\" (1938), in which NKVD spies help the amateur protagonist survive, are especially remarkable among English-language spy fiction.\n\"Above Suspicion\" (1939) by Helen MacInnes, about an anti-Nazi husband and wife spy team, features literate writing and fast-paced, intricate, and suspenseful stories occurring against contemporary historical backgrounds. MacInnes wrote many other spy novels in the course of a long career, including \"Assignment in Brittany\" (1942), \"Decision at Delphi\" (1961), and \"Ride a Pale Horse\" (1984).\nManning Coles published \"Drink to Yesterday\" (1940), a grim story occurring during the Great War, which introduces the hero Thomas Elphinstone Hambledon. However, later novels featuring Hambledon were lighter-toned, despite being set either in Nazi Germany or Britain during the Second World War (1939\u201345). After the War, the Hambledon adventures fell to formula, losing critical and popular interest.\nThe events leading up to the Second World War, and the War itself, continue to be fertile ground for authors of spy fiction. Notable examples include Ken Follett, \"Eye of the Needle\" (1978); Alan Furst, \"Night Soldiers\" (1988); and David Downing, the Station series, beginning with \"Zoo Station\" (2007).\nCold War.\nEarly.\nThe metamorphosis of the Second World War (1939\u201345) into the Soviet\u2013American Cold War (1945\u201391) gave new impetus to spy novelists. \"Atomsk\" by Paul Linebarger (later known as Cordwainer Smith), written in 1948 and published in 1949, appears to be the first espionage novel of the dawning conflict.\nThe \"secret world\" of espionage allowed a situation when writers could project anything they wanted onto the \"secret world\". The author Bruce Page complained in his 1969 book \"The Philby Conspiracy\": \"The trouble is that a man can hold almost any theory he cares to about the secret world, and defend it against large quantities of hostile evidence by the simple expedient of retreating behind further and further screens of postulated inward mystery. Secret services have in common with Freemasons and \"mafiosi\" that they inhabit an intellectual twilight-a kind of ambiguous gloom in which it is hard to distinguish with certainty between the menacing and the merely ludicrous. In such circumstances the human affinity for myth and legend easily gets out of control\". This inability to know for certain about what is going on in the \"secret world\" of intelligence-gathering affected both non-fiction and fiction books about espionage. The Cold War and the struggle between Soviet intelligence-known as the KGB from 1954 onward-vs. the CIA and MI6 made the subject of espionage a popular one for novelists to write about. Most of the spy novels of the Cold War were really action thrillers with little resemblance to the actual work of spies. The writer Malcolm Muggeridge who had worked as a spy in World War Two commented that thriller writers in the Cold War took to writing about espionage \"as easily as the mentally unstable become psychiatrists or the impotent pornographers\". The city that was considered to be the \"capital of the Cold War\" was Berlin, owing to its post-war status as the city was divided between the two German states while Britain, France, the Soviet Union and the United States all had occupations zones in Berlin. As a result, Berlin was a beehive of espionage during the Cold War with the city full of American, British, East German, French, Soviet and West German spies; it was estimated that there was an average of about 8,000 spies in Berlin at any given moment during the Cold War. Because Berlin was a center of espionage, the city was frequently a setting for spy novels and films. Furthermore, the construction of the Berlin Wall in 1961 made the wall into a symbol of Communist tyranny, which further increased the attraction for Western writers of setting a Cold War spy novel in Berlin. Perhaps the most memorable story set in Berlin was \"The Spy Who Came In From The Cold\" which in both the novel and the film ended with disillusioned British spy Alec Leamas and his lover, the na\u00efve young woman Liz Gold being shot down while trying to cross the Berlin Wall from East Berlin into West Berlin.\nBritish.\nWith \"Secret Ministry\" (1951), Desmond Cory introduced Johnny Fedora, the secret agent with a licence to kill, the government-sanctioned assassin. Ian Fleming, a former member of naval intelligence, followed swiftly with the glamorous James Bond, secret agent 007 of the British Secret Service, a mixture of counter-intelligence officer, assassin and playboy. Perhaps the most famous fictional spy, Bond was introduced in \"Casino Royale\" (1953). After Fleming's death the franchise continued under other British and American authors, including Kingsley Amis, Christopher Wood, John Gardner, Raymond Benson, Sebastian Faulks, Jeffery Deaver, William Boyd and Anthony Horowitz. The Bond novels, which were extremely popular in the 1950s, inspired an even more popular series of films starting in 1962. The success of the Bond novels and films has greatly influenced popular images of the work of spies even though the character of Bond is more of an assassin than a spy.\nDespite the commercial success of Fleming's extravagant novels, John le Carr\u00e9, himself a former spy, created anti-heroic protagonists who struggled with the ethical issues involved in espionage and sometimes resorted to immoral tactics. Le Carr\u00e9 depicted spies as living a morally grey world having to constantly make morally dubious decisions in an essentially amoral struggle where lies, paranoia and betrayal are the norm for both sides. In le Carr\u00e9 best known novel, \"The Spy Who Came In From The Cold\" (1963), the hero Alec Leamas views himself as serving in \"...a war fought on a tiny scale, at close range\" and complained that he has seen too many \"people cheated and misled, whole lives thrown away, people shot and in prison, whole groups and classes of men written off for nothing\". Le Carr\u00e9's middle-class hero George Smiley is a middle-aged spy burdened with an unfaithful, upper-class wife who publicly cuckolds him for sport. The American scholars Norman Polmar and Thomas Allen described Smiley as the fictional spy most likely to be successful as a real spy, citing le Carr\u00e9's description of him in \"A Murder of Quality\": \"Obscurity was his nature, as well as his profession. The byways of espionage are not populated by the brash and colorful adventurers of fiction. A man who, like Smiley has lived and worked for years among his country's enemies learns only one prayer: that he may never, never be noticed. Assimilation is his highest aim, he learns to love the crowds who pass him in the street without a glance; he clings to them for his anonymity and his safety. His fear makes him servile\u2014he could embrace the shoppers who jostle him in their impatience and force him from the pavement. He could adore the officials, the police, the bus conductors, for the terse indifference of their attitudes.But this fear, this servility, this dependence had developed in Smiley a perception for the colour of human beings: a swift, feminine sensitivity to their characters and motives. He knew mankind as a huntsman knows his cover, as a fox the woods. For a spy must hunt while he is hunted, and the crowd is his estate. He could collect their gestures, record the interplay of glance and movement, as a huntsman can record the twisted bracken and broken twig, or as a fox detects the signs of danger\".\nLike Le Carr\u00e9, former British Intelligence officer Graham Greene also examined the morality of espionage in left-wing, anti-imperialist novels such as \"The Heart of the Matter\" (1948), set in Sierra Leone, the seriocomic \"Our Man in Havana\" (1959) occurring in Cuba under the regime of dictator Fulgencio Batista before his deposition in the Cuban Revolution (1953\u201359), and \"The Human Factor\" (1978) about a MI6 agent's attempts to uncover a mole in apartheid-era South Africa. Greene had worked as a MI6 agent in Freetown, an important British naval base during World War Two, searching for German spies who would radio information about the movements of ships to the \"Kriegsmarine\", experiences which inspired \"The Heart of the Matter\". Greene's case officer during World War Two was Harold \"Kim\" Philby, who was later revealed in 1963 to be a long time Soviet spy, who had been recruited by Soviet intelligence in the early 1930s while he was an undergraduate at Cambridge. Greene's best known spy novel \"The Quiet American\" (1955), set in 1952 Vietnam featured a thinly disguised version of the real American intelligence officer, Major General Edward Lansdale as the villain. Greene had covered the Vietnam war in 1951-52 as a newspaper correspondent where he met Lansdale who appears in \"The Quiet American\" as Alden Pyle while the character of Thomas Fowler, a cynical, but goodhearted British journalist in Saigon was partly based on himself.\nMI6 was outraged by \"Our Man In Havana\" with its story of James Wormold, a British vacuum cleaner salesman in Cuba, recruited to work for MI6 who bamboozles his employers by selling them diagrams of vacuum cleaners, which he persuades MI6 are really diagrams of Soviet missiles. MI6 pressed for Greene to be prosecuted for violating the Official Secrets Act, claiming that he revealed too much about MI6's methods in \"Our Man in Havana\", but it decided against charging Greene out of the fear that prosecuting him would suggest the unflattening picture of MI6 in \"Our Man in Havana\" was based on reality. Greene's older brother, Herbert, a professional con-man had briefly worked as a spy for the Japanese in the 1930s before his employers realised that the \"secrets\" that he was selling them was merely information culled from the newspapers. The bumbling vacuum cleaner salesman Wormold in \"Our Man in Havana\" seems to been inspired by Herbert Greene. In \"The Human Factor\", Greene portrayed MI6 again in a highly unsympathetic light, depicting the British government as supporting the \"apartheid\" regime of South Africa because it was pro-Western while the book's protagonist, the MI6 officer Maurice Castle, married to a black South African woman, provides information to the KGB to thwart MI6 operations. Much of the plot of \"The Human Factor\" concerned a secret plan by the British, American and West German governments to buy up South African gold in bulk in order to stabilise the economy of South Africa, which Greene presented as fundamentally amoral, arguing that the Western powers were betraying their values by supporting the white supremacist South African government. Much controversy ensued when shortly after the publication of \"The Human Factor\" it emerged that such a plan had in fact been carried out, which led to much speculation about whether this was a coincidence or whether Greene had more access to secret information than he let on. There was also much speculation that the character of Maurice Castle was inspired by Philby, but Greene consistently denied this. Other novelists followed a similar path. Len Deighton's anonymous spy protagonist of \"The IPCRESS File\" (1962), \"Horse Under Water\" (1963), \"Funeral in Berlin\" (1964), and others, is a working-class man with a negative view of \"the Establishment\".\nOther notable examples of espionage fiction during this period were also built around recurring characters. These include James Mitchell's 'John Craig' series, written under his pseudonym 'James Munro', beginning with \"The Man Who Sold Death\" (1964); and Trevor Dudley-Smith's Quiller spy novel series written under the pseudonym 'Adam Hall', beginning with \"The Berlin Memorandum\" (US: \"The Quiller Memorandum\", 1965), a hybrid of glamour and dirt, Fleming and Le Carr\u00e9; and William Garner's fantastic Michael Jagger in \"Overkill\" (1966), \"The Deep, Deep Freeze\" (1968), \"The Us or Them War\" (1969) and \"A Big Enough Wreath\" (1974).\nOther important British writers who first became active in spy fiction during this period include Padraig Manning O'Brine, \"Killers Must Eat\" (1951); Michael Gilbert, \"Be Shot for Sixpence\" (1956); Alistair MacLean, \"The Last Frontier\" (1959); Brian Cleeve, \"Assignment to Vengeance\" (1961); Jack Higgins, \"The Testament of Caspar Schulz\" (1962); and Desmond Skirrow, \"It Won't Get You Anywhere\" (1966). Dennis Wheatley's 'Gregory Sallust' (1934-1968) and 'Roger Brook' (1947-1974) series were also largely written during this period.\nNotable recurring characters from this era include Adam Diment's Philip McAlpine as a long-haired, hashish-smoking fop in the novels \"The Dolly Dolly Spy\" (1967), \"The Great Spy Race\" (1968), \"The Bang Bang Birds\" (1968) and \"Think, Inc.\" (1971); James Mitchell's 'David Callan' series, written in his own name, beginning with \"Red File for Callan\" (1969); William Garner's John Morpurgo in \"Think Big, Think Dirty\" (1983), \"Rats' Alley\" (1984), and \"Zones of Silence\" (1986); and Joseph Hone's 'Peter Marlow' series, beginning with \"The Private Sector\" (1971), set during Israel's Six-Day War (1967) against Egypt, Jordan and Syria. In all of these series the writing is literary and the tradecraft believable.\nNoteworthy examples of the journalistic style and successful integration of fictional characters with historical events were the politico-military novels \"The Day of the Jackal\" (1971) by Frederick Forsyth and \"Eye of the Needle\" (1978) by Ken Follett. With the explosion of technology, Craig Thomas, launched the techno-thriller with \"Firefox\" (1977), describing the Anglo\u2013American theft of a superior Soviet jet aeroplane.\nOther important British writers who first became active in spy fiction during this period include Ian Mackintosh, \"A Slaying in September\" (1967); Kenneth Benton, \"Twenty-fourth Level\" (1969); Desmond Bagley, \"Running Blind\" (1970); Anthony Price, \"The Labyrinth Makers\" (1971); Gerald Seymour, \"Harry's Game\" (1975); Brian Freemantle, \"Charlie M\" (1977); Bryan Forbes, \"Familiar Strangers\" (1979); Reginald Hill, \"The Spy's Wife\" (1980); and Raymond Harold Sawkins, writing as Colin Forbes, \"Double Jeopardy\" (1982).\nPhilip Gooden provides an analysis of British spy fiction in four categories: professionals, amateurs, dandies and literary types.\nAmerican.\nDuring the war E. Howard Hunt wrote his first spy novel, \"East of Farewell\" (1943). In 1949 he joined the recently created CIA and continued to write spy fiction for many years. Paul Linebarger, a China specialist for the CIA, published \"Atomsk\", the first novel of the Cold War, in 1949. During the 1950s, most of American spy stories were not about the CIA, instead being about agents from the Federal Bureau of Investigation (FBI) who tracked down and arrested Soviet spies. The popular American image of the FBI was as \"coolly efficient super-cop\" who always successful in performing his duties. The FBI director, J.E. Hoover, had long cultivated the American press and Hollywood to promote a favorable image of the FBI. In 1955, Edward S. Aarons began publishing the Sam Durell CIA \"Assignment\" series, which began with \"Assignment to Disaster\" (1955). Donald Hamilton published \"Death of a Citizen\" (1960) and \"The Wrecking Crew\" (1960), beginning the series featuring Matt Helm, a CIA assassin and counter-intelligence agent.\nMajor General Edward Lansdale, a charismatic intelligence officer who was widely credited with having masterminded the defeat of the Communist Huk rebellion in the Philippines inspired several fictional versions of himself. Besides for \"The Quiet American\", he appeared as Colonel Edwin Barnum in \"The Ugly American\" (1958) by William J. Lederer and Eugene Burdick and as Colonel Lionel Teryman in the novel \"La Mal Jaune\" (1965) by the French writer Jean Lart\u00e9guy. \"The Ugly American\" was written as a rebuttal to \"The Quiet American\" under which the idealistic Colonel Barnum operating in the fictional Vietnam-like Southeast Asian nation of Sarkhan shows the way to defeat Communist guerillas by understanding local people in just the same way that Lansdale with his understanding and sympathy for ordinary Filipinos was credited with defeating the Communist Huk guerrillas. \"The Ugly American\" was greatly influenced by the modernization theory, which held Communism was something alike to a childhood disease as the modernization theory held that as Third World nations modernized that this created social-economic tensions which a ruthless minority of Communists exploited to seize power; what was required from the United States were experts who knew the local concerns in order to defeat the Communists until the modernization process was completed.\nThe Nick Carter-Killmaster series of spy novels, initiated by Michael Avallone and Valerie Moolman, but authored anonymously, ran to over 260 separate books between 1964 and the early 1990s and invariably pitted American, Soviet and Chinese spies against each other. With the proliferation of male protagonists in the spy fiction genre, writers and book packagers also started bringing out spy fiction with a female as the protagonist. One notable spy series is \"The Baroness\", featuring a sexy female superspy, with the novels being more action-oriented, in the mould of Nick Carter-Killmaster.\nOther important American authors who became active in spy fiction during this period include Ross Thomas, \"The Cold War Swap\" (1966). \"The Scarlatti Inheritance\" (1971) by Robert Ludlum is usually considered the first American modern (glamour and dirt) spy thriller weighing action and reflection. Richard Helms, the director-general of the CIA from 1966 to 1973 loathed le Carr\u00e9's morally grey spy novels, which he felt damaged the image of the CIA, and encouraged Hunt to write spy novels as a rebuttal. Helms had hopes that Hunt might write an \"American James Bond\" novel, which would be adopted by Hollywood and do for the image of the CIA what Fleming's Bond novels did for the image of MI6. In the 1970s, former CIA man Charles McCarry began the Paul Christopher series with \"The Miernik Dossier\" (1973) and \"The Tears of Autumn\" (1978), which were well written, with believable tradecraft. McCarry was a former CIA agent who worked as an editor for \"National Geographic\" and his hero Christopher likewise is an American spy who works for a thinly disguised version of the CIA while posing as a journalist. Writing under the pen name Trevanian, Roger Whitaker published a series of brutal spy novels starting with \"The Eiger Sanction\" (1972) featuring an amoral art collector/CIA assassin who ostensibly kills for the United States, but in fact kills for money. Whitaker followed up \"The Eiger Sanction\" with \"The Loo Sanction\" (1973) and \"Shibumi\" (1979). Starting in 1976 with his novel \"Saving the Queen\", the conservative American journalist and former CIA agent William F. Buckley published the first of his Blackford Oakes novels featuring a CIA agent whose politics were the same as the author's. Blackford Oakes was portrayed as a \"sort of an American James Bond\" who ruthlessly dispatches villainous KGB agents with much aplomb.\nThe first American techno-thriller was \"The Hunt for Red October\" (1984) by Tom Clancy. It introduced CIA deskman (analyst) Jack Ryan as a field agent; he reprised the role in the sequel \"The Cardinal of the Kremlin\" (1987).\nOther important American authors who became active in spy fiction during this period include Robert Littell, \"The Defection of A. J. Lewinter\" (1973); James Grady, \"Six Days of the Condor\" (1974); William F. Buckley Jr., \"Saving the Queen\" (1976); Nelson DeMille, \"The Talbot Odyssey\" (1984); W. E. B. Griffin, the \"Men at War\" series (1984\u2013); Stephen Coonts, \"Flight of the Intruder\" (1986); Canadian-American author David Morrell, \"The League of Night and Fog\" (1987); David Hagberg, \"Without Honor\" (1989); Noel Hynd, \"False Flags\" (1990); and Richard Ferguson, \"Oiorpata\" (1990).\nSoviet.\nThe culture of Imperial Russia was deeply influenced by the culture of France, and traditionally spy novels in France had a very low status. One consequence of the French influence on Russian culture was that the subject of espionage was usually ignored by Russian writers during the Imperial period. Traditionally, the subject of espionage was treated in the Soviet Union as a story of villainous foreign spies threatening the USSR. The organisation established to hunt down German spies in 1943, SMERSH, was an acronym for the wartime slogan \"Smert shpionam!\" (\"Death to Spies!\"), which reflected the picture promoted by the Soviet state of spies as a class of people who deserved to be killed without mercy. The unfavorable picture of spies ensured that before the early 1960s there were no novels featuring Soviet spies as the heroes as espionage was portrayed as a disreputable activity that only the enemies of the Soviet Union engaged in. Unlike in Britain and the United States, where the achievements of Anglo-American intelligence during the Second World War were to a certain extent publicized soon after the war such as the fact that the Americans had broken the Japanese naval codes (which came out in 1946) and the British deception operation of 1943, Operation Mincemeat (which was revealed in 1953), there was nothing equivalent in the Soviet Union until the early 1960s. Soviet novels prior to the 1960s to the extent that espionage was portrayed at all concerned heroic scouts in the Red Army who during the Great Patriotic War as the war with Germany is known in the Soviet Union who go on dangerous missions deep behind the Wehrmacht's lines to find crucial information. The scout stories were more action-adventure stories than espionage stories proper and significantly always portrayed Red Army scouts rather than \"Chekisty\" (\"Chekists\") as secret policemen are always called in Russia as their heroes. The protagonists of the scout stories always almost ended being killed at the climax of the stories, giving up their lives up to save the Motherland from the German invaders.\nIn November 1961, Vladimir Semichastny became the chairman of the KGB and sent out to improve the image of the \"Chekisty\". The acronym KGB (\"Komitet Gosudarstvennoy Bezopasnosti\"-Committee of State Security) was adopted in 1954, but the organisation had been founded in 1917 as the Cheka. The frequent name changes for the secret police made no impression with the Russian people who still call any secret policeman a \"Chekisty\". Semichastny felt that the legacy of the \"Yezhovshchina\" (\"Yezhovz times\") of 1936-1939 had given the KGB a fearsome reputation that he wanted to erase as wanted ordinary people to have a more favorable and positive image of the \"Chekisty\" as the protectors and defenders of the Soviet Union instead of torturers and killers. As such, Semichastny encouraged the publication of a series of spy novels that featured heroic \"Chekisty\" defending the Soviet Union. It was also during Semichastny's time as KGB chairman that the cult of the \"hero spies\" began in the Soviet Union as publications lionised the achievements of Soviet spies such as Colonel Rudolf Abel, Harold \"Kim\" Philby, Richard Sorge and of the men and women who served in the \"Rote Kapelle\" spy network. Seeing the great popularity of Ian Fleming's James Bond novels in Britain and the United States, Soviet spy novels of the 1960s used the Bond novels as inspiration for both their plots and heroes, through Soviet prurience about sex ensured that the \"Chekisty\" heroes did not engage in the sort of womanising that Bond did. The first Bond-style novel was \"The Zakhov Mission\" (1963) by the Bulgarian writer Andrei Gulyashki who had commissioned by Semichastny and was published simultaneously in Russian and Bulgarian. The success of \"The Zakhov Mission\" led to a follow-up novel, \"Zakhov vs. 007\", where Gulyashki freely violated English copyright laws by using the James Bond character without the permission of the Fleming estate (he had asked for permission in 1966 and was denied). In \"Zakhov vs. 007\", the hero Avakoum Zakhov defeats James Bond, who is portrayed in an inverted fashion to how Fleming portrayed him; in \"Zakhov vs. 007\", Bond is portrayed as a sadistic killer, a brutal rapist and an arrogant misogynist, which stands in marked contrast to the kindly and gentle Zakhov who always treats women with respect. Zakhov is described as a spy, he more of a detective and unlike Bond, his tastes are modest.\nIn 1966, the Soviet writer Yulian Semyonov published a novel set in the Russian Civil War featuring a Cheka agent Maxim Maximovich Isa\u0443ev as its hero. Inspired by its success, the KGB encouraged Semyonov to write a sequel, \"Semnadtsat' mgnoveniy vesny\" (\"Seventeen Moments of Spring\"), which proved to one of the most popular Soviet spy novels when it was serialized in \"Pravda\" in January\u2013February 1969 and then published as a book later in 1969. In \"Seventeen Moments of Spring\", the story is set in the Great Patriotic War as Isayev goes undercover, using the alias of a Baltic German nobleman Max Otto von Stierlitz to infiltrate the German high command. The plot of \"Seventeen Moments of Spring\" takes place in Berlin between January\u2013May 1945 during the last days of the Third Reich as the Red Army advances onto Berlin and the Nazis grew more desperate. In 1973, \"Semnadtsat' mgnoveniy vesny\" was turned into a television mini-series, which was extremely popular in the Soviet Union and turned the Isayev character into a cultural phenomena. The Isayev character plays a role in Russian culture, even today, that is analogous to the role James Bond plays in modern British culture. As aspect of \"Seventeen Moments of Spring\", both as a novel and the TV mini-series that has offended Westerners who are more accustomed to seeing spy stories via the prism of the fast-paced Bond stories is the way that Isayev spends much time interacting with ordinary Germans despite the fact these interactions do nothing to advance the plot and are merely superfluous to the story. However, the point of these scenes are to show that Isayev is still a moral human being, who remains sociable and kind to all people, including the citizens of the state that his country is at war with. Unlike Bond, Isayev is devoted to his wife who he deeply loves and despite spending at least ten years as a spy in Germany and having countless chances to sleep with attractive German women remains faithful towards her. Through Isayev is a spy for the NKVD as the Soviet secret police was known from 1934 to 1946, it is stated quite explicitly in \"Semnadtsat' mgnoveniy vesny\" (which is set in 1945) that he left the Soviet Union to go undercover in Nazi Germany \"more than ten years ago\", which means that Isayev was not involved in the \"Yezhovshchina\".\nLater.\nThe June 1967 Six-Day War between Israel and its neighbours introduced new themes to espionage fiction - the conflict between Israel and the Palestinians, against the backdrop of continuing Cold War tensions, and the increasing use of terrorism as a political tool.\nPost\u2013Cold War.\nThe end of the Cold War in 1991 mooted the USSR, Russia and other Iron Curtain countries as credible enemies of democracy, and the US Congress even considered disestablishing the CIA. Espionage novelists found themselves at a temporary loss for obvious nemeses. \"The New York Times\" ceased publishing a spy novel review column. Nevertheless, counting on the aficionado, publishers continued to issue spy novels by writers popular during the Cold War era, among them \"Harlot's Ghost\" (1991) by Norman Mailer.\nIn the US, the new novels \"Moscow Club\" (1991) by Joseph Finder, \"Coyote Bird\" (1993) by Jim DeFelice, \"Masquerade\" (1996) by Gayle Lynds, and \"The Unlikely Spy\" (1996) by Daniel Silva maintained the spy novel in the post\u2013Cold War world. Other important American authors who first became active in spy fiction during this period include David Ignatius, \"Agents of Innocence\" (1997); David Baldacci, \"Saving Faith\" (1999); and Vince Flynn, with \"Term Limits\" (1999) and a series of novels featuring counter-terrorism expert Mitch Rapp.\nIn 1993, the American novelist Philip Roth published \"Operation Shylock\", an account of his supposed work as a Mossad spy in Greece. The book was published as a novel, but Roth insisted that the book was not a novel as he argued that the book was presented only as a novel in order to give it deniability. At the end of the book, the character of Philip Roth is ordered to publish the account as a novel, and it ends with Roth the character saying: \"And I became quite convinced that it was my interest to do that...I'm just a good Mossadnik\".\nIn the UK, Robert Harris entered the spy genre with \"Enigma\" (1995). Other important British authors who became active during this period include Hugh Laurie, \"The Gun Seller\" (1996); Andy McNab, \"Remote Control\" (1998); Henry Porter, \"Remembrance Day\" (2000); and Charles Cumming, \"A Spy By Nature\" (2001).\nPost\u20139/11.\nThe terrorist attacks against the US on 11 September 2001, and the subsequent War on Terror, reawakened interest in the peoples and politics of the world beyond its borders. Espionage genre elders such as John le Carr\u00e9, Frederick Forsyth, Robert Littell, and Charles McCarry resumed work, and many new authors emerged.\nImportant British writers who wrote their first spy novels during this period include Stephen Leather, \"Hard Landing\" (2004); and William Boyd, \"Restless\" (2006).\nNew American writers include Brad Thor, \"The Lions of Lucerne\" (2002); Ted Bell, \"Hawke\" (2003); Alex Berenson, with John Wells appearing for the first time in \"The Faithful Spy\" (2006); Brett Battles, \"The Cleaner\" (2007); Ellis Goodman, \"Bear Any Burden\" (2008); Olen Steinhauer, \"The Tourist\" (2009); and Richard Ferguson, \"Oiorpata\" (2012). A number of other established writers began to write spy fiction for the first time, including Kyle Mills, \"Fade\" (2005) and James Patterson, \"Private\" (2010).\nSwede Stieg Larsson, who died in 2004, was the world's second best-selling author for 2008 due to his \"Millennium series\", featuring Lisbeth Salander, published posthumously between 2005 and 2007. Other authors of note include Australian James Phelan, beginning with \"Fox Hunt\" (2010).\nRecognising the importance of the thriller genre, including spy fiction, International Thriller Writers (ITW) was established in 2004, and held its first conference in 2006.\nInsider spy fiction.\nMany authors of spy fiction have themselves been intelligence officers working for British agencies such as MI5 or MI6, or American agencies such as the OSS or its successor, the CIA. 'Insider' spy fiction has a special claim to authenticity and overlaps with biographical and other documentary accounts of secret service.\nThe first insider fiction emerged after World War 1 as the thinly disguised reminiscences of former British intelligence officers such as W. Somerset Maugham, Alexander Wilson, and Compton Mackenzie. The tradition continued during World War II with Helen MacInnes and Manning Coles.\nNotable British examples from the Cold War period and beyond include Ian Fleming, John le Carr\u00e9, Graham Greene, Brian Cleeve, Ian Mackintosh, Kenneth Benton, Bryan Forbes, Andy McNab and Chris Ryan. Notable American examples include Charles McCarry, William F. Buckley Jr., W. E. B. Griffin and David Hagberg.\nMany post-9/11 period novels are written by insiders. At the CIA, the number of manuscripts submitted for pre-publication vetting doubled between 1998 and 2005. American examples include Barry Eisler, \"A Clean Kill in Tokyo\" (2002); Charles Gillen, \"Saigon Station\" (2003); R J Hillhouse, \"Rift Zone\" (2004); Gene Coyle, \"The Dream Merchant of Lisbon\" (2004) and \"No Game For Amateurs\" (2009); Thomas F. Murphy, \"Edge of Allegiance\" (2005); Mike Ramsdell, \"A Train to Potevka\" (2005); T. H. E. Hill, \"Voices Under Berlin: The Tale of a Monterey Mary\" (2008); Duane Evans, \"North from Calcutta\" (2009); Jason Matthews, \"Red Sparrow\" (2013).; and T.L. Williams, \"Zero Day: China's Cyber Wars\" (2017).\nBritish examples include \"The Code Snatch\" (2001) by Alan Stripp, formerly a cryptographer at Bletchley Park; \"At Risk\" (2004), \"Secret Asset\" (2006), \"Illegal Action\" (2007), and \"Dead Line\" (2008), by Dame Stella Rimington (Director General of MI5 from 1992 to 1996); and Matthew Dunn's \"Spycatcher\" (2011) and sequels.\nSpy television and cinema.\nCinema.\nMuch spy fiction was adapted as spy films in the 1960s, ranging from the fantastical James Bond series to the realistic \"The Spy Who Came in from the Cold\" (1965), and the hybrid \"The Quiller Memorandum\" (1966). While Hamilton's Matt Helm novels were adult and well written, their cinematic interpretations were adolescent parody. This phenomenon spread widely in Europe in the 1960s and is known as the Eurospy genre.\nEnglish-language spy films of the 2000s include \"The Bourne Identity\" (2002), \"\" (1996); \"Munich\" (2005), \"Syriana\" (2005), and \"The Constant Gardener\" (2005).\nAmong the comedy films focusing on espionage are 1974's \"S*P*Y*S\", 1985's \"Spies Like Us\", and the \"Austin Powers\" film series starring Mike Myers.\nTelevision.\nThe American adaptation of \"Casino Royale\" (1954) featured Jimmy Bond in an episode of the \"Climax!\" anthology series. The narrative tone of television espionage ranged from the drama of \"Danger Man\" (1960\u20131968) to the sardonicism of \"The Man from U.N.C.L.E.\" (1964\u20131968) and the flippancy of \"I Spy\" (1965\u20131968) until the exaggeration, akin to that of William Le Queux and E. Phillips Oppenheim before the World War I, degenerated to the parody of \"Get Smart\" (1965\u20131970).\nIn 1973, Semyonov's novel \"Seventeen Moments of Spring\" (1968) was adapted to television as a twelve-part mini-series about the Soviet spy Maksim Isaev operating in wartime Nazi Germany as Max Otto von Stierlitz, charged with preventing a separate peace between Nazi Germany and America which would exclude the USSR. The programme \"TASS Is Authorized to Declare...\" also derives from his work.\nHowever, the circle closed in the late 1970s when \"The Sandbaggers\" (1978\u20131980) presented the grit and bureaucracy of espionage.\nIn the 1980s, American television featured the light espionage programmes \"Airwolf\" (1984\u201387) and \"MacGyver\" (1985\u201392), each rooted in the Cold War yet reflecting American citizens' distrust of their government, after the crimes of the Nixon Government (the internal, political espionage of the Watergate Scandal and the Vietnam War) were exposed. The spy heroes were independent of government; MacGyver, in later episodes and post-DXS employment, works for a non-profit, private think tank, and aviator Hawke and two friends work free-lance adventures. Although each series features an intelligence agency, the DXS in \"MacGyver\", and the FIRM, in \"Airwolf\", its agents could alternately serve as adversaries as well as allies for the heroes.\nTelevision espionage programmes of the late 1990s to the early 2010s include \"La Femme Nikita\" (1997\u20132001), \"Alias\" (2001\u20132006), \"24\" (2001\u20132010, 2014), \"Spooks\" in the UK (release as \"MI-5\" in the US and Canada) (2002\u20132011), CBBC's \"The Secret Show\" (2006\u20132011), NBC's \"Chuck\" (2007\u20132012), FX's \"Archer\" (2009\u20132023), \"Burn Notice\" (2007\u20132013), \"Covert Affairs\" (2010\u20132014), \"Homeland\" (2011\u20132020), \"The Americans\" (2013\u20132018) and ABC's \"Agents of S.H.I.E.L.D.\" (2013\u20132020).\nIn 2015, \"Deutschland 83\" is a German television series starring a 24-year-old native of East Germany who is sent to the West as an undercover spy for the HVA, the foreign intelligence agency of the Stasi.\nFor children and adolescents.\nBooks and novels.\nIn every medium, spy thrillers introduce children and adolescents to deception and espionage at earlier ages. The genre ranges from action-adventure, such as Chris Ryan's \"Alpha Force\" series, through the historical espionage dramas of Y. S. Lee, to the girl orientation of Ally Carter's \"Gallagher Girls\" series, beginning with \"I'd Tell You I Love You, But Then I'd Have to Kill You\".\nLeading examples include the \"Agent Cody Banks\" film, the Alex Rider adventure novels by Anthony Horowitz, and the CHERUB series, by Robert Muchamore. Ben Allsop, one of England's youngest novelists, also writes spy fiction. His titles include \"Sharp\" and \"The Perfect Kill\".\nOther authors writing for adolescents include A. J. Butcher, Joe Craig, Charlie Higson, Andy McNab and Francine Pascal.\nFilms and shows.\nSpy-related films that are aimed towards younger audiences include movies such as the Spy Kids series of films and \"The Spy Next Door\". Shows and series in this category also include a subplot of \"Phineas and Ferb\" following Perry the Platypus in his attempt to sabotage Doofenshmirtz's plans to take over the geographically ambiguous Tri-state area. However, the Cartoon Network show \"\" is solely focused on the eponymous Kids Next Door organization, consisting of child spies and child soldiers fighting and spying on adult and teenage villains, who are personifications of the things children dislike while growing up (e.g. bullying, grounding, homework, going to the dentist, going to school, being force-fed vegetables, getting banned from drinking soda, helicopter parenting, piano lessons, and spanking), and whilst not being traditional government sponsored intelligence, the Kids Next Door market themselves as so. Another example of a kids' show in the spy genre is Disney's \"Kim Possible\", which centers on the eponymous protagonist as she fights megalomaniac villains in a similar manner to James Bond, while foiling the evil plans of the main antagonist of the show, Dr. Drakken.\nVideo games, tabletop roleplaying games and theme parks.\nIn contemporary digital video games, the player can be a vicarious spy, as in \"Team Fortress 2\" and the \"Metal Gear series\", especially in the series' third installment, \"Metal Gear Solid\", unlike the games of the third-person shooter genre, \"Syphon Filter\", and \"Splinter Cell\". The games feature complex stories and cinematic images. Games such as \"No One Lives Forever\" and the sequel \"\" humorously combine espionage and 1960s design. \"Evil Genius\", a real-time strategy game and contemporary of the \"No One Lives Forever\" series, allows the player to take on the role of the villain in a setting heavily influenced by spy thriller fiction like the \"James Bond\" series.\nThe \"Deus Ex\" series, particularly ' and ', are also examples of spy fiction. Protagonist Adam Jensen must frequently use spycraft and stealth to obtain sensitive information for a variety of clients and associates.\n\"Top Secret\", TSR, Inc., (1980) is a contemporary espionage-themed tabletop role-playing game\n\"James Bond 007\": Role-Playing In Her Majesty's Secret Service, Victory Games (1983), is a tabletop roleplaying game based on Flemming's 007 novels.\nActivision published \"\" (1996), notable for the collaboration with former CIA director William Colby and former KGB Major-General Oleg Kalugin, who also appear in the game as themselves.\nNamco Bandai's \"Time Crisis\" series of light gun shooters centers on the exploits of a fictional multinational intelligence agency called the VSSE (Vital Situation, Swift Elimination), whose agents, armed with a license to kill, must stop terrorists and megalomaniac villains in a similar manner to \"\" and the \"James Bond\" movies.\nThe \"Spyland\" espionage theme park, in the Gran Scala pleasure dome, in Zaragoza province, Spain, opened in 2012.\nNotable writers.\nDeceased.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nLiving.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "27647", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=27647", "title": "Star height problem", "text": "Problem in formal language theory\nThe star height problem in formal language theory is the question whether all regular languages can be expressed using regular expressions of limited star height, i.e. with a limited nesting depth of Kleene stars. Specifically, is a nesting depth of one always sufficient? If not, is there an algorithm to determine how many are required? The problem was first introduced by Eggan in 1963.\nFamilies of regular languages with unbounded star height.\nThe first question was answered in the negative when in 1963, Eggan gave examples of regular languages of star height \"n\" for every \"n\". Here, the star height \"h\"(\"L\") of a regular language \"L\" is defined as the minimum star height among all regular expressions representing \"L\". The first few languages found by Eggan are described in the following, by means of giving a regular expression for each language:\nformula_1\nThe construction principle for these expressions is that expression formula_2 is obtained by concatenating two copies of formula_3, appropriately renaming the letters of the second copy using fresh alphabet symbols, concatenating the result with another fresh alphabet symbol, and then by surrounding the resulting expression with a Kleene star. The remaining, more difficult part, is to prove that for formula_3 there is no equivalent regular expression of star height less than \"n\"; a proof is given in .\nHowever, Eggan's examples use a large alphabet, of size 2\"n\"-1 for the language with star height \"n\". He thus asked whether we can also find examples over binary alphabets. This was proved to be true shortly afterwards by Dejean and Sch\u00fctzenberger in 1966.\nTheir examples can be described by an inductively defined family of regular expressions over the binary alphabet formula_5 as follows\u2013cf. :\nformula_6\nAgain, a rigorous proof is needed for the fact that formula_3 does not admit an equivalent regular expression of lower star height. Proofs are given by and by .\nComputing the star height of regular languages.\nIn contrast, the second question turned out to be much more difficult, and the question became a famous open problem in formal language theory for over two decades. For years, there was only little progress. The pure-group languages were the first interesting family of regular languages for which the star height problem was proved to be decidable. But the general problem remained open for more than 25 years until it was settled by Hashiguchi, who in 1988 published an algorithm to determine the star height of any regular language. The algorithm wasn't at all practical, being of non-elementary complexity. To illustrate the immense resource consumptions of that algorithm, give some actual numbers:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[The procedure described by Hashiguchi] leads to computations that are by far impossible, even for very small examples. For instance, if \"L\" is accepted by a 4 state automaton of loop complexity 3 (and with a small 10 element transition monoid), then a \"very low minorant\" of the number of languages to be tested with \"L\" for equality is:\nNotice that alone the number formula_9 has 10 billion zeros when written down in decimal notation, and is already \"by far\" larger than the number of atoms in the observable universe.\nA much more efficient algorithm than Hashiguchi's procedure was devised by Kirsten in 2005. This algorithm runs, for a given nondeterministic finite automaton as input, within double-exponential space. Yet the resource requirements of this algorithm still greatly exceed the margins of what is considered practically feasible.\nThis algorithm has been optimized and generalized to trees by Colcombet and L\u00f6ding in 2008, as part of the theory of regular cost functions.\nIt has been implemented in 2017 in the tool suite Stamina.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27648", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=27648", "title": "William Crookes", "text": "English chemist and physicist (1832\u20131919)\nSir William Crookes (; 17 June 1832 \u2013 4 April 1919) was an English chemist and physicist who attended the Royal College of Chemistry, now part of Imperial College London, and worked on spectroscopy. He was a pioneer of vacuum tubes, inventing the Crookes tube, which was made in 1875. Observing cathode rays generated in these tubes, Crookes posited that \"radiant matter\" was a unique fourth state of matter, a foundational contribution to plasma physics.\nHe is credited with discovering the element thallium, announced in 1861, with the help of spectroscopy. He was also the first to describe the spectrum of terrestrial helium, in 1865. Crookes was the inventor of the Crookes radiometer, but did not discern the true explanation of the phenomenon he detected. Crookes also invented a 100% ultraviolet blocking sunglass lens. For a time, he was interested in spiritualism and became president of the Society for Psychical Research.\nAt the end of his life, Crookes was appraised as \"superb experimentalist\" known for the originality of his inquiry into physics and chemistry. He was praised for his industriousness and intellectual qualities. His interests, ranging over pure and applied science, economic and practical problems, and psychic research, made him a well-known personality and earned him a substantial income. He received many public and academic honours.\nBiography.\nEarly years.\nWilliam Crookes was born in London in 1832, the eldest of eight surviving children (eight others died young) of Joseph Crookes (1792\u20131889), a wealthy tailor and real estate investor of north-country origin, and his second wife, Mary (n\u00e9e Scott; 1806\u20131884). Joseph Crookes's father, William (1734\u20131814), was also a tailor, whose grandfather, John Crookes (b. 1660), had been Mayor of Hartlepool, County Durham on three occasions.\nJoseph Crookes had had five children with his first wife; two sons from that marriage, Joseph and Alfred, took over the tailoring business, leaving William free to choose his own path. In 1848, at age 16, Crookes entered the Royal College of Chemistry (now the Imperial College chemistry department) to study organic chemistry. Crookes lived with his parents about three miles from the College in Oxford Street. His father's shop was about half a mile away. Crookes paid \u00a325 for his first year's tuition and had to provide his own apparatus and some of the more expensive chemicals. At the end of his first year, Crookes won the Ashburton scholarship which covered his second year's tuition. At the end of his second year, Crookes became a junior assistant to August Wilhelm von Hofmann, doing laboratory demonstrations and helping with research and commercial analysis. In October 1851, Crookes was promoted to senior assistant, a position he held until 1854.\nAlthough Crookes revered Hofmann, he did not share his primary interest in organic chemistry. \nOne of Crookes's students was the Reverend John Barlow, Secretary of the Royal Institution, who chose to take a course in analytical chemistry. Through Barlow, Crookes met scientists such as George Gabriel Stokes and Michael Faraday.\nSuch friends reinforced Crookes's interest in optical physics which was respected by Hofmann.\nBy 1851, Crookes's interest in photography and optics caused his father to build him a laboratory in the garden at home for his research.\nWhen Crookes embarked upon original work, it wasn't in organic chemistry, but rather into new compounds of selenium. These were the subject of his first published papers, in 1851. He worked with Manuel Johnson at the Radcliffe Observatory in Oxford in 1854, where he adapted the recent innovation of wax paper photography to machines built by Francis Ronalds to continuously record meteorological parameters. In 1855 he was appointed lecturer in chemistry at the Chester Diocesan Training College.\nIn April 1856 Crookes married Ellen, daughter of William Humphrey of Darlington. Since staff at Chester were required to be bachelors, he had to resign his position. William's father, Joseph Crookes, gave the couple a house at 15 Stanley Street, Brompton. Ellen's mother, Mrs. Humphrey, lived with them for the rest of her life, nearly forty years. A devoted couple, William and Ellen Crookes had six sons and three daughters. Their first child, Alice Mary (born 1857, later Mrs. Cowland) remained unmarried for forty years, living with her parents and working as an assistant to her father. Two of Crookes's sons became engineers, and two lawyers.\nMarried and living in London, Crookes sought to support his new family through independent work as a photographic chemist. In 1859, he founded the \"Chemical News\", a science magazine which he edited for many years and conducted on much less formal lines than was usual for the journals of scientific societies. In his \"Chemical News\" Crookes published Faraday's \"A Course of Six Lectures on the Various Forces of Nature\" and in 1860 he edited the lectures and published it as a book (he also wrote the Preface). Between 1864 and 1869, he was also involved with the \"Quarterly Journal of Science\". At various times he edited the \"Journal of the Photographic Society\" and the \"Photographic News\".\nMiddle years.\nCrookes was effective in experimentation. The method of spectral analysis, introduced by Bunsen and Kirchhoff, was received by Crookes with great enthusiasm and to great effect.\nHis first important discovery was that of the element thallium, made with the help of flame spectroscopy. Crookes discovered a previously unknown element with a bright green emission line in its spectrum. He named the element thallium, from Greek , , meaning \"a green shoot or twig\". Crookes's findings were published on 30 March 1861.\nThallium was also independently discovered by Frenchman Claude Auguste Lamy, who had the advantage of access to large amounts of materials via his brother-in-law, Charles Fr\u00e9d\u00e9ric Kuhlmann. Both Crookes and Lamy isolated the element in 1862.\nCrookes was elected a fellow of the \"Royal Society\" in 1863. Crookes wrote a standard treatise on \"Select Methods in Chemical Analysis\" in 1871.\nIn 1866, Adolf Erik Nordenski\u00f6ld identified a rare mineral from Skrikerum as a selenide of copper, thallium, and silver (Cu7(Tl, Ag)Se4), and named the mineral crookesite in honor of Sir William Crookes.\nCrookes developed the Crookes tube, investigating cathode rays. He published numerous papers on spectroscopy and conducted research on a variety of minor subjects. In his investigations of the conduction of electricity in low pressure gases, he discovered that as the pressure was lowered, the negative electrode (cathode) appeared to emit rays (the so-called \"cathode rays\", now known to be a stream of free electrons, and used in cathode-ray display devices). As these examples indicate, he was a pioneer in the construction and use of vacuum tubes for the study of physical phenomena. He was, as a consequence, one of the first scientists to investigate what is now called a plasma and identified it as the fourth state of matter in 1879. He also devised one of the first instruments for studying nuclear radioactivity, the spinthariscope.\nCrookes investigated the properties of cathode rays, showing that they travel in straight lines, cause fluorescence when they fall on some substances, and that their impact can produce great heat. He believed that he had discovered a fourth state of matter, which he called \"radiant matter\", but his theoretical views on the nature of \"radiant matter\" were to be superseded. He believed the rays to consist of streams of particles of ordinary molecular magnitude. It remained for Sir J. J. Thomson to expound on the subatomic nature of cathode rays (consisting of streams of negative electrons). Nevertheless, Crookes's experimental work in this field was the foundation of discoveries which eventually changed the whole of chemistry and physics.\nCrookes's attention had been attracted to the vacuum balance in the course of his research into thallium. He soon discovered the phenomenon which drives the movement in a Crookes radiometer, in which a set of vanes, each blackened on one side and polished on the other, rotate when exposed to radiant energy. Crookes did not, however, provide the true explanation of this apparent \"attraction and repulsion resulting from radiation\".\nAfter 1880, Crookes lived at 7 Kensington Park Gardens in the fashionable area of Notting Hill. His household included a large multigenerational family and a number of servants. There all his later work was done, in what was then \"the finest private laboratory in Britain\". It comprised an entire floor of the house and included three interconnected laboratory rooms, for chemistry, physics, and mechanical construction, and a library. Crookes was able to purchase the house and build the laboratory because of his income from the National Guano Company and from various patents.\nBy 1880 Crookes employed a paid full-time scientific assistant (first Charles Gimingham and after 1883 James Gardiner). He was also helped by his daughter Alice, who was \"adept at fractionating rare earth elements\" and \"no mean interpreter of spectra\".\nHis daily routine was to manage his commercial affairs in the morning, do further business or go to scientific meetings in the afternoon, eat dinner at 7, work in his library from 8 to 9, and then in the laboratory until after midnight. From his home, Crookes could easily reach the \"Chemical News\" offices, the Royal Society, the Chemica Society, and the Athenaeum Club.\nOn 16 January 1884, Crookes's father died. Crookes's daughter Florence died of scarlet fever in the same week. Joseph Crookes's estate was left in trust, divided between his three surviving sons, Alfred, William and Frank. Combined with his previous income, this ensured that Crookes was very well off.\nIn 1886, Crookes was elected as a member to the American Philosophical Society.\nLater years.\nOn 13 August 1894, John William Strutt, 3rd Baron Rayleigh and William Ramsay announced the detection of a new gas in the atmosphere. On 31 January 1895 they made a full report to the Royal Society on the new gas, argon. In addition, William Crookes, who had been asked to examine a sample, presented on the spectra of argon, reported that argon displayed two distinct spectra. A few month later Ramsay isolated helium from the mineral cleveite. Crookes confirmed the identify of the gas helium establishing its correspondence with the earlier observations of Janssen and Lockyer of solar helium. The discovery of argon and of helium led to identification of the noble gases and the extension of the periodic system to include an additional column. \nCrookes himself suggested a design for a Periodic table in the style of a space lemniscate in 1898.\nCrookes was knighted in 1897.\nCrookes was named president of the British Association for the Advancement of Science in 1898. In his inaugural address, he outlined in detail a coming catastrophe: The wheat-eating peoples of the world were going to start running out of food in the 1930s. The reason, he said, was a dearth of nitrogen fertilizer available from natural sources. Crookes called on chemists to develop new ways of making fertilizer from the enormous stock of nitrogen in the atmosphere (which is roughly 80 percent nitrogen). His remarks on the coming famine achieved wide distribution in the press and were turned into a popular book. Scientists addressing the problem in the first years of the twentieth century included Kristian Birkeland, whose technology helped found Norsk Hydro, and Fritz Haber and Carl Bosch, whose Haber\u2013Bosch process forms the foundation of today's nitrogen fertilizer industry.\nIn 1903, Crookes turned his attention to the newly discovered phenomenon of radioactivity, achieving the separation from uranium of its active transformation product, \"uranium-X\" (later established to be protactinium). Crookes observed the gradual decay of the separated transformation product, and the simultaneous reproduction of a fresh supply in the original uranium. At about the same time as this important discovery, he observed that when \"p-particles\", ejected from radio-active substances, impinge upon zinc sulfide, each impact is accompanied by a minute scintillation, an observation which forms the basis of one of the most useful methods in the detection of radioactivity.\nIn 1913, Crookes created an ultraviolet blocking lens made from glass containing cerium, but only lightly tinted. These inventions were the by-product of Crookes's research to find a lens glass formulation that would protect glass workers from cataracts. Crookes tested more than 300 formulations, each numbered and labelled. Crookes Glass 246 was the tint recommended for glassworkers. The best-known Crookes tints are \"A\" (withdrawn due to its uranium), \"A1\", \"B\", and \"B2\", which absorb all ultraviolet below 350\u00a0nm while darkening visual light. Crookes's samples were made by Whitefriars, London, stained glass makers, and Chance Brothers, Birmingham.\nCrookes died in 1919 at the age of 86.\nSpiritualism.\nCrookes became interested in spiritualism in the late 1860s, and was most strongly involved around 1874\u20131875. Eric Deeson notes that Crookes's studies of the occult are related to his scientific work on radiometry in that both involved the detection of previously undiscovered forces.\nCrookes was possibly influenced by the death of his younger brother Philip in 1867 at 21 from yellow fever contracted while he was on an expedition to lay a telegraph cable from Cuba to Florida. \nIn 1867, influenced by Cromwell Fleetwood Varley, Crookes attended a s\u00e9ance to try to get in touch with his brother.\nBetween 1871 and 1874, Crookes studied the mediums Kate Fox, Florence Cook, and Daniel Dunglas Home. After his investigation, he believed that the mediums could produce genuine paranormal phenomena and communicate with spirits. Psychologists Leonard Zusne and Warren H. Jones have described Crookes as gullible as he endorsed fraudulent mediums as genuine.\nThe anthropologist Edward Clodd noted that Crookes had poor eyesight, which may have explained his belief in spiritualist phenomena and quoted William Ramsay as saying that Crookes is \"so shortsighted that, despite his unquestioned honesty, he cannot be trusted in what he tells you he has seen.\" Biographer William Hodson Brock wrote that Crookes was \"evidently short-sighted, but did not wear spectacles until the 1890s. Until then he may have used a monocle or pocket magnifying glass when necessary. What limitations this imposed upon his psychic investigations we can only imagine.\"\nAfter studying the reports of Florence Cook, the science historian Sherrie Lynne Lyons wrote that the alleged spirit \"Katie King\" was at times Cook herself and at other times an accomplice. Regarding Crookes, Lyons wrote, \"Here was a man with a flawless scientific reputation, who discovered a new element, but could not detect a real live maiden who was masquerading as a ghost\". Cook was repeatedly exposed as a fraudulent medium but she had been \"trained in the arts of the s\u00e9ance\" which managed to trick Crookes. Some researchers such as Trevor H. Hall suspected that Crookes had an affair with Cook.\nIn a series of experiments in London, England at the house of Crookes in February 1875, the medium Anna Eva Fay managed to fool Crookes into believing she had genuine psychic powers. Fay later confessed to her fraud and revealed the tricks that she had used. Regarding Crookes and his experiments with mediums, the magician Harry Houdini suggested that Crookes had been deceived. The physicist Victor Stenger wrote that the experiments were poorly controlled and \"his desire to believe blinded him to the chicanery of his psychic subjects.\"\nIn 1897, John Grier Hibben wrote that Crookes's idea of ether waves explaining telepathy was not a scientific hypothesis \"he presents no facts to indicate its probability or to save it from being relegated to the sphere of bare conjecture.\"\nIn 1916, William Hope tricked Crookes with a fake spirit photograph of his wife. Oliver Lodge revealed there had been obvious signs of double exposure, the picture of Lady Crookes had been copied from a wedding anniversary photograph, but Crookes was a convinced spiritualist and claimed it was genuine evidence for spirit photography.\nThe physiologist Gordon Stein suspected that Crookes was too ashamed to admit he had been duped by the medium Florence Cook or that he conspired with her for sexual favors. He also suggested that Crookes had conspired with Anna Eva Fay. He noted that contrary to popular belief, Hope had been exposed as a fraud on several occasions. Stein concluded that all feats of Hope were conjuring tricks. In a review, biographer William Brock wrote that Stein made his \"case against Crookes and Hope clearly and logically.\"\nCrookes joined the Society for Psychical Research, becoming its president in the 1890s: he also joined the Theosophical Society and The Ghost Club, of which he was president from 1907 to 1912. In 1890 he was initiated into the Hermetic Order of the Golden Dawn.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27650", "revid": "47226626", "url": "https://en.wikipedia.org/wiki?curid=27650", "title": "September 16", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearSeptember 16 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27651", "revid": "754658", "url": "https://en.wikipedia.org/wiki?curid=27651", "title": "September 23", "text": "&lt;templatestyles src=\"This date in recent years/styles.css\"/&gt;\nDay of the yearSeptember 23 is the day of the year in the Gregorian calendar\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27653", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=27653", "title": "Symbolic Logic", "text": ""}
{"id": "27654", "revid": "45807063", "url": "https://en.wikipedia.org/wiki?curid=27654", "title": "Symbolic logic", "text": ""}
{"id": "27655", "revid": "33081259", "url": "https://en.wikipedia.org/wiki?curid=27655", "title": "Sonny Bono", "text": "American singer, record producer, comedian, actor, and politician (1935\u20131998)\nSalvatore Phillip \"Sonny\" Bono ( ; February 16, 1935 \u2013 January 5, 1998) was an American singer, songwriter, actor, and politician. In partnership with his second wife, Cher, he formed the singing duo Sonny &amp; Cher. A member of the Republican Party, Bono served as the 16th mayor of Palm Springs, California, from 1988 to 1992, and served as the U.S. representative for California's 44th district from 1995 until his death in 1998.\nThe United States Copyright Term Extension Act of 1998, which extended the term of copyright by 20 years, was named in honor of Bono when it was passed by Congress nine months after his death. Mary Bono (his widow and successor in Congress) had been one of the original sponsors of the legislation, commonly known as the Sonny Bono Copyright Term Extension Act.\nEarly life.\nBono was born in Detroit, the son of Zena \"Jean\" (n\u00e9e\u00a0DiMercurio) and Santo Bono. His father was born in Montelepre, Palermo, Italy; his mother was also of Italian descent. His mother called him \"Sono\" as a term of endearment, which evolved over time into \"Sonny\". Sonny was the youngest of three siblings; he had two sisters, Fran and Betty. The family moved to Inglewood, California, when he was seven, and his parents divorced soon afterwards. Bono decided early in life to become part of the music business, and began writing songs as a teenager. \"Koko Joe\", a song he wrote at age 16, was recorded by Don and Dewey in 1958, and later covered by several other artists, including the Righteous Brothers. Bono attended Inglewood High School, but did not graduate, opting to drop out so he could begin to pursue a career as a songwriter and performer. He worked at a variety of jobs while trying to break into the music business, including waiter, truck driver, construction laborer, and butcher's helper.\nCareer.\nEntertainment career.\nBono began his music career as a songwriter at Specialty Records, where his song \"Things You Do to Me\" was recorded by Sam Cooke, and went on to work for record producer Phil Spector in the early 1960s as a promotion man, percussionist and \"gofer\". One of his earliest songwriting efforts, \"Needles and Pins\", was co-written with Jack Nitzsche, another member of Spector's production team. In 1965, he achieved commercial success with his wife Cher in the singing duo Sonny &amp; Cher. Bono wrote, arranged and produced a number of hit records including the singles \"I Got You Babe\" and \"The Beat Goes On\", although Cher received more attention as a performer. He played a major part in Cher's early solo recording career, writing and producing singles including \"Bang Bang\" and \"You Better Sit Down Kids\".\nUnder a pseudonym, Bono co-wrote \"She Said Yeah\", covered by the Rolling Stones on their 1965 LP \"Out of Our Heads\". His lone hit single as a solo artist, \"Laugh at Me\", was released in 1965 and peaked at No. 10 on the \"Billboard\" Hot 100. In live concerts, Bono introduced the song by saying \"I'd like to sing a medley of my hit\". \"Laugh at Me\" was also covered by Mott The Hoople on their first album, released in 1969. His only other single as a solo artist, \"The Revolution Kind\", reached No. 70 on the \"Billboard\" Hot 100 later that year. His solo album, \"Inner Views\", was released in 1967.\nBono continued to work with Cher through the early and mid-1970s, starring in a popular television variety show, \"The Sonny &amp; Cher Comedy Hour\", which ran on CBS from 1971 to 1974. Around the time that \"The Sonny and Cher Comedy Hour\" was in development, Bono grew his now famous mustache, which he would continue to wear for the rest of his life. In 1974, his solo variety show, \"The Sonny Comedy Revue\", ran on ABC for one season. From 1976 to 1977, the duo, since divorced, returned to perform together on \"The Sonny and Cher Show\". Their last appearance together was on \"Late Night with David Letterman\" on November 13, 1987, on which they sang \"I Got You Babe\".\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n \"What we call a hook hits you, ... then you're almost not writing, lyrics come to you, a sort of magic takes over, and it's not like work at all.\"\n -Sonny Bono on songwriting, 1967 \"Pop Chronicles\" interview.\nIn 2011, Sonny Bono was inducted into the Michigan Rock and Roll Legends Hall of Fame.\nFilm and television.\nBono's acting career included bit parts as a guest performer in such television series as \"The Man From U.N.C.L.E.\" (appearing with Cher in \"The Hot Number Affair\"), \"Fantasy Island\", \"Charlie's Angels\", \"The Love Boat\", \"The Six Million Dollar Man\" and \"CHiPs\". In the 1975 TV movie \"Murder on Flight 502\", he played the role of rock star Jack Marshall. He appeared in the 1980 miniseries \"Top of the Hill\". He played the role of mad bomber Joe Selucci in ' (1982) and played the role of Peter Dickinson in the 1986 horror film \"Troll.\" He portrayed racist entrepreneur Franklin Von Tussle in the John Waters film \"Hairspray\" (1988). He appeared as the Mayor of Palm Springs (which he was at the time) in several episodes of \"P.S. I Luv U\" during the 1991\u201392 TV season, and on ' (in season 1, episode 9 \"The Man of Steel Bars\", which aired November 21, 1993), he played Mayor Frank Berkowitz. He made a minor appearance as himself in the comedy film \"First Kid\" (1996). A portrait of Bono appeared in the \"VeggieTales\" episode \"Dave and the Giant Pickle\", in the Silly Songs with Larry segment \"I Love My Lips\" (original version).\nBono guest-starred as himself on \"The Golden Girls\" episode \"Mrs. George Devereaux\" (originally broadcast November 17, 1990), in which he vied with Lyle Waggoner for Dorothy's (Bea Arthur) affection in a dream sequence. In Blanche's (Rue McClanahan) dream, her husband is still alive, and Bono uses his power as Mayor of Palm Springs to have Waggoner falsely arrested so he can have Dorothy to himself. Sophia (Estelle Getty) had been hoping for Sonny and Dorothy to get together and actively supported Sonny.\nPolitical career.\nBono entered politics after experiencing frustration with local government bureaucracy while trying to open a restaurant in Palm Springs, California. He made a successful bid for mayor and served from 1988 to 1992. As mayor, Bono spearheaded the creation of the Palm Springs International Film Festival, which is held each year in his memory.\nBono ran for the Republican nomination for United States Senate in 1992, but lost to the more conservative Bruce Herschensohn, who lost the general election to Democrat Barbara Boxer. Bono and Herschensohn became close friends after the campaign. In 1994, Bono planned to run for lieutenant governor, but decided to run for Congress when Republican Al McCandless announced his retirement. Bono won the Republican nomination and the general election to represent California's 44th congressional district. He was reelected in 1996 and served from January 1995 until his death.\nIn the House, Bono was one of 12 co-sponsors of a House bill extending the length of terms for copyright protection. Although that bill was never voted on in the Senate, a similar Senate bill was passed after his death and named the Sonny Bono Copyright Term Extension Act in his memory. It is also known (derisively) as the Mickey Mouse Protection Act.\nHe championed the restoration of the Salton Sea, bringing the giant lake's plight to national attention. In 1998, then Speaker of the House Newt Gingrich made a public appearance and speech at the shore of the lake on Bono's behalf.\nIn their book \"Tell Newt to Shut Up\", David Maraniss and Michael Weisskopf credit Bono with being the first person to recognize Gingrich's public relations problems in 1995. Drawing on his long experience as a celebrity and entertainment producer, Bono (according to Maraniss and Weisskopf) recognized that Gingrich's status had changed from politician to celebrity and that he was not making allowances for that change:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;You're a celebrity now... The rules are different for celebrities. I know it. I've been there. I've been a celebrity. I used to be a bigger celebrity. But let me tell you, you're not being handled right. This is not political news coverage. This is celebrity status. You need handlers. You need to understand what you're doing. You need to understand the attitude of the media toward celebrities.\nBono remains the only member of Congress to have scored a number-one pop single on the US \"Billboard\" Hot 100 chart.\nPersonal life.\nBono was a godparent of Anthony Kiedis of the band Red Hot Chili Peppers; he was a close friend of Kiedis's father, Blackie Dammett.\nMarriages.\nBono was married four times. He married his first wife, Donna Rankin, on November 3, 1954. Their daughter Christine (\"Christy\") was born on June 24, 1958. They divorced in 1962.\nIn 1964, Bono and Cheryl Sarkisian, later known as singer and actress Cher, had an unofficial wedding. The couple legally married in 1969 after the birth of their daughter, Chastity Sun Bono, who later, as a trans man legally changed his name to Chaz Salvatore Bono. Bono and Cher ended their romantic relationship in 1972 but remained married to maintain their public image. Their marriage had deteriorated due to Sonny's infidelity and controlling behavior. By 1973, they were still sharing a home but were dating other people. After competing legal filings in 1974 and a bitter custody battle, they finalized their divorce in 1975. Despite the turmoil, they eventually co-parented amicably, and Bono later apologized to Cher for how he treated her.\nOn New Year's Eve 1981, Bono married actress-model Susie Coelho. They divorced in 1984.\nIn 1986, he married Mary Whitaker. The couple had two children: son Chesare Elan in 1988 and daughter Chianna Maria in 1991. Mary was widowed by Sonny's death.\nSalton Sea.\nBono was a champion of the Salton Sea in southeastern California, where a park was named in his honor. The 2005 documentary film \"Plagues &amp; Pleasures on the Salton Sea\" (narrated by John Waters) features Bono and documented the lives of the inhabitants of Bombay Beach, Niland and Salton City, as well as the ecological issues associated with the Sea.\nReligion.\nHe became interested in Scientology and took Scientology courses partly because of the influence of Mimi Rogers, but stated that he was a Roman Catholic on all official documents, campaign materials and websites. His wife Mary also took Scientology courses. However, after his death, Mary Bono stated that \"Sonny did try to break away [from the Church of Scientology] at one point, and they made it very difficult for him.\" The Church of Scientology said there was no estrangement from Bono.\nDeath.\nBono died on January 5, 1998, of injuries incurred when he hit a tree while skiing at Heavenly Mountain Resort in South Lake Tahoe, California.\nSonny's funeral was held a few days later. He was buried at Desert Memorial Park in Cathedral City, California. The epitaph on Bono's headstone reads \"AND THE BEAT GOES ON\".\nMary Bono was elected to fill the remainder of her husband's congressional term. She was elected in her own right seven subsequent times before being defeated in the 2012 election.\nHonors and tributes.\nSonny Bono has been honored and memorialized with:"}
{"id": "27656", "revid": "19275268", "url": "https://en.wikipedia.org/wiki?curid=27656", "title": "Single market", "text": "Type of trade bloc with most trade barriers removed\nA single market, sometimes called common market or internal market, is a type of trade bloc in which most trade barriers have been removed (for goods) with some common policies on product regulation, and freedom of movement of the factors of production (capital and labour) and of enterprise and services. The goal is that the movement of capital, labour, goods, and services between the members is as easy as within them. The physical (borders), technical (standards) and fiscal (taxes) barriers among the member states are removed to the maximum extent possible. These barriers obstruct the freedom of movement of the four factors of production (goods, capital, services, workers).\nA common market is usually referred to as the first stage towards the creation of a single market. It usually is built upon a free trade area with no tariffs for goods and relatively free movement of capital, workers and services, but not so advanced in reduction of other trade barriers.\nA unified market is the last stage and ultimate goal of a single market. It requires the total free movement of goods and services, capital and people without regard to national boundaries.\nIntegration phases.\nA common market allows for the free movement of capital and services but large amounts of trade barriers remain. It eliminates all quotas and tariffs\u00a0\u2013 duties on imported goods\u00a0\u2013 from trade in goods within it. However non-tariff barriers to trade remain, such as differences between the Member States' rules on product safety, packaging requirements and national administrative procedures. These prevent manufacturers from marketing the same goods in all member states. The objective of a common market is most often economic convergence and the creation of an integrated single market. It is sometimes considered as the first stage of a single market. The European Economic Community was the first large-scale example of a common market.\nA single market allows for people, goods, services and capital to move around a union as freely as they do within a single country \u2013 instead of being obstructed by national borders and barriers as they were in the past. Citizens can study, live, shop, work and retire in any member state. Consumers enjoy a vast array of products from all member states and businesses have unrestricted access to more consumers. A single market is commonly described as \"frontier-free\". However, several barriers remain such as differences in national tax systems, differences in parts of the services sector and different requirements for e-commerce. In addition separate national markets still exist for financial services, energy and transport. Laws concerning the recognition of professional qualifications also may not be fully harmonized.\nThe Eurasian Economic Union, the Gulf Cooperation Council, CARICOM and the European Union are current examples of single markets, although the GCC's single market has been described as \"malfunctioning\" in 2014. The European Union is the only economic union whose objective is \"completing the single market\".\nA completed, unified market usually refers to the complete removal of barriers and integration of the remaining national markets. Complete economic integration can be seen within many countries, whether in a single unitary state with a single set of economic rules, or among the members of a strong national federation. For example, the sovereign states of the United States do to some degree have different local economic regulations (e.g. licensing requirements for professionals, rules and pricing for utilities and insurance, consumer safety laws, environmental laws, minimum wage) and taxes, but are subordinate to the federal government on any matter of interstate commerce the national government chooses to assert itself. Movement of people and goods among the states is unrestricted and without tariffs.\nBenefits and costs.\nA single market has many benefits: with full freedom of movement for all the factors of production between the member countries, the factors of production become more efficiently allocated, further increasing productivity.\nFor both business within the market and consumers, a single market is a competitive environment, making the existence of monopolies more difficult. This means that inefficient companies will suffer a loss of market share and may have to close down. However, efficient firms can benefit from economies of scale, increased competitiveness and lower costs, as well as expecting profitability to increase as a result. This is true especially for companies selling goods and services easily distributed all around the countries of single market.\nConsumers are benefited by the single market in the sense that the competitive environment brings them cheaper products, more efficient providers of products and also increased choice of products and their quality. What is more, businesses in competition will innovate to create new products; another benefit for consumers.\nSingle market play significant role in increasing prosperity of nations involved in this area. For example, single market helps European Union to achieve annual growth of GDP with 2.2% p.a. between 1992 and 2006, rise in employment and job creation.\nTransition to a single market can have a negative impact on some sectors of a national economy due to increased international competition. Enterprises that previously enjoyed national market protection and national subsidy (and could therefore continue in business despite falling short of international performance benchmarks) may struggle to survive against their more efficient peers, even for its traditional markets. Ultimately, if the enterprise fails to improve its organization and methods, it will fail. The consequence may be unemployment or migration.\nNational participation into single market opens political debates, about skills loss through worker migration from less developed countries, and wage suppression in countries to which they migrate.\nList of common markets.\nEvery economic union and economic and monetary union includes a common market.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27658", "revid": "677153", "url": "https://en.wikipedia.org/wiki?curid=27658", "title": "Special administrative regions of China", "text": "Province-level autonomous subdivisions of the People's Republic of China\nThe special administrative regions (SAR) of the People's Republic of China are one of four types of province-level divisions of the People's Republic of China directly under the control of its Central People's Government (State Council). As a region, they possess the highest degree of autonomy from China's central government. However, despite the relative autonomy that the Central People's Government offers the special administrative regions, the National People's Congress and its Standing Committee remain capable of enforcing laws for the special administrative regions.\nThe legal basis for the establishment of SARs, unlike the other administrative divisions of China, is provided for by Article 31, rather than Article 30, of the Constitution of China of 1982. Article 31 reads: \"The state may establish special administrative regions when necessary. The systems to be instituted in special administrative regions shall be prescribed by law enacted by the National People's Congress in the light of the specific conditions\".\nAt present, there are two SARs established by the Constitution: Hong Kong and Macau. These former British and Portuguese territories were transferred to China in 1997 and 1999 respectively, following the Sino-British and Sino-Portuguese Joint Declarations signed in 1984 and 1987, respectively. Pursuant to their Joint Declarations, which are binding inter-state treaties registered with the United Nations, and their Basic laws, the Chinese SARs \"shall enjoy a high degree of autonomy\". Generally, the two SARs are not considered to constitute a part of mainland China, by both SAR and mainland Chinese authorities.\nThe provision to establish special administrative regions appeared in the constitution in 1982, in anticipation of the talks with the United Kingdom over the question of the sovereignty of Hong Kong. It was envisioned as the model for the eventual unification with Taiwan and other islands, where the Republic of China has resided since 1949.\nUnder the one country, two systems principle, the Chinese Central Government is responsible for the diplomatic, military and other state-level affairs of the two SARs. Both two SARs continue to possess their own multi-party legislatures, legal systems, police forces, separate customs territory, immigration policies, left-hand traffic, official languages, academic and educational systems, representation on certain international bodies and representation in international competitions, and other aspects that fall within the autonomous level.\nSpecial administrative regions should not be confused with special economic zones, which are areas in which special economic laws apply to promote trade and investments. The Wolong Special Administrative Region in Sichuan province is a nature reserve and not a political division.\nList of special administrative regions of China.\nThere are currently two special administrative regions established according to Article 31 of the Chinese Constitution. For the Wolong Special Administrative Region in Sichuan Province, please see the section Wolong below.\nCharacteristics.\nThe two special administrative regions of Hong Kong and Macau (created in 1997 and 1999 respectively) each have a codified constitution called Basic Law. The law provides the regions with a high degree of autonomy, a separate political system, and a capitalist economy under the principle of \"one country, two systems\" proposed by Deng Xiaoping.\nHigh degree of autonomy.\nCurrently, the two SARs of Hong Kong and Macau are responsible for all affairs except those regarding diplomatic relations and national defence. Consequently, the National People's Congress authorises the SAR to exercise a high degree of autonomy and enjoy executive, legislative and independent judicial powers, and each with their own Courts of Final Appeal.\nExternal affairs.\nSpecial administrative regions are empowered to contract a wide range of agreements with other countries and territories such as mutual abolition of visa requirement, mutual legal aid, air services, extradition, handling of double taxation and others, with no Chinese government involvement. However, in some diplomatic talks involving an SAR, the SAR concerned may choose to send officials to be part of the Chinese delegation. For example, when former Director of Health of Hong Kong Margaret Chan became the World Health Organization (WHO) Director-General, she served as a delegate from the People's Republic of China to the WHO.\nAt the same time they are members of various international organisations such as WTO, APEC, etc.\n Hong Kong\n Macao\nThe Government of Hong Kong and Government of Macao have established Hong Kong Economic and Trade Offices (HKETOs) and Delega\u00e7\u00e3o Econ\u00f3mica e Comercial de Macaus (DECMs) respectively in some countries, as well as in the Greater China Region. HKETOs serve as a quasi-interests section in favour of Hong Kong. DECMs serve as a quasi-interests section in favour of Macao. For regions with no HKETOs and DECMs, Chinese diplomatic missions take charge of protecting Hong Kong-related and Macau-related interests.\nSome countries which have a diplomatic relationship with the central Chinese government maintain Consulate-General offices in Hong Kong and Macau.\nOlympic Games.\nIn sporting events such as the Olympic Games or Asian Games, the SARs may have their own independent teams. They participate under the respective names of \"Hong Kong, China\" and \"Macau, China\", and compete as different entities as they had done since they were under foreign rules, but both SARs are usually allowed to omit the term \", China\" for informal use.\nDefence and military.\nThe People's Liberation Army is garrisoned in both SARs. PRC authorities have said the PLA will not be allowed to interfere with the local affairs of Hong Kong and Macau, and must abide by its laws. In 1988, scholar Chen Fang of the Academy of Military Science even tried to propose the \"One military, two systems\" concept to separate the defence function and public functions in the army. The PLA does not participate in the governance of the SAR but the SAR may request them for civil-military participation, in times of emergency such as natural disasters. Defence is the responsibility of the PRC government.\nA 1996 draft PRC law banned People's Liberation Army\u2013run businesses in Hong Kong, but loopholes allow them to operate while the profits are ploughed back into the military. There are many PLA-run corporations in Hong Kong. The PLA also have sizeable land holdings in Hong Kong worth billions of dollars.\nImmigration and nationality.\nEach of the SARs issues passports on its own to its permanent residents who are concurrently Chinese (PRC) citizens. PRC citizens must also satisfy one of the following conditions:\nApart from affording the holder consular protection by the Ministry of Foreign Affairs, these passports also specify that the holder has right of abode in the issuing SAR.\nThe National People's Congress has also put each SAR in charge of administering the PRC's Nationality Law in its respective realms, namely naturalisation, renunciation and restoration of PRC nationality and issuance of proof of nationality.\nDue to their colonial past, many inhabitants of the SARs hold some form of non-Chinese nationality (e.g. British National (Overseas) status, British citizenship, British Overseas citizenship or Portuguese citizenship). However, SAR residents who are Chinese descent have always been considered as Chinese citizens by the PRC authorities, an exception to this case is Macau, wherein residents of Chinese descent may choose Chinese or Portuguese nationality. Special interpretation of the Nationality Law, while not recognising dual nationality, has allowed Chinese citizens to keep their foreign \"right of abode\" and use travel documents issued by the foreign country. However, such travel documents cannot be used to travel to mainland China and persons concerned must use Home Return Permit. Therefore, master nationality rule applies so the holder may not enjoy consular protection while in China. Chinese citizens who also have foreign citizenship may declare a change of nationality at the Immigration Department of the respective SARs, and upon approval, would no longer be considered Chinese citizens.\nSAR permanent residents who are not Chinese citizens (including stateless persons) are not eligible for SAR passports. Persons who hold a non-Chinese citizenship must obtain passports from foreign diplomatic missions which represents their countries of citizenship. For those who are stateless, each SAR may issue its own form of certificates of identity, e.g. Document of Identity, in lieu of national passports to the persons concerned. Chinese citizens who are non-permanent residents of two SARs are also ineligible for SAR passports but may obtain CIs just like stateless persons.\nOffer to Taiwan and other ROC-controlled areas.\nThe status of a special administrative region for Taiwan and other areas controlled by the Republic of China (ROC) was first proposed in 1981. The 1981 proposal was put forth by NPC chairman Ye Jianying called \"Ye's nine points\" (). A series of different offers have since appeared. On 25 June 1983 Deng Xiaoping appeared at Seton Hall University in the US to propose \"Deng's six points\" (), which called for a \"Taiwan Special Administrative Region\" (). It was envisioned that after Taiwan's unification with the PRC as an SAR, the PRC would become the sole representative of China. Under this proposal, Taiwan would be guaranteed its own military, its own administrative and legislative powers, an independent judiciary and the right of adjudication, although it would not be considered a separate government of China.\nIn 2005 the Anti-Secession Law of the PRC was enacted. It promises the lands currently ruled by the authorities of Taiwan a high degree of autonomy, among other things. The PRC can also employ non-peaceful means and other necessary measures to defend its claims to sovereignty over the ROC's territories in the event of an outright declaration of independence by Taiwan (ROC).\nIn January 2019, the 40-year anniversary of a statement made by the PRC to Taiwan in 1979, Chinese Communist Party general secretary Xi Jinping outlined in a speech how the \"one country, two systems\" principle would be applied to Taiwan. Several major points from the speech include:\nWolong.\nThe Wolong Special Administrative Region () is located in the southwest of Wenchuan County, Ngawa Tibetan and Qiang Autonomous Prefecture of Sichuan. It was formerly known as Wolong Special Administrative Region of Wenchuan County, Sichuan Province and was founded in March 1983 with approval of the State Council. It was given its current name and placed under Sichuan provincial government with administrative supervision by the provincial department of forestry. Its area supersedes Sichuan Wolong National Nature Reserve and its administrative office is the same as the Administrative Bureau of the State Forestry Administration for the reserve. It currently has a population of 5,343.\nDespite its name, This is primarily because the Wolong Special Administrative Region was established with the approval of the State Council, rather than \"by law enacted by the National People's Congress\" as stipulated in Article 31 of the Constitution.\nDefunct SARs.\nIn the Republic of China (ROC) era between 1912 and 1949, the \"special administrative regions\" () were historically used to designate special areas by the Beiyang government, most of which were eventually converted into provinces by the Nationalist government in 1928. All were suspended or abolished after the end of the Chinese Civil War, with the establishment of the People's Republic of China (PRC) and the ROC government's retreat to Taiwan, but they continued to exist as provinces under ROC law. The regions were:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27660", "revid": "45219732", "url": "https://en.wikipedia.org/wiki?curid=27660", "title": "Seattle Mariners", "text": "Major League Baseball franchise in Seattle, Washington\nThe Seattle Mariners are an American professional baseball team based in Seattle. The Mariners compete in Major League Baseball (MLB) as a member club of the American League (AL) West Division. The team joined the American League as an expansion team in 1977, originally playing their home games in the Kingdome. Since July 1999, the Mariners' home ballpark has been T-Mobile Park, located in the SoDo neighborhood of Seattle.\nThe \"Mariners\" name originates from the prominence of marine culture in the city of Seattle. They are nicknamed the M's, a title featured in their primary logo from 1987 to 1992. They adopted their current team colors \u2013 navy blue, northwest green (teal), and silver \u2013 before the 1993 season, after having been royal blue and gold since the team's inception; the original colors continue to be used in alternate uniforms. Their mascot is the Mariner Moose.\nThe Mariners first fielded a winning team in 1991, and reached the playoffs in 1995, 1997, 2000, and 2001; the most successful period in franchise history. Led by Hall of Fame players Edgar Mart\u00ednez, Ken Griffey Jr., and Randy Johnson, the Mariners clinched their first playoff berth in 1995 when they won their first division championship and defeated the New York Yankees in the ALDS. Mart\u00ednez's walk-off double in Game 5 drove Griffey in from first base to win the game in the 11th inning, clinched a series win for the Mariners, served as a powerful impetus to preserve baseball in Seattle, and has become an iconic moment in team history. They would win their second division title in 1997.\nAfter Griffey, Johnson, and Alex Rodriguez left the team, the Mariners, bolstered by the signing of Ichiro Suzuki, won 116 games in 2001, which set the American League record for most wins in a single season and tied the 1906 Chicago Cubs for the Major League record for most wins in a single season. The team would not make the postseason again until 2022, breaking the longest active drought in the four major North American sports. They won their fourth AL West division title in 2025, their first title since 2001.\nThe franchise has finished with a losing record in 30 of 49 seasons, as of 2025[ [update]]. The Mariners are the only active MLB franchise to never appear in the World Series, holding the sport's longest active World Series appearance drought.\nAs of the end of the 2025 season[ [update]], the Mariners' all-time win\u2013loss record is 3,689\u20134,022\u20132 ().\nHistory.\nThe Mariners were created as a result of a lawsuit. In , after Bud Selig bought the Seattle Pilots and moved them to Milwaukee to become the Milwaukee Brewers, the city of Seattle, King County, and the state of Washington (represented by then-state Attorney General and future U.S. Senator Slade Gorton) sued the American League for breach of contract. Confident that Major League Baseball would return to Seattle within a few years, King County built the multi-purpose Kingdome, which would become home to the National Football League's expansion Seattle Seahawks in 1976. The name \"Mariners\" was chosen by club officials in August 1976 from more than 600 names submitted by 15,000 entrants in a \"name the team\" contest. The name was submitted by Roger Szmodis of Bellevue, Washington. When the Mariners attempted to reach Szmodis about the prize he had won, they were unsuccessful\u2014initially and ultimately.\nThe first home run in team history was hit on April 10, 1977, by designated hitter Juan Bernhardt.\nThat year, pitcher Diego Segu\u00ed, in his last major league season, became the only player to play for both the Pilots and the Mariners. The Mariners finished with a 64\u201398 record, matching the 1969 Pilots' record. The team avoided finishing in last place in the AL West by half a game. The Mariners would not post a winning record or finish above 4th place in any of their first 14 seasons. In 1979, Seattle hosted the 50th Major League Baseball All-Star Game. After the 1981 season, the Mariners were sold to California businessman George Argyros, who in turn sold the team in 1989 to a group led by Indianapolis-based Jeff Smulyan, who owned radio and television stations, for $76\u00a0million. Smulyan proposed moving the team to Tampa, Florida or another market in 1992 before he put the team up for sale. Nintendo of America bought the team in 1992; Nintendo CEO Hiroshi Yamauchi, who held a 49 percent share of the franchise, had never been to a baseball game but sought to thank the city for its role in the company's success.\nBefore the 1993 season, the Mariners hired manager Lou Piniella, who had led the Cincinnati Reds to victory in the 1990 World Series. Mariners fans embraced Piniella, and he would helm the team from 1993 through 2002, winning two American League Manager of the Year Awards during his tenure. (Piniella was selected by the Pilots in the 1968 expansion draft but did not play for Seattle, being traded in April 1969 to the Kansas City Royals, where he earned AL Rookie of the Year honors that year.)\nThe Mariners first won the AL West and made the playoffs in 1995, despite star outfielder Ken Griffey Jr. missing much of the season after breaking his wrist crashing into the center field wall. The team defeated the California Angels in a one-game playoff to win the division. The Mariners returned to the playoffs in 1997 and 2000.\nThe Mariners finished with a record of 116\u201346, leading all of Major League Baseball in winning percentage for the duration of the season and easily winning the American League West division title. In doing so, the team broke the 1998 New York Yankees' American League single-season record of 114 wins and matched the all-time MLB single-season record for wins set by the 1906 Chicago Cubs. At the end of the season, Ichiro Suzuki won the AL MVP, AL Rookie of the Year, and one of three outfield Gold Glove Awards, becoming the first player since the Fred Lynn in 1975 to win all three in the same season. The Mariners advanced through the postseason but lost to the Yankees in the 2001 ALCS. It would be the team's last playoff appearance until 2022.\nThe Mariners had a 93-win season in but failed to make the postseason. Manager Lou Piniella was traded to the Tampa Bay Rays during the offseason. The team repeated with 93 wins in 2003 and also did not qualify for the playoffs. On October 22, the Mariners announced the hiring of Jack Zduriencik, formerly scouting director of the Milwaukee Brewers, as their general manager. Weeks later, on November 18, the team named Oakland Athletics bench coach Don Wakamatsu as its new manager. The off-season also saw a litany of roster moves, headlined by a 12-player, 3-team trade that sent All-Star closer J. J. Putz to the New York Mets and brought 5 players, including prospect Mike Carp and outfielder Endy Ch\u00e1vez from New York and outfielder Franklin Guti\u00e9rrez from the Cleveland Indians, to Seattle. Many of the moves, like the free-agent signing of Mike Sweeney, were made in part with the hope of squelching the clubhouse infighting that plagued the Mariners in 2008. It also saw the return of Seattle favorite Griffey Jr. The 2009\u201310 offseason was highlighted by the trade for 2008 AL Cy Young Award winner Cliff Lee from the Philadelphia Phillies, the signing of third baseman Chone Figgins, and the contract extension of star pitcher F\u00e9lix Hern\u00e1ndez.\nGriffey Jr. announced his retirement on June 2, 2010, after 22 MLB seasons. After its busy offseason and high expectations, the team flopped in 2010, finishing with the worst record in the American League. The Mariners fired Wakamatsu along with several coaches on August 9, 2010. Daren Brown, the manager of the Triple-A affiliate Tacoma Rainiers, took over as interim manager. Dave Niehaus, the Mariners' play-by-play announcer since the team's inception, died of a heart attack on November 10, 2010, at the age of 75. In memory of Niehaus, Seattle rapper Macklemore wrote a tribute song called \"My Oh My\" in December 2010. He performed the song at the Mariners' Opening Day game on April 8, . The Mariners hired former Cleveland manager Eric Wedge as their new manager on October 19, 2010. \nOn April 21, 2012, Philip Humber of the Chicago White Sox threw the third perfect game in White Sox history against the Mariners in Seattle. It was the 21st perfect game in MLB history. Mariners starter Kevin Millwood and five relievers combined to throw the tenth combined no-hitter in MLB history, the first in Mariners history, on June 8. The six pitchers used in a no-hitter tied a major league record, first set by the Houston Astros in 2003. F\u00e9lix Hern\u00e1ndez pitched the first perfect game in team history, shutting down the Tampa Bay Rays 1\u20130 at Safeco Field on August 15. It was the 23rd perfect game in Major League Baseball history. The Mariners became the first team in Major League Baseball to be involved in two perfect games in one season.\nZduriencik was fired on August 28, 2015. Jerry Dipoto, a former general manager of the Los Angeles Angels of Anaheim, was hired as the Mariners' new general manager one month later. On October 9, manager Lloyd McClendon was fired. Scott Servais was named the new Mariners' manager on October 23.\nNintendo announced on April 27, 2016, that it would sell its controlling stake in the Mariners to First Avenue Entertainment limited partnership, led by John W. Stanton. Nintendo retained a 10 percent ownership share of the team after the sale was completed in August 2016. The franchise was valued at $1.4\u00a0billion at the time and included Root Sports Northwest, the team's regional television network.\nThe Stanton/Dipoto/Servais era was characterized by two competitive phases. In the first phase, the organization tried to contend for a championship with the existing core of Robinson Cano, F\u00e9lix Hern\u00e1ndez, Nelson Cruz, and Kyle Seager. The team came close but ultimately missed the playoffs. Following the 2018 season, the organization pivoted to a rebuild, trading several players for prospects. Following a fallow period of 2019\u201320, the team returned to contention in 2021, winning 90 games but falling short of the playoffs. In 2022, with a new core including Julio Rodr\u00edguez, J. P. Crawford, Cal Raleigh, Luis Castillo, George Kirby, and Logan Gilbert, the team reached the postseason for the first time since 2001. This broke what was at the time the longest playoff drought of any team in the \"Big Four\" North American sports. Rodr\u00edguez also won the American League Rookie of the Year Award. In 2023, the team won 88 games but finished one game out of a playoff spot. In August 2024, Servais was fired after blowing a 10 game lead in the AL West over the Houston Astros and having the team fall back to .500. Servais was replaced by the team's former catcher Dan Wilson as manager. The team again missed the playoffs by one game. In 2025, the Mariners won their first AL West title since 2001. The 2025 Season marks the closest the Mariners have ever been to the World Series.\nUniforms.\n1977\u20131980.\nThe Mariners' original colors were blue and gold, the color scheme previously used by the Seattle Pilots and its successor Milwaukee Brewers. For the first four seasons, they wore white pullover jerseys at home with the team name in front and numbers on the left chest. The \"M\" in \"Mariners\" was shaped to resemble a trident. On the road, they wore baby blue pullover jerseys with the city name in front and numbers on the left chest. The lettering colors were blue with gold trim, though in the 1977 season the trim on the road jersey was white and the \"Seattle\" wordmark appeared smaller. The trident logo was added to the left sleeve prior to the 1979 season.\nThe cap was all-blue and featured the gold trident logo with white trim.\n1981\u20131986.\nThe Mariners made some subtle changes to the uniform in 1981. The trident logo was replaced by blue and gold racing stripes on the shoulders, and the lettering received an extra blue outline. The number font also changed from rounded to block style. In 1985, the road jersey color was changed to grey.\nThe cap logo also featured a slight update of the trident logo, changing its color to blue, along with additional outlines and a white star background, a logo first used for the 1979 All-Star Game.\n1987\u20131992.\nIn 1987, the Mariners changed its uniform style to traditional buttoned tops and belted pants. Both uniforms incorporated blue piping and a block \"Mariners\" wordmark in blue with gold and blue outlines. The numbers remained blue, but eliminated the trim outlines.\nThe cap logo was changed to a gold \"S.\"\n1993\u2013present.\nThe Mariners donned their current uniforms in . The white home uniform originally featured \"Mariners\" in navy with trim in a shade of teal green, exclusively named \"Northwest Green\" for the team, and featured the \"compass rose\" logo atop the \"M.\" The grey road uniform originally featured \"Seattle\" in navy with Northwest Green and white trim; in 2001, the compass rose logo was added in the middle of the \"S.\" In 2015, a silver inline was added to the wordmark of both uniforms, which was also applied to the block letters and numbers. The primary logo is applied to the left sleeve.\nFrom 1997 to 2000, the Mariners also wore sleeveless versions of their primary uniforms, accompanied with a navy undershirt.\nThe Mariners have also worn Northwest Green alternate uniforms at different points in their history. The original version was unveiled in 1994 and had \"Mariners\" in silver with navy and white trim. The next season, the white trim was removed to improve visibility. The Mariners did not wear green uniforms from 1997 to 2010, after which it became a regular part of their uniform rotation. Formerly worn on Friday home games, the Northwest Green alternates are currently worn on select Saturday home games and on road games in which the home team wears either navy or black uniforms.\nThe navy alternate uniform originally replaced the Northwest Green alternate in 1997 and featured the team name in silver with Northwest Green and navy trim. In 1999, the alternates were updated to feature the city name with the \"S\" behind the compass rose logo and silver piping; this became their road alternate the following season after a corresponding home navy alternate was introduced. In 2003, the silver piping was removed and the letter and number fonts were changed to match the wordmark. In 2012, after the Northwest Green home alternates were brought back, the navy uniforms were tweaked anew, this time with the city name in front and stylized serifed letters instead of the normal block letters. In 2024, the names were changed to block lettering. It is now worn on most road games, though they have also donned them at home on occasion.\nA navy blue cap that features a ball and compass rose \"S\" logo is paired with the home white, road gray, and navy blue jerseys. A variation of this cap with a Northwest Green brim is worn with the home alternate jersey. In 1994, the Mariners also wore Northwest Green caps with navy brims, and in 1997 the team wore a navy cap with grey brims.\nIn January 2015, the team announced a new alternate uniform to be worn for Sunday home games. This cream-colored \"fauxback\" uniform features the current logo and lettering style in a royal blue and gold color scheme, a throwback to the original team colors. Unlike the rest of the uniform set, the back of the jersey does not display the player name. The cap features the current cap logo in the throwback colors.\nIn January 2019, the Mariners announced a new home and away uniform to be worn during spring training. The jersey has a design similar to their home white jerseys but features a powder blue throwback to the team colors during the 1980s. The cap has the usual navy blue color, but with a logo that features the signature compass rose and with a large M in the center.\nFor the 2023 season, MLB and Nike have instituted a \"four plus one\" model for team uniforms, consisting of a home uniform, away uniform, two alternate uniforms, and a City Connect uniform featuring \"color schemes and logos that pay homage to a team's city.\" The Mariners confirmed that they will replace the gray jerseys with the navy blue jerseys as their standard away uniforms for the 2023 season. The team will also stop using the powder blue jerseys during spring training. The choice to remove the gray and powder blue jerseys was based on feedback from players and fans, according to Kevin Martinez, the Mariners senior vice president of marketing and communications.\nThe Mariners' City Connect uniform is a visual nod to Seattle's baseball history. The jersey includes a sleeve patch featuring Mount Rainier and the letters \"PNW,\" an acronym for Pacific Northwest. The jersey also includes the inaugural colors of the team and the word \"Seattle\" across the chest lettering reminiscent of the Seattle Pilots. The jersey is paired with a cap that has new design of the trident logo and black pants. This uniform is primarily worn during Friday home games. The team wore the City Connect uniform more frequently in 2024, since they won most of their games in the alternates. In the 2025 season, the Mariners partnered with Nintendo of America to have the Nintendo \u201cracetrack\u201d logo on the sleeve of the home game jersey and the logo of Nintendo on the sleeve of the away game jersey.\nSpring training.\nThe Peoria Sports Complex in Peoria, Arizona has been the Mariners' home spring training facility since 1994. The complex is shared with the San Diego Padres. On March 25, 2013, in a 16\u20130 victory over the Cincinnati Reds, the Mariners broke the team record for total home runs during a spring training season with 52.\nSeason records.\n\"This is a partial list listing the past 24 completed regular seasons. For the full season records, see here.\"\nT-Mobile Park.\nT-Mobile Park (known as \"Safeco Field\" from 1999 to 2018) has been home to the Seattle Mariners since the first game vs. the San Diego Padres on July 15, 1999. There were 44,607 people in attendance that night.\nSeattle Mariners Hall of Fame.\nMariners then-chairman and CEO John Ellis announced on June 14, 1997, the creation of a Mariners Hall of Fame. The Mariners operate the hall of fame, which honors players, staff, and other individuals that greatly contributed to the history and success of the Mariners franchise. It is located at the Baseball Museum of the Pacific Northwest in T-Mobile Park. The most recent Mariners Hall of Fame member, F\u00e9lix Hern\u00e1ndez, was inducted August 12, 2023.\nRetired numbers.\nThe Mariners criteria for retiring a uniform number is more selective than the standards for the Mariners Hall of Fame. To be eligible to have one's number retired, in addition to the criteria outlined for the Mariners' Hall of Fame, the former Mariners should have either:\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0a) been elected to the National Baseball Hall of Fame and been in a Mariners uniform for at least five years, or \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0b) come close to such election and have spent substantially his entire career with the Mariners. Eligibility shall not commence until after the former player has been voted on once for the National Baseball Hall of Fame, which for effectively means six years after retirement.\nKen Griffey Jr.'s No. 24 was retired at the beginning of the 2016 season, with the retirement ceremony taking place on August 6, 2016. Griffey was elected to the Hall of Fame in January of that year.\nEdgar Mart\u00ednez's No. 11 was retired during the 2017 season, with the retirement ceremony taking place on August 12, 2017. Mart\u00ednez played his entire career with the Mariners and first appeared on the Hall of Fame ballot in 2010. His No. 11 was retired in 2017, predating his 2019 election to the Hall of Fame and seemingly establishing the 58.6% of the vote he received that year as sufficiently \"close\" to election to satisfy the criteria. The number 11 was not issued to anyone after Mart\u00ednez's retirement as a player in 2004 until his return to the Mariners as hitting coach in 2015.\nIchiro Suzuki's No. 51 was retired by the Mariners in a pregame ceremony on August 9, 2025, days after his induction into the Hall of Fame. He has continued to wear the number after retiring, including during spring training in 2020 and 2022 as well as when throwing out a ceremonial first pitch in 2022.\nJackie Robinson's No. 42 was retired throughout MLB on April 15, 1997, the 50th anniversary of him breaking MLB's racially exclusionist color line.\nRandy Johnson is also scheduled to have his No. 51 retired by the Mariners in 2026. Johnson played 10 seasons with the Mariners, from 1989 until mid-1998, and was elected to the Hall of Fame in 2015. The #51 was withheld until 2001, when it was issued to Ichiro Suzuki upon his request. No other player, besides Suzuki, has worn that number for the Mariners since Johnson. On June 2, 2025, the Mariners announced they will also retire 51 in honor of Johnson the in following season, with the date later set for May 2, 2026.\nCulture.\n\"Louie Louie\".\nFrom the 1990 season through the 2021 season, as part of the seventh-inning stretch, after the crowd was led in singing \"Take Me Out to the Ball Game\" or \"God Bless America\" the public address system played the Kingsmen's version of \"Louie Louie\". The song was a regional hit in the Northwest, covered by many local bands for nearly a decade until the Portland-based Kingsmen recorded their version in 1963. In 1985, the song's regional importance was publicized by a campaign to make it the official state song of Washington. The tradition to play the song during the seventh inning stretch began as an attempt for the then new ownership group to put its stamp on the team, and was solidified on June 2, 1990, when the Kingsmen performed the song in the middle of the seventh inning live from atop a dugout. That game, Randy Johnson threw the first no-hitter in Mariners history.\nFor the 2022 season, the Mariners replaced \"Louie Louie\" with the Macklemore &amp; Ryan Lewis song \"Can't Hold Us\". Replacing \"Louie Louie\" was a source of contention for some fans. The Mariners stopped playing \"Can't Hold Us\" after Macklemore's comments at a Palestinian benefit concert in September 2024. \n'Louie Louie' returned to games in the 2025 season, remixed to a slightly more uptempo beat and interspersed with encouragement to the crowd (\"Let's Go Mariners!\"). \nBuhner Buzz Cut Night.\nIn 1994, the Mariners started a promotion called \"Buhner Buzz Cut Night.\" Inspired by Jay Buhner's shaved head, any fan who was willing to have their head shaved before the game\u2014or was already bald\u2014would receive a free ticket to the game and a T-shirt with a slogan, such as \"Bald Is Buhnerful\" or \"Take Me Out to the Bald Game\". Hair 10 inches or longer was collected for charity. The promotion continued until Buhner's retirement in 2001, with a year's hiatus in 2000, and is still remembered by fans today. The club revived the promotion for its 30th anniversary in 2024, with Buhner giving catcher Cal Raleigh a buzz cut.\nRally Fries.\nRally Fries are a baseball tradition started by Mariners broadcaster Mike Blowers in 2007. During a game against the Cincinnati Reds, a fan tried to catch a foul ball along the right-field line but in turn spilled his tray of french fries along the track. While chatting on the air and seeing the mishap, Blowers's partner, Dave Sims, suggested that he should send a new tray of fries to the fan. Blowers agreed, and sent his intern to deliver a plate of fries to the man.\nAt the Mariners' next game, fans made signs and boards asking Blowers for fries as well. Coincidentally, every time the fries were delivered, the Mariners seem to score or rally from a deficit, and thus the \"Rally Fries\" were created. This became so popular with the fans that signs were even seen when the Mariners were the visiting team, although on August 1, 2009, Blowers established that he only gives out fries at home games.\nGenerally, Blowers would select a person or a group of people that appealed to him, whether it was through fans wearing elaborate costumes or waving funny signs and boards. The fries were usually delivered from Ivar's, a Seattle-based seafood restaurant with a location at T-Mobile Park. The amount of fries given out varied with the size of the winning group of fans. The winners were generally selected around the 5th or 6th inning, although potential candidates were shown in almost every inning beforehand.\nSims confirmed in 2012 that the Rally Fries promotion had ended.\nJROD Squad.\nThe JROD Squad honors Mariners center fielder Julio Rodr\u00edguez. The location varies, although it is usually in seats in center field. Fans buying tickets to the JROD Squad section (referred to collectively as \"JROD's Squad\") receive a T-shirt showing a replica of a gold chain Rodr\u00edguez wears around his neck. Rodr\u00edguez often interacts with the JROD Squad, waving to them and throwing them balls at the end of innings.\nPrevious fan sections.\nArea 51.\nWhen Ichiro Suzuki played right field for the Mariners, seats in right field were often informally called Area 51, a nod to Suzuki's uniform number (51) and the top-secret government site in Nevada of the same name.\nKing's Court.\nAs the 2011 season progressed, the Mariners' marketing staff came up with an idea to encourage the growing fanbase of star pitcher \"King\" F\u00e9lix Hern\u00e1ndez. Every Hern\u00e1ndez start at T-Mobile Park was accompanied by the King's Court, a designated cheering section for fans to sing, chant, and cheer while donning yellow T-shirts and \"K\" cards supplied by the team. It was located in the lower seating area along the third baseline. The team encouraged other activities, such as dressing like Larry Bernandez, Hern\u00e1ndez's alter ego from a Mariners TV commercial, and awarding fans wearing costumes with a turkey leg.\nThe Supreme Court was a special event where the King's Court section was extended to the entirety of T-Mobile Park. The first Supreme Court was Hern\u00e1ndez's first home game following his perfect game in 2012. Following opening day 2012, it occurred each year at Hern\u00e1ndez's first home game of each season.\nThe King's Court ended following Hern\u00e1ndez's departure from the Mariners at the end of the 2019 season.\nMaple Grove.\nDuring the 2017 season, fans created the Maple Grove, a celebration of Canadian pitcher James Paxton and inspired by the King's Court. At home games where Paxton started, a group of fans sat under a Maple Grove banner, typically in the left-field bleachers. A potted maple tree was also present in their section, provided by the Mariners. The Grove dubbed the tree \"Stick Rizzs,\" referencing long-time broadcaster Rick Rizzs.\nWhen Paxton got to two strikes on a batter, the Grove held up \u201cEh\u201d Cards, a tip of the cap to Paxton's home country of Canada and a nod to the \"K\" (for strikeout) cards held up in King's Court. Variant cards have also been produced for special occasions, such as when a planned Paxton start turned into a Hern\u00e1ndez start (a King's Grove, with \"K'eh\" cards to cheer for Hern\u00e1ndez). Other special cards celebrated Paxton reaching 300 strikeouts, and a tribute to broadcaster Angie Mentink (\"A\" cards, to show support after she had publicly disclosed her breast cancer diagnosis). An \"Eh\" card is now part of the Baseball Hall of Fame's collection.\nThe Maple Grove differed from the King's Court in that it was created and organized by fans, while the Court was promoted by the Mariners' marketing team. When asked, Paxton stated that fans creating the Maple Grove was really special to him and that he never imagined that something of the sort would ever be done for him. The Grove continued until Paxton was traded to the Yankees following the 2018 season.\nRivalries.\nDivisional.\nThe Mariners also held a longstanding divisional rivalry with the Oakland Athletics as they would often battle for playoff contention or lead of the division through the early 2000s. Following the realignment of the division in 2013, the Mariners have also built a recent rivalry with the Houston Astros as both teams have handily fought for control of the division.\nLos Angeles Angels.\nThe Los Angeles Angels have maintained an off-and-on rivalry with the Mariners as both teams have often fought for control of the division or a playoff berth. Both teams often clashed for playoff positions during the early 2000s as the Mariners boasted a 116 win team in 2001 while the Angels managed to win the World Series in 2002. Despite both teams encountering a decline through the decade, regular matchups often developed into clashes for relevance in the division. Recently, both teams were each trying to end postseason droughts, bolstered by players such as Julio Rodr\u00edguez and Ty France for Seattle and Shohei Ohtani and Mike Trout for the Angels The two teams have met 741 times, with the Angels leading the series 400\u2013341. The teams have yet to meet in the postseason. In 2022, tensions were heightened after Angels pitcher Andrew Wantz intentionally hit Mariners batter Jesse Winker with a pitch, leading to a major brawl that resulted in several players receiving suspensions.\nHouston Astros.\nThe Mariners and Houston Astros have fought for control of the division in recent years, since the Mariners started periodically contending for the postseason in 2014. The Astros lead the all-time regular season series 132\u201397 and have a perfect 3\u20130 record against Seattle in the postseason. \nThe 2022 season saw the Mariners return to the playoffs, in a year that included multiple bench-clearing incidents between the Astros and Mariners during the regular season. The Mariners and Astros faced off in the 2022 ALDS, with Houston sweeping the Mariners in 3 games. The third and final game of that series was among the longest in postseason history; its 18 innings matched the longest game in playoff history and its 6 hours, 22 minutes was the third-longest in time ,\nThe rivalry was a brief focus at the end of the 2025 season, after a 3 game sweep of the Astros in Houston by the Mariners in late September. The sweep put the Mariners 3 games up in the division with 6 to play while all but eliminating the Astros from postseason contention. 3 days later, the Mariners won the AL West Division championship for the first time in 24 years.\nToronto Blue Jays.\nAlthough the Toronto Blue Jays are not a divisional rival, many Blue Jays fans from Western Canada travel to Seattle when the Blue Jays are the visiting team. Seattle is about a three-hour drive from Vancouver. The Seattle Times estimated that Blue Jays fans represented around 70 percent of the crowd in Safeco Field for a June 2017 weekend series. The Mariners broke their playoff drought in Toronto in the 2022 American League Wild Card Series with a dramatic comeback in Game Two, after being down by a deficit of one run to eight, winning by a final score of ten to nine. The two teams faced off again in the post-season in the 2025 American League Championship Series. Seattle took the first two games on the road in Toronto, but ultimately lost the series three games to four, with the Blue Jays winning game seven at home.\nInterleague.\nThe Vedder Cup.\nAn unusual rivalry exists between the Mariners and the National League's San Diego Padres. The matchup was designated one of the 15 \"naturalized rivalries\" when interleague play began in 1997, and the teams have played every year since, except 2017. \nLittle on the surface links the two teams to any actual hostility, as both play in separate leagues and in cities that sit about 1,250 miles apart. Still, they share a spring training facility\u2014the Peoria Sports Complex in Peoria, Arizona\u2014and in many years have competed for draft picks and prospects after failing to make the playoffs. (The Padres did not win a playoff series between 1999 and 2019, while the Mariners did not reach the playoffs from 2002 to 2021.) Far from a bitter rivalry, it is viewed by the teams and most fans as more of a humorous contest.\nThe rivalry had long been unofficially called the Vedder Cup after Pearl Jam frontman Eddie Vedder, who claims both Seattle and San Diego as hometowns but is a vocal fan of the Chicago Cubs. In 2025, the rivalry was officially recognized as \"The Vedder Cup\", with the winner of the series being awarded a trophy designed by Vedder. The Mariners won the Vedder Cup in 2025.\nPlayers.\nBaseball Hall of Famers.\nThe following elected members of the Baseball Hall of Fame spent part of their careers with the Mariners.\nSeattle Mariners Hall of Famers\nAffiliation according to the National Baseball Hall of Fame and Museum\nSeattle Mariners\n&lt;templatestyles src=\"Plainlist/styles.css\"/&gt;\nFord C. Frick Award recipients.\nSeattle Mariners Ford C. Frick Award recipients\nAffiliation according to the National Baseball Hall of Fame and Museum\n&lt;templatestyles src=\"Plainlist/styles.css\"/&gt;\nMinor league affiliations.\nThe Seattle Mariners farm system consists of six minor league affiliates.\nRadio and television.\nThe Mariners' flagship radio station is KIRO-AM, which previously broadcast Mariners contests from 1985 to 2002. Former flagship stations include KOMO, from 2003 to 2008, and KVI from 1977 to 1984. Television rights are held by MLB Local Media beginning in 2026. Games had been broadcast on cable network Root Sports Northwest, which the Mariners own. The team announced they would shutter the station after the 2025 season. In the past, Mariners games appeared in Seattle on over-the-air stations KING, KIRO, KTZZ, and KSTW. Selected Mariners games are also available on Canadian television due to an agreement between Root Sports Northwest and Rogers Sportsnet Pacific.\nThe Mariners made significant changes to their broadcast team in 2025, following the departure of long-time announcer Dave Sims in November 2024. Rick Rizzs is the primary radio play-by-play announcer with color commentator Gary Hill Jr. Aaron Goldsmith is the television play-by-play announcer, with a rotating group of commentators, including Angie Mentink and former Mariner players Jay Buhner, Ryan Rowland-Smith, and Dave Valle. Brad Adam and Jen Mueller are television reporters and hosts. Shannon Drayer is the radio pre-game and post-game host and clubhouse reporter. Steve Guasch is the Spanish-language radio broadcaster.\nDave Niehaus broadcast for the Mariners since their 1977 inaugural season until he died on November 10, 2010. For the 2011 and 2012 seasons, Niehaus's broadcast duties were filled by a collection of former Mariners broadcasters such as Ron Fairly, Ken Levine, and Ken Wilson and former Mariners players including Buhner, Valle, Dave Henderson, and Dan Wilson.\nTom Hutyler has been the Mariners' public address announcer since 1987, first at the Kingdome and now at T-Mobile Park. When KOMO AM was the Mariners' flagship radio station, Hutyler occasionally hosted the post-game radio show.\nFranchise records and award winners.\nCareer records.\nSources:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27661", "revid": "39151475", "url": "https://en.wikipedia.org/wiki?curid=27661", "title": "Source code", "text": "Human-readable instructions a computer can execute \nIn computing, source code, or simply code or source, is human readable plain text that can eventually result in controlling the behavior of a computer. In order to control a computer, it must be processed by a computer program \u2013 either executed directly via an interpreter or translated into a more computer-consumable form such as via a compiler. Sometimes, code is compiled directly to machine code so that it can be run in the native language of the computer without further processing. But, many modern environments involve compiling to an intermediate representation such as bytecode that can either run via an interpreter or be compiled on-demand to machine code via just-in-time compilation.\nBackground.\nThe first programmable computers, which appeared at the end of the 1940s, were programmed in machine language (simple instructions that could be directly executed by the processor). Machine language was difficult to debug and was not portable between different computer systems. Initially, hardware resources were scarce and expensive, while human resources were cheaper. As programs grew more complex, programmer productivity became a bottleneck. This led to the introduction of high-level programming languages such as Fortran in the mid-1950s. These languages abstracted away the details of the hardware,and were designed to express algorithms that could be understood more easily by humans. As instructions distinct from the underlying computer hardware, software is therefore relatively recent, dating to these early high-level programming languages such as Fortran, Lisp, and Cobol. The invention of high-level programming languages was simultaneous with the compilers needed to translate the source code automatically into machine code that can be directly executed on the computer hardware. \nSource code is the form of code that is modified directly by humans, typically in a high-level programming language. Object code can be directly executed by the machine and is generated automatically from the source code, often via an intermediate step, assembly language. While object code will only work on a specific platform, source code can be ported to a different machine and recompiled there. For the same source code, object code can vary significantly\u2014not only based on the machine for which it is compiled, but also based on performance optimization from the compiler.\nOrganization.\nMost programs do not contain all the resources needed to run them and rely on external libraries. Part of the compiler's function is to link these files in such a way that the program can be executed by the hardware.\nSoftware developers often use configuration management to track changes to source code files (version control). The configuration management system also keeps track of which object code file corresponds to which version of the source code file.\nPurposes.\nEstimation.\nThe number of source lines of code (SLOC) is often used as a metric when evaluating the productivity of computer programmers, the economic value of a code base, effort estimation for projects in development, and the ongoing cost of software maintenance after release.\nCommunication.\nSource code is also used to communicate algorithms between parties, e.g., code snippets online or in books.\nComputer programmers can find it helpful to review extant source code to learn about programming techniques. The sharing of source code between developers is often cited as a contributing factor to the maturing of their programming skills. Some consider source code an expressive artistic medium.\nSource code often contains comments\u2014blocks of text marked for the compiler to ignore. This content is not part of the program logic, but is instead intended to help readers understand the program.\nCompanies often keep the source code confidential in order to hide algorithms considered a trade secret. Proprietary, secret source code and algorithms are widely used for sensitive government applications such as criminal justice, which results in black box behavior with a lack of transparency into the algorithm's methodology. The result is avoidance of public scrutiny of issues such as bias.\nModification.\nAccess to the source code (not just the object code) is essential to modifying it. Understanding extant code is necessary to understand how it works and before modifying it. The rate of understanding depends both on the code base as well as the skill of the programmer. Experienced programmers have an easier time understanding what the code does at a high level. Software visualization is sometimes used to speed up this process. \nMany software programmers use an integrated development environment (IDE) to improve their productivity. IDEs typically have several features built in, including a source-code editor that can alert the programmer to common errors. Modification often includes code refactoring (improving structure without changing function) and restructuring (improving structure and function simultaneously). Nearly every change to code introduces new bugs or unexpected ripple effects, which require another round of fixes. \nCode reviews by other developers are often used to scrutinize new code added to a project. The purpose of this phase is often to verify that the code meets style and maintainability standards and that it is a correct implementation of the software design. According to some estimates, code review dramatically reduce the number of bugs persisting after software testing is complete. Along with software testing that works by executing the code, static program analysis uses automated tools to detect problems with the source code. Many IDEs support code analysis tools, which might provide metrics on the clarity and maintainability of the code. Debuggers are tools that often enable programmers to step through execution while keeping track of which source code corresponds to each change of state.\nCompilation and execution.\nSource code files in a high-level programming language must go through a stage of preprocessing into machine code before the instructions can be carried out. After being compiled, the program can be saved as an object file and the loader (part of the operating system) can take this saved file and execute it as a process on the computer hardware. Some programming languages use an interpreter instead of a compiler. An interpreter converts the program into machine code at run time, which makes them 10 to 100 times slower than compiled programming languages.\nPortability.\nAnother reason many programs are distributed in source code form, instead of as executable binary files, is that (often) a single source code file can be written once and will run on a variety of different end-user machines (each with their own localized compiler or interpreter), unlike an executable code file which generally only works on nearly-identical machines. Source code was used this way to distribute the Unix operating system early in the history of Unix, and later to allow programs written in scripting languages (in particular the JavaScript client-side scripting language) to run on a wide variety of machines.\nFor this goal, \"minified\", \"obfuscated\", or \"decompiled\" source code files (all of which eliminate the comments in the original code) are generally just as portable as the original source code files (which nearly always include commments), even though they are far less useful for modification, and therefore don't meet the definition of source code in the GNU General Public License, version 2 (GPL2).\nQuality.\nSoftware quality is an overarching term that can refer to a code's correct and efficient behavior, its reusability and portability, or the ease of modification. It is usually more cost-effective to build quality into a product from the start rather than try to add it later in a development process. Higher quality code reduces lifetime cost to both suppliers and customers as via higher reliability and maintainability. \nMaintainability is the quality of software enabling it to be easily modified without breaking extant functions. Following coding conventions such as using clear function and variable names that correspond to their purpose makes maintenance easier. Use of conditional loop statements only if the code could execute more than once, and eliminating code that will never execute can also increase understandability. Many software development organizations neglect maintainability during the development phase, even though it will increase long-term costs. Technical debt is incurred when programmers, often out of laziness or urgency to meet a deadline, choose quick and dirty solutions rather than build maintainability into their code. A common cause is underestimates in software development effort estimation, leading to insufficient resources allocated to development. A challenge with maintainability is that many software engineering courses do not emphasize it. Development engineers who know that they will not be responsible for maintaining the software do not have an incentive to build in maintainability.\nCopyright and licensing.\nThe situation varies worldwide, but in the United States before 1974, software and its source code was not copyrightable and therefore always public domain software. In 1974, the US Commission on New Technological Uses of Copyrighted Works (CONTU) decided that \"computer programs, to the extent that they embody an author's original creation, are proper subject matter of copyright\".\nProprietary software is rarely distributed as source code. Although the term open-source software literally refers to public access to the source code, open-source software has additional requirements: free redistribution, permission to modify the source code and release derivative works under the same license, and nondiscrimination between different uses\u2014including commercial use. The free reusability of open-source software can speed up development.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "27664", "revid": "42316941", "url": "https://en.wikipedia.org/wiki?curid=27664", "title": "SemiGroup", "text": ""}
{"id": "27667", "revid": "50236370", "url": "https://en.wikipedia.org/wiki?curid=27667", "title": "Space", "text": "Framework of distances and directions\nSpace is a three-dimensional continuum containing positions and directions. In classical physics, physical space is often conceived in three linear dimensions. Modern physicists usually consider it, with time, to be part of a boundless four-dimensional continuum known as \"spacetime\". The concept of space is considered to be of fundamental importance to an understanding of the physical universe. However, disagreement continues between philosophers over whether it is itself an entity, a relationship between entities, or part of a conceptual framework.\nIn the 19th and 20th centuries mathematicians began to examine geometries that are non-Euclidean, in which space is conceived as \"curved\", rather than \"flat\", as in the Euclidean space. According to Albert Einstein's theory of general relativity, space around gravitational fields deviates from Euclidean space. Experimental tests of general relativity have confirmed that non-Euclidean geometries provide a better model for the shape of space.\nPhilosophy of space.\nDebates concerning the nature, essence and the mode of existence of space date back to antiquity; namely, to treatises like the \"Timaeus\" of Plato, or Socrates in his reflections on what the Greeks called \"kh\u00f4ra\" (i.e. \"space\"), or in the \"Physics\" of Aristotle (Book IV, Delta) in the definition of \"topos\" (i.e. place), or in the later \"geometrical conception of place\" as \"space \"qua\" extension\" in the \"Discourse on Place\" (\"Qawl fi al-Makan\") of the 11th-century Arab polymath Alhazen. Many of these classical philosophical questions were discussed in the Renaissance and then reformulated in the 17th century, particularly during the early development of classical mechanics.\nIsaac Newton viewed space as absolute, existing permanently and independently of whether there was any matter in it. In contrast, other natural philosophers, notably Gottfried Leibniz, thought that space was in fact a collection of relations between objects, given by their distance and direction from one another. In the 18th century, the philosopher and theologian George Berkeley attempted to refute the \"visibility of spatial depth\" in his \"Essay Towards a New Theory of Vision\". Later, the metaphysician Immanuel Kant said that the concepts of space and time are not empirical ones derived from experiences of the outside world\u2014they are elements of an already given systematic framework that humans possess and use to structure all experiences. Kant referred to the experience of \"space\" in his \"Critique of Pure Reason\" as being a subjective \"pure \"a priori\" form of intuition\".\nGalileo.\nGalilean and Cartesian theories about space, matter, and motion are at the foundation of the Scientific Revolution, which is understood to have culminated with the publication of Newton's \"Principia Mathematica\" in 1687. Newton's theories about space and time helped him explain the movement of objects. While his theory of space is considered the most influential in physics, it emerged from his predecessors' ideas about the same.\nAs one of the pioneers of modern science, Galileo revised the established Aristotelian and Ptolemaic ideas about a geocentric cosmos. He backed the Copernican theory that the universe was heliocentric, with a stationary Sun at the center and the planets\u2014including the Earth\u2014revolving around the Sun. If the Earth moved, the Aristotelian belief that its natural tendency was to remain at rest was in question. Galileo wanted to prove instead that the Sun moved around its axis, that motion was as natural to an object as the state of rest. In other words, for Galileo, celestial bodies, including the Earth, were naturally inclined to move in circles. This view displaced another Aristotelian idea\u2014that all objects gravitated towards their designated natural place-of-belonging.\nRen\u00e9 Descartes.\nDescartes set out to replace the Aristotelian worldview with a theory about space and motion as determined by natural laws. In other words, he sought a metaphysical foundation or a mechanical explanation for his theories about matter and motion. Cartesian space was Euclidean in structure\u2014infinite, uniform and flat. It was defined as that which contained matter; conversely, matter by definition had a spatial extension so that there was no such thing as empty space.\nThe Cartesian notion of space is closely linked to his theories about the nature of the body, mind and matter. He is famously known for his \"cogito ergo sum\" (I think therefore I am), or the idea that we can only be certain of the fact that we can doubt, and therefore think and therefore exist. His theories belong to the rationalist tradition, which attributes knowledge about the world to our ability to think rather than to our experiences, as the empiricists believe. He posited a clear distinction between the body and mind, which is referred to as the Cartesian dualism.\nLeibniz and Newton.\nFollowing Galileo and Descartes, during the seventeenth century the philosophy of space and time revolved around the ideas of Gottfried Leibniz, a German philosopher\u2013mathematician, and Isaac Newton, who set out two opposing theories of what space is. Rather than being an entity that independently exists over and above other matter, Leibniz held that space is no more than the collection of spatial relations between objects in the world: \"space is that which results from places taken together\". Unoccupied regions are those that \"could\" have objects in them, and thus spatial relations with other places. For Leibniz, then, space was an idealised abstraction from the relations between individual entities or their possible locations and therefore could not be continuous but must be discrete.\nSpace could be thought of in a similar way to the relations between family members. Although people in the family are related to one another, the relations do not exist independently of the people.\nLeibniz argued that space could not exist independently of objects in the world because that implies a difference between two universes exactly alike except for the location of the material world in each universe. But since there would be no observational way of telling these universes apart then, according to the identity of indiscernibles, there would be no real difference between them. According to the principle of sufficient reason, any theory of space that implied that there could be these two possible universes must therefore be wrong.\nNewton took space to be more than relations between material objects and based his position on observation and experimentation. For a relationist there can be no real difference between inertial motion, in which the object travels with constant velocity, and non-inertial motion, in which the velocity changes with time, since all spatial measurements are relative to other objects and their motions. But Newton argued that since non-inertial motion generates forces, it must be absolute. He used the example of water in a spinning bucket to demonstrate his argument. Water in a bucket is hung from a rope and set to spin, starts with a flat surface. After a while, as the bucket continues to spin, the surface of the water becomes concave. If the bucket's spinning is stopped then the surface of the water remains concave as it continues to spin. The concave surface is therefore apparently not the result of relative motion between the bucket and the water. Instead, Newton argued, it must be a result of non-inertial motion relative to space itself. For several centuries the bucket argument was considered decisive in showing that space must exist independently of matter.\nKant.\nIn the eighteenth century the German philosopher Immanuel Kant published his theory of space as \"a property of our mind\" by which \"we represent to ourselves objects as outside us, and all as in space\" in the Critique of Pure Reason On his view the nature of spatial predicates are \"relations that only attach to the form of intuition alone, and thus to the subjective constitution of our mind, without which these predicates could not be attached to anything at all.\" This develops his theory of knowledge in which knowledge about space itself can be both \"a priori\" and \"synthetic\".\nAccording to Kant, knowledge about space is \"synthetic\" because any proposition about space cannot be true \"merely\" in virtue of the meaning of the terms contained in the proposition. In the counter-example, the proposition \"all unmarried men are bachelors\" \"is\" true by virtue of each term's meaning. Further, space is \"a priori\" because it is the form of our receptive abilities to receive information about the external world. For example, someone without sight can still perceive spatial attributes via touch, hearing, and smell. Knowledge of space itself is \"a priori\" because it belongs to the subjective constitution of our mind as the form or manner of our intuition of external objects.\nNon-Euclidean geometry.\nEuclid's \"Elements\" contained five postulates that form the basis for Euclidean geometry. One of these, the parallel postulate, has been the subject of debate among mathematicians for many centuries. It states that on any plane on which there is a straight line \"L1\" and a point \"P\" not on \"L1\", there is exactly one straight line \"L2\" on the plane that passes through the point \"P\" and is parallel to the straight line \"L1\". Until the 19th century, few doubted the truth of the postulate; instead debate centered over whether it was necessary as an axiom, or whether it was a theory that could be derived from the other axioms. Around 1830 though, the Hungarian J\u00e1nos Bolyai and the Russian Nikolai Ivanovich Lobachevsky separately published treatises on a type of geometry that does not include the parallel postulate, called hyperbolic geometry. In this geometry, an infinite number of parallel lines pass through the point \"P\". Consequently, the sum of angles in a triangle is less than 180\u00b0 and the ratio of a circle's circumference to its diameter is greater than pi. In the 1850s, Bernhard Riemann developed an equivalent theory of elliptical geometry, in which no parallel lines pass through \"P\". In this geometry, triangles have more than 180\u00b0 and circles have a ratio of circumference-to-diameter that is less than pi.\nGauss and Poincar\u00e9.\nAlthough there was a prevailing Kantian consensus at the time, once non-Euclidean geometries had been formalised, some began to wonder whether or not physical space is curved. Carl Friedrich Gauss, a German mathematician, was the first to consider an empirical investigation of the geometrical structure of space. He thought of making a test of the sum of the angles of an enormous stellar triangle, and there are reports that he actually carried out a test, on a small scale, by triangulating mountain tops in Germany.\nHenri Poincar\u00e9, a French mathematician and physicist of the late 19th century, introduced an important insight in which he attempted to demonstrate the futility of any attempt to discover which geometry applies to space by experiment. He considered the predicament that would face scientists if they were confined to the surface of an imaginary large sphere with particular properties, known as a sphere-world. In this world, the temperature is taken to vary in such a way that all objects expand and contract in similar proportions in different places on the sphere. With a suitable falloff in temperature, if the scientists try to use measuring rods to determine the sum of the angles in a triangle, they can be deceived into thinking that they inhabit a plane, rather than a spherical surface. In fact, the scientists cannot in principle determine whether they inhabit a plane or sphere and, Poincar\u00e9 argued, the same is true for the debate over whether real space is Euclidean or not. For him, which geometry was used to describe space was a matter of convention. Since Euclidean geometry is simpler than non-Euclidean geometry, he assumed the former would always be used to describe the 'true' geometry of the world.\nEinstein.\nIn 1905, Albert Einstein published his special theory of relativity, which led to the concept that space and time can be viewed as a single construct known as \"spacetime\". In this theory, the speed of light in vacuum is the same for all observers\u2014which has the result that two events that appear simultaneous to one particular observer will not be simultaneous to another observer if the observers are moving with respect to one another. Moreover, an observer will measure a moving clock to tick more slowly than one that is stationary with respect to them; and objects are measured to be shortened in the direction that they are moving with respect to the observer.\nSubsequently, Einstein worked on a general theory of relativity, which is a theory of how gravity interacts with spacetime. Instead of viewing gravity as a force field acting in spacetime, Einstein suggested that it modifies the geometric structure of spacetime itself. According to the general theory, time goes more slowly at places with lower gravitational potentials and rays of light bend in the presence of a gravitational field. Scientists have studied the behaviour of binary pulsars, confirming the predictions of Einstein's theories. Non-Euclidean geometry is usually used to describe spacetime.\nMathematics.\nIn modern mathematics spaces are defined as sets with some added structure. They are typically topological spaces, in which a concept of neighbourhood is defined, frequently by means of a distance (metric spaces). The elements of a space are often called \"points\", but they can have other names such as vectors in vector spaces and functions in function spaces.\nPhysics.\n&lt;templatestyles src=\"Hlist/styles.css\"/&gt;\nSpace is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.\nToday, our three-dimensional space is viewed as embedded in a four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind spacetime is that time is hyperbolic-orthogonal to each of the three spatial dimensions.\nRelativity.\nBefore Albert Einstein's work on relativistic physics, time and space were viewed as independent dimensions. Einstein's discoveries showed that due to relativity of motion our space and time can be mathematically combined into one object\u2013spacetime. It turns out that distances in space or in time separately are not invariant with respect to Lorentz coordinate transformations, but distances in Minkowski space along spacetime intervals are\u2014which justifies the name.\nIn addition, time and space dimensions should not be viewed as exactly equivalent in Minkowski space. One can freely move in space but not in time. Thus, time and space coordinates are treated differently both in special relativity (where time is sometimes considered an imaginary coordinate) and in general relativity (where different signs are assigned to time and space components of spacetime metric).\nFurthermore, in Einstein's general theory of relativity, it is postulated that spacetime is geometrically distorted \u2013 \"curved\" \u2013 near to gravitationally significant masses.\nOne consequence of this postulate, which follows from the equations of general relativity, is the prediction of moving ripples of spacetime, called gravitational waves. While indirect evidence for these waves has been found (in the motions of the Hulse\u2013Taylor binary system, for example) experiments attempting to directly measure these waves are ongoing at the LIGO and Virgo collaborations. LIGO scientists reported the first such direct observation of gravitational waves on 14 September 2015.\nCosmology.\nRelativity theory leads to the cosmological question of what shape the universe is, and where space came from. It appears that space was created in the Big Bang, 13.8\u00a0billion years ago and has been expanding ever since. The overall shape of space is not known, but space is known to be expanding very rapidly due to the cosmic inflation.\nSpatial measurement.\nThe measurement of \"physical space\" has long been important. Although earlier societies had developed measuring systems, the International System of Units, (SI), is now the most common system of units used in the measuring of space, and is almost universally used.\nCurrently, the standard space interval, called a standard meter or simply meter, is defined as the distance traveled by light in vacuum during a time interval of exactly 1/299,792,458 of a second. This definition coupled with present definition of the second is based on the special theory of relativity in which the speed of light plays the role of a fundamental constant of nature.\nGeographical space.\nGeography is the branch of science concerned with identifying and describing places on Earth, utilizing spatial awareness to try to understand why things exist in specific locations. Cartography is the mapping of spaces to allow better navigation, for visualization purposes and to act as a locational device. Geostatistics apply statistical concepts to collected spatial data of Earth to create an estimate for unobserved phenomena.\nGeographical space is often considered as land, and can have a relation to ownership usage (in which space is seen as property or territory). While some cultures assert the rights of the individual in terms of ownership, other cultures will identify with a communal approach to land ownership, while still other cultures such as Australian Aboriginals, rather than asserting ownership rights to land, invert the relationship and consider that they are in fact owned by the land. Spatial planning is a method of regulating the use of space at land-level, with decisions made at regional, national and international levels. Space can also impact on human and cultural behavior, being an important factor in architecture, where it will impact on the design of buildings and structures, and on farming.\nOwnership of space is not restricted to land. Ownership of airspace and of waters is decided internationally. Other forms of ownership have been recently asserted to other spaces\u2014for example to the radio bands of the electromagnetic spectrum or to cyberspace.\nPublic space is a term used to define areas of land as collectively owned by the community, and managed in their name by delegated bodies; such spaces are open to all, while private property is the land culturally owned by an individual or company, for their own use and pleasure.\nAbstract space is a term used in geography to refer to a hypothetical space characterized by complete homogeneity. When modeling activity or behavior, it is a conceptual tool used to limit extraneous variables such as terrain.\nIn psychology.\nPsychologists first began to study the way space is perceived in the middle of the 19th century. Those now concerned with such studies regard it as a distinct branch of psychology. Psychologists analyzing the perception of space are concerned with how recognition of an object's physical appearance or its interactions are perceived, see, for example, visual space.\nOther, more specialized topics studied include amodal perception and object permanence. The perception of surroundings is important due to its necessary relevance to survival, especially with regards to hunting and self preservation as well as simply one's idea of personal space.\nSeveral space-related phobias have been identified, including agoraphobia (the fear of open spaces), astrophobia (the fear of celestial space) and claustrophobia (the fear of enclosed spaces).\nThe understanding of three-dimensional space in humans is thought to be learned during infancy using unconscious inference, and is closely related to hand-eye coordination. The visual ability to perceive the world in three dimensions is called depth perception.\nIn the social sciences.\nSpace has been studied in the social sciences from the perspectives of Marxism, feminism, postmodernism, postcolonialism, urban theory and critical geography. These theories account for the effect of the history of colonialism, transatlantic slavery and globalization on our understanding and experience of space and place. The topic has garnered attention since the 1980s, after the publication of Henri Lefebvre's \"The Production of Space .\" In this book, Lefebvre applies Marxist ideas about the production of commodities and accumulation of capital to discuss space as a social product. His focus is on the multiple and overlapping social processes that produce space.\nIn his book \"The Condition of Postmodernity,\" David Harvey describes what he terms the \"time-space compression.\" This is the effect of technological advances and capitalism on our perception of time, space and distance. Changes in the modes of production and consumption of capital affect and are affected by developments in transportation and technology. These advances create relationships across time and space, new markets and groups of wealthy elites in urban centers, all of which annihilate distances and affect our perception of linearity and distance.\nIn his book \"Thirdspace,\" Edward Soja describes space and spatiality as an integral and neglected aspect of what he calls the \"trialectics of being,\" the three modes that determine how we inhabit, experience and understand the world. He argues that critical theories in the Humanities and Social Sciences study the historical and social dimensions of our lived experience, neglecting the spatial dimension. He builds on Henri Lefebvre's work to address the dualistic way in which humans understand space\u2014as either material/physical or as represented/imagined. Lefebvre's \"lived space\" and Soja's \"thirdspace\" are terms that account for the complex ways in which humans understand and navigate place, which \"firstspace\" and \"Secondspace\" (Soja's terms for material and imagined spaces respectively) do not fully encompass.\nPostcolonial theorist Homi Bhabha's concept of Third Space is different from Soja's Thirdspace, even though both terms offer a way to think outside the terms of a binary logic. Bhabha's Third Space is the space in which hybrid cultural forms and identities exist. In his theories, the term hybrid describes new cultural forms that emerge through the interaction between colonizer and colonized.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27669", "revid": "50412405", "url": "https://en.wikipedia.org/wiki?curid=27669", "title": "Spanish cuisine", "text": "Culinary traditions of Spain\nSpanish cuisine () consists of the traditions and practices of Spanish cooking. It features considerable regional diversity, with significant differences among the traditions of each of Spain's regional cuisines.\nOlive oil (of which Spain is the world's largest producer) is extensively used in Spanish cuisine. It forms the base of many vegetable sauces (known in Spanish as \"sofritos\"). Herbs most commonly used include parsley, oregano, rosemary and thyme. The use of garlic has been noted as common in Spanish cooking. The most-used meats in Spanish cuisine include chicken, pork, lamb and veal. Fish and seafood are also consumed on a regular basis. Tapas and pinchos are snacks and appetizers commonly served in bars and cafes.\nHistory.\nAntiquity.\nAuthors such as Strabo wrote about the aboriginal people of Spain using nuts and acorns as staple foods. The extension of vineyards along the Mediterranean seemed to be due to the colonization of Greeks and Phoenicians, who also introduced the production of olive oil. Spain became the largest producer of olive oil in the world. The growing of crops of the so-called (the \"Mediterranean triad\": wheat, grapes, and olives) underpinned the staple meal products for the inhabitants of the south of the Iberian Peninsula during the Roman Era (bread, wine and oil).\nMiddle Ages.\nThe Visigoths' limited but lasting contributions to Spanish cuisine included the spread of the consumption of fermented milk and the preference for avoiding mixing water and wine.\nRice was possibly introduced for the first time by Byzantines in the Iberian Peninsula by the 6th century. After the Muslim conquest of the Iberian peninsula in the 8th century, Arabs expanded rice cultivation, bringing new irrigation techniques originally from the Indian subcontinent that also allowed for the cultivation of crops such as sugar cane, watermelon, lemon and oranges. Other ingredients possibly introduced in the Iberian Peninsula during the Hispano-Muslim period include sorghum, spinach, eggplant, peach, apricot and saffron. The most famous Spanish dish, paella, uses two ingredients that were probably popularized during the Al-Andalus period: rice and saffron.\nMoors also developed the basis for the art of pastry-making and introduced \"escabeche\", a food preservation technique relying on vinegar. Dishes like \"ajo blanco\", ', \"alaj\u00fa\", \"hallulla\", ', \"mojama\", \"arrope\", were some of the many legacies of Moorish cuisine. Although Muslim religion did not allow alcoholic drinks, the consumption of wine was widespread as the Qur'anic precepts never got to overrule the preexisting traditions. There are many accounts of the \"drinking chats\" of Abd al-Rahman II, Abd al-Rahman III and Almanzor.\nObserving the \"kashrut\" regulations, Jews and \"judaizantes\" opted for blood-drained meat without fat, outright rejecting bacon. \"Potajes\" were an important part of the Jewish cuisine in the Middle Ages, most notably \"adafina\" (a local name for a \"\u1e25amin\" dish) along with other Jewish culinary legacies in Spain. \"\" (a formerly popular sauce preparation out of vogue since the late 17th century) was a Sephardic recipe in origin.\nThe history of cookbooks in Spain can be traced back to works such as the \"Llibre de Sent Sov\u00ed\" (1324) and Ruperto de Nola's \"Llibre de Coch\" (1520), both written in the Catalan language. Other of the earliest cooking books in pre-modern Iberia are the \"Fi\u1e0d\u0101lat al-Jiw\u0101n f\u012b \u1e6cayyib\u0101t al-\u1e6ca\u02bf\u0101m wa-l-Alw\u0101n\" by Murcia-born Ibn Raz\u012bn al-Tuj\u012bb\u012b and the anonymous \"Kit\u0101b al-\u1e6cabikh f\u012b al-Maghrib wa al-Andalus f\u012b \u02bdA\u1e63r al-Muwa\u1e25\u1e25id\u012bn, li-mu\u02bdallif majh\u016bl\", written in Arabic.\nModern era.\nThe arrival of Europeans in the Americas in 1492 initiated the advent of new culinary elements, such as tomatoes, potatoes, maize, bell peppers, spicy peppers, paprika, vanilla and cocoa. Spain was where chocolate was first mixed with sugar to temper its natural bitterness. Other ingredients traveled to the Americas, such as rice, grapes, olives, and many types of cereals.\nInfluenced by Arabic \"harisa\", grain-based soups such as \"\" (along the Mediterranean coast) and, similarly, \"gachas\" (in the Central Plateau) were customary in Early Modern Spain.\nForeign visitors noted with disdain the Spaniards' use of olive oil and lard for cooking rather than their preferred butter. The latter was barely available and, according to the 17th-century account of Madame d'Aulnoy, on the rare occasions that it was, would come \"from afar, preserved in pig's tripes and full of worms\". Butter was only produced locally in places such as Galicia, Asturias and Soria, or was imported, preserved in potassium nitrate, (the so-called \"Flanders' butter\").\nBy the 18th century, many American ingredients, such as peppers and tomatoes, had been fully incorporated into Spanish cuisine. Contemporary foreign visitors, such as French ambassador Jean-Fran\u00e7ois de Bourgoing, judged negatively this change happening in Spain by the late part of the century: \"Spanish cooking, which they have inherited, is not generally pleasing to foreigners. Spaniards like strong condiments such as pepper, tomato sauce, hot peppers and saffron, which color or infect nearly all their dishes.\"\nSpain was the bridge for the Columbian exchange between the rest of Europe and the New World. Many traditional Spanish dishes such as \"tortilla de patata\" (an omelette made with potatoes), would not be possible without the Columbian exchange. \"Gazpacho\", \"salmorejo\", and \"pan con tomate\" are made with tomatoes, which traveled from the New World to the Old World.\nFor most of the 19th century, the aristocracy consumed a set of dishes that was largely an imitation of French cuisine. That was the available cuisine at the time, together with the degeneration of regional cuisines. One positive foreign take on the Spanish dishes\u2014opposing the largely negative views of other foreign commentators\u2014was that of Richard Ford, who was fond of Spanish specialties such as sherry and ham.\nModern Spanish cuisine was gestated in the late-19th to early-20th century, with gastronomes and writers such as Mariano Pardo de Figueroa (Dr. Thebussem), , , Emilia Pardo Baz\u00e1n, and Dionisio P\u00e9rez, some of whom put effort into developing the idea of a \"national cuisine\" recognisable by Spaniards as their own.\nKeen on participating in the Spanish nation-building process, Dr. Thebussem, in an autochthonous example of culinary nationalism, proposed to the King's Chef that the \"olla podrida\" (a rustic stew typically made of meat, legumes and other vegetables) should be served at official banquets as a national dish. This could be considered an important step in the process of straying away from the French cooking paradigm, which was dominant in the 19th century in Europe. \"Olla podrida\" had been previously ridiculed in foreign (most notably French) satires.\nAlthough the new foodscape built in opposition to the French centralist culinary model accounted for the awareness of the distinctive regional singularities, subsequent food writers in the country would continue to cope with the tension between the Spanish peripheral and centralist foodscapes.\nThe influential cooking book \"1080 recetas de cocina\" by Simone Ortega (first published in 1972) became a hit in Spain, remaining as of 2019 the third best-selling book ever in the history of the country after \"Don Quixote\" and the Bible. This was not a book exclusively of Spanish traditional recipes, but also included French recipes, bringing an exotic penchant to Spanish homes.\nTelevised cooking shows started in the country in 1984 with \"Con las manos en la masa\".\nMeal routines.\nA continental-style breakfast (\"desayuno\") may be eaten just after waking up, or before entering the workplace. Common breakfast items include coffee, milk, chocolate drink, biscuits (most notably Marie biscuits), \"magdalenas\", toast (featuring ingredients such as oil, tomato and butter), and churros.\nDue to the large time span between breakfast and lunch, it is not uncommon to halt the working schedule to take a mid-morning snack.\nLunch (\"el almuerzo\" or \"la comida\", literally meaning \"the meal\"), the large midday meal in Spain, contains several courses, especially in restaurants. In some regions of Spain, the word \"almuerzo\" refers to the mid-morning snack, instead of lunch. Lunch usually starts around 2:00\u20132:30\u00a0p.m. and finishes around 3:00\u20133:30\u00a0p.m., and is usually followed by \"sobremesa\", which refers to the table talk that Spanish people undertake. Menus are organized according to these courses and include five or six choices in each course. At home, Spanish meals contain one or two courses and a dessert. The content of this meal is usually a soup dish, salad, a meat or a fish dish, and a dessert such as fruit, yoghurt or something sweet. \"Tapas\" may also be typically served before or during lunch.\nAccording to a 2017 report, the Spanish government has taken steps to shorten the traditional long lunch break in an effort to end the workday earlier. Most businesses shut down for two or three hours for lunch, then resume the working day until dinner time in the evening.\n\"La cena\", meaning both dinner or supper, is taken between 8:30\u00a0p.m. and 11:00\u00a0p.m. It typically consists of one course and dessert. Due to the large time span between lunch and dinner, an afternoon snack, \"la merienda\", equivalent to afternoon tea, may take place at about 6:00\u00a0p.m. At \"merienda\", people typically drink coffee, eat something sweet, or eat a sandwich or a piece of fruit.\nSome country-wide staple dishes common throughout Spain include \"croquetas\" (croquettes), \"paella\" (a rice dish from the Valencian community), \"ensaladilla rusa\" (Olivier salad), \"gazpacho\" (a cold vegetable soup), and \"tortilla de patatas\" (Spanish omelette). There is a disagreement in Spanish society regarding onion as an ingredient in the Spanish omelette, often accompanied by highly opinionated views on either side.\n\"Tapas\" (appetizers), served before lunch or dinner, or during them, are common. It is also common for \"tapas\" to be provided as a complimentary appetizer in bars and cafes when ordering a drink. Other common \"tapas\" include \"mejillones en escabeche\" (marinated mussels), , \"\" (meatballs), \"callos\", \"torreznos,\" or \"raxo de cerdo\".\nRegional cuisines.\nAndalusia.\nAndalusian cuisine is twofold: rural and coastal. Of all the Spanish regions, this region uses the most olive oil in its cuisine. The Andalusian dish that has possibly achieved the most international fame is \"gazpacho\", a cold soup made with chopped vegetables, such as tomatoes and green peppers, vinegar, water, salt, olive oil, and bread (crumbs). Other cold soups include \"pole\u00e1\", \"zoque\" and \"salmorejo\".\nEating olives as a snack is common. Meat dishes include \"flamenqu\u00edn\", \"pring\u00e1\", oxtail stew, and \"menudo gitano\" (also called Andalusian tripe). Hot soups include \"sopa de gato\" (made with bread), \"caldillo de perro\" (fish soup with orange juice) and \"migas canas\". Fish dishes include \"pesca\u00edto frito\", \"soldaditos de Pav\u00eda\", and \"\".\nCured meats include serrano ham and ib\u00e9rico ham. Typical drinks in the area include anise, wine (such as Malaga, Jerez, and Pedro Xim\u00e9nez), and sherry brandy.\nAragon.\nAragonese cuisine has a rural origin. One of its most famous dishes is \"asado de ternasco\" (roast lamb), in which lamb is cooked with garlic, salt, olive oil, laurel leaves, thyme, and parsley. Pork dishes are also very popular, among them, \"magras con tomate\". Popular Aragonese recipes made with bread are \"migas de Pastor\", \"migas con chocolate\", \"rega\u00f1aos\", and \"goguera\".\nLegumes are very important to Aragonese dishes, but the most popular vegetables are borage and thistle, as well as the famed \"tomate rosa de Barbastro\". \"Jam\u00f3n de Teruel\" and ham from Huesca are frequently used cured meats. Among the cheeses, \"queso de Tronch\u00f3n\" is notable. Fruit-based cuisine includes \"frutas de Arag\u00f3n\" (English: 'fruits of Aragon', candied fruits covered in chocolate) and maraschino cherries. \"Melocot\u00f3n con vino\" consists of \"melocot\u00f3n de Calanda\", a regional peach variant, infused in red wine with sugar and cinnamon.\nOther sweet Aragonese specialities are \"trenza de Almudevar\", \"tortas de alma\", \"guirlache\" (a type of nougat), \"adoqu\u00edn del Pilar\", and \"Espa\u00f1oletas\" (a kind of local cookie).\nThe prevalence of peaches in Aragonese cuisine extends to drinks. \"Sopeta\" is a traditional beverage emerging from sliced peach, white wine and sugar. The best-known wines of Aragon are those from Cari\u00f1ena, Somontano (Huesca), Calatayud, and Campo de Borja.\nAsturias.\nAsturian cuisine has a long and rich history, deeply rooted in Celtic traditions of Atlantic Europe. One of its most famous dishes is \"fabada asturiana\". \"Fabada\" is the traditional stew of the region, made with white beans, sausages (such as \"chorizo\" and \"morcilla\"), and pork. A well-known recipe is \"fabes con almejas\" (beans with clams). Asturian beans (\"fabes\") can also be cooked with hare, partridge, prawns, or octopus. Another known recipe is \"pote asturiano\" (made with white beans, kale, potatoes, and a variety of sausages and bacon) and \"potaje de vigilia\".\nPork-based foods such as \"chosco\", \"callos a l'asturiana\", and \"bollu pre\u00f1\u00e1u\" (chorizo-stuffed bread rolls) are popular. Common meat dishes include \"carne gobernada\" (roasted veal), \"cachopo\" (a crunchy, crumb-coated veal steak stuffed with ham and cheese), and \"caldereta\". Fish and seafood play an important role in Asturian cuisine. The Cantabrian Sea provides a rich variety of species, including tuna, hake and sardines.\nAsturian cheeses are very popular in the rest of Spain. Among them, the most representative is Cabrales cheese, a pungent, blue cheese developed in the regions near the Picos de Europa. Other popular cheese types are \"gamon\u00e9u\" \"afuega'l pitu\", and \"queso de Pr\u00eda\". These are usually enjoyed with the local cider, a low-alcohol drink made of Asturian apples with a distinctive sourness.\nAsturian cider, \"Sidra de Asturias\", made of a special type of apple, is traditionally poured \"escanciada\" from a certain height, usually over the head of the waiter/server. When the cider falls into the glass from above, the drink \"breaks\", becoming aerated and bubbly. It is consumed immediately after being served, in consecutive, tiny shots.\nNotable desserts are \"frisuelos\" (similar to \"cr\u00eapes\", usually filled with cream or apple jam), rice pudding (white rice cooked with milk, lemon zest and sugar), and \"carbay\u00f3n (dulce)\" (puff pastry cakes filled with almond mash and covered with sugar glaze).\nBalearic Islands.\nBalearic cuisine has purely Mediterranean characteristics due to its location. The islands have been conquered several times throughout their history by the French and the English, which left some culinary influences. Some well-known food items are the \"sobrassada\", \"arroz brut\", mah\u00f3n cheese, \"gin de Menorca\" (\"pelota\"), and mayonnaise. Among the dishes are \"tumbet\", \"frito mallorqu\u00edn\", and roasted suckling pig. Popular desserts include \"ensa\u00efmada\", \"tambor d'ametlla\", and \"suspiros de Manacor\".\nBasque Country.\nThe cuisine of the Basque Country has a wide and varied range of ingredients and preparations. Food and drinks are especially important in the Basque culture. Highlights include meat and fish dishes. Among fish, cod (\"bacalao\") is produced in various preparations, such as \"bacalao al pil pil\" and \"bacalao a la vizca\u00edna\". Also popular are anchovies, bream, and bonito. Among the most famous dishes is \"changurro\" (stuffed king crab). Common meat dishes include beef steaks, pork loin with milk, fig leaf quail, and marinated goose.\n\"Txakoli\" or \"chacol\u00ed\" (a white wine characterised by its high acidity and a lesser-than-average alcohol content) is a staple drink from the Basque Country, produced in \u00c1lava, Guip\u00fazcoa and Biscay. Basque cider is popular following the apple harvest and is served in cider houses and bars.\nCanary Islands.\nThe Canary Islands have a unique cuisine due to their geographical location in the Atlantic Ocean. The Canary Islands were part of the trading routes to the Americas, hence creating a melting pot of different culinary traditions. Fish (fresh or salted) and potatoes are among the most common staple foods in the islands. The consumption of cheese, fruits, and pork also characterizes Canarian cuisine. The islands' close proximity to continental Africa influences the climate and creates a range of warm temperatures that in modern times have fostered the agriculture of tropical and semitropical crops: bananas, yams, mangoes, avocados, and persimmons. These crops are heavily used in Canarian cuisine.\nThe aboriginal people, Guanches, based their diet on \"gofio\" (a type of flour made of different toasted grains), shellfish, and goat and pork products. \"Gofio\" is still consumed in the islands and has become part of the traditional cuisine.\nA sauce called \"mojo\" is very common throughout the islands. It has been adapted and developed in many ways, so that it may complement various main dishes. Fish dishes usually require a \"green \"mojo\"\" made from coriander or parsley, while roasted meats require a red variety made from chilli peppers that are commonly known as \"mojo pic\u00f3n\".\nSome classic dishes in the Canary Islands include \"papas arrugadas\", \"almogrote\", \"frangollo\", rabbit in \"salmorejo\" sauce, and stewed goat.\nSome popular desserts are \"truchas\" (pastries filled with sweet potato or pumpkin), roasted \"gofio\" (a \"gofio\"-based dough with nuts and honey), \"pr\u00edncipe Alberto\" (a mousse-like preparation with almonds, coffee, and chocolate), and \"quesillo\" (a variety of flan made with condensed milk).\nWineries are common in the islands. However, only Malvasia wine from Lanzarote has gained international recognition.\nCantabria.\nA popular Cantabrian dish is \"cocido monta\u00f1\u00e9s\" (highlander stew): a rich stew made with beans, cabbage, and pork. Seafood is widely used and bonito is present in the typical \"sorropot\u00fan\" or \"marmita de bonitu\" (tuna pot). Recognized quality meats include Tudanca veal and game meat.\nCantabrian pastries include \"sobaos\" and \"quesadas pasiegas\". Dairy products include Cantabrian cream cheese, smoked cheeses, \"pic\u00f3n Bejes-Tresviso\", and \"quesucos de Li\u00e9bana\".\nAs for alcohol, \"orujo\" is the Cantabrian pomace brandy. Cider (\"sidra\") and \"chacoli\" wine are also favorites. Cantabria has two wines labelled \"denominaci\u00f3n de origen calificada\" ('denomination of qualified origin'): Costa de Cantabria and Li\u00e9bana.\nCastile-La Mancha.\nIn Castilla-La Mancha, the culinary habits reflect the origin of foods eaten by shepherds and peasants. Wheat and grains are a dominant product and ingredient\u2014they are used in bread, soups, \"gazpacho manchego\", migas, and porridge. One of the most abundant ingredients in Manchego cuisine is garlic, leading to dishes such as \"ajoarriero\", \"ajo puerco\", and \"ajo mataero\".\nSome traditional recipes are \"gazpacho manchego\", \"pisto manchego\", and \"migas ruleras\". Also popular in this region is \"morteruelo\", a kind of foie gras. Manchego cheese is also renowned.\nBecause its lands are dry, and thus unable to sustain large amounts of cattle living on grass, an abundance of small animals, such as rabbit and especially birds (pheasant, quail, partridge, squab), can be found. This has led to game meat being incorporated into traditional dishes, such as \"conejo al Ajillo\" (rabbit in garlic sauce), \"perdiz escabechada\" (marinated partridge) or \"huevos de codorniz\" (quail eggs).\nCastile and Le\u00f3n.\nIn Castile and Le\u00f3n, characteristic dishes include \"morcilla\" (a black pudding made with special spices), \"judi\u00f3n de la Granja\", \"sopa de ajo\" (garlic soup), \"cochinillo asado\" (roast piglet), \"lechazo\" (roast lamb), and \"chulet\u00f3n de \u00c1vila\" (\u00c1vila rib steak). Other foods include \"botillo del Bierzo\", \"hornazo\" from Salamanca, \"jam\u00f3n de Guijuelo\" (a cured ham from Guijuelo, Salamanca), \"salchicha de Zarat\u00e1n\", other sausages, Serrada cheese (made from sheep's milk), \"queso de Burgos\", and Ribera del Duero wines.\nMajor wines in Castilian\u2013Leonese cuisine include the robust wine of Toro, reds from Ribera del Duero, whites from Rueda, and clarets from Cigales.\nCatalonia.\nThe extensive cuisine of Catalonia has rural origins and features foods from three climates: coastal, mountains, and the interiors. Some famous dishes include \"escudella\", \"pa amb tom\u00e0quet\", \"coca de recapte\", \"samfaina\", thyme soup, \"caragols a la llauna\", and the bomba de Barceloneta. Notable sauces are romesco sauce, aioli, bouillabaisse of Catalan origin, and \"picada\".\nCured pork cuisine includes \"botifarra\" (white and black) and the \"fuet\" of Vic. Fish dishes include \"suquet\" (fish stew), cod stew, and \"arr\u00f2s negre\". Among the vegetable dishes, the most famous are \"cal\u00e7ots\" and \"escalivada\" (roasted vegetables). Desserts include Catalan cream, \"carquinyolis\", \"panellets\", \"tortell\", and \"neules\".\nExtremadura.\nThe cuisine of Extremadura is simple at heart, with dishes based on those prepared by shepherds. It is very similar to the cuisine of Castilla. Extremaduran cuisine is abundant in pork; it is said that the region is one of the best for breeding pigs in Spain thanks to the acorns that grow in their fields. Iberian pig herds raised in the fields of Mont\u00e1nchez are characterized by dark skin and thin legs. This breed of pig is found exclusively in Southwestern Iberia, both in Spain and Portugal. Iberian pork products such as sausages are common and often added to stews (\"cocido extreme\u00f1o\"), as well as \"cachuela\" (pork liver p\u00e2t\u00e9 seasoned with paprika, garlic and other spices).\nOther meat dishes include lamb stew or goat stew (\"caldereta de cordero\" and \"caldereta de cabrito\"). Additionally, meat dishes can include game meats, such as wild boar, partridge, pheasant, or venison.\nDistinctive cheeses from the region include the so-called \"quesos de torta\" (sheep milk cheeses typically curdled with the infusion of thistle). Both the torta of La Serena and the torta of El Casar enjoy a protected designation of origin. Among the desserts are \"leche frita\", \"perrunilla\", and \"pesti\u00f1os\" (fritters), as well as many sweets that have their origins in convents.\nCod preparations are well-known, and tench is among the most traditional freshwater fish, with fish and vegetable dishes such as \"moje de peces\" or \"escarapuche\".\nSoups are often bread-based and are served in both hot and cold forms. Pennyroyal mint is sometimes used to season \"gazpachos\" or soups such as \"sopa de poleo\". Extremaduran \"ajoblanco\" (\"ajoblanco extreme\u00f1o\") is a cold soup, which is different from Andalusian \"ajoblanco\" since it contains egg yolk in the emulsion and vegetables but no almonds.\nThe northeastern \"comarca\" of La Vera produces \"piment\u00f3n de la Vera\", a smoked paprika highly valued all over Spain and extensively used in Extremaduran cuisine.\nThe region is also known for its \"vino de pitarra\" tradition (homemade wine made in small earthenware vessels).\nGalicia.\nGalician cuisine is well known throughout Spain because of emigration to other regions. Similar to neighbouring Asturias, Galicia shares some culinary traditions in stews and soups with the Celtic nations of Atlantic Europe. One of the most noted Galician dishes is soup. Also notable in this region is pork with turnip tops, a popular component of the Galician carnival meal \"laconadas\". Another recipe is \"caldo de casta\u00f1as\" (a chestnut broth), which is commonly consumed during winter. Pork products are also popular. Cattle raising is very common in Galicia, consequently red meat is consumed often, typically with potatoes.\nThe simplicity and authenticity of Galician cooking methods were praised in the early 20th century by the prominent gastronome Manuel Puga e Parga (also known as \"Picadillo\"), who praised dishes such as \"lac\u00f3n con grelos\" or \"caldeiradas\" (fish stew), in opposition to the perceived sophistication of the French cuisine.\nGalician seafood dishes are well-known and rich in variety. Among these are \"empanadas\", octopus, scallops, crab, and barnacles. In the city of Santiago de Compostela, located along an ancient pilgrim trail from the Pyrenees, it was customary for travelers to eat scallops upon first arriving in the city.\nAmong the many dairy products is \"queso de tetilla\".\nThe \"queimadas\" (a folkloric preparation of \"orujo\") consists of mixing alcoholic beverages with peels of orange or lemon, sugar or coffee beans, prepared in a nearly ritual ceremony involving the \"flamb\u00e9\" of the beverage. Sweets that are famous throughout the Iberian Peninsula are the \"tarta de Santiago\" and \"filloas\" (cr\u00eapes).\nLa Rioja.\nLa Rioja is recognized for its use of meats such as pork and cold cuts, which are produced after the traditional slaughter. Lamb is perhaps the second most popular meat product in this region (\"chuletillas al sarmiento\"). Veal is common in mountainous areas. The most famous dishes are Rioja style potatoes and \"fritada\". Another well-known dish is \"caparrones\" (Rioja stew). Lesser-known dishes are \"almuerzo del Santo\" and \"ajo huevo\" (garlic eggs). \"Pimientos asados\" (roasted peppers) is a notable vegetable dish.\nLa Rioja is famously known in Spain for its red wine, so most of these dishes are served with wine. Rioja wine has designated origin status.\nMadrid.\nMadrid did not have a special regional identity before 1561, when king Philip II made it the capital of Spain. Since then, due to immigration, many of Madrid's culinary dishes have been made from modifications to dishes from other Spanish regions. Madrid, due to the influx of visitors from the nineteenth century onwards, was one of the first cities to introduce the concept of the restaurant, hosting some of the earliest examples.\nMurcia.\nThe cuisine of the region of Murcia can be said to have two versions, one linked to the \"huerta\" (irrigated areas) and another one closer to Manchego cuisine. The region of Murcia is famous for its varied fruit production. Among the most outstanding dishes are ', \"zarangollo\", ', \"aubergine a la cr\u00e8me\", and '. A typical sauce of this area is ', used to accompany meat dishes.\nRegional dishes include ' (beans cooked with bay leaves, hot peppers and garlic), ', \"\", and \"sopa de mondongo\".\nSome meat products from Murcia are ' (black pudding), which is flavored with oregano, and ', made with ground beef. Among fish and seafood preparations are the \"dorada a la sal\", prawns from the Mar Menor, and baked octopus. Rice dishes are common and include ', ', \"paella Valenciana (rice with rabbit and snails)\", ', and '.\nConfectionery products include \"exploradores\" and \"\", typical in Murcia gastronomy and found in almost every pastry shop in Murcia. They are both sweet and savoury at the same time. \"Paparajotes\" is another dessert, made from lemon leaves.\nThis region also has wine appellations of origin, such as the wines from Jumilla, Bullas and Yecla.\nNavarra.\nThe gastronomy of Navarra has many similarities with Basque cuisine. Some of its best-known dishes are ' (Navarra-style trout), ', , and \"relleno\". There are also recipes such as Carlists eggs (omelet).\nSalted products are common and include \"chorizo de Pamplona\", \"bacalao al ajoarriero\", stuffing and sausage. The lamb and beef have, at present, designations of origin. Some dairy products are Roncal cheese, curd, and Idiazabal cheese. Typical alcoholic drinks include claret and \"pachar\u00e1n\".\nValencia.\nThe cuisine of Valencia has two aspects: rural and coastal. A popular and famous Valencia creation is \"paella\", a rice dish cooked in a circular pan and topped with vegetables and meats (originally rabbit and chicken). Dishes such as \"\", \"arr\u00f2s negre\", \"fideu\u00e1\", \" (arr\u00f2s al forn\" in the Valencian language\")\", and rice with beans and turnips are also common in the city.\nCoastal towns supply the region with fish, leading to popular dishes like \"\" (fish stew), typical of the Albufera.\nThe desserts in this region include coffee liqueur, chocolate Alicante, ', and \"horchata\", the last two being of Muslim origin. Notably, during Christmas, nougat is made in Alicante and Jijona. Another well-known dessert is ' (almonds wrapped in a thick layer of caramel).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27670", "revid": "50662793", "url": "https://en.wikipedia.org/wiki?curid=27670", "title": "Santiago de Compostela", "text": "Santiago de Compostela, simply Santiago, or Compostela, in the province of A Coru\u00f1a, is the capital of the autonomous community of Galicia, in northwestern Spain. The city has its origin in the shrine of Saint James the Great, now the Cathedral of Santiago de Compostela, as the destination of the Way of St. James, a leading Catholic pilgrimage route since the 9th century. In 1985, the city's Old Town was designated a UNESCO World Heritage Site.\nSantiago de Compostela has a very mild climate for its latitude with heavy winter rainfall courtesy of its relative proximity to the prevailing winds from Atlantic low-pressure systems.\nToponym.\nAccording to Richard A. Fletcher, scholars now agree that the origin of the name Compostela comes from the Latin \"compositum tella\", meaning a well-ordered burial ground, possibly referring to an ancient burial ground on the site of the Church of Santiago de Compostela that pre-dates the Christian building.\n is the local Galician evolution of Vulgar Latin \"Sanctus Iacobus\" \"Saint James\". According to folk etymology \"Compostela\" derives from the ('field of the star').\nCity.\nAccording to a medieval legend, the remains of the apostle James, son of Zebedee were brought to Galicia for burial, where they were lost. Eight hundred years later the light of a bright star guided a shepherd, Pelagius the Hermit, who was watching his flock at night to the burial site in Santiago de Compostela. This site was originally called Mount Libredon and its physical topography leads prevalent seaborne winds to clear the cloud deck immediately overhead. The shepherd quickly reported his discovery to the bishop of Iria, Theodemir. The bishop declared that the remains were those of the apostle James and immediately notified King Alfonso II in Oviedo. To honour St. James, the cathedral was built on the spot where his remains were said to have been found. The legend, which included numerous miraculous events, enabled the Catholic faithful to bolster support for their stronghold in northern Spain during the Christian crusades against the Moors, but also led to the growth and development of the city.\nAlong the western side of the \"Praza do Obradoiro\" is the elegant 18th-century Pazo de Raxoi, now the city hall. On the right from the cathedral steps is the Hostal dos Reis Cat\u00f3licos, founded in 1492 by the Catholic Monarchs, Isabella of Castille and Ferdinand II of Aragon, as a pilgrims' hospice (now a Parador). The Obradoiro fa\u00e7ade of the cathedral, the best known, is depicted on the Spanish euro coins of 1 cent, 2 cents, and 5 cents (\u20ac0.01, \u20ac0.02, and \u20ac0.05).\nSantiago is the site of the University of Santiago de Compostela, established in the early 16th century. The main campus can be seen best from an alcove in the large municipal park in the centre of the city.\nSantiago de Compostela has a substantial nightlife. Both in the new town ( in Galician, in Spanish or ) and the old town (, , trade-branded as \"zona monumental\"), a mix of middle-aged residents and younger students maintain a lively presence until the early hours of the morning. Radiating from the centre of the city, the historic cathedral is surrounded by paved granite streets, tucked away in the old town, and separated from the newer part of the city by the largest of many parks throughout the city, .\nSantiago gives its name to one of the four military orders of Spain: Santiago, Calatrava, Alc\u00e1ntara and Montesa.\nOne of the most important economic centres in Galicia, Santiago is the seat for organisations like Association for Equal and Fair Trade Pangaea.\nClimate.\nUnder the K\u00f6ppen climate classification, Santiago de Compostela has a temperate oceanic climate (\"Cfb\") with mild to warm and somewhat dry summers and mild, wet winters. The prevailing winds from the Atlantic and the surrounding mountains combine to give Santiago some of Spain's highest rainfall: about annually. The winters are mild, despite being far inland and at an altitude of frosts are only common in December, January and February, with an average of just 13 days per year. Snow is uncommon, with 2-3 snowy days per year. Temperatures above are very exceptional.\nAdministration.\nThe city is governed by a mayor\u2013council form of government. Following the 2023 Spanish local elections the mayor of Santiago is Goretti Sanmart\u00edn\n, of BNG.\nPopulation.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;The population of the city in 2019 was 96,260 inhabitants, while the metropolitan area reaches 178,695.\nIn 2010 there were 4,111 foreigners living in the city, representing 4.3% of the total population. The main nationalities are Brazilians (11%), Portuguese (8%) and Colombians (7%).\nBy language, according to 2008 data, 21.17% of the population always speak in Galician, 15% always speak in Spanish, 31% mostly in Galician and the 32.17% mostly in Spanish. According to a Xunta de Galicia 2010 study the 38.5% of the city primary and secondary education students had Galician as their mother tongue.\nHistory.\nThe area of Santiago de Compostela was a Roman cemetery by the 4th century and was occupied by the Suebi in the early 5th century, when they settled in Galicia and Portugal during the initial collapse of the Roman Empire. The area was later attributed to the bishopric of Iria Flavia in the 6th century, in the partition usually known as Parochiale Suevorum, ordered by King Theodemar. In 585, the settlement was annexed along with the rest of Suebi Kingdom by Leovigild as the sixth province of the Visigothic Kingdom.\nPossibly raided from 711 to 739 by the Arabs, the bishopric of Iria was incorporated into the Kingdom of Asturias c.\u2009750. At some point between 818 and 842, during the reign of Alfonso II of Asturias, bishop Theodemar of Iria (d. 847) claimed to have found some remains which were attributed to Saint James the Greater. This discovery was accepted in part because Pope Leo III and Charlemagne\u2014who had died in 814\u2014had acknowledged Asturias as a kingdom and Alfonso II as king, and had also crafted close political and ecclesiastic ties. Around the place of the discovery a new settlement and centre of pilgrimage emerged, which was known to the author Usuard in 865 and which was called \"Compostella\" by the 10th century.\nThe devotion to Saint James of Compostela was just one of many arising throughout northern Iberia during the 10th and 11th centuries, as rulers encouraged their own region-specific devotions, such as Saint Eulalia in Oviedo and Saint Aemilian in Castile. After the centre of Asturian political power moved from Oviedo to Le\u00f3n in 910, Compostela became more politically relevant, and several kings of Galicia and of Le\u00f3n were acclaimed by the Galician noblemen and crowned and anointed by the local bishop at the cathedral, among them Ordo\u00f1o IV in 958, Bermudo II in 982, and Alfonso VII in 1111, by which time Compostela had become capital of the Kingdom of Galicia. Later, 12th-century kings were also sepulchered in the cathedral, namely Fernando II and Alfonso IX, last of the Kings of Le\u00f3n and Galicia before both kingdoms were united with the Kingdom of Castile.\nDuring this same 10th century and in the first years of the 11th century Viking raiders tried to assault the town\u2014Galicia is known in the Nordic sagas as \"Jackobsland\" or \"Gallizaland\"\u2014and bishop Sisenand II, who was killed in battle against them in 968, ordered the construction of a walled fortress to protect the sacred place. In 997 Compostela was assaulted and partially destroyed by Ibn Abi Aamir (known as al-Mansur), Andalusian leader accompanied in his raid by Christian lords, who all received a share of the booty. However, the Andalusian commander showed no interest in the alleged relics of St James. In response to these challenges bishop Cresconio, in the mid-11th century, fortified the entire town, building walls and defensive towers.\nAccording to some authors, by the middle years of the 11th century the site had already become a pan-European \"place of peregrination\", while others maintain that the devotion to Saint James was before 11-12th centuries an essentially Galician affair, supported by Asturian and Leonese kings to win over faltering Galician loyalties. Santiago would become in the course of the following century a main Catholic shrine second only to Rome and Jerusalem. In the 12th century, under the impulse of bishop Diego Gelm\u00edrez, Compostela became an archbishopric, attracting a large and multinational population. Under the rule of this prelate, the townspeople rebelled, headed by the local council, beginning a secular tradition of confrontation by the people of the city\u2014who fought for self-government\u2014against the local bishop, the secular and jurisdictional lord of the city and of its fief, the semi-independent (\"land of Saint James\"). The culminating moment in this confrontation was reached in the 14th century, when the new prelate, the Frenchman B\u00e9renger de Landore, treacherously executed the counselors of the city in his castle of \"A Rocha Forte\" (\"the strong rock, castle\"), after inviting them for talks.\nSantiago de Compostela was captured and sacked by the French during the Napoleonic Wars; as a result, the remains attributed to the apostle were lost for near a century, hidden inside a cist in the crypts of the cathedral of the city.\nThe excavations conducted in the cathedral during the 19th and 20th centuries uncovered a Roman \"cella memoriae\" or martyrium, around which grew a small cemetery in Roman and Suevi times which was later abandoned. This \"martyrium\", which proves the existence of an old Christian holy place, has been sometimes attributed to Priscillian, although without further proof.\nEconomy.\nSantiago's economy, although still heavily dependent on public administration (i.e. being the headquarters of the autonomous government of Galicia), cultural tourism, industry, and higher education through its university, is becoming increasingly diversified. New industries such as timber transformation (FINSA), the automotive industry (UROVESA), and telecommunications and electronics (Blusens and Telev\u00e9s) have been established. Banco Gallego, a banking institution owned by Novacaixagalicia, has its headquarters in downtown \"r\u00faa do H\u00f3rreo\".\nTourism is very important thanks to the Way of St. James, particularly in Holy Compostelan Years (when the Feast of Saint James falls on a Sunday). Following the Xunta's considerable investment and hugely successful advertising campaign for the Holy Year of 1993, the number of pilgrims completing the route has been steadily rising. More than 272,000 pilgrims made the trip during the course of the Holy Year of 2010. Following 2010, the next Holy Year will not be for another 11 years when St James feast day again falls on a Sunday. Outside of Holy Years, the city still receives a remarkable number of pilgrims. In 2013, 215,880 people completed the pilgrimage. In 2014, there were 237,983 persons. In 2015, there were 262,513 persons and in 2016, there were 277,854 persons.\nEditorial Compostela owns daily newspaper \"El Correo Gallego\", a local TV, and a radio station. Galician-language online news portal \"Galicia Hoxe\" is also based in the city. Televisi\u00f3n de Galicia, the public broadcaster corporation of Galicia, has its headquarters in Santiago.\nWay of St. James.\nDuring medieval times, the Santiago de Compostela pilgrimage emerged as one of the most significant Christian journeys in Europe, attracting thousands of pilgrims seeking spiritual redemption and fulfillment. Believed to be the final resting place of Saint James the Apostle, the pilgrimage route traversed many countries and scenic locations. The pilgrimage not only fostered spiritual growth but also facilitated cultural exchange, as towns along the route thrived with the influx of visitors, leading to the construction of churches, and further development of the towns. This sacred journey symbolized a profound devotion to faith, enduring trials, and the hope of divine grace. A symbol of the Pilgrimage is the scallop shell, as seen in a sculpture, depicted below, in Santo Domingo de Silos, in which Jesus is shown as a pilgrim with a satchel that is embroidered with the scallop shell. The Scallop shell comes from a legend about St. James\u2019s arrival: he frightened a horse, scaring it into the sea, and the horse reemerged with the shell covering itself.\nSantiago de Compostela\u2019s pilgrimage, known as the Camino de Santiago, is one of the world's most significant and historical Christian pilgrimages. This sacred journey leads to the Cathedral of Santiago de Compostela in the Galicia region of northwest Spain, where the remains of Saint James the Apostle are believed to be buried. The pilgrimage dates back to the Middle Ages and continues to draw thousands of pilgrims annually from all corners of the globe. Participants embark on various routes, the most popular being the Camino Franc\u00e9s, traversing hundreds of kilometers on foot, by bicycle, or even on horseback. The journey is not just a physical challenge but also a profound spiritual and introspective experience, offering a sense of community, personal reflection, and fulfillment. Along the way, pilgrims pass through diverse landscapes and historic towns and encounter symbols of faith and support.\nThe legend that St. James found his way to the Iberian Peninsula and had preached there is one of a number of early traditions concerning the missionary activities and final resting places of the apostles of Jesus. Although the 1884 Bull of Pope Leo XIII \"Omnipotens Deus\" accepted the authenticity of the relics at Compostela, the Vatican remains uncommitted as to whether the relics are those of Saint James the Greater, while continuing to promote the more general benefits of pilgrimage to the site. Pope Benedict XVI undertook a ceremonial pilgrimage to the site on his visit to Spain in 2010.\nEstablishment of the shrine.\nThe 1,000-year-old pilgrimage to the shrine of St. James in the Santiago de Compostela Cathedral is known in English as the Way of St. James and in Spanish as the . Over 200,000 pilgrims travel to the city each year from points all over Europe and other parts of the world. The pilgrimage has been the subject of many books, television programmes, and films, notably Brian Sewell's \"The Naked Pilgrim\" produced for the British television channel Channel 5 and the Martin Sheen/Emilio Estevez collaboration \"The Way\".\nLegends.\nAccording to a tradition that can be traced back at least to the 12th century, when it was recorded in the \"Codex Calixtinus\", Saint James decided to return to the Holy Land after preaching in Galicia. There he was beheaded, but his disciples got his body to Jaffa, where they found a marvelous stone ship which miraculously conducted them and the apostle's body to Iria Flavia, back in Galicia. There, the disciples asked the local pagan queen \"Loba\" ('She-wolf') for permission to bury the body; she, annoyed, decided to deceive them, sending them to pick a pair of oxen she allegedly had by the \"Pico Sacro\", a local sacred mountain where a dragon dwelt, hoping that the dragon would kill the Christians, but as soon as the beast attacked the disciples, at the sight of the cross, the dragon exploded. Then the disciples marched to collect the oxen, which were actually wild bulls which the queen used to punish her enemies; but again, at the sight of the Christian's cross, the bulls calmed down, and after being subjected to a yoke they carried the apostle's body to the place where now Compostela is. The legend was again referred with minor changes by the Czech traveller Jaroslav Lev of Ro\u017emit\u00e1l, in the 15th century.\nThe relics were said to have been later rediscovered in the 9th century by a hermit named Pelagius, who after observing strange lights in a local forest went for help after the local bishop, Theodemar of Iria, in the west of Galicia. The legend affirms that Theodemar was then guided to the spot by a star, drawing upon a familiar myth-element, hence \"Compostela\" was given an etymology as a corruption of Campus Stellae, \"Field of Stars.\"\nIn the 15th century, the red banner which guided the Galician armies to battle, was still preserved in the Cathedral of Santiago de Compostela, in the centre Saint James riding a white horse and wearing a white cloak, sword in hand: The legend of the miraculous armed intervention of Saint James, disguised as a white knight to help the Christians when battling the Muslims, was a recurrent myth during the High Middle Ages.\nPre-Christian legends.\nAs the lowest-lying land on that stretch of coast, the city's site took on added significance. Legends supposed of Celtic origin made it the place where the souls of the dead gathered to follow the sun across the sea. Those unworthy of going to the Land of the Dead haunted Galicia as the \"Santa Compa\u00f1a\" or \"Estadea\".\nIn popular culture.\nSantiago de Compostela is featured prominently in the 1988 historical fiction novel \"Sharpe's Rifles\", by Bernard Cornwell, which takes place during the French Invasion of Galicia, January 1809, during the Napoleonic Wars.\nThe music video for \"Una Cerveza\", by R\u00e1faga, is set in the historic part of Santiago de Compostela.\nA pilgrimage to Santiago de Compostela provides the narrative framework of the Luis Bu\u00f1uel film La Voie lact\u00e9e (The Milky Way).\nA mystic pilgrimage was portrayed in the autobiography and romance The Pilgrimage (\"O Di\u00e1rio de um Mago\") of Brazilian writer Paulo Coelho, published in 1987.\nTransport.\nSantiago de Compostela is served by Santiago de Compostela Airport and a Renfe rail service.\nAirport.\nSantiago de Compostela Airport is the 2nd busiest airport in northern Spain after Bilbao Airport. The airport is located in the parish of Lavacolla, 12\u00a0km from the city center and handled 2,903,427 passengers in 2019.\nRailway.\nSantiago de Compostela railway station is linked to the Spanish High Speed Railway Network. Madrid is reached in 3 hours.\nPorto can also be reached in less than 5 hours changing to the Celta train in Vigo.\nOn 24 July 2013 there was a serious rail accident near the city in which 79 people died and at least 130 were injured when a train derailed on a bend as it approached Compostela station.\nInternational relations.\nTwin towns/Sister cities.\nSantiago de Compostela is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27672", "revid": "3492060", "url": "https://en.wikipedia.org/wiki?curid=27672", "title": "Sailing", "text": "Propulsion of a vehicle by wind power\nSailing employs the wind\u2014acting on sails, wingsails or kites\u2014to propel a craft on the surface of the \"water\" (sailing ship, sailboat, raft, windsurfer, or kitesurfer), on \"ice\" (iceboat) or on \"land\" (land yacht) over a chosen course, which is often part of a larger plan of navigation.\nFrom prehistory until the second half of the 19th century, sailing craft were the primary means of maritime trade and transportation; exploration across the seas and oceans was reliant on sail for anything other than the shortest distances. Naval power in this period used sail to varying degrees depending on the current technology, culminating in the gun-armed sailing warships of the Age of Sail. Sail was slowly replaced by steam as the method of propulsion for ships over the latter part of the 19th century \u2013 seeing a gradual improvement in the technology of steam through a number of developmental steps. Steam allowed scheduled services that ran at higher average speeds than sailing vessels. Large improvements in fuel economy allowed steam to progressively outcompete sail in, ultimately, all commercial situations, giving ship-owning investors a better return on capital.\nIn the 21st century, most sailing represents a form of recreation or sport. Recreational sailing or yachting can be divided into racing and cruising. Cruising can include extended offshore and ocean-crossing trips, coastal sailing within sight of land, and daysailing.\nSailing relies on the physics of sails as they derive power from the wind, generating both lift and drag. On a given course, the sails are set to an angle that optimizes the development of wind power, as determined by the apparent wind, which is the wind as sensed from a moving vessel. The forces transmitted via the sails are resisted by forces from the hull, keel, and rudder of a sailing craft, by forces from skate runners of an iceboat, or by forces from wheels of a land sailing craft which are steering the course. This combination of forces means that it is possible to sail an upwind course as well as downwind. The course with respect to the true wind direction (as would be indicated by a stationary flag) is called a point of sail. Conventional sailing craft cannot derive wind power on a course with a point of sail that is too close into the wind.\nHistory.\nThroughout history, sailing was a key form of propulsion that allowed for greater mobility than travel over land. This greater mobility increased capacity for exploration, trade, transport, warfare, and fishing, especially when compared to overland options.ch.2\nUntil the significant improvements in land transportation that occurred during the 19th century, if water transport was an option, it was faster, cheaper and safer than making the same journey by land. This applied equally to sea crossings, coastal voyages and use of rivers and lakes. Examples of the consequences of this include the large grain trade in the Mediterranean during the classical period. Cities such as Rome were totally reliant on the delivery by sailing ships of the large amounts of grain needed. It has been estimated that it cost less for a sailing ship of the Roman Empire to carry grain the length of the Mediterranean than to move the same amount 15 miles by road. Rome consumed about 150,000 tons of Egyptian grain each year over the first three centuries AD.ch. 2\nA similar but more recent trade, in coal, was from the mines situated close to the River Tyne to London \u2013 which was already being carried out in the 14th century and grew as the city increased in size. In 1795, 4,395 cargoes of coal were delivered to London. This would have needed a fleet of about 500 sailing colliers (making 8 or 9 trips a year). This quantity had doubled by 1839. (The first steam-powered collier was not launched until 1852 and sailing colliers continued working into the 20th century.) \nExploration and research.\nThe earliest image suggesting the use of sail on a boat may be on a piece of pottery from Mesopotamia, dated to the 6th millennium BCE. The image is thought to show a bipod mast mounted on the hull of a reed boat \u2013 no sail is depicted. The earliest representation of a sail, from Egypt, is dated to circa 3100 BCE. The Nile is considered a suitable place for early use of sail for propulsion. This is because the river's current flows from south to north, whilst the prevailing wind direction is north to south. Therefore, a boat of that time could use the current to go north \u2013 an unobstructed trip of 750 miles \u2013 and sail to make the return trip.11 Evidence of early sailors has also been found in other locations, such as Kuwait, Turkey, Syria, Minoa, Bahrain, and India, among others.\nAustronesian peoples used sails from some time before 2000 BCE. Their expansion from what is now Southern China and Taiwan started in 3000 BCE. Their technology came to include outriggers, catamarans, and crab claw sails, which enabled the Austronesian Expansion at around 3000 to 1500 BCE into the islands of Maritime Southeast Asia, and thence to Micronesia, Island Melanesia, Polynesia, and Madagascar. Since there is no commonality between the boat technology of China and the Austronesians, these distinctive characteristics must have been developed at or some time after the beginning of the expansion. They traveled vast distances of open ocean in outrigger canoes using navigation methods such as stick charts. The windward sailing capability of Austronesian boats allowed a strategy of sailing to windward on a voyage of exploration, with a return downwind either to report a discovery or if no land was found. This was well suited to the prevailing winds as Pacific islands were steadily colonized.\nBy the time of the Age of Discovery\u2014starting in the 15th century\u2014square-rigged, multi-masted vessels were the norm and were guided by navigation techniques that included the magnetic compass and making sightings of the sun and stars that allowed transoceanic voyages.\nDuring the Age of Discovery, sailing ships figured in European voyages around Africa to China and Japan; and across the Atlantic Ocean to North and South America. Later, sailing ships ventured into the Arctic to explore northern sea routes and assess natural resources. In the 18th and 19th centuries sailing vessels made Hydrographic surveys to develop charts for navigation and, at times, carried scientists aboard as with the voyages of James Cook and the Second voyage of HMS \"Beagle\" with naturalist Charles Darwin.\nCommerce.\nIn the early 1800s, fast blockade-running schooners and brigantines\u2014Baltimore Clippers\u2014evolved into three-masted, typically ship-rigged sailing vessels with fine lines that enhanced speed, but lessened capacity for high-value cargo, like tea from China. Masts were as high as and were able to achieve speeds of , allowing for passages of up to per 24 hours. Clippers yielded to bulkier, slower vessels, which became economically competitive in the mid 19th century. Sail plans with just fore-and-aft sails (schooners), or a mixture of the two (brigantines, barques and barquentines) emerged. Coastal top-sail schooners with a crew as small as two managing the sail handling became an efficient way to carry bulk cargo, since only the fore-sails required tending while tacking and steam-driven machinery was often available for raising the sails and the anchor.\nIron-hulled sailing ships represented the final evolution of sailing ships at the end of the Age of Sail. They were built to carry bulk cargo for long distances in the nineteenth and early twentieth centuries. They were the largest of merchant sailing ships, with three to five masts and square sails, as well as other sail plans. They carried bulk cargoes between continents. Iron-hulled sailing ships were mainly built from the 1870s to 1900, when steamships began to outpace them economically because of their ability to keep a schedule regardless of the wind. Steel hulls also replaced iron hulls at around the same time. Even into the twentieth century, sailing ships could hold their own on transoceanic voyages such as Australia to Europe, since they did not require bunkerage for coal nor fresh water for steam, and they were faster than the early steamers, which usually could barely make . Ultimately, the steamships' independence from the wind and their ability to take shorter routes, passing through the Suez and Panama Canals, made sailing ships uneconomical.\nNaval power.\nUntil the general adoption of carvel-built ships that relied on an internal skeleton structure to bear the weight of the ship and for gun ports to be cut in the side, sailing ships were just vehicles for delivering fighters to the enemy for engagement. Early Phoenician, Greek, Roman galleys would ram each other, then pour onto the decks of the opposing force and continue the fight by hand, meaning that these galleys required speed and maneuverability. This need for speed translated into longer ships with multiple rows of oars along the sides, known as biremes and triremes. Typically, the sailing ships during this time period were the merchant ships.\nBy 1500, Gun ports allowed sailing vessels to sail alongside an enemy vessel and fire a broadside of multiple cannon. This development allowed for naval fleets to array themselves into a line of battle, whereby, warships would maintain their place in the line to engage the enemy in a parallel or perpendicular line.\nModern applications.\nWhile the use of sailing vessels for commerce or naval power has been supplanted with engine-driven vessels, there continue to be commercial operations that take passengers on sailing cruises. Modern navies also employ sailing vessels to train cadets in seamanship. Recreation or sport accounts for the bulk of sailing in modern boats.\nRecreation.\nRecreational sailing can be divided into two categories, day-sailing, where one gets off the boat for the night, and cruising, where one stays aboard.\nDay-sailing primarily affords experiencing the pleasure of sailing a boat. No destination is required. It is an opportunity to share the experience with others. A variety of boats with no overnight accommodations, ranging in size from to over , may be regarded as day sailors.\nCruising on a sailing yacht may be either near-shore or passage-making out of sight of land and entails the use of sailboats that support sustained overnight use. Coastal cruising grounds include areas of the Mediterranean and Black Seas, Northern Europe, Western Europe and islands of the North Atlantic, West Africa and the islands of the South Atlantic, the Caribbean, and regions of North and Central America. Passage-making under sail occurs on routes through oceans all over the world. Circular routes exist between the Americas and Europe, and between South Africa and South America. There are many routes from the Americas, Australia, New Zealand, and Asia to island destinations in the South Pacific. Some cruisers circumnavigate the globe.\nSport.\nSailing as a sport is organized on a hierarchical basis, starting at the yacht club level and reaching up into national and international federations; it may entail racing yachts, sailing dinghies, or other small, open sailing craft, including iceboats and land yachts. Sailboat racing is governed by World Sailing with most racing formats using the Racing Rules of Sailing. It entails a variety of different disciplines, including:\nNavigation.\nPoint of sail.\nA sailing craft's ability to derive power from the wind depends on the point of sail it is on\u2014the direction of travel under sail in relation to the true wind direction over the surface. The principal points of sail roughly correspond to 45\u00b0 segments of a circle, starting with 0\u00b0 directly into the wind. For many sailing craft, the arc spanning 45\u00b0 on either side of the wind is a \"no-go\" zone, where a sail is unable to mobilize power from the wind. Sailing on a course as close to the wind as possible\u2014approximately 45\u00b0\u2014is termed \"close-hauled\". At 90\u00b0 off the wind, a craft is on a \"beam reach\". At 135\u00b0 off the wind, a craft is on a \"broad reach\". At 180\u00b0 off the wind (sailing in the same direction as the wind), a craft is \"running downwind\".\nIn points of sail that range from close-hauled to a broad reach, sails act substantially like a wing, with lift predominantly propelling the craft. In points of sail from a broad reach to down wind, sails act substantially like a parachute, with drag predominantly propelling the craft. For craft with little forward resistance, such as ice boats and land yachts, this transition occurs further off the wind than for sailboats and sailing ships.\nWind direction for points of sail always refers to the \"true wind\"\u2014the wind felt by a stationary observer. The \"apparent wind\"\u2014the wind felt by an observer on a moving sailing craft\u2014determines the motive power for sailing craft.\nThe waves give an indication of the \"true wind\" direction. The flag gives an indication of \"apparent wind\" direction.\nEffect on apparent wind.\nTrue wind velocity (VT) combines with the sailing craft's velocity (VB) to give the \"apparent wind velocity\" (VA), the air velocity experienced by instrumentation or crew on a moving sailing craft. Apparent wind velocity provides the motive power for the sails on any given point of sail. It varies from being the true wind velocity of a stopped craft in irons in the no-go zone, to being faster than the true wind speed as the sailing craft's velocity adds to the true windspeed on a reach. It diminishes towards zero for a craft sailing dead downwind.\nSailing craft A is close-hauled. Sailing craft B is on a beam reach. Sailing craft C is on a broad reach.Boat velocity (in black) generates an equal and opposite apparent wind component (not shown), which combines with the true wind to become apparent wind.\nThe speed of sailboats through the water is limited by the resistance that results from hull drag in the water. Ice boats typically have the least resistance to forward motion of any sailing craft. Consequently, a sailboat experiences a wider range of apparent wind angles than does an ice boat, whose speed is typically great enough to have the apparent wind coming from a few degrees to one side of its course, necessitating sailing with the sail sheeted in for most points of sail. On conventional sailboats, the sails are set to create lift for those points of sail where it's possible to align the leading edge of the sail with the apparent wind.\nFor a sailboat, point of sail affects lateral force significantly. The higher the boat points to the wind under sail, the stronger the lateral force, which requires resistance from a keel or other underwater foils, including daggerboard, centerboard, skeg and rudder. Lateral force also induces heeling in a sailboat, which requires resistance by weight of ballast from the crew or the boat itself and by the shape of the boat, especially with a catamaran. As the boat points off the wind, lateral force and the forces required to resist it become less important.\nOn ice boats, lateral forces are countered by the lateral resistance of the blades on ice and their distance apart, which generally prevents heeling.\nCourse under sail.\nWind and currents are important factors to plan on for both offshore and inshore sailing. Predicting the availability, strength and direction of the wind is key to using its power along the desired course. Ocean currents, tides and river currents may deflect a sailing vessel from its desired course.\nIf the desired course is within the no-go zone, then the sailing craft must follow a zig-zag route into the wind to reach its waypoint or destination. Downwind, certain high-performance sailing craft can reach the destination more quickly by following a zig-zag route on a series of broad reaches.\nNegotiating obstructions or a channel may also require a change of direction with respect to the wind, necessitating changing of tack with the wind on the opposite side of the craft, from before.\nChanging tack is called \"tacking\" when the wind crosses over the bow of the craft as it turns and \"jibing\" (or \"gybing\") if the wind passes over the stern.\nUpwind.\nA sailing craft can sail on a course anywhere outside of its no-go zone. If the next waypoint or destination is within the arc defined by the no-go zone from the craft's current position, then it must perform a series of tacking maneuvers to get there on a zigzag route, called \"beating to windward\". The progress along that route is called the \"course made good\"; the speed between the starting and ending points of the route is called the \"speed made good\" and is calculated by the distance between the two points, divided by the travel time. The limiting line to the waypoint that allows the sailing vessel to leave it to leeward is called the \"layline\". Whereas some Bermuda-rigged sailing yachts can sail as close as 30\u00b0 to the wind, most 20th-Century square riggers are limited to 60\u00b0 off the wind. Fore-and-aft rigs are designed to operate with the wind on either side, whereas square rigs and kites are designed to have the wind come from one side of the sail only.\nBecause the lateral wind forces are highest when sailing close-hauled, the resisting water forces around the vessel's keel, centerboard, rudder and other foils must also be highest in order to limit sideways motion or leeway. Ice boats and land yachts minimize lateral motion with resistance from their blades or wheels.\nChanging tack by tacking.\n\"Tacking\" or \"coming about\" is a maneuver by which a sailing craft turns its bow into and through the wind (referred to as \"the eye of the wind\") so that the apparent wind changes from one side to the other, allowing progress on the opposite tack. The type of sailing rig dictates the procedures and constraints on achieving a tacking maneuver. Fore-and-aft rigs allow their sails to hang limp as they tack; square rigs must present the full frontal area of the sail to the wind, when changing from side to side; and windsurfers have flexibly pivoting and fully rotating masts that get flipped from side to side.\nDownwind.\nA sailing craft can travel directly downwind only at a speed that is less than the wind speed. However, some sailing craft such as iceboats, sand yachts, and some high-performance sailboats can achieve a higher downwind velocity made good by traveling on a series of broad reaches, punctuated by jibes in between. It was explored by sailing vessels starting in 1975 and now extends to high-performance skiffs, catamarans and foiling sailboats.\nNavigating a channel or a downwind course among obstructions may necessitate changes in direction that require a change of tack, accomplished with a jibe.\nChanging tack by jibing.\n\"Jibing\" or \"gybing\" is a sailing maneuver by which a sailing craft turns its stern past the eye of the wind so that the apparent wind changes from one side to the other, allowing progress on the opposite tack. This maneuver can be done on smaller boats by pulling the tiller towards yourself (the opposite side of the sail). As with tacking, the type of sailing rig dictates the procedures and constraints for jibing. Fore-and-aft sails with booms, gaffs or sprits are unstable when the free end points into the eye of the wind and must be controlled to avoid a violent change to the other side; square rigs as they present the full area of the sail to the wind from the rear experience little change of operation from one tack to the other; and windsurfers again have flexibly pivoting and fully rotating masts that get flipped from side to side.\nWind and currents.\nWinds and oceanic currents are both the result of the sun powering their respective fluid media. Wind powers the sailing craft and the ocean bears the craft on its course, as currents may alter the course of a sailing vessel on the ocean or a river.\nTrimming.\nTrimming refers to adjusting the lines that control sails, including the sheets that control angle of the sails with respect to the wind, the halyards that raise and tighten the sail, and to adjusting the hull's resistance to heeling, yawing or progress through the water.\nSails.\nIn their most developed version, square sails are controlled by two each of: sheets, braces, clewlines, and reef tackles, plus four buntlines, each of which may be controlled by a crew member as the sail is adjusted. Towards the end of the Age of Sail, steam-powered machinery reduced the number of crew required to trim sail.\nAdjustment of the angle of a fore-and-aft sail with respect to the apparent wind is controlled with a line, called a \"sheet\". On points of sail between close-hauled and a broad reach, the goal is typically to create flow along the sail to maximize power through lift. Streamers placed on the surface of the sail, called tell-tales, indicate whether that flow is smooth or turbulent. Smooth flow on both sides indicates proper trim. A jib and mainsail are typically configured to be adjusted to create a smooth laminar flow, leading from one to the other in what is called the \"slot effect\".\nOn downwind points of sail, power is achieved primarily with the wind pushing on the sail, as indicated by drooping tell-tales. Spinnakers are light-weight, large-area, highly curved sails that are adapted to sailing off the wind.\nIn addition to using the sheets to adjust the angle with respect to the apparent wind, other lines control the shape of the sail, notably the outhaul, halyard, boom vang and backstay. These control the curvature that is appropriate to the windspeed, the higher the wind, the flatter the sail. When the wind strength is greater than these adjustments can accommodate to prevent overpowering the sailing craft, sail area is reduced through reefing, substituting a smaller sail or by other means.\nReducing sail.\nReducing sail on square-rigged ships could be accomplished by exposing less of each sail, by tying it off higher up with reefing points. Additionally, as winds get stronger, sails can be furled or removed from the spars, entirely until the vessel is surviving hurricane-force winds under \"bare poles\".\nOn fore-and-aft rigged vessels, reducing sail may furling the jib and by reefing or partially lowering the mainsail, that is reducing the area of a sail without actually changing it for a smaller sail. This results both in a reduced sail area but also in a lower centre of effort from the sails, reducing the heeling moment and keeping the boat more upright.\nThere are three common methods of reefing the mainsail:\nHull.\nHull trim has three aspects, each tied to an axis of rotation, they are controlling:\nEach is a reaction to forces on sails and is achieved either by weight distribution or by management of the center of force of the underwater foils (keel, daggerboard, etc.), compared with the center of force on the sails.\nHeeling.\nA sailing vessel heels when the boat leans over to the side in reaction to wind forces on the sails.\nA sailing vessel's form stability (derived from the shape of the hull and the position of the center of gravity) is the starting point for resisting heeling. Catamarans and iceboats have a wide stance that makes them resistant to heeling. Additional measures for trimming a sailing craft to control heeling include:\nHelm force.\nThe alignment of center of force of the sails with center of resistance of the hull and its appendices controls whether the craft will track straight with little steering input, or whether correction needs to be made to hold it away from turning into the wind (a weather helm) or turning away from the wind (a lee helm). A center of force behind the center of resistance causes a weather helm. The center of force ahead of the center of resistance causes a lee helm. When the two are closely aligned, the helm is neutral and requires little input to maintain course.\nHull drag.\nFore-and-aft weight distribution changes the cross-section of a vessel in the water. Small sailing craft are sensitive to crew placement. They are usually designed to have the crew stationed midships to minimize hull drag in the water.\nOther aspects of seamanship.\nSeamanship encompasses all aspects of taking a sailing vessel in and out of port, navigating it to its destination, and securing it at anchor or alongside a dock. Important aspects of seamanship include employing a common language aboard a sailing craft and the management of lines that control the sails and rigging.\nNautical terms.\nNautical terms for elements of a vessel: starboard (right-hand side), port or larboard (left-hand side), forward or fore (frontward), aft or abaft (rearward), bow (forward part of the hull), stern (aft part of the hull), beam (the widest part). Spars, supporting sails, include masts, booms, yards, gaffs and poles. Moveable lines that control sails or other equipment are known collectively as a vessel's running rigging. Lines that raise sails are called \"halyards\" while those that strike them are called \"downhauls\". Lines that adjust (trim) the sails are called \"sheets\". These are often referred to using the name of the sail they control (such as \"main sheet\" or \"jib sheet\"). \"Guys\" are used to control the ends of other spars such as spinnaker poles. Lines used to tie a boat up when alongside are called \"docklines\", \"docking cables\" or \"mooring warps\". A \"rode\" is what attaches an anchored boat to its anchor. Other than starboard and port, the sides of the boat are defined by their relationship to the wind. The terms to describe the two sides are Windward and leeward. The windward side of the boat is the side that is upwind while the leeward side is the side that is downwind.\nManagement of lines.\nThe following knots are commonly used to handle ropes and lines on sailing craft:\nLines and halyards are typically coiled neatly for stowage and reuse.\nSail physics.\nThe physics of sailing arises from a balance of forces between the wind powering the sailing craft as it passes over its sails and the resistance by the sailing craft against being blown off course, which is provided in the water by the keel, rudder, underwater foils and other elements of the underbody of a sailboat, on ice by the runners of an iceboat, or on land by the wheels of a sail-powered land vehicle.\nForces on sails depend on wind speed and direction and the speed and direction of the craft. The speed of the craft at a given point of sail contributes to the \"apparent wind\"\u2014the wind speed and direction as measured on the moving craft. The apparent wind on the sail creates a total aerodynamic force, which may be resolved into drag\u2014the force component in the direction of the apparent wind\u2014and lift\u2014the force component normal (90\u00b0) to the apparent wind. Depending on the alignment of the sail with the apparent wind (\"angle of attack\"), lift or drag may be the predominant propulsive component. Depending on the angle of attack of a set of sails with respect to the apparent wind, each sail is providing motive force to the sailing craft either from lift-dominant attached flow or drag-dominant separated flow. Additionally, sails may interact with one another to create forces that are different from the sum of the individual contributions of each sail, when used alone.\nApparent wind velocity.\nThe term \"velocity\" refers both to speed and direction. As applied to wind, \"apparent wind velocity\" (VA) is the air velocity acting upon the leading edge of the most forward sail or as experienced by instrumentation or crew on a moving sailing craft. In nautical terminology, wind speeds are normally expressed in knots and wind angles in degrees. All sailing craft reach a constant \"forward velocity\" (VB) for a given \"true wind velocity\" (VT) and \"point of sail\". The craft's point of sail affects its velocity for a given true wind velocity. Conventional sailing craft cannot derive power from the wind in a \"no-go\" zone that is approximately 40\u00b0 to 50\u00b0 away from the true wind, depending on the craft. Likewise, the directly downwind speed of all conventional sailing craft is limited to the true wind speed. As a sailboat sails further from the wind, the apparent wind becomes smaller and the lateral component becomes less; boat speed is highest on the beam reach. To act like an airfoil, the sail on a sailboat is sheeted further out as the course is further off the wind. As an iceboat sails further from the wind, the apparent wind increases slightly and the boat speed is highest on the broad reach. In order to act like an airfoil, the sail on an iceboat is sheeted in for all three points of sail.\nLift and drag on sails.\n\"Lift\" on a sail, acting as an airfoil, occurs in a direction \"perpendicular\" to the incident airstream (the apparent wind velocity for the headsail) and is a result of pressure differences between the windward and leeward surfaces and depends on the angle of attack, sail shape, air density, and speed of the apparent wind. The lift force results from the average pressure on the windward surface of the sail being higher than the average pressure on the leeward side. These pressure differences arise in conjunction with the curved airflow. As air follows a curved path along the windward side of a sail, there is a pressure gradient perpendicular to the flow direction with higher pressure on the outside of the curve and lower pressure on the inside. To generate lift, a sail must present an \"angle of attack\" between the chord line of the sail and the apparent wind velocity. The angle of attack is a function of both the craft's point of sail and how the sail is adjusted with respect to the apparent wind.\nAs the lift generated by a sail increases, so does lift-induced drag, which together with parasitic drag constitute total \"drag\", which acts in a direction \"parallel\" to the incident airstream. This occurs as the angle of attack increases with sail trim or change of course and causes the lift coefficient to increase up to the point of aerodynamic stall along with the lift-induced drag coefficient. At the onset of stall, lift is abruptly decreased, as is lift-induced drag. Sails with the apparent wind behind them (especially going downwind) operate in a stalled condition.\nLift and drag are components of the total aerodynamic force on sail, which are resisted by forces in the water (for a boat) or on the traveled surface (for an iceboat or land sailing craft). Sails act in two basic modes; under the \"lift-predominant\" mode, the sail behaves in a manner analogous to a \"wing\" with airflow attached to both surfaces; under the \"drag-predominant\" mode, the sail acts in a manner analogous to a \"parachute\" with airflow in detached flow, eddying around the sail.\nLift predominance (wing mode).\nSails allow progress of a sailing craft to windward, thanks to their ability to generate lift (and the craft's ability to resist the lateral forces that result). Each sail configuration has a characteristic coefficient of lift and attendant coefficient of drag, which can be determined experimentally and calculated theoretically. Sailing craft orient their sails with a favorable angle of attack between the entry point of the sail and the apparent wind even as their course changes. The ability to generate lift is limited by sailing too close to the wind when no effective angle of attack is available to generate lift (causing luffing) and sailing sufficiently off the wind that the sail cannot be oriented at a favorable angle of attack to prevent the sail from stalling with flow separation.\nDrag predominance (parachute mode).\nWhen sailing craft are on a course where the angle between the sail and the apparent wind (the angle of attack) exceeds the point of maximum lift, separation of flow occurs. Drag increases and lift decreases with increasing angle of attack as the separation becomes progressively pronounced until the sail is perpendicular to the apparent wind, when lift becomes negligible and drag predominates. In addition to the sails used upwind, spinnakers provide area and curvature appropriate for sailing with separated flow on downwind points of sail, analogous to parachutes, which provide both lift and drag.\nWind variation with height and time.\nWind speed increases with height above the surface; at the same time, wind speed may vary over short periods of time as gusts.\nWind shear affects sailing craft in motion by presenting a different wind speed and direction at different heights along the mast. Wind shear occurs because of friction above a water surface slowing the flow of air. The ratio of wind at the surface to wind at a height above the surface varies by a power law with an exponent of 0.11-0.13 over the ocean. This means that a wind at 3 m above the water would be approximately at above the water. In hurricane-force winds with at the surface the speed at would be This suggests that sails that reach higher above the surface can be subject to stronger wind forces that move the centre of effort on them higher above the surface and increase the heeling moment. Additionally, apparent wind direction moves aft with height above water, which may necessitate a corresponding twist in the shape of the sail to achieve attached flow with height.\nGusts may be predicted by the same value that serves as an exponent for wind shear, serving as a gust factor. So, one can expect gusts to be about 1.5 times stronger than the prevailing wind speed (a 10-knot wind might gust up to 15 knots). This, combined with changes in wind direction suggest the degree to which a sailing craft must adjust sail angle to wind gusts on a given course.\nHull physics.\nWaterborne sailing craft rely on the design of the hull and keel to provide minimal forward drag in opposition to the sails' propulsive power and maximum resistance to the sails' lateral forces. In modern sailboats, drag is minimized by control of the hull's shape (blunt or fine), appendages, and slipperiness. The keel or other underwater foils provide the lateral resistance to forces on the sails. Heeling increases both drag and the ability of the boat to track along its desired course. Wave generation for a displacement hull is another important limitation on boat speed.\nDrag.\nDrag from its form is described by a prismatic coefficient, Cp = displaced volume of the vessel divided by waterline length times maximum displaced section area\u2014the maximum value of Cp = 1.0 being for a constant displace cross section area, as would be found on a barge. For modern sailboats, values of 0.53 \u2264 Cp \u2264 0.6 are likely because of the tapered shape of the submerged hull towards both ends. Reducing interior volume allows creating a finer hull with less drag. Because a keel or other underwater foil produces lift, it also produces drag, which increases as the boat heels. Wetted area of the hull affects total the amount of friction between the water and the hull's surface, creating another component of drag.\nLateral resistance.\nSailboats use some sort of underwater foil to generate lift that maintains the forward direction of the boat under sail. Whereas sails operate at angles of attack between 10\u00b0 and 90\u00b0 incident to the wind, underwater foils operate at angles of attack between 0\u00b0 and 10\u00b0 incident to the water passing by. Neither their angle of attack nor surface is adjustable (except for moveable foils) and they are never intentionally stalled, while making way through the water. Heeling the vessel away from perpendicular into the water significantly degrades the boat's ability to point into the wind.\nHull speed and beyond.\nHull speed is the speed at which the wavelength of a vessel's bow wave is equal to its waterline length and is proportional to the square root of the vessel's length at the waterline. Applying more power does not significantly increase the speed of a displacement vessel beyond hull speed. This is because the vessel is climbing up an increasingly steep bow wave with the addition of power without the wave propagating forward faster.\nPlaning and foiling vessels are not limited by hull speed, as they rise out of the water without building a bow wave with the application of power. Long narrow hulls, such as those of catamarans, surpass hull speed by piercing through the bow wave. Hull speed does not apply to sailing craft on ice runners or wheels because they do not displace water.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27674", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=27674", "title": "Slashdotted", "text": ""}
{"id": "27675", "revid": "46051904", "url": "https://en.wikipedia.org/wiki?curid=27675", "title": "Simple Mail Transfer Protocol", "text": "Internet protocol used for relaying e-mails\nThe Simple Mail Transfer Protocol (SMTP) is an Internet standard communication protocol for electronic mail transmission. Mail servers and other message transfer agents use SMTP to send and receive mail messages. User-level email clients typically use SMTP only for sending messages to a mail server for relaying, and typically submit outgoing email to the mail server on port 465 or 587 per . For retrieving messages, IMAP (which replaced the older POP3) is standard, but proprietary servers also often implement proprietary protocols, e.g., Exchange ActiveSync.\nSMTP's origins began in 1980, building on concepts implemented on the ARPANET since 1971. It has been updated, modified and extended multiple times. The protocol version in common use today has extensible structure with various extensions for authentication, encryption, binary data transfer, and internationalized email addresses. SMTP servers commonly use the Transmission Control Protocol on port number 25 (between servers) and 587 (for submission from authenticated clients), both with or without encryption, and 465 with encryption for submission.\nHistory.\nPredecessors to SMTP.\nVarious forms of one-to-one electronic messaging were used in the 1960s. Users communicated using systems developed for specific mainframe computers. As more computers were interconnected, especially in the U.S. Government's ARPANET, standards were developed to permit exchange of messages between different operating systems. \nMail on the ARPANET traces its roots to 1971: the Mail Box Protocol, which was not implemented, but is discussed in ; and the SNDMSG program, which Ray Tomlinson of BBN adapted that year to send messages across two computers on the ARPANET. A further proposal for a Mail Protocol was made in RFC 524 in June 1973, which was not implemented.\nThe use of the File Transfer Protocol (FTP) for \"network mail\" on the ARPANET was proposed in RFC 469 in March 1973. Through RFC 561, RFC 680, RFC 724, and finally RFC 733 in November 1977, a standardized framework for \"electronic mail\" using FTP mail servers on was developed.\nSMTP grew out of these standards developed during the 1970s. Ray Tomlinson discussed network mail among the International Network Working Group in \"INWG Protocol note 2\", written in September 1974. INWG discussed protocols for electronic mail in 1979, which was referenced by Jon Postel in his early work on Internet email. Postel first proposed an Internet Message Protocol in 1979 as part of the Internet Experiment Note (IEN) series.\nOriginal SMTP.\nIn 1980, Postel and Suzanne Sluizer published which proposed the Mail Transfer Protocol as a replacement for the use of the FTP for mail. of May 1981 removed all references to FTP and allocated port 57 for TCP and UDP, an allocation that has since been removed by IANA. In November 1981, Postel published \"Simple Mail Transfer Protocol\".\nThe SMTP standard was developed around the same time as Usenet, a one-to-many communication network with some similarities.\nSMTP became widely used in the early 1980s. At the time, it was a complement to the Unix to Unix Copy Program (UUCP), which was better suited for handling email transfers between machines that were intermittently connected. SMTP, on the other hand, works best when both the sending and receiving machines are connected to the network all the time. Both used a store and forward mechanism and are examples of push technology. Though Usenet's newsgroups were still propagated with UUCP between servers, UUCP as a mail transport has virtually disappeared along with the \"bang paths\" it used as message routing headers.\nSendmail, released with 4.1cBSD in 1983, was one of the first mail transfer agents (MTA) to implement SMTP. Over time, as BSD Unix became the most popular operating system on the Internet, Sendmail became the most common mail transfer agent.\nThe original SMTP protocol supported only unauthenticated unencrypted 7-bit ASCII text communications, susceptible to trivial man-in-the-middle attack, spoofing, and spamming, and requiring any binary data to be encoded to readable text before transmission. Due to absence of a proper authentication mechanism, by design every SMTP server was an open mail relay. The Internet Mail Consortium (IMC) reported that 55% of mail servers were open relays in 1998, but less than 1% in 2002. Because of spam concerns most email providers blocklist open relays, making original SMTP essentially impractical for general use on the Internet.\nModern SMTP.\nIn November 1995, defined Extended Simple Mail Transfer Protocol (ESMTP), which established a general structure for all existing and future extensions which aimed to add-in the features missing from the original SMTP. ESMTP defines consistent and manageable means by which ESMTP clients and servers can be identified and servers can indicate supported extensions. \nMessage submission () and SMTP-AUTH () were introduced in 1998 and 1999, both describing new trends in email delivery. Originally, SMTP servers were typically internal to an organization, receiving mail for the organization \"from the outside\", and relaying messages from the organization \"to the outside\". But as time went on, SMTP servers (mail transfer agents), in practice, were expanding their roles to become message submission agents for mail user agents, some of which were now relaying mail \"from the outside\" of an organization. (e.g. a company executive wishes to send email while on a trip using the corporate SMTP server.) This issue, a consequence of the rapid expansion and popularity of the World Wide Web, meant that SMTP had to include specific rules and methods for relaying mail and authenticating users to prevent abuses such as relaying of unsolicited email (spam). Work on message submission () was originally started because popular mail servers would often rewrite mail in an attempt to fix problems in it, for example, adding a domain name to an unqualified address. This behavior is helpful when the message being fixed is an initial submission, but dangerous and harmful when the message originated elsewhere and is being relayed. Cleanly separating mail into submission and relay was seen as a way to permit and encourage rewriting submissions while prohibiting rewriting relay. As spam became more prevalent, it was also seen as a way to provide authorization for mail being sent out from an organization, as well as traceability. This separation of relay and submission quickly became a foundation for modern email security practices.\nAs this protocol started out purely ASCII text-based, it did not deal well with binary files, or characters in many non-English languages. Standards such as Multipurpose Internet Mail Extensions (MIME) were developed to encode binary files for transfer through SMTP. Mail transfer agents (MTAs) developed after Sendmail also tended to be implemented 8-bit clean, so that the alternate \"just send eight\" strategy could be used to transmit arbitrary text data (in any 8-bit ASCII-like character encoding) via SMTP. Mojibake was still a problem due to differing character set mappings between vendors, although the email addresses themselves still allowed only ASCII. 8-bit-clean MTAs today tend to support the 8BITMIME extension, permitting some binary files to be transmitted almost as easily as plain text (limits on line length and permitted octet values still apply, so that MIME encoding is needed for most non-text data and some text formats). In 2012, the codice_1 extension was created to support UTF-8 text, allowing international content and addresses in non-Latin scripts like Cyrillic or Chinese.\nMany people contributed to the core SMTP specifications, among them Jon Postel, Eric Allman, Dave Crocker, Ned Freed, Randall Gellens, John Klensin, and Keith Moore.\nMail processing model.\nEmail is submitted by a mail client (mail user agent, MUA) to a mail server (mail submission agent, MSA) using SMTP on TCP port 465 or 587. Most mailbox providers still allow submission on traditional port 25. The MSA delivers the mail to its mail transfer agent (MTA). Often, these two agents are instances of the same software launched with different options on the same machine. Local processing can be done either on a single machine, or split among multiple machines; mail agent processes on one machine can share files, but if processing is on multiple machines, they transfer messages between each other using SMTP, where each machine is configured to use the next machine as a smart host. Each process is an MTA (an SMTP server) in its own right.\nThe boundary MTA uses DNS to look up the MX (mail exchanger) record for the recipient's domain (the part of the email address on the right of codice_2). The MX record contains the name of the target MTA. Based on the target host and other factors, the sending MTA selects a recipient server and connects to it to complete the mail exchange.\nMessage transfer can occur in a single connection between two MTAs, or in a series of hops through intermediary systems. A receiving SMTP server may be the ultimate destination, an intermediate \"relay\" (that is, it stores and forwards the message) or a \"gateway\" (that is, it may forward the message using some protocol other than SMTP). Per section 2.1, each hop is a formal handoff of responsibility for the message, whereby the receiving server must either deliver the message or properly report the failure to do so.\nOnce the final hop accepts the incoming message, it hands it to a mail delivery agent (MDA) for local delivery. An MDA saves messages in the relevant mailbox format. As with sending, this reception can be done using one or multiple computers, but in the diagram above the MDA is depicted as one box near the mail exchanger box. An MDA may deliver messages directly to storage, or forward them over a network using SMTP or other protocol such as Local Mail Transfer Protocol (LMTP), a derivative of SMTP designed for this purpose.\nOnce delivered to the local mail server, the mail is stored for batch retrieval by authenticated mail clients (MUAs). Mail is retrieved by end-user applications, called email clients, using Internet Message Access Protocol (IMAP), a protocol that both facilitates access to mail and manages stored mail, or the Post Office Protocol (POP) which typically uses the traditional mbox mail file format or a proprietary system such as Microsoft Exchange/Outlook or Lotus Notes/Domino. Webmail clients may use either method, but the retrieval protocol is often not a formal standard.\nSMTP defines message \"transport\", not the message \"content\". Thus, it defines the mail \"envelope\" and its parameters, such as the envelope sender, but not the header (except \"trace information\") nor the body of the message itself. STD 10 and define SMTP (the envelope), while STD 11 and define the message (header and body), formally referred to as the Internet Message Format.\nProtocol overview.\nSMTP is a connection-oriented, text-based protocol in which a mail sender communicates with a mail receiver by issuing command strings and supplying necessary data over a reliable ordered data stream channel, typically a Transmission Control Protocol (TCP) connection. An \"SMTP session\" consists of commands originated by an SMTP client (the initiating agent, sender, or transmitter) and corresponding responses from the SMTP server (the listening agent, or receiver) so that the session is opened, and session parameters are exchanged. A session may include zero or more SMTP transactions. An \"SMTP transaction\" consists of three command/reply sequences:\nBesides the intermediate reply for DATA, each server's reply can be either positive (2xx reply codes) or negative. Negative replies can be permanent (5xx codes) or transient (4xx codes). A reject is a permanent failure and the client should send a bounce message to the server it received it from. A drop is a positive response followed by message discard rather than delivery.\nThe initiating host, the SMTP client, can be either an end-user's email client, functionally identified as a mail user agent (MUA), or a relay server's mail transfer agent (MTA), that is an SMTP server acting as an SMTP client, in the relevant session, in order to relay mail. Fully capable SMTP servers maintain queues of messages for retrying message transmissions that resulted in transient failures.\nA MUA knows the \"outgoing mail\" SMTP server from its configuration. A relay server typically determines which server to connect to by looking up the MX (Mail eXchange) DNS resource record for each recipient's domain name. If no MX record is found, a conformant relaying server (not all are) instead looks up the A record. Relay servers can also be configured to use a smart host. A relay server initiates a TCP connection to the server on the \"well-known port\" for SMTP: port 25, or for connecting to an MSA, port 465 or 587. The main difference between an MTA and an MSA is that connecting to an MSA requires SMTP Authentication.\nSMTP vs mail retrieval.\nSMTP is a delivery protocol only. In normal use, mail is \"pushed\" to a destination mail server (or next-hop mail server) as it arrives. Mail is routed based on the destination server, not the individual user(s) to which it is addressed. Other protocols, such as the Post Office Protocol (POP) and the Internet Message Access Protocol (IMAP) are specifically designed for use by individual users retrieving messages and managing mailboxes. To permit an intermittently-connected mail server to \"pull\" messages from a remote server on demand, SMTP has a feature to initiate mail queue processing on a remote server (see Remote Message Queue Starting below). POP and IMAP are unsuitable protocols for relaying mail by intermittently-connected machines; they are designed to operate after final delivery, when information critical to the correct operation of mail relay (the \"mail envelope\") has been removed.\nRemote Message Queue Starting.\nRemote Message Queue Starting enables a remote host to start processing of the mail queue on a server so it may receive messages destined to it by sending a corresponding command. The original codice_3 command was deemed insecure and was extended in with the codice_4 command which operates more securely using an authentication method based on Domain Name System information.\nOutgoing mail SMTP server.\nAn email client needs to know the IP address of its initial SMTP server and this has to be given as part of its configuration (usually given as a DNS name). This server will deliver outgoing messages on behalf of the user.\nOutgoing mail server access restrictions.\nServer administrators need to impose some control on which clients can use the server. This enables them to deal with abuse, for example spam. Two solutions have been in common use:\nRestricting access by location.\nUnder this system, an ISP's SMTP server will not allow access by users who are outside the ISP's network. More precisely, the server may only allow access to users with an IP address provided by the ISP, which is equivalent to requiring that they are connected to the Internet using that same ISP. A mobile user may often be on a network other than that of their normal ISP, and will then find that sending email fails because the configured SMTP server choice is no longer accessible.\nThis system has several variations. For example, an organisation's SMTP server may only provide service to users on the same network, enforcing this by firewalling to block access by users on the wider Internet. Or the server may perform range checks on the client's IP address. These methods were typically used by corporations and institutions such as universities which provided an SMTP server for outbound mail only for use internally within the organisation. However, most of these bodies now use client authentication methods, as described below.\nWhere a user is mobile, and may use different ISPs to connect to the internet, this kind of usage restriction is onerous, and altering the configured outbound email SMTP server address is impractical. It is highly desirable to be able to use email client configuration information that does not need to change.\nClient authentication.\nModern SMTP servers typically require authentication of clients by credentials before allowing access, rather than restricting access by location as described earlier. This more flexible system is friendly to mobile users and allows them to have a fixed choice of configured outbound SMTP server. SMTP Authentication, often abbreviated SMTP AUTH, is an extension of the SMTP in order to log in using an authentication mechanism.\nPorts.\nCommunication between mail servers generally uses the standard TCP port 25 designated for SMTP.\nMail \"clients\" however generally don't use this, instead using specific \"submission\" ports. Mail services generally accept email submission from clients on one of:\nPort 2525 and others may be used by some individual providers, but have never been officially supported.\nMany Internet service providers now block all outgoing port 25 traffic from their customers. Mainly as an anti-spam measure, but also to cure for the higher cost they have when leaving it open, perhaps by charging more from the few customers that require it open.\nSMTP transport example.\nA typical example of sending a message via SMTP to two mailboxes (\"alice\" and \"theboss\") located in the same mail domain (\"example.com\") is reproduced in the following session exchange. (In this example, the conversation parts are prefixed with \"S:\" and \"C:\", for \"server\" and \"client\", respectively; these labels are not part of the exchange.)\nAfter the message sender (SMTP client) establishes a reliable communications channel to the message receiver (SMTP server), the session is opened with a greeting by the server, usually containing its fully qualified domain name (FQDN), in this case \"smtp.example.com\". The client initiates its dialog by responding with a codice_5 command identifying itself in the command's parameter with its FQDN (or an address literal if none is available).\n S: 220 smtp.example.com ESMTP Postfix\n C: HELO relay.example.org\n S: 250 Hello relay.example.org, I am glad to meet you\n C: MAIL FROM:&lt;bob@example.org&gt;\n S: 250 Ok\n C: RCPT TO:&lt;alice@example.com&gt;\n S: 250 Ok\n C: RCPT TO:&lt;theboss@example.com&gt;\n S: 250 Ok\n C: DATA\n S: 354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;\n C: \n C: \n C: \n C: \n C: \n C:\n C: Hello Alice.\n C: This is a test message with 5 header fields and 4 lines in the message body.\n C: Your friend,\n C: Bob\n C: .\n S: 250 Ok: queued as 12345\n C: QUIT\n S: 221 Bye\nThe client notifies the receiver of the originating email address of the message in a codice_6 command. This is also the return or bounce address in case the message cannot be delivered. In this example the email message is sent to two mailboxes on the same SMTP server: one for each recipient listed in the codice_7 and codice_8 header fields. The corresponding SMTP command is codice_9. Each successful reception and execution of a command is acknowledged by the server with a result code and response message (e.g., codice_10).\nThe transmission of the body of the mail message is initiated with a codice_11 command after which it is transmitted verbatim line by line and is terminated with an end-of-data sequence. This sequence consists of a new-line (codice_12), a single full stop (codice_13), followed by another new-line (codice_12). Since a message body can contain a line with just a period as part of the text, the client sends \"two\" periods every time a line starts with a period; correspondingly, the server replaces every sequence of two periods at the beginning of a line with a single one. Such escaping method is called \"dot-stuffing\".\nThe server's positive reply to the end-of-data, as exemplified, implies that the server has taken the responsibility of delivering the message. A message can be doubled if there is a communication failure at this time, e.g. due to a power outage: Until the sender has received that codice_10 reply, it must assume the message was not delivered. On the other hand, after the receiver has decided to accept the message, it must assume the message has been delivered to it. Thus, during this time span, both agents have active copies of the message that they will try to deliver. The probability that a communication failure occurs exactly at this step is directly proportional to the amount of filtering that the server performs on the message body, most often for anti-spam purposes. The limiting timeout is specified to be 10 minutes.\nThe codice_16 command ends the session. If the email has other recipients located elsewhere, the client would codice_16 and connect to an appropriate SMTP server for subsequent recipients after the current destination(s) had been queued. The information that the client sends in the codice_5 and codice_6 commands are added (not seen in example code) as additional header fields to the message by the receiving server. It adds a codice_20 and codice_21 header field, respectively.\nSome clients are implemented to close the connection after the message is accepted (codice_22), so the last two lines may actually be omitted. This causes an error on the server when trying to send the codice_23 reply.\nSMTP Extensions.\nExtension discovery mechanism.\nClients learn a server's supported options by using the codice_24 greeting, as exemplified below, instead of the original codice_5. Clients fall back to codice_5 only if the server does not support codice_24 greeting.\nModern clients may use the ESMTP extension keyword codice_28 to query the server for the maximum message size that will be accepted. Older clients and servers may try to transfer excessively sized messages that will be rejected after consuming network resources, including connect time to network links that is paid by the minute.\nUsers can manually determine in advance the maximum size accepted by ESMTP servers. The client replaces the codice_5 command with the codice_24 command.\n S: 220 smtp2.example.com ESMTP Postfix\n C: EHLO bob.example.org\n S: 250-smtp2.example.com Hello bob.example.org [192.0.2.201]\n S: 250-SIZE 14680064\n S: 250-PIPELINING\n S: 250 HELP\nThus \"smtp2.example.com\" declares that it can accept a fixed maximum message size no larger than 14,680,064 octets (8-bit bytes).\nIn the simplest case, an ESMTP server declares a maximum codice_28 immediately after receiving an codice_24. According to , however, the numeric parameter to the codice_28 extension in the codice_24 response is optional. Clients may instead, when issuing a codice_6 command, include a numeric estimate of the size of the message they are transferring, so that the server can refuse receipt of overly-large messages.\nBinary data transfer.\nOriginal SMTP supports only a single body of ASCII text, therefore any binary data needs to be encoded as text into that body of the message before transfer, and then decoded by the recipient. Binary-to-text encodings, such as uuencode and BinHex were typically used.\nThe 8BITMIME command was developed to address this. It was standardized in 1994 as It facilitates the transparent exchange of e-mail messages containing octets outside the seven-bit ASCII character set by encoding them as MIME content parts, typically encoded with Base64.\nOn-Demand Mail Relay.\nOn-Demand Mail Relay (ODMR) is an SMTP extension standardized in that allows an intermittently-connected SMTP server to receive email queued for it when it is connected.\nInternationalization extension.\nOriginal SMTP supports email addresses composed of ASCII characters only, which is inconvenient for users whose native script is not Latin based, or who use diacritic not in the ASCII character set. This limitation was alleviated via extensions enabling UTF-8 in address names. introduced experimental codice_36 command and later was superseded by that introduced codice_1 command. These extensions provide support for multi-byte and non-ASCII characters in email addresses, such as those with diacritics and other language characters such as Greek and Chinese.\nCurrent support is limited, but there is strong interest in broad adoption of and the related RFCs in countries like China that have a large user base where Latin (ASCII) is a foreign script.\nExtensions.\nLike SMTP, ESMTP is a protocol used to transport Internet mail. It is used as both an inter-server transport protocol and (with restricted behavior enforced) a mail submission protocol.\nThe main identification feature for ESMTP clients is to open a transmission with the command codice_24 (Extended HELLO), rather than codice_5 (Hello, the original standard). A server will respond with success (code 250), failure (code 550) or error (code 500, 501, 502, 504, or 421), depending on its configuration. An ESMTP server returns the code 250 OK in a multi-line reply with its domain and a list of keywords to indicate supported extensions. A RFC 821 compliant server returns error code 500, allowing ESMTP clients to try either codice_5 or codice_16.\nEach service extension is defined in an approved format in subsequent RFCs and registered with the Internet Assigned Numbers Authority (IANA). The first definitions were the RFC 821 optional services: codice_42, codice_43 (Send or Mail), codice_44 (Send and Mail), codice_45, codice_46, and codice_3. The format of additional SMTP verbs was set and for new parameters in codice_48 and codice_49.\nSome relatively common keywords (not all of them corresponding to commands) used today are:\nThe ESMTP format was restated in (superseding RFC 821) and updated to the latest definition in in 2008. Support for the codice_24 command in servers became mandatory, and codice_5 designated a required fallback.\nNon-standard, unregistered, service extensions can be used by bilateral agreement, these services are indicated by an codice_24 message keyword starting with \"X\", and with any additional parameters or verbs similarly marked.\nSMTP commands are case-insensitive. They are presented here in capitalized form for emphasis only. An SMTP server that requires a specific capitalization method is a violation of the standard.\n8BITMIME.\nAt least the following servers advertise the 8BITMIME extension:\nThe following servers can be configured to advertise 8BITMIME, but do not perform conversion of 8-bit data to 7-bit when connecting to non-8BITMIME relays:\nSMTP-AUTH.\nThe SMTP-AUTH extension provides an access control mechanism. It consists of an authentication step through which the client effectively logs into the mail server during the process of sending mail. Servers that support SMTP-AUTH can usually be configured to require clients to use this extension, ensuring the true identity of the sender is known. The SMTP-AUTH extension is defined in .\nSMTP-AUTH can be used to allow legitimate users to relay mail while denying relay service to unauthorized users, such as spammers. It does not necessarily guarantee the authenticity of either the SMTP envelope sender or the \"From:\" header. For example, spoofing, in which one sender masquerades as someone else, is still possible with SMTP-AUTH unless the server is configured to limit message from-addresses to addresses this AUTHed user is authorized for.\nThe SMTP-AUTH extension also allows one mail server to indicate to another that the sender has been authenticated when relaying mail. In general this requires the recipient server to trust the sending server, meaning that this aspect of SMTP-AUTH is rarely used on the Internet.\nSMTPUTF8.\nSupporting servers include:\nSecurity extensions.\nMail delivery can occur both over plain text and encrypted connections, however the communicating parties might not know in advance of other party's ability to use secure channel.\nSTARTTLS or \"Opportunistic TLS\".\nThe STARTTLS extensions enables supporting SMTP servers to notify connecting clients that it supports TLS encrypted communication and offers the opportunity for clients to upgrade their connection by sending the STARTTLS command. Servers supporting the extension do not inherently gain any security benefits from its implementation on its own, as upgrading to a TLS encrypted session is dependent on the connecting client deciding to exercise this option, hence the term \"opportunistic\" TLS.\nSTARTTLS is effective only against passive observation attacks, since the STARTTLS negotiation happens in plain text and an active attacker can trivially remove STARTTLS commands. This type of man-in-the-middle attack is sometimes referred to as STRIPTLS, where the encryption negotiation information sent from one end never reaches the other. In this scenario both parties take the invalid or unexpected responses as indication that the other does not properly support STARTTLS, defaulting to traditional plain-text mail transfer. Note that STARTTLS is also defined for IMAP and POP3 in other RFCs, but these protocols serve different purposes: SMTP is used for communication between message transfer agents, while IMAP and POP3 are for end clients and message transfer agents.\nIn 2014 the Electronic Frontier Foundation began \"STARTTLS Everywhere\" project that, similarly to \"HTTPS Everywhere\" list, allowed relying parties to discover others supporting secure communication without prior communication. The project stopped accepting submissions on 29 April 2021, and EFF recommended switching to DANE and MTA-STS for discovering information on peers' TLS support.\n officially declared plain text obsolete and recommend always using TLS for mail submission and access, adding ports with implicit TLS.\nDANE for SMTP.\n introduced the ability for DNS records to declare the encryption capabilities of a mail server. Utilising DNSSEC, mail server operators are able to publish a hash of their TLS certificate, thereby mitigating the possibility of unencrypted communications.\nMicrosoft expects to enable full SMTP DANE support for Exchange Online customers by the end of 2024. \nSMTP MTA Strict Transport Security.\nA newer 2018 called \"SMTP MTA Strict Transport Security (MTA-STS)\" aims to address the problem of active adversaries by defining a protocol for mail servers to declare their ability to use secure channels in specific files on the server and specific DNS TXT records. The relying party would regularly check existence of such record, and cache it for the amount of time specified in the record and never communicate over insecure channels until record expires. Note that MTA-STS records apply only to SMTP traffic between mail servers while communications between a user's client and the mail server are protected by Transport Layer Security with SMTP/MSA, IMAP, POP3, or HTTPS in combination with an organizational or technical policy. Essentially, MTA-STS is a means to extend such a policy to third parties.\nIn April 2019 Google Mail announced support for MTA-STS.\nSMTP TLS Reporting.\nProtocols designed to securely deliver messages can fail due to misconfigurations or deliberate active interference, leading to undelivered messages or delivery over unencrypted or unauthenticated channels. \"SMTP TLS Reporting\" describes a reporting mechanism and format for sharing statistics and specific information about potential failures with recipient domains. Recipient domains can then use this information to both detect potential attacks and diagnose unintentional misconfigurations.\nIn April 2019 Google Mail announced support for SMTP TLS Reporting.\nSpoofing and spamming.\nThe original design of SMTP had no facility to authenticate senders, or check that servers were authorized to send on their behalf, with the result that email spoofing is possible, and commonly used in email spam and phishing.\nOccasional proposals are made to modify SMTP extensively or replace it completely. One example of this is Internet Mail 2000, but neither it, nor any other has made much headway in the face of the network effect of the huge installed base of classic SMTP. \nInstead, mail servers now use a range of techniques, such as stricter enforcement of standards such as , DomainKeys Identified Mail, Sender Policy Framework and DMARC, DNSBLs and greylisting to reject or quarantine suspicious emails.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27676", "revid": "5272735", "url": "https://en.wikipedia.org/wiki?curid=27676", "title": "Shuttlecock", "text": "Sport equipment\nA shuttlecock (also called a birdie or shuttle, or ball) is a high-drag projectile used in multiple sports, most notably badminton. It has an open conical shape formed by feathers or a synthetic material, such as plastic, embedded into a rounded cork (or rubber) base. The shuttlecock's shape makes it extremely aerodynamically stable. Regardless of initial orientation, it will turn to fly cork first, and remain in the cork-first orientation.\nOrigins.\nThe object resembles a hawk's lure, used from ancient times in the training of hunting birds. It is frequently shortened to shuttle. The \"shuttle\" part of the name is derived from its back-and-forth motion during the game, resembling the shuttle of a 14th-century loom, while the \"cock\" part of the name is derived from the resemblance of the feathers to those on a rooster.\nSpecifications.\nA regulation standard shuttlecock weighs around . It has 16 feathers with each feather in length, and the diameter of the cork is . The diameter of the circle that the feathers make is around .\nConstruction and materials.\nA shuttlecock is formed from 16 or so overlapping feathers, usually goose or duck, embedded into a rounded cork base. Feathers are plucked from the wings of a live goose or duck, a method which has been deemed cruel by animal rights activists in recent years. The cork is covered with thin leather. To ensure satisfactory flight properties, it is considered preferable to use feathers from right or left wings only in each shuttlecock, and not mix feathers from different wings, as the feathers from different wings are shaped differently. Badminton companies make shuttlecock corks by sandwiching polyurethane between corks and/or using a whole piece of natural cork. With the first method, the cork becomes misshaped after use, while the cork in the latter method changes very little after use. This is because the structure of the shuttlecock is more durable when made with a single piece of natural cork.\nFeather or synthetic shuttlecocks.\nThe feathers are brittle; shuttlecocks break easily and often need to be replaced several times during a game. For this reason, synthetic shuttlecocks have been developed that replace the feathers with a plastic skirt. Players often refer to synthetic shuttlecocks as \"plastics\" and feathered shuttlecocks as \"feathers\". Feather shuttles need to be properly humidified for at least 4 hours prior to play in order to fly the correct distance at the proper speed and to last longer. Properly humidified feathers flex during play, enhancing the shuttle's speed change and durability. Dry feathers are brittle and break easily, causing the shuttle to wobble. Saturated feathers are 'mushy', making the feather cone narrow too much when strongly hit, which causes the shuttle to fly overly far and fast. Typically a humidification box is used, or a small moist sponge is inserted in the feather end of the closed shuttle tube container, avoiding any water contact with the cork of the shuttle. Shuttles are tested prior to play to make sure they fly true and at the proper speed, and cover the proper distance. Different weights of shuttles are used to compensate for local atmospheric conditions. Both humidity and height above sea level affect shuttle flight. World Badminton Federation Rules say the shuttle should reach the far doubles service line plus or minus half the width of the tram. According to manufacturers proper shuttles will generally travel from the back line of the court to just short of the long doubles service line on the opposite side of the net, with a full underhand hit from an average player.\nThe cost of good quality feathers is similar to that of good quality plastics, but plastics are far more durable, typically lasting many matches without any impairment to their flight. Feather shuttles are easily damaged and should be replaced every three or four games or sooner if they are damaged and do not fly straight. Damaged shuttles interfere with play as any impairment may misdirect the flight of the shuttlecock.\nMost experienced and skillful players greatly prefer feathers, and serious tournaments or leagues are always played using feather shuttlecocks of the highest quality.\nThe playing characteristics of plastics and feathers are substantially different. Plastics fly more slowly on initial impact, but slow down less towards the end of their flight. Feathers, however, tend to drop straight down on a clear shot, plastics never quite return to a straight drop, falling more on a diagonal. Feather shuttles may come off the strings at speeds in excess of 565\u00a0km/h (351\u00a0mph) but slow down faster as they drop. Furthermore, feathered shuttlecocks are recorded as having a constant drag coefficient. Contrarily, championship-grade synthetic shuttlecocks show less consistency with this factor. This shows that feathered shuttlecocks have a capacity for a higher standard speed range at which the game is typically played that synthetics cannot quite reach. This impacts the feel of the bird during the game for players, especially in the case of deformation of the shuttlecock. A feathered shuttlecock will still feel dull and heavy while in play because of the feathers, but a synthetic cannot maintain energy in flight in the same manner.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27678", "revid": "48911350", "url": "https://en.wikipedia.org/wiki?curid=27678", "title": "SNMP", "text": ""}
{"id": "27679", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=27679", "title": "Soldering iron", "text": "Hand tool for soldering\nA soldering iron is a hand tool used in soldering. It supplies heat to melt solder so that it can flow into the joint between two workpieces.\nA soldering iron is composed of a heated metal tip (the \"bit\") and an insulated handle. Heating is often achieved electrically, by passing an electric current (supplied through an electrical cord or battery cables) through a resistive heating element. Cordless irons can be heated by combustion of gas stored in a small tank, often using a catalytic heater rather than a flame. Simple irons, less commonly used today than in the past, were simply a large copper \"bit\" on a handle, heated in a flame.\nSolder melts at approximately . Soldering irons are designed to reach a temperature range of . \nSoldering irons are most often used for installation, repairs, and limited production work in electronics assembly. High-volume production lines use other soldering methods. Large irons may be used for soldering joints in sheet metal objects. Less common uses include pyrography (burning designs into wood) and plastic welding (as an alternative to ultrasonic welding).\nHistory.\nBefore the development of electric soldering irons, the typical soldering iron consisted of a copper block, with an appropriately shaped point, supported on an iron rod and held in a wood handle. Immediately before use, the iron was heated over a fire or in a charcoal brazier, and it had to be reheated whenever it became too cool for use. Soldering irons were primarily used by tinsmiths and coppersmiths to work with thin sheet metal.\nA large copper block was required in order to have sufficient thermal capacity to provide\nuseful heat after removal from the fire, and copper is expensive. This led to the development of\nsoldering irons that had a small copper tip attached to an inexpensive cast-iron block. Some irons even had removable and replaceable copper tips.\nThe first electric soldering iron had a very lightweight platinum tip heated by electric current flowing through the tip itself. By 1889, electric soldering irons were being developed with a resistance wire wrapped around the back end of the copper head and enclosed in a protective shell. Alternatively, the heating element could be enclosed in a relatively light-weight hollow copper head. \nIn 1894, the American Electrical Heater Company began manufacturing electrical soldering irons on a large scale in Detroit. They started producing them and shortly after American Electrical Heater Company released their line of soldering irons. In 1905, \"Scientific American Magazine,\" published a tutorial on making a soldering iron that\nclearly explains how early irons were made.\nIn 1921, a German company founded by Ernst Sachs developed an electrical soldering iron similar to American Electrical Heater Company iron. in 1926, William Alferink applied for a patent for the first soldering station.\nActual \"Form factor\" of soldering irons.\nIn 1946, Carl E. Weller applied for a patent for his soldering gun that could heat instantaneously and began production of the \"Speedy Iron\" in Pennsylvania. It was manufactured through the Weller Manufacturing Company, and this product was the first instantaneous thermal soldering gun. Few years later, they released to the market a soldering iron on with self-adjusting temperature. In 1951, the company WEN Products began manufacturing its own instantaneous soldering iron. After a three years trial Weller won for patent infringement.\nIn 1960 Weller got the patent for the soldering iron \"Magnastat\", renewed in 1964 and 1971. This iron could control the temperature by using a temperature-sensitive magnetic tip. The \"Magnastat\" became a best seller and it was included it in the W-TCP soldering station in 1967. In fact, within the patent, as a complementary description, it defines what today has become the \"de facto\", the redundancy is worth it, \"form factor\" of the vast majority of current Japanese and Chinese irons: Hakko, Baku, etc... The now-expired patent, which even Weller has stopped using on some models, described an outer tube holding the coated copper tip, clamped with a nut to the handle.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Located at the remote rear end of draw tube 26, and held thereto by peripheral flange 27, is a freely rotatable threaded nut 28 adapted to be threaded about nut receiving stud 29 of cover flange 21 to draw the tip 11 into proper engagement\"\nTypes.\nSimple iron.\nFor electrical and electronics work, a low-power iron, a power rating between 15 and 35\u00a0watts, is used. Higher ratings are available, but do not run at higher temperature; instead there is more heat available for making soldered connections to things with large thermal capacity, for example, a metal chassis. Some irons are temperature-controlled, running at a fixed temperature in the same way as a soldering station, with higher power available for joints with large heat capacity. Simple irons run at an uncontrolled temperature determined by thermal equilibrium; when heating something large their temperature drops.\nA variation is the Scope soldering iron, common in Australia, which operates from a low-voltage source such as transformer or battery, and heats in seconds when the user pushes the thumb-guard, which then acts as a heat controller.\nCordless iron.\nSmall irons heated by a battery, or by combustion of a gas such as butane in a small self-contained tank, can be used when electricity is unavailable or cordless operation is required. The operating temperature of these irons is not regulated directly; gas irons may change power by adjusting gas flow. Gas-powered irons may have interchangeable tips including different size soldering tips, hot knife for cutting plastics, miniature blow-torch with a hot flame, and small hot air blower for such applications as shrinking heat shrink tubing.\nTemperature-controlled soldering iron.\nSimple soldering irons reach a temperature determined by thermal equilibrium, dependent upon power input and cooling by the environment and the materials it comes into contact with. The iron temperature will drop when in contact with a large mass of metal such as a chassis; a small iron will lose too much temperature to solder a large connection. More advanced irons for use in electronics have a mechanism with a temperature sensor and method of temperature control to keep the tip temperature steady; more power is available if a connection is large. Temperature-controlled irons may be free-standing, or may comprise a head with heating element and tip, controlled by a base called a soldering station, with control circuitry and temperature adjustment and sometimes display.\nA variety of means are used to control temperature. The simplest of these is a variable power control, much like a light dimmer, which changes the equilibrium temperature of the iron without automatically measuring or regulating the temperature. Another type of system uses a thermostat, often inside the iron's tip, which automatically switches power on and off to the element. A thermal sensor such as a thermocouple may be used in conjunction with circuitry to monitor the temperature of the tip and adjust power delivered to the heating element to maintain a desired temperature. In some models, the firmware for the control circuitry is free software that can be modified by the end-user.\nAnother approach is to use magnetized soldering tips which lose their magnetic properties at a specific temperature, the Curie point. As long as the tip is magnetic, it closes a switch to supply power to the heating element. When it exceeds the design temperature it opens the contacts, cooling until the temperature drops enough to restore magnetisation. More complex Curie-point irons circulate a high-frequency AC current through the tip, using magnetic physics to direct heating only where the surface of the tip drops below the Curie point.\nSoldering station.\nA soldering station has a temperature control and consists of an electrical power supply, control circuitry with provision for user adjustment of temperature and display, and a soldering iron or soldering head with a tip temperature sensor. The station will normally have a stand for the hot iron when not in use, and a wet sponge for cleaning. It is most commonly used for soldering electronic components. Other functions may be combined; for example a rework station, mainly for surface-mount components may have a hot air gun, vacuum pickup tool, and a soldering head; a desoldering station will have a desoldering head with vacuum pump for desoldering through-hole components, and a soldering iron head.\nSoldering tweezers.\nFor soldering and desoldering small surface-mount components with two terminals, such as some links, resistors, capacitors, and diodes, soldering tweezers can be used; they can be either free-standing or controlled from a soldering station. The tweezers have two heated tips mounted on arms whose separation can be manually varied by squeezing gently against spring force, like simple tweezers; the tips are applied to the two ends of the component. The main purpose of the soldering tweezers is to melt solder in the correct place; components are usually moved by simple tweezers or vacuum pickup.\nHot knife.\nA hot knife is a form of soldering iron equipped with a double-edged blade that is situated on a heating element. These tools can reach temperatures of up to 1,000 degrees Fahrenheit (538 degrees Celsius) allowing for cuts of fabric and foam materials without worry of fraying or beading. Hot knives can be utilized in automotive, marine, and carpeting applications, as well as other industrial and personal uses.\nStands.\nA soldering iron stand keeps the iron away from flammable materials, and often also comes with a cellulose sponge and flux pot for cleaning the tip. Some soldering irons for continuous and professional use come as part of a \"soldering station,\" which allows the exact temperature of the tip to be adjusted, kept constant, and sometimes displayed.\nTips.\nMost soldering irons for electronics have interchangeable tips, also known as \"bits\", that vary in size and shape for different types of work. Common tip shapes include: \"bevel\", \"chisel\", and \"conical\". An example of a more specialist tip is spoon or gull wing, which features concavity. See the image for renderings of a few different tip shapes and some of the names given to them.\nPyramid tips with a triangular flat face and chisel tips with a wide flat face are useful for soldering sheet metal. Fine conical or tapered chisel tips are typically used for electronics work. Tips may be straight or have a bend. Concave or wicking tips with a chisel face with a concave well in the flat face to hold a small amount of solder are available. Tip selection depends upon the type of work and access to the joint; soldering of 0.5mm pitch surface-mount ICs, for example, is quite different from soldering a through-hole connection to a large area. A concave tip well is said to help prevent bridging of closely spaced leads; different shapes are recommended to correct bridging that has occurred. Due to patent restrictions not all manufacturers offer concave tips everywhere; in particular there are restrictions in the USA.\nOlder and very cheap irons typically use a bare copper tip, which is shaped with a file or sandpaper. This dissolves gradually into the solder, suffering pitting and erosion of the shape. Copper tips are sometimes filed when worn down. Iron-plated copper tips have become increasingly popular since the 1980s. Because iron is not readily dissolved by molten solder, the plated tip is more durable than a bare copper one, though it will eventually wear out and need replacing. This is especially important when working at the higher temperatures needed for modern lead-free solders. Solid iron and steel tips are seldom used because they store less heat, conduct it poorly, and rusting can break the heating element.\nIron-plated tips may feature a layer of nickel between the copper core and the iron surface. A nickel-chrome outer plating may be used further back from the very tip, as solder does not stick well to this material: this avoids solder wetting parts of the tip where it would be unwanted.\nSome tips have a heater and a thermocouple-based temperature sensor embedded to facilitate a more precise temperature control (TS100 and T12, for instance).\nCleaning.\nWhen the iron tip oxidises and burnt flux accumulates on it, solder no longer wets the tip, impeding heat transfer and making soldering difficult or impossible; tips must be periodically cleaned in use. Such problems happen with all kinds of solder, but are much more severe with the lead-free solders which have become widespread in electronics work, which require higher temperatures than solders containing lead. Exposed iron plating oxidises; if the tip is kept tinned with molten solder oxidation is inhibited. A clean unoxidised tip is tinned by applying a little solder and flux.\nA wet small sponge, often supplied with soldering equipment, can be used to wipe the tip. For lead-free solder a slightly more aggressive cleaning, with brass shavings, can be used. Soldering flux will help to remove oxide; the more active the flux the better the cleaning, although acidic flux used on circuit boards that is not carefully cleaned off will cause corrosion. A tip which is cleaned but not retinned is susceptible to oxidation.\nSoldering iron tips are made of a copper core plated various metals including iron. The copper is used for heat transfer and the other platings are for durability. Copper is very easily corroded, eating away the tip, particularly in lead-free work; iron is not. Cleaning tips requires the removal of oxide without damaging the iron plating and exposing the copper to rapid corrosion. The use of solder already containing a small amount of copper can slow corrosion of copper tips.\nIn cases of severe oxidation not removable by gentler methods, abrasion with something hard enough to remove oxide but not so hard as to scratch the iron plating can be used. A brass wire scourer, brush, or wheel on a bench grinder, can be used with care. Sandpaper and other tools may be used but are likely to damage the plating.\nSafety precautions.\nElectro-static Discharge.\nNot all soldering irons are ESD-safe.\nAlthough some manufacturers' mains-powered models are built with the element shaft (and hence the tip) electrically connected to ground via the iron's mains lead, other models' tips may float at arbitrary voltages unless an additional grounding wire is used.\nLead Exposure.\nMelting the solder can release toxic fumes, particularly solder with higher proportions of lead. It is recommended to work in a well-ventilated area; to avoid inhaling the toxic fumes; and to wash hands with soap after operating with a soldering iron. It is also recommended to use lead-free solder to mitigate the risks of lead exposure.\nHigh Temperature.\nWhen operating, the iron can reach temperatures in the range of 200-480\u00b0C. Never touch the heating element, since doing so while hot will cause one to contract skin burns. Make sure to switch off the device and hold it on the soldering stand when not in use - placing it directly on the workbench may cause it to make contact and damage the surface and nearby objects. Ensure that the cleaning sponge is wet while in operation.\nSolder Ejection.\nSometimes, molten solder can \"spit\" or spatter, ejecting out from the wire and potentially causing damage to eyes and skin. This is caused by air pockets. Always wear appropriate eye protection, like safety goggles and fireproof clothing (like 100% cotton), covering the arms and legs and wear toe-closed shoes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27680", "revid": "14650386", "url": "https://en.wikipedia.org/wiki?curid=27680", "title": "Supernova", "text": "Astrophysical phenomenon\nA supernova (pl.: supernovae) is a powerful and luminous explosion of a star. A supernova occurs during the last evolutionary stages of a massive star, or when a white dwarf is triggered into runaway nuclear fusion. The original object, called the \"progenitor\", either collapses to a neutron star or black hole, or is completely destroyed to form a diffuse nebula. The peak optical luminosity of a supernova can be comparable to that of an entire galaxy before fading over several weeks or months.\nThe last supernova directly observed in the Milky Way was Kepler's Supernova in 1604, appearing not long after Tycho's Supernova in 1572, both of which were visible to the naked eye. Observations of recent supernova remnants within the Milky Way, coupled with studies of supernovae in other galaxies, suggest that these powerful stellar explosions occur in our galaxy approximately three times per century on average. A supernova in the Milky Way would almost certainly be observable through modern astronomical telescopes. The most recent naked-eye supernova was SN 1987A, which was the explosion of a blue supergiant star in the Large Magellanic Cloud, a satellite galaxy of the Milky Way in 1987.\nTheoretical studies indicate that most supernovae are triggered by one of two basic mechanisms: the sudden re-ignition of nuclear fusion in a white dwarf, or the sudden gravitational collapse of a massive star's core.\nSupernovae can expel several solar masses of material at speeds up to several percent of the speed of light. This drives an expanding shock wave into the surrounding interstellar medium, sweeping up an expanding shell of gas and dust observed as a supernova remnant. Supernovae are a major source of elements in the interstellar medium from oxygen to rubidium. The expanding shock waves of supernovae can trigger the formation of new stars. Supernovae are a major source of cosmic rays. They might also produce gravitational waves.\nEtymology.\nThe word \"supernova\" has the plural form \"supernovae\" () or \"supernovas\" and is often abbreviated as SN or SNe. It is derived from the Latin word , meaning 'new', which refers to what appears to be a temporary new bright star. Adding the prefix \"super-\" distinguishes supernovae from ordinary novae, which are far less luminous. The word \"supernova\" was coined by Walter Baade and Fritz Zwicky, who began using it in astrophysics lectures in 1931. Its first use in a journal article came the following year in a publication by Knut Lundmark, who may have coined it independently.\nObservation history.\nCompared to a star's entire history, the visual appearance of a supernova is very brief, sometimes spanning several months, so that the chances of observing one with the naked eye are roughly once in a lifetime. Only a tiny fraction of the 100\u00a0billion stars in a typical galaxy have the capacity to become a supernova, the ability being restricted to those having high mass and those in rare kinds of binary star systems with at least one white dwarf.\nEarly discoveries.\nThe earliest record of a possible supernova, known as HB9, was likely viewed by an unknown prehistoric people of the Indian subcontinent and recorded on a rock carving in the Burzahama region of Kashmir, dated to . Later, SN 185 was documented by Chinese astronomers in 185 AD. The brightest recorded supernova was SN 1006, which was observed in AD 1006 in the constellation of Lupus. This event was described by observers in China, Japan, Iraq, Egypt and Europe. The widely observed supernova SN 1054 produced the Crab Nebula.\nSupernovae SN 1572 and SN 1604, the latest Milky Way supernovae to be observed with the naked eye, had a notable influence on the development of astronomy in Europe because they were used to argue against the Aristotelian idea that the universe beyond the Moon and planets was static and unchanging. Johannes Kepler began observing SN 1604 at its peak on 17 October 1604, and continued to make estimates of its brightness until it faded from naked eye view a year later. It was the second supernova to be observed in a generation, after Tycho Brahe observed SN 1572 in Cassiopeia.\nThere is some evidence that the youngest known supernova in our galaxy, G1.9+0.3, occurred in the late 19th century, considerably more recently than Cassiopeia A from around 1680. Neither was noted at the time. In the case of G1.9+0.3, high extinction from dust along the plane of the galactic disk could have dimmed the event sufficiently for it to go unnoticed. The situation for Cassiopeia A is less clear; infrared light echoes have been detected showing that it was not in a region of especially high extinction.\nTelescope findings.\nWith the development of the astronomical telescope, observation and discovery of fainter and more distant supernovae became possible. The first such observation was of SN 1885A in the Andromeda Galaxy. A second supernova, SN 1895B, was discovered in NGC 5253 a decade later. Early work on what was originally believed to be simply a new category of novae was performed during the 1920s. These were variously called \"upper-class Novae\", \"Hauptnovae\", or \"giant novae\". The name \"supernovae\" is thought to have been coined by Walter Baade and Zwicky in lectures at Caltech in 1931. It was used, as \"super-Novae\", in a journal paper published by Knut Lundmark in 1933, and in a 1934 paper by Baade and Zwicky. By 1938, the hyphen was no longer used and the modern name was in use.\nAmerican astronomers Rudolph Minkowski and Fritz Zwicky developed the modern supernova classification scheme beginning in 1941. During the 1960s, astronomers found that the maximum intensities of supernovae could be used as standard candles, hence indicators of astronomical distances. Some of the most distant supernovae observed in 2003 appeared dimmer than expected. This supports the view that the expansion of the universe is accelerating. Techniques were developed for reconstructing supernovae events that have no written records of being observed. The date of the Cassiopeia A supernova event was determined from light echoes off nebulae, while the age of supernova remnant RX J0852.0-4622 was estimated from temperature measurements and the gamma ray emissions from the radioactive decay of titanium-44.\nThe most luminous supernova ever recorded is ASASSN-15lh, at a distance of 3.82 gigalight-years. It was first detected in June 2015 and peaked at 570 billion\u00a0L\u2609, which is twice the bolometric luminosity of any other known supernova. The nature of this supernova is debated and several alternative explanations, such as tidal disruption of a star by a black hole, have been suggested.\nSN 2013fs was recorded three hours after the supernova event on 6 October 2013, by the Intermediate Palomar Transient Factory. This is among the earliest supernovae caught after detonation, and it is the earliest for which spectra have been obtained, beginning six hours after the actual explosion. The star is located in a spiral galaxy named NGC 7610, 160\u00a0million light-years away in the constellation of Pegasus.\nThe supernova SN 2016gkg was detected by amateur astronomer Victor Buso from Rosario, Argentina, on 20 September 2016. It was the first time that the initial \"shock breakout\" from an optical supernova had been observed. The progenitor star has been identified in Hubble Space Telescope images from before its collapse. Astronomer Alex Filippenko noted: \"Observations of stars in the first moments they begin exploding provide information that cannot be directly obtained in any other way.\"\nDiscovery programs.\nBecause supernovae are relatively rare events within a galaxy, occurring about three times a century in the Milky Way, obtaining a good sample of supernovae to study requires regular monitoring of many galaxies. Today, amateur and professional astronomers are finding about two thousand every year, some when near maximum brightness, others on old astronomical photographs or plates. Supernovae in other galaxies cannot be predicted with any meaningful accuracy. Normally, when they are discovered, they are already in progress. To use supernovae as standard candles for measuring distance, observation of their peak luminosity is required. It is therefore important to discover them well before they reach their maximum. Amateur astronomers, who greatly outnumber professional astronomers, have played an important role in finding supernovae, typically by looking at some of the closer galaxies through an optical telescope and comparing them to earlier photographs.\nToward the end of the 20th century, astronomers increasingly turned to computer-controlled telescopes and CCDs for hunting supernovae. While such systems are popular with amateurs, there are also professional installations such as the Katzman Automatic Imaging Telescope. The Supernova Early Warning System (SNEWS) project uses a network of neutrino detectors to give early warning of a supernova in the Milky Way galaxy. Neutrinos are subatomic particles that are produced in great quantities by a supernova, and they are not significantly absorbed by the interstellar gas and dust of the galactic disk.\nSupernova searches fall into two classes: those focused on relatively nearby events and those looking farther away. Because of the expansion of the universe, the distance to a remote object with a known emission spectrum can be estimated by measuring its Doppler shift (or redshift); on average, more-distant objects recede with greater velocity than those nearby, and so have a higher redshift. Thus the search is split between high redshift and low redshift, with the boundary falling around a redshift range of z=0.1\u20130.3, where z is a dimensionless measure of the spectrum's frequency shift. \nHigh redshift searches for supernovae usually involve the observation of supernova light curves. These are useful for standard or calibrated candles to generate Hubble diagrams and make cosmological predictions. Supernova spectroscopy, used to study the physics and environments of supernovae, is more practical at low than at high redshift. Low redshift observations also anchor the low-distance end of the Hubble curve, which is a plot of distance versus redshift for visible galaxies.\nAs survey programmes rapidly increase the number of detected supernovae, collated collections of observations (light decay curves, astrometry, pre-supernova observations, spectroscopy) have been assembled. The Pantheon data set, assembled in 2018, detailed 1048 supernovae. In 2021, this data set was expanded to 1701 light curves for 1550 supernovae taken from 18 different surveys, a 50% increase in under 3 years.\nNaming convention.\nSupernova discoveries are reported to the International Astronomical Union's Central Bureau for Astronomical Telegrams, which sends out a circular with the name it assigns to that supernova. The name is formed from the prefix \"SN\", followed by the year of discovery, suffixed with a one or two-letter designation. The first 26 supernovae of the year are designated with a capital letter from \"A\" to \"Z\". Next, pairs of lower-case letters are used: \"aa\", \"ab\", and so on. Hence, for example, \"SN 2003C\" designates the third supernova reported in the year 2003. The last supernova of 2005, SN 2005nc, was the 367th (14 \u00d7 26 + 3 = 367). Since 2000, professional and amateur astronomers have been finding several hundred supernovae each year (572 in 2007, 261 in 2008, 390 in 2009; 231 in 2013).\nHistorical supernovae are known simply by the year they occurred: SN 185, SN 1006, SN 1054, SN 1572 (called \"Tycho's Nova\") and SN 1604 (\"Kepler's Star\"). Since 1885 the additional letter notation has been used, even if there was only one supernova discovered that year (for example, SN 1885A, SN 1907A, etc.); this last happened with SN 1947A. \"SN\", for SuperNova, is a standard prefix. Until 1987, two-letter designations were rarely needed; since 1988, they have been needed every year. Since 2016, the increasing number of discoveries has regularly led to the additional use of three-letter designations. After zz comes aaa, then aab, aac, and so on. For example, the last supernova retained in the Asiago Supernova Catalogue\u2009 when it was terminated on 31 December 2017 bears the designation SN 2017jzp.\nClassification.\nAstronomers classify supernovae according to their light curves and the absorption lines of different chemical elements that appear in their spectra. If a supernova's spectrum contains lines of hydrogen (known as the Balmer series in the visual portion of the spectrum) it is classified \"Type II\"; otherwise it is \"Type I\". In each of these two types there are subdivisions according to the presence of lines from other elements or the shape of the light curve (a graph of the supernova's apparent magnitude as a function of time).\nType I.\nType I supernovae are subdivided on the basis of their spectra, with Type Ia showing a strong ionised silicon absorption line. Type I supernovae without this strong line are classified as Type Ib and Ic, with Type Ib showing strong neutral helium lines and Type Ic lacking them. Historically, the light curves of Type I supernovae were seen as all broadly similar, too much so to make useful distinctions. While variations in light curves have been studied, classification continues to be made on spectral grounds rather than light-curve shape.\nA small number of Type Ia supernovae exhibit unusual features, such as non-standard luminosity or broadened light curves, and these are typically categorised by referring to the earliest example showing similar features. For example, the sub-luminous SN 2008ha is often referred to as SN 2002cx-like or class Ia-2002cx.\nA small proportion of Type Ic supernovae show highly broadened and blended emission lines which are taken to indicate very high expansion velocities for the ejecta. These have been classified as Type Ic-BL or Ic-bl.\nCalcium-rich supernovae are a rare type of very fast supernova with unusually strong calcium lines in their spectra. Models suggest they occur when material is accreted from a helium-rich companion rather than a hydrogen-rich star. Because of helium lines in their spectra, they can resemble Type Ib supernovae, but are thought to have very different progenitors.\nType Ien has been proposed to explain observations of the supernova SN 2021yfj. Having lost its outer layers of hydrogen, helium and carbon, the star, just before the explosion, released an unusual, hidden layer of silicon, sulfur and argon, elements that are not often seen in dying stars. During the explosion, the material from the star's core collided with the gaseous shell, and the heat of the collision caused the silicon and sulfur layer to glow. The explosion showed that stars can be completely stripped down and still produce a brilliant explosion observable from very far distance. The discovery provided direct evidence of the long-theorized, but difficult to observe, internal structure of massive stars. In the type's name, \"e\" describes the position of the silicon/sulfur layer in the internal structure, while \"n\" signifies narrow emission lines.\nType II.\nThe supernovae of Type II can also be sub-divided based on their spectra. While most Type II supernovae show very broad emission lines which indicate expansion velocities of many thousands of kilometres per second, some, such as SN 2005gl, have relatively narrow features in their spectra. These are called Type IIn, where the \"n\" stands for \"narrow\".\nA few supernovae, such as SN 1987K and SN 1993J, appear to change types: they show lines of hydrogen at early times, but, over a period of weeks to months, become dominated by lines of helium. The term \"Type IIb\" is used to describe the combination of features normally associated with Type II and Type Ib.\nType II supernovae with normal spectra dominated by broad hydrogen lines that remain for the life of the decline are classified on the basis of their light curves. The most common type shows a distinctive \"plateau\" in the light curve shortly after peak brightness where the visual luminosity stays relatively constant for several months before the decline resumes. These are called Type II-P referring to the plateau. Less common are Type II-L supernovae that lack a distinct plateau. The \"L\" signifies \"linear\" although the light curve is not actually a straight line.\nSupernovae that do not fit into the normal classifications are designated peculiar, or \"pec\".\nTypes III, IV and V.\nZwicky defined additional supernovae types based on a very few examples that did not cleanly fit the parameters for Type I or Type II supernovae. SN 1961i in NGC 4303 was the prototype and only member of the Type III supernova class, noted for its broad light curve maximum and broad hydrogen Balmer lines that were slow to develop in the spectrum. SN 1961f in NGC 3003 was the prototype and only member of the Type IV class, with a light curve similar to a Type II-P supernova, with hydrogen absorption lines but weak hydrogen emission lines. The Type V class was coined for SN 1961V in NGC 1058, an unusual faint supernova or supernova impostor with a slow rise to brightness, a maximum lasting many months, and an unusual emission spectrum. The similarity of SN 1961V to the Eta Carinae Great Outburst was noted. Supernovae in M101 (1909) and M83 (1923 and 1957) were also suggested as possible Type IV or Type V supernovae.\nThese types would now all be treated as peculiar Type II supernovae (IIpec), of which many more examples have been discovered, although it is still debated whether SN 1961V was a true supernova following an LBV outburst or an impostor.\nCurrent models.\nSupernova type codes, as summarised in the table above, are \"taxonomic\": the type number is based on the light observed from the supernova, not necessarily its cause. For example, Type Ia supernovae are produced by runaway fusion ignited on degenerate white dwarf progenitors, while the spectrally similar Type Ib/c are produced from massive stripped progenitor stars by core collapse.\nThermal runaway.\nA white dwarf star may accumulate sufficient material from a stellar companion to raise its core temperature enough to ignite carbon fusion, at which point it undergoes runaway nuclear fusion, completely disrupting it. There are three avenues by which this detonation is theorised to happen: stable accretion of material from a companion, the collision of two white dwarfs, or accretion that causes ignition in a shell that then ignites the core. The dominant mechanism by which Type Ia supernovae are produced remains unclear. Despite this uncertainty in how Type Ia supernovae are produced, Type Ia supernovae have very uniform properties and are useful standard candles over intergalactic distances. Some calibrations are required to compensate for the gradual change in properties or different frequencies of abnormal luminosity supernovae at high redshift, and for small variations in brightness identified by light curve shape or spectrum.\nNormal Type Ia.\nThere are several means by which a supernova of this type can form, but they share a common underlying mechanism. If a carbon-oxygen white dwarf accreted enough matter to reach the Chandrasekhar limit of about 1.44 solar masses (for a non-rotating star), it would no longer be able to support the bulk of its mass through electron degeneracy pressure and would begin to collapse. However, the current view is that this limit is not normally attained; increasing temperature and density inside the core ignite carbon fusion as the star approaches the limit (to within about 1%) before collapse is initiated. In contrast, for a core primarily composed of oxygen, neon and magnesium, the collapsing white dwarf will typically form a neutron star. In this case, only a fraction of the star's mass will be ejected during the collapse.\nWithin a few seconds of the collapse process, a substantial fraction of the matter in the white dwarf undergoes nuclear fusion, releasing enough energy (1\u2013) to unbind the star in a supernova. An outwardly expanding shock wave is generated, with matter reaching velocities on the order of 5,000\u201320,000 km/s, or roughly 3% of the speed of light. There is also a significant increase in luminosity, reaching an absolute magnitude of \u221219.3 (or 5\u00a0billion times brighter than the Sun), with little variation.\nThe model for the formation of this category of supernova is a close binary star system. The more massive of the two stars is the first to evolve off the main sequence, and it expands to form a red giant. The two stars now share a common envelope, causing their mutual orbit to shrink. The giant star then sheds most of its envelope, losing mass until it can no longer continue nuclear fusion. At this point, it becomes a white dwarf star, composed primarily of carbon and oxygen. Eventually, the secondary star also evolves off the main sequence to form a red giant. Matter from the giant is accreted by the white dwarf, causing the latter to increase in mass. The exact details of initiation and of the heavy elements produced in the catastrophic event remain unclear.\nType Ia supernovae produce a characteristic light curve\u2014the graph of luminosity as a function of time\u2014after the event. This luminosity is generated by the radioactive decay of nickel-56 through cobalt-56 to iron-56. The peak luminosity of the light curve is extremely consistent across normal Type Ia supernovae, having a maximum absolute magnitude of about \u221219.3. This is because typical Type Ia supernovae arise from a consistent type of progenitor star by gradual mass acquisition, and explode when they acquire a consistent typical mass, giving rise to very similar supernova conditions and behaviour. This allows them to be used as a secondary standard candle to measure the distance to their host galaxies.\nA second model for the formation of Type Ia supernovae involves the merger of two white dwarf stars, with the combined mass momentarily exceeding the Chandrasekhar limit. This is sometimes referred to as the double-degenerate model, as both stars are degenerate white dwarfs. Due to the possible combinations of mass and chemical composition of the pair there is much variation in this type of event, and, in many cases, there may be no supernova at all, in which case they will have a less luminous light curve than the more normal SN Type Ia.\nNon-standard Type Ia.\nAbnormally bright Type Ia supernovae occur when the white dwarf already has a mass higher than the Chandrasekhar limit, possibly enhanced further by asymmetry, but the ejected material will have less than normal kinetic energy. This super-Chandrasekhar-mass scenario can occur, for example, when the extra mass is supported by differential rotation.\nThere is no formal sub-classification for non-standard Type Ia supernovae. It has been proposed that a group of sub-luminous supernovae that occur when helium accretes onto a white dwarf should be classified as Type Iax. This type of supernova may not always completely destroy the white dwarf progenitor and could leave behind a zombie star.\nOne specific type of supernova originates from exploding white dwarfs, like Type Ia, but contains hydrogen lines in their spectra, possibly because the white dwarf is surrounded by an envelope of hydrogen-rich circumstellar material. These supernovae have been dubbed Type Ia/IIn, Type Ian, Type IIa and Type IIan.\nThe quadruple star HD 74438, belonging to the open cluster IC 2391 the Vela constellation, has been predicted to become a non-standard Type Ia supernova.\nCore collapse.\nVery massive stars can undergo core collapse when nuclear fusion becomes unable to sustain the core against its own gravity; passing this threshold is the cause of all types of supernova except Type Ia. The collapse may cause violent expulsion of the outer layers of the star resulting in a supernova. However, if the release of gravitational potential energy is insufficient, the star may instead collapse into a black hole or neutron star with little radiated energy. \nCore collapse can be caused by several different mechanisms: exceeding the Chandrasekhar limit; electron capture; pair-instability; or photodisintegration.\nThe table below lists the known reasons for core collapse in massive stars, the types of stars in which they occur, their associated supernova type, and the remnant produced. The metallicity is the proportion of elements other than hydrogen or helium, as compared to the Sun. The initial mass is the mass of the star prior to the supernova event, given in multiples of the Sun's mass, although the mass at the time of the supernova may be much lower.\nType IIn supernovae are not listed in the table. They can be produced by various types of core collapse in different progenitor stars, possibly even by Type Ia white dwarf ignitions, although it seems that most will be from iron core collapse in luminous supergiants or hypergiants (including LBVs). The narrow spectral lines for which they are named occur because the supernova is expanding into a small dense cloud of circumstellar material. It appears that a significant proportion of supposed Type IIn supernovae are supernova impostors, massive eruptions of LBV-like stars similar to the Great Eruption of Eta Carinae. In these events, material previously ejected from the star creates the narrow absorption lines and causes a shock wave through interaction with the newly ejected material.\nDetailed process.\nWhen a stellar core is no longer supported against gravity, it collapses in on itself with velocities reaching 70,000\u00a0km/s (0.23\"c\"), resulting in a rapid increase in temperature and density. What follows depends on the mass and structure of the collapsing core, with low-mass degenerate cores forming neutron stars, higher-mass degenerate cores mostly collapsing completely to black holes, and non-degenerate cores undergoing runaway fusion.\nThe initial collapse of degenerate cores is accelerated by beta decay, photodisintegration and electron capture, which causes a burst of electron neutrinos. As the density increases, neutrino emission is cut off as they become trapped in the core. The inner core eventually reaches typically 30\u00a0km in diameter with a density comparable to that of an atomic nucleus, and neutron degeneracy pressure tries to halt the collapse. If the core mass is more than about 15 solar masses then neutron degeneracy is insufficient to stop the collapse and a black hole forms directly with no supernova.\nIn lower mass cores the collapse is stopped and the newly formed neutron core has an initial temperature of about 100\u00a0billion kelvins, 6,000 times the temperature of the Sun's core. At this temperature, neutrino-antineutrino pairs of all flavours are efficiently formed by thermal emission. These thermal neutrinos are several times more abundant than the electron-capture neutrinos. About 1046 joules, approximately 10% of the star's rest mass, is converted into a ten-second burst of neutrinos, which is the main output of the event. The suddenly halted core collapse rebounds and produces a shock wave that stalls in the outer core within milliseconds as energy is lost through the dissociation of heavy elements. A process that is not clearly understood[ [update]] is necessary to allow the outer layers of the core to reabsorb around 1044 joules (1 foe) from the neutrino pulse, producing the visible brightness, although there are other theories that could power the explosion.\nSome material from the outer envelope falls back onto the neutron star, and, for cores beyond about 8\u00a0M\u2609, there is sufficient fallback to form a black hole. This fallback will reduce the kinetic energy created and the mass of expelled radioactive material, but in some situations, it may also generate relativistic jets that result in a gamma-ray burst or an exceptionally luminous supernova.\nThe collapse of a massive non-degenerate core will ignite further fusion. When the core collapse is initiated by pair instability (photons turning into electron-positron pairs, thereby reducing the radiation pressure) oxygen fusion begins and the collapse may be halted. For core masses of 40\u201360\u00a0M\u2609, the collapse halts and the star remains intact, but collapse will occur again when a larger core has formed. For cores of around 60\u2013130\u00a0M\u2609, the fusion of oxygen and heavier elements is so energetic that the entire star is disrupted, causing a supernova. At the upper end of the mass range, the supernova is unusually luminous and extremely long-lived due to many solar masses of ejected 56Ni. For even larger core masses, the core temperature becomes high enough to allow photodisintegration and the core collapses completely into a black hole.\nType II.\nStars with initial masses less than about 8\u00a0M\u2609 never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least 9\u00a0M\u2609 (possibly as much as 12\u00a0M\u2609) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super-AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.\nIf core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a Type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a supernova of Type II. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.\nStars with an initial mass up to about 90 times the Sun, or a little less at high metallicity, result in a Type II-P supernova, which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a Type II-L supernova. At very low metallicity, stars of around 140\u2013250\u00a0M\u2609 will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with Type II characteristics but a very large mass of ejected 56Ni and high luminosity.\nType Ib and Ic.\nThese supernovae, like those of Type II, are massive stars that undergo core collapse. Unlike the progenitors of Type II supernovae, the stars which become Type Ib and Type Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf\u2013Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass-loss rates. Observations of Type Ib/c supernova do not match the observed or expected occurrence of Wolf\u2013Rayet stars. Alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed.\nType Ib supernovae are the more common and result from Wolf\u2013Rayet stars of type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining, and these are the progenitors of Type Ic supernovae.\nA few percent of the Type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped Type Ib or Ic supernova could produce a GRB, depending on the circumstances of the geometry. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell, producing a super-luminous supernova.\nUltra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (c. 0.1\u00a0M\u2609). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be the first observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core. Ultra-stripped supernovae are believed to be associated with the second supernova explosion in a binary system, producing for example a tight double neutron star system.\nIn 2022 a team of astronomers led by researchers from the Weizmann Institute of Science reported the first supernova explosion showing direct evidence for a Wolf-Rayet progenitor star. SN 2019hgp was a Type Icn supernova and is also the first in which the element neon has been detected.\nElectron-capture supernovae.\nIn 1980, a \"third type\" of supernova was predicted by Ken'ichi Nomoto of the University of Tokyo, called an electron-capture supernova. It would arise when a star \"in the transitional range (~8 to 10 solar masses) between white dwarf formation and iron core-collapse supernovae\", and with a degenerate O+Ne+Mg core, imploded after its core ran out of nuclear fuel, causing gravity to compress the electrons in the star's core into their atomic nuclei, leading to a supernova explosion and leaving behind a neutron star. In June 2021, a paper in the journal \"Nature Astronomy\" reported that the 2018 supernova SN 2018zd (in the galaxy NGC 2146, about 31 million light-years from Earth) appeared to be the first observation of an electron-capture supernova. The 1054 supernova explosion that created the Crab Nebula in our galaxy had been thought to be the best candidate for an electron-capture supernova, and the 2021 paper makes it more likely that this was correct.\nFailed supernovae.\nThe core collapse of some massive stars may not result in a visible supernova. This happens if the initial core collapse cannot be reversed by the mechanism that produces an explosion, usually because the core is too massive. These events are difficult to detect, but large surveys have detected possible candidates. The red supergiant N6946-BH1 in NGC 6946 underwent a modest outburst in March 2009, before fading from view. Only a faint infrared source remains at the star's location.\nLight curves.\nThe ejecta gases would dim quickly without some energy input to keep them hot. The source of this energy\u2014which can maintain the optical supernova glow for months\u2014was, at first, a puzzle. Some considered rotational energy from the central pulsar as a source. Although the energy that initially powers each type of supernovae is delivered promptly, the light curves are dominated by subsequent radioactive heating of the rapidly expanding ejecta. The intensely radioactive nature of the ejecta gases was first calculated on sound nucleosynthesis grounds in the late 1960s, and this has since been demonstrated as correct for most supernovae. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.\nIt is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a Type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of 56Ni through its daughters 56Co to 56Fe produces gamma-ray photons, primarily with energies of and , that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of 56Ni to 56Co (half-life 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of 56Co decaying to 56Fe. Later measurements by space gamma-ray telescopes of the small fraction of the 56Co and 57Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.\nThe late-time decay phase of visual light curves for different supernova types all depend on radioactive heating, but they vary in shape and amplitude because of the underlying mechanisms, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial event, but that breakout is hardly detectable optically.\nThe light curves for Type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half-life 6 days), which then decays to radioactive cobalt-56 (half-life 77 days). These radioisotopes excite the surrounding material to incandescence. Modern studies of cosmology rely on 56Ni radioactivity providing the energy for the optical brightness of supernovae of Type Ia, which are the \"standard candles\" of cosmology but whose diagnostic and gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half-life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission from the remaining cobalt-56 becomes dominant, although this portion of the light curve has been little-studied.\nType Ib and Ic light curves are similar to Type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional Type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous Type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.\nThe light curves for Type II supernovae are characterised by a much slower decline than Type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of Type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in Type I supernovae, due to the efficiency of conversion into light by all the hydrogen.\nIn Type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In Type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a Type I supernova and the hydrogen even disappears from the spectrum after several weeks.\nType IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.\nLarge numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.\nNotes:\nAsymmetry.\nA long-standing puzzle surrounding Type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high peculiar velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500\u00a0km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object remains[ [update]] a puzzle. Proposed explanations for this kick include convection in the collapsing star, asymmetric ejection of matter during neutron star formation, and asymmetrical neutrino emissions.\nOne possible explanation for this asymmetry is large-scale convection above the core. The convection can create radial variations in density giving rise to variations in the amount of energy absorbed from neutrino outflow. However analysis of this mechanism predicts only modest momentum transfer. Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is used for explaining long gamma-ray bursts.) The dominant mechanism may depend upon the mass of the progenitor star.\nInitial asymmetries have also been confirmed in Type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarisation of the emitted light.\nEnergy output.\nAlthough supernovae are primarily known as luminous events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.\nThere is a fundamental difference between the balance of energy production in the different types of supernova. In Type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.\nStandard Type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is 56Ni generated from silicon burning. 56Ni is radioactive and decays into 56Co by beta plus decay (with a half life of six days) and gamma rays. 56Co itself decays by the beta plus (positron) path with a half-life of 77 days into stable 56Fe. These two processes are responsible for the electromagnetic radiation from Type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.\nCore collapse supernovae are on average visually fainter than Type Ia supernovae, but the total energy released is far higher, as outlined in the following table.\nIn some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high-luminosity supernovae and is thought to be the cause of Type Ic hypernovae and long-duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low-luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.\nWhen a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause Type IIn hypernovae.\nAlthough pair-instability supernovae are core collapse supernovae with spectra and light curves similar to Type II-P, the nature after core collapse is more like that of a giant Type Ia with runaway fusion of carbon, oxygen and silicon. The total energy released by the highest-mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.\nProgenitor.\nThe supernova classification type is closely tied to the type of progenitor star at the time of the collapse. The occurrence of each type of supernova depends on the star's metallicity, since this affects the strength of the stellar wind and thereby the rate at which the star loses mass.\nType Ia supernovae are produced from white dwarf stars in binary star systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.\nType Ib and Ic supernovae are hypothesised to have been produced by core collapse of massive stars that have lost their outer layer of hydrogen and helium, either via strong stellar winds or mass transfer to a companion. They normally occur in regions of new star formation, and are extremely rare in elliptical galaxies. The progenitors of Type IIn supernovae also have high rates of mass loss in the period just prior to their explosions. Type Ic supernovae have been observed to occur in regions that are more metal-rich and have higher star-formation rates than average for their host galaxies. The table shows the progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.\nThere are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about 18\u00a0M\u2609 and 100,000\u00a0L\u2609, respectively. Most progenitors of Type II supernovae are not detected and must be considerably fainter, and presumably less massive. This discrepancy has been referred to as the red supergiant problem. It was first described in 2009 by Stephen Smartt, who also coined the term. After performing a volume-limited search for supernovae, Smartt et al. found the lower and upper mass limits for Type II-P supernovae to form to be M\u2609 and M\u2609, respectively. The former is consistent with the expected upper mass limits for white dwarf progenitors to form, but the latter is not consistent with massive star populations in the Local Group. The upper limit for red supergiants that produce a visible supernova explosion has been calculated at .\nIt is thought that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of Type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for Type IIb supernovae, and almost all Type IIb supernovae near enough to observe have shown such progenitors.\nBlue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf\u2013Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a Type IIn supernova. Several examples of hot luminous progenitors of Type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.\nThe progenitors of Type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed Type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal Type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly rotating massive stars, likely corresponding to the highly energetic Type Ic-BL events that are associated with long-duration gamma-ray bursts.\nExternal impact.\nSupernovae events generate heavier elements that are scattered throughout the surrounding interstellar medium. The expanding shock wave from a supernova can trigger star formation. Galactic cosmic rays are generated by supernova explosions.\nSource of heavy elements.\nSupernovae are a major source of elements in the interstellar medium from oxygen through to rubidium, though the theoretical abundances of the elements produced or seen in the spectra varies significantly depending on the various supernova types. Type Ia supernovae produce mainly silicon and iron-peak elements, metals such as nickel and iron. Core collapse supernovae eject much smaller quantities of the iron-peak elements than Type Ia supernovae, but larger masses of light alpha elements such as oxygen and neon, and elements heavier than zinc. The latter is especially true with electron capture supernovae. The bulk of the material ejected by Type II supernovae is hydrogen and helium. The heavy elements are produced by: nuclear fusion for nuclei up to 34S; silicon photodisintegration rearrangement and quasiequilibrium during silicon burning for nuclei between 36Ar and 56Ni; and rapid capture of neutrons (r-process) during the supernova's collapse for elements heavier than iron. The r-process produces highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. In supernovae, r-process reactions are responsible for about half of all the isotopes of elements beyond iron, although neutron star mergers may be the main astrophysical source for many of these elements.\nIn the modern universe, old asymptotic giant branch (AGB) stars are the dominant source of dust from oxides, carbon and s-process elements. However, in the early universe, before AGB stars formed, supernovae may have been the main source of dust.\nRole in stellar evolution.\nRemnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.\nThe Big Bang produced hydrogen, helium and traces of lithium, while all heavier elements are synthesised in stars, supernovae, and collisions between neutron stars (thus being indirectly due to supernovae). Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as \"metals\". These ejected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star's life, and may influence the possibility of having planets orbiting it: more giant planets form around stars of higher metallicity.\nThe kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.\nEvidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5\u00a0billion years ago, and may even have triggered the formation of this system.\nFast radio bursts (FRBs) are intense, transient pulses of radio waves that typically last no more than milliseconds. Many explanations for these events have been proposed; magnetars produced by core-collapse supernovae are leading candidates.\nCosmic rays.\nSupernova remnants are thought to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production has only been found in a small number of remnants. Gamma rays from pion-decay have been detected from the supernova remnants IC 443 and W44. These are produced when accelerated protons from the remnant impact on interstellar material.\nGravitational waves.\nSupernovae are potentially strong galactic sources of gravitational waves, but none have so far been detected. The only gravitational wave events so far detected are from mergers of black holes and neutron stars, probable remnants of supernovae. Like the neutrino emissions, the gravitational waves produced by a core-collapse supernova are expected to arrive without the delay that affects light. Consequently, they may provide information about the core-collapse process that is unavailable by other means. Most gravitational-wave signals predicted by supernova models are short in duration, lasting less than a second, and thus difficult to detect. Using the arrival of a neutrino signal may provide a trigger that can identify the time window in which to seek the gravitational wave, helping to distinguish the latter from background noise.\nEffect on Earth.\nA near-Earth supernova is a supernova close enough to the Earth to have noticeable effects on its biosphere. Depending upon the type and energy of the supernova, it could be as far as 3,000 light-years away.\nIn 1996 it was theorised that traces of past supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Iron-60 enrichment was later reported in deep-sea rock of the Pacific Ocean. In 2009, elevated levels of nitrate ions were found in Antarctic ice, which coincided with the 1006 and 1054 supernovae. Gamma rays from these supernovae could have boosted atmospheric levels of nitrogen oxides, which became trapped in the ice.\nHistorically, nearby supernovae may have influenced the biodiversity of life on the planet. Geological records suggest that nearby supernova events have led to an increase in cosmic rays, which in turn produced a cooler climate. A greater temperature difference between the poles and the equator created stronger winds, increased ocean mixing, and resulted in the transport of nutrients to shallow waters along the continental shelves. This led to greater biodiversity.\nType Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because these supernovae arise from dim, common white dwarf stars in binary systems, it is likely that a supernova that can affect the Earth will occur unpredictably and in a star system that is not well studied. The closest-known candidate is IK Pegasi (HR 8210), about 150 light-years away, but observations suggest that it could be as long as 1.9\u00a0billion years before the white dwarf can accrete the critical mass required to become a Type Ia supernova. \nAccording to a 2003 estimate, a Type II supernova would have to be closer than to destroy half of the Earth's ozone layer, and there are no such candidates closer than about 500 light-years.\nMilky Way candidates.\nThe next supernova in the Milky Way will likely be detectable even if it occurs on the far side of the galaxy. It is likely to be produced by the collapse of an unremarkable red supergiant, and it is very probable that it will already have been catalogued in infrared surveys such as 2MASS. There is a smaller chance that the next core collapse supernova will be produced by a different type of massive star such as a yellow hypergiant, luminous blue variable, or Wolf\u2013Rayet. The chances of the next supernova being a Type Ia produced by a white dwarf are calculated to be about a third of those for a core collapse supernova. Again it should be observable wherever it occurs, but it is less likely that the progenitor will ever have been observed. It is not even known exactly what a Type Ia progenitor system looks like, and it is difficult to detect them beyond a few parsecs. The total supernova rate in the Milky Way is estimated to be between 2 and 12 per century, although one has not actually been observed for several centuries.\nStatistically, the most common variety of core-collapse supernova is Type II-P, and the progenitors of this type are red supergiants. It is difficult to identify which of those supergiants are in the final stages of heavy element fusion in their cores and which have millions of years left. The most-massive red supergiants shed their atmospheres and evolve to Wolf\u2013Rayet stars before their cores collapse. All Wolf\u2013Rayet stars end their lives from the Wolf\u2013Rayet phase within a million years or so, but again it is difficult to identify those that are closest to core collapse. One class that is expected to have no more than a few thousand years before exploding are the WO Wolf\u2013Rayet stars, which are known to have exhausted their core helium. Only eight of them are known, and only four of those are in the Milky Way.\nA number of close or well-known stars have been identified as possible core collapse supernova candidates: the high-mass blue stars Spica, Rigel and Deneb, the red supergiants Betelgeuse, Antares, and VV Cephei A; the yellow hypergiant Rho Cassiopeiae; the luminous blue variable Eta Carinae that has already produced a supernova impostor; and both components, a blue supergiant and a Wolf\u2013Rayet star, of the Regor or Gamma Velorum system. Mimosa, Acrux and Hadar or Beta Centauri, three bright star systems in the southern constellation of Crux and Centaurus respectively, each contain blue stars with sufficient masses to explode as supernovae. Others have gained notoriety as possible, although not very likely, progenitors for a gamma-ray burst; for example WR 104.\nIdentification of candidates for a Type Ia supernova is much more speculative. Any binary with an accreting white dwarf might produce a supernova, although the exact mechanism and timescale is still debated. These systems are faint and difficult to identify, but the novae and recurrent novae are such systems that conveniently advertise themselves. One example is U\u00a0Scorpii.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27681", "revid": "7083768", "url": "https://en.wikipedia.org/wiki?curid=27681", "title": "Sergei Prokofiev", "text": "Russian composer and pianist (1891\u20131953)\nSergei Sergeyevich Prokofiev (27 April\u00a0[O.S. 15 April]\u00a01891\u00a0\u2013 5 March 1953) was a Russian composer, pianist, and conductor who later worked in the Soviet Union. As the creator of acknowledged masterpieces across numerous music genres, he is regarded as one of the major composers of the 20th century. His works include such widely heard pieces as the March from \"The Love for Three Oranges,\" the suite \"Lieutenant Kij\u00e9\", the ballet \"Romeo and Juliet\"\u2014from which \"Dance of the Knights\" is taken\u2014and \"Peter and the Wolf.\" Of the established forms and genres in which he worked, he created\u2014excluding juvenilia\u2014seven completed operas, seven symphonies, eight ballets, five piano concertos, two violin concertos, a cello concerto, a symphony-concerto for cello and orchestra, and nine completed piano sonatas.\nA graduate of the Saint Petersburg Conservatory, Prokofiev initially made his name as an iconoclastic composer-pianist, achieving notoriety with a series of ferociously dissonant and virtuosic works for his instrument, including his first two piano concertos. In 1915, Prokofiev made a decisive break from the standard composer-pianist category with his orchestral \"Scythian Suite\", compiled from music originally composed for a ballet commissioned by Sergei Diaghilev of the Ballets Russes. Diaghilev commissioned three further ballets from Prokofiev\u2014\"Chout,\" \"Le pas d'acier\" and \"The Prodigal Son\"\u2014which, at the time of their original production, all caused a sensation among both critics and colleagues. But Prokofiev's greatest interest was opera, and he composed several works in that genre, including \"The Gambler\" and \"The Fiery Angel\". Prokofiev's one operatic success during his lifetime was \"The Love for Three Oranges\", composed for the Chicago Opera and performed over the following decade in Europe and Russia.\nAfter the Revolution of 1917, Prokofiev left Russia with the approval of Soviet People's Commissar Anatoly Lunacharsky, and resided in the United States, then Germany, then Paris, making his living as a composer, pianist and conductor. In 1923 he married a Spanish singer, Carolina (Lina) Codina, with whom he had two sons; they divorced in 1947. In the early 1930s, the Great Depression diminished opportunities for Prokofiev's ballets and operas to be staged in America and Western Europe. Prokofiev, who regarded himself as a composer foremost, resented the time taken by touring as a pianist, and increasingly turned to the Soviet Union for commissions of new music; in 1936, he finally returned to his homeland with his family. His greatest Soviet successes included \"Lieutenant Kij\u00e9\", \"Peter and the Wolf\", \"Romeo and Juliet\", \"Cinderella\", \"Alexander Nevsky\", the Fifth and Sixth Symphonies, \"On Guard for Peace\", and the Piano Sonatas Nos. 6\u20138.\nThe Nazi invasion of the USSR spurred Prokofiev to compose his most ambitious work, an operatic version of Leo Tolstoy's \"War and Peace\"; he co-wrote the libretto with Mira Mendelson, his longtime companion and later second wife. In 1948, Prokofiev was attacked for producing \"anti-democratic formalism\". Nevertheless, he enjoyed personal and artistic support from a new generation of Russian performers, notably Sviatoslav Richter and Mstislav Rostropovich: he wrote his Ninth Piano Sonata for the former and his Symphony-Concerto for the latter.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nLife and career.\nChildhood and first compositions.\nProkofiev was born in 1891 in a rural estate in Sontsovka, Bakhmut uezd, Yekaterinoslav Governorate, Russian Empire (now known as Sontsivka, Pokrovsk Raion, Donetsk Oblast, Ukraine). His father, Sergei Alekseyevich Prokofiev, was an agronomist from a mercantile family in Moscow. Prokofiev's mother, Maria (n\u00e9e Zhitkova), came from a Saint Petersburg family of former serfs who had been owned by the Sheremetev family, under whose patronage serf-children were taught theatre and arts from an early age. She was described by Reinhold Gli\u00e8re, Prokofiev's first composition teacher, as \"a tall woman with beautiful, clever eyes \u2026 who knew how to create an atmosphere of warmth and simplicity about her.\" After their wedding in the summer of 1877, the Prokofievs moved to a small estate in the Smolensk governorate. Eventually, Sergei Alekseyevich found employment as a soil engineer, employed by one of his former fellow-students, Dmitri Sontsov, to whose estate in the Ukrainian steppes the Prokofievs moved.\nBy the time of Prokofiev's birth, Maria\u2014having previously lost two daughters\u2014had devoted her life to music; during her son's early childhood, she spent two months a year in Moscow or St Petersburg taking piano lessons. Sergei Prokofiev was inspired by hearing his mother practicing the piano in the evenings, mostly works by Chopin and Beethoven, and wrote his first piano composition at the age of five, an \"Indian Gallop\", which was written down by his mother: it was in the F Lydian mode (a major scale with a raised 4th scale degree), as the young Prokofiev felt \"reluctance to tackle the black notes\". By seven, he had also learned to play chess. Chess remained a passion of his, and he became acquainted with world chess champions Jos\u00e9 Ra\u00fal Capablanca, whom he beat in a simultaneous exhibition match in 1914, and Mikhail Botvinnik, with whom he played several matches in the 1930s. At the age of nine, he composed his first opera, \"The Giant\", as well as an overture and various other pieces. Opera remained thereafter as the genre Prokofiev was most fond of working in.\nEducation and early works.\nIn 1902, Prokofiev's mother met Sergei Taneyev, director of the Moscow Conservatory, who initially suggested that Prokofiev should start lessons in piano and composition with Alexander Goldenweiser. Unable to arrange that, Taneyev instead arranged for composer and pianist Reinhold Gli\u00e8re to spend the summer of 1902 in Sontsovka teaching Prokofiev. The first series of lessons culminated, at the 11-year-old Prokofiev's insistence, with the budding composer making his first attempt to write a symphony. The following summer, Gli\u00e8re revisited Sontsovka to give further tuition. When, decades later, Prokofiev wrote about his lessons with Gli\u00e8re, he gave due credit to his teacher's sympathetic method but complained that Gli\u00e8re had introduced him to \"square\" phrase structure and conventional modulations, which he subsequently had to unlearn. Nonetheless, equipped with the necessary theoretical tools, Prokofiev started experimenting with dissonant harmonies and unusual time signatures in a series of short piano pieces he called \"ditties\" (after the so-called \"song form\", more accurately ternary form, on which they were based), laying the basis for his own musical style.\nDespite his growing talent, Prokofiev's parents hesitated over starting their son on a musical career at such an early age, and considered the possibility of his attending a good high school in Moscow. By 1904, his mother had decided instead on Saint Petersburg, and she and Prokofiev visited the then capital to explore the possibility of moving there for his education. They were introduced to composer Alexander Glazunov, a professor at the Saint Petersburg Conservatory, who asked to see Prokofiev and his music; Prokofiev had composed two more operas, \"Desert Islands\" and \"The Feast during the Plague\", and was working on his fourth, \"Undina\". Glazunov was so impressed that he urged Prokofiev's mother to have her son apply for admission to the Conservatory. He passed the introductory tests and enrolled that year.\nSeveral years younger than most of his class, Prokofiev was viewed as eccentric and arrogant and annoyed a number of his classmates by keeping statistics on their errors. During that period, he studied under, among others, Alexander Winkler for piano, Anatoly Lyadov for harmony and counterpoint, Nikolai Tcherepnin for conducting, and Nikolai Rimsky-Korsakov for orchestration (though when Rimsky-Korsakov died in 1908, Prokofiev noted that he had only studied with him \"after a fashion\"\u2014he was just one of many students in a heavily attended class\u2014and regretted that he otherwise \"never had the opportunity to study with him\"). He also shared classes with the composers Boris Asafyev and Nikolai Myaskovsky, the latter becoming a close and lifelong friend.\nAs a member of the Saint Petersburg music scene, Prokofiev developed a reputation as a musical rebel, while getting praise for his original compositions, which he performed himself on the piano. In 1909, he graduated from his class in composition with unimpressive marks. He continued at the Conservatory, studying piano under Anna Yesipova and continuing his conducting lessons under Tcherepnin.\nIn 1910, Prokofiev's father died and Sergei's financial support ceased. Fortunately, he had started making a name for himself as a composer and pianist outside the Conservatory, making appearances at the St Petersburg Evenings of Contemporary Music. There he performed several of his more adventurous piano works, such as his highly chromatic and dissonant Etudes, Op. 2 (1909). His performance of it impressed the organisers of the Evenings sufficiently for them to invite Prokofiev to give the Russian premiere of Arnold Schoenberg's Drei Klavierst\u00fccke, Op. 11. Prokofiev's harmonic experimentation continued with \"Sarcasms\" for piano, Op. 17 (1912), which makes extensive use of polytonality. He composed his first two piano concertos around then, the latter of which caused a scandal at its premiere (23 August 1913, Pavlovsk). According to one account, the audience left the hall with exclamations of \"'To hell with this futuristic music! The cats on the roof make better music!'\", but the modernists were in rapture.\nIn 1911, help arrived from renowned Russian musicologist and critic Alexander Ossovsky, who wrote a supportive letter to music publisher Boris P. Jurgenson (son of publishing-firm founder Peter Jurgenson [1836\u20131904]); thus a contract was offered to the composer. Prokofiev made his first foreign trip in 1913, travelling to Paris and London where he first encountered Sergei Diaghilev's Ballets Russes.\nFirst ballets.\nIn 1914, Prokofiev finished his career at the Conservatory by entering the 'battle of the pianos', a competition open to the five best piano students for which the prize was a Schroeder grand piano; Prokofiev won by performing his own Piano Concerto No. 1.\nSoon afterwards, he journeyed to London where he made contact with the impresario Sergei Diaghilev. Diaghilev commissioned Prokofiev's first ballet, \"Ala and Lolli\"; but when Prokofiev brought the work in progress to him in Italy in 1915 he rejected it as \"non-Russian\". Urging Prokofiev to write \"music that was national in character\", Diaghilev then commissioned the ballet \"Chout\" (\"The Buffoon\"). (The original Russian-language full title was \u0421\u043a\u0430\u0437\u043a\u0430 \u043f\u0440\u043e \u0448\u0443\u0442\u0430, \u0441\u0435\u043c\u0435\u0440\u044b\u0445 \u0448\u0443\u0442\u043e\u0432 \u043f\u0435\u0440\u0435\u0448\u0443\u0442\u0438\u0432\u0448\u0435\u0433\u043e, meaning \"The Tale of the Buffoon who Outwits Seven Other Buffoons\".) Under Diaghilev's guidance, Prokofiev chose his subject from a collection of folk tales by the ethnographer Alexander Afanasyev; the story, concerning a buffoon and a series of confidence tricks, had been previously suggested to Diaghilev by Igor Stravinsky as a possible subject for a ballet, and Diaghilev and his choreographer L\u00e9onide Massine helped Prokofiev to shape it into a ballet scenario. Prokofiev's inexperience with ballet led him to revise the work extensively in the 1920s, following Diaghilev's detailed critique, prior to its first production.\nThe ballet's premiere in Paris on 17 May 1921 was a huge success and was greeted with great admiration by an audience that included Jean Cocteau, Igor Stravinsky and Maurice Ravel. Stravinsky called the ballet \"the single piece of modern music he could listen to with pleasure\", while Ravel called it \"a work of genius\".\nFirst World War and Revolution.\nDuring World War I, Prokofiev returned to the Conservatory and studied organ to avoid conscription. In 1916, he debuted his \"Toccata\". Shortly after this he composed \"The Gambler\" based on Fyodor Dostoyevsky's novel of the same name, but rehearsals were plagued by problems, and the scheduled 1917 premi\u00e8re had to be cancelled because of the February Revolution. In the summer of that year, Prokofiev composed his first symphony, the \"Classical\". The name was Prokofiev's own; the music is in a style that, according to Prokofiev, Joseph Haydn would have used if he were alive at the time. The music is more or less Classical in style but incorporates more modern musical elements (see Neoclassicism).\nThe symphony was also an exact contemporary of Prokofiev's Violin Concerto No. 1 in D major, Op. 19, which was scheduled to premiere in November 1917. The first performances of both works had to wait until 21 April 1918 and 18 October 1923, respectively. Prokofiev stayed briefly with his mother in Kislovodsk in the Caucasus.\nAfter completing the score of \"Seven, They Are Seven\", a \"Chaldean invocation\" for chorus and orchestra, Prokofiev was \"left with nothing to do and time hung heavily on [his] hands\". Believing that Russia \"had no use for music at the moment\", Prokofiev decided to try his fortunes in America until the turmoil in his homeland had passed. He set out for Moscow and Petersburg in March 1918 to sort out financial matters and to arrange for his passport. In May, he headed for the US, having obtained official permission to do so from Anatoly Lunacharsky, the People's Commissar for Education, who told him: \"You are a revolutionary in music, we are revolutionaries in life. We ought to work together. But if you want to go to America I shall not stand in your way.\"\nLife abroad.\nArriving in San Francisco after having been released from questioning by immigration officials on Angel Island on 11 August 1918, Prokofiev was soon compared to other famous Russian exiles, such as Sergei Rachmaninoff. His debut solo concert in New York led to several further engagements. He also received a contract from the music director of the Chicago Opera Association, Cleofonte Campanini, for the production of his new opera \"The Love for Three Oranges\", but due to Campanini's illness and death, the premiere was postponed. The delay was another example of Prokofiev's bad luck in operatic matters. The failure also cost him his American solo career since the opera took too much time and effort. He soon found himself in financial difficulties, and in April 1920, he left for Paris, not wanting to return to Russia as a failure.\nIn Paris, Prokofiev reaffirmed his contacts with Diaghilev's Ballets Russes. He also completed some of his older, unfinished works, such as his Third Piano Concerto. \"The Love for Three Oranges\" finally premi\u00e8red in Chicago, under the composer's baton, on 30 December 1921. Diaghilev became sufficiently interested in the opera to request Prokofiev play the vocal score to him in June 1922, while they were both in Paris for a revival of \"Chout\", so he could consider it for a possible production. Stravinsky, who was present at the audition, refused to listen to more than the first act. When he then accused Prokofiev of \"wasting time composing operas\", Prokofiev retorted that Stravinsky \"was in no position to lay down a general artistic direction, since he is himself not immune to error\". According to Prokofiev, Stravinsky \"became incandescent with rage\" and \"we almost came to blows and were separated only with difficulty\". As a result, \"our relations became strained and for several years Stravinsky's attitude toward me was critical.\"In March 1922, Prokofiev moved with his mother to the town of Ettal in the Bavarian Alps, where for over a year he concentrated on an opera project, \"The Fiery Angel\", based on the novel by Valery Bryusov. His later music had acquired a following in Russia, and he received invitations to return there, but decided to stay in Western Europe. In 1923, Prokofiev married the Spanish singer Carolina Codina (1897\u20131989, stage name Lina Llubera) before moving back to Paris.\nIn Paris, several of his works, including the Second Symphony, were performed, but their reception was lukewarm and Prokofiev sensed that he \"was evidently no longer a sensation\". Still, the Symphony appeared to prompt Diaghilev to commission \"Le pas d'acier\" (\"The Steel Step\"), a \"modernist\" ballet score intended to portray the industrialisation of the Soviet Union. It was enthusiastically received by Parisian audiences and critics.\nAround 1924, Prokofiev was introduced to Christian Science. He began to practice its teachings, which he believed to be beneficial to his health and to his fiery temperament and to which he remained faithful for the rest of his life, according to biographer Simon Morrison.\nProkofiev and Stravinsky restored their friendship, though Prokofiev particularly disliked Stravinsky's \"stylization of Bach\" in such recent works as the Octet and the Concerto for Piano and Wind Instruments. For his part, Stravinsky described Prokofiev as the greatest Russian composer of his day, after himself.\nFirst visits to the Soviet Union.\nProkofiev met Boris Krasin in the violinist Joseph Szigeti's Paris apartment in 1924. In 1927, Prokofiev made his first concert tour in the Soviet Union. Over more than two months, he spent time in Moscow and Leningrad (as St Petersburg had been renamed), where he enjoyed a very successful staging of \"The Love for Three Oranges\" in the Mariinsky Theatre. In 1928, Prokofiev completed his Third Symphony, which was broadly based on his unperformed opera \"The Fiery Angel\". The conductor Serge Koussevitzky characterized the Third as \"the greatest symphony since Tchaikovsky's Sixth\".\nIn the meantime, under the influence of the teachings of Christian Science, Prokofiev had turned against the expressionist style and the subject matter of \"The Fiery Angel\". He now preferred what he called a \"new simplicity\", which he found more sincere than the \"contrivances and complexities\" of so much modern music of the 1920s. In 1928\u201329, Prokofiev composed his last ballet for Diaghilev, \"The Prodigal Son\". When first staged in Paris on 21 May 1929, choreographed by George Balanchine with Serge Lifar in the title role, the audience and critics were particularly struck by the final scene, in which the prodigal son drags himself across the stage on his knees to be welcomed by his father. Diaghilev had recognised that in the music to the scene, Prokofiev had \"never been more clear, more simple, more melodious, and more tender\". Only months later, Diaghilev died.\nThat summer, Prokofiev completed the Divertimento, Op. 43 (which he had started in 1925) and revised his Sinfonietta, Op. 5/48, a work started in his days at the Conservatory. In October of that year, he had a car crash while driving his family back to Paris from their holiday: as the car turned over, Prokofiev pulled some muscles on his left hand. Prokofiev was therefore unable to perform in Moscow during his tour shortly after the accident, but he was able to enjoy watching performances of his music from the audience. Prokofiev also attended the Bolshoi Theatre's \"audition\" of his ballet \"Le pas d'acier\", and was interrogated by members of the Russian Association of Proletarian Musicians (RAPM) about the work: he was asked whether the factory portrayed \"a capitalist factory, where the worker is a slave, or a Soviet factory, where the worker is the master? If it is a Soviet factory, when and where did Prokofiev examine it, since from 1918 to the present he has been living abroad and came here for the first time in 1927 for two weeks [sic]?\" Prokofiev replied, \"That concerns politics, not music, and therefore I won't answer.\" The RAPM condemned the ballet as a \"flat and vulgar anti-Soviet anecdote, a counter-revolutionary composition bordering on Fascism\". The Bolshoi had no option but to reject the ballet.\nWith his left hand healed, Prokofiev toured the United States successfully at the start of 1930, propped up by his recent European success. That year, Prokofiev began his first non-Diaghilev ballet \"On the Dnieper\", Op. 51, a work commissioned by Serge Lifar, who had been appointed \"maitre de ballet\" at the Paris Op\u00e9ra. In 1931 and 1932, he completed his fourth and fifth piano concertos. The following year saw the completion of the Symphonic Song, Op. 57, which Prokofiev's friend Myaskovsky\u2014thinking of its potential audience in the Soviet Union\u2014told him \"isn't quite for us\u2026 it lacks that which we mean by monumentalism\u2014a familiar simplicity and broad contours, of which you are extremely capable, but temporarily are carefully avoiding.\"\nBy the early 1930s, both Europe and America were suffering from the Great Depression, which inhibited both new opera and ballet productions, though audiences for Prokofiev's appearances as a pianist were, in Europe at least, undiminished. But Prokofiev saw himself as a composer first and foremost, and increasingly resented the time lost to composition through his appearances as a pianist. Having been homesick for some time, Prokofiev began to build substantial bridges with the Soviet Union.\nFollowing the dissolution of the RAPM in 1932, he acted increasingly as a musical ambassador between his homeland and western Europe, and his premieres and commissions were increasingly under the auspices of the Soviet Union. One such was \"Lieutenant Kij\u00e9\", which was commissioned as the score to a Soviet film.\nAnother commission, from the Kirov Theatre (as the Mariinsky had now been renamed) in Leningrad, was the ballet \"Romeo and Juliet\", composed to a scenario created by Adrian Piotrovsky and Sergei Radlov following the precepts of \"drambalet\" (dramatised ballet, officially promoted at the Kirov to replace works based primarily on choreographic display and innovation). Following Radlov's acrimonious resignation from the Kirov in June 1934, a new agreement was signed with the Bolshoi Theatre in Moscow on the understanding that Piotrovsky would remain involved. But the ballet's original happy ending (contrary to Shakespeare) provoked controversy among Soviet cultural officials, and the ballet's production was postponed indefinitely when the staff of the Bolshoi was overhauled at the behest of the chairman of the Committee on Arts Affairs, Platon Kerzhentsev.\nReturn to Russia.\nIn 1936, Prokofiev and his family settled permanently in Moscow, after shifting back and forth between Moscow and Paris for the previous four years.\nThat year, Prokofiev composed one of his most famous works, \"Peter and the Wolf\", for Natalya Sats' Central Children's Theatre. Sats also persuaded him to write two songs for children, \"Sweet Song\", and \"Chatterbox\"; they were eventually joined by \"The Little Pigs\" and published as \"Three Children's Songs\", Op. 68. Prokofiev also composed the gigantic \"Cantata for the 20th Anniversary of the October Revolution,\" originally intended for performance during the anniversary year but effectively blocked by Kerzhentsev, who demanded at the work's audition before the Committee on Arts Affairs, \"Just what do you think you're doing, Sergey Sergeyevich, taking texts that belong to the people and setting them to such incomprehensible music?\" The Cantata was not performed until 5 April 1966, just over 13 years after the composer's death.\nForced to adapt to the new circumstances (whatever private misgivings he had about them), Prokofiev wrote a series of \"mass songs\" (Opp. 66, 79, 89), using the lyrics of officially approved Soviet poets. In 1938, he collaborated with Eisenstein on the historical epic \"Alexander Nevsky\", composing some of his most inventive and dramatic music. Although the film had very poor sound recording, Prokofiev adapted much of his score into a large-scale cantata for mezzo-soprano, orchestra and chorus, which was extensively performed and recorded. In the wake of \"Alexander Nevsky\"'s success, Prokofiev composed his first Soviet opera, \"Semyon Kotko\", which was intended to be produced by the director Vsevolod Meyerhold. The opera's premi\u00e8re was postponed because Meyerhold was arrested on 20 June 1939 by the NKVD, and shot on 2 February 1940. At the end of the same year, Prokofiev was commissioned to compose \"Zdravitsa\" (literally \"Cheers!\", but sometimes subtitled \"Hail to Stalin\" in English) (Op. 85) to celebrate Joseph Stalin's 60th birthday.\nLater in 1939, Prokofiev composed his Piano Sonatas Nos. 6, 7, and 8, Opp. 82\u201384, widely known today as the \"War Sonatas\". Premiered respectively by Prokofiev (No. 6: 8 April 1940), Sviatoslav Richter (No. 7: Moscow, 18 January 1943) and Emil Gilels (No. 8: Moscow, 30 December 1944), they were subsequently championed in particular by Richter. Biographer Daniel Jaff\u00e9 argued that Prokofiev, \"having forced himself to compose a cheerful evocation of the nirvana Stalin wanted everyone to believe he had created\" (i.e. in \"Zdravitsa\") then subsequently, in the three sonatas, \"expressed his true feelings\". As evidence, Jaff\u00e9 has pointed out that the central movement of Sonata No. 7 opens with a theme based on the Robert Schumann lied \"Wehmut\" (\"Sadness\", from the \"Liederkreis\", Op. 39): its words translate, \"I can sometimes sing as if I were glad, yet secretly tears well and so free my heart. Nightingales \u2026 sing their song of longing from their dungeon's depth \u2026 everyone delights, yet no one feels the pain, the deep sorrow in the song.\" Sonata No. 7 received a Stalin Prize (Second Class) and No. 8 a Stalin Prize (First Class).\nMeanwhile, \"Romeo and Juliet\" was staged by the Kirov Ballet, choreographed by Leonid Lavrovsky, on 11 January 1940. To the surprise of all of its participants, the dancers having struggled to cope with the music's syncopated rhythms and almost having boycotted the production, the ballet was an instant success and became recognised as the crowning achievement of Soviet dramatic ballet.\nWar years.\nProkofiev had been considering making an opera out of Leo Tolstoy's epic novel \"War and Peace\", when news of the German invasion of the Soviet Union on 22 June 1941 made the subject seem all the more timely. Because of the war, he was evacuated together with a large number of other artists, initially to the Georgian SSR, where he lived in Tbilisi from 11 November 1941 until 29 June 1942. While there he began to compose the original version of \"War and Peace\". While in the Georgian SSR he also composed his Second String Quartet and Piano Sonata No. 7. His relationship with the 25-year-old writer and librettist Mira Mendelson had finally led to his separation from his wife Lina. Despite their acrimonious separation, Prokofiev tried to persuade Lina and their sons to accompany him as evacuees out of Moscow, but Lina opted to stay.\nDuring the war years, restrictions on style and the demand that composers write in a 'socialist realist' style were slackened, and Prokofiev was generally able to compose in his own way. The Violin Sonata No. 1, Op. 80, \"The Year 1941\", Op. 90, and the \"Ballade for the Boy Who Remained Unknown\", Op. 93 all came from this period. In 1943, Prokofiev joined Eisenstein in Alma-Ata, the largest city in Kazakhstan, to compose more film music (\"Ivan the Terrible\"), and the ballet \"Cinderella\" (Op. 87), one of his most melodious and celebrated compositions. Early that year, he also played excerpts from \"War and Peace\" to members of the Bolshoi Theatre collective, but the Soviet government had opinions about the opera that resulted in many revisions. In 1944, Prokofiev composed his Fifth Symphony (Op. 100) at a composer's colony outside Moscow. He conducted its first performance on 13 January 1945, just a fortnight after the triumphant premieres on 30 December 1944 of his Eighth Piano Sonata and, on the same day, the first part of Eisenstein's \"Ivan the Terrible\". With the premiere of his Fifth Symphony, which was programmed alongside \"Peter and the Wolf\" and the \"Classical\" Symphony (conducted by Nikolai Anosov), Prokofiev appeared to reach the peak of his celebrity as a leading Soviet composer.\nOn 20 January 1945, Prokofiev suffered a concussion after fainting in his apartment due to untreated chronic hypertension. The composer Dmitry Kabalevsky visited him in hospital and found him semi-conscious, and \"with a heavy heart, I left him, I thought it was the end.\" He never fully recovered from the injury, and, following medical advice, restricted his composing activity.\nPostwar.\nProkofiev had time to write his postwar Sixth Symphony and his Ninth Piano Sonata (for Sviatoslav Richter) before the so-called \"Zhdanov Doctrine\". On the day before the decree was published, 10 February 1948, Prokofiev was at a ceremony in the Kremlin to mark his elevation to the status of People's Artist of the RSFSR.\nThe decree followed a three-day conference of more than 70 composers, musicians and music lecturers convened on 10 January, presided over by Zhdanov. Prokofiev was berated by a minor composer, Viktor Bely, who accused him of \"innovation for innovation's sake\" and \"artistic snobbishness\", but unlike Dmitri Shostakovich, Aram Khachaturian and others, Prokofiev gave no speech. His silence set off rumors that he had been deliberately defiant and uncooperative. There is no official record, but according to a variety of witnesses, Prokofiev did not attend on the first day, and had to be fetched, arriving on day two wearing a brown suit and baggy-kneed trousers tucked into his felt boots. Ilya Ehrenburg, who was not in the hall, claimed in his memoirs that Prokofiev fell asleep, woke up suddenly and loudly asked who Zhdanov was. The cellist Mstislav Rostropovich heard that Prokofiev was chatting to the person next to him when a senior figure sitting nearby warned him to be quiet. Prokofiev asked: \"Who are you?\" The official said that his name did not matter, but that Prokofiev had better pay attention to him, to which Prokofiev retorted: \"I never pay attention to comments from people who haven't been introduced to me.\" This possibly apocryphal story was corroborated by the head of the composers' union, Tikhon Khrennikov, who said that the person Prokofiev snubbed was the Stalinist official Matvei Shkiryatov.\nThe decree, published on 11 February, denounced six artists\u2014Shostakovich, Prokofiev, Khachaturian, Shebalin, Popov, and Myaskovsky, in that order\u2014for the crime of \"formalism\", described as a \"renunciation of the basic principles of classical music\" in favor of \"muddled, nerve-racking\" sounds that \"turned music into cacophony\". Eight of Prokofiev's works were banned from performance: \"The Year 1941\", \"Ode to the End of the War\", \"Festive Poem\", \"Cantata for the Thirtieth Anniversary of October\", \"Ballad of an Unknown Boy\", the 1934 piano cycle \"Thoughts\", and Piano Sonatas Nos. 6 and 8. Such was the perceived threat behind the banning of the works that even works that had avoided censure were no longer programmed. By August 1948, Prokofiev was in severe financial straits, his personal debt amounting to 180,000 rubles.\nOn 22 November 1947, Prokofiev filed a petition in court to begin divorce proceedings against his estranged wife. Five days later the court ruled that the marriage had no legal basis since it had taken place in Germany, and had not been registered with Soviet officials, thus making it null and void. After a second judge upheld the verdict, he and his partner Mira wed on 13 January 1948. On 20 February 1948, Prokofiev's first wife Lina was arrested and charged with espionage for trying to send money to her mother in Spain. After nine months of interrogation, she was sentenced by a three-member Military Collegium of the Supreme Court of the USSR to 20 years of hard labor. She was released eight years later on 30 June 1956 and in 1974 left the Soviet Union.\nProkofiev's latest opera projects, among them his desperate attempt to appease the cultural authorities, \"The Story of a Real Man\", were quickly cancelled by the Kirov Theatre. The snub, in combination with his declining health, caused Prokofiev to progressively withdraw from public life and from various activities, even chess, and increasingly devote himself to his own work. After he had a stroke on 7 July 1949, his doctors ordered him to limit his composing to an hour a day.\nIn spring 1949, Prokofiev wrote his Cello Sonata in C major, Op. 119, for the 22-year-old Mstislav Rostropovich, who gave the first performance in 1950, with Sviatoslav Richter. For Rostropovich, Prokofiev also extensively recomposed his Cello Concerto, transforming it into a Symphony-Concerto, a landmark in the cello and orchestra repertory today. The last public performance he attended, on 11 October 1952, was the premi\u00e8re of the Seventh Symphony, his last completed work. The symphony was written for the Children's Radio Division.\nDeath.\nProkofiev died of hypertensive crisis at age 61 on 5 March 1953, the same day as Joseph Stalin. He had lived in a communal apartment on Chamberlain Lane next to the Red Square, and for three days throngs gathered to mourn Stalin, making it impossible to hold Prokofiev's funeral service at the headquarters of the Soviet Composers' Union. Because the hearse was not allowed near Prokofiev's house, his coffin had to be moved by hand through back streets in the opposite direction of the masses of people going to visit Stalin's body. About 30 people attended the funeral, Shostakovich among them. Although they had not seemed to get along when they met, in the later years their interactions had become far more amicable, with Shostakovich writing to Prokofiev, \"I wish you at least another hundred years to live and create. Listening to such works as your Seventh Symphony makes it much easier and more joyful to live.\" Prokofiev is buried in Moscow's Novodevichy Cemetery.\nThe leading Soviet musical periodical reported Prokofiev's death as a brief item on page 116 (the first 115 pages were devoted to Stalin's death). Prokofiev's death is usually attributed to cerebral hemorrhage. He had been chronically ill for eight years.\nProkofiev's wife Mira Mendelson spent her final years living in the Moscow apartment they had shared. She occupied her time organizing her husband's papers, promoting his music, and writing her memoirs, having been strongly encouraged by Prokofiev to embark on the latter. Work on the memoirs was difficult for her; she left them incomplete at her death. Mendelson died of a heart attack in Moscow in 1968, 15 years after Prokofiev. Inside her purse a message dated February 1950 and signed by Prokofiev and Mendelson instructed: \"We wish to be buried next to each other.\" Their remains are buried together at Novodevichy Cemetery.\nLina Prokofiev outlived her ex-husband by many years, dying in London in early 1989. Royalties from his music provided her with a modest income, and she acted as storyteller for a recording of her husband's \"Peter and the Wolf\" (released on CD by Chandos Records) with Neeme J\u00e4rvi conducting the Scottish National Orchestra. Their sons Sviatoslav (1924\u20132010), an architect, and Oleg (1928\u20131998), an artist, painter, sculptor and poet, dedicated much of their lives to promoting their father's work.\nLegacy.\nReputation.\nArthur Honegger said that Prokofiev would \"remain for us the greatest figure of contemporary music\", and the American scholar Richard Taruskin wrote of Prokofiev's \"gift, virtually unparalleled among 20th-century composers, for writing distinctively original diatonic melodies\". Yet for some time Prokofiev's reputation in the West suffered as a result of Cold War antipathies, and his music has never won from Western academics and critics the same esteem as Igor Stravinsky's and Arnold Schoenberg's, which had greater influence on younger musicians. Nonetheless, his unique approach to composition (which he reflected on significantly in autobiographical writings) continues to receive steady musicological examination.\nIn Donetsk Oblast, the Donetsk State Music Academy Named After Sergei Prokofiev and Donetsk Sergei Prokofiev International Airport are named in Prokofiev's honor. The latter facility was destroyed in 2014 during the First and Second Battle of Donetsk Airport.\nThe All-Ukrainian open pianists' competition named after Prokofiev is held annually in Kyiv and comprises three categories: piano, composition, and conducting.\nRecordings.\nProkofiev was a soloist with the London Symphony Orchestra, conducted by Piero Coppola, in the first recording of his Piano Concerto No. 3, recorded in London by His Master's Voice in June 1932. Prokofiev also recorded some of his solo piano music for HMV in Paris in February 1935; these recordings were issued on CD by Pearl and Naxos. In 1938, he conducted the Moscow Philharmonic Orchestra in a recording of the second suite from his \"Romeo and Juliet\" ballet; this performance was later released on LP and CD. \nA short sound film has been discovered of Prokofiev playing some of the music from his opera \"War and Peace\" and then explaining the music.\n (1943), 2nd degree \u2013 for Piano Sonata No. 7\n (1946), 1st degree \u2013 for Symphony No. 5 and Piano Sonata No. 8\n (1946), 1st degree \u2013 for the music for the film \"Ivan the Terrible\" Part 1 (1944)\n (1946), 1st degree \u2013 for the ballet \"Cinderella\" (1944)\n (1947), 1st degree \u2013 for Violin Sonata No. 1\n (1951), 2nd degree \u2013 for vocal-symphonic suite \"Winter Bonfire\" and the oratorio \"On Guard for Peace\" on poems by Samuil Marshak\nWorks.\nImportant works include (in chronological order):\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nWritings.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27683", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=27683", "title": "Satellite", "text": "Objects intentionally placed into orbit\nA satellite or an artificial satellite is an object, typically a spacecraft, placed into orbit around a celestial body. They have a variety of uses, including communication relay, weather forecasting, navigation (GPS), broadcasting, scientific research, and Earth observation. Additional military uses are reconnaissance, early warning, signals intelligence and, potentially, weapon delivery. Other satellites include the final rocket stages that place satellites in orbit and formerly useful satellites that later become defunct.\nExcept for passive satellites, most satellites have an electricity generation system for equipment on board, such as solar panels or radioisotope thermoelectric generators (RTGs). Most satellites also have a method of communication to ground stations, called transponders. Many satellites use a standardized bus to save cost and work, the most popular of which are small CubeSats. Similar satellites can work together as groups, forming constellations. Because of the high launch cost to space, most satellites are designed to be as lightweight and robust as possible. Most communication satellites are radio relay stations in orbit and carry dozens of transponders, each with a bandwidth of tens of megahertz.\nSpaceships become satellites by accelerating or decelerating to reach orbital velocities, occupying an orbit high enough to avoid orbital decay due to drag in the presence of an atmosphere and above their Roche limit. Satellites are spacecraft launched from the surface into space by launch systems. Satellites can then change or maintain their orbit by propulsion, usually by chemical or ion thrusters. As of 2018, about 90% of the satellites orbiting the Earth are in low Earth orbit or geostationary orbit; geostationary means the satellites stay still in the sky (relative to a fixed point on the ground). Some imaging satellites choose a Sun-synchronous orbit because they can scan the entire globe with similar lighting. As the number of satellites and amount of space debris around Earth increases, the threat of collision has become more severe. An orbiter is a spacecraft that is designed to perform an orbital insertion, entering orbit around an astronomical body from another, and as such becoming an artificial satellite. A small number of satellites orbit other bodies (such as the Moon, Mars, and the Sun) or many bodies at once (two for a halo orbit, three for a Lissajous orbit).\nEarth observation satellites gather information for reconnaissance, mapping, monitoring the weather, ocean, forest, etc. Space telescopes take advantage of outer space's near perfect vacuum to observe objects with the entire electromagnetic spectrum. Because satellites can see a large portion of the Earth at once, communications satellites can relay information to remote places. The signal delay from satellites and their orbit's predictability are used in satellite navigation systems, such as GPS. Crewed spacecrafts which are in orbit or remain in orbit, like Space stations, are artificial satellites as well.\nThe first artificial satellite launched into the Earth's orbit was the Soviet Union's Sputnik 1, on October 4, 1957. As of June 28, 2025, there are 12,952 satellites in the Earth's orbit, of which 8,530 belong to the United States, 1,559 to Russia, and 908 to China.\nHistory.\nEarly proposals.\nThe first published mathematical study of the possibility of an artificial satellite was Newton's cannonball, a thought experiment by Isaac Newton to explain the motion of natural satellites, in his \"Philosophi\u00e6 Naturalis Principia Mathematica\" (1687). The first fictional depiction of a satellite being launched into orbit was a short story by Edward Everett Hale, \"The Brick Moon\" (1869). The idea surfaced again in Jules Verne's \"The Begum's Fortune\" (1879).\nIn 1903, Konstantin Tsiolkovsky (1857\u20131935) published \"Exploring Space Using Jet Propulsion Devices\", which was the first academic treatise on the use of rocketry to launch spacecraft. He calculated the orbital speed required for a minimal orbit, and inferred that a multi-stage rocket fueled by liquid propellants could achieve this.\nHerman Poto\u010dnik explored the idea of using orbiting spacecraft for detailed peaceful and military observation of the ground in his 1928 book, \"The Problem of Space Travel\". He described how the special conditions of space could be useful for scientific experiments. The book described geostationary satellites (first put forward by Konstantin Tsiolkovsky) and discussed the communication between them and the ground using radio, but fell short with the idea of using satellites for mass broadcasting and as telecommunications relays.\nIn a 1945 \"Wireless World\" article, English science fiction writer Arthur C. Clarke described in detail the possible use of communications satellites for mass communications. He suggested that three geostationary satellites would provide coverage over the entire planet.\nIn May 1946, the United States Air Force's Project RAND released the Preliminary Design of an Experimental World-Circling Spaceship, which stated \"A satellite vehicle with appropriate instrumentation can be expected to be one of the most potent scientific tools of the Twentieth Century.\" The United States had been considering launching orbital satellites since 1945 under the Bureau of Aeronautics of the United States Navy. Project RAND eventually released the report, but considered the satellite to be a tool for science, politics, and propaganda, rather than a potential military weapon.\nIn 1946, American theoretical astrophysicist Lyman Spitzer proposed an orbiting space telescope.\nIn February 1954, Project RAND released \"Scientific Uses for a Satellite Vehicle\", by R. R. Carhart. This expanded on potential scientific uses for satellite vehicles and was followed in June 1955 with \"The Scientific Use of an Artificial Satellite\", by H. K. Kallmann and W. W. Kellogg.\nFirst satellites.\nThe first artificial satellite was Sputnik 1, launched by the Soviet Union on 4 October 1957 under the Sputnik program, with Sergei Korolev as chief designer. Sputnik 1 helped to identify the density of high atmospheric layers through measurement of its orbital change and provided data on radio-signal distribution in the ionosphere. The unanticipated announcement of Sputnik 1's success precipitated the Sputnik crisis in the United States and ignited the so-called Space Race within the Cold War.\nIn the context of activities planned for the International Geophysical Year (1957\u20131958), the White House announced on 29 July 1955 that the U.S. intended to launch satellites by the spring of 1958. This became known as Project Vanguard. On 31 July, the Soviet Union announced its intention to launch a satellite by the fall of 1957.\nSputnik 2 was launched on 3 November 1957 and carried the first living passenger into orbit, a dog named Laika. The dog was sent without possibility of return.\nIn early 1955, after being pressured by the American Rocket Society, the National Science Foundation, and the International Geophysical Year, the Army and Navy worked on Project Orbiter with two competing programs. The army used the Jupiter C rocket, while the civilian\u2013Navy program used the Vanguard rocket to launch a satellite. Explorer 1 became the United States' first artificial satellite, on 31 January 1958. The information sent back from its radiation detector led to the discovery of the Earth's Van Allen radiation belts. The TIROS-1 spacecraft, launched on April 1, 1960, as part of NASA's Television Infrared Observation Satellite (TIROS) program, sent back the first television footage of weather patterns to be taken from space.\nIn June 1961, three and a half years after the launch of Sputnik 1, the United States Space Surveillance Network cataloged 115 Earth-orbiting satellites.\nWhile Canada was the third country to build a satellite which was launched into space, it was launched aboard an American rocket from an American spaceport. The same goes for Australia, whose launch of the first satellite involved a donated U.S. Redstone rocket and American support staff as well as a joint launch facility with the United Kingdom. The first Italian satellite San Marco 1 was launched on 15 December 1964 on a U.S. Scout rocket from Wallops Island (Virginia, United States) with an Italian launch team trained by NASA. In similar occasions, almost all further first national satellites were launched by foreign rockets.\nFrance was the third country to launch a satellite on its own rocket. On 26 November 1965, the Ast\u00e9rix or A-1 (initially conceptualized as FR.2 or FR-2), was put into orbit by a Diamant A rocket launched from the CIEES site at Hammaguir, Algeria. With Ast\u00e9rix, France became the sixth country to have an artificial satellite.\nLater Satellite Development.\nEarly satellites were built to unique designs. With advancements in technology, multiple satellites began to be built on single model platforms called satellite buses. The first standardized satellite bus design was the HS-333 geosynchronous (GEO) communication satellite launched in 1972. Beginning in 1997, FreeFlyer is a commercial off-the-shelf software application for satellite mission analysis, design, and operations.\nAfter the late 2010s, and especially after the advent and operational fielding of large satellite internet constellations\u2014where on-orbit active satellites more than doubled over a period of five years\u2014the companies building the constellations began to propose regular planned deorbiting of the older satellites that reached the end of life, as a part of the regulatory process of obtaining a launch license. The largest artificial satellite ever is the International Space Station.\nBy the early 2000s, and particularly after the advent of CubeSats and increased launches of microsats\u2014frequently launched to the lower altitudes of low Earth orbit (LEO)\u2014satellites began to more frequently be designed to get destroyed, or breakup and burnup entirely in the atmosphere.\nFor example, SpaceX Starlink satellites, the first large satellite internet constellation to exceed 1000 active satellites on orbit in 2020, are designed to be 100% demisable and burn up completely on their atmospheric reentry at the end of their life, or in the event of an early satellite failure.\nIn different periods, many countries, such as Algeria, Argentina, Australia, Austria, Brazil, Canada, Chile, China, Denmark, Egypt, Finland, France, Germany, India, Iran, Israel, Italy, Japan, Kazakhstan, South Korea, Malaysia, Mexico, the Netherlands, Norway, Pakistan, Poland, Russia, Saudi Arabia, South Africa, Spain, Switzerland, Thailand, Turkey, Ukraine, the United Kingdom and the United States, had some satellites in orbit.\nJapan's space agency (JAXA) and NASA plan to send a wooden satellite prototype called LingoSat into orbit in the summer of 2024. They have been working on this project for few years and sent first wood samples to the space in 2021 to test the material's resilience to space conditions.\nComponents.\nOrbit and altitude control.\nMost satellites use chemical or ion propulsion to adjust or maintain their orbit,78 coupled with reaction wheels to control their three axis of rotation or attitude. Satellites close to Earth are affected the most by variations in the Earth's magnetic, gravitational field and the Sun's radiation pressure; satellites that are further away are affected more by other bodies' gravitational field by the Moon and the Sun. Satellites utilize ultra-white reflective coatings to prevent damage from UV radiation. Without orbit and orientation control, satellites in orbit will not be able to communicate with ground stations on the Earth.\nChemical thrusters on satellites usually use monopropellant (one-part) or bipropellant (two-parts) that are hypergolic. Hypergolic means able to combust spontaneously when in contact with each other or to a catalyst. The most commonly used propellant mixtures on satellites are hydrazine-based monopropellants or monomethylhydrazine\u2013dinitrogen tetroxide bipropellants. Ion thrusters on satellites usually are Hall-effect thrusters, which generate thrust by accelerating positive ions through a negatively-charged grid. Ion propulsion is more efficient propellant-wise than chemical propulsion but its thrust is very small (around ), and thus requires a longer burn time. The thrusters usually use xenon because it is inert, can be easily ionized, has a high atomic mass and storable as a high-pressure liquid.\nPower.\nMost satellites use solar panels to generate power, and a few in deep space with limited sunlight use radioisotope thermoelectric generators. Slip rings attach solar panels to the satellite; the slip rings can rotate to be perpendicular with the sunlight and generate the most power. All satellites with a solar panel must also have batteries, because sunlight is blocked inside the launch vehicle and at night. The most common types of batteries for satellites are lithium-ion, and in the past nickel\u2013hydrogen.\nApplications.\nEarth observation.\nEarth observation satellites are designed to monitor and survey the Earth, called remote sensing. Most Earth observation satellites are placed in low Earth orbit for a high data resolution, though some are placed in a geostationary orbit for an uninterrupted coverage. Some satellites are placed in a Sun-synchronous orbit to have consistent lighting and obtain a total view of the Earth. Depending on the satellites' functions, they might have a normal camera, radar, lidar, photometer, or atmospheric instruments. Earth observation satellite's data is most used in archaeology, cartography, environmental monitoring, meteorology, and reconnaissance applications. As of 2021, there are over 950 Earth observation satellites, with the largest number of satellites operated with Planet Labs.\nWeather satellites monitor clouds, city lights, fires, effects of pollution, auroras, sand and dust storms, snow cover, ice mapping, boundaries of ocean currents, energy flows, etc. Environmental monitoring satellites can detect changes in the Earth's vegetation, atmospheric trace gas content, sea state, ocean color, and ice fields. By monitoring vegetation changes over time, droughts can be monitored by comparing the current vegetation state to its long term average. Anthropogenic emissions can be monitored by evaluating data of tropospheric NO2 and SO2.\nSpy satellites.\nWhen an\u00a0Earth observation satellite or\u00a0a communications satellite is deployed for\u00a0military\u00a0or\u00a0intelligence\u00a0purposes, it is known as a spy satellite or reconnaissance satellite.\nTheir uses include early missile warning, nuclear explosion detection, electronic reconnaissance, and optical or radar imaging surveillance.\nNavigation.\nNavigational satellites are satellites that use radio time signals transmitted to enable mobile receivers on the ground to determine their exact location. The relatively clear line of sight between the satellites and receivers on the ground, combined with ever-improving electronics, allows satellite navigation systems to measure location to accuracies on the order of a few meters in real time.\nTelescope.\nAstronomical satellites are satellites used for observation of distant planets, galaxies, and other outer space objects.\nExperimental.\nTether satellites are satellites that are connected to another satellite by a thin cable called a tether. Recovery satellites are satellites that provide a recovery of reconnaissance, biological, space-production and other payloads from orbit to Earth. Biosatellites are satellites designed to carry living organisms, generally for scientific experimentation. Space-based solar power satellites are proposed satellites that would collect energy from sunlight and transmit it for use on Earth or other places.\nWeapon.\nSince the mid-2000s, satellites have been hacked by militant organizations to broadcast propaganda and to pilfer classified information from military communication networks. For testing purposes, satellites in low earth orbit have been destroyed by ballistic missiles launched from the Earth. Russia, United States, China and India have demonstrated the ability to eliminate satellites. In 2007, the Chinese military shot down an aging weather satellite, followed by the US Navy shooting down a defunct spy satellite in February 2008. On 18 November 2015, after two failed attempts, Russia successfully carried out a flight test of an anti-satellite missile known as \"Nudol\". On 27 March 2019, India shot down a live test satellite at 300\u00a0km altitude in 3 minutes, becoming the fourth country to have the capability to destroy live satellites.\nEnvironmental impact.\nThe environmental impact of satellites is not currently well understood as they were previously assumed to be benign due to the rarity of satellite launches. However, the exponential increase and projected growth of satellite launches are bringing the issue into consideration. The main issues are resource use and the release of pollutants into the atmosphere which can happen at different stages of a satellite's lifetime.\nResource use.\nResource use is difficult to monitor and quantify for satellites and launch vehicles due to their commercially sensitive nature. However, aluminium is a preferred metal in satellite construction due to its lightweight and relative cheapness and typically constitutes around 40% of a satellite's mass. Through mining and refining, aluminium has numerous negative environmental impacts and is one of the most carbon-intensive metals. Satellite manufacturing also requires rare elements such as lithium, gold, and gallium, some of which have significant environmental consequences linked to their mining and processing and/or are in limited supply. Launch vehicles require larger amounts of raw materials to manufacture and the booster stages are usually dropped into the ocean after fuel exhaustion. They are not normally recovered. Two empty boosters used for Ariane 5, which were composed mainly of steel, weighed around 38 tons each, to give an idea of the quantity of materials that are often left in the ocean.\nLaunches.\nRocket launches release numerous pollutants into every layer of the atmosphere, especially affecting the atmosphere above the tropopause where the byproducts of combustion can reside for extended periods. These pollutants can include black carbon, CO2, nitrogen oxides (NOx), aluminium and water vapour, but the mix of pollutants is dependent on rocket design and fuel type. The amount of green house gases emitted by rockets is considered trivial as it contributes significantly less, around 0.01%, than the aviation industry yearly which itself accounts for 2-3% of the total global greenhouse gas emissions.\nRocket emissions in the stratosphere and their effects are only beginning to be studied and it is likely that the impacts will be more critical than emissions in the troposphere. The stratosphere includes the ozone layer and pollutants emitted from rockets can contribute to ozone depletion in a number of ways. Radicals such as NOx, HOx, and ClOx deplete stratospheric O3 through intermolecular reactions and can have huge impacts in trace amounts. However, it is currently understood that launch rates would need to increase by ten times to match the impact of regulated ozone-depleting substances. Whilst emissions of water vapour are largely deemed as inert, H2O is the source gas for HOx and can also contribute to ozone loss through the formation of ice particles. Black carbon particles emitted by rockets can absorb solar radiation in the stratosphere and cause warming in the surrounding air which can then impact the circulatory dynamics of the stratosphere. Both warming and changes in circulation can then cause depletion of the ozone layer.\nOperational.\nLow earth orbit satellites.\nSeveral pollutants are released in the upper atmospheric layers during the orbital lifetime of LEO satellites. Orbital decay is caused by atmospheric drag and to keep the satellite in the correct orbit the platform occasionally needs repositioning. To do this nozzle-based systems use a chemical propellant to create thrust. In most cases hydrazine is the chemical propellant used which then releases ammonia, hydrogen and nitrogen as gas into the upper atmosphere. Also, the environment of the outer atmosphere causes the degradation of exterior materials. The atomic oxygen in the upper atmosphere oxidises hydrocarbon-based polymers like Kapton, Teflon and Mylar that are used to insulate and protect the satellite which then emits gasses like CO2 and CO into the atmosphere.\nNight sky.\nGiven the current surge in satellites in the sky, soon hundreds of satellites may be clearly visible to the human eye at dark sites. It is estimated that the overall levels of diffuse brightness of the night skies has increased by up to 10% above natural levels. This has the potential to confuse organisms, like insects and night-migrating birds, that use celestial patterns for migration and orientation. The impact this might have is currently unclear. The visibility of human-made objects in the night sky may also impact people's linkages with the world, nature, and culture.\nGround-based infrastructure.\nAt all points of a satellite's lifetime, its movement and processes are monitored on the ground through a network of facilities. The environmental cost of the infrastructure as well as day-to-day operations is likely to be quite high, but quantification requires further investigation.\nDegeneration.\nParticular threats arise from uncontrolled de-orbit. \nSome notable satellite failures that polluted and dispersed radioactive materials are Kosmos 954, Kosmos 1402 and the Transit 5-BN-3.\nWhen in a controlled manner satellites reach the end of life they are intentionally deorbited or moved to a graveyard orbit further away from Earth in order to reduce space debris. Physical collection or removal is not economical or even currently possible. Moving satellites out to a graveyard orbit is also unsustainable because they remain there for hundreds of years. It will lead to the further pollution of space and future issues with space debris. \nWhen satellites deorbit much of it is destroyed during re-entry into the atmosphere due to the heat. This introduces more material and pollutants into the atmosphere. There have been concerns expressed about the potential damage to the ozone layer and the possibility of increasing the earth's albedo, reducing warming but also resulting in accidental geoengineering of the earth's climate. After deorbiting 70% of satellites end up in the ocean and are rarely recovered.\nMitigation.\nUsing wood as an alternative material has been posited in order to reduce pollution and debris from satellites that reenter the atmosphere.\nInterference.\nCollision threat.\nSpace debris pose dangers to the spacecraft (including satellites) in or crossing geocentric orbits and have the potential to drive a Kessler syndrome which could potentially curtail humanity from conducting space endeavors in the future.\nWith increase in the number of satellite constellations, like SpaceX Starlink, the astronomical community, such as the IAU, report that orbital pollution is getting increased significantly. A report from the SATCON1 workshop in 2020 concluded that the effects of large satellite constellations can severely affect some astronomical research efforts and lists six ways to mitigate harm to astronomy. The IAU is establishing a center (CPS) to coordinate or aggregate measures to mitigate such detrimental effects.\nRadio interference.\nDue to the low received signal strength of satellite transmissions, they are prone to jamming by land-based transmitters. Such jamming is limited to the geographical area within the transmitter's range. GPS satellites are potential targets for jamming, but satellite phone and television signals have also been subjected to jamming.\nAlso, it is very easy to transmit a carrier radio signal to a geostationary satellite and thus interfere with the legitimate uses of the satellite's transponder. It is common for Earth stations to transmit at the wrong time or on the wrong frequency in commercial satellite space, and dual-illuminate the transponder, rendering the frequency unusable. Satellite operators now have sophisticated monitoring tools and methods that enable them to pinpoint the source of any carrier and manage the transponder space effectively. \nRegulation.\nIssues like space debris, radio and light pollution are increasing in magnitude and at the same time lack progress in national or international regulation. \nLiability.\nGenerally liability has been covered by the Liability Convention.\nOperation.\nThe operation capabilities and use have very much diversified and is broadening increasingly.\nSatellite operation needs not only access to financial, manufacturing and launch capabilities, but also a ground segment infrastructure.\nList.\nA list of selected Earth satellites with a high mass is given. Excluded are crewed satellites i.e. space stations and launch vehicles, and satellites not intended for long-term Earth orbit, such as missions to beyond Earth orbit. Included are satellites at Sun-Earth Lagrange point 2.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27684", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=27684", "title": "Steampunk", "text": "Subgenre of science fiction\nSteampunk is a subgenre of science fiction that incorporates retro-futuristic technology and aesthetics prominently inspired by 19th-century industrial steam-powered machinery and design. Steampunk works are often set in an alternative history of the Victorian era or the American frontier where steam power remains in mainstream use, or in a fantasy world that similarly employs steam power.\nSteampunk features anachronistic technologies or retro-futuristic inventions as people in the 19th century might have envisioned them \u2013 distinguishing it from Neo-Victorianism \u2013 and is likewise rooted in the era's perspective on fashion, culture, architectural style, and art. Such technologies may include fictional machines like those found in the works of H. G. Wells and Jules Verne. Other examples of steampunk contain alternative history-style presentations of such technology as steam cannons, lighter-than-air airships, analog computers or such digital mechanical computers as Charles Babbage's Analytical Engine.\nSteampunk may also incorporate additional elements from the genres of fantasy, horror, historical fiction, alternate history or other branches of speculative fiction, making it often a hybrid genre. As a form of speculative fiction, it explores alternative futures or pasts but can also address real-world social issues. The first known appearance of the term \"steampunk\" was in 1987, though it now retroactively refers to many works of fiction created as far back as the 1950s or earlier. A popular subgenre is Japanese steampunk, consisting of steampunk-themed manga and anime.\nSteampunk also refers to any of the artistic styles, clothing fashions, or subcultures that have developed from the aesthetics of steampunk fiction, Victorian-era fiction, art nouveau design, and films from the mid-20th century. Various modern utilitarian objects have been modded by individual artisans into a pseudo-Victorian mechanical 'steampunk' style, and a number of visual and musical artists have been described as steampunk.\nHistory.\nPrecursors.\nSteampunk is influenced by and often adopts the style of the 19th-century scientific romances of Jules Verne, H. G. Wells, Mary Shelley, and Edward S. Ellis's \"The Steam Man of the Prairies\". Several more modern works of art and fiction significant to the development of the genre were produced before the genre had a name. \"Titus Alone\" (1959), by Mervyn Peake, is widely regarded by scholars as the first novel in the genre proper, while others point to Michael Moorcock's 1971 novel \"The Warlord of the Air\", which was heavily influenced by Peake's work. The film \"Brazil\" (1985) was an early cinematic influence, although it can also be considered a precursor to the steampunk offshoot dieselpunk. \"The Adventures of Luther Arkwright\" was an early (1970s) comic version of the Moorcock-style mover between timestreams.\nIn fine art, Remedios Varo's paintings combine elements of Victorian dress, fantasy, and technofantasy imagery. In television, one of the earliest manifestations of the steampunk ethos in the mainstream media was the CBS television series \"The Wild Wild West\" (1965\u201369), which inspired the later film.\nOrigin of the term.\nAlthough many works now considered seminal to the genre were published in the 1960s and 1970s, the term \"steampunk\" originated largely in the 1980s as a tongue-in-cheek variant of \"cyberpunk\". It was coined by science fiction author K. W. Jeter, who was trying to find a general term for works by Tim Powers (\"The Anubis Gates\", 1983), James Blaylock (\"Homunculus\", 1986), and himself (\"Morlock Night\", 1979, and \"Infernal Devices\", 1987) \u2014 all of which took place in a 19th-century (usually Victorian) setting and imitated conventions of such actual Victorian speculative fiction as H. G. Wells' \"The Time Machine\". In a letter to science fiction magazine \"Locus\", printed in the April 1987 issue, Jeter wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Dear Locus,\nEnclosed is a copy of my 1979 novel \"Morlock Night\"; I'd appreciate your being so good as to route it to Faren Miller, as it's a prime piece of evidence in the great debate as to who in \"the Powers/Blaylock/Jeter fantasy triumvirate\" was writing in the \"gonzo-historical manner\" first. Though of course, I did find her review in the March Locus to be quite flattering.\nPersonally, I think Victorian fantasies are going to be the next big thing, as long as we can come up with a fitting collective term for Powers, Blaylock and myself. Something based on the appropriate technology of the era; like \"steam-punks,\" perhaps...\u2014\u200a\nModern steampunk.\nWhile Jeter's \"Morlock Night\" and \"Infernal Devices\", Powers' \"The Anubis Gates\", and Blaylock's \"Lord Kelvin's Machine\" were the first novels to which Jeter's neologism would be applied, the three authors gave the term little thought at the time. They were far from the first modern science fiction writers to speculate on the development of steam-based technology or alternative histories. Keith Laumer's \"Worlds of the Imperium\" (1962) and Ronald W. Clark's \"Queen Victoria's Bomb\" (1967) apply modern speculation to past-age technology and society. Michael Moorcock's \"Warlord of the Air\" (1971) is another early example. Harry Harrison's novel \"A Transatlantic Tunnel, Hurrah!\" (1973) portrays Britain in an alternative 1973, full of atomic locomotives, coal-powered flying boats, ornate submarines, and Victorian dialogue. \"The Adventures of Luther Arkwright\" (mid-1970s) was one of the first steampunk comics. In February 1980, Richard A. Lupoff and Steve Stiles published the first \"chapter\" of their 10-part comic strip \"The Adventures of Professor Thintwhistle and His Incredible Aether Flyer\". In 2004, one anonymous author described steampunk as \"Colonizing the Past so we can dream the future.\"\nThe first use of the word \"steampunk\" in a title was in Paul Di Filippo's 1995 \"Steampunk Trilogy\", consisting of three short novels: \"Victoria\", \"Hottentots\", and \"Walt and Emily\", which, respectively, imagine the replacement of Queen Victoria by a human/newt clone; an invasion of Massachusetts by Lovecraftian monsters, drawing its title from the historic racial taxonomy \"hottentot\"; and a love affair between Walt Whitman and Emily Dickinson.\nJapanese steampunk.\nJapanese steampunk consists of steampunk manga comics and anime productions from Japan. Steampunk elements have consistently appeared in mainstream manga since the 1940s, dating back to Osamu Tezuka's epic science-fiction trilogy consisting of \"Lost World\" (1948), \"Metropolis\" (1949) and \"Nextworld\" (1951). The steampunk elements found in manga eventually made their way into mainstream anime productions starting in the 1970s. Influenced by 19th-century European authors such as Jules Verne, steampunk anime and manga arose from a Japanese fascination with an imaginary fantastical version of old Industrial Europe, linked to a phenomenon called \"akogare no Pari\" (\"the Paris of our dreams\"), comparable to the West's fascination with an \"exotic\" East.\nThe most influential steampunk animator was Hayao Miyazaki, who was creating steampunk anime since the 1970s, starting with the television show \"Future Boy Conan\" (1978). His manga \"Nausica\u00e4 of the Valley of the Wind\" (1982) and its 1984 anime film adaptation also contained steampunk elements. Miyazaki's most influential steampunk production was the Studio Ghibli anime film \"Laputa: Castle in the Sky\" (1986), which became a major milestone in the genre and has been described by \"The Steampunk Bible\" as \"one of the first modern steampunk classics.\" Archetypal steampunk elements in \"Laputa\" include airships, air pirates, steam-powered robots, and a view of steam power as a limitless but potentially dangerous source of power.\nThe success of \"Laputa\" inspired Hideaki Anno and Studio Gainax to create their first hit production, ' (1990), a steampunk anime show which loosely adapts elements from Verne's \"Twenty Thousand Leagues Under the Seas\", with Captain Nemo making an appearance. Based on a concept by Miyazaki, \"Nadia\" was influential on later steampunk anime such as Katsuhiro Otomo's anime film \"Steamboy\" (2004). Disney's animated steampunk film ' (2001) was influenced by anime, particularly Miyazaki's works and possibly \"Nadia\". Other popular Japanese steampunk works include Miyazaki's Studio Ghibli anime film \"Howl's Moving Castle\" (2004), Sega's video game and anime franchise \"Sakura Wars\" (1996) which is set in a steampunk version of Meiji/Taish\u014d era Japan, and Square Enix's manga and anime franchise \"Fullmetal Alchemist\" (2001).\nRelationships to retrofuturism, DIY craft and making.\nSteampunk used to be confused with retrofuturism. Indeed, both sensibilities recall \"the older but still modern eras in which technological change seemed to anticipate a better world, one remembered as relatively innocent of industrial decline.\" For some scholars, retrofuturism is considered a strand of steampunk, one that looks at alternatives to historical imagination and usually created with the same kinds of social protagonists and written for the same type of audiences.\nOne of steampunk's most significant contributions is the way in which it mixes digital media with traditional handmade art forms. As scholars Rachel Bowser and Brian Croxall put it, \"the tinkering and tinker-able technologies within steampunk invite us to roll up our sleeves and get to work re-shaping our contemporary world.\" In this respect, steampunk bears much in common with DIY craft and bricolage artmaking.\nArt, entertainment, and media.\nArt and design.\nMany of the visualisations of steampunk have their origins with, among others, Walt Disney's film \"20,000 Leagues Under the Sea\" (1954), including the design of the story's submarine the \"Nautilus\", its interiors, and the crew's underwater gear.\nAspects of steampunk design emphasise a balance between form and function. In this, it is like the Arts and Crafts Movement. But John Ruskin, William Morris, and the other reformers in the late nineteenth century rejected machines and industrial production. In contrast, steampunk enthusiasts present a \"non-luddite critique of technology\". In Dutch amusement park De Efteling, there is a dive coaster themed to a steampunk Victorian haunted goldmine called Baron 1898.\nVarious modern utilitarian objects have been modified by enthusiasts into a pseudo-Victorian mechanical \"steampunk\" style. Examples include computer keyboards and electric guitars. The goal of such redesigns is to employ appropriate materials (such as polished brass, iron, wood, and leather) with design elements and craftsmanship consistent with the Victorian era, rejecting the aesthetic of industrial design.\nIn 1994, the Paris Metro station at Arts et M\u00e9tiers was redesigned by Belgian artist Francois Schuiten in steampunk style, to honor the works of Jules Verne. The station is reminiscent of a submarine, sheathed in brass with giant cogs in the ceiling and portholes that look out onto fanciful scenes.\nThe artist group \"Kinetic Steam Works\" brought a working steam engine to the Burning Man festival in 2006 and 2007. The group's founding member, Sean Orlando, created a Steampunk Tree House (in association with a group of people who would later form the \"Five Ton Crane Arts Group\") that has been displayed at a number of festivals. The Steampunk Tree House is now permanently installed at the Dogfish Head Brewery in Milton, Delaware.\nThe Neverwas Haul is a three-story, self-propelled mobile art vehicle built to resemble a Victorian house on wheels. Designed by Shannon O'Hare, it was built by volunteers in 2006 and presented at the Burning Man festival from 2006 through 2015. When fully built, the Haul propelled itself at a top speed of 5 miles per hour and required a crew of ten people to operate safely. Currently, the Neverwas Haul makes her home at Obtainium Works, an \"art car factory\" in Vallejo, CA owned by O'Hare and home to several other self-styled \"contraptionists\".\nIn May\u2013June 2008, multimedia artist and sculptor Paul St George exhibited outdoor interactive video installations linking London and Brooklyn, New York, in a Victorian era-styled telectroscope. Utilizing this device, New York promoter Evelyn Kriete organised a transatlantic wave between steampunk enthusiasts from both cities, prior to White Mischief's \"Around the World in 80 Days\" steampunk-themed event.\nIn 2009, for Questacon, artist Tim Wetherell created a large wall piece that represented the concept of the clockwork universe. This steel artwork contains moving gears, a working clock, and a movie of the moon's terminator in action. The 3D moon movie was created by Antony Williams.\nSteampunk became a common descriptor for homemade objects sold on the craft network Etsy between 2009 and 2011, though many of the objects and fashions bear little resemblance to earlier established descriptions of steampunk. Thus the craft network may not strike observers as \"sufficiently steampunk\" to warrant its use of the term. Comedian April Winchell, author of the book \"Regretsy: Where DIY Meets WTF\", cataloged some of the most egregious and humorous examples on her website \"Regretsy\". The blog was popular among steampunks and even inspired a music video that went viral in the community and was acclaimed by steampunk \"notables\".\nFrom October 2009 through February 2010, the Museum of the History of Science, Oxford, hosted the first major exhibition of steampunk art objects, curated and developed by New York artist and designer Art Donovan, who also exhibited his own \"electro-futuristic\" lighting sculptures, and presented by Dr. Jim Bennett, museum director. From redesigned practical items to fantastical contraptions, this exhibition showcased the work of eighteen steampunk artists from around the globe. The exhibit proved to be the most successful and highly attended in the museum's history and attracted more than eighty thousand visitors. The event was detailed in the official artist's journal \"The Art of Steampunk\", by curator Donovan.\nIn November 2010, The Libratory Steampunk Art Gallery was opened by Damien McNamara in Oamaru, New Zealand. Created from papier-m\u00e2ch\u00e9 to resemble a large cave and filled with industrial equipment from yesteryear, rayguns, and general steampunk quirks, its purpose is to provide a place for steampunkers in the region to display artwork for sale all year long. A year later, a more permanent gallery, Steampunk HQ, was opened in the former Meeks Grain Elevator Building across the road from The Woolstore, and has since become a notable tourist attraction for Oamaru.\nFashion.\nSteampunk fashion has no set guidelines but tends to synthesize modern styles with influences from the Victorian era. Such influences may include bustles, corsets, gowns, and petticoats; suits with waistcoats, coats, top hats and bowler hats (themselves originating in 1850 England), tailcoats and spats; or military-inspired garments. Steampunk-influenced outfits are usually accented with several technological and \"period\" accessories: timepieces, parasols, flying/driving goggles, and ray guns. Modern accessories like cell phones or music players can be found in steampunk outfits, after being modified to give them the appearance of Victorian-era objects. Post-apocalyptic elements, such as gas masks, ragged clothing, and tribal motifs, can also be included. Aspects of steampunk fashion have been anticipated by mainstream high fashion, the Lolita and aristocrat styles, neo-Victorianism, and the Romantic Goth subculture.\nIn 2005, Kate Lambert, known as \"Kato\", founded the first steampunk clothing company, \"Steampunk Couture\", mixing Victorian and post-apocalyptic influences. In 2013, IBM predicted, based on an analysis of more than a half million public posts on message boards, blogs, social media sites, and news sources, \"that 'steampunk,' a subgenre inspired by the clothing, technology and social mores of Victorian society, will be a major trend to bubble up and take hold of the retail industry\". Indeed, high fashion lines such as Prada, Dolce &amp; Gabbana, Versace, Chanel, and Christian Dior had already been introducing steampunk styles on the fashion runways.\nIn episode 7 of Lifetime's \"Under the Gunn\" reality series, contestants were challenged to create avant-garde \"steampunk chic\" looks. \"America's Next Top Model\" tackled steampunk fashion in a 2012 episode where models competed in a steampunk-themed photo shoot, posing in front of a steam train while holding a live owl.\nLiterature.\nIn 1988, the first version of the science fiction tabletop role-playing game \"\" was published. The game is set in an alternative history in which certain now discredited Victorian scientific theories were probable and led to new technologies. Contributing authors included Frank Chadwick, Loren Wiseman, and Marcus Rowland.\nWilliam Gibson and Bruce Sterling's novel \"The Difference Engine\" (1990) is often credited with bringing about widespread awareness of steampunk. The novel applies the principles of Gibson and Sterling's cyberpunk writings to an alternative Victorian era where Ada Lovelace and Charles Babbage's proposed steam-powered mechanical computer, which Babbage called a difference engine (a later, more general-purpose version was known as an Analytical Engine), was actually built, and led to the dawn of the Information Age more than a century \"ahead of schedule\". This setting was different from most steampunk settings in that it takes a dim and dark view of this future, rather than the more prevalent utopian versions.\nNick Gevers's original anthology \"Extraordinary Engines\" (2008) features newer steampunk stories by some of the genre's writers, as well as other science fiction and fantasy writers experimenting with neo-Victorian conventions. A retrospective reprint anthology of steampunk fiction was released, also in 2008, by Tachyon Publications. Edited by Ann and Jeff VanderMeer and appropriately entitled \"Steampunk\", it is a collection of stories by James Blaylock, whose \"Narbondo\" trilogy is typically considered steampunk; Jay Lake, author of the novel \"Mainspring\", sometimes labeled \"clockpunk\"; the aforementioned Michael Moorcock; as well as Jess Nevins, known for his annotations to \"The League of Extraordinary Gentlemen\" (first published in 1999).\nYounger readers have also been targeted by steampunk themes, by authors such as Philip Reeve and Scott Westerfeld. Reeve's quartet \"Mortal Engines\" is set far in Earth's future where giant moving cities consume each other in a battle for resources, a concept Reeve coined as \"Municipal Darwinism\". Westerfeld's \"Leviathan\" trilogy is set during an alternate First World War fought between the \"clankers\" (Central Powers), who use steam technology, and \"darwinists\" (Allied Powers), who use genetically engineered creatures instead of machines.\n\"Mash-ups\" are also becoming increasingly popular in books aimed at younger readers, mixing steampunk with other genres. Stefan Bachmann's \"The Peculiar\" duology was labeled a \"steampunk fairytale,\" and imagines steampunk technology as a means to stave off an incursion of faeries in Victorian England. Suzanne Lazear's \"Aether Chronicles\" series also mixes steampunk with faeries, and \"The Unnaturalists\", by Tiffany Trent, combines steampunk with mythological creatures and alternate history.\nSelf-described author of \"far-fetched fiction\" Robert Rankin has incorporated elements of steampunk into narrative worlds that are both Victorian and re-imagined contemporary. In 2009, he was made a Fellow of the Victorian Steampunk Society.\nThe comic book series \"Hellboy\", created by Mike Mignola, and the two \"Hellboy\" films featuring Ron Perlman and directed by Guillermo del Toro, all have steampunk elements. In the comic book and the first (2004) film, Karl Ruprecht Kroenen is a Nazi SS scientist who has an addiction to having himself surgically altered, and who has many mechanical prostheses, including a clockwork heart. The character Johann Krauss is featured in the comic and in the second film, \"\" (2008), as an ectoplasmic medium (a gaseous form in a partly mechanical suit). This second film also features the Golden Army itself, which is a collection of 4,900 mechanical steampunk warriors.\nSteampunk settings.\nAlternative world.\nSince the 1990s, the application of the steampunk label has expanded beyond works set in recognisable historical periods, to works set in fantasy worlds that rely heavily on steam- or spring-powered technology. One of the earliest short stories relying on steam-powered flying machines is Percival Leigh's \"The Aerial Burglar\" first published in 1844. An example from juvenile fiction is \"The Edge Chronicles\" by Paul Stewart and Chris Riddell.\nFantasy steampunk settings abound in tabletop and computer role-playing games. Notable examples include \"Skies of Arcadia\", ', and '.\nOne of the first steampunk novels set in a Middle-earth-like world was the \"Forest of Boland Light Railway\" by BB, about gnomes who build a steam locomotive. Fifty years later, Terry Pratchett wrote the Discworld novel \"Raising Steam,\" about the ongoing industrial revolution and railway mania in Ankh-Morpork.\nThe gnomes and goblins in \"World of Warcraft\" also have technological societies that could be described as steampunk, as they are vastly ahead of the technologies of men, but still run on steam and mechanical power.\nThe Dwarves of the \"Elder Scrolls\" series, described therein as a race of Elves called the Dwemer, also use steam-powered machinery, with gigantic brass-like gears, throughout their underground cities. However, magical means are used to keep ancient devices in motion despite the Dwemer's ancient disappearance.\nThe 1998 game \"\", as well as the other sequels including its 2014 reboot, feature heavy steampunk-inspired architecture, setting, and technology.\nAmidst the historical and fantasy subgenres of steampunk is a type that takes place in a hypothetical future or a fantasy equivalent of our future involving the domination of steampunk-style technology and aesthetics. Examples include Jean-Pierre Jeunet and Marc Caro's \"The City of Lost Children\" (1995), \"Turn A Gundam\" (1999\u20132000), \"Trigun\", and Disney's film \"Treasure Planet\" (2002). In 2011, musician Thomas Dolby heralded his return to music after a 20-year hiatus with an online steampunk alternate fantasy world called the Floating City, to promote his album \"A Map of the Floating City\".\nAmerican West.\nAnother setting is \"Western\" steampunk, which overlaps with both the weird West and science fiction Western subgenres. One of the earliest steampunk books set in America was \"The Steam Man of the Prairies\" by Edward S. Ellis. Recent examples include the TV show \"The Wild Wild West\" and the movie adaption \"Wild Wild West\", the Italian comics about Magico Vento, and Devon Monk's \"Dead Iron.\"\nFantasy and horror.\nKaja Foglio introduced the term \"Gaslamp Fantasy\", for the series Girl Genius. Gaslamp fantasy, which John Clute and John Grant define as \"steampunk stories ... most commonly set in a romanticised, smoky, 19th-century London, as are Gaslight Romances. But the latter category focuses nostalgically on icons from the late years of that century and the early years of the 20th century\u2014on Dracula, Jekyll and Hyde, Jack the Ripper, Sherlock Holmes and even Tarzan\u2014and can normally be understood as combining supernatural fiction and recursive fantasy, though some gaslamp romances can be read as fantasies of history.\" Author/artist James Richardson-Brown coined the term \"steamgoth\" to refer to steampunk expressions of fantasy and horror with a \"darker\" bent.\nPost-apocalyptic.\nMary Shelley's \"The Last Man\", set near the end of the 21st century after a plague had brought down civilization, was probably the ancestor of post-apocalyptic steampunk literature. Post-apocalyptic steampunk is set in a world where some cataclysm has precipitated the fall of civilization and steam power is once again ascendant, such as in Hayao Miyazaki's post-apocalyptic anime \"Future Boy Conan\" (1978, loosely based on Alexander Key's \"The Incredible Tide\" (1970)), where a war fought with superweapons has devastated the planet. Robert Brown's novel, \"The Wrath of Fate\" (as well as much of Abney Park's music) is set in a Victorianesque world where an apocalypse was set into motion by a time-traveling mishap. Cherie Priest's Boneshaker series is set in a world where a zombie apocalypse happened during the Civil War era. \"The Peshawar Lancers\" by S.M. Stirling is set in a post-apocalyptic future in which a meteor shower in 1878 caused the collapse of industrialized civilization. The movie 9 (which might be better classified as \"stitchpunk\" but was largely influenced by steampunk) is also set in a post-apocalyptic world after a self-aware war machine ran amok. \"Steampunk Magazine\" even published a book called \"A Steampunk's Guide to the Apocalypse\", about how steampunks could survive should such a thing actually happen.\nVictorian.\nIn general, this category includes any recent science fiction that takes place in a recognizable historical period (sometimes an alternate history version of an actual historical period) in which the Industrial Revolution has already begun, but electricity is not yet widespread, \"usually Britain of the early to mid-nineteenth century or the fantasized Wild West-era United States\", with an emphasis on steam- or spring-propelled gadgets. The most common historical steampunk settings are the Victorian and Edwardian eras, though some in this \"Victorian steampunk\" category are set as early as the beginning of the Industrial Revolution and as late as the end of World War I.\nSome examples of this type include the novel \"The Difference Engine\", the comic book series \"League of Extraordinary Gentlemen\", the Disney animated film \"\", Scott Westerfeld's \"Leviathan\" trilogy, and the roleplaying game \".\" The anime film \"Steamboy\" (2004) is another example of Victorian steampunk, taking place in an alternate 1866 where steam technology is far more advanced than reality. Some, such as the comic series \"Girl Genius\", have their own unique times and places despite partaking heavily of the flavor of historic settings. Other comic series are set in a more familiar London, as in the \"Victorian Undead\", which has Sherlock Holmes, Doctor Watson, and others taking on zombies, Doctor Jekyll and Mister Hyde, and Count Dracula, with advanced weapons and devices. Another example of this genre is the \"Tunnels\" novels by Roderick Gordon and Brian Williams. These are set in the modern day, but with an underground Victorian world that is working to overthrow the world above. Detective graphic novel series Lady Mechanika is set in an alternative Victorian-like world.\nKarel Zeman's film \"The Fabulous World of Jules Verne\" (1958) is a very early example of cinematic steampunk. Based on Jules Verne novels, Zeman's film imagines a past that never was, based on those novels. Other early examples of historical steampunk in cinema include Hayao Miyazaki's anime films such as \"\" (1986) and \"Howl's Moving Castle\" (2004), which contain many archetypal anachronisms characteristic of the steampunk genre.\n\"Historical\" steampunk usually leans more towards science fiction than fantasy, but a number of historical steampunk stories have incorporated magical elements as well. For example, \"Morlock Night\", written by K. W. Jeter, revolves around an attempt by the wizard Merlin to raise King Arthur to save the Britain of 1892 from an invasion of Morlocks from the future.\nPaul Guinan's \"Boilerplate\", a \"biography\" of a robot in the late 19th century, began as a website that garnered international press coverage when people began believing that Photoshop images of the robot with historic personages were real. The site was adapted into the illustrated hardbound book \"Boilerplate: History's Mechanical Marvel\", which was published by Abrams in October 2009. Because the story was not set in an alternative history, and in fact contained accurate information about the Victorian era, some booksellers referred to the tome as \"historical steampunk\".\nEast Asia.\nFictional settings inspired by East Asian rather than European history, especially those inspired by Chinese history, have been called \"silkpunk\". The term originated with the author Ken Liu, who defined it as \"a blend of science fiction and fantasy [that] draws inspiration from classical East Asian antiquity\", with a \"technology vocabulary (...) based on organic materials historically important to East Asia (bamboo, paper, silk) and seafaring cultures of the Pacific (coconut, feathers, coral)\", rather than the brass and leather associated with steampunk. Liu used the term to describe his \"Dandelion Dynasty\" series, which began in 2015. Other works described as silkpunk include Neon Yang's \"Tensorate\" series of novellas, which began in 2017. Lyndsie Manusos of Book Riot has argued that the genre does \"not fit in a direct analogy with steampunk. Silkpunk is technology and poetics. It is engineering and language.\"\nMusic.\nSteampunk music is very broadly defined. Abney Park's lead singer Robert Brown defined it as \"mixing Victorian elements and modern elements\". There is a broad range of musical influences that make up the steampunk sound, from industrial dance and world music to folk rock, dark cabaret to straightforward punk, Carnatic to industrial, hip-hop to opera (and even industrial hip-hop opera), darkwave to progressive rock, barbershop to big band.\nJoshua Pfeiffer (of Vernian Process) is quoted as saying, \"As for Paul Roland, if anyone deserves credit for spearheading Steampunk music, it is him. He was one of the inspirations I had in starting my project. He was writing songs about the first attempt at manned flight, and an Edwardian airship raid in the mid-80s long before almost anyone else...\" Thomas Dolby is also considered one of the early pioneers of retro-futurist (i.e., Steampunk and Dieselpunk) music. Amanda Palmer was once quoted as saying, \"Thomas Dolby is to Steampunk what Iggy Pop was to Punk!\"\nSteampunk has also appeared in the work of musicians who do not specifically identify as steampunk. For example, the music video of \"Turn Me On\", by David Guetta and featuring Nicki Minaj, takes place in a steampunk universe where Guetta creates human androids. Another music video is \"The Ballad of Mona Lisa\", by Panic! at the Disco, which has a distinct Victorian steampunk theme. A continuation of this theme has been used throughout the 2011 album \"Vices &amp; Virtues\", in the music videos, album art, and tour set and costumes. In addition, the album \"Clockwork Angels\" (2012) and its supporting tour by progressive rock band Rush contain lyrics, themes, and imagery based around steampunk. Similarly, Abney Park headlined the first \"Steamstock\" outdoor steampunk music festival in Richmond, California, which also featured Thomas Dolby, Frenchy and the Punk, Lee Presson and the Nails, Vernian Process, and others.\nThe music video for the Lindsey Stirling song \"Roundtable Rival\", has a Western steampunk setting.\nTelevision and films.\n\"The Fabulous World of Jules Verne\" (1958) and \"The Fabulous Baron Munchausen\" (1962), both directed by Karel Zeman, have steampunk elements. The 1965 television series \"The Wild Wild West\", as well as the 1999 film of the same name, features many elements of advanced steam-powered technology set in the Wild West time period of the United States. \"Two Years' Vacation\" (or \"The Stolen Airship\") (1967) directed by Karel Zeman contains steampunk elements.\nThe BBC series \"Doctor Who\" also incorporates steampunk elements. Several storylines can be classed as steampunk, most notably \"The Evil of the Daleks\" (1966), wherein Victorian scientists invent a time travel device using mirrors and static electricity. During season 14 of the show (in 1976), the formerly futuristic looking interior set was replaced with a Victorian-styled wood-panel and brass affair. In the 1996 American co-production, the TARDIS interior was re-designed to resemble an almost Victorian library with the central control console made up of an eclectic array of anachronistic objects. Modified and streamlined for the 2005 revival of the series, the TARDIS console continued to incorporate steampunk elements, including a Victorian typewriter and gramophone, for many years.\n\"Dinner for Adele\" (1977) directed by Old\u0159ich Lipsk\u00fd involves steampunk contraptions. The 1979 film \"Time After Time\" has Herbert George \"H.G.\" Wells following a surgeon named John Leslie Stevenson into the future, as John is suspected of being Jack the Ripper. Both separately use Wells's time machine to travel.\n\"The Mysterious Castle in the Carpathians\", (1981) directed by Old\u0159ich Lipsk\u00fd, contains steampunk elements. The 1982 American TV series \"Q.E.D.\" is set in Edwardian England, stars Sam Waterston as Professor Quentin Everett Deverill (from whose initials, by which he is primarily known, the series title is derived, initials which also stand for the Latin phrase \"quod erat demonstrandum\", which translates as \"which was to be demonstrated\"). The Professor is an inventor and scientific detective, in the mold of Sherlock Holmes. The plot of the Soviet film \"Kin-dza-dza!\" (1986) centers on a desert planet, depleted of its resources, where an impoverished dog-eat-dog society uses steampunk machines, the movements and functions of which defy Earthly logic.\nIn making his 1986 Japanese film \"Castle in the Sky\", Hayao Miyazaki was heavily influenced by steampunk culture, the film featuring various airships and steampowered contraptions as well as a mysterious island that floats through the sky, accomplished not through magic as in most stories, but instead by harnessing the physical properties of a rare crystal\u2014analogous to the lodestone used in the of Swift's \"Gulliver's Travels\"\u2014augmented by massive propellers, as befitting the Victorian motif. The first \"Wallace &amp; Gromit\" animation \"A Grand Day Out\" (1989) features a space rocket in the steampunk style.\nThe second half of \"Back to the Future III\" (1990) gradually evolves into steampunk.\n\"The Adventures of Brisco County, Jr.\", a 1993 Fox Network TV science fiction-Western set in the 1890s, features elements of steampunk as represented by the character Professor Wickwire, whose inventions were described as \"the coming thing\". The short-lived 1995 TV show \"Legend\", on UPN, set in 1876 Arizona, features such classic inventions as a steam-driven \"quadrovelocipede\", trigoggle and night-vision goggles (\u00e0 la teslapunk), and stars John de Lancie as a thinly disguised Nikola Tesla.\nAlan Moore's and Kevin O'Neill's 1999 \"The League of Extraordinary Gentlemen\" graphic novel series (and the subsequent 2003 film adaption) greatly popularised the steampunk genre.\n\"Steamboy\" (2004) is a Japanese animated action film directed and co-written by Katsuhiro Otomo (\"Akira\"). It is a retro science-fiction epic set in a steampunk Victorian England. It features steamboats, trains, airships and inventors. The 2004 film \"Lemony Snicket's A Series of Unfortunate Events\" contains steampunk-esque elements such as costumes and vehicle interiors. The 2007 Syfy miniseries \"Tin Man\" incorporates a considerable number of steampunk-inspired themes into a reimagining of L. Frank Baum's \"The Wonderful Wizard of Oz\". Despite leaning more towards Gothic influences, the \"parallel reality\" of Meanwhile, City, within the 2009 film \"Franklyn\" contains many steampunk themes, such as costumery, architecture, minimal use of electricity (with a preference for gaslight), and absence of modern technology (such as there being no motorised vehicles or advanced weaponry, and the manual management of information without computers).\nThe 2009\u20132014 Syfy television series \"Warehouse 13\" features many steampunk-inspired objects and artifacts, including computer designs created by steampunk artisan Richard Nagy, a.k.a. \"Datamancer\". The 2010 episode of the TV series \"Castle\" entitled \"Punked\" (first aired October 11, 2010) prominently features the steampunk subculture and uses Los Angeles-area steampunks (such as the League of STEAM) as extras. The 2011 film \"The Three Musketeers\" has many steampunk elements, including gadgets and airships.\n\"The Legend of Korra\", a 2012\u20132014 Nickelodeon animated series, incorporates steampunk elements in an industrialized world with East Asian themes.\nThe \"Penny Dreadful\" (2014) television series is a Gothic Victorian fantasy series with steampunk props and costumes.\nThe 2013\u20132014 ABC3 game show \"Steam Punks!\", sees Paul Verhoeven playing The Inquisitor, who helps teams complete multiple challenges who have become trapped in a bizarre world controlled by an evil genius named The Machine.\nThe 2015 GSN reality television game show \"Steampunk'd\" features a competition to create steampunk-inspired art and designs which are judged by notable steampunks Thomas Willeford, Kato, and Matthew Yang King (as Matt King). Based on the work of cartoonist Jacques Tardi, \"April and the Extraordinary World\" (2015) is an animated movie set in a steampunk Paris. It features airships, trains, submarines, and various other steam-powered contraptions. Tim Burton's 2016 film \"Alice Through the Looking Glass\" features steampunk costumes, props, and vehicles.\nJapanese anime \"Kabaneri of the Iron Fortress\" (2016) features a steampunk zombie apocalypse.\nThe American fantasy animated sitcom, \"Disenchantment\", created by Matt Groening for Netflix, features a steampunk country named Steamland, led by an odd industrialist named Alva Gunderson voiced by Richard Ayoade, first appears in the season 1 episode, \"The Electric Princess.\" The country is portrayed as driven by logic and is egalitarian, governed by science, rather than magic, as is the case for Dreamland, where the protagonist, Princess Bean, is from. The country has cars, automatic lights, submarines, and other modern technologies, all of which are steam-powered, and references to Groening's other series, \"Futurama\". Steamland appears in three episodes of the show's second season, showing an explorers club as part of the country's high society, flying zeppelins, and robots with light bulbs for heads that chase the protagonists through the streets. Some even argued that Steamland is \"dieselpunk-inspired.\"\nThe 2023 film \"Poor Things\" has been noted for its \"steampunk-infused\" production design.\nVideo games.\nA variety of styles of video games have used steampunk settings.\n\"Steel Empire\" (1992), a shoot 'em up game originally released as \"Koutetsu Teikoku\" on the Sega Mega Drive console in Japan, is considered to be the first steampunk video game. Designed by Yoshinori Satake and inspired by Hayao Miyazaki's anime film \"\" (1986), \"Steel Empire\" is set in an alternate timeline dominated by steam-powered technology. The commercial success of \"Steel Empire\", both in Japan and the West, helped propel steampunk into the video game market, and had a significant influence on later steampunk games. The most notable steampunk game it influenced is \"Final Fantasy VI\" (1994), a Japanese role-playing game developed by Squaresoft and designed by Hiroyuki Ito for the Super Nintendo Entertainment System. \"Final Fantasy VI\" was both critically and commercially successful, and had a considerable influence on later steampunk video games.\n\"The Chaos Engine\" (1993) is a run and gun video game inspired by the Gibson/Sterling novel \"The Difference Engine\" (1990), set in a Victorian steampunk age. Developed by the Bitmap Brothers, it was first released on the Amiga in 1993; a sequel was released in 1996. The graphic adventure puzzle video games \"Myst\" (1993), \"Riven\" (1997), ' (2001), and ' (all produced by or under the supervision of Cyan Worlds) take place in an alternate steampunk universe, where elaborate infrastructures have been built to run on steam power. \"The Elder Scrolls\" (since 1994, last release in 2014) is an action role-playing game where one can find an ancient extinct race called dwemers or dwarves, whose steampunk technology is based on steam-powered levers and gears made of bronze or brass, which are maintained by magical techniques that have kept them in working order over the centuries.\n\"Sakura Wars\" (1996), a visual novel and tactical role-playing game developed by Sega for the Saturn console, is set in a steampunk version of Japan during the Meiji and Taish\u014d eras, and features steam-powered mecha robots. ' (1998), its sequels, \"Thief II\" (2000), ' (2004) and its reboot \"Thief\" (2014) are set in a steampunk metropolis. The 2001 computer role-playing game \"\" mixes fantasy tropes with steampunk.\nThe \"Professor Layton\" series of games (2007 debut) has several entries showcasing steampunk machinery and vehicles. Notably \"Professor Layton and the Unwound Future\" features a quasi-steampunk future setting. \"Solatorobo\" (2010) is a role-playing video game developed by CyberConnect2 set in a floating island archipelago populated by anthropomorphic cats and dogs, who pilot steampunk airships and engage in combat with robots. \"Resonance of Fate\" (2010) is a role-playing video game developed by tri-Ace and published by Sega for the PlayStation 3 and Xbox 360. It is set in a steampunk environment with combat involving guns.\n\"Impossible Creatures\" (2003) real-time strategy game inspired by the works of H. G. Welles, especially \"The Island of Doctor Moreau\". Developed by Relic Entertainment, it sees an adventurer building an army of genetically spliced animals to battle against a mad scientist who has abducted his father. The player's headquarters is a steam-powered \"Hovertrain\" locomotive, which functions as both a science lab and mobile command center. Coal is a key resource in the game, and must be burned to provide power to the players many base buildings.\nThe \"SteamWorld\" series of games (2010 debut) has the player controlling steam-powered robots. \"Minecraft\" (2011) has a steampunk-themed texture pack. \"Terraria\" (2011) is a video game developed by Re-Logic. It is a 2D open world platform game in which the player controls a single character in a generated world. It has a Steampunker non-player character in the game who sells items referencing Steampunk. \n\"LittleBigPlanet 2\" (2011) has the world Victoria's Laboratory, run by Victoria von Bathysphere, which mixes steampunk themes with confections. \"Guns of Icarus Online\" (2012) is multiplayer game with steampunk themes.\n\"Dishonored\" is a series (2012 debut) of stealth games with role-playing elements developed by Arkane Studios and widely considered to be a spiritual successor of the original \"Thief\" trilogy. Set in the Empire of the Isles, a steampunk Victorian metropolis where technology and supernatural magic coexist. Steam-powered robots and mechanical combat suits are present as enemies, as well as the presence of magic. The major locations in the Isles include Dunwall, the Empire's capital city which uses the burning of whale oil as the city's main fuel source, and Karnaca, which is powered by wind turbines fed by currents generated by a cleft mountain along the city's borders.\n\"BioShock Infinite\" (2013) is a first-person shooter game set in 1912, in a fictional city called Columbia, which uses technology to float in the sky and has many historical and religious scenes.\n\"\" (2014), a Japanese otome game for the PS Vita is set in a steampunk Victorian London, and features a cast with several historical figures with steampunk aesthetics. \"Code Name S.T.E.A.M.\" (2015), a Japanese tactical RPG game for the 3DS set in a steampunk fantasy version of London where you are a conscript in the strike force S.T.E.A.M. (short for Strike Team Eliminating the Alien Menace). \"They Are Billions\" (2017), is a steampunk strategy game in a post-apocalyptic setting. Players build a colony and attempt to ward off waves of zombies. \"Frostpunk\" (2018) is a city-building game set in 1888, but where the Earth is in the midst of a great Ice Age. Players must construct a city around a large steampunk heat generator with many steampunk aesthetics and mechanics, such as a \"Steam Core.\"\nCulture and community.\nBecause of the popularity of steampunk, there is a growing movement of adults that want to establish steampunk as a culture and lifestyle. Some fans of the genre adopt a steampunk aesthetic through fashion, home decor, music, and film. While Steampunk is considered the amalgamation of Victorian aesthetic principles with modern sensibilities and technologies, it can be more broadly categorised as neo-Victorianism, described by scholar Marie-Luise Kohlke as \"the afterlife of the nineteenth century in the cultural imaginary\". The subculture has its own magazine, blogs, and online shops.\nIn September 2012, a panel, chaired by steampunk entertainer Veronique Chevalier and with panelists including magician Pop Hadyn and members of the steampunk performance group the League of STEAM, was held at Stan Lee's Comikaze Expo. The panel suggested that because steampunk was inclusive of and incorporated ideas from various other subcultures such as goth, neo-Victorian, and cyberpunk, as well as a growing number of fandoms, it was fast becoming a \"super-culture\" rather than a mere subculture. Other steampunk notables such as Professor Elemental have expressed similar views about steampunk's inclusive diversity.\nSome have proposed a steampunk philosophy that incorporates punk-inspired anti-establishment sentiments typically bolstered by optimism about human potential. A 2004 \"Steampunk Manifesto,\" later republished in \"SteamPunk Magazine\", lamented that most \"so-called\" steampunk was nothing more than dressed-up recreationary nostalgia and proposed that \"authentic\" steampunk would \"take the levers of technology from the [technocrats] and powerful.\" American activist and performer Miriam Rosenberg Rocek impersonated anarcha-feminist Emma Goldman to inspire discussions around gender, society and politics. \"SteamPunk Magazine\" was edited and published by anarchists. Its founder, Margaret Killjoy, argued \"there have always been radical politics at the core of steampunk.\" Diana M. Pho, a science-fiction editor and author of the multicultural steampunk blog https://, similarly argued steampunk's \"progressive roots\" can be traced to its literary inspirations, including Verne's Captain Nemo. Steampunk authors Phenderson Dj\u00e8l\u00ed Clark, Jaymee Goh, Dru Pagliassotti, and Charlie Stross consider their work political.\nThese views are not universally shared. Killjoy lamented that even some diehard enthusiasts believe steampunk \"has nothing to offer but designer clothes.\" Pho argued many steampunk fans \"don't like to acknowledge that their attitudes could be considered ideological.\" The largest online steampunk community, http://, which is dedicated to what it calls the \"lighter side\" of steampunk, banned discussion about politics. Cory Gross, who was one of the first to write about the history and theory of steampunk, argued that the \"sepia-toned yesteryear more appropriate for Disney and grandparents than a vibrant and viable philosophy or culture\" denounced in the \"Steampunk Manifesto\" was in fact representative of the genre. Author Catherynne M. Valente called the punk in steampunk \"nearly meaningless.\" Kate Franklin and James Schafer, who at the time managed one of the largest steampunk groups on Facebook, admitted in 2011 that steampunk hadn't created the \"revolutionary, or even a particularly progressive\" community they wanted. Blogger and podcaster Eric Renderking Fisk announced in 2017 that steampunk was no longer punk, since it had \"lost the anti-authoritarian, anti-establishment aspects.\"\nOthers argued explicitly against turning steampunk into a political movement, preferring to see steampunk as \"escapism\" or a \"fandom\". In 2018, Nick Ottens, editor of the online alternate-history magazine \"Never Was\", declared that the \"lighter side\" of steampunk had won out. To the extent that steampunk is politicized, it appears to be an American and British phenomenon. Continental Europeans and Latin Americans are more likely to consider steampunk a hobby than a cause.\nSocial events.\nJune 19, 2005 marked the grand opening of the world's first steampunk club night, Malediction Society, in Los Angeles. The event ran for nearly 12 years at The Monte Cristo nightclub, interrupted by a single year residency at Argyle Hollywood, until both the club night and The Monte Cristo closed in April 2017. Though the steampunk aesthetic eventually gave way to a more generic goth and industrial aesthetic, Malediction Society celebrated its roots every year with \"The Steampunk Ball\".\nThe year 2006 saw the first \"SalonCon\", a neo-Victorian/steampunk convention. It ran for three consecutive years and featured artists, musicians (Voltaire and Abney Park), and authors (Catherynne M. Valente, Ekaterina Sedia, and G. D. Falksen). It also featured salons led by people prominent in their respective fields, workshops and panels on steampunk, and a seance, ballroom dance instruction, and the Chrononauts' Parade. The event was covered by MTV and \"The New York Times\". Since then, a number of popular steampunk conventions have sprung up the world over, with names like Steamcon (Seattle), the Steampunk World's Fair (Piscataway, New Jersey), Up in the Aether: The Steampunk Convention (Dearborn, Michigan), Steampunk NZ (Oamaru, New Zealand), Steampunk Unlimited (Strasburg Railroad, Lancaster, PA). Each year, on Mother's Day weekend, the city of Waltham, MA, turns over its city center and surrounding areas to host the Watch City Steampunk Festival, a US outdoor steampunk festival. In Kennebunk, ME the Brick Store Museum hosts the Southern Maine Steampunk Fair annually. During the first weekend of May, the Australian town of Nimmitabel celebrates Steampunk @ Altitude with some 2,000 people in attendance.\nIn recent years, steampunk has also become a regular feature at San Diego Comic-Con, with the Saturday of the four-day event being generally known among steampunks as \"Steampunk Day\", and culminating with a photo-shoot for the local press. In 2010, this was recorded in the Guinness Book of World Records as the world's largest steampunk photo shoot. In 2013, Comic-Con announced four official 2013 T-shirts, one of them featuring the official Rick Geary Comic-Con toucan mascot in steampunk attire. The Saturday steampunk \"after-party\" has also become a major event on the steampunk social calendar: in 2010, the headliners included The Slow Poisoner, Unextraordinary Gentlemen, and Aurelio Voltaire, with Veronique Chevalier as Mistress of Ceremonies and special appearance by the League of STEAM; in 2011, UXG returned with Abney Park.\nSteampunk has also sprung up recently at Renaissance Festivals and Renaissance Faires, in the US. Some festivals have organised events or a \"Steampunk Day\", while others simply support an open environment for donning steampunk attire. The Bristol Renaissance Faire in Kenosha, Wisconsin, on the Wisconsin/Illinois border, featured a Steampunk costume contest during the 2012 season, the previous two seasons having seen increasing participation in the phenomenon.\nSteampunk also has a growing following in the UK and Europe. The largest European event is \"Weekend at the Asylum\", held at The Lawn, Lincoln, every September since 2009. Organised as a not-for-profit event by the Ministry of Steampunk (formerly Victorian Steampunk Society), the Asylum is a dedicated steampunk event which takes over much of the historical quarter of Lincoln, England, along with Lincoln Castle. In 2011, there were over 1,000 steampunks in attendance. The event features the Empire Ball, Majors Review, Bazaar Eclectica, and the international Tea Duelling final.\n The Surrey Steampunk Convivial was originally held in New Malden, but since 2019 has been held in Stoneleigh in southwestern London, within walking distance of H. G. Wells's home. The Surrey Steampunk Convivial started as an annual event in 2012, and now takes place thrice a year, and has spanned three boroughs and five venues. Attendees have been interviewed by Phill Jupitus for BBC Radio 4 and filmed by the BBC World Service. The West Yorkshire village of Haworth has held an annual Steampunk weekend since 2013, on each occasion as a charity event raising funds for Sue Ryder's \"Manorlands\" hospice in Oxenhope. In September 2021, Finland's first steampunk festival was held at the V\u00e4in\u00f6 Linna Square and the Werstas Workers' House in Tampere, Pirkanmaa, Finland.\nOther.\nPhysicist Nicole Yunger Halpern's 2018 Ph.D. dissertation used the phrase \"Quantum Steampunk\" in the title. Yunger Halpern has discussed the relationship between her research in physics to historical thermodynamics research. In 2022 she published the book \"Quantum Steampunk\", which uses the idea of quantum steampunk to explain quantum physics. While the term has not been widely adopted, Yunger Halpern has promoted and discussed quantum steampunk as a genre, organizing a quantum steampunk writing competition at the University of Maryland.\nA 2012 conference paper on human factors in computing systems examined the use of steampunk as a design fiction for human-computer interaction (HCI). It concludes that \"the practices of DIY and appropriation that are evident in Steampunk design provide a useful set of design strategies and implications for HCI\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27686", "revid": "28766260", "url": "https://en.wikipedia.org/wiki?curid=27686", "title": "Spreadsheet", "text": "Computer program for working with tabular data\nA spreadsheet is a computer application for computation, organization, analysis and storage of data in tabular form. Spreadsheets were developed as computerized analogs of paper accounting worksheets. The program operates on data entered in cells of a table. Each cell may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells. The term \"spreadsheet\" may also refer to one such electronic document.\nIn modern spreadsheet applications, several spreadsheets, often known as \"worksheets\" or simply \"sheets\", are gathered together to form a \"workbook\". A workbook is physically represented by a file containing all the data for the book, the sheets, and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, \"Sheet 1!C10\". Some systems extend this syntax to allow cell references to different workbooks.\nSpreadsheet users can adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for \"what-if\" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets and can display data either as text and numerals or in graphical form.\nBesides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial accountancy and statistical operations. Such calculations as net present value, standard deviation, or regression analysis can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.\nSpreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.\nBasics.\nLANPAR, available in 1969, was the first electronic spreadsheet on mainframe and time sharing computers. LANPAR was an acronym: LANguage for Programming Arrays at Random. VisiCalc (1979) was the first electronic spreadsheet on a microcomputer, and it helped turn the Apple II into a popular and widely used personal computer. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system. Microsoft Excel now has the largest market share on the Windows and Macintosh platforms. A spreadsheet program is a standard feature of an office productivity suite. In 2006 Google launched a beta release spreadsheet web application, this is currently known as Google Sheets and one of the applications provided in Google Drive.\nA spreadsheet consists of a table of \"cells\" arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, \"A,\" \"B,\" \"C,\" etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, \"C10\". This electronic concept of cell references was first introduced in LANPAR (Language for Programming Arrays at Random) (co-invented by Rene Pardo and Remy Landau) and a variant used in VisiCalc and known as \"A1 notation\". \nAdditionally, spreadsheets have the concept of a \"range\", a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range \"A1:A10\". LANPAR innovated forward referencing/natural order calculation which didn't re-appear until Lotus 123 and Microsoft's MultiPlan Version 2.\nUsers interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text codice_1, the number codice_2 or the date codice_3. A formula would begin with the equals sign, codice_4, but this would normally be invisible because the display shows the \"result\" of the calculation, codice_5 in this case, not the formula itself. This may lead to confusion in some cases.\nThe key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may, in turn, be the result of a formula. To make such a formula, one replaces a number with a cell reference. For instance, the formula codice_6 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value codice_7 the result will be codice_5. But C10 might also hold its formula referring to other cells, and so on.\nThe ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical steps, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the codice_9 function that adds up all the numbers within a range.\nSpreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same things. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable\u2014sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.\nA spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually, sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands instead of a graphical user interface.\nHistory.\nPaper spreadsheets.\nHumans have organized data into tables, that is, grids of columns and rows, since ancient times. The Babylonians used clay tablets to store data as far back as 1800 BCE. Other examples can be found in book-keeping ledgers and astronomical records.\nSince at least 1906 the term \"spread sheet\" has been used in accounting to mean a grid of columns and rows in a ledger. Prior to the rise of computerized spreadsheets, \"spread\" also referred to a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the centerfold and treating the two pages as one large page. The compound word 'spread-sheet' came to mean the format used to present book-keeping ledgers\u2014with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect\u2014which were, traditionally, a \"spread\" across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed 'analysis paper') ruled into rows and columns in that format and approximately twice as wide as ordinary paper.\nElectronic spreadsheets.\nBatch spreadsheet report generator BSRG.\nA batch \"spreadsheet\" is indistinguishable from a batch compiler with added input data, producing an output report, \"i.e.\", a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper \"Budgeting Models and System Simulation\" by Richard Mattessich. The subsequent work by Mattessich (1964a, Chpt. 9, \"Accounting and Analytical Methods\") and its companion volume, Mattessich (1964b, \"Simulation of the Firm through a Budget Computer Program\") applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual \"cells\".\nIn 1962, this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled \"Business Computer Language\" was written by Kimball, Stoffells and Walsh. Both the book and program were copyrighted in 1966 and years later that copyright was renewed.\nApplied Data Resources had a FORTRAN preprocessor called Empires.\nIn the late 1960s, Xerox used BCL to develop a more sophisticated version for their timesharing system.\nLANPAR spreadsheet compiler.\nA key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 https:// on a spreadsheet automatic natural order calculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the Predecessor Court of the Federal Circuit (CCPA), overturning the Patent Office in 1983\u00a0\u2014 establishing that \"something does not cease to become patentable merely because the point of novelty is in an algorithm.\" However, in 1995 a federal district court ruled the patent unenforceable due to inequitable conduct by the inventors during the application process. The United States Court of Appeals for the Federal Circuit upheld that decision in 1996.\nThe actual software was called LANPAR\u00a0\u2014 LANguage for Programming Arrays at Random. This was conceived and entirely developed in the summer of 1969, following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having an electronic computer calculate results in the right order (\"Forward Referencing/Natural Order Calculation\"). Pardo and Landau developed and implemented the software in 1969.\nLANPAR was used by Bell Canada, AT&amp;T, and the 18 operating telephone companies nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first \"non-procedural\" computer languages) as opposed to the left-to-right, top-to-bottom sequence for calculating the results in each cell that was used by VisiCalc, SuperCalc, and the first version of MultiPlan. Without forward referencing/natural order calculation, the user had to refresh the spreadsheet until the values in all cells remained unchanged. Once the cell values stayed constant, the user was assured that there were no remaining uncalculated forward references within the spreadsheet.\nAutoplan/Autotab spreadsheet programming language.\nIn 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. \"AutoPlan\" ran on GE's Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name \"AutoTab\". (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.)\nAutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column. In 1975, Autotab-II was advertised as extending the original to a maximum of \"1,500 rows and columns, combined in any proportion the user requires...\"\nGE Information Services, which operated the time-sharing service, also launched its own spreadsheet system, Financial Analysis Language (FAL), circa 1974. It was later supplemented by an additional spreadsheet language, TABOL, which was developed by an independent author, Oliver Vellacott in the UK. Both FAL and TABOL were integrated with GEIS's database system, DMS.\nIBM Financial Planning and Control System.\nThe IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was the first application for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial planning data drawn from the legacy batch system into each user's spreadsheet monthly. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.\nAPLDOT modeling language.\nAn example of an early \"industrial weight\" spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD. The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a \"spreadsheet\" because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.\nVisiCalc for the Apple II.\nThe concept of spreadsheets became widely known due to VisiCalc, developed for the Apple II in 1979 by VisiCorp staff Dan Bricklin and Bob Frankston.\nSignificantly, it also turned the personal computer from a hobby for computer enthusiasts into a business tool.\nVisiCalc was the first spreadsheet that combined many of the essential features of modern spreadsheet applications, such as a WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, and formula building by selecting referenced cells. Unaware of LANPAR at the time, \"PC World\" magazine called VisiCalc the first electronic spreadsheet.\nBricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite several sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc.\nVisiCalc for the Apple II went on to become the first killer application, a program so compelling, people would buy a particular computer just to use it. It was ported to other computers, including CP/M machines, Atari 8-bit computers, and the Commodore PET, but VisiCalc remains best known as an Apple II program.\nSuperCalc for CP/M.\nSuperCalc was a spreadsheet application published by Sorcim in 1980, and originally bundled (along with WordStar) as part of the CP/M software package included with the Osborne 1 portable computer. It quickly became the de facto standard spreadsheet for CP/M.\nLotus 1-2-3 spreadsheet for IBM PC DOS.\nThe introduction of Lotus 1-2-3 in November 1982 accelerated the acceptance of the IBM Personal Computer. It was written especially for IBM PC DOS and had improvements in speed and graphics compared to VisiCalc on the Apple II, this helped it grow in popularity. Lotus 1-2-3 was the leading spreadsheet for several years.\nMicrosoft Excel for Apple Macintosh and Windows.\nMicrosoft released the first version of Excel for the Apple Macintosh on September 30, 1985, and then ported it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. Microsoft's Windows 3.x platforms of the early 1990s made it possible for their Excel spreadsheet application to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3, and in 2013, IBM discontinued Lotus 1-2-3 altogether.\nGoogle Sheets, Online, Web-based spreadsheets.\nIn 2006 Google launched their beta release Google Sheets, a web based spreadsheet application that can be accessed by multiple users from any device type using a compatible web browser, it can be used online and offline (with or without internet connectivity). Google Sheets originated from a web-based spreadsheet application XL2Web developed by 2Web Technologies, combined with DocVerse which enabled multiple-user online collaboration of Office documents.\nIn 2016 Collabora Online Calc was launched, notable in that the web based spreadsheet could be hosted and integrated into any environment without dependency on a 3rd party for authentication or maintenance. Collabora Online runs LibreOffice kit at its core, which grew from StarOffice that was launched in 1985.\nOther spreadsheets.\nNotable current spreadsheet software:\nDiscontinued spreadsheet software:\nOther products.\nSeveral companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day.\nSpreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.\nConcepts.\nThe main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are general numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.\nCells.\nA \"cell\" can be thought of as a box for holding data. A single cell is usually referenced by its column and row (C2 would represent the cell containing the value 30 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).\nAn array of cells is called a \"sheet\" or \"worksheet\". It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its containing cell).\nA cell may contain a value or a formula, or it may simply be left empty.\nBy convention, formulas usually begin with = sign.\nValues.\nA value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.\nThe Spreadsheet \"Value Rule\"\nComputer scientist Alan Kay used the term \"value rule\" to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell. The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.\nAutomatic recalculation.\nA standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate since the recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.\nRecalculation generally requires that there are no circular dependencies in a spreadsheet. A dependency graph is a graph that has a vertex for each object to be updated, and an edge connecting two objects whenever one of them needs to be updated earlier than the other. Dependency graphs without circular dependencies form directed acyclic graphs, representations of partial orderings (in this case, across a spreadsheet) that can be relied upon to give a definite result.\nReal-time update.\nThis feature refers to updating a cell's contents periodically with a value from an external source\u2014such as a cell in a \"remote\" spreadsheet. For shared, Web-based spreadsheets, it applies to \"immediately\" updating cells another user has updated. All dependent cells must be updated also.\nLocked cell.\nOnce entered, selected cells (or the entire spreadsheet) can optionally be \"locked\" to prevent accidental overwriting. Typically this would apply to cells containing formulas but might apply to cells containing \"constants\" such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.\nData format.\nA cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example \"31/12/2007\" or \"31 Dec 2007\" would default to the cell format of \"date\".\nSimilarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.\nSome cell formats such as \"numeric\" or \"currency\" can also specify the number of decimal places.\nThis can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.\nCell formatting.\nDepending on the capability of the spreadsheet application, each cell (like its counterpart the \"style\" in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.\nA cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.\nNamed cells.\nIn most implementations, a cell, or group of cells in a column or row, can be \"named\" enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). The use of named variables and named functions also makes the spreadsheet structure more transparent.\nCell reference.\nIn place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same sheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or a value from a remote application.\nA typical cell reference in \"A1\" style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A\u2013Z and AA\u2013IV) followed by a row number (e.g., in the range 1\u201365536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative \"R1C1\" reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.\nWhen the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), causes the computer to fetch the value of the named cell(s).\nA cell on the same \"sheet\" is usually addressed as:\ncodice_10\nA cell on a different sheet of the same spreadsheet is usually addressed as:\n =SHEET2!A1 (that is; the first cell in sheet 2 of the same spreadsheet).\nSome spreadsheet implementations in Excel allow cell references to another spreadsheet (not the currently open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:\n ='C:\\Documents and Settings\\Username\\My spreadsheets\\[main sheet]Sheet1!A1\nIn a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the values of the additional rows\u2014which they often do not.\nA circular reference occurs when the formula in one cell refers\u2014directly, or indirectly through a chain of cell references\u2014to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.\nCell ranges.\nLikewise, instead of using a named range of cells, a range reference can be used. A reference to a range of cells typically takes the form (A1:A6), which specifies all the cells in column A from row 1 through row 6. A formula such as \"=SUM(A1:A6)\" would add all the cells specified and put the result in the cell containing the formula itself.\nSheets.\nIn the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.\nFormulas.\nA formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula, therefore, has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by \"clicking\" the mouse over a particular cell; otherwise, it contains the result of the calculation.\nA formula assigns values to a cell or range of cells, and typically has the format:\nwhere the expression consists of:\nWhen a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., codice_14, or codice_15), absolute (e.g., codice_26, or codice_27) or mixed row\u2013 or column-wise absolute/relative (e.g., codice_28 is column-wise absolute and codice_29 is row-wise absolute).\nThe available options for valid formulas depend on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.\nA formula may contain a condition (or nested conditions)\u2014with or without an actual calculation\u2014and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.\n=IF(SUM(A1:A6) &gt; 100, \"More than 100%\", SUM(A1:A6))\nFurther examples:\n=IF(AND(A1&lt;&gt;\",B1&lt;&gt;\"),A1/B1,\") means that if both cells A1 and B1 are not &lt;&gt; empty \", then divide A1 by B1 and display, other do not display anything.\n=IF(AND(A1&lt;&gt;\",B1&lt;&gt;\"),IF(B1&lt;&gt;0,A1/B1,\"Division by zero\"),\"\") means that if cells A1 and B1 are not empty, and B1 is not zero, then divide A1 by B1, if B1 is zero, then display \"Division by zero\", and do not display anything if either A1 and B1 are empty.\n=IF(OR(A1&lt;&gt;\",B1&lt;&gt;\"),\"Either A1 or B1 show text\",\"\") means to display the text if either cells A1 or B1 are not empty.\nThe best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.\nA spreadsheet does not have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable, or simple list. Because of its ease of use, formatting, and hyperlinking capabilities, many spreadsheets are used solely for this purpose.\nFunctions.\nSpreadsheets usually contain several supplied functions, such as arithmetic operations (for example, summations, averages, and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for \"user-defined functions\". In Microsoft Excel, these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. Also, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name \"sq\" is user-assigned, and the function \"sq\" is introduced using the \"Visual Basic\" editor supplied with Excel. \"Name Manager\" displays the spreadsheet definitions of named variables \"x\" &amp; \"y\".\nSubroutines.\nFunctions themselves cannot write into the worksheet but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable \"x\", calculates its square, and writes this value into the corresponding element of named column variable \"y\". The \"y\" column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.\nRemote spreadsheet.\nWhenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a \"remote\" spreadsheet. The contents of the referenced cell may be accessed either on the first reference with a manual update or more recently in the case of web-based spreadsheets, as a near real-time value with a specified automatic refresh interval.\nCharts.\nMany spreadsheet applications permit charts and graphs (e.g., histograms, pie charts) to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object. To create an Excel histogram, a formula based on the REPT function can be used.\nMulti-dimensional spreadsheets.\nIn the late 1980s and early 1990s, first Javelin Software and Lotus Improv appeared. Unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin\u2014the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation.\nIn these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two-time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets\u2014variables, and therefore data, could not be destroyed by deleting a row, column, or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables, was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, the program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names and to perform multidimensional analysis and massive, but easily editable consolidations.\nTrapeze, a spreadsheet on the Mac, went further and explicitly supported\nnot just table columns, but also matrix operators.\nLogical spreadsheets.\nSpreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.\nProgramming issues.\nJust as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.\nEnd-user development.\nSpreadsheets are a popular end-user development tool. EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.\nSpreadsheet programs.\nA \"spreadsheet program\" is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.\nIt is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.\nSpreadsheets usually attempt to automatically update cells when the cells depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cell formulas are not generally invertible, though, this technique is of somewhat limited value.\nMany of the concepts common to sequential programming models have analogs in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).\nSpreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.\nShortcomings.\nWhile spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.\nDespite the high error risks often associated with spreadsheet authorship and use, specific steps can be taken to significantly enhance control and reliability by structurally reducing the likelihood of error occurrence at their source.\nThese drawbacks are mitigated by the use of named variables for cell designations, and employing variables in formulas rather than cell locations and cell-by-cell manipulations. Graphs can be used to show instantly how results are changed by changes in parameter values. The spreadsheet can be made invisible except for a transparent user interface that requests pertinent input from the user, displays results requested by the user, creates reports, and has built-in error traps to prompt correct input.\nSpecifically, spreadsheets typically contain many copies of the same formula. When the formula is modified, the user has to change every cell containing that formula. In contrast, most computer languages allow a formula to appear only once in the code and achieve repetition using loops: making them much easier to implement and audit.\nOther problems associated with spreadsheets include:\nWhile there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness, and use of these is generally low. Many professionals in the financial field \"don't know\" how their spreadsheets are audited; only 6% invest in a third-party solution.\nSpreadsheet risk.\nSpreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilized in making a related (usually numerically based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses, or the size of a load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g., out of date exchange rates). Some single-instance errors have exceeded US$1\u00a0billion. Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.\nDespite this, research carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over \u00a350m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.\nIn 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst, found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in \"Growth in a Time of Debt\", a very influential 2010 journal article which was widely used as justification to drive 2010\u20132013 European austerity programs.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27687", "revid": "38001712", "url": "https://en.wikipedia.org/wiki?curid=27687", "title": "St. Louis", "text": "St. Louis ( , sometimes referred to as St. Louis City, Saint Louis or STL) is an independent city in the U.S. state of Missouri. It lies near the confluence of the Mississippi and the Missouri rivers. In 2020, the city proper had a population of 301,578, while its metropolitan area, which extends into Illinois, had an estimated population of over 2.8 million. It is the largest metropolitan area in Missouri and the second-largest in Illinois. The city's combined statistical area is the 20th-largest in the United States.\nThe land that became St. Louis had been occupied by Native American cultures for thousands of years before European settlement. The city was founded on February 14, 1764, by French fur traders Gilbert Antoine de St. Maxent, Pierre Lacl\u00e8de, and Auguste Chouteau. They named it for King Louis IX of France, and it quickly became the regional center of the French Illinois Country. In 1804, the United States acquired St. Louis as part of the Louisiana Purchase. In the 19th century, St. Louis developed as a major port on the Mississippi River; from 1870 until the 1920 census, it was the fourth-largest city in the country. It separated from St. Louis County in 1877, becoming an independent city and limiting its political boundaries. In 1904, it hosted the Louisiana Purchase Exposition, also known as the St. Louis World's Fair, and the Summer Olympics.\nSt. Louis is designated as one of 173 global cities by the Globalization and World Cities Research Network. The GDP of Greater St. Louis was $226.6 billion in 2023. St. Louis has a diverse economy with strengths in the service, manufacturing, trade, transportation, and aviation industries. It is home to sixteen \"Fortune\" 1000 companies, six of which are also \"Fortune\" 500 companies. Federal agencies headquartered in the city or with significant operations there include the Federal Reserve Bank of St. Louis, the U.S. Department of Agriculture, and the National Geospatial-Intelligence Agency.\nThe city's attractions include the Gateway Arch in Downtown St. Louis, the St. Louis Zoo, the Missouri Botanical Garden, the Saint Louis Art Museum, and Bellefontaine Cemetery. Major research universities in Greater St. Louis include Washington University in St. Louis, Saint Louis University, and the University of Missouri\u2013St. Louis. The Washington University Medical Center hosts an agglomeration of medical and pharmaceutical institutions, including Barnes-Jewish Hospital. St. Louis has four professional sports teams: the St. Louis Cardinals of Major League Baseball, the St. Louis Blues of the National Hockey League, St. Louis City SC of Major League Soccer, and the St. Louis BattleHawks of the United Football League.\nHistory.\nMississippian culture and European exploration.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nHistorical affiliations\n Kingdom of France 1690s\u20131763 Kingdom of Spain 1763\u20131800 French First Republic 1800\u20131803 1803\u2013present\nThe area that became St. Louis was a center of the Native American Mississippian culture, which built numerous temple and residential earthwork mounds on both sides of the Mississippi River. Their major regional center was at Cahokia Mounds, active from 900 to 1500. Due to numerous major earthworks within St. Louis boundaries, the city was nicknamed as the \"Mound City\". These mounds were mostly demolished during the city's development. Historic Native American tribes in the area encountered by early Europeans included the Siouan-speaking Osage people, whose territory extended west, and the Illiniwek. Sugarloaf Mound in South St. Louis was repatriated to the Osage Nation in 2025.\nEuropean exploration of the area was first recorded in 1673, when French explorers Louis Jolliet and Jacques Marquette traveled through the Mississippi River valley. Five years later, La Salle claimed the region for France as part of La Louisiane, also known as Louisiana. The earliest European settlements in the Illinois Country (also known as Upper Louisiana) were built by the French during the 1690s and early 1700s at Cahokia, Kaskaskia, and Fort de Chartres. Migrants from the French villages on the east side of the Mississippi River, such as Kaskaskia, also founded Ste. Genevieve in the 1730s.\nIn 1764, after France lost the Seven Years' War, Pierre Lacl\u00e8de and his stepson Auguste Chouteau founded what was to become the city of St. Louis. (French lands east of the Mississippi had been ceded to Great Britain and the lands west of the Mississippi to Spain; Catholic France and Spain were 18th-century allies. Louis XV of France and Charles III of Spain were cousins, both from the House of Bourbon.) The French families built the city's economy on the fur trade with the Osage, and with more distant tribes along the Missouri River. The Chouteau brothers gained a monopoly from Spain on the fur trade with Santa Fe. French colonists used African slaves as domestic servants and workers in the city.\nDuring the negotiations for the 1763 Treaty of Paris, French negotiators agreed to transfer France's colonial territories west of the Mississippi and Missouri rivers to New Spain to compensate for Spanish territorial losses during the war. These areas remained under Spanish control until 1803, when they were transferred to the French First Republic. During the American Revolutionary War, St. Louis was unsuccessfully attacked by British-allied Native Americans in the 1780 Battle of St. Louis.\nFounding.\nThe founding of St. Louis was preceded by a trading business between Gilbert Antoine de St. Maxent and Pierre Lacl\u00e8de (Liguest) in late 1763. St. Maxent invested in a Mississippi River expedition led by Lacl\u00e8de, who searched for a location to base the company's fur trading operations. Though Ste. Genevieve was already established as a trading center, he sought a place less prone to flooding. He found an elevated area overlooking the flood plain of the Mississippi River, not far south from its confluence with the Missouri and Illinois rivers. In addition to having an advantageous natural drainage system, there were nearby forested areas to supply timber and grasslands which could easily be converted for agricultural purposes. Lacl\u00e8de declared that this place \"might become, hereafter, one of the finest cities in America\". He dispatched his 14-year-old stepson, Auguste Chouteau, to the site, with the support of 30 settlers in February 1764.\nLacl\u00e8de arrived at the future town site two months later and produced a plan for St. Louis based on the New Orleans street plan. The default block size was 240 by 300 feet, with just three long avenues running parallel to the west bank of the Mississippi. He established a public corridor of 300 feet fronting the river, but later this area was released for private development.\nFor the city's first few years, it was not recognized by any governments. Although the settlement was thought to be under the control of the Spanish government, no one asserted any authority over it, and thus St. Louis had no local government. This vacuum led Lacl\u00e8de to assume civil control, and all problems were disposed in public settings, such as communal meetings. In addition, Lacl\u00e8de granted new settlers lots in town and the surrounding countryside. In hindsight, many of these original settlers thought of these first few years as \"the golden age of St. Louis\". In 1763, the Native Americans in the region around St. Louis began expressing dissatisfaction with the victorious British, objecting to their refusal to continue to the French tradition of supplying gifts to Natives. Odawa chieftain Pontiac began forming a pan-tribal alliance to counter British control over the region but received little support from the indigenous residents of St. Louis. By 1765, the city began receiving visits from representatives of the British, French, and Spanish governments.\nSt. Louis was transferred to the French First Republic in 1800 (although all of the colonial lands continued to be administered by Spanish officials), then sold by the French to the U.S. in 1803 as part of the Louisiana Purchase. St. Louis became the capital of, and gateway to, the new territory. Shortly after the official transfer of authority was made, the Lewis and Clark Expedition was commissioned by President Thomas Jefferson. The expedition departed from St. Louis in May 1804 along the Missouri River to explore the vast territory. There were hopes of finding a water route to the Pacific Ocean, but the party had to go overland in the Upper West. They reached the Pacific Ocean via the Columbia River in summer 1805. They returned, reaching St. Louis on September 23, 1806. Both Lewis and Clark lived in St. Louis after the expedition. Many other explorers, settlers, and trappers (such as Ashley's Hundred) would later take a similar route to the West.\n19th century.\nThe city elected its first municipal legislators (called trustees) in 1808. Steamboats first arrived in St. Louis in 1817, improving connections with New Orleans and eastern markets. Missouri was admitted as a state in 1821. St. Louis was incorporated as a city in 1822, and continued to develop largely due to its busy port and trade connections.\nImmigrants from Ireland and Germany arrived in St. Louis in significant numbers starting in the 1840s, and the population of St. Louis grew from less than 20,000 inhabitants in 1840, to 77,860 in 1850, to more than 160,000 by 1860. By the mid-1800s, St. Louis had a greater population than New Orleans.\nSettled by many Southerners in a slave state, the city was split in political sympathies and became polarized during the American Civil War. In 1861, 28 civilians were killed in a clash with Union troops. The war hurt St. Louis economically, due to the Union blockade of river traffic to the south on the Mississippi River. The St. Louis Arsenal constructed ironclads for the Union Navy.\nSlaves worked in many jobs on the waterfront and on the riverboats. Given the city's location close to the free state of Illinois and others, some slaves escaped to freedom. Others, especially women with children, sued in court in freedom suits, and several prominent local attorneys aided slaves in these suits. About half the slaves achieved freedom in hundreds of suits before the American Civil War began in 1861. The printing press of abolitionist Elijah Parish Lovejoy was destroyed for the third time by townsfolk. He was murdered the next year in nearby Alton, Illinois.\nAfter the war, St. Louis profited via trade with the West, aided by the 1874 completion of the Eads Bridge, named for its design engineer. Industrial developments on both banks of the river were linked by the bridge, the second in the Midwest over the Mississippi River after the Hennepin Avenue Bridge in Minneapolis. The bridge connects St. Louis, Missouri to East St. Louis, Illinois. The Eads Bridge became a symbolic image of the city of St. Louis, from the time of its erection until 1965 when the Gateway Arch Bridge was constructed. The bridge crosses the St. Louis riverfront between Laclede's Landing, to the north, and the grounds of the Gateway Arch, to the south. Today the road deck has been restored, allowing vehicular and pedestrian traffic to cross the river. The St. Louis MetroLink light rail system has used the rail deck since 1993. An estimated 8,500 vehicles pass through it daily.\nOn August 22, 1876, the city of St. Louis voted to secede from St. Louis County and become an independent city, and, following a recount of the votes in November, officially did so in March 1877. The 1877 St. Louis general strike caused significant upheaval, in a fight for the eight-hour day and the banning of child labor.\nIndustrial production continued to increase during the late 19th century. Major corporations such as the Anheuser-Busch brewery, Ralston Purina company and Desloge Consolidated Lead Company were established at St. Louis which was also home to several brass era automobile companies, including the Success Automobile Manufacturing Company; St. Louis is the site of the Wainwright Building, a skyscraper designed in 1892 by architect Louis Sullivan.\n20th century.\nIn 1900, the entire streetcar system was shut down by a several months-long strike, with significant unrest occurring in the city &amp; violence against the striking workers.\nIn 1904, the city hosted the World's Fair and the Olympics, becoming the first non-European city to host the games. The formal name for the 1904 World's Fair was the Louisiana Purchase Exposition. Permanent facilities and structures remaining from the fair are located in Forest Park, and other notable structures within the park's boundaries include the St. Louis Art Museum, the St. Louis Zoo and the Missouri History Museum, and Tower Grove Park and the Botanical Gardens.\nAfter the Civil War, social and racial discrimination in housing and employment were common in St. Louis. In 1916, during the Jim Crow Era, St. Louis passed a residential segregation ordinance saying that if 75% of the residents of a neighborhood were of a certain race, no one from a different race was allowed to move in. That ordinance was struck down in a court challenge, by the NAACP, after which racial covenants were used to prevent the sale of houses in certain neighborhoods to \"persons not of Caucasian race\". Again, St. Louisans offered a lawsuit in challenge, and such covenants were ruled unconstitutional by the U.S. Supreme Court in 1948 in \"Shelley v. Kraemer\".\nIn 1926, Douglass University, a historically black university was founded by B. F. Bowles in St. Louis, and at the time no other college in St. Louis County admitted black students.\nIn the first half of the 20th century, St. Louis was a destination in the Great Migration of African Americans from the rural South seeking better opportunities. During World War II, the NAACP campaigned to integrate war factories. In 1964, civil rights activists protested at the construction of the Gateway Arch to publicize their effort to gain entry for African Americans into the skilled trade unions, where they were underrepresented. The Department of Justice filed the first suit against the unions under the Civil Rights Act of 1964.\nBetween 1900 and 1929, St. Louis, had about 220 automakers, close to 10 percent of all American carmakers, about half of which built cars exclusively in St. Louis. Notable names include Dorris, Gardner and Moon.\nIn the first part of the century, St. Louis had some of the worst air pollution in the United States. In April 1940, the city banned the use of soft coal mined in nearby states. The city hired inspectors to ensure that only anthracite was burned. By 1946, the city had reduced air pollution by about 75%.\n\"De jure\" educational segregation continued into the 1950s, and \"de facto\" segregation continued into the 1970s, leading to a court challenge and interdistrict desegregation agreement. Students have been bused mostly from the city to county school districts to have opportunities for integrated classes, although the city has created magnet schools to attract students.\nSt. Louis, like many Midwestern cities, expanded in the early 20th century due to industrialization, which provided jobs to new generations of immigrants and migrants from the South. It reached its peak population of 856,796 at the 1950 census. Suburbanization from the 1950s through the 1990s dramatically reduced the city's population, as did restructuring of industry and loss of jobs. The effects of suburbanization were exacerbated by the small geographical size of St. Louis due to its earlier decision to become an independent city, and it lost much of its tax base. During the 19th and 20th century, most major cities aggressively annexed surrounding areas as residential development occurred away from the central city; however, St. Louis was unable to do so.\nSeveral urban renewal projects were built in the 1950s, as the city worked to replace old and substandard housing. Some of these were poorly designed and resulted in problems. One prominent example, Pruitt\u2013Igoe, became a symbol of failure in public housing, and was torn down less than two decades after it was built. The degradation and razing of Mill Creek Valley in this time was featured as an example of disenfranchisement in the 2024 Reparations Commission Report.\nSince the 1980s, several revitalization efforts have focused on Downtown St. Louis.\n21st century.\nThe urban revitalization projects that started in the 1980s continued into the new century. The city's old garment district, centered on Washington Avenue in the Downtown and Downtown West neighborhoods, experienced major development starting in the late 1990s as many of the old factory and warehouse buildings were converted into lofts. The American Planning Association designated Washington Avenue as one of 10 Great Streets for 2011. The Cortex Innovation Community, located within the city's Central West End neighborhood, was founded in 2002 and has become a multi-billion dollar economic engine for the region, with companies such as Microsoft and Boeing currently leasing office space. The Forest Park Southeast neighborhood in the central corridor has seen major investment starting in the early 2010s. Between 2013 and 2018, over $50 million worth of residential construction has been built in the neighborhood. The population of the neighborhood has increased by 19% from the 2010 to 2020 Census.\nThe St. Louis Rams of the National Football League controversially returned to Los Angeles in 2016. The city of St. Louis sued the NFL in 2017, alleging the league breached its own relocation guidelines to profit at the expense of the city. In 2021, the NFL and Rams owner Stan Kroenke agreed to settle out of court with the city for $790 million.\nOn May 16, 2025, a mile-wide EF3 tornado struck northern parts of St. Louis, killing 5. According to mayor Cara Spencer, the tornado produced up to $1.6 billion in damage, and governor Mike Kehoe stated on behalf of FEMA that the scope of residential damage was the worst since the Joplin tornado over a decade prior.\nGeography.\nTopography.\nAccording to the United States Census Bureau, St. Louis has a total area of , of which is land and (6.2%) is water. The city is built on bluffs and terraces that rise 100\u2013200 feet above the western banks of the Mississippi River, just south its confluence with the Missouri River and directly across from Illinois. The eastern city limits follow the Mississippi River as the state border. Much of the area is a fertile and gently rolling prairie that features low hills and broad, shallow valleys. Both the Mississippi River and the Missouri River have cut large valleys with wide flood plains.\nNear the southern boundary of the city of St. Louis (separating it from St. Louis County) is the River des Peres, practically the only river or stream within the city limits that is not entirely underground. Most of River des Peres was confined to a channel or put underground in the 1920s and early 1930s. The lower section of the river was the site of some of the worst flooding of the Great Flood of 1993.\nThe St. Louis area experienced a significant flood in 1973, primarily caused by the Mississippi River reaching its highest level in over 150 years. While heavy rainfall and snowmelt contributed, the flood was exacerbated by a saturated drainage basin and high water levels in the Missouri River. The flooding led to levee failures, inundation of homes, businesses, and parkland, and prompted evacuations and sandbagging efforts.\nLimestone and dolomite of the Mississippian epoch underlie the area, and parts of the city are karst in nature. This is particularly true of the area south of downtown, which has numerous sinkholes and caves. Most of the caves in the city have been sealed, but many springs are visible along the riverfront. Coal, brick clay, and millerite ore were once mined in the city. The predominant surface rock, known as \"St. Louis limestone\", is used as dimension stone and rubble for construction.\nArchitecture.\nThe architecture of St. Louis exhibits a variety of commercial, residential, and monumental architecture. St. Louis is known for the Gateway Arch, the tallest monument constructed in the United States at . The Arch pays homage to Thomas Jefferson and St. Louis's position as the gateway to the West. Architectural influences reflected in the area include French Colonial, German, early American, and modern architectural styles.\nSeveral examples of religious structures are extant from the pre-Civil War period, and most reflect the common residential styles of the time. Among the earliest is the Basilica of St. Louis, King of France (referred to as the \"Old Cathedral\"). The Basilica was built between 1831 and 1834 in the Federal style. Other religious buildings from the period include SS. Cyril and Methodius Church (1857) in the Romanesque Revival style and Christ Church Cathedral (completed in 1867, designed in 1859) in the Gothic Revival style.\nA few civic buildings were constructed during the early 19th century. The original St. Louis courthouse was built in 1826 and featured a Federal style stone facade with a rounded portico. However, this courthouse was replaced during renovation and expansion of the building in the 1850s. The Old St. Louis County Courthouse (known as the \"Old Courthouse\") was completed in 1864 and was notable for having a cast iron dome and for being the tallest structure in Missouri until 1894. Finally, a customs house was constructed in the Greek Revival style in 1852, but was demolished and replaced in 1873 by the U.S. Customhouse and Post Office.\nBecause much of the city's commercial and industrial development was centered along the riverfront, many pre-Civil War buildings were demolished during construction of the Gateway Arch. The city's remaining architectural heritage of the era includes a multi-block district of cobblestone streets and brick and cast-iron warehouses called Laclede's Landing. Now popular for its restaurants and nightclubs, the district is located north of Gateway Arch along the riverfront. Other industrial buildings from the era include some portions of the Anheuser-Busch Brewery, which date to the 1860s.\nSt. Louis saw a vast expansion in variety and number of religious buildings during the late 19th century and early 20th century. The largest and most ornate of these is the Cathedral Basilica of St. Louis, designed by Thomas P. Barnett and constructed between 1907 and 1914 in the Neo-Byzantine style. The St. Louis Cathedral, as it is known, has one of the largest mosaic collections in the world. Another landmark in religious architecture of St. Louis is the St. Stanislaus Kostka, which is an example of the Polish Cathedral style. Among the other major designs of the period were St. Alphonsus Liguori (known as \"The Rock Church\") (1867) in the Gothic Revival and Second Presbyterian Church of St. Louis (1900) in Richardsonian Romanesque.\nBy the 1900 census, St. Louis was the fourth largest city in the country. In 1904, the city hosted a world's fair at Forest Park called the Louisiana Purchase Exposition. Its architectural legacy is somewhat scattered. Among the fair-related cultural institutions in the park are the St. Louis Art Museum designed by Cass Gilbert, part of the remaining lagoon at the foot of Art Hill, and the Flight Cage at the St. Louis Zoo. The Missouri History Museum was built afterward, with the profit from the fair. But 1904 left other assets to the city, like Theodore Link's 1894 St. Louis Union Station, and an improved Forest Park.\nOne US Bank Plaza, the local headquarters for US Bancorp, was constructed in 1976 in the structural expressionist style. Several notable postmodern commercial skyscrapers were built downtown in the 1970s and 1980s, including the former AT&amp;T building at 909 Chestnut Street (1986), and One Metropolitan Square (1989), which is the tallest building in St. Louis.\nDuring the 1990s, St. Louis saw the construction of the largest United States courthouse by area, the Thomas F. Eagleton United States Courthouse (2000). The Eagleton Courthouse is home to the United States District Court for the Eastern District of Missouri and the United States Court of Appeals for the Eighth Circuit. The most recent high-rise buildings in St. Louis include two residential towers: One Hundred in the Central West End neighborhood and One Cardinal Way in the Downtown neighborhood.\nNeighborhoods.\nThe city is divided into 79 officially-recognized neighborhoods.\nClimate.\nThe urban area of St. Louis has a humid subtropical climate (K\u00f6ppen: \"Cfa\"); however, its metropolitan region even to the south may present a hot-summer humid continental climate (\"Dfa\"), which shows the effect of the urban heat island in the city. The city experiences hot, humid summers and chilly to cold winters. It is subject to both cold Arctic air and hot, humid tropical air from the Gulf of Mexico. The average annual temperature recorded at nearby Lambert\u2013St. Louis International Airport, is . temperatures can be seen on an average 3 and 1 days per year, respectively. Precipitation averages , but has ranged from in 1953 to in 2015. The highest recorded temperature in St. Louis was on July 14, 1954, and the lowest was on January 5, 1884.\nSt. Louis experiences thunderstorms 48 days a year on average. Especially in the spring, these storms can often be severe, with high winds, large hail and tornadoes. Lying within the hotbed of Tornado Alley, St. Louis is one of the most frequently tornado-struck metropolitan areas in the U.S. and has an extensive history of damaging tornadoes. Severe flooding, such as the Great Flood of 1993, may occur in spring and summer; the (often rapid) melting of thick snow cover upstream on the Missouri or Mississippi Rivers can contribute to springtime flooding.\nFlora and fauna.\nBefore the founding of the city, the area was mostly prairie and open forest. Native Americans maintained this environment, good for hunting, by burning underbrush. Trees are mainly oak, maple, and hickory, similar to the forests of the nearby Ozarks; common understory trees include eastern redbud, serviceberry, and flowering dogwood. Riparian areas are forested with mainly American sycamore.\nMost of the residential areas of the city are planted with large native shade trees. The largest native forest area is found in Forest Park. In autumn, the changing color of the trees is notable. Most species here are typical of the eastern woodland, although numerous decorative non-native species are found. The most notable invasive species is Japanese honeysuckle, which officials are trying to manage because of its damage to native trees. It is removed from some parks.\nWildlife includes urbanized coyotes, white-tailed deer, eastern gray squirrel, cottontail rabbit, and the nocturnal Virginia opossum. Large bird species are abundant in parks and include Canada goose, mallard duck, and shorebirds, including the great egret and great blue heron. Gulls are common along the Mississippi River; these species follow barge traffic.\nWinter populations of bald eagles are along the Mississippi River around the Chain of Rocks Bridge. The city is on the Mississippi Flyway, used by migrating birds, and has a large variety of small bird species, common to the eastern U.S. The Eurasian tree sparrow, an introduced species, is limited in North America to the counties surrounding St. Louis. The city has special sites for birdwatching of migratory species, including Tower Grove Park.\nCommon frog species include the American toad and species of chorus frogs called spring peepers, which are found in nearly every pond. Some years have outbreaks of cicadas or ladybugs. Mosquitoes, no-see-ums, and houseflies are common insect nuisances, especially in July and August; because of this, windows are almost always fitted with screens. Invasive populations of honeybees have declined in recent years. Numerous native species of pollinator insects have recovered to fill their ecological niche, and armadillos are throughout the St. Louis area.\nDemographics.\n&lt;templatestyles src=\"US Census population/styles.css\"/&gt;\nSt. Louis grew slowly until the American Civil War, when industrialization and immigration sparked a boom. Mid-19th century immigrants included many Irish and Germans; later there were immigrants from southern and eastern Europe. In the early 20th century, African American and white migrants came from the South; the former as part of the Great Migration out of rural areas of the Deep South. Many came from Mississippi and Arkansas. Italians, Serbians, Lebanese, Syrians, and Greeks settled in St. Louis by the late 19th-Century.\nAfter years of immigration, migration, and expansion, the city reached its peak population in 1950. That year, the Census Bureau reported St. Louis's population as 82% White and 17.9% African American. After World War II, St. Louis began losing population to the suburbs, first because of increased demand for new housing, unhappiness with city services, ease of commuting by highways, and later, white flight. St. Louis's population decline has resulted in a significant increase of abandoned residential housing units and vacant lots throughout the city proper; this blight has attracted much wildlife (such as deer and coyotes) to the many abandoned overgrown lots. As of the 2020 Census, St. Louis has lost 64.8% of its population since the 1950 United States census. During this period, the population of Greater St. Louis, which includes more than one county, has grown every year and continues to do so.\nAccording to the 2010 United States census, St. Louis had 319,294 people living in 142,057 households, of which 67,488 households were families. The population density was . About 24% of the population was 19 or younger, 9% were 20 to 24, 31% were 25 to 44, 25% were 45 to 64, and 11% were 65 or older. The median age was about 34 years.\nThe African-American population is concentrated in the north side of the city (the area north of Delmar Boulevard is 94.0% black, compared with 35.0% in the central corridor and 26.0% in the south side of St. Louis). Among the Asian-American population in the city, the largest ethnic group is Vietnamese (0.9%), followed by Chinese (0.6%) and Indians (0.5%). The Vietnamese community has concentrated in the Dutchtown neighborhood of south St. Louis; Chinese are concentrated in the Central West End. People of Mexican descent are the largest Latino group, and make up 2.2% of St. Louis's population. They have the highest concentration in the Dutchtown, Benton Park West (Cherokee Street), and Gravois Park neighborhoods. People of Italian descent are concentrated in The Hill.\nIn 2010, St. Louis's per-capita rates of online charitable donations and volunteerism were among the highest among major U.S. cities.\nAs of 2010[ [update]], 91.05% (270,934) of St. Louis city residents age 5 and older spoke English at home as a primary language, while 2.86% (8,516) spoke Spanish, 0.91% (2,713) Serbo-Croatian, 0.74% (2,200) Vietnamese, 0.50% (1,495) African languages, 0.50% (1,481) Chinese, and French was spoken as a main language by 0.45% (1,341) of the population over the age of five. In total, 8.95% (26,628) of St. Louis's population age 5 and older spoke a mother language other than English.\nBosnian population.\nAbout fifteen families from Bosnia settled in St. Louis between 1960 and 1970. After the Bosnian War started in 1992, more Bosnian refugees began arriving and by 2000, tens of thousands of Bosnian refugees settled in St. Louis with the help of Catholic aid societies. Many of them were professionals and skilled workers who had to take any job opportunity to be able to support their families. Most Bosnian refugees are Muslim, ethnically Bosniaks (87%); they have settled primarily in south St. Louis and South County. Bosnian-Americans are well integrated into the city, developing many businesses and ethnic/cultural organizations.\nAn estimated 70,000 Bosnians live in the metro area, which is tied with Chicago for largest population of Bosnians in the United States and the largest Bosnian population outside their homeland. The highest concentration of Bosnians is in the neighborhood of Bevo Mill and in Affton, Mehlville, and Oakville of south St. Louis County.\nBosnian Muslim Romani people have also settled in St. Louis.\nCrime.\nSince 2014 the city of St. Louis has had, as of April\u00a02017[ [update]], one of the highest murder rates, per capita, in the United States, with 188 homicides in 2015 (59.3 homicides per 100,000) and ranks No. 13 of the most dangerous cities in the world by homicide rate. Detroit, Flint, Memphis, Birmingham, and Baltimore have higher overall violent crime rates than St. Louis, when comparing other crimes such as rape, robbery, and aggravated assault. These crime rates are high relative to other American cities, but St. Louis index crime rates have declined almost every year since the peak in 1993 (16,648), to the 2014 level of 7,931 (which is the sum of violent crimes and property crimes) per 100,000. In 2015, the index crime rate reversed the 2005\u20132014 decline to a level of 8,204. Between 2005 and 2014, violent crime has declined by 20%, although rates of violent crime remains 6 times higher than the United States national average and property crime in the city remains 2 &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 times the national average. St. Louis has a higher homicide rate than the rest of the U.S. for both whites and blacks and a higher proportion committed by males. As of October\u00a02016[ [update]], 7 of the homicide suspects were white, 95 black, 0 Hispanic, 0 Asian and 1 female out of the 102 suspects. In 2016, St. Louis was the most dangerous city in the United States with populations of 100,000 or more, ranking 1st in violent crime and 2nd in property crime. It was also ranked 6th of the most dangerous of all establishments in the United States, and East St. Louis, a suburb of the city itself, was ranked 1st. The St. Louis Police Department at the end of 2016 reported a total of 188 murders for the year, the same number of homicides that had occurred in the city in 2015. According to the STLP At the end of 2017, St. Louis had 205 murders but the city recorded only 159 inside St. Louis city limits. The new Chief of Police, John Hayden said two-thirds (67%) of all the murders and one-half of all the assaults are concentrated in a triangular area in the North part of the city.\nYet another factor when comparing the murder rates of St. Louis and other cities is the manner of drawing municipal boundaries. While many other municipalities have annexed many suburbs, St. Louis has not annexed as much suburban area as most American cities. According to a 2018 estimate, the St. Louis metro area included about 3 million residents and the city included about 300,000 residents. Therefore, the city contains about ten percent of the metro population, a low ratio indicating that the municipal boundaries include only a small part of the metro population.\nEconomy.\nThe gross domestic product of Greater St. Louis was $209.9 billion in 2022, up from $192.9 billion the previous year. Greater St. Louis had a GDP per capita of $68,574 in 2021, up 10% from the previous year. As of November, 2024, the education and health services industries employed the greatest number of people in the region, followed by the trade, transportation, and utilities industries and professional and business services.\nMajor companies and institutions.\nAs of 2024, Greater St. Louis is home to six \"Fortune\" 500 companies: Centene Corporation, Reinsurance Group of America, Emerson Electric, Edward Jones, Graybar Electric, and Ameren. An additional ten other area companies are listed on the \"Fortune\" 1000: Post Holdings, Olin Corporation, Core &amp; Main, Stifel Financial, Peabody Energy, Arch Resources, Energizer Holdings, Caleres, Spire, and Belden.\nOther major corporations headquartered in the region include Anheuser-Busch, Bunge Global, Wells Fargo Advisors, Enterprise Holdings, World Wide Technology, Arco Construction, McCarthy Holdings, Clayco Construction, Apex Oil, Alberici, and Schnuck Market. Notable corporations with operations in St. Louis but headquarters elsewhere include Boeing, Bayer, Mastercard, U.S. Bank, and BMO Bank.\nThe Federal Reserve Bank of St. Louis is one of two federal reserve banks in Missouri.\nSt. Louis is a center of medicine and biotechnology. The Washington University School of Medicine is affiliated with Barnes-Jewish Hospital, the fifth largest hospital in the world. Both institutions operate the Alvin J. Siteman Cancer Center. The School of Medicine also is affiliated with St. Louis Children's Hospital, one of the country's top pediatric hospitals. Both hospitals are owned by BJC HealthCare. The McDonnell Genome Institute at Washington University played a major role in the Human Genome Project. Saint Louis University Medical School is affiliated with SSM Health's Cardinal Glennon Children's Hospital and Saint Louis University Hospital. It also has a cancer center, vaccine research center, geriatric center, and a bioethics institute. Several different organizations operate hospitals in the area, including BJC HealthCare, Mercy, SSM Health Care, and Tenet. Other health care and biotechnology institutions with operations in the region include Pfizer, the Donald Danforth Plant Science Center, Bayer, Sigma-Aldrich, Mallinckrodt, and Multidata Systems International.\nSeveral once-independent pillars of the local economy have been purchased by other corporations. Among them are Anheuser-Busch, purchased by Belgium-based InBev; Missouri Pacific Railroad, merged with the Omaha, Nebraska-based Union Pacific Railroad in 1982; McDonnell Douglas, whose operations are now part of Boeing Defense, Space &amp; Security; Trans World Airlines, which was headquartered in the city for its last decade of existence prior to being acquired by American Airlines; and Ralston Purina, now a wholly owned subsidiary of Nestl\u00e9. The May Department Stores Company was purchased by Federated Department Stores, now Macy's, although it still has its regional headquarters in the area. Most of the assets of Furniture Brands International were sold to Heritage Home Group in 2013, which moved to North Carolina.\nCortex Innovation Community in Midtown is the region's largest innovation hub. Cortex is home to offices of Square, Microsoft, Aon, Boeing, and Centene. Cortex has generated 3,800 tech jobs in 14 years, and once built out, is projected to generate $2 billion in development and create 13,000 jobs for the region. The nonprofit Arch Grants is attracting new startups to the region, while the nonprofit LaunchCode trains future tech workers.\nAccording to the \"St. Louis Business Journal\", the top employers in Greater St. Louis as of March 29, 2023 are:\nAccording to St. Louis's 2022 Annual Comprehensive Financial Report, the top employers in the city only as of 2021 are:\nArts and culture.\nThe same year as the 1904 World's Fair, the Strassberger Music Conservatory Building was constructed at 2300 Grand. Otto Wilhelmi was the architect. In 1911, the conservatory had over 1,100 students. The building is presently in the National Register of Historic Places. A well known graduate was Alfonso D'Artega.\nWith its French past and waves of Catholic immigrants in the 19th and 20th centuries, from Ireland, Germany and Italy, St. Louis is a major center of Roman Catholicism in the United States. St. Louis also boasts the largest Ethical Culture Society in the United States and is one of the most generous cities in the United States, ranking ninth in 2013. Several places of worship in the city are noteworthy, such as the Cathedral Basilica of St. Louis, home of the world's largest mosaic installation. Other churches include the Basilica of St. Louis, King of France, the oldest Roman Catholic cathedral west of the Mississippi River and the oldest church in St. Louis; the St. Louis Abbey, whose distinctive architectural style garnered multiple awards at the time of its completion in 1962; and St. Francis de Sales Oratory, a neo-Gothic church completed in 1908 in South St. Louis and the second largest church in the city.\nThe city is identified with music and the performing arts, especially blues, jazz, and ragtime. The St. Louis Symphony is the second oldest symphony orchestra in the United States. Until 2010, it was also home to KFUO-FM, one of the oldest classical music FM radio stations west of the Mississippi River. Opera Theatre of St. Louis has been called \"one of America's best summer festivals\" by the \"Washington Post\". Former general director Timothy O'Leary was known for drawing the community into discussions of challenging operas. John Adams's \"The Death of Klinghoffer\", which touched off protests and controversy when performed by the Metropolitan Opera in 2014, had no such problems in St. Louis three years before, because the company fostered a citywide discussion, with interfaith dialogues addressing the tough issues of terrorism, religion and the nature of evil that the opera brings up. St. Louis's Jewish Community Relations Council gave O'Leary an award. Under O'Leary, the company\u2014always known for innovative work\u2014gave second chances to other major American operas, such as John Corigliano's \"The Ghosts of Versailles\", presented in 2009 in a smaller-scale version.\nThe Gateway Arch anchors downtown St. Louis and a historic center that includes: the Federal courthouse where the Dred Scott case was first argued, an expanded public library, major churches and businesses, and retail. An increasing downtown residential population has taken to adapted office buildings and other historic structures. In nearby University City is the Delmar Loop, ranked by the American Planning Association as a \"great American street\" for its variety of shops and restaurants, and the Tivoli Theater, all within walking distance.\nUnique city and regional cuisine reflecting various immigrant groups include toasted ravioli, gooey butter cake, provel cheese, the slinger, the Gerber sandwich, and the St. Paul sandwich. Some St. Louis chefs have begun emphasizing use of local produce, meats and fish, and neighborhood farmers' markets have become more popular. Artisan bakeries, salumeria, and chocolatiers also operate in the city.\nSt. Louis-style pizza has thin crust, provel cheese, and is cut in small squares. Frozen-custard purveyor Ted Drewes offers its \"Concrete\": frozen custard blended with any combination of dozens of ingredients into a mixture so thick that a spoon inserted into the custard does not fall if the cup is inverted.\nSports.\nSt. Louis hosts the St. Louis Cardinals of Major League Baseball and the St. Louis Blues of the National Hockey League. In 2019, it became the eighth North American city to have won titles in all four major leagues (MLB, NBA, NFL, and NHL) when the Blues won the Stanley Cup championship. It has collegiate-level soccer teams and is one of three American cities to have hosted the Summer Olympic Games. A third major team, the St. Louis City SC of Major League Soccer, began play in 2023.\nProfessional sports.\nPro teams in the St. Louis area include:\nThe St. Louis Cardinals are one of the most successful franchises in Major League Baseball. The Cardinals have won 19 National League (NL) titles (the most pennants for the league franchise in one city) and 11 World Series titles (second to the New York Yankees and the most by any NL franchise), recently in 2011. They play at Busch Stadium. Previously, the St. Louis Browns played in the American League (AL) from 1902 to 1953, before moving to Baltimore, Maryland to become the current incarnation of the Orioles. The 1944 World Series was an all-St. Louis World Series, matching up the St. Louis Cardinals and St. Louis Browns at Sportsman's Park, won by the Cardinals in six games. It was the third and final time that the teams shared a home field. St. Louis also was home to the St. Louis Stars, also known as the St. Louis Giants from 1906 to 1921, who played in Negro league baseball from 1920 to 1931 and won championships in 1928, 1930, and 1931, and the St. Louis Maroons who played in the Union Association in 1884 and in the National League from 1885 to 1889. In 1884, The St. Louis Maroons won the Union Association pennant and started the season with 20 straight wins, a feat that was not surpassed by any major professional sports team in the United States until the 2015-16 Golden State Warriors season when they started their NBA season with 24 straight wins.\nThe St. Louis Blues of the National Hockey League (NHL) play at the Enterprise Center. They were one of the six teams added to the NHL in the 1967 expansion. The Blues went to the Stanley Cup finals in their first three years, but got swept every time. Although they were the first 1967 expansion team to make the Stanley Cup Finals, they were also the last of the 1967 expansion teams to win the Stanley Cup. They finally won their first Stanley Cup in 2019 after beating the Boston Bruins in the final. This championship made St. Louis the eighth city to win a championship in each of the four major U.S. sports. Prior to the Blues, the city was home to the St. Louis Eagles. The team played in the 1934\u201335 season.\nSt. Louis has been home to four National Football League (NFL) teams. The St. Louis All-Stars played in the city in 1923, the St. Louis Gunners in 1934, the St. Louis Cardinals from 1960 to 1987, and the St. Louis Rams from 1995 to 2015. The football Cardinals advanced to the NFL playoffs four times (1964, 1974, 1975 and 1982), never hosting in any appearance. They did, however, win the 1964 Playoff Bowl for third place against the Green Bay Packers by a score of 24\u201317. The Cardinals moved to Phoenix, Arizona, in 1988. The Rams played at the Edward Jones Dome from 1995 to 2015 and won Super Bowl XXXIV in 2000. They also went to Super Bowl XXXVI but lost to the New England Patriots. The Rams then returned to Los Angeles in 2016.\nThe St. Louis Hawks of the National Basketball Association (NBA) played at Kiel Auditorium from 1955 to 1968. They won the NBA championship in 1958 and played in three other NBA Finals: 1957, 1960, and 1961. In 1968 the Hawks moved to Atlanta. St. Louis was also the home to the St. Louis Bombers of the Basketball Association of America from 1946 to 1949 and the National Basketball Association from 1949 to 1950 and the Spirits of St. Louis of the American Basketball Association from 1974 to 1976 when the ABA and NBA merged.\nMajor League Soccer's St. Louis City SC began play in 2023 at Energizer Park. Their MLS Next Pro affiliate is St. Louis City 2, which began play in 2022 and also plays at Energizer Park. Formerly, USL Championship's Saint Louis FC played in the area from 2015 to 2020 at World Wide Technology Soccer Park.\nThe St. Louis BattleHawks of the XFL began play in 2020, using The Dome at America's Center as their home field. After a two-year hiatus of the league, the Battlehawks returned in 2023, when the XFL resumed play.\nSt. Louis hosts several minor league sports teams. The Gateway Grizzlies of the independent Frontier League play in the area in Sauget, IL. The St. Louis Trotters of the Independent Basketball Association play at Matthews-Dickey Boys and Girls Club. The St. Louis Ambush indoor soccer team plays in nearby St. Charles at the Family Arena as a part of the Major Arena Soccer League. The St. Louis Slam play in the Women's Football Alliance at Harlen C. Hunter Stadium.\nThe region hosts INDYCAR, NHRA drag racing, and NASCAR events at World Wide Technology Raceway at Gateway in Madison, Illinois. Thoroughbred flat racing events are hosted at Fairmount Park Racetrack near Collinsville, Illinois.\nChess.\nSt. Louis is home to the Saint Louis Chess Club where the U.S. Chess Championship is held. St. Louisan Rex Sinquefield founded the Chess Club and Scholastic Center of St. Louis (which was renamed as St. Louis Chess Club later) and moved the World Chess Hall of Fame to St. Louis in 2011. The Sinquefield Cup Tournament started at St. Louis in 2013. In 2014 the Sinquefield Cup was the highest-rated chess tournament of all time. Former U.S. Chess Champions Fabiano Caruana and Hikaru Nakamura have lived in St. Louis. Former women's chess champion Susan Polgar also resides in St. Louis.\nCollege and amateur sports.\nSt. Louis has hosted the Final Four of both the women's and men's college basketball NCAA Division I championship tournaments, and the men's Frozen Four collegiate ice hockey tournament. Saint Louis University has won 10 NCAA men's soccer championships, and the city has hosted the College Cup several times. In addition to collegiate soccer, many St. Louisans have played for the United States men's national soccer team, and 20 St. Louisans have been elected into the National Soccer Hall of Fame. St. Louis also is the origin of the sport of corkball, a type of baseball in which there is no base running.\nAlthough the area does not have a National Basketball Association team, it hosts the St. Louis Phoenix, an American Basketball Association team.\nClub Atletico Saint Louis, a semi-professional soccer team, competes within the National Premier Soccer League and plays out of St. Louis University High School Soccer Stadium.\nParks and recreation.\nThe city operates more than 100 parks, with amenities that include sports facilities, playgrounds, concert areas, picnic areas, and lakes. Forest Park, located on the western edge of city, is the largest, occupying 1,400 acres of land, making it almost twice as large as Central Park in New York City. The park is home to five major institutions, including the St. Louis Art Museum, the St. Louis Zoo, the St. Louis Science Center, the Missouri History Museum, and the Muny amphitheatre. Another significant park in the city is Gateway Arch National Park, which was known as Jefferson National Expansion Memorial until 2018 and is located on the riverfront in downtown St. Louis. The centerpiece of the park is the tall Gateway Arch, a National Memorial designed by noted architect Eero Saarinen and completed on October 28, 1965. Also part of the historic park is the Old Courthouse, where the first two trials of \"Dred Scott v. Sandford\" were held in 1847 and 1850.\nOther parks include the Missouri Botanical Garden, Tower Grove Park, Carondelet Park, and Citygarden. The Missouri Botanical Garden, a private garden and botanical research facility, is a National Historic Landmark and one of the oldest botanical gardens in the United States. The Garden features 79 acres of horticultural displays from around the world. This includes a Japanese strolling garden, Henry Shaw's original 1850 estate home and a geodesic dome called the Climatron. Immediately south of the Missouri Botanical Garden is Tower Grove Park, a gift to the city by Henry Shaw. Citygarden is an urban sculpture park located in downtown St. Louis, with art from Fernand L\u00e9ger, Aristide Maillol, Julian Opie, Tom Otterness, Niki de Saint Phalle, and Mark di Suvero. The park is divided into three sections, each of which represent a different theme: river bluffs; flood plains; and urban gardens. Another downtown sculpture park is the Serra Sculpture Park, with the 1982 Richard Serra sculpture \"Twain\".\nGovernment.\nSt. Louis is one of the 41 independent cities in the U.S. that does not legally belong to any county. St. Louis has a strong mayor\u2013council government with legislative authority and oversight vested in the Board of Aldermen and with executive authority in the mayor and six other elected officials. The mayor is the chief executive officer of the city and is responsible for appointing city department heads. The Board of Aldermen is made up of 14 members (one elected from each of the city's wards) plus a board president who is elected citywide. The 2014 fiscal year budget topped $1 billion for the first time, a 1.9% increase over the $985.2 million budget in 2013. 238,253 registered voters lived in the city in 2012, down from 239,247 in 2010, and 257,442 in 2008.\nMunicipal elections in St. Louis are held in odd-numbered years, with the primary elections in March and the general election in April. The mayor is elected in odd-numbered years following the United States presidential election using a top-two approval voting primary. The aldermen representing odd-numbered wards are up for election at the same time as the mayor. The president of the board of aldermen and the aldermen from even-numbered wards are elected in the off-years. The Democratic Party has dominated St. Louis city politics for decades. The city has not had a Republican mayor since 1949, and the last time a Republican was elected to another citywide office was in the 1970s. As of 2015[ [update]], all 28 of the city's aldermen are Democrats.\nForty-eight individuals have held the office of mayor of St. Louis, four of whom\u2014William Carr Lane, John Fletcher Darby, John Wimer, and John How\u2014served non-consecutive terms. The most terms served by a mayor was by Lane, who served 8 full terms plus the unexpired term of Darby. The current mayor is Cara Spencer, who took office on April 15, 2025. She is the city's third consecutive female mayor. Her predecessor Tishaura Jones who took office April 20, 2021, was the first African-American woman to hold the post. Jones succeeded Lyda Krewson, the first female mayor of the city, who retired in 2021 after serving for four years. The longest-serving mayor was Francis Slay, who took office April 17, 2001, and left office April 18, 2017, a total of 16 years and six days over four terms in office. The shortest-serving mayor was Arthur Barret, who died 11 days after taking office.\nAlthough St. Louis separated from St. Louis County in 1876, some mechanisms have been put in place for joint funding management and funding of regional assets. The St. Louis Zoo-Museum district collects property taxes from residents of both St. Louis City and County, and the funds are used to support cultural institutions including the St. Louis Zoo, St. Louis Art Museum and the Missouri Botanical Gardens. Similarly, the Metropolitan Sewer District provides sanitary and storm sewer service to the city and much of St. Louis County. The Bi-State Development Agency (now known as Metro) runs the region's MetroLink light rail system and bus system.\nThe City of St. Louis Sheriff's Office (STLSO or STLCSO) primarily provides security services for the courtrooms, and serves court documents and issues gun carry permits. In 2022, it gained the ability to make arrests and traffic stops. Formed in 1876, the Sheriff's Office has over 200 employees, of which 165 are deputies. Its budget was $11.97 million in 2025.\nState and federal government.\nSt. Louis is split between 8 districts in the Missouri House of Representatives: the 76th, 77th, 78th, 79th, 80th, 81st, 82nd, and 84th districts. The 5th Missouri Senate district is entirely within the city, while the 4th is shared with St. Louis County.\nAt the federal level, St. Louis is the heart of Missouri congressional district, which also includes part of northern St. Louis County. A Republican has not represented a significant portion of St. Louis in the U.S. House since 1953. The city shifted from Republican voting to a Democratic stronghold at the presidential level since 1928. George H. W. Bush in 1988 was the most recent Republican to win even a quarter of the city's votes in a presidential election.\nThe United States Court of Appeals for the Eighth Circuit and the United States District Court for the Eastern District of Missouri are based in the Thomas F. Eagleton United States Courthouse in downtown St. Louis. St. Louis is also home to a Federal Reserve System branch, the Federal Reserve Bank of St. Louis. The National Geospatial-Intelligence Agency (NGA) also maintains major facilities in the St. Louis area.\nEducation.\nColleges and universities.\nThe city is home to three national research universities, Washington University in St. Louis, Saint Louis University, and the University of Missouri-St. Louis, as classified under the Carnegie Classification of Institutions of Higher Education. Washington University School of Medicine in St. Louis has been ranked among the top 10 medical schools in the country by \"U.S. News &amp; World Report\" for as long as the list has been published, and as high as second, in 2003 and 2004. \"U.S. News &amp; World Report\" also ranks the undergraduate school and other graduate schools, such as the Washington University School of Law, in the top 20 in the nation.\nSt. Louis Metropolitan Region is home to St. Louis Community College. It is also home to several other four-year colleges &amp; universities, including Harris\u2013Stowe State University, a historically black public university, Webster University, Missouri Baptist University, University of Health Sciences &amp; Pharmacy (the former Saint Louis College of Pharmacy), Southern Illinois University-Edwardsville (SIUE), and Lindenwood University.\nIn addition to Catholic theological institutions such as Kenrick-Glennon Seminary and Aquinas Institute of Theology sponsored by the Order of Preachers, St. Louis is home to three Protestant seminaries: Eden Theological Seminary of the United Church of Christ, Covenant Theological Seminary of the Presbyterian Church in America, and Concordia Seminary of the St. Louis-based Lutheran Church\u2013Missouri Synod.\nPrimary and secondary schools.\nThe St. Louis Public Schools (SLPS), which covers the entire city, operates more than 75 schools, attended by more than 25,000 students, including several magnet schools. Since 2017, SLPS operates under full accreditation from the state of Missouri, having previously lost its accreditation in 2007. The Board of Education of the city of St. Louis oversees the district. Since 2000, charter schools have operated in the city of St. Louis using authorization from Missouri state law. These schools are sponsored by local institutions or corporations and take in students from kindergarten through high school. In addition, several private schools exist in the city, and the Archdiocese of St. Louis operates dozens of parochial schools in the city, including parochial high schools. The city also has several private high schools, including secular, Montessori, Catholic and Lutheran schools. St. Louis University High School \u2013 a Jesuit preparatory high school founded in 1818 \u2013 is the oldest secondary educational institution in the U.S. west of the Mississippi River. The state-operated K-12 boarding school Missouri School for the Blind is in St. Louis.\nMedia.\nGreater St. Louis commands the 24th-largest media market in the United States. All of the major U.S. television networks have affiliates in St. Louis, including KTVI 2 (Fox), KMOV 4 (CBS, with MyNetworkTV on channel 32.1), KSDK 5 (NBC), KETC 9 (PBS), KPLR-TV 11 (The CW), KNLC 24 (MeTV), KDNL 30 (ABC), WRBU 46 (Ion), and WPXS 51 Daystar Television Network. Among the area's most popular radio stations are KMOX (AM sports and talk, notable as the longtime flagship station for St. Louis Cardinals broadcasts), KLOU (FM oldies), WIL-FM (FM country), WARH (FM adult hits), and KSLZ (FM Top 40 mainstream). St. Louis also supports public radio's KWMU, an NPR affiliate, and community radio's KDHX. All-sports stations, such as KFNS 590 AM \"The Fan\" and WXOS \"101.1 ESPN\" are also popular. KSHE 95 FM \"Real Rock Radio\" has broadcast rock music since November 1967 - longer than any other radio station in the United States.\nThe \"St. Louis Post-Dispatch\" is the region's major newspaper. Others in the region include \"Ladue News\", \"West Newsmagazine\", the \"Webster-Kirkwood Times,\" and the \"Call Newspapers\" which all serve parts of St. Louis County. Three weeklies serve the African-American community: the \"St. Louis Argus\", the \"St. Louis American\", and the \"St. Louis Sentinel\". \"St. Louis Magazine\", a monthly magazine, covers topics such as local history, cuisine, and lifestyles, while the weekly \"St. Louis Business Journal\" provides coverage of regional business news. St. Louis was served by an online newspaper, the \"St. Louis Beacon\", but that publication merged with KWMU in 2013. The primary alternative newspaper was the \"Riverfront Times\" before it was closed in 2024.\nMany books and movies have been written about St. Louis. A few of the most influential and prominent films are \"Meet Me in St. Louis\" and \"American Flyers\", and novels include \"The Killing Dance\", \"Meet Me in St. Louis\", \"The Runaway Soul\", \"The Rose of Old St. Louis\", and \"Circus of the Damned\".\nBecause St. Louis was a prime location for immigrants to move to, much of the early social work depicting immigrant life was based on St. Louis, such as in the book \"The Immigrant in St. Louis\".\nTransportation.\nRoad, rail, ship, and air transportation modes connect the city with surrounding communities in Greater St. Louis, national transportation networks, and international locations. St. Louis also supports a public transportation network that includes bus and light rail service.\nRoads and highways.\nFour interstate highways connect the city to a larger regional highway system. Interstate 70, an east\u2013west highway, runs from the northwest corner of the city to downtown St. Louis. The north\u2013south Interstate 55 enters the city at the south near the Carondelet neighborhood and runs toward the center of the city, and both Interstate 64 and Interstate 44 enter the city on the west, running parallel to the east. Two of the four interstates (Interstates 55 and 64) merge south of Gateway Arch National Park and leave the city on the Poplar Street Bridge into Illinois, while Interstate 44 terminates at Interstate 70 at its new interchange near N Broadway and Cass Ave. A small portion of the Interstate 270 outer belt freeway runs through the northern end of the city.\nThe 563-mile Avenue of the Saints links St. Louis with St. Paul, Minnesota.\nMajor roadways include the north\u2013south Memorial Drive, located on the western edge of Gateway Arch National Park and parallel to Interstate 70, the north\u2013south streets of Grand Boulevard and Jefferson Avenue, both of which run the length of the city, and Gravois Road, which runs from the southeastern portion of the city to downtown and used to be signed as U.S. Route 66. An east-west roadway that connects the city with surrounding communities is Martin Luther King, Jr. Drive, which carries traffic from the western edge of the city to downtown.\nBuses and taxis.\nLocal bus service in the city of St. Louis is provided by the Bi-State Development Agency via MetroBus, with more than 75 routes connecting to MetroLink light rail transit and stops in the city and region. The city is also served by Madison County Transit, which connects downtown St. Louis to Madison County, Illinois. National bus service in the city is offered by Greyhound Lines, Burlington Trailways and Amtrak Thruway, with a station at the Gateway Transportation Center, and Megabus, with a stop at St. Louis Union Station.\nTaxicab service in the city is provided by private companies regulated by the Metropolitan Taxicab Commission. Rates vary by vehicle type, size, passengers and distance, and by regulation all taxicab fares must be calculated using a taximeter and be payable in cash or credit card. Solicitation by a driver is prohibited, although a taxicab may be hailed on the street or at a stand.\nLight rail and subways.\nThe St. Louis metropolitan area is served by MetroLink (known as Metro) and is the 11th-largest light rail system in the country with of double track light rail. The Red Line and The Blue Line both serve all the stations in the inner city, and branch to different destinations beyond in the suburban areas. Both lines enter the city north of Forest Park on the western edge of the city or on the Eads Bridge in downtown St. Louis to Illinois. All of the system track is in independent right of way, with both surface level and underground subway track in the city. All stations are independent entry, and all platforms are flush-level with trains. Rail service is provided by the Bi-State Development Agency (also known as Metro), which is funded by a sales taxes levied in the city and other counties in the region. The Gateway Multimodal Transportation Center acts as the hub station in the city of St. Louis, linking the city's light rail system, local bus system, passenger rail service, and national bus service. It is located just east of the historic grand St. Louis Union Station.\nHeavy rail.\nInter-city rail passenger train service in the city is provided by Amtrak at the Gateway Multimodal Transportation Center downtown. Amtrak trains terminating in the city include the \"Lincoln Service\" to Chicago and the \"Missouri River Runner\" to Kansas City, Missouri. St. Louis is an intermediate stop on the \"Texas Eagle\" route which provides long-distance passenger service between Chicago, San Antonio, and three days a week, to Los Angeles.\nSt. Louis is the nation's third-largest freight rail hub, moving Missouri exports such as fertilizer, gravel, crushed stone, prepared foodstuffs, fats, oils, nonmetallic mineral products, grain, alcohol, tobacco products, automobiles, and automobile parts. Freight rail service in St. Louis is provided on tracks owned by Union Pacific Railroad, Norfolk Southern Railway, Foster Townsend Rail Logistics \u2013 formerly Manufacturers Railway (St. Louis), Terminal Railroad Association of St. Louis, Affton Trucking, and the BNSF Railway.\nThe Terminal Railroad Association of St. Louis (reporting mark: TRRA) is a switching and terminal railroad jointly owned by all the major rail carriers in St. Louis. The company operates 30 diesel-electric locomotives to move railcars around the classification yards, deliver railcars to local industries, and ready trains for departure. The TRRA processes and dispatches a significant portion of railroad traffic moving through the city and owns and operates a network of rail bridges and tunnels including the MacArthur Bridge (St. Louis) and the Merchants Bridge. This infrastructure is also used by inter-city rail and long-distance passenger trains serving St. Louis.\nWater.\nRiver transportation is available through the Port of St. Louis, which is 19.3 miles of riverbank on the Mississippi River that handles more than 32 million tons of freight annually. The Port is the second largest inland port by trip-ton miles, and the third largest by tonnage in the United States, with more than 100 docks for barges and 16 public terminals on the river. The Port Authority added two new small fire and rescue craft in 2012 and 2013.\nAir.\nSt. Louis is served by two passenger airports. St. Louis Lambert International Airport, owned and operated by the City of St. Louis, is 11 miles northwest of downtown along highway I-70 between I-170 and I-270 in St. Louis County. It is the largest and busiest airport in the state. In 2016, when the airport had more than 255 daily departures to about 90 domestic and international locations, it served more than 15 million passengers. The airport serves as a focus hub city for Southwest Airlines; it was once a hub for Trans World Airlines and a focus-city for American Airlines and AmericanConnection. The airport has two terminals with a total of five concourses. International flights and passengers use Terminal 2, whose lower level holds the Immigration and Customs gates. Passengers can move between the terminals on complimentary buses that run continuously, or via MetroLink for a fee. It was possible to walk between the terminals until Concourse D was closed in 2008.\nMidAmerica St. Louis Airport is the secondary passenger airport serving the metropolitan area. Located 17 miles east of the city downtown core, the airport serves domestic passengers. Air cargo transportation is available at Lambert International and at other nearby regional airports, including MidAmerica St. Louis Airport, Spirit of St. Louis Airport, and St. Louis Downtown Airport.\nSister cities.\nSt. Louis has 16 sister cities:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27688", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=27688", "title": "Saint Louis Missouri", "text": ""}
{"id": "27689", "revid": "1749459", "url": "https://en.wikipedia.org/wiki?curid=27689", "title": "Saxon people", "text": ""}
{"id": "27691", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=27691", "title": "Social security", "text": ""}
{"id": "27692", "revid": "2613762", "url": "https://en.wikipedia.org/wiki?curid=27692", "title": "Steam engine", "text": "Engine that uses steam to perform mechanical work\nA steam engine is a heat engine that performs mechanical work using steam as its working fluid. The steam engine uses the force produced by steam pressure to push a piston back and forth inside a cylinder. This pushing force can be transformed by a connecting rod and crank into rotational force for work. The term \"steam engine\" is most commonly applied to reciprocating engines as just described, although some authorities have also referred to the steam turbine and devices such as Hero's aeolipile as \"steam engines\". The essential feature of steam engines is that they are external combustion engines, where the working fluid is separated from the combustion products. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle. In general usage, the term \"steam engine\" can refer to either complete steam plants (including boilers etc.), such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine.\nSteam-driven devices such as the aeolipile were known in the first century AD, and there were a few other uses recorded in the 16th century. In 1606 Jer\u00f3nimo de Ayanz y Beaumont patented his invention of the first steam-powered water pump for draining mines. Thomas Savery is considered the inventor of the first commercially used steam powered device, a steam pump that used steam pressure operating directly on the water. The first commercially successful engine that could transmit continuous power to a machine was developed in 1712 by Thomas Newcomen. In 1764, James Watt made a critical improvement by removing spent steam to a separate vessel for condensation, greatly improving the amount of work obtained per unit of fuel consumed. By the 19th century, stationary steam engines powered the factories of the Industrial Revolution. Steam engines replaced sails for ships on paddle steamers, and steam locomotives operated on the railways.\nReciprocating piston type steam engines were the dominant source of power until the early 20th century. The efficiency of stationary steam engines increased dramatically until about 1922. The highest Rankine Cycle Efficiency of 91% and combined thermal efficiency of 31% was demonstrated and published in 1921 and 1928. Advances in the design of electric motors and internal combustion engines resulted in the gradual replacement of steam engines in commercial usage. Steam turbines replaced reciprocating engines in power generation, due to lower cost, higher operating speed, and higher efficiency. Note that small scale steam turbines are much less efficient than large ones.\nAs of 2023[ [update]], large reciprocating piston steam engines are still being manufactured in Germany.\nHistory.\nEarly experiments.\nOne recorded rudimentary steam-powered engine was the aeolipile described by Hero of Alexandria, a Hellenistic mathematician and engineer in Roman Egypt during the first century AD. In the following centuries, the few steam-powered engines known were, like the aeolipile, essentially experimental devices used by inventors to demonstrate the properties of steam.\nA rudimentary steam turbine device was described by Taqi al-Din in Ottoman Egypt in 1551 and by Giovanni Branca in Italy in 1629. The Spanish inventor Jer\u00f3nimo de Ayanz y Beaumont received patents in 1606 for 50 steam-powered inventions, including a water pump for draining inundated mines. By 1615, Salomon de Caus developed a solar-powered atmospheric engine. Frenchman Denis Papin did some useful work on the steam digester in 1679, and first used a piston to raise weights in 1690.\nPumping engines.\nThe first commercial steam-powered device was a water pump, developed in 1698 by Thomas Savery. It used condensing steam to create a vacuum which raised water from below and then used steam pressure to raise it higher. Small engines were effective though larger models were problematic. They had a very limited lift height and were prone to boiler explosions. Savery's engine was used in mines, pumping stations and supplying water to water wheels powering textile machinery. One advantage of Savery's engine was its low cost. Bento de Moura Portugal introduced an improvement of Savery's construction \"to render it capable of working itself\", as described by John Smeaton in the Philosophical Transactions published in 1751. It continued to be manufactured until the late 18th century. At least one engine was still known to be operating in 1820.\nPiston steam engines.\nThe first commercially successful engine that could transmit continuous power to a machine was the atmospheric engine, invented by Thomas Newcomen around 1712. It improved on Savery's steam pump, using a piston as proposed by Papin. Newcomen's engine was relatively inefficient, and mostly used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths originally impractical using traditional means, and for providing reusable water for driving waterwheels at factories sited away from a suitable \"head\". Water that passed over the wheel was pumped up into a storage reservoir above the wheel.\nIn 1780 James Pickard patented the use of a flywheel and crankshaft to provide rotative motion from an improved Newcomen engine.\nIn 1720, Jacob Leupold described a two-cylinder high-pressure steam engine. The invention was published in his major work \"Theatri Machinarum Hydraulicarum\". The engine used two heavy pistons to provide motion to a water pump. Each piston was raised by the steam pressure and returned to its original position by gravity. The two pistons shared a common four-way rotary valve connected directly to a steam boiler.\nThe next major step occurred when James Watt developed (1763\u20131775) an improved version of Newcomen's engine, with a separate condenser. Boulton and Watt's early engines used half as much coal as John Smeaton's improved version of Newcomen's. Newcomen's and Watt's early engines were \"atmospheric\". They were powered by air pressure pushing a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam. The engine cylinders had to be large because the only usable force acting on them was atmospheric pressure.\nWatt developed his engine further, modifying it to provide a rotary motion suitable for driving machinery. This enabled factories to be sited away from rivers, and accelerated the pace of the Industrial Revolution.\nHigh-pressure engines.\nThe meaning of high pressure, together with an actual value above ambient, depends on the era in which the term was used. For early use of the term Van Reimsdijk refers to steam being at a sufficiently high pressure that it could be exhausted to atmosphere without reliance on a vacuum to enable it to perform useful work. states that Watt's condensing engines were known, at the time, as low pressure compared to high pressure, non-condensing engines of the same period.\nWatt's patent prevented others from making high pressure and compound engines. Shortly after Watt's patent expired in 1800, Richard Trevithick and, separately, Oliver Evans in 1801 introduced engines using high-pressure steam; Trevithick obtained his high-pressure engine patent in 1802, and Evans had made several working models before then. These were much more powerful for a given cylinder size than previous engines and could be made small enough for transport applications. Thereafter, technological developments and improvements in manufacturing techniques (partly brought about by the adoption of the steam engine as a power source) resulted in the design of more efficient engines that could be smaller, faster, or more powerful, depending on the intended application.\nThe Cornish engine was developed by Trevithick and others in the 1810s. It was a compound cycle engine that used high-pressure steam expansively, then condensed the low-pressure steam, making it relatively efficient. The Cornish engine had irregular motion and torque through the cycle, limiting it mainly to pumping. Cornish engines were used in mines and for water supply until the late 19th century.\nHorizontal stationary engine.\nEarly builders of stationary steam engines considered that horizontal cylinders would be subject to excessive wear. Their engines were therefore arranged with the piston axis in vertical position. In time the horizontal arrangement became more popular, allowing compact, but powerful engines to be fitted in smaller spaces.\nThe acme of the horizontal engine was the Corliss steam engine, patented in 1849, which was a four-valve counter flow engine with separate steam admission and exhaust valves and automatic variable steam cutoff. When Corliss was given the Rumford Medal, the committee said that \"no one invention since Watt's time has so enhanced the efficiency of the steam engine\". In addition to using 30% less steam, it provided more uniform speed due to variable steam cut off, making it well suited to manufacturing, especially cotton spinning.\nRoad vehicles.\nThe first experimental road-going steam-powered vehicles were built in the late 18th century, but it was not until after Richard Trevithick had developed the use of high-pressure steam, around 1800, that mobile steam engines became a practical proposition. The first half of the 19th century saw great progress in steam vehicle design, and by the 1850s it was becoming viable to produce them on a commercial basis. This progress was dampened by legislation which limited or prohibited the use of steam-powered vehicles on roads. Improvements in vehicle technology continued from the 1860s to the 1920s. Steam road vehicles were used for many applications. \nIn the 20th century, the rapid development of internal combustion engine technology led to the demise of the steam engine as a source of propulsion of vehicles on a commercial basis, with relatively few remaining in use beyond the Second World War. Many of these vehicles were acquired by enthusiasts for preservation, and numerous examples are still in existence. In the 1960s, the air pollution problems in California gave rise to a brief period of interest in developing and studying steam-powered vehicles as a possible means of reducing the pollution. Apart from interest by steam enthusiasts, the occasional replica vehicle, and experimental technology, no steam vehicles are in production at present.\nMarine engines.\nNear the end of the 19th century, compound engines came into widespread use. Compound engines exhausted steam into successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved efficiency. These stages were called expansions, with double- and triple-expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the dominant source of power until the early 20th century, when advances in the design of the steam turbine, electric motors, and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines, with merchant shipping relying increasingly upon diesel engines, and warships on the steam turbine.\nSteam locomotives.\nAs the development of steam engines progressed through the 18th century, various attempts were made to apply them to road and railway use. In 1784, William Murdoch, a Scottish inventor, built a model steam road locomotive. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the United States probably during the 1780s or 1790s.\nHis steam locomotive used interior bladed wheels guided by rails or tracks.\nThe first full-scale working railway steam locomotive was built by Richard Trevithick in the United Kingdom and, on 21 February 1804, the world's first railway journey took place as Trevithick's steam locomotive hauled 10 tonnes of iron, 70 passengers and five wagons along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil to Abercynon in south Wales. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency. Trevithick visited the Newcastle area later in 1804 and the colliery railways in north-east England became the leading centre for experimentation and development of steam locomotives.\nTrevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can in 1808. Only four years later, the successful twin-cylinder locomotive \"Salamanca\" by Matthew Murray was used by the edge railed rack and pinion Middleton Railway. In 1825 George Stephenson built the \"Locomotion\" for the Stockton and Darlington Railway. This was the first public steam railway in the world and then in 1829, he built \"The Rocket\" which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in 1830 making exclusive use of steam power for both passenger and freight trains.\nSteam locomotives continued to be manufactured until the late twentieth century in places such as China and the former East Germany (where the DR Class 52.80 was produced).\nSteam turbines.\nThe final major evolution of the steam engine design was the use of steam turbines starting in the late part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines (for outputs above several hundred horsepower), have fewer moving parts, and provide rotary power directly instead of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In the United States, 90% of the electric power is produced in this way using a variety of heat sources. Steam turbines were extensively applied for propulsion of large ships throughout most of the 20th century.\nPresent development.\nAlthough the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines.\nThe world's smallest working \"steam engine\" was revealed in 2011. The micro-scale engine was developed by two German scientists at the University of Stuttgart. It operates on the principle of a Stirling engine.\nComponents and accessories of steam engines.\nThere are two fundamental components of a steam plant: the boiler or steam generator, and the \"motor unit\", referred to itself as a \"steam engine\". Stationary steam engines in fixed buildings may have the boiler and engine in separate buildings some distance apart. For portable or mobile use, such as steam locomotives, the two are mounted together.\nThe widely used reciprocating engine typically consisted of a cast-iron cylinder, piston, connecting rod and beam or a crank and flywheel, and miscellaneous linkages. Steam was alternately supplied and exhausted by one or more valves. Speed control was either automatic, using a governor, or by a manual valve. The cylinder casting contained steam supply and exhaust ports.\nEngines equipped with a condenser are a separate type than those that exhaust to the atmosphere.\nOther components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox.\nHeat source.\nThe heat required for boiling the water and raising the temperature of the steam can be derived from various sources, most commonly from burning combustible materials with an appropriate supply of air in a closed space (e.g., combustion chamber, firebox, furnace). In the case of model or toy steam engines and a few full scale cases, the heat source can be an electric heating element.\nBoilers.\nBoilers are pressure vessels that contain water to be boiled, and features that transfer the heat to the water as effectively as possible.\nThe two most common types are:\nFire-tube boilers were the main type used for early high-pressure steam (typical steam locomotive practice), but they were to a large extent displaced by more economical water tube boilers in the late 19th century for marine propulsion and large stationary applications.\nMany boilers raise the temperature of the steam after it has left that part of the boiler where it is in contact with the water. Known as superheating it turns 'wet steam' into 'superheated steam'. It avoids the steam condensing in the engine cylinders, and gives a significantly higher efficiency.\nMotor units.\nIn a steam engine, a piston or steam turbine or any other similar device for doing mechanical work takes a supply of steam at high pressure and temperature and gives out a supply of steam at lower pressure and temperature, using as much of the difference in steam energy as possible to do mechanical work.\nThese \"motor units\" are often called 'steam engines' in their own right. Engines using compressed air or other gases differ from steam engines only in details that depend on the nature of the gas although compressed air has been used in steam engines without change.\nCold sink.\nAs with all heat engines, the majority of primary energy must be emitted as waste heat at relatively low temperature.\nThe simplest cold sink is to vent the steam to the environment. This is often used on steam locomotives to avoid the weight and bulk of condensers. Some of the released steam is vented up the chimney so as to increase the draw on the fire, which greatly increases engine power, but reduces efficiency.\nSometimes the waste heat from the engine is useful itself, and in those cases, very high overall efficiency can be obtained.\nSteam engines in stationary power plants use surface condensers as a cold sink. The condensers are cooled by water flow from oceans, rivers, lakes, and often by cooling towers which evaporate water to provide cooling energy removal. The resulting condensed hot water (\"condensate\"), is then pumped back up to pressure and sent back to the boiler. A dry-type cooling tower is similar to an automobile radiator and is used in locations where water is costly. Waste heat can also be ejected by evaporative (wet) cooling towers, which use a secondary external water circuit that evaporates some of flow to the air.\nRiver boats initially used a jet condenser in which cold water from the river is injected into the exhaust steam from the engine. Cooling water and condensate mix. While this was also applied for sea-going vessels, generally after only a few days of operation the boiler would become coated with deposited salt, reducing performance and increasing the risk of a boiler explosion. Starting about 1834, the use of surface condensers on ships eliminated fouling of the boilers, and improved engine efficiency.\nEvaporated water cannot be used for subsequent purposes (other than rain somewhere), whereas river water can be re-used. In all cases, the steam plant boiler feed water, which must be kept pure, is kept separate from the cooling water or air.\nWater pump.\nMost steam boilers have a means to supply water whilst at pressure, so that they may be run continuously. Utility and industrial boilers commonly use multi-stage centrifugal pumps; however, other types are used. Another means of supplying lower-pressure (typically about ) boiler feed water is an injector, which uses a steam jet usually supplied from the boiler. Injectors became popular in the 1850s but are no longer widely used, except in applications such as steam locomotives. It is the pressurization of the water that circulates through the steam boiler that allows the water to be raised to temperatures well above boiling point of water at one atmospheric pressure, and by that means to increase the efficiency of the steam cycle.\nMonitoring and control.\nFor safety reasons, nearly all steam engines are equipped with mechanisms to monitor the boiler, such as a pressure gauge and a sight glass to monitor the water level.\nMany engines, stationary and mobile, are also fitted with a governor to regulate the speed of the engine without the need for human interference.\nThe most useful instrument for analyzing the performance of steam engines is the steam engine indicator. Early versions were in use by 1851, but the most successful indicator was developed for the high speed engine inventor and manufacturer Charles Porter by Charles Richard and exhibited at London Exhibition in 1862. The steam engine indicator traces on paper the pressure in the cylinder throughout the cycle, which can be used to spot various problems and calculate developed horsepower. It was routinely used by engineers, mechanics and insurance inspectors. The engine indicator can also be used on internal combustion engines. See image of indicator diagram below (in \"Types of motor units\" section).\nGovernor.\nThe centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one on the equipment of a flour mill Boulton &amp; Watt were building. The governor could not actually hold a set speed, because it would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped only with this governor were not suitable for operations requiring constant speed, such as cotton spinning. The governor was improved over time and coupled with variable steam cut off, good speed control in response to changes in load was attainable near the end of the 19th century.\nEngine configuration.\nSimple engine.\nIn a simple engine, or \"single expansion engine\", the charge of steam passes through the entire expansion process in an individual cylinder. A simple engine may have one or more individual cylinders. It is then exhausted directly into the atmosphere or into a condenser. As steam expands in passing through a high-pressure engine, its temperature drops because no heat is being added to the system; this is known as adiabatic expansion and results in steam entering the cylinder at high temperature and leaving at lower temperature. This causes a cycle of heating and cooling of the cylinder with every stroke, which is a source of inefficiency.\nThe dominant efficiency loss in reciprocating steam engines is cylinder condensation and re-evaporation. The steam cylinder and adjacent metal parts/ports operate at a temperature about halfway between the steam admission saturation temperature and the saturation temperature corresponding to the exhaust pressure. As high-pressure steam is admitted into the working cylinder, much of the high-temperature steam is condensed as water droplets onto the metal surfaces, significantly reducing the steam available for expansive work. When the expanding steam reaches low pressure (especially during the exhaust stroke), the previously deposited water droplets that had just been formed within the cylinder/ports now boil away (re-evaporation) and this steam does no further work in the cylinder.\nThere are practical limits on the expansion ratio of a steam engine cylinder, as increasing cylinder surface area tends to exacerbate the cylinder condensation and re-evaporation issues. This negates the theoretical advantages associated with a high ratio of expansion in an individual cylinder.\nCompound engines.\nA method to lessen the magnitude of energy loss to a very long cylinder was invented in 1804 by British engineer Arthur Woolf, who patented his \"Woolf high-pressure compound engine\" in 1805. In the compound engine, high-pressure steam from the boiler expands in a high-pressure (HP) cylinder and then enters one or more subsequent lower-pressure (LP) cylinders. The complete expansion of the steam now occurs across multiple cylinders, with the overall temperature drop within each cylinder reduced considerably. By expanding the steam in steps with smaller temperature range (within each cylinder) the condensation and re-evaporation efficiency issue (described above) is reduced. This reduces the magnitude of cylinder heating and cooling, increasing the efficiency of the engine. By staging the expansion in multiple cylinders, variations of torque can be reduced. To derive equal work from lower-pressure cylinder requires a larger cylinder volume as this steam occupies a greater volume. Therefore, the bore, and in rare cases the stroke, are increased in low-pressure cylinders, resulting in larger cylinders.\nDouble-expansion (usually known as compound) engines expanded the steam in two stages. The pairs may be duplicated or the work of the large low-pressure cylinder can be split with one high-pressure cylinder exhausting into two low pressure cylinders, giving a three-cylinder layout where cylinder and piston diameter are about the same, making the reciprocating masses easier to balance.\nTwo-cylinder compounds can be arranged as:\nWith two-cylinder compounds used in railway work, the pistons are connected to the cranks as with a two-cylinder simple at 90\u00b0 out of phase with each other (\"quartered\"). When the double-expansion group is duplicated, producing a four-cylinder compound, the individual pistons within the group are usually balanced at 180\u00b0, the groups being set at 90\u00b0 to each other. In one case (the first type of Vauclain compound), the pistons worked in the same phase driving a common crosshead and crank, again set at 90\u00b0 as for a two-cylinder engine. With the three-cylinder compound arrangement, the LP cranks were either set at 90\u00b0 with the HP one at 135\u00b0 to the other two, or in some cases, all three cranks were set at 120\u00b0.\nThe adoption of compounding was common for industrial units, for road engines and almost universal for marine engines after 1880; it was not universally popular in railway locomotives where it was often perceived as complicated. This is partly due to the harsh railway operating environment and limited space afforded by the loading gauge (particularly in Britain, where compounding was never common and not employed after 1930). However, although never in the majority, it was popular in many other countries.\nMultiple-expansion engines.\nIt is a logical extension of the compound engine (described above) to split the expansion into yet more stages to increase efficiency. The result is the multiple-expansion engine. Such engines use either three or four expansion stages and are known as \"triple-\" and \"quadruple-expansion engines\" respectively. These engines use a series of cylinders of progressively increasing diameter. These cylinders are designed to divide the work into equal shares for each expansion stage. As with the double-expansion engine, if space is at a premium, then two smaller cylinders may be used for the low-pressure stage. Multiple-expansion engines typically had the cylinders arranged inline, but various other formations were used. In the late 19th century, the Yarrow-Schlick-Tweedy balancing \"system\" was used on some marine triple-expansion engines. Y-S-T engines divided the low-pressure expansion stages between two cylinders, one at each end of the engine. This allowed the crankshaft to be better balanced, resulting in a smoother, faster-responding engine which ran with less vibration. This made the four-cylinder triple-expansion engine popular with large passenger liners (such as the \"Olympic\" class), but this was ultimately replaced by the virtually vibration-free turbine engine. It is noted, however, that triple-expansion reciprocating steam engines were used to drive the World War II Liberty ships, by far the largest number of identical ships ever built. Over 2700 ships were built, in the United States, from a British original design. \nThe image in this section shows an animation of a triple-expansion engine. The steam travels through the engine from left to right. The valve chest for each of the cylinders is to the left of the corresponding cylinder.\nLand-based steam engines could exhaust their steam to atmosphere, as feed water was usually readily available. Prior to and during World War I, the expansion engine dominated marine applications, where high vessel speed was not essential. It was, however, superseded by the British invention steam turbine where speed was required, for instance in warships, such as the dreadnought battleships, and ocean liners. of 1905 was the first major warship to replace the proven technology of the reciprocating engine with the then-novel steam turbine.\nTypes of motor units.\nReciprocating piston.\nIn most reciprocating piston engines, the steam reverses its direction of flow at each stroke (counterflow), entering and exhausting from the same end of the cylinder. The complete engine cycle occupies one rotation of the crank and two piston strokes; the cycle also comprises four \"events\" \u2013 admission, expansion, exhaust, compression. These events are controlled by valves often working inside a \"steam chest\" adjacent to the cylinder; the valves distribute the steam by opening and closing steam \"ports\" communicating with the cylinder end(s) and are driven by valve gear, of which there are many types.\nThe simplest valve gears give events of fixed length during the engine cycle and often make the engine rotate in only one direction. Many however have a reversing mechanism which additionally can provide means for saving steam as speed and momentum are gained by gradually \"shortening the cutoff\" or rather, shortening the admission event; this in turn proportionately lengthens the expansion period. However, as one and the same valve usually controls both steam flows, a short cutoff at admission adversely affects the exhaust and compression periods which should ideally always be kept fairly constant; if the exhaust event is too brief, the totality of the exhaust steam cannot evacuate the cylinder, choking it and giving excessive compression (\"kick back\").\nIn the 1840s and 1850s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff. The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated. The usual compromise solution has been to provide \"lap\" by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred. This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy, and Walschaerts motions. Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.\nCompression.\nBefore the exhaust phase is quite complete, the exhaust side of the valve closes, shutting a portion of the exhaust steam inside the cylinder. This determines the compression phase where a cushion of steam is formed against which the piston does work whilst its velocity is rapidly decreasing; it moreover obviates the pressure and temperature shock, which would otherwise be caused by the sudden admission of the high-pressure steam at the beginning of the following cycle.\nLead in the valve timing.\nThe above effects are further enhanced by providing \"lead\": as was later discovered with the internal combustion engine, it has been found advantageous since the late 1830s to advance the admission phase, giving the valve \"lead\" so that admission occurs a little before the end of the exhaust stroke in order to fill the \"clearance volume\" comprising the ports and the cylinder ends (not part of the piston-swept volume) before the steam begins to exert effort on the piston.\nUniflow (or unaflow) engine.\nUniflow engines attempt to remedy the difficulties arising from the usual counterflow cycle where, during each stroke, the port and the cylinder walls will be cooled by the passing exhaust steam, whilst the hotter incoming admission steam will waste some of its energy in restoring the working temperature. The aim of the uniflow is to remedy this defect and improve efficiency by providing an additional port uncovered by the piston at the end of each stroke making the steam flow only in one direction. By this means, the simple-expansion uniflow engine gives efficiency equivalent to that of classic compound systems with the added advantage of superior part-load performance, and comparable efficiency to turbines for smaller engines below one thousand horsepower. However, the thermal expansion gradient uniflow engines produce along the cylinder wall gives practical difficulties..\nTurbine engines.\nA steam turbine consists of one or more \"rotors\" (rotating discs) mounted on a drive shaft, alternating with a series of \"stators\" (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the United States with 60 Hertz power, and 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications, due to enormous size, the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.\nSteam turbines provide direct rotational force and therefore do not require a linkage mechanism to convert reciprocating to rotary motion. Thus, they produce smoother rotational forces on the output shaft. This contributes to a lower maintenance requirement and less wear on the machinery they power than a comparable reciprocating engine.\nThe main use for steam turbines is in electricity generation (in the 1990s about 90% of the world's electric production was by use of steam turbines) however the recent widespread application of large gas turbine units and typical combined cycle power plants has resulted in reduction of this percentage to the 80% regime for steam turbines. In electricity production, the high speed of turbine rotation matches well with the speed of modern electric generators, which are typically direct connected to their driving turbines. In marine service, (pioneered on the \"Turbinia\"), steam turbines with reduction gearing (although the Turbinia has direct turbines to propellers with no reduction gearbox) dominated large ship propulsion throughout the late 20th century, being more efficient (and requiring far less maintenance) than reciprocating steam engines. In recent decades, reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications.\nVirtually all nuclear power plants generate electricity by heating water to provide steam that drives a turbine connected to an electrical generator. Nuclear-powered ships and submarines either use a steam turbine directly for main propulsion, with generators providing auxiliary power, or else employ turbo-electric transmission, where the steam drives a turbo generator set with propulsion provided by electric motors. A limited number of steam turbine railroad locomotives were manufactured. Some non-condensing direct-drive locomotives did meet with some success for long haul freight operations in Sweden and for express passenger work in Britain, but were not repeated. Elsewhere, notably in the United States, more advanced designs with electric transmission were built experimentally, but not reproduced. It was found that steam turbines were not ideally suited to the railroad environment and these locomotives failed to oust the classic reciprocating steam unit in the way that modern diesel and electric traction has done.\nOscillating cylinder steam engines.\nAn oscillating cylinder steam engine is a variant of the simple expansion steam engine which does not require valves to direct steam into and out of the cylinder. Instead of valves, the entire cylinder rocks, or oscillates, such that one or more holes in the cylinder line up with holes in a fixed port face or in the pivot mounting (trunnion). These engines are mainly used in toys and models because of their simplicity, but have also been used in full-size working engines, mainly on ships where their compactness is valued.\nRotary steam engines.\nIt is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed, from the time of James Watt to the present day, but relatively few were actually built and even fewer went into quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very inefficient. Lack of expansive working, or any means of control of the cutoff, is also a serious problem with many such designs.\nBy the 1840s, it was clear that the concept had inherent problems and rotary engines were treated with some derision in the technical press. However, the arrival of electricity on the scene, and the obvious advantages of driving a dynamo directly from a high-speed engine, led to something of a revival in interest in the 1880s and 1890s, and a few designs had some limited success..\nOf the few designs that were manufactured in quantity, those of the Hult Brothers Rotary Steam Engine Company of Stockholm, Sweden, and the spherical engine of Beauchamp Tower are notable. Tower's engines were used by the Great Eastern Railway to drive lighting dynamos on their locomotives, and by the Admiralty for driving dynamos on board the ships of the Royal Navy. They were eventually replaced in these niche applications by steam turbines.\nRocket type.\nThe aeolipile represents the use of steam by the rocket-reaction principle, although not for direct propulsion.\nIn more modern times there has been limited use of steam for rocketry \u2013 particularly for rocket cars. Steam rocketry works by filling a pressure vessel with hot water at high pressure and opening a valve leading to a suitable nozzle. The drop in pressure immediately boils some of the water and the steam leaves through a nozzle, creating a propulsive force.\nFerdinand Verbiest's carriage was powered by an aeolipile in 1679.\nSafety.\nSteam engines possess boilers and other components that are pressure vessels that contain a great deal of potential energy. Steam escapes and boiler explosions (typically BLEVEs) can and have in the past caused great loss of life. While variations in standards may exist in different countries, stringent legal, testing, training, care with manufacture, operation and certification is applied to ensure safety.\nFailure modes may include:\nSteam engines frequently possess two independent mechanisms for ensuring that the pressure in the boiler does not go too high; one may be adjusted by the user, the second is typically designed as an ultimate fail-safe. Such safety valves traditionally used a simple lever to restrain a plug valve in the top of a boiler. One end of the lever carried a weight or spring that restrained the valve against steam pressure. Early valves could be adjusted by engine drivers, leading to many accidents when a driver fastened the valve down to allow greater steam pressure and more power from the engine. The more recent type of safety valve uses an adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal is illegally broken. This arrangement is considerably safer.\nLead fusible plugs may be present in the crown of the boiler's firebox. If the water level drops, such that the temperature of the firebox crown increases significantly, the lead melts and the steam escapes, warning the operators, who may then manually suppress the fire. Except in the smallest of boilers the steam escape has little effect on dampening the fire. The plugs are also too small in area to lower steam pressure significantly, depressurizing the boiler. If they were any larger, the volume of escaping steam would itself endanger the crew.\nSteam cycle.\nThe Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and uses the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal, and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath.\nThe Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition (in the boiler) and rejection (in the condenser) are isobaric (constant pressure) processes in the Rankine cycle and isothermal (constant temperature) processes in the theoretical Carnot cycle. In this cycle, a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the energy needed to compress the working fluid in gaseous form in a compressor (as in the Carnot cycle). The cycle of a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in the cylinder or in the steam inlet passages.\nThe working fluid in a Rankine cycle can operate as a closed loop system, where the working fluid is recycled continuously, or may be an \"open loop\" system, where the exhaust steam is directly released to the atmosphere, and a separate source of water feeding the boiler is supplied. Normally water is the fluid of choice due to its favourable properties, such as non-toxic and unreactive chemistry, abundance, low cost, and its thermodynamic properties. Mercury is the working fluid in the mercury vapor turbine. Low boiling hydrocarbons can be used in a binary cycle.\nThe steam engine contributed much to the development of thermodynamic theory; however, the only applications of scientific theory that influenced the steam engine were the original concepts of harnessing the power of steam and atmospheric pressure and knowledge of properties of heat and steam. The experimental measurements made by Watt on a model steam engine led to the development of the separate condenser. Watt independently discovered latent heat, which was confirmed by the original discoverer Joseph Black, who also advised Watt on experimental procedures. Watt was also aware of the change in the boiling point of water with pressure. Otherwise, the improvements to the engine itself were more mechanical in nature. The thermodynamic concepts of the Rankine cycle did give engineers the understanding needed to calculate efficiency which aided the development of modern high-pressure and -temperature boilers and the steam turbine.\nEfficiency.\nThe efficiency of an engine cycle can be calculated by dividing the energy output of mechanical work that the engine produces by the energy put into the engine.\nThe historical measure of a steam engine's energy efficiency was its \"duty\". The concept of duty was first introduced by Watt in order to illustrate how much more efficient his engines were over the earlier Newcomen designs. Duty is the number of foot-pounds of work delivered by burning one bushel () of coal. The best examples of Newcomen designs had a duty of about 7\u00a0million, but most were closer to 5\u00a0million. Watt's original low-pressure designs were able to deliver duty as high as 25\u00a0million, but averaged about 17. This was a three-fold improvement over the average Newcomen design. Early Watt engines equipped with high-pressure steam improved this to 65\u00a0million.\nThe Carnot cycle, a theoretical ideal thermodynamic cycle, sets the limit of a steam engine's efficiency. Carnot cycle describes a system of two thermal reservoirs with heat transferred between them. In a steam engine part of the heat transferred is converted into mechanical work. The greater the difference in temperature between the two reservoirs, the more efficient the engine. One way of increasing the difference is by using superheated steam.\nThe efficiency of a Rankine cycle is usually limited by the working fluid. Without the pressure reaching supercritical levels for the working fluid, the temperature range over which the cycle can operate is small; in steam turbines, turbine entry temperatures are typically (the creep limit of stainless steel) and condenser temperatures are around . This gives a theoretical Carnot efficiency of about 64% compared with an actual efficiency of 42% for a modern coal-fired power station. This low turbine entry temperature (compared with a gas turbine) is why the Rankine cycle is often used as a bottoming cycle in combined-cycle gas turbine power stations.\nOne principal advantage the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine (or reciprocating engine) power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching . Nonetheless, the efficiencies of actual large steam cycles and large modern simple cycle gas turbines are fairly well matched.\nIn practice, a reciprocating steam engine cycle exhausting the steam to atmosphere will typically have an efficiency (including the boiler) in the range of 1\u201310%. However, with the addition of a condenser, Corliss valves, multiple expansion, and high steam pressure/temperature, it may be greatly improved. Historically into the range of 10\u201320%, and very rarely slightly higher. A modern, large electrical power station (producing several hundred megawatts of electrical output) with steam reheat, economizer etc. will achieve efficiency in the mid 40% range, with the most efficient units approaching 50% thermal efficiency.\nIt is also possible to capture the waste heat using cogeneration in which the waste heat is used for heating a lower boiling point working fluid or as a heat source for district heating via saturated low-pressure steam.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBooks.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "27694", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=27694", "title": "Satan", "text": "Figure in Abrahamic religions\nSatan, also known as the Devil, is an entity in Abrahamic religions who entices humans into sin or falsehood. In Judaism, Satan is seen as an agent subservient to God, typically regarded as a metaphor for the , or 'evil inclination'. In Christianity and Islam, he is usually seen as a fallen angel or jinn who has rebelled against God, who nevertheless allows him temporary power over the fallen world and a host of demons.\nA figure known as (\"the satan\") first appears in the Hebrew Bible as a heavenly prosecutor, subordinate to Yahweh (God); he prosecutes the nation of Judah in the heavenly court and tests the loyalty of Yahweh's followers. During the intertestamental period, possibly due to influence from the Zoroastrian figure of Angra Mainyu, the satan developed into a malevolent entity with abhorrent qualities in dualistic opposition to God. In the apocryphal Book of Jubilees, Yahweh grants the satan (referred to as Mastema) authority over a group of fallen angels, or their offspring, to tempt humans to sin and punish them.\nAlthough the Book of Genesis does not name him specifically, Christians often identify the serpent in the Garden of Eden as Satan. In the Synoptic Gospels, Satan tempts Jesus in the desert and is identified as the cause of illness and temptation. In the Book of Revelation, Satan appears as a Great Red Dragon, who is defeated by Michael the Archangel and cast down from Heaven. He is later bound for one thousand years, but is briefly set free before being ultimately defeated and cast into the Lake of Fire.\nIn the Quran, Iblis (Shaitan), the leader of the devils (), is made of fire and was cast out of Heaven because he refused to bow before the newly created Adam. He incites humans to sin by infecting their minds with \"wasw\u0101s\" ('evil suggestions').\nIn the Middle Ages, Satan played a minimal role in Christian theology and was used as a comic relief figure in mystery plays. During the early modern period, Satan's significance greatly increased as beliefs such as demonic possession and witchcraft became more prevalent. During the Age of Enlightenment, belief in the existence of Satan was harshly criticized by thinkers such as Voltaire. Nonetheless, belief in Satan has persisted, particularly in the Americas.\nAlthough Satan is generally viewed as evil, some groups have very different beliefs. In theistic Satanism, Satan is considered a deity who is either worshipped or revered. In LaVeyan Satanism, Satan is a symbol of virtuous characteristics and liberty. Satan's appearance is never described in the Bible, but, since the ninth century, he has often been shown in Christian art with horns, cloven hooves, unusually hairy legs, and a tail, naked and holding a pitchfork. These are an amalgam of traits derived from various pagan deities, including Pan, Poseidon, and Bes. Satan appears frequently in Christian literature, most notably in Dante Alighieri's \"Inferno\", all variants of the classic Faust story, John Milton's \"Paradise Lost\" and \"Paradise Regained\", and the poems of William Blake. He continues to appear in literature, film, television, video games, and music.\nHistorical development.\nHebrew Bible.\nThe Hebrew term \"\u015b\u0101\u1e6d\u0101n\" () is a generic noun meaning \"accuser\" or \"adversary\", and is derived from a verb meaning primarily \"to obstruct, oppose\". In the earlier biblical books, e.g. , the term refers to human adversaries, but in the later books, especially Job 1\u20132 and Zechariah 3, to a supernatural entity. When used without the definite article (simply \"satan\"), it can refer to any accuser, but when it is used with the definite article (\"ha-satan\"), it usually refers specifically to the heavenly accuser, literally, \"the\" satan.\nThe word with the definite article \"Ha-Satan\" ( \"hasS\u0101\u1e6d\u0101n\") occurs 17 times in the Masoretic Text, in two books of the Hebrew Bible: Job ch. 1\u20132 (14\u00d7) and Zechariah 3:1\u20132 (3\u00d7). It is translated in English Bibles mostly as 'Satan'.\nThe word does not occur in the Book of Genesis, which mentions only a talking serpent and does not identify the serpent with any supernatural entity. The first occurrence of the word \"satan\" in the Hebrew Bible in reference to a supernatural figure comes from , which describes the Angel of Yahweh confronting Balaam on his donkey: \"Balaam's departure aroused the wrath of Elohim, and the Angel of Yahweh stood in the road as a satan against him.\" In Yahweh sends the \"Angel of Yahweh\" to inflict a plague against Israel for three days, killing 70,000 people as punishment for David having taken a census without his approval. repeats this story, but replaces the \"Angel of Yahweh\" with an entity referred to as \"a Satan\".\nSome passages may refer to Satan, without using the word itself. describes the sons of Eli as \"sons of Belial\"; the name \"Belial\" may be a synonym for \"satan\", although elsewhere in the Bible \"belial\" is a word meaning \"worthlessness\" and the phrase \"sons of belial\" is translated as \"worthless fellows\". In Yahweh sends a \"troubling spirit\" to torment King Saul as a mechanism to ingratiate David with the king. In the prophet Micaiah describes to King Ahab a vision of Yahweh sitting on his throne surrounded by the Host of Heaven. Yahweh asks the Host which of them will lead Ahab astray. A \"spirit\", whose name is not specified, but who is analogous to the satan, volunteers to be \"a Lying Spirit in the mouth of all his Prophets\".\nBook of Job.\nThe satan appears in the Book of Job, a poetic dialogue set within a prose framework, which may have been written around the time of the Babylonian captivity. In the text, Job is a righteous man favored by Yahweh. describes the \"sons of God\" (\"b\u0259n\u00ea h\u0101\u02bc\u0115l\u014dh\u00eem\") presenting themselves before Yahweh. Yahweh asks one of them, \"the satan\", where he has been, to which he replies that he has been roaming around the earth. Yahweh asks, \"Have you considered My servant Job?\" The satan replies by urging Yahweh to let him torture Job, promising that Job will abandon his faith at the first tribulation. Yahweh consents: the satan destroys Job's servants and flocks, yet Job refuses to condemn Yahweh. The first scene repeats itself, with the satan presenting himself to Yahweh alongside the other \"sons of God\". Yahweh points out Job's continued faithfulness, to which the satan insists that more testing is necessary; Yahweh once again gives him permission to test Job. In the end, Job remains faithful and righteous, and it is implied that the satan is shamed in his defeat.\nBook of Zechariah.\n contains a description of a vision dated to the middle of February of 519 BC, in which an angel shows Zechariah a scene of Joshua the High Priest dressed in filthy rags, representing the nation of Judah and its sins, on trial with Yahweh as the judge and the satan standing as the prosecutor. Yahweh rebukes Satan and orders for Joshua to be given clean clothes, representing Yahweh's forgiveness of Judah's sins.\nSecond Temple period.\nFor much of Second Temple Period Jews lived under the Achaemenid Empire, providing the opportunity for Jews to be influenced by Zoroastrianism, the religion of the Achaemenids. Jewish conceptions of Satan were impacted by Angra Mainyu, the Zoroastrian spirit of evil, darkness, and ignorance. In the Septuagint, the Hebrew \"ha-Satan\" in Job and Zechariah is translated by the Greek word \"diabolos\" (slanderer), the same word in the Greek New Testament from which the English word \"devil\" is derived. Where \"satan\" is used to refer to human enemies in the Hebrew Bible, such as Hadad the Edomite and Rezon the Syrian, the word is left untranslated but is instead transliterated in the Greek as \"satan\", a neologism in Greek.\nThe idea of Satan as an opponent of God and a purely evil figure seems to have taken root in Jewish pseudepigrapha during the Second Temple Period, particularly in the \"apocalypses\". The Book of Enoch, which the Dead Sea Scrolls have revealed to have been nearly as popular as the Torah, describes a group of 200 angels known as the \"Watchers\", who are assigned to supervise the earth but instead abandon their duties and have sexual intercourse with human women. The leader of the Watchers is Semj\u00e2z\u00e2 and another member of the group, known as Azazel, spreads sin and corruption among humankind. The Watchers are ultimately sequestered in isolated caves across the earth and are condemned to face judgement at the end of time. The Book of Jubilees, written around 150 BC, retells the story of the Watchers' defeat, but, in deviation from the Book of Enoch, Mastema, the \"Chief of Spirits\", intervenes before all of their demon offspring are sealed away, requesting for Yahweh to let him keep some of them to become his workers. Yahweh acquiesces to this request and Mastema uses them to tempt humans into committing more sins, so that he may punish them for their wickedness. Later, Mastema induces Yahweh to test Abraham by ordering him to sacrifice Isaac.\nThe Second Book of Enoch, also called the Slavonic Book of Enoch, contains references to a Watcher called Satanael. It is a pseudepigraphic text of an uncertain date and unknown authorship. The text describes Satanael as being the prince of the Grigori who was cast out of heaven and an evil spirit who knew the difference between what was \"righteous\" and \"sinful\". In the Book of Wisdom, the devil is taken to be the being who brought death into the world, but originally the culprit was recognized as Cain. The name Samael, which is used in reference to one of the fallen angels, later became a common name for Satan in Jewish Midrash and Kabbalah.\nJudaism.\nMost Jews do not believe in the existence of a supernatural omnimalevolent figure. Traditionalists and philosophers in medieval Judaism adhered to rational theology, rejecting any belief in rebel or fallen angels, and viewing evil as abstract. The rabbis usually interpreted the word \"satan\" lacking the article \"ha-\", as it is used in the Tanakh, as referring strictly to \"human\" adversaries. Nonetheless, the word \"satan\" has occasionally been metaphorically applied to evil influences, such as the Jewish exegesis of the \"yetzer hara\" (\"evil inclination\") mentioned in Genesis 6:5.\nThe Talmudic image of Satan is contradictory. While Satan's identification with the abstract \"yetzer hara\" remains uniform over the sages' teachings, he is generally identified as an entity with independent agency. For instance, one rabbi identified Satan both with the angel of death and with the \"yetzer hara\". Satan's status as a personality is strengthened by numerous other rabbinical anecdotes: one tale describes incidents where Satan appeared as a woman in order to tempt Rabbi Meir and Rabbi Akiva into sin, while another describes Satan taking the form of an ill-mannered, diseased beggar in order to tempt the sage Peleimu into breaking the mitzvah of hospitality. Another passage relates that Satan once kissed the feet of Aha bar Jacob for having taught his students that his objectionable actions are done only to serve the intents of God.\nRabbinical scholarship on the Book of Job generally follows the Talmud and Maimonides in identifying \"the satan\" from the prologue as a metaphor for the \"yetzer hara\" and not an actual entity. Satan is rarely mentioned in Tannaitic literature, but is found in Babylonian aggadah. According to a narration, the sound of the shofar, which is primarily intended to remind Jews of the importance of \"teshuva\", is also intended symbolically to \"confuse the accuser\" (Satan) and prevent him from rendering any litigation to God against the Jews. Kabbalah presents Satan as an agent of God whose function is to tempt humans into sinning so that he may accuse them in the heavenly court. \nModern denominations of Judaism have differing interpretations of Satan's identity. Orthodox Judaism accepts the diversity of Talmudic teachings on Satan, and Satan is mentioned in some standard prayers, such as the Shema blessings in Maariv. In Reform Judaism, Satan is generally seen in his Talmudic role as a metaphor for the \"yetzer hara\" and the symbolic representation of innate human qualities such as selfishness.\nChristianity.\nNames.\nThe most common English synonym for \"Satan\" is \"devil\", which descends from Middle English \"devel\", from Old English \"d\u0113ofol,\" that in turn represents an early Germanic borrowing of Latin \"diabolus\" (also the source of \"diabolical\"). This in turn was borrowed from Greek \"diabolos\" \"slanderer\", from \"diaballein\" \"to slander\": \"dia-\" \"across, through\" + \"ballein\" \"to hurl\". In the New Testament, the words \"Satan\" and \"diabolos\" are used interchangeably as synonyms. Beelzebub, meaning \"Lord of Flies\", is the contemptuous name given in the Hebrew Bible and New Testament to a Philistine god whose original name has been reconstructed as most probably \"Ba'al Zabul\", meaning \"Baal the Prince\". The Synoptic Gospels identify Satan and Beelzebub as the same. The name Abaddon (meaning \"place of destruction\") is used six times in the Old Testament, mainly as a name for one of the regions of Sheol. describes Abaddon, whose name is translated into Greek as \"Apollyon\", meaning \"the destroyer\", as an angel who rules the Abyss. In modern usage, Abaddon is sometimes equated with Satan.\nNew Testament.\nGospels, Acts, and epistles.\nThe three Synoptic Gospels all describe the temptation of Christ by Satan in the desert (, , and ). Satan first shows Jesus a stone and tells him to turn it into bread. He also takes him to the pinnacle of the Temple in Jerusalem and commands Jesus to throw himself down so that the angels will catch him. Satan takes Jesus to the top of a tall mountain as well; there, he shows him the kingdoms of the earth and promises to give them all to him if he will bow down and worship him. Each time Jesus rebukes Satan and, after the third temptation, he is administered by the angels. Satan's promise in and to give Jesus all the kingdoms of the earth implies that all those kingdoms belong to him. The fact that Jesus does not dispute Satan's promise indicates that the authors of those gospels believed this to be true.\nSatan plays a role in some of the parables of Jesus, namely the Parable of the Sower, the Parable of the Weeds, Parable of the Sheep and the Goats, and the Parable of the Strong Man. According to the Parable of the Sower, Satan \"profoundly influences\" those who fail to understand the gospel. The latter two parables say that Satan's followers will be punished on Judgement Day, with the Parable of the Sheep and the Goats stating that the Devil, his angels, and the people who follow him will be consigned to \"eternal fire\". When the Pharisees accused Jesus of exorcising demons through the power of Beelzebub, Jesus responded by telling the Parable of the Strong Man, saying: \"how can someone enter a strong man's house and plunder his goods, unless he first binds the strong man? Then indeed he may plunder his house\" (). The strong man in this parable represents Satan.\nThe Synoptic Gospels identify Satan and his demons as the causes of illness, including fever (), leprosy (), and arthritis (), while the Epistle to the Hebrews describes the Devil as \"him who holds the power of death\" (). The author of Luke-Acts attributes more power to Satan than either Matthew and Mark. In , Jesus grants Satan the authority to test Peter and the other apostles. states that Judas Iscariot betrayed Jesus because \"Satan entered\" him and, in , Peter describes Satan as \"filling\" Ananias's heart and causing him to sin. The Gospel of John only uses the name \"Satan\" three times. In , Jesus says that his Jewish or Judean enemies are the children of the Devil rather than the children of Abraham. The same verse describes the Devil as \"a man-killer from the beginning\" and \"a liar and the father of lying.\" describes the Devil as inspiring Judas to betray Jesus and identifies Satan as \"prince of this world\", who is destined to be overthrown through Jesus's death and resurrection. promises that the Holy Spirit will \"accuse the World concerning sin, justice, and judgement\", a role resembling that of the Satan in the Old Testament.\n refers to a dispute between Michael the Archangel and the Devil over the body of Moses. Some interpreters understand this reference to be an allusion to the events described in . The classical theologian Origen attributes this reference to the non-canonical Assumption of Moses. According to James H. Charlesworth, there is no evidence the surviving book of this name ever contained any such content. Others believe it to be in the lost ending of the book. The second chapter of the pseudepigraphical Second Epistle of Peter copies much of the content of the Epistle of Jude, but omits the specifics of the example regarding Michael and Satan, with instead mentioning only an ambiguous dispute between \"Angels\" and \"Glories\". Throughout the New Testament, Satan is referred to as a \"tempter\" (), \"the ruler of the demons\" (), \"the God of this Age\" (), \"the evil one\" (), and \"a roaring lion\" ().\nBook of Revelation.\nThe Book of Revelation represents Satan as the supernatural ruler of the Roman Empire and the ultimate cause of all evil in the world. In , as part of the letter to the church at Smyrna, John of Patmos refers to the Jews of Smyrna as \"a synagogue of Satan\" and warns that \"the Devil is about to cast some of you into prison as a test [\"peirasmos\"], and for ten days you will have affliction.\" In , in the letter to the church of Pergamum, John warns that Satan lives among the members of the congregation and declares that \"Satan's throne\" is in their midst. Pergamum was the capital of the Roman Province of Asia and \"Satan's throne\" may be referring to the monumental Pergamon Altar in the city, which was dedicated to the Greek god Zeus, or to a temple dedicated to the Roman emperor Augustus.\n describes a vision of a Great Red Dragon with seven heads, ten horns, seven crowns, and a massive tail, an image which is likely inspired by the vision of the four beasts from the sea in the Book of Daniel and the Leviathan described in various Old Testament passages. The Great Red Dragon knocks \"a third of the sun... a third of the moon, and a third of the stars\" out the sky and pursues the Woman of the Apocalypse. declares: \"And war broke out in Heaven. Michael and his angels fought against the Dragon. The Dragon and his angels fought back, but they were defeated, and there was no longer any place for them in Heaven. Dragon the Great was thrown down, that ancient serpent who is called Devil and Satan, the one deceiving the whole inhabited World \u2013 he was thrown down to earth, and his angels were thrown down with him.\" Then a voice booms down from Heaven heralding the defeat of \"the Accuser\" (\"ho Kantegor\"), identifying the Satan of Revelation with the satan of the Old Testament.\nIn , Satan is bound with a chain and hurled into the Abyss, where he is imprisoned for one thousand years. In , he is set free and gathers his armies along with Gog and Magog to wage war against the righteous, but is defeated with fire from Heaven, and cast into the lake of fire. Some Christians associate Satan with the number 666, which describes as the Number of the Beast. However, the beast mentioned in Revelation 13 is not Satan, and the use of 666 in the Book of Revelation has been interpreted as a reference to the Roman Emperor Nero, as 666 is the numeric value of his name in Hebrew.\nPatristic era.\n\"The Garden of Eden with the Fall of Man\" by Jan Brueghel the Elder and Pieter Paul Rubens, c.\u20091615, depicting Eve reaching for the forbidden fruit beside the Devil portrayed as a serpent\nChristians have traditionally interpreted the unnamed serpent in the Garden of Eden as Satan due to , which calls Satan \"that ancient serpent\". This verse, however, is probably intended to identify Satan with the Leviathan, a monstrous sea-serpent whose destruction by Yahweh is prophesied in . The first recorded individual to identify Satan with the serpent from the Garden of Eden was the second-century AD Christian apologist Justin Martyr, in chapters 45 and 79 of his \"Dialogue with Trypho\". Other early Church Fathers to mention this identification include Theophilus and Tertullian. The early Christian Church, however, encountered opposition from pagans such as Celsus, who claimed in his treatise \"The True Word\" that \"it is blasphemy... to say that the greatest God... has an adversary who constrains his capacity to do good\" and said that Christians \"impiously divide the kingdom of God, creating a rebellion in it, as if there were opposing factions within the divine, including one that is hostile to God\".\nThe name \"Heylel\", meaning \"morning star\" (or, in Latin, \"Lucifer\"), was a name for Attar, the god of the planet Venus in Canaanite mythology, who attempted to scale the walls of the heavenly city, but was vanquished by the god of the sun. The name is used in in metaphorical reference to the king of Babylon. uses a description of a cherub in Eden as a polemic against Ithobaal II, the king of Tyre.\nThe Church Father Origen of Alexandria (c. 184 \u2013 c. 253), who was only aware of the actual text of these passages and not the original myths to which they refer, concluded in his treatise \"On the First Principles\", which is preserved in a Latin translation by Tyrannius Rufinus, that neither of these verses could literally refer to a human being. He concluded that Isaiah 14:12 is an allegory for Satan and that Ezekiel 28:12\u201315 is an allusion to \"a certain Angel who had received the office of governing the nation of the Tyrians\", but was hurled down to Earth after he was found to be corrupt. In his apologetic treatise \"Contra Celsum\", however, Origen interprets both Isaiah 14:12 and Ezekiel 28:12\u201315 as referring to Satan. According to Henry Ansgar Kelly, Origen seems to have adopted this new interpretation to refute unnamed persons who, perhaps under the influence of Zoroastrian radical dualism, believed \"that Satan's original nature was Darkness.\" The later Church Father Jerome (c. 347 \u2013 420), translator of the Latin Vulgate, accepted Origen's theory of Satan as a fallen angel and wrote about it in his commentary on the Book of Isaiah. In Christian tradition ever since, both Isaiah 14:12 and Ezekiel 28:12\u201315 have been understood as allegorically referring to Satan. For most Christians, Satan has been regarded as an angel who rebelled against God.\nAccording to the ransom theory of atonement, which was popular among early Christian theologians, Satan gained power over humanity through Adam and Eve's sin, and Christ's death on the cross was a ransom to Satan in exchange for humanity's liberation. This theory holds that Satan was tricked by God because Christ was not only free of sin, but also the incarnate Deity, whom Satan lacked the ability to enslave. Irenaeus of Lyons described a prototypical form of the ransom theory, but Origen was the first to propose it in its fully developed form. The theory was later expanded by theologians such as Gregory of Nyssa and Rufinus of Aquileia. In the eleventh century, Anselm of Canterbury criticized the ransom theory, along with the associated Christus Victor theory, resulting in the theory's decline in western Europe. The theory has nonetheless retained some of its popularity in the Eastern Orthodox Church.\nMost early Christians firmly believed that Satan and his demons had the power to possess humans, and exorcisms were widely practiced by Jews, Christians, and pagans alike. Belief in demonic possession continued through the Middle Ages into the early modern period. Exorcisms were seen as a display of God's power over Satan. The vast majority of people who thought they were possessed by the Devil did not suffer from hallucinations or other \"spectacular symptoms\", but \"complained of anxiety, religious fears, and evil thoughts\".\nMiddle Ages.\nSatan had minimal role in medieval Christian theology, but he frequently appeared as a recurring comedic stock character in late medieval mystery plays, in which he was portrayed as a comic relief figure who \"frolicked, fell, and farted in the background\". Jeffrey Burton Russell describes the medieval conception of Satan as \"more pathetic and repulsive than terrifying\" and he was seen as little more than a nuisance to God's overarching plan. The \"Golden Legend\", a collection of saints' lives compiled in around 1260 by the Dominican Friar Jacobus de Voragine, contains numerous stories about encounters between saints and Satan, in which Satan is constantly duped by the saints' cleverness and by the power of God. Henry Ansgar Kelly remarks that Satan \"comes across as the opposite of fearsome\". The \"Golden Legend\" was the most popular book during the High and Late Middle Ages and more manuscripts of it have survived from the period than of any other book, including even the Bible itself.\nThe \"Canon Episcopi\", written in the eleventh century AD, condemns belief in witchcraft as heretical, but also documents that many people at the time apparently believed in it. Witches were believed to fly through the air on broomsticks, consort with demons, perform in \"lurid sexual rituals\" in the forests, murder human infants and eat them as part of Satanic rites, and engage in conjugal relations with demons. In 1326, Pope John XXII issued the papal bull \"Super illius Specula\", which condemned folk divination practices as consultation with Satan. By the 1430s, the Catholic Church began to regard witchcraft as part of a vast conspiracy led by Satan himself.\nEarly modern period.\nDuring the Early Modern Period, Christians gradually began to regard Satan as increasingly powerful and the fear of Satan's power became a dominant aspect of the worldview of Christians across Europe. During the Protestant Reformation, Martin Luther taught that, rather than trying to argue with Satan, Christians should avoid temptation altogether by seeking out pleasant company; Luther especially recommended music as a safeguard against temptation, since the Devil \"cannot endure gaiety\". John Calvin repeated a maxim from Saint Augustine that \"Man is like a horse, with either God or the devil as rider.\"\nIn the late fifteenth century, a series of witchcraft panics erupted in France and Germany. The German Inquisitors Heinrich Kramer and Jacob Sprenger argued in their book \"Malleus Maleficarum\", published in 1487, that all \"maleficia\" (\"sorcery\") was rooted in the work of Satan. In the mid-sixteenth century, the panic spread to England and Switzerland. Both Protestants and Catholics alike firmly believed in witchcraft as a real phenomenon and supported its prosecution. In the late 1500s, the Dutch demonologist Johann Weyer argued in his treatise \"De praestigiis daemonum\" that witchcraft did not exist, but that Satan promoted belief in it to lead Christians astray. The panic over witchcraft intensified in the 1620s and continued until the end of the 1600s. Brian Levack estimates that around 60,000 people were executed for witchcraft during the entire span of the witchcraft hysteria.\nThe early English settlers of North America, especially the Puritans of New England, believed that Satan \"visibly and palpably\" reigned in the New World. John Winthrop claimed that the Devil made rebellious Puritan women give birth to stillborn monsters with claws, sharp horns, and \"on each foot three claws, like a young fowl\". Cotton Mather wrote that devils swarmed around Puritan settlements \"like the frogs of Egypt\". The Puritans believed that the Native Americans were worshippers of Satan and described them as \"children of the Devil\". Some settlers claimed to have seen Satan himself appear in the flesh at native ceremonies. During the First Great Awakening, the \"new light\" preachers portrayed their \"old light\" critics as ministers of Satan. By the time of the Second Great Awakening, Satan's primary role in American evangelicalism was as the opponent of the evangelical movement itself, who spent most of his time trying to hinder the ministries of evangelical preachers, a role he has largely retained among present-day American fundamentalists.\nBy the early 1600s, skeptics in Europe, including the English author Reginald Scot and the Anglican bishop John Bancroft, had begun to criticize the belief that demons still had the power to possess people. This skepticism was bolstered by the belief that miracles only occurred during the Apostolic Age, which had long since ended. Later, Enlightenment thinkers, such as David Hume, Denis Diderot, and Voltaire, attacked the notion of Satan's existence altogether. Voltaire labelled John Milton's \"Paradise Lost\" a \"disgusting fantasy\" and declared that belief in Hell and Satan were among the many lies propagated by the Catholic Church to keep humanity enslaved. By the eighteenth century, trials for witchcraft had ceased in most western countries, with the notable exceptions of Poland and Hungary, where they continued. Belief in the power of Satan, however, remained strong among traditional Christians.\nModern era.\nMormonism developed its own views on Satan. According to the Book of Moses, the Devil offered to be the redeemer of mankind for the sake of his own glory. Conversely, Jesus offered to be the redeemer of mankind so that his father's will would be done. After his offer was rejected, Satan became rebellious and was subsequently cast out of heaven. In the Book of Moses, Cain is said to have \"loved Satan more than God\" and conspired with Satan to kill Abel. It was through this pact that Cain became a Master Mahan. The Book of Moses also says that Moses was tempted by Satan before calling upon the name of the \"Only Begotten\", which caused Satan to depart. Douglas Davies asserts that this text \"reflects\" the temptation of Jesus in the Bible.\nBelief in Satan and demonic possession remains strong among Christians in the United States and Latin America. According to a 2013 poll conducted by YouGov, fifty-seven percent of people in the United States believe in a literal Devil, compared to eighteen percent of people in Britain. Fifty-one percent of Americans believe that Satan has the power to possess people. W. Scott Poole, author of \"Satan in America: The Devil We Know\", has opined that \"In the United States over the last forty to fifty years, a composite image of Satan has emerged that borrows from both popular culture and theological sources\" and that most American Christians do not \"separate what they know [about Satan] from the movies from what they know from various ecclesiastical and theological traditions\". The Catholic Church generally played down Satan and exorcism during late twentieth and early twenty-first centuries, but Pope Francis brought renewed focus on the Devil in the early 2010s, stating, among many other pronouncements, that \"The devil is intelligent, he knows more theology than all the theologians together.\" According to the \"Encyclop\u00e6dia Britannica\", liberal Christianity tends to view Satan \"as a [figurative] mythological attempt to express the reality and extent of evil in the universe, existing outside and apart from humanity but profoundly influencing the human sphere\".\nBernard McGinn describes multiple traditions detailing the relationship between the Antichrist and Satan. In the dualist approach, Satan will become incarnate in the Antichrist, just as God became incarnate in Jesus. However, in Orthodox Christian thought, this view is problematic because it is too similar to Christ's incarnation. Instead, the \"indwelling\" view has become more accepted, which stipulates that the Antichrist is a human figure inhabited by Satan, since the latter's power is not to be seen as equivalent to God's.\nIslam.\nThe Arabic equivalent of the word \"Satan\" is \"Shaitan\" (\u0634\u064a\u0637\u0627\u0646, from the triliteral root \u0161-\u1e6d-n \u0634\u0637\u206c\u0646). The word itself is an adjective (meaning \"astray\" or \"distant\", sometimes translated as \"devil\") that can be applied to both man (\"al-ins\", \u0627\u0644\u0625\u0646\u0633) and \"al-jinn\" (\u0627\u0644\u062c\u0646), but it is also used in reference to Satan in particular. In the Quran, Satan's name is Iblis (), probably a derivative of the Greek word \"diabolos\". Muslims do not regard Satan as the cause of evil, but as a tempter, who takes advantage of humans' inclinations toward self-centeredness.\nQuran.\nSeven chapters in the Quran describe how God ordered all the angels and Iblis to bow before the newly created human, Adam. All the angels bowed, but Iblis refused, claiming to be superior to Adam because he was made from fire, whereas Adam was made from clay ( https://). Consequently, God expelled him from Paradise and condemned him to Jahannam. Iblis thereafter became a \"kafir\", \"an ungrateful disbeliever\", whose sole mission is to lead humanity astray. (Q https://) God allows Iblis to do this, because he knows that the righteous will be able to resist Iblis's attempts to misguide them. On Judgement Day, while the lot of Satan remains in question, those who followed him will be thrown into the fires of Jahannam. After his banishment from Paradise, Iblis, who thereafter became known as \"Al-Shaitan\" (\"the Demon\"), lured Adam and Eve into eating the forbidden fruit.\nThe primary characteristic of Satan, aside from his hubris and despair, is his ability to cast evil suggestions (\"wasw\u0101s\") into men and women. https:// states that Satan has no influence over the righteous, but that those who fall in error are under his power. https:// implies that those who obey God's laws are immune to the temptations of Satan. https:// warns that Satan tries to keep Muslims from reading the Quran and https:// recommends reciting the Quran as an antidote against Satan. https:// refers to Satan as the enemy of humanity and https:// forbids humans from worshipping him. In the Quranic retelling of the story of Job, Job knows that Satan is the one tormenting him.\nIslamic tradition.\nAffiliation.\nIn the Quran, Satan is apparently an angel, while, in https://, he is described as \"from the jinns\". This, combined with the fact that he describes himself as having been made from fire, posed a major problem for Muslim exegetes of the Quran, who disagree on whether Satan is a fallen angel or the leader of a group of evil jinn. According to a hadith from Ibn Abbas, Iblis was actually an angel whom God created out of fire. Ibn Abbas asserts that the word \"jinn\" could be applied to earthly jinn, but also to \"fiery angels\" like Satan.\nHasan of Basra, an eminent Muslim theologian who lived in the seventh century AD, was quoted as saying: \"Iblis was not an angel even for the time of an eye wink. He is the origin of Jinn as Adam is of Mankind.\" The medieval Persian scholar Abu al-Zamakhshari states that the words \"angels\" and \"jinn\" are synonyms. Another Persian scholar, al-Baydawi, instead argues that Satan was an angel in essence, but behaved like the jinn. Abu Mansur al-Maturidi who is revered as the founder of \"M\u0101tur\u012bdiyya\" Sunni orthodoxy (\"kalam\") argued that since angels can be blessed by God, they are also put to a test and can be punished. Accordingly, Satan became a devil (\"shai\u1e6d\u0101n\") or jinn after he refused to obey. The \"Tarikh Khamis\" narrates that Satan was a jinn who was admitted into Paradise as a reward for his righteousness and, unlike the angels, was given the choice to obey or disobey God. When he was expelled from Paradise, Satan blamed humanity for his punishment. \nConcerning the fiery origin of Iblis, al-Baydawi asserts that fire and light are of the same substance but with different attributes and thus, there is no real difference. Similarly, the historian Zakariya al-Qazwini and also Muhammad ibn Ahmad Ibshihi state that there is no fundamental difference between light and fire, for all supernatural creatures were created from fire, but the angels from its light and the jinn from its blaze. Abd al-Ghani al-Maqdisi and Ibn Barrajan argued that only the angels of mercy are created from light, but angels of punishment have been created from fire.\nThe Muslim historian and theologian Al-Tabari, who died in around 923 AD, writes that, before Adam was created, earthly jinn made of smokeless fire roamed the earth and spread corruption. He further relates that Iblis was originally an angel named \"Azazil\" or \"Al-Harith\", from a group of angels, created from the \"fires of simoom\", sent by God to confront the earthly jinn. Azazil defeated the jinn in battle and drove them into the mountains, but he became convinced that he was superior to humans and all the other angels, leading to his downfall. In this account, Azazil's group of angels were called \"jinn\" because they guarded Jannah (Paradise). In another tradition recorded by Al-Tabari, Satan was one of the earthly jinn, who was taken captive by the angels and brought to Heaven as a prisoner. God appointed him as judge over the other jinn and he became known as \"Al-Hakam\". He fulfilled his duty for a thousand years before growing negligent, but was rehabilitated again and resumed his position until his refusal to bow before Adam.\nOther traditions.\nDuring the first two centuries of Islam, Muslims almost unanimously accepted the traditional story known as the Satanic Verses as true. According to this narrative, Muhammad was told by Satan to add words to the Quran which would allow Muslims to pray for the intercession of pagan goddesses. He mistook the words of Satan for divine inspiration. Modern Muslims almost universally reject this story as heretical, as it calls the integrity of the Quran into question.\nOn the third day of the Hajj, Muslim pilgrims to Mecca throw seven stones at a pillar known as the \"Jamrah al-\u2019Aqabah\", symbolizing the stoning of the Devil. This ritual is based on the Islamic tradition that, when God ordered Abraham to sacrifice his son Ishmael, Satan tempted him three times not to do it, and, each time, Abraham responded by throwing seven stones at him.\nThe hadith teach that newborn babies cry because Satan touches them while they are being born, and that this touch causes people to have an aptitude for sin. This doctrine bears some similarities to the doctrine of original sin. Muslim tradition holds that only Jesus and Mary were not touched by Satan at birth. However, when he was a boy, Muhammad's heart was literally opened by an angel, who removed a black clot that symbolized sin.\nMuslim tradition preserves a number of stories involving dialogues between Jesus and Iblis, all of which are intended to demonstrate Jesus's virtue and Satan's depravity. Ahmad ibn Hanbal records an Islamic retelling of Jesus's temptation by Satan in the desert from the Synoptic Gospels. Ahmad quotes Jesus as saying, \"The greatest sin is love of the world. Women are the ropes of Satan. Wine is the key to every evil.\" Abu Uthman al-Jahiz credits Jesus with saying, \"The world is Satan's farm, and its people are his plowmen.\" Al-Ghazali tells an anecdote about how Jesus went out one day and saw Satan carrying ashes and honey; when he asked what they were for, Satan replied, \"The honey I put on the lips of backbiters so that they achieve their aim. The ashes I put on the faces of orphans, so that people come to dislike them.\" The thirteenth-century scholar Sibt ibn al-Jawzi states that, when Jesus asked him what truly broke his back, Satan replied, \"The neighing of horses in the cause of Allah.\"\nMuslims believe that Satan is also the cause of deceptions originating from the mind and desires for evil. He is regarded as a cosmic force for separation, despair and spiritual envelopment. Muslims do distinguish between the satanic temptations and the murmurings of the bodily lower self (\"nafs\"). The lower self commands the person to do a specific task or to fulfill a specific desire; whereas the inspirations of Satan tempt the person to do evil in general and, after a person successfully resists his first suggestion, Satan returns with new ones. If a Muslim feels that Satan is inciting him to sin, he is advised to seek refuge with God by reciting: \"In the name of Allah, I seek refuge in you, from Satan the outcast.\" Muslims are also obliged to \"seek refuge\" before reciting the Quran.\nIslamic mysticism.\nAccording to some adherents of Sufi mysticism, Iblis refused to bow to Adam because he was fully devoted to God alone and refused to bow to anyone else. For this reason, Sufi masters regard Satan and Muhammad as the two most perfect monotheists. Sufis reject the concept of dualism and instead believe in the unity of existence. In the same way that Muhammad was the instrument of God's mercy, Sufis regard Satan as the instrument of God's wrath. For the Muslim Sufi scholar Ahmad Ghazali, Iblis was the paragon of lovers in self-sacrifice for refusing to bow down to Adam out of pure devotion to God. Ahmad Ghazali's student Sheikh Adi ibn Musafir was among the Sunni Muslim mystics who defended Iblis and asserted that evil was also God's creation. Sheikh Adi argued that if evil existed without the will of God, then God would be powerless and powerlessness can't be attributed to God. Some Sufis assert that since Iblis was destined by God to become a devil, God will also restore him to his former angelic nature. Attar compares Iblis's damnation to the Biblical Benjamin: both were accused unjustly, but their punishment had a greater meaning. In the end, Iblis will be released from hell.\nHowever, not all Muslim Sufi mystics are in agreement with a positive depiction of Iblis. Rumi's viewpoint on Iblis is much more in tune with Islamic orthodoxy. Rumi views Iblis as the manifestation of the great sins of haughtiness and envy. He states: \"(Cunning) intelligence is from Iblis, and love from Adam.\"\nBah\u00e1\u02bc\u00ed Faith.\nIn the Bah\u00e1\u02bc\u00ed Faith, Satan is not regarded as an independent evil power as he is in some faiths, but signifies the \"lower nature\" of humans. `Abdu'l-Bah\u00e1 explains: \"This lower nature in man is symbolized as Satan\u2014the evil ego within us, not an evil personality outside.\" All other evil spirits described in various faith traditions\u2014such as fallen angels, demons, and jinns\u2014are also metaphors for the base character traits a human being may acquire and manifest when he turns away from God. Actions that are described as \"satanic\" in some Bah\u00e1\u02bc\u00ed writings denote humans' deeds caused by selfish desires.\nSatanism.\nNontheistic Satanism.\nNontheistic Satanism, as exemplified by LaVeyan Satanism (practiced by the Church of Satan and First Satanic Church) and The Satanic Temple, holds that Satan does not exist as a literal anthropomorphic entity, but rather as a symbol of a cosmos which Satanists perceive to be permeated and motivated by a force that has been given many names by humans over the course of time. In this religion, \"Satan\" is not viewed or depicted as a hubristic, irrational, and fraudulent creature, but rather is revered with Prometheus-like attributes, symbolizing liberty and individual empowerment. To adherents, he also serves as a conceptual framework and an external metaphorical projection of the Satanist's highest personal potential. In his essay \"Satanism: The Feared Religion\", the current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that \"...Satan is a symbol of Man living as his prideful, carnal nature dictates. The reality behind Satan is simply the dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things. Satan is not a conscious entity to be worshiped, rather a reservoir of power inside each human to be tapped at will\".\nLaVeyan Satanists embrace the original etymological meaning of the word \"Satan\" (Hebrew: \u05e9\u05b8\u05bc\u05c2\u05d8\u05b8\u05df \"satan\", meaning \"adversary\"). According to Gilmore, \"The Church of Satan has chosen Satan as its primary symbol because in Hebrew it means adversary, opposer, one to accuse or question. We see ourselves as being these Satans; the adversaries, opposers and accusers of all spiritual belief systems that would try to hamper enjoyment of our life as a human being.\"\nPost-LaVeyan Satanists, like the adherents of The Satanic Temple, argue that the human animal has a natural altruistic and communal tendency, and frame Satan as a figure of struggle against injustice and activism. They also believe in bodily autonomy, that personal beliefs should conform to science and inspire nobility, and that people should atone for their mistakes.\nTheistic Satanism.\nTheistic Satanism, otherwise referred to as spiritual Satanism, or devil worship, views Satan as a deity, whom individuals may supplicate to. It consists of loosely affiliated or independent groups and cabals, which all agree that Satan is a real entity.\nAllegations of worship.\nThe main deity in the tentatively Indo-European pantheon of the Yazidis, Melek Taus, is similar to the devil in Islamic and Christian traditions, as he refused to bow down before humanity. Therefore, Muslims and Christians often consider Melek Taus to be Satan. However, rather than being Satanic, Yazidism can be understood as a remnant of a pre-Islamic Middle Eastern Indo-European religion, and/or a ghulat Sufi movement founded by Shaykh Adi. In fact, there is no entity in Yazidism which represents evil in opposition to God; such dualism is rejected by Yazidis.\nIn the Middle Ages, the Cathars, practitioners of a dualistic religion, were accused of worshipping Satan by the Catholic Church. Pope Gregory IX stated in his work \"Vox in Rama\" that the Cathars believed that God had erred in casting Lucifer out of heaven and that Lucifer would return to reward his faithful. On the other hand, according to Catharism, the creator god of the material world worshipped by the Catholic Church is actually Satan.\nWicca is a modern, syncretic Neopagan religion whose practitioners many Christians have incorrectly assumed to worship Satan. In actuality, Wiccans do not believe in the existence of Satan or any analogous figure and have repeatedly and emphatically rejected the notion that they venerate such an entity. \nThe cult of the skeletal figure of Santa Muerte, which has grown exponentially in Mexico, has been denounced by the Catholic Church as devil worship. However, devotees of Santa Muerte view her as an angel of death created by God, and many of them identify as Catholic.\nMuch modern folklore about Satanism does not originate from the actual beliefs or practices of theistic Satanists, but rather from a mixture of medieval Christian folk beliefs, political or sociological conspiracy theories, and contemporary urban legends. An example is the \"Satanic ritual abuse\" scare of the 1980s\u2014beginning with the memoir \"Michelle Remembers\"\u2014which depicted Satanism as a vast conspiracy of elites with a predilection for child abuse and human sacrifice. This genre frequently describes Satan as physically incarnating in order to receive worship.\nIn culture.\nIn visual art.\nSatan's appearance is not described in the Bible or in early Christian writings, though Paul the Apostle does write that \"Satan disguises himself as an angel of light\" (). Satan was never shown in early Christian artwork and may have first appeared in the sixth century in one of the mosaics of the Basilica of Sant'Apollinare Nuovo. The mosaic \"Christ the Good Sheppard\" features a blue-violet angel at the left hand side of Christ behind three goats.\nDepictions of Satan became more common in the ninth century, where he is shown with cloven hooves, hairy legs, the tail of a goat, pointed ears, a beard, a flat nose, and a set of horns. Satan may have first become associated with goats through the Parable of the Sheep and the Goats, recorded in , in which Jesus separates sheep (representing the saved) from goats (representing the damned); the damned are thrown into an \"everlasting fire\" along with Satan and his angels.\nMedieval Christians were known to adapt previously existing pagan iconography to suit depictions of Christian figures. Much of Satan's traditional iconography in Christianity appears to be derived from Pan, a rustic, goat-legged fertility god in ancient Greek religion. Early Christian writers such as Saint Jerome equated the Greek satyrs and the Roman fauns, whom Pan resembled, with demons. Satan's pitchfork appears to have been adapted from the trident wielded by the Greek god Poseidon and Satan's flame-like hair seems to have originated from the Egyptian god Bes. By the High Middle Ages, Satan and devils appear in all works of Christian art: in paintings, sculptures, and on cathedrals. Satan is usually depicted naked, but his genitals are rarely shown and are often covered by animal furs. The goat-like portrayal of Satan was especially closely associated with him in his role as the object of worship by sorcerers and as the incubus, a demon believed to rape human women in their sleep.\nItalian frescoes from the late Middle Ages onward frequently show Satan chained in Hell, feeding on the bodies of the perpetually damned. These frescoes are early enough to have inspired Dante's portrayal in his \"Inferno\". As the serpent in the Garden of Eden, Satan is often shown as a snake with arms and legs as well as the head and full-breasted upper torso of a woman. Satan and his demons could take any form in medieval art, but, when appearing in their true form, they were often shown as short, hairy, black-skinned humanoids with clawed and bird feet and extra faces on their chests, bellies, genitals, buttocks, and tails. The modern popular culture image of Satan as a well-dressed gentleman with small horns and a tail originates from portrayals of Mephistopheles in the operas \"La damnation de Faust\" (1846) by Hector Berlioz, \"Mefistofele\" (1868) by Arrigo Boito, and \"Faust\" by Charles Gounod.\nIllustrations of Satan in Islamic paintings often depict him black-faced, a feature which would later symbolize any satanic figure or heretic, and with a black body, to symbolize his corrupted nature. Another common depiction of Iblis shows him wearing a special head covering, clearly different from the traditional Islamic turban. In one painting, however, Iblis wears a traditional Islamic head covering. The turban probably refers to a narration of Iblis' fall: there he wore a turban, then he was sent down from heaven. Many other pictures show and describe Iblis at the moment when the angels prostrate themselves before Adam. Here, he is usually seen beyond the outcrop, his face transformed, with his wings burned, to the envious countenance of a devil. Iblis and his cohorts (\"div\" or \"shayatin\") are often portrayed in Turko-Persian art as bangled creatures with flaming eyes, only covered by a short skirt. Similar to European artists, who took traits of pagan deities to depict devils, they depicted such demons often in a similar fashion to that of Hindu deities.\nIn literature.\n&lt;templatestyles src=\"Rquote/styles.css\"/&gt;{ class=\"rquote pullquote floatright\" role=\"presentation\" style=\"display:table; border-collapse:collapse; border-style:none; float:right; margin:0.5em 0.75em; width:33%; \"\n&lt;templatestyles src=\"Rquote/styles.css\"/&gt;{ class=\"rquote pullquote floatright\" role=\"presentation\" style=\"display:table; border-collapse:collapse; border-style:none; float:right; margin:0.5em 0.75em; width:33%; \"\nIn Dante Alighieri's \"Inferno\", Satan appears as a giant demon, frozen mid-breast in ice at the center of the Ninth Circle of Hell. Satan has three faces and a pair of bat-like wings affixed under each chin. In his three mouths, Satan gnaws on Brutus, Judas Iscariot, and Cassius, whom Dante regarded as having betrayed the \"two greatest heroes of the human race\": Julius Caesar, the founder of the new order of government, and Jesus, the founder of the new order of religion. As Satan beats his wings, he creates a cold wind that continues to freeze the ice surrounding him and the other sinners in the Ninth Circle. Dante and Virgil climb up Satan's shaggy legs until gravity is reversed and they fall through the earth into the southern hemisphere.\nSatan appears in several stories from \"The Canterbury Tales\" by Geoffrey Chaucer, including \"The Summoner's Prologue\", in which a friar arrives in Hell and sees no other friars, but is told there are millions. Then Satan lifts his tail to reveal that all of the friars live inside his anus. Chaucer's description of Satan's appearance is clearly based on Dante's. The legend of Faust, recorded in the 1589 chapbook \"The History of the Damnable Life and the Deserved Death of Doctor John Faustus\", concerns a pact allegedly made by the German scholar Johann Georg Faust with a demon named Mephistopheles, agreeing to sell his soul to Satan in exchange for twenty-four years of earthly pleasure. This chapbook became the source for Christopher Marlowe's \"The Tragical History of the Life and Death of Doctor Faustus\".\nJohn Milton's epic poem \"Paradise Lost\" features Satan as its main protagonist. Milton portrays Satan as a tragic antihero destroyed by his own hubris. The poem, which draws extensive inspiration from Greek tragedy, recreates Satan as a complex literary character, who dares to rebel against the \"tyranny\" of God in spite of God's own omnipotence. The English poet and painter William Blake famously quipped that \"The reason Milton wrote in fetters when he wrote of Angels &amp; God, and at liberty when of Devils &amp; Hell, is because he was a true poet and of the Devils party without knowing it.\" \"Paradise Regained\", the sequel to \"Paradise Lost\", is a retelling of Satan's temptation of Jesus in the desert.\nWilliam Blake regarded Satan as a model of rebellion against unjust authority and featured him in many of his poems and illustrations, including his 1780 book \"The Marriage of Heaven and Hell\", in which Satan is celebrated as the ultimate rebel, the incarnation of human emotion and the epitome of freedom from all forms of reason and orthodoxy. Based on the Biblical passages portraying Satan as the accuser of sin, Blake interpreted Satan as \"a promulgator of moral laws\".\nIn music.\nReferences to Satan in music can be dated back to the Middle Ages. Giuseppe Tartini was inspired to write his most famous work, the Violin Sonata in G minor, also known as \"The Devil's Trill\", after dreaming of the Devil playing the violin. Tartini claimed that the sonata was a lesser imitation of what the Devil had played in his dream. Niccol\u00f2 Paganini was believed to have derived his musical talent from a deal with the Devil. Charles Gounod's \"Faust\" features a narrative that involves Satan.\nIn the early 1900s, jazz and blues became known as the \"Devil's Music\" as they were considered \"dangerous and unholy\". According to legend, blues musician Tommy Johnson was a terrible guitarist before exchanging his soul to the Devil for a guitar. Later, Robert Johnson claimed that he had sold his soul in return for becoming a great blues guitarist. Satanic symbolism appears in rock music from the 1960s. Mick Jagger assumes the role of Lucifer in the Rolling Stones' \"Sympathy for the Devil\" (1968), while Black Sabbath portrayed the Devil in numerous songs, including \"War Pigs\" (1970) and \"N.I.B.\" (1970).\nIn film and television.\nThe Devil is depicted as a vampire bat in Georges M\u00e9li\u00e8s' \"The Haunted Castle\" (1896), which is often considered the first horror film. So-called \"Black Masses\" have been portrayed in sensationalist B-movies since the 1960s. One of the first films to portray such a ritual was the 1965 film \"Eye of the Devil\", also known as \"13\". Alex Sanders, a former black magician, served as a consultant on the film to ensure that the rituals portrayed in it were depicted accurately. Over the next thirty years, the novels of Dennis Wheatley and the films of Hammer Film Productions both played a major role in shaping the popular image of Satanism.\nThe film version of Ira Levin's \"Rosemary's Baby\" made Satanic themes a staple of mainstream horror fiction. Later films such as \"The Exorcist\" (1973), \"The Omen\" (1976), \"Angel Heart\" (1987) and \"The Devil's Advocate\" (1997) feature Satan as an antagonist. The Turkish horror film \"Semum\" (2008) is based the representation of Satan in Islamic scriptures.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "27695", "revid": "45320532", "url": "https://en.wikipedia.org/wiki?curid=27695", "title": "Structured programming", "text": "Programming paradigm based on block-based control flow\nStructured programming is a programming paradigm characterized by source code that uses block-based source code structure to encode control flow such as sequence, selection (i.e. if-then-else and switch) and iteration (i.e. for and while). \nOriginally, the central goal of the structured programming movement was to eliminate the need for and use of the goto statement. As goto provides powerful and flexible flow control, it can be used to write any arbitrarily complex algorithm, but the resulting code often has significant quality issues, commonly described as spaghetti code. Structured programming replaces goto with constructs that tend to result in better code. The paradigm became popular and for the most part achieved the goal of supplanting goto. In fact, its ubiquity is so thorough that for much of software development, it is simply the way code is written, no longer a topic of discussion as it once was.\nStructured programming is sometimes associated with modular programming even though they are different. In a general sense, \"structured\" implies a sense of modularity and of being written to be efficient and easy to understand and modify, but this is not what structured programming means in a narrow sense.\nAfter structured programming became popular, the style of programming that preceded it was retroactively called non-structured programming. Although technically a programming paradigm, it differs from other paradigms in that it was not intentionally designed. It was simply the state-of-the-art before structured programming was envisioned.\nHistory.\nThe paradigm emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages, with the latter including support for block structures. \nContributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the publication of what is now known as the structured program theorem in 1966, and the publication of the influential \"Go To Statement Considered Harmful\" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term \"structured programming\".\nTheoretical foundation.\nThe structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs\u2014sequencing, selection, and iteration\u2014are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a \"structured program\" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by B\u00f6hm and Jacopini, possibly because Dijkstra cited this paper himself. The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.\nDebate.\nP. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Us converts waved this interesting bit of news under the noses of the unreconstructed assembly-language programmers who kept trotting forth twisty bits of logic and saying, 'I betcha can't structure this.' Neither the proof by B\u00f6hm and Jacopini nor our repeated successes at writing structured code brought them around one day sooner than they were ready to convince themselves.\nDonald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed with abolishing the GOTO statement, and as of 2018[ [update]] has continued to use it in his programs. In his 1974 paper, \"Structured Programming with Goto Statements\", he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Some of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs.\nStructured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for \"The New York Times\" research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.\nAs late as 1987, it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled \"'GOTO Considered Harmful' Considered Harmful\". Multiple objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.\nOutcome.\nBy the end of the 20th century, nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.\nControl structures.\nFollowing the structured program theorem, a program is composed of three control structures:\nAlthough not part of the structured program theorem, languages generally include a block concept that groups a sequence of code so that it acts much like a single statement. A language includes a way to mark a sequence of statements as a block that, unless it contains flow control, will be executed sequentially, top-to-bottom. For example, a block is enclosed in curly braces codice_1 in C and other curly-brace languages, enclosed in codice_2 in PL/I and Pascal, and indicated via indentation in Python. Some blocks use different syntax for each structure. For example, an if-statement is enclosed in codice_3 in ALGOL 68.\nLanguage support.\nGenerally, a language is intended to support one or more programming paradigms. At the same time, even though a language is not intended to support a paradigm, it often can be used in that way regardless. In theory, any language can be used for structured programming. Some of the languages initially used for structured programming include ALGOL, Pascal, PL/I, Ada, and RPL, but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features \u2013 notably GOTO \u2013 to avoid its pitfalls.\nCommon deviations.\nWhile the use of goto has largely been replaced by structured constructs, most languages provide features that are not strictly consistent with the structured programming theorem. \nEarly return.\nMany languages provide a return statement that allows for multiple, early return exit points from a function. As a function is a block, this feature is counter to the single exit point described in the theorem.\nEarly exit.\nMany languages provide for early exit from a block (other than via return). For example, a loop construct may support a break statement that exits the loop block before its end. As the theorem describes, such early-exit logic can be eliminated by adding branches or tests, but this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have \"labeled breaks\", which allow breaking out of more than just the innermost loop.\nThe need for multiple exits can arise for a variety of reasons, most often either that the function has no more work to do (if returning a value, it has completed the calculation), or has encountered \"exceptional\" circumstances that prevent it from continuing, hence needing exception handling.\nA problem with early exit is that cleanup statements might not be executed. For example, allocated memory is not deallocated, or open files are not closed, causing memory leak and resource leak. Cleanup must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action that should be performed at the end of a function (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as Pascal, Lisp, and OCaml, do not have this problem.\nMost modern languages provide language-level support to prevent such leaks (see resource management). As a structured alternative to using goto and a cleanup block, unwind protection ensures that certain code is runs when execution exits a block; often implemented in connection with exception handling as a try-finally. An alternative approach, in C++, is resource acquisition is initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.\nKent Beck, Martin Fowler, and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that \"one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don\u2019t\". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors). Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single exit point is an obsolete requirement.\nIn his 2004 textbook, David Watt writes that \"single-entry multi-exit control flows are often desirable\". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as \"escape sequencers\", defined as a \"sequencer that terminates execution of a textually enclosing command or procedure\", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce \"spaghetti code\". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.\nIn contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like break and continue \"are just the old goto in sheep's clothing\" and strongly advised against their use.\nException handling.\nPeter Ritchie notes that, in principle, even a single throw right before the return constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions that wrap exceptions for the sake of creating a single exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages that support exceptions of engaging in cargo-cult thinking.\nDavid Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic overflows or input/output failures like file not found) is a kind of error that \"is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit\". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where \"the application code tends to get cluttered by tests of status flags\" and that \"the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!\" He notes that in contrast to status-flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) are not as suitable as a dedicated exception sequencer with the semantics discussed above.\nThe textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like while loops because the transfer of control \"is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred.\" Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like for, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if codice_8 throws an exception in codice_9, then the usual exit point after check() is not reached. Citing multiple prior studies by others (1999\u20132004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they \"create hidden control-flow paths that are difficult for programmers to reason about\".\nThe necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like codice_10, do not allow early exit from inside to the outside of the parallel construct; this restriction includes all manner of exits, including break and exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside.\nMultiple entry.\nRelatively rarely, functions allow multiple \"entry.\" This is most commonly only \"re\"-entry into a coroutine (or generator/semicoroutine), where a function yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code-execution point of view, yielding from a coroutine is closer to structured programming than returning from a function, as the function has not actually terminated, and will continue when called again \u2013 it is not an early exit. However, coroutines mean that multiple functions have execution state \u2013 rather than a single call stack of functions \u2013 and thus introduce a different form of complexity.\nIt is rare for functions to allow entry to an arbitrary position in the function, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is similar to a goto.\nState machines.\nSome programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.\nHowever, it is possible to structure these systems by making each state-change a separate function and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "27696", "revid": "18779361", "url": "https://en.wikipedia.org/wiki?curid=27696", "title": "Semiconductor device fabrication", "text": "Manufacturing process used to create integrated circuits\nSemiconductor device fabrication is the process used to manufacture semiconductor devices, typically integrated circuits (ICs) such as microprocessors, microcontrollers, and memories (such as RAM and flash memory). It is a multiple-step photolithographic and physico-chemical process (with steps such as thermal oxidation, thin-film deposition, ion implantation, etching) during which electronic circuits are gradually created on a wafer, typically made of pure single-crystal semiconducting material. Silicon is almost always used, but various compound semiconductors are used for specialized applications. Steps such as etching and photolithography can be used to manufacture other devices, such as LCD and OLED displays.\nThe fabrication process is performed in highly specialized semiconductor fabrication plants, also called foundries or \"fabs\", with the central part being the \"clean room\". In more advanced semiconductor devices, such as modern 14/10/7\u00a0nm nodes, fabrication can take up to 15 weeks, with 11\u201313 weeks being the industry average. Production in advanced fabrication facilities is completely automated, with automated material handling systems taking care of the transport of wafers from machine to machine.\nA wafer often has several integrated circuits, which are called dies as they are pieces diced from a single wafer. Individual dies are separated from a finished wafer in a process called die singulation, also called wafer dicing. The dies can then undergo further assembly and packaging.\nWithin fabrication plants, the wafers are transported inside special sealed plastic boxes called FOUPs. FOUPs in many fabs contain an internal nitrogen atmosphere which helps prevent copper from oxidizing on the wafers. Copper is used in modern semiconductors for wiring. The insides of the processing equipment and FOUPs is kept cleaner than the surrounding air in the cleanroom. This internal atmosphere is known as a mini-environment and helps improve yield, which is the number of working devices on a wafer. This mini environment is within an EFEM (equipment front end module) which allows a machine to receive FOUPs, and introduces wafers from the FOUPs into the machine. Additionally, many machines also handle wafers in clean nitrogen or vacuum environments to reduce contamination and improve process control. Fabrication plants need large amounts of liquid nitrogen to maintain the atmosphere inside production machinery and FOUPs, which are constantly purged with nitrogen. There can also be an air curtain or a mesh between the FOUP and the EFEM which helps reduce the amount of humidity that enters the FOUP and improves yield. \nSome of the companies that manufacture machines used in the industrial semiconductor fabrication process include ASML, Applied Materials, Tokyo Electron, and Lam Research.\nFeature size.\nFeature size (or process size) is determined by the width of the smallest lines that can be patterned in a semiconductor fabrication process; this measurement is known as the linewidth. Patterning often refers to photolithography which allows a device design or pattern to be defined on the device during fabrication. F2 is used as a measurement of area for different parts of a semiconductor device, based on the feature size of a semiconductor manufacturing process. Many semiconductor devices are designed in sections called cells, and each cell represents a small part of the device, such as a memory cell to store data. Thus F2 is used to measure the area taken up by these cells or sections.\nA specific semiconductor process has specific rules on the minimum size (width or CD/Critical Dimension) and spacing for features on each layer of the chip.\nNormally, a new semiconductor process has smaller minimum sizes and tighter spacing. In some cases, this allows a simple die shrink of a currently produced chip design to reduce costs, improve performance, and increase transistor density (number of transistors per unit area) without the expense of a new design.\nEarly semiconductor processes had arbitrary names for generations (viz., HMOS I/II/III/IV and CHMOS III/III-E/IV/V). Later each new generation process became known as a technology node or process node, designated by the process' minimum feature size in nanometers (or historically micrometers) of the process's transistor gate length, such as the \"90 nm process\". However, this has not been the case since 1994, and the number of nanometers used to name process nodes (see the International Technology Roadmap for Semiconductors) has become more of a marketing term that has no standardized relation with functional feature sizes or with transistor density (number of transistors per unit area).\nInitially transistor gate length was smaller than that suggested by the process node name (e.g. 350\u00a0nm node); however, this trend reversed in 2009. Feature sizes can have no connection to the nanometers (nm) used in marketing.\nFor example, Intel's former 10 nm process actually has features (the tips of FinFET fins) with a width of 7\u00a0nm, so the Intel 10\u00a0nm process is similar in transistor density to TSMC's 7 nm process. As another example, GlobalFoundries' 12 and 14\u00a0nm processes have similar feature sizes.\nHistory.\n20th century.\nIn 1955, Carl Frosch and Lincoln Derick, working at Bell Telephone Laboratories, accidentally grew a layer of silicon dioxide over the silicon wafer, for which they observed surface passivation effects. By 1957 Frosch and Derick, using masking and predeposition, were able to manufacture silicon dioxide transistors; the first planar field effect transistors, in which drain and source were adjacent at the same surface. At Bell Labs, the importance of their discoveries was immediately realized. Memos describing the results of their work circulated at Bell Labs before being formally published in 1957. At Shockley Semiconductor, Shockley had circulated the preprint of their article in December 1956 to all his senior staff, including Jean Hoerni, who would later invent the planar process in 1959 while at Fairchild Semiconductor.\nIn 1948, Bardeen patented an insulated-gate transistor (IGFET) with an inversion layer; Bardeen's concept forms the basis of MOSFET technology today. An improved type of MOSFET technology, CMOS, was developed by Chih-Tang Sah and Frank Wanlass at Fairchild Semiconductor in 1963. CMOS was commercialised by RCA in the late 1960s. RCA commercially used CMOS for its 4000-series integrated circuits in 1968, starting with a 20\u03bcm process before gradually scaling to a 10\u00a0\u03bcm process over the next several years. Many early semiconductor device manufacturers developed and built their own equipment such as ion implanters.\nIn 1963, Harold M. Manasevit was the first to document epitaxial growth of silicon on sapphire while working at the Autonetics division of North American Aviation (now Boeing). In 1964, he published his findings with colleague William Simpson in the \"Journal of Applied Physics\". In 1965, C.W. Mueller and P.H. Robinson fabricated a MOSFET (metal\u2013oxide\u2013semiconductor field-effect transistor) using the silicon-on-sapphire process at RCA Laboratories.\nSemiconductor device manufacturing has since spread from Texas and California in the 1960s to the rest of the world, including Asia, Europe, and the Middle East. \nWafer size has grown over time, from 25\u00a0mm (1 inch) in 1960, to 50\u00a0mm (2 inches) in 1969, 100\u00a0mm (4 inches) in 1976, 125\u00a0mm (5 inches) in 1981, 150\u00a0mm (6 inches) in 1983 and 200\u00a0mm in 1992.\nIn the era of 2-inch wafers, these were handled manually using tweezers and held manually for the time required for a given process. Tweezers were replaced by vacuum wands as they generate fewer particles which can contaminate the wafers. Wafer carriers or cassettes, which can hold several wafers at once, were developed to carry several wafers between process steps, but wafers had to be individually removed from the carrier, processed, and returned to the carrier, so acid-resistant carriers were developed to eliminate this time consuming process, so the entire cassette with wafers was dipped into wet etching and wet cleaning tanks. When wafer sizes increased to 100\u00a0mm, the entire cassette would often not be dipped as uniformly, and the quality of the results across the wafer became hard to control. By the time 150\u00a0mm wafers arrived, the cassettes were not dipped and were only used as wafer carriers and holders to store wafers, and robotics became prevalent for handling wafers. With 200\u00a0mm wafers, manual handling of wafer cassettes becomes risky as they are heavier.\nIn the 1970s and 1980s, several companies migrated their semiconductor manufacturing technology from bipolar to MOSFET technology. Semiconductor manufacturing equipment has been considered costly since 1978.\nIn 1984, KLA developed the first automatic reticle and photomask inspection tool. In 1985, KLA developed an automatic inspection tool for silicon wafers, which replaced manual microscope inspection.\nIn 1985, SGS (now STmicroelectronics) invented BCD, also called BCDMOS, a semiconductor manufacturing process using bipolar, CMOS and DMOS devices. Applied Materials developed the first practical multi-chamber, or cluster wafer processing tool, the Precision 5000.\nUntil the 1980s, physical vapor deposition was the primary technique used for depositing materials onto wafers, until the advent of chemical vapor deposition. Equipment with diffusion pumps was replaced with those using turbomolecular pumps, as the latter do not use oil, which often contaminates wafers during processing in vacuum.\n200\u00a0mm diameter wafers were first used in 1990 and became the standard until the introduction of 300\u00a0mm diameter wafers in 2000. Bridge tools were used in the transition from 150\u00a0mm wafers to 200\u00a0mm wafers and in the transition from 200\u00a0mm to 300\u00a0mm wafers. The semiconductor industry has adopted larger wafers to cope with the increased demand for chips as larger wafers provide more surface area per wafer. Over time, the industry shifted to 300\u00a0mm wafers which brought along the adoption of FOUPs, but many products that are not advanced are still produced in 200\u00a0mm wafers such as analog ICs, RF chips, power ICs, BCDMOS and MEMS devices. \nSome processes such as cleaning, ion implantation, etching, annealing and oxidation started to adopt single wafer processing instead of batch wafer processing to improve the reproducibility of results. A similar trend existed in MEMS manufacturing. In 1998, Applied Materials introduced the Producer, a cluster tool that had chambers grouped in pairs for processing wafers, which shared common vacuum and supply lines but were otherwise isolated, which was revolutionary at the time as it offered higher productivity than other cluster tools without sacrificing quality, due to the isolated chamber design.\n21st century.\nThe semiconductor industry is a global business today. The leading semiconductor manufacturers typically have facilities all over the world. Samsung Electronics, the world's largest manufacturer of semiconductors, has facilities in South Korea and the US. Intel, the second-largest manufacturer, has facilities in Europe and Asia as well as the US. TSMC, the world's largest pure-play foundry, has facilities in Taiwan, China, Singapore, and the US. Qualcomm and Broadcom are among the biggest fabless semiconductor companies, outsourcing their production to companies like TSMC. They also have facilities spread in different countries. As the average utilization of semiconductor devices increased, durability became an issue and manufacturers started to design their devices to ensure they last for enough time, and this depends on the market the device is designed for. This especially became a problem at the 10\u00a0nm node.\nSilicon on insulator (SOI) technology has been used in AMD's 130\u00a0nm, 90\u00a0nm, 65\u00a0nm, 45\u00a0nm and 32\u00a0nm single, dual, quad, six and eight core processors made since 2001. During the transition from 200\u00a0mm to 300\u00a0mm wafers in 2001, many bridge tools were used which could process both 200\u00a0mm and 300\u00a0mm wafers. At the time, 18 companies could manufacture chips in the leading edge 130nm process.\nIn 2006, 450\u00a0mm wafers were expected to be adopted in 2012, and 675\u00a0mm wafers were expected to be used by 2021.\nSince 2009, \"node\" has become a commercial name for marketing purposes that indicates new generations of process technologies, without any relation to gate length, metal pitch or gate pitch. For example, GlobalFoundries' 7\u00a0nm process was similar to Intel's 10 nm process, thus the conventional notion of a process node has become blurred. Additionally, TSMC and Samsung's 10\u00a0nm processes are only slightly denser than Intel's 14\u00a0nm in transistor density. They are actually much closer to Intel's 14\u00a0nm process than they are to Intel's 10\u00a0nm process (e.g. Samsung's 10\u00a0nm processes' fin pitch is the same as that of Intel's 14\u00a0nm process: 42\u00a0nm). Intel has changed the name of its 10\u00a0nm process to position it as a 7\u00a0nm process. As transistors become smaller, new effects start to influence design decisions, such as self-heating of the transistors, and other effects, such as electromigration, have become more evident since the 16nm node.\nIn 2011, Intel demonstrated Fin field-effect transistors (FinFETs), where the gate surrounds the channel on three sides, allowing for increased energy efficiency and lower gate delay\u2014and thus greater performance\u2014over planar transistors at the 22nm node, because planar transistors which only have one surface acting as a channel, started to suffer from short channel effects. A startup called SuVolta created a technology called Deeply Depleted Channel (DDC) to compete with FinFET transistors; it uses very lightly doped planar transistors at the 65\u00a0nm node.\nBy 2018, a number of transistor architectures had been proposed for the eventual replacement of FinFET, most of which were based on the concept of GAAFET: horizontal and vertical nanowires, horizontal nanosheet transistors (Samsung MBCFET, Intel Nanoribbon), vertical FET (VFET) and other vertical transistors, complementary FET (CFET), stacked FET, vertical TFETs, FinFETs with III-V semiconductor materials (III-V FinFET), several kinds of horizontal gate-all-around transistors such as nano-ring, hexagonal wire, square wire, and round wire gate-all-around transistors and negative-capacitance FET (NC-FET) which uses drastically different materials. FD-SOI was seen as a potential low cost alternative to FinFETs.\nAs of 2019, 14 nanometer and 10 nanometer chips are in mass production by Intel, UMC, TSMC, Samsung, Micron, SK Hynix, Toshiba Memory and GlobalFoundries, with 7 nanometer process chips in mass production by TSMC and Samsung, although their 7 nanometer node definition is similar to Intel's 10 nanometer process. The 5 nanometer process began being produced by Samsung in 2018. As of 2019, the node with the highest transistor density is TSMC's 5nanometer N5 node, with a density of 171.3million transistors per square millimeter. In 2019, Samsung and TSMC announced plans to produce 3 nanometer nodes. GlobalFoundries has decided to stop the development of new nodes beyond 12 nanometers to save resources, as it has determined that setting up a new fab to handle sub-12\u00a0nm orders would be beyond the company's financial abilities.\nFrom 2020 to 2023, there was a global chip shortage. During this shortage caused by the COVID-19 pandemic, many semiconductor manufacturers banned employees from leaving company grounds. Many countries granted subsidies to semiconductor companies for building new fabrication plants or fabs. Many companies were affected by counterfeit chips. Semiconductors have become vital to the world economy and the national security of some countries. The US has asked TSMC to not produce semiconductors for Huawei, a Chinese company. CFET transistors were explored, which stacks NMOS and PMOS transistors on top of each other. Two approaches were evaluated for constructing these transistors: a monolithic approach which built both types of transistors in one process, and a sequential approach which built the two types of transistors separately and then stacked them.\nList of steps.\nThis is a list of processing techniques that are employed numerous times throughout the construction of a modern electronic device; this list does not necessarily imply a specific order, nor that all techniques are taken during manufacture as, in practice the order and which techniques are applied, are often specific to process offerings by foundries, or specific to an integrated device manufacturer (IDM) for their own products, and a semiconductor device might not need all techniques. Equipment for carrying out these processes is made by a handful of companies. All equipment needs to be tested before a semiconductor fabrication plant is started. These processes are done after integrated circuit design. A semiconductor fab operates 24/7 and many fabs use large amounts of water, primarily for rinsing the chips.\nAdditionally, steps such as Wright etch may be carried out.\nPrevention of contamination and defects.\nWhen feature widths were far greater than about 10 micrometres, semiconductor purity was not as big of an issue as it is today in device manufacturing. In the 1960s, workers could work on semiconductor devices in street clothing. As devices become more integrated, cleanrooms must become even cleaner. Today, fabrication plants are pressurized with filtered air to remove even the smallest particles, which could come to rest on the wafers and contribute to defects. The ceilings of semiconductor cleanrooms have fan filter units (FFUs) at regular intervals to constantly replace and filter the air in the cleanroom; semiconductor capital equipment may also have its own FFUs to clean air in the equipment's EFEM, which allows the equipment to receive wafers in FOUPs. The FFUs, combined with raised floors with grills, help ensure a laminar air flow, to ensure that particles are immediately brought down to the floor and do not stay suspended in the air due to turbulence. The workers in a semiconductor fabrication facility are required to wear cleanroom suits to protect the devices from contamination by humans. To increase yield, FOUPs and semiconductor capital equipment may have a mini environment with ISO class 1 level of dust, and FOUPs can have an even cleaner micro environment. FOUPs and SMIF pods isolate the wafers from the air in the cleanroom, increasing yield because they reduce the number of defects caused by dust particles. Also, fabs have as few people as possible in the cleanroom to make maintaining the cleanroom environment easier, since people, even when wearing cleanroom suits, shed large amounts of particles, especially when walking.\nWafers.\nA typical wafer is made out of extremely pure silicon that is grown into mono-crystalline cylindrical ingots (boules) up to 300\u00a0mm (slightly less than 12\u00a0inches) in diameter using the Czochralski process. These ingots are then sliced into wafers about 0.75\u00a0mm thick and polished to obtain a very regular and flat surface. During the production process wafers are often grouped into lots, which are represented by a FOUP, SMIF or a wafer cassette, which are wafer carriers. FOUPs and SMIFs can be transported in the fab between machines and equipment with an automated OHT (Overhead Hoist Transport) AMHS (Automated Material Handling System). Besides SMIFs and FOUPs, wafer cassettes can be placed in a wafer box or a wafer carrying box.\nProcessing.\nIn semiconductor device fabrication, the various processing steps fall into four general categories: deposition, removal, patterning, and modification of electrical properties.\nModification of electrical properties now also extends to the reduction of a material's dielectric constant in low-\u03ba insulators via exposure to ultraviolet light in UV processing (UVP). Modification is frequently achieved by oxidation, which can be carried out to create semiconductor-insulator junctions, such as in the local oxidation of silicon (LOCOS) to fabricate metal oxide field effect transistors. Modern chips have up to eleven or more metal levels produced in over 300 or more sequenced processing steps. \nA recipe in semiconductor manufacturing is a list of conditions under which a wafer will be processed by a particular machine in a processing step during manufacturing. Process variability is a challenge in semiconductor processing, in which wafers are not processed evenly or the quality or effectiveness of processes carried out on a wafer are not even across the wafer surface.\nFront-end-of-line (FEOL) processing.\nWafer processing is separated into FEOL and BEOL stages. FEOL processing refers to the formation of the transistors directly in the silicon. The raw wafer is engineered by the growth of an ultrapure, virtually defect-free silicon layer through epitaxy. In the most advanced logic devices, \"prior\" to the silicon epitaxy step, tricks are performed to improve the performance of the transistors to be built. One method involves introducing a \"straining step\" wherein a silicon variant such as silicon-germanium (SiGe) is deposited. Once the epitaxial silicon is deposited, the crystal lattice becomes stretched somewhat, resulting in improved electronic mobility. Another method, called \"silicon on insulator\" technology, involves the insertion of an insulating layer between the raw silicon wafer and the thin layer of subsequent silicon epitaxy. This method results in the creation of transistors with reduced parasitic effects. Semiconductor equipment may have several chambers that process wafers in processes such as deposition and etching. Many pieces of equipment handle wafers between these chambers in an internal nitrogen or vacuum environment to improve process control. Wet benches with tanks containing chemical solutions were historically used for cleaning and etching wafers.\nAt the 90nm node, transistor channels made with strain engineering were introduced to improve drive current in PMOS transistors by introducing regions with Silicon-Germanium in the transistor. The same was done in NMOS transistors at the 20nm node.\nIn 2007, HKMG (high-k/metal gate) transistors were introduced by Intel at the 45nm node, which replaced polysilicon gates which in turn replaced metal gate (aluminum gate) technology in the 1970s. High-k dielectric such as hafnium oxide (HfO2) replaced silicon oxynitride (SiON), to prevent large amounts of leakage current in the transistor while allowing for continued scaling or shrinking of the transistors. However, HfO2 is not compatible with polysilicon gates; it requires the use of a metal gate. Two approaches were used in production: gate-first and gate-last. Gate-first consists of depositing the high-k dielectric and then the gate metal such as Tantalum nitride whose workfunction depends on whether the transistor is NMOS or PMOS, polysilicon deposition, gate line patterning, source and drain ion implantation, dopant anneal, and silicidation of the polysilicon and the source and drain. In DRAM memories this technology was first adopted in 2015.\nGate-last consisted of first depositing the High-\u03ba dielectric, creating dummy gates, manufacturing sources and drains by ion deposition and dopant annealing, depositing an \"interlevel dielectric (ILD)\" and then polishing, and removing the dummy gates to replace them with a metal whose workfunction depended on whether the transistor was NMOS or PMOS, thus creating the metal gate. A third process, full silicidation (FUSI) was not pursued due to manufacturing problems. Gate-first became dominant at the 22nm/20nm node. HKMG has been extended from planar transistors for use in FinFET and nanosheet transistors. Hafnium silicon\noxynitride can also be used instead of Hafnium oxide. \nSince the 16nm/14nm node, Atomic layer etching (ALE) is increasingly used for etching as it offers higher precision than other etching methods. In production, plasma ALE is commonly used, which removes materials unidirectionally, creating structures with vertical walls. Thermal ALE can also be used to remove materials isotropically, in all directions at the same time but without the capability to create vertical walls. Plasma ALE was initially adopted for etching contacts in transistors, and since the 7nm node it is also used to create transistor structures by etching them.\nGate oxide and implants.\nFront-end surface engineering is followed by growth of the gate dielectric (traditionally silicon dioxide), patterning of the gate, patterning of the source and drain regions, and subsequent implantation or diffusion of dopants to obtain the desired complementary electrical properties. In dynamic random-access memory (DRAM) devices, storage capacitors are also fabricated at this time, typically stacked above the access transistor (the now defunct DRAM manufacturer Qimonda implemented these capacitors with trenches etched deep into the silicon surface).\nBack-end-of-line (BEOL) processing.\nMetal layers.\nOnce the various semiconductor devices have been created, they must be interconnected to form the desired electrical circuits. This occurs in a series of wafer processing steps collectively referred to as BEOL (not to be confused with \"back end\" of chip fabrication, which refers to the packaging and testing stages). BEOL processing involves creating metal interconnecting wires that are isolated by dielectric layers. The insulating material has traditionally been a form of SiO2 or a silicate glass, but new low dielectric constant materials, also called low-\u03ba dielectrics, are being used (such as silicon oxycarbide), typically providing dielectric constants around 2.7 (compared to 3.82 for SiO2), although materials with constants as low as 2.2 are being offered to chipmakers. \nBEoL has been used since 1995 at the 350nm and 250nm nodes (0.35 and 0.25 micron nodes), at the same time, chemical mechanical polishing began to be employed. At the time, 2 metal layers for interconnect, also called metallization was state-of-the-art.\nSince the 22nm node, some manufacturers have added a new process called middle-of-line (MOL) which connects the transistors to the rest of the interconnect made in the BEoL process. The MOL is often based on tungsten and has upper and lower layers: the lower layer connects the junctions of the transistors, and an upper layer which is a tungsten plug that connects the transistors to the interconnect. Intel at the 10nm node introduced contact-over-active-gate (COAG) which, instead of placing the contact for connecting the transistor close to the gate of the transistor, places it directly over the gate of the transistor to improve transistor density.\nInterconnect.\nHistorically, the metal wires have been composed of aluminum. In this approach to wiring (often called \"subtractive aluminum\"), blanket films of aluminum are deposited first, patterned, and then etched, leaving isolated wires. Dielectric material is then deposited over the exposed wires. The various metal layers are interconnected by etching holes (called \"\"vias\")\" in the insulating material and then depositing tungsten in them with a CVD technique using tungsten hexafluoride; this approach can still be (and often is) used in the fabrication of many memory chips such as dynamic random-access memory (DRAM), because the number of interconnect levels can be small (no more than four). The aluminum was sometimes alloyed with copper for preventing recrystallization. Gold was also used in interconnects in early chips.\nMore recently, as the number of interconnect levels for logic has substantially increased due to the large number of transistors that are now interconnected in a modern microprocessor, the timing delay in the wiring has become so significant as to prompt a change in wiring material (from aluminum to copper interconnect layer) alongside a change in dielectric material in the interconnect (from silicon dioxides to newer low-\u03ba insulators). This performance enhancement also comes at a reduced cost via damascene processing, which eliminates processing steps. As the number of interconnect levels increases, planarization of the previous layers is required to ensure a flat surface prior to subsequent lithography. Without it, the levels would become increasingly crooked, extending outside the depth of focus of available lithography, and thus interfering with the ability to pattern. CMP (chemical-mechanical planarization) is the primary processing method to achieve such planarization, although dry \"etch back\" is still sometimes employed when the number of interconnect levels is no more than three. Copper interconnects use an electrically conductive barrier layer to prevent the copper from diffusing into (\"poisoning\") its surroundings, often made of tantalum nitride. In 1997, IBM was the first to adopt copper interconnects.\nIn 2014, Applied Materials proposed the use of cobalt in interconnects at the 22nm node, used for encapsulating copper interconnects in cobalt to prevent electromigration, replacing tantalum nitride since it needs to be thicker than cobalt in this application.\nWafer metrology.\nThe highly serialized nature of wafer processing has increased the demand for metrology in between the various processing steps. For example, thin film metrology based on ellipsometry or reflectometry is used to tightly control the thickness of gate oxide, as well as the thickness, refractive index, and extinction coefficient of photoresist and other coatings. Wafer metrology equipment/tools, or wafer inspection tools are used to verify that the wafers haven't been damaged by previous processing steps up until testing; if too many dies on one wafer have failed, the entire wafer is scrapped to avoid the costs of further processing. Virtual metrology has been used to predict wafer properties based on statistical methods without performing the physical measurement itself.\nDevice test.\nOnce the front-end process has been completed, the semiconductor devices or chips are subjected to a variety of electrical tests to determine if they function properly. The percent of devices on the wafer found to perform properly is referred to as the yield. Manufacturers are typically secretive about their yields, but it can be as low as 30%, meaning that only 30% of the chips on the wafer work as intended. Process variation is one among many reasons for low yield. Testing is carried out to prevent faulty chips from being assembled into relatively expensive packages.\nThe yield is often but not necessarily related to device (die or chip) size. As an example, in December 2019, TSMC announced an average yield of ~80%, with a peak yield per wafer of &gt;90% for their 5nm test chips with a die size of 17.92\u00a0mm2. The yield went down to 32% with an increase in die size to 100\u00a0mm2. The number of killer defects on a wafer, regardless of die size, can be noted as the defect density (or D0) of the wafer per unit area, usually cm2.\nThe fab tests the chips on the wafer with an electronic tester that presses tiny probes against the chip. The machine marks each bad chip with a drop of dye. Currently, electronic dye marking is possible if wafer test data (results) are logged into a central computer database and chips are \"binned\" (i.e. sorted into virtual bins) according to predetermined test limits such as maximum operating frequencies/clocks, number of working (fully functional) cores per chip, etc. The resulting binning data can be graphed, or logged, on a wafer map to trace manufacturing defects and mark bad chips. This map can also be used during wafer assembly and packaging. Binning allows chips that would otherwise be rejected to be reused in lower-tier products, as is the case with GPUs and CPUs, increasing device yield, especially since very few chips are fully functional (have all cores functioning correctly, for example). eFUSEs may be used to disconnect parts of chips such as cores, either because they did not work as intended during binning, or as part of market segmentation (using the same chip for low, mid and high-end tiers). Chips may have spare parts to allow the chip to fully pass testing even if it has several non-working parts.\nChips are also tested again after packaging, as the bond wires may be missing, or analog performance may be altered by the package. This is referred to as the \"final test\". Chips may also be imaged using x-rays.\nUsually, the fab charges for testing time, with prices on the order of cents per second. Testing times vary from a few milliseconds to a couple of seconds, and the test software is optimized for reduced testing time. Multiple chip (multi-site) testing is also possible because many testers have the resources to perform most or all of the tests in parallel and on several chips at once.\nChips are often designed with \"testability features\" such as scan chains or a \"built-in self-test\" to speed testing and reduce testing costs. In certain designs that use specialized analog fab processes, wafers are also laser-trimmed during testing, in order to achieve tightly distributed resistance values as specified by the design.\nGood designs try to test and statistically manage \"corners\" (extremes of silicon behavior caused by a high operating temperature combined with the extremes of fab processing steps). Most designs cope with at least 64 corners.\nDevice yield.\nDevice yield or die yield is the number of working chips or dies on a wafer, given in percentage since the number of chips on a wafer (Die per wafer, DPW) can vary depending on the chips' size and the wafer's diameter. Yield degradation is a reduction in yield, which historically was mainly caused by dust particles, however since the 1990s, yield degradation is mainly caused by process variation, the process itself and by the tools used in chip manufacturing, although dust still remains a problem in many older fabs. Dust particles have an increasing effect on yield as feature sizes are shrunk with newer processes. Automation and the use of mini environments inside of production equipment, FOUPs and SMIFs have enabled a reduction in defects caused by dust particles. Device yield must be kept high to reduce the selling price of the working chips since working chips have to pay for those chips that failed, and to reduce the cost of wafer processing. Yield can also be affected by the design and operation of the fab.\nTight control over contaminants and the production process are necessary to increase yield. Contaminants may be chemical contaminants or be dust particles. \"Killer defects\" are those caused by dust particles that cause complete failure of the device (such as a transistor). There are also harmless defects. A particle needs to be 1/5 the size of a feature to cause a killer defect. So if a feature is 100\u00a0nm across, a particle only needs to be 20\u00a0nm across to cause a killer defect. Electrostatic electricity can also affect yield adversely. Chemical contaminants or impurities include heavy metals such as iron, copper, nickel, zinc, chromium, gold, mercury and silver, alkali metals such as sodium, potassium and lithium, and elements such as aluminum, magnesium, calcium, chlorine, sulfur, carbon, and fluorine. It is important for these elements to not remain in contact with the silicon, as they could reduce yield. Chemical mixtures may be used to remove these elements from the silicon; different mixtures are effective against different elements.\nSeveral models are used to estimate yield. They are Murphy's model, Poisson's model, the binomial model, Moore's model and Seeds' model. There is no universal model; a model has to be chosen based on actual yield distribution (the location of defective chips). For example, Murphy's model assumes that yield loss occurs more at the edges of the wafer (non-working chips are concentrated on the edges of the wafer), Poisson's model assumes that defective dies are spread relatively evenly across the wafer, and Seeds's model assumes that defective dies are clustered together.\nSmaller dies cost less to produce (since more fit on a wafer, and wafers are processed and priced as a whole), and can help achieve higher yields since smaller dies have a lower chance of having a defect, due to their lower surface area on the wafer. However, smaller dies require smaller features to achieve the same functions of larger dies or surpass them, and smaller features require reduced process variation and increased purity (reduced contamination) to maintain high yields. Metrology tools are used to inspect the wafers during the production process and predict yield, so wafers predicted to have too many defects may be scrapped to save on processing costs.\nDie preparation.\nOnce tested, a wafer is typically reduced in thickness in a process also known as \"backlap\",6 \"backfinish\", \"wafer backgrind\" or \"wafer thinning\" before the wafer is scored and then broken into individual dies, a process known as wafer dicing. Only the good, unmarked chips are packaged.\nPackaging.\nAfter the dies are tested for functionality and binned, they are packaged. Plastic or ceramic packaging involves mounting the die, connecting the die/bond pads to the pins on the package, and sealing the die. Tiny bondwires are used to connect the pads to the pins. In the 'old days' (1970s), wires were attached by hand, but now specialized machines perform the task. Traditionally, these wires have been composed of gold, leading to a lead frame (pronounced \"leed frame\") of solder-plated copper; lead is poisonous, so lead-free \"lead frames\" are now mandated by RoHS. Traditionally the bond pads are located on the edges of the die, however, Flip-chip packaging can be used to place bond pads across the entire surface of the die.\nChip scale package (CSP) is another packaging technology. A plastic dual in-line package, like most packages, is many times larger than the actual die hidden inside, whereas CSP chips are nearly the size of the die; a CSP can be constructed for each die \"before\" the wafer is diced.\nThe packaged chips are retested to ensure that they were not damaged during packaging and that the die-to-pin interconnect operation was performed correctly. A laser then etches the chip's name and numbers on the package. The steps involving testing and packaging of dies, followed by final testing of finished, packaged chips, are called the back end, post-fab, ATMP (Assembly, Test, Marking, and Packaging) or ATP (Assembly, Test and Packaging) of semiconductor manufacturing, and may be carried out by OSAT (OutSourced Assembly and Test) companies which are separate from semiconductor foundries. A foundry is a company or fab performing manufacturing processes such as photolithography and etching that are part of the front end of semiconductor manufacturing.\nHazardous materials.\nMany toxic materials are used in the fabrication process. These include:\nIt is vital that workers not be directly exposed to these dangerous substances. The high degree of automation common in the IC fabrication industry helps to reduce the risks of exposure. Most fabrication facilities employ exhaust management systems, such as wet scrubbers, combustors, heated absorber cartridges, etc., to control the risk to workers and to the environment.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27697", "revid": "300", "url": "https://en.wikipedia.org/wiki?curid=27697", "title": "Spectroscopic analysis", "text": ""}
{"id": "27698", "revid": "42094721", "url": "https://en.wikipedia.org/wiki?curid=27698", "title": "Sanskrit", "text": "Ancient Indo-Aryan language of South Asia\n&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nSanskrit (; stem form ; nominal singular , ,) is a classical language belonging to the Indo-Aryan branch of the Indo-European languages. It arose in northwest South Asia after its predecessor languages had diffused there from the northwest in the late Bronze Age. Sanskrit is the sacred language of Hinduism, the language of classical Hindu philosophy, and of historical texts of Buddhism and Jainism. It was a link language in ancient and medieval South Asia, and upon transmission of Hindu and Buddhist culture to Southeast Asia, East Asia and Central Asia in the early medieval era, it became a language of religion and high culture, and of the political elites in some of these regions. As a result, Sanskrit had a lasting effect on the languages of South Asia, Southeast Asia and East Asia, especially in their formal and learned vocabularies.\nSanskrit generally connotes several Old Indo-Aryan language varieties. The most archaic of these is the Vedic Sanskrit found in the Rigveda, a collection of 1,028 hymns composed between 1500 and 1200 BCE by Indo-Aryan tribes migrating east from the mountains of what is today northern Afghanistan across northern Pakistan and into northwestern India. Vedic Sanskrit interacted with the preexisting ancient languages of the subcontinent, absorbing names of newly encountered plants and animals; in addition, the ancient Dravidian languages influenced Sanskrit's phonology and syntax. \"Sanskrit\" can also more narrowly refer to Classical Sanskrit, a refined and standardized grammatical form that emerged in the mid-1st millennium BCE and was codified in the most comprehensive of ancient grammars, the \"A\u1e63\u1e6d\u0101dhy\u0101y\u012b\" ('Eight chapters') of P\u0101\u1e47ini. The greatest dramatist in Sanskrit, K\u0101lid\u0101sa, wrote in classical Sanskrit, and the foundations of modern arithmetic were first described in classical Sanskrit. The two major Sanskrit epics, the Mah\u0101bh\u0101rata and the R\u0101m\u0101ya\u1e47a, however, were composed in a range of oral storytelling registers called Epic Sanskrit which was used in northern India between 400 BCE and 300 CE, and roughly contemporary with classical Sanskrit. In the following centuries, Sanskrit became tradition-bound, stopped being learned as a first language, and ultimately stopped developing as a living language.\nThe hymns of the Rigveda are notably similar to the most archaic poems of the Iranian and Greek language families, the \"Gathas\" of old Avestan and \"Iliad\" of Homer. As the Rigveda was orally transmitted by methods of memorisation of exceptional complexity, rigour and fidelity, as a single text without variant readings, its preserved archaic syntax and morphology are of vital importance in the reconstruction of the common ancestor language Proto-Indo-European. Sanskrit does not have an attested native script: from around the turn of the 1st-millennium CE, it has been written in various Brahmic scripts, and in the modern era most commonly in Devanagari.\nSanskrit's status, function, and place in India's cultural heritage are recognized by its inclusion in the Constitution of India's Eighth Schedule languages. However, despite attempts at revival, there are no first-language speakers of Sanskrit in India. In each of India's recent decennial censuses, several thousand citizens have reported Sanskrit to be their mother tongue, but the numbers are thought to signify a wish to be aligned with the prestige of the language. Sanskrit has been taught in traditional gurukulas since ancient times; it is widely taught today at the secondary school level. The oldest Sanskrit college is the Benares Sanskrit College founded in 1791 during East India Company rule. Sanskrit continues to be widely used as a ceremonial and ritual language in Hindu and Buddhist hymns and chants.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nEtymology and nomenclature.\nIn Sanskrit, the verbal adjective \"\" is a compound word consisting of ('together, good, well, perfected') and \"-\" ('made, formed, work'). It connotes a work that has been \"well prepared, pure and perfect, polished, sacred\". According to Biderman, the perfection contextually being referred to in the etymological origins of the word is its tonal\u2014rather than semantic\u2014qualities. Sound and oral transmission were highly valued qualities in ancient India, and its sages refined the alphabet, the structure of words, and its exacting grammar into a \"collection of sounds, a kind of sublime musical mold\" as an integral language they called \"Sa\u1e43sk\u1e5bta\". From the late Vedic period onwards, state Annette Wilke and Oliver Moebus, resonating sound and its musical foundations attracted an \"exceptionally large amount of linguistic, philosophical and religious literature\" in India. Sound was visualized as \"pervading all creation\", another representation of the world itself; the \"mysterious magnum\" of Hindu thought. The search for perfection in thought and the goal of liberation were among the dimensions of sacred sound, and the common thread that wove all ideas and inspirations together became the quest for what the ancient Indians believed to be a perfect language, the \"phonocentric episteme\" of Sanskrit.\nSanskrit as a language competed with numerous, less exact vernacular Indian languages called \"Prakritic languages\" (\"\"). The term literally means \"original, natural, normal, artless\", states Franklin Southworth. The relationship between Prakrit and Sanskrit is found in Indian texts dated to the 1st millennium CE. Pata\u00f1jali acknowledged that Prakrit is the first language, one instinctively adopted by every child with all its imperfections and later leads to the problems of interpretation and misunderstanding. The purifying structure of the Sanskrit language removes these imperfections. The early Sanskrit grammarian Da\u1e47\u1e0din states, for example, that much in the Prakrit languages is etymologically rooted in Sanskrit, but involves \"loss of sounds\" and corruptions that result from a \"disregard of the grammar\". Da\u1e47\u1e0din acknowledged that there are words and confusing structures in Prakrit that thrive independent of Sanskrit. This view is found in the writing of Bharata Muni, the author of the ancient \"Natya Shastra\" text. The early Jain scholar Namis\u0101dhu acknowledged the difference, but disagreed that Prakrit language was a corruption of Sanskrit. Namis\u0101dhu stated that the Prakrit language was the ('came before, origin') and that it came naturally to children, while Sanskrit was a refinement of Prakrit through \"purification by grammar\".\nHistory.\nOrigin and development.\nSanskrit belongs to the Indo-European family of languages. It is one of the three earliest ancient documented languages that arose from a common root language now referred to as Proto-Indo-European:\nOther Indo-European languages distantly related to Sanskrit include archaic and Classical Latin (c. 600\u00a0BCE\u2013100\u00a0CE, Italic languages), Gothic (archaic Germanic language, c.\u2009350\u00a0CE), Old Norse (c. 200\u00a0CE and after), Old Avestan (c.\u2009late 2nd millennium\u00a0BCE) and Younger Avestan (c. 900\u00a0BCE). The closest ancient relatives of Vedic Sanskrit in the Indo-European languages are the Nuristani languages found in the remote Hindu Kush region of northeastern Afghanistan and northwestern Himalayas, as well as the extinct Avestan and Old Persian \u2013 both Iranian languages. Sanskrit belongs to the satem group of the Indo-European languages.\nColonial era scholars familiar with Latin and Greek were struck by the resemblance of the Sanskrit language, both in its vocabulary and grammar, to the classical languages of Europe.\nIn \"The Oxford Introduction to Proto-Indo-European and the Proto-Indo-European World\", Mallory and Adams illustrate the resemblance with the following examples of cognate forms (with the addition of Old English for further comparison):\nThe correspondences suggest some common root, and historical links between some of the distant major ancient languages of the world.\nThe Indo-Aryan migrations theory explains the common features shared by Sanskrit and other Indo-European languages by proposing that the original speakers of what became Sanskrit arrived in South Asia from a region of common origin, somewhere north-west of the Indus region, during the early 2nd millennium\u00a0BCE. Evidence for such a theory includes the close relationship between the Indo-Iranian tongues and the Baltic and Slavic languages, vocabulary exchange with the non-Indo-European Uralic languages, and the nature of the attested Indo-European words for flora and fauna.\nThe pre-history of Indo-Aryan languages which preceded Vedic Sanskrit is unclear and various hypotheses place it over a fairly wide limit. According to Thomas Burrow, based on the relationship between various Indo-European languages, the origin of all these languages may possibly be in what is now Central or Eastern Europe, while the Indo-Iranian group possibly arose in Central Russia. The Iranian and Indo-Aryan branches separated quite early. It is the Indo-Aryan branch that moved into eastern Iran and then south into South Asia in the first half of the 2nd millennium\u00a0BCE. Once in ancient India, the Indo-Aryan language underwent rapid linguistic change and morphed into the Vedic Sanskrit language.\nVedic Sanskrit.\nThe pre-Classical form of Sanskrit is known as \"Vedic Sanskrit\". The earliest attested Sanskrit text is the Rigveda (), a Hindu scripture from the mid- to late-second millennium\u00a0BCE. No written records from such an early period survive, if any ever existed, but scholars are generally confident that the oral transmission of the texts is reliable: they are ceremonial literature, where the exact phonetic expression and its preservation were a part of the historic tradition.\nHowever some scholars have suggested that the original \u1e5ag-veda differed in some fundamental ways in phonology compared to the sole surviving version available to us. In particular that retroflex consonants did not exist as a natural part of the earliest Vedic language, and that these developed in the centuries after the composition had been completed, and as a gradual unconscious process during the oral transmission by generations of reciters.\nThe primary source for this argument is internal evidence of the text which betrays an instability of the phenomenon of retroflexion, with the same phrases having sandhi-induced retroflexion in some parts but not other. This is taken along with evidence of controversy, for example, in passages of the Aitareya-\u0100ra\u1e47yaka (700 BCE), which features a discussion on whether retroflexion is valid in particular cases.\nThe \u1e5ag-veda is a collection of books, created by multiple authors. These authors represented different generations, and the mandalas\u00a02 to 7 are the oldest while the mandalas\u00a01 and 10 are relatively the youngest. Yet, the Vedic Sanskrit in these books of the \u1e5ag-veda \"hardly presents any dialectical diversity\", states Louis Renou \u2013 an Indologist known for his scholarship of the Sanskrit literature and the \u1e5ag-veda in particular. According to Renou, this implies that the Vedic Sanskrit language had a \"set linguistic pattern\" by the second half of the 2nd millennium\u00a0BCE. Beyond the \u1e5ag-veda, the ancient literature in Vedic Sanskrit that has survived into the modern age include the \"Samaveda\", \"Yajurveda\", \"Atharvaveda\", along with the embedded and layered Vedic texts such as the Brahmanas, Aranyakas, and the early Upanishads. These Vedic documents reflect the dialects of Sanskrit found in the various parts of the northwestern, northern, and eastern Indian subcontinent.\nAccording to Michael Witzel, Vedic Sanskrit was a spoken language of the semi-nomadic Aryans. The Vedic Sanskrit language or a closely related Indo-European variant was recognized beyond ancient India as evidenced by the \"Mitanni Treaty\" between the ancient Hittite and Mitanni people, carved into a rock, in a region that now includes parts of Syria and Turkey. Parts of this treaty, such as the names of the Mitanni princes and technical terms related to horse training, for reasons not understood, are in early forms of Vedic Sanskrit. The treaty also invokes the gods Varuna, Mitra, Indra, and Nasatya found in the earliest layers of the Vedic literature.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\nO B\u1e5bhaspati, when in giving names\nthey first set forth the beginning of Language,\nTheir most excellent and spotless secret\nwas laid bare through love,\nWhen the wise ones formed Language with their mind,\npurifying it like grain with a winnowing fan,\nThen friends knew friendships \u2013\nan auspicious mark placed on their language.\n&lt;/poem&gt;\n \u2014 \"Rigveda 10.71.1\u20134\"Translated by Roger Woodard\nThe Vedic Sanskrit found in the \u1e5ag-veda is distinctly more archaic than other Vedic texts, and in many respects, the Rigvedic language is notably more similar to those found in the archaic texts of Old Avestan Zoroastrian \"Gathas\" and Homer's \"Iliad\" and \"Odyssey\". According to Stephanie W. Jamison and Joel P. Brereton \u2013 Indologists known for their translation of the \u1e5ag-veda \u2013 the Vedic Sanskrit literature \"clearly inherited\" from Indo-Iranian and Indo-European times the social structures such as the role of the poet and the priests, the patronage economy, the phrasal equations, and some of the poetic metres. While there are similarities, state Jamison and Brereton, there are also differences between Vedic Sanskrit, the Old Avestan, and the Mycenaean Greek literature. For example, unlike the Sanskrit similes in the \u1e5ag-veda, the Old Avestan \"Gathas\" lack simile entirely, and it is rare in the later version of the language. The Homerian Greek, like \u1e5ag-vedic Sanskrit, deploys simile extensively, but they are structurally very different.\nClassical Sanskrit.\nThe early Vedic form of the Sanskrit language was far less homogenous compared to the Classical Sanskrit as defined by grammarians by about the mid-1st millennium BCE. According to Richard Gombrich\u2014an Indologist and a scholar of Sanskrit, P\u0101li and Buddhist Studies\u2014the archaic Vedic Sanskrit found in the \"Rigveda\" had already evolved in the Vedic period, as evidenced in the later Vedic literature. Gombrich posits that the language in the early Upanishads of Hinduism and the late Vedic literature approaches Classical Sanskrit, while the archaic Vedic Sanskrit had by the Buddha's time become unintelligible to all except ancient Indian sages.\nThe formalization of the Sa\u1e43sk\u1e5bta language is credited to , along with Pata\u00f1jali's and Katyayana's commentary that preceded Pata\u00f1jali's work. Panini composed \"\" ('Eight-Chapter Grammar'), which became the foundation of Vy\u0101kara\u1e47a, a Ved\u0101nga. The was not the first description of Sanskrit grammar, but it is the earliest that has survived in full, and the culmination of a long grammatical tradition that Fortson says, is \"one of the intellectual wonders of the ancient world\". P\u0101\u1e47ini cites ten scholars on the phonological and grammatical aspects of the Sanskrit language before him, as well as the variants in the usage of Sanskrit in different regions of India. The ten Vedic scholars he quotes are \u0100pi\u015bali, Ka\u015byapa, G\u0101rgya, G\u0101lava, Cakravarma\u1e47a, Bh\u0101radv\u0101ja, \u015a\u0101ka\u1e6d\u0101yana, \u015a\u0101kalya, Senaka and Spho\u1e6d\u0101yana.\nIn the , language is observed in a manner that has no parallel among Greek or Latin grammarians. P\u0101\u1e47ini's grammar, according to Renou and Filliozat, is a classic that defines the linguistic expression and sets the standard for the Sanskrit language. P\u0101\u1e47ini made use of a technical metalanguage consisting of a syntax, morphology and lexicon. This metalanguage is organised according to a series of meta-rules, some of which are explicitly stated while others can be deduced. Despite differences in the analysis from that of modern linguistics, P\u0101\u1e47ini's work has been found valuable and the most advanced analysis of linguistics until the twentieth century.\nP\u0101\u1e47ini's comprehensive and scientific theory of grammar is conventionally taken to mark the start of Classical Sanskrit. His systematic treatise inspired and made Sanskrit the preeminent Indian language of learning and literature for two millennia. It is unclear whether P\u0101\u1e47ini himself wrote his treatise or he orally created the detailed and sophisticated treatise then transmitted it through his students. Modern scholarship generally accepts that he knew of a form of writing, based on references to words such as \"lipi\" ('script') and ('scribe') in section 3.2 of the .\nThe Classical Sanskrit language formalized by P\u0101\u1e47ini, states Renou, is \"not an impoverished language\", rather it is \"a controlled and a restrained language from which archaisms and unnecessary formal alternatives were excluded\". The Classical form of the language simplified the \"sandhi\" rules but retained various aspects of the Vedic language, while adding rigor and flexibilities, so that it had sufficient means to express thoughts as well as being \"capable of responding to the future increasing demands of an infinitely diversified literature\", according to Renou. P\u0101\u1e47ini included numerous \"optional rules\" beyond the Vedic Sanskrit's framework, to respect liberty and creativity so that individual writers separated by geography or time would have the choice to express facts and their views in their own way, where tradition followed competitive forms of the Sanskrit language.\nThe phonetic differences between Vedic Sanskrit and Classical Sanskrit, as discerned from the current state of the surviving literature, are negligible when compared to the intense change that must have occurred in the pre-Vedic period between the Proto-Indo-Aryan language and Vedic Sanskrit. The noticeable differences between the Vedic and the Classical Sanskrit include the much-expanded grammar and grammatical categories as well as the differences in the accent, the semantics and the syntax. There are also some differences between how some of the nouns and verbs end, as well as the \"sandhi\" rules, both internal and external. Quite many words found in the early Vedic Sanskrit language are never found in late Vedic Sanskrit or Classical Sanskrit literature, while some words have different and new meanings in Classical Sanskrit when contextually compared to the early Vedic Sanskrit literature.\nArthur Macdonell was among the early colonial era scholars who summarized some of the differences between the Vedic and Classical Sanskrit. Louis Renou published in 1956, in French, a more extensive discussion of the similarities, the differences and the evolution of the Vedic Sanskrit within the Vedic period and then to the Classical Sanskrit along with his views on the history. This work has been translated by Jagbans Balbir.\nSanskrit and Prakrit languages.\nThe earliest known use of the word (Sanskrit), in the context of a speech or language, is found in verses 5.28.17\u201319 of the \"Ramayana\". Outside the learned sphere of written Classical Sanskrit, vernacular colloquial dialects (Prakrits) continued to evolve. Sanskrit co-existed with numerous other Prakrit languages of ancient India. The Prakrit languages of India also have ancient roots and some Sanskrit scholars have called these , literally 'spoiled'. The Vedic literature includes words whose phonetic equivalent are not found in other Indo-European languages but which are found in the regional Prakrit languages, which makes it likely that the interaction, the sharing of words and ideas began early in the Indian history. As the Indian thought diversified and challenged earlier beliefs of Hinduism, particularly in the form of Buddhism and Jainism, the Prakrit languages such as Pali in Theravada Buddhism and Ardhamagadhi in Jainism competed with Sanskrit in the ancient times. However, states Paul Dundas, these ancient Prakrit languages had \"roughly the same relationship to Sanskrit as medieval Italian does to Latin\". The Indian tradition states that the Buddha and the Mahavira preferred the Prakrit language so that everyone could understand it. However, scholars such as Dundas have questioned this hypothesis. They state that there is no evidence for this and whatever evidence is available suggests that by the start of the common era, hardly anybody other than learned monks had the capacity to understand the old Prakrit languages such as Ardhamagadhi.\nA section of European scholars state that Sanskrit was never a spoken language. However, evidences shows that Sanskrit was a spoken language, essential for oral tradition that preserved the vast number of Sanskrit manuscripts from ancient India. The textual evidence in the works of Yaksa, Panini, and Patanajali affirms that Classical Sanskrit in their era was a spoken language () used by the cultured and educated. Some \"sutras\" expound upon the variant forms of spoken Sanskrit versus written Sanskrit. Chinese Buddhist pilgrim Xuanzang mentioned in his memoir that official philosophical debates in India were held in Sanskrit, not in the vernacular language of that region.\nAccording to Sanskrit linguist professor Madhav Deshpande, Sanskrit was a spoken language in a colloquial form by the mid-1st\u00a0millennium\u00a0BCE which coexisted with a more formal, grammatically correct form of literary Sanskrit. This, states Deshpande, is true for modern languages where colloquial incorrect approximations and dialects of a language are spoken and understood, along with more \"refined, sophisticated and grammatically accurate\" forms of the same language being found in the literary works. The Indian tradition, states Winternitz, has favored the learning and the usage of multiple languages from the ancient times. Sanskrit was a spoken language in the educated and the elite classes, but it was also a language that must have been understood in a wider circle of society because the widely popular folk epics and stories such as the \"Ramayana\", the \"Mahabharata\", the \"Bhagavata Purana\", the \"Panchatantra\" and many other texts are all in the Sanskrit language. The Classical Sanskrit with its exacting grammar was thus the language of the Indian scholars and the educated classes, while others communicated with approximate or ungrammatical variants of it as well as other natural Indian languages. Sanskrit, as the learned language of Ancient India, thus existed alongside the vernacular Prakrits. Many Sanskrit dramas indicate that the language coexisted with the vernacular Prakrits. The cities of Varanasi, Paithan, Pune and Kanchipuram were centers of classical Sanskrit learning and public debates until the arrival of the colonial era.\nAccording to Lamotte, Sanskrit became the dominant literary and inscriptional language because of its precision in communication. It was, states Lamotte, an ideal instrument for presenting ideas, and as knowledge in Sanskrit multiplied, so did its spread and influence. Sanskrit was adopted voluntarily as a vehicle of high culture, arts, and profound ideas. Pollock disagrees with Lamotte, but concurs that Sanskrit's influence grew into what he terms a \"Sanskrit Cosmopolis\" over a region that included all of South Asia and much of southeast Asia. The Sanskrit language cosmopolis thrived beyond India between 300 and 1300\u00a0CE.\nToday, it is believed that Kashmiri is the closest language to Sanskrit.\nDravidian influence on Sanskrit.\nRein\u00f6hl mentions that not only have the Dravidian languages borrowed from Sanskrit vocabulary, but they have also affected Sanskrit on deeper levels of structure, \"for instance in the domain of phonology where Indo-Aryan retroflexes have been attributed to Dravidian influence\". Similarly, Ferenc Ruzca states that all the major shifts in Indo-Aryan phonetics over two millennia can be attributed to the constant influence of a Dravidian language with a similar phonetic structure to Tamil. Hock et al. quoting George Hart state that there was influence of Old Tamil on Sanskrit. Hart compared Old Tamil and Classical Sanskrit to arrive at a conclusion that there was a common language from which these features both derived \u2013 \"that both Tamil and Sanskrit derived their shared conventions, metres, and techniques from a common source, for it is clear that neither borrowed directly from the other.\"\nRein\u00f6hl further states that there is a symmetric relationship between Dravidian languages like Kannada or Tamil, with Indo-Aryan languages like Bengali or Hindi, whereas the same relationship is not found for non-Indo-Aryan languages, for example, Persian or English:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A sentence in a Dravidian language like Tamil or Kannada becomes ordinarily good Bengali or Hindi by substituting Bengali or Hindi equivalents for the Dravidian words and forms, without modifying the word order; but the same thing is not possible in rendering a Persian or English sentence into a non-Indo-Aryan language.\u2014\u200a\nShulman mentions that \"Dravidian nonfinite verbal forms (called in Tamil) shaped the usage of the Sanskrit nonfinite verbs (originally derived from inflected forms of action nouns in Vedic). This particularly salient case of the possible influence of Dravidian on Sanskrit is only one of many items of syntactic assimilation, not least among them the large repertoire of morphological modality and aspect that, once one knows to look for it, can be found everywhere in classical and postclassical Sanskrit\".\nThe main influence of Dravidian on Sanskrit is found to have been concentrated in the timespan between the late Vedic period and the crystallization of Classical Sanskrit. As in this period the Indo-Aryan tribes had not yet made contact with the inhabitants of the South of the subcontinent, this suggests a significant presence of Dravidian speakers in North India (the central Gangetic plain and the classical Madhyade\u015ba) who were instrumental in this substratal influence on Sanskrit.\nInfluence.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nExtant manuscripts in Sanskrit number over 30\u00a0million, one hundred times those in Greek and Latin combined, constituting the largest cultural heritage that any civilization has produced prior to the invention of the printing press.\n\u2014 Foreword of \"Sanskrit Computational Linguistics\" (2009), G\u00e9rard Huet, Amba Kulkarni and Peter Scharf\nSanskrit has been the predominant language of Hindu texts encompassing a rich tradition of philosophical and religious texts, as well as poetry, music, drama, scientific, technical and others. It is the predominant language of one of the largest collection of historic manuscripts. The earliest known inscriptions in Sanskrit are from the 1st\u00a0century\u00a0BCE, such as the Ayodhya Inscription of Dhana and Ghosundi-Hathibada (Chittorgarh).\nThough developed and nurtured by scholars of orthodox schools of Hinduism, Sanskrit has been the language for some of the key literary works and theology of heterodox schools of Indian philosophies such as Buddhism and Jainism. The structure and capabilities of the Classical Sanskrit language launched ancient Indian speculations about \"the nature and function of language\", what is the relationship between words and their meanings in the context of a community of speakers, whether this relationship is objective or subjective, discovered or is created, how individuals learn and relate to the world around them through language, and about the limits of language. They speculated on the role of language, the ontological status of painting word-images through sound, and the need for rules so that it can serve as a means for a community of speakers, separated by geography or time, to share and understand profound ideas from each other. These speculations became particularly important to the M\u012bm\u0101\u1e43s\u0101 and the Nyaya schools of Hindu philosophy, and later to Vedanta and Mahayana Buddhism, states Frits Staal\u2014a scholar of Linguistics with a focus on Indian philosophies and Sanskrit. Though written in a number of different scripts, the dominant language of Hindu texts has been Sanskrit. It or a hybrid form of Sanskrit became the preferred language of Mahayana Buddhism scholarship; for example, one of the early and influential Buddhist philosophers, Nagarjuna (c.200\u00a0CE), used Classical Sanskrit as the language for his texts. According to Renou, Sanskrit had a limited role in the Theravada tradition (formerly known as the Hinayana) but the Prakrit works that have survived are of doubtful authenticity. Some of the canonical fragments of the early Buddhist traditions, discovered in the 20th century, suggest the early Buddhist traditions used an imperfect and reasonably good Sanskrit, sometimes with a Pali syntax, states Renou. The Mah\u0101s\u0101\u1e43ghika and Mahavastu, in their late Hinayana forms, used hybrid Sanskrit for their literature. Sanskrit was also the language of some of the oldest surviving, authoritative and much followed philosophical works of Jainism such as the \"Tattvartha Sutra\" by Umaswati.\nThe Sanskrit language has been one of the major means for the transmission of knowledge and ideas in Asian history. Indian texts in Sanskrit were already in China by 402\u00a0CE, carried by the influential Buddhist pilgrim Faxian who translated them into Chinese by 418\u00a0CE. Xuanzang, another Chinese Buddhist pilgrim, learnt Sanskrit in India and carried 657\u00a0Sanskrit texts to China in the 7th century where he established a major center of learning and language translation under the patronage of Emperor Taizong. By the early 1st millennium CE, Sanskrit had spread Buddhist and Hindu ideas to Southeast Asia, parts of the East Asia and the Central Asia. It was accepted as a language of high culture and the preferred language by some of the local ruling elites in these regions. According to the Dalai Lama, the Sanskrit language is a parent language that is at the foundation of many modern languages of India and the one that promoted Indian thought to other distant countries. In Tibetan Buddhism, states the Dalai Lama, Sanskrit language has been a revered one and called \"legjar lhai-ka\" or \"elegant language of the gods\". It has been the means of transmitting the \"profound wisdom of Buddhist philosophy\" to Tibet.\nThe Sanskrit language created a pan-Indo-Aryan accessibility to information and knowledge in the ancient and medieval times, in contrast to the Prakrit languages which were understood just regionally. It created a cultural bond across the subcontinent. As local languages and dialects evolved and diversified, Sanskrit served as the common language. It connected scholars from distant parts of South Asia such as Tamil Nadu and Kashmir, states Deshpande, as well as those from different fields of studies, though there must have been differences in its pronunciation given the first language of the respective speakers. The Sanskrit language brought Indo-Aryan speaking people together, particularly its elite scholars. Some of these scholars of Indian history regionally produced vernacularized Sanskrit to reach wider audiences, as evidenced by texts discovered in Rajasthan, Gujarat, and Maharashtra. Once the audience became familiar with the easier to understand vernacularized version of Sanskrit, those interested could graduate from colloquial Sanskrit to the more advanced Classical Sanskrit. Rituals and the rites-of-passage ceremonies have been and continue to be the other occasions where a wide spectrum of people hear Sanskrit, and occasionally join in to speak some Sanskrit words such as .\nClassical Sanskrit is the standard register as laid out in the grammar of , around the fourth century\u00a0BCE. Its position in the cultures of Greater India is akin to that of Latin and Ancient Greek in Europe. Sanskrit has significantly influenced most modern languages of the Indian subcontinent, particularly the languages of the northern, western, central and eastern Indian subcontinent.\nDecline.\nIndian authors such as M Ramakrishnan Nair state that Sanskrit was a dead language by the 1st millennium BCE. The decline of Sanskrit began in the 13th century. This coincides with the beginning of Islamic invasions of South Asia to create, and thereafter expand the Muslim rule in the form of Sultanates, and later the Mughal Empire. Sheldon Pollock characterises the decline of Sanskrit as a long-term \"cultural, social, and political change\". He dismisses the idea that Sanskrit declined due to \"struggle with barbarous invaders\", and emphasises factors such as the increasing attractiveness of vernacular language for literary expression.\nWith the fall of Kashmir around the 13th century, a premier center of Sanskrit literary creativity, Sanskrit literature there disappeared, perhaps in the \"fires that periodically engulfed the capital of Kashmir\" or the \"Mongol invasion of 1320\" states Pollock. The Sanskrit literature which was once widely disseminated out of the northwest regions of the subcontinent, stopped after the 12th century. As Hindu kingdoms fell in the eastern and the South India, such as the great Vijayanagara Empire, so did Sanskrit. There were exceptions and short periods of imperial support for Sanskrit, mostly concentrated during the reign of the tolerant Mughal emperor Akbar. Muslim rulers patronized the Middle Eastern language and scripts found in Persia and Arabia, and the Indians linguistically adapted to this Persianization to gain employment with the Muslim rulers. Hindu rulers such as Shivaji of the Maratha Empire, reversed the process, by re-adopting Sanskrit and re-asserting their socio-linguistic identity. After Islamic rule disintegrated in South Asia and the colonial rule era began, Sanskrit re-emerged but in the form of a \"ghostly existence\" in regions such as Bengal. This decline was the result of \"political institutions and civic ethos\" that did not support the historic Sanskrit literary culture and the failure of new Sanskrit literature to assimilate into the changing cultural and political environment.\nSheldon Pollock states that in some crucial way, \"Sanskrit is dead\". After the 12th century, the Sanskrit literary works were reduced to \"reinscription and restatements\" of ideas already explored, and any creativity was restricted to hymns and verses. This contrasted with the previous 1,500 years when \"great experiments in moral and aesthetic imagination\" marked the Indian scholarship using Classical Sanskrit, states Pollock.\nScholars maintain that the Sanskrit language did not die, but rather only declined. Jurgen Hanneder disagrees with Pollock, finding his arguments elegant but \"often arbitrary\". According to Hanneder, a decline or regional absence of creative and innovative literature constitutes a negative evidence to Pollock's hypothesis, but it is not positive evidence. A closer look at Sanskrit in the Indian history after the 12th century suggests that Sanskrit survived despite the odds. According to Hanneder,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;On a more public level the statement that Sanskrit is a dead language is misleading, for Sanskrit is quite obviously not as dead as other dead languages and the fact that it is spoken, written and read will probably convince most people that it cannot be a dead language in the most common usage of the term. Pollock's notion of the \"death of Sanskrit\" remains in this unclear realm between academia and public opinion when he says that \"most observers would agree that, in some crucial way, Sanskrit is dead.\"\nThe Sanskrit language scholar Moriz Winternitz states that Sanskrit was never a dead language and it is still alive though its prevalence is lesser than ancient and medieval times. Sanskrit remains an integral part of Hindu journals, festivals, Ramlila plays, drama, rituals and the rites-of-passage. Similarly, Brian Hatcher states that the \"metaphors of historical rupture\" by Pollock are not valid, that there is ample proof that Sanskrit was very much alive in the narrow confines of surviving Hindu kingdoms between the 13th and 18th centuries, and its reverence and tradition continues.\nHanneder states that modern works in Sanskrit are either ignored or their \"modernity\" contested.\nAccording to Robert P. Goldman and Sally Sutherland, Sanskrit is neither \"dead\" nor \"living\" in the conventional sense. It is a special, timeless language that lives in the numerous manuscripts, daily chants, and ceremonial recitations, a \"heritage language\" that Indians contextually prize, and which some practice.\nWhen the British introduced English to India in the 19th century, knowledge of Sanskrit and ancient literature continued to flourish as the study of Sanskrit changed from a more traditional style into a form of analytical and comparative scholarship mirroring that of Europe.\nModern Indo-Aryan languages.\nThe relationship of Sanskrit to the Prakrit languages, particularly the modern form of Indian languages, is complex and spans about 3,500\u00a0years, states Colin Masica\u2014a linguist specializing in South Asian languages. A part of the difficulty is the lack of sufficient textual, archaeological and epigraphical evidence for the ancient Prakrit languages with rare exceptions such as Pali, leading to a tendency toward anachronistic errors. Sanskrit and Prakrit languages may be divided into Old Indo-Aryan (1500\u00a0BCE \u2013 600\u00a0BCE), Middle Indo-Aryan (600\u00a0BCE \u2013 1000\u00a0CE) and New Indo-Aryan (1000\u00a0CE \u2013 present), each can further be subdivided into early, middle or second, and late evolutionary substages.\nVedic Sanskrit belongs to the early Old Indo-Aryan stage, while Classical Sanskrit to the later Old Indo-Aryan stage. The evidence for Prakrits such as Pali (Theravada Buddhism) and Ardhamagadhi (Jainism), along with Magadhi, Maharashtri, Sinhala, Sauraseni and Niya (Gandhari), emerge in the Middle Indo-Aryan stage in two versions\u2014archaic and more formalized\u2014that may be placed in early and middle substages of the 600\u00a0BCE \u2013 1000\u00a0CE period. Two literary Indo-Aryan languages can be traced to the late Middle Indo-Aryan stage and these are \"Apabhramsa\" and \"Elu\" (a literary form of Sinhalese). Numerous North, Central, Eastern and Western Indian languages, such as Hindi, Gujarati, Sindhi, Punjabi, Kashmiri, Nepali, Braj, Awadhi, Bengali, Assamese, Oriya, Marathi, and others belong to the New Indo-Aryan stage.\nThere is an extensive overlap in the vocabulary, phonetics and other aspects of these New Indo-Aryan languages with Sanskrit, but it is neither universal nor identical across the languages. They likely emerged from a synthesis of the ancient Sanskrit language traditions and an admixture of various regional dialects. Each language has some unique and regionally creative aspects, with unclear origins. Prakrit languages do have a grammatical structure, but like Vedic Sanskrit, it is far less rigorous than Classical Sanskrit. While the roots of all Prakrit languages may be in Vedic Sanskrit and ultimately the Proto-Indo-Aryan language, their structural details vary from Classical Sanskrit. It is generally accepted by scholars and widely believed in India that the modern Indo-Aryan languages \u2013 such as Bengali, Gujarati, Hindi, and Punjabi \u2013 are descendants of the Sanskrit language. Sanskrit, states Burjor Avari, can be described as \"the mother language of almost all the languages of north India\".\nGeographic distribution.\nThe Sanskrit language's historic presence is attested across a wide geography beyond South Asia. Inscriptions and literary evidence suggests that Sanskrit language was already being adopted in Southeast Asia and Central Asia in the 1st millennium CE, through monks, religious pilgrims and merchants.\nSouth Asia has been the geographic range of the largest collection of the ancient and pre-18th-century Sanskrit manuscripts and inscriptions. Beyond ancient India, significant collections of Sanskrit manuscripts and inscriptions have been found in China (particularly the Tibetan monasteries), Myanmar, Indonesia, Cambodia, Laos, Vietnam, Thailand, and Malaysia. Sanskrit inscriptions, manuscripts or its remnants, including some of the oldest known Sanskrit written texts, have been discovered in dry high deserts and mountainous terrains such as in Nepal, Tibet, Afghanistan, Mongolia, Uzbekistan, Turkmenistan, Tajikistan, and Kazakhstan. Some Sanskrit texts and inscriptions have also been discovered in Korea and Japan.\nOfficial status.\nIn India, Sanskrit is among the 22\u00a0official languages of India in the Eighth Schedule to the Constitution. In 2010, Uttarakhand became the first state in India to make Sanskrit its second official language. In 2019, Himachal Pradesh made Sanskrit its second official language, becoming the second state in India to do so.\nPhonology.\nSanskrit shares many Proto-Indo-European phonological features, although it features a larger inventory of distinct phonemes. The consonantal system is the same, though it systematically enlarged the inventory of distinct sounds. For example, Sanskrit added a voiceless aspirated \"t\u02b0\", to the voiceless \"t\", voiced \"d\" and voiced aspirated \"d\u02b0\" found in PIE languages.\nThe most significant and distinctive phonological development in Sanskrit is vowel merger. The short \"*e\", \"*o\" and \"*a\", all merge as \"a\" (\u0905) in Sanskrit, while long \"*\u0113\", \"*\u014d\" and \"*\u0101\", all merge as long \"\u0101\" (\u0906). Compare Sanskrit \"n\u0101man\" to Latin \"n\u014dmen\". These mergers occurred very early and significantly affected Sanskrit's morphological system. Some phonological developments in it mirror those in other PIE languages. For example, the labiovelars merged with the plain velars as in other satem languages. The secondary palatalization of the resulting segments is more thorough and systematic within Sanskrit. For example, unlike the loss of the morphological clarity from vowel contraction that is found in early Greek and related southeast European languages, Sanskrit deployed \"*y\", \"*w\", and \"*s\" intervocalically to provide morphological clarity.\nVowels.\nThe cardinal vowels (\"svaras\") \"a\" (\u0905), \"i\" (\u0907), and \"u\" (\u0909) distinguish length in Sanskrit. The short \"a\" (\u0905) in Sanskrit is a closer vowel than \u0101, equivalent to schwa. The mid vowels \u0113 (\u090f) and \u014d (\u0913) in Sanskrit are monophthongizations of the Indo-Iranian diphthongs \"*ai\" and \"*au\". They are inherently long, though often transcribed \"e\" and \"o\" without the diacritic. The vocalic liquid \"r\u0325\" in Sanskrit is a merger of PIE \"*r\u0325\" and \"*l\u0325\". The long \"r\u0325\" is an innovation and it is used in a few analogically generated morphological categories.\nAccording to Masica, Sanskrit has four traditional semivowels, with which were classed, \"for morphophonemic reasons, the liquids: y, r, l, and v; that is, as y and v were the non-syllabics corresponding to i, u, so were r, l in relation to r\u0325 and l\u0325\". The northwestern, the central and the eastern Sanskrit dialects have had a historic confusion between \"r\" and \"l\". The Paninian system that followed the central dialect preserved the distinction, likely out of reverence for the Vedic Sanskrit that distinguished the \"r\" and \"l\". However, the northwestern dialect only had \"r\", while the eastern dialect probably only had \"l\", states Masica. Thus literary works from different parts of ancient India appear inconsistent in their use of \"r\" and \"l\", resulting in doublets that are occasionally semantically differentiated.\nThe nasal '\u1e43' is optionally the corresponding nasal consonant before plosives (a\u1e43 + k = a\u1e45k or am k) and an 'm'-sound before r, s, \u015b, \u1e63, and h. Before y, l, v, it can cause nasalaization and gemination (am + y = a\u1ef9y or am y).\nConsonants.\nSanskrit consonants are conventionally arranged into a symmetrical table that is based on how the sound is articulated, though this arrangement is somewhat forced and conceals a certain lack of parallelism among the actual sounds.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nThe system of Sanskrit Sounds[The alphabetic] order of Sanskrit sounds works along three principles: it goes from simple to complex; it goes from the back to the front of the mouth; and it groups similar sounds together. [...] Among themselves, both the vowels and consonants are ordered according to where in the mouth they are pronounced, going from back to front.\n\u2014 A. M. Ruppel, \"The Cambridge Introduction to Sanskrit\"\nPhonological alternations, sandhi rules.\nSanskrit deploys extensive phonological alternations on different linguistic levels through \"sandhi\" rules (literally, the rules of \"putting together, union, connection, alliance\"), similar to the English alteration of \"going to\" as \"gonna\". The Sanskrit language accepts such alterations within it, but offers formal rules for the \"sandhi\" of any two words next to each other in the same sentence or linking two sentences. The external \"sandhi\" rules state that similar short vowels coalesce into a single long vowel, while dissimilar vowels form glides or undergo diphthongization. Among the consonants, most external \"sandhi\" rules recommend regressive assimilation for clarity when they are voiced. These rules ordinarily apply at compound seams and morpheme boundaries. In Vedic Sanskrit, the external \"sandhi\" rules are more variable than in Classical Sanskrit.\nVedic Pitch Accent.\nVedic Sanskrit included a three-way pitch accent system of \"ud\u0101tta\" (raised), \"anud\u0101tta\" (not-raised or grave), and \"svarita\" (sounded) which were high, low and falling pitches respectively. Each word generally had one \"ud\u0101tta\" accent except for certain compound words. The classical language ultimately lost this system, but it was preserved in Vedic texts and phonological treatises. Traditional chanting renders the \"ud\u0101tta\" as an unaccented mid-tone and the first half of the \"svarita\" as higher than an \"ud\u0101tta\"; the other half is rendered the same level of that of an \"ud\u0101tta.\"\nMorphology.\nThe basis of Sanskrit morphology is the root, states Jamison, \"a morpheme bearing lexical meaning\". The verbal and nominal stems of Sanskrit words are derived from this root through the phonological vowel-gradation processes, the addition of affixes, verbal and nominal stems. It then adds an ending to establish the grammatical and syntactic identity of the stem. According to Jamison, the \"three major formal elements of the morphology are (i) root, (ii) affix, and (iii) ending; and they are roughly responsible for (i) lexical meaning, (ii) derivation, and (iii) inflection respectively\".\nA Sanskrit word has the following canonical structure:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\"Root\" + \"Affix\" + \"Ending\"\nThe root structure has certain phonological constraints. Two of the most important constraints of a \"root\" is that it does not end in a short \"a\" (\u0905) and that it is monosyllabic. In contrast, the affixes and endings commonly do. The affixes in Sanskrit are almost always suffixes, with exceptions such as the augment \"a-\" added as prefix to past tense verb forms and the \"-na/n-\" infix in single verbal present class, states Jamison.\nSanskrit verbs have the following canonical structure:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;\"Root\" + \"Suffix\" + \"Suffix\" + \"Ending\"\nAccording to Ruppel, verbs in Sanskrit express the same information as other Indo-European languages such as English. Sanskrit verbs describe an action or occurrence or state, its embedded morphology informs as to \"who is doing it\" (person or persons), \"when it is done\" (tense) and \"how it is done\" (mood, voice). The Indo-European languages differ in the detail. For example, the Sanskrit language attaches the affixes and ending to the verb root, while the English language adds small independent words before the verb. In Sanskrit, these elements co-exist within the word.\nBoth verbs and nouns in Sanskrit are either thematic or athematic, states Jamison. \"Guna\" (strengthened) forms in the active singular regularly alternate in athematic verbs. The finite verbs of Classical Sanskrit have the following grammatical categories: person, number, voice, tense-aspect, and mood. According to Jamison, a portmanteau morpheme generally expresses the person-number-voice in Sanskrit, and sometimes also the ending or only the ending. The mood of the word is embedded in the affix.\nThese elements of word architecture are the typical building blocks in Classical Sanskrit, but in Vedic Sanskrit these elements fluctuate and are unclear. For example, in the \"Rigveda\" preverbs regularly occur in tmesis, states Jamison, which means they are \"separated from the finite verb\". This indecisiveness is likely linked to Vedic Sanskrit's attempt to incorporate accent. With nonfinite forms of the verb and with nominal derivatives thereof, states Jamison, \"preverbs show much clearer univerbation in Vedic, both by position and by accent, and by Classical Sanskrit, tmesis is no longer possible even with finite forms\".\nWhile roots are typical in Sanskrit, some words do not follow the canonical structure. A few forms lack both inflection and root. Many words are inflected (and can enter into derivation) but lack a recognizable root. Examples from the basic vocabulary include kinship terms such as \"m\u0101tar-\" (mother), \"nas-\" (nose), \"\u015bvan-\" (dog). According to Jamison, pronouns and some words outside the semantic categories also lack roots, as do the numerals. Similarly, the Sanskrit language is flexible enough to not mandate inflection.\nThe Sanskrit words can contain more than one affix that interact with each other. Affixes in Sanskrit can be athematic as well as thematic, according to Jamison. Athematic affixes can be alternating. Sanskrit deploys eight cases, namely nominative, accusative, instrumental, dative, ablative, genitive, locative, vocative.\nStems, that is \"root + affix\", appear in two categories in Sanskrit: vowel stems and consonant stems. Unlike some Indo-European languages such as Latin or Greek, according to Jamison, \"Sanskrit has no closed set of conventionally denoted noun declensions\". Sanskrit includes a fairly large set of stem-types. The linguistic interaction of the roots, the phonological segments, lexical items and the grammar for the Classical Sanskrit consist of four \"Paninian\" components. These, states Paul Kiparsky, are the \"Astadhyaayi\", a comprehensive system of 4,000 grammatical rules, of which a small set are frequently used; \"Sivasutras\", an inventory of \"anubandhas\" (markers) that partition phonological segments for efficient abbreviations through the \"pratyharas\" technique; \"Dhatupatha\", a list of 2,000 verbal roots classified by their morphology and syntactic properties using diacritic markers, a structure that guides its writing systems; and, the \"Ganapatha\", an inventory of word groups, classes of lexical systems. There are peripheral adjuncts to these four, such as the \"Unadisutras\", which focus on irregularly formed derivatives from the roots.\nSanskrit morphology is generally studied in two broad fundamental categories: the nominal forms and the verbal forms. These differ in the types of endings and what these endings mark in the grammatical context. Pronouns and nouns share the same grammatical categories, though they may differ in inflection. Verb-based adjectives and participles are not formally distinct from nouns. Adverbs are typically frozen case forms of adjectives, states Jamison, and \"nonfinite verbal forms such as infinitives and gerunds also clearly show frozen nominal case endings\".\nVerbal forms.\nThe Sanskrit language includes five tenses: present, future, past imperfect, past aorist and past perfect. It outlines three types of voices: active, passive and the middle. The middle is also referred to as the mediopassive, or more formally in Sanskrit as (word for another) and (word for oneself).\nThe paradigm for the tense-aspect system in Sanskrit is the three-way contrast between the \"present\", the \"aorist\" and the \"perfect\" architecture. Vedic Sanskrit is more elaborate and had several additional tenses. For example, the \"Rigveda\" includes perfect and a marginal pluperfect. Classical Sanskrit simplifies the \"present\" system down to two tenses, the perfect and the imperfect, while the \"aorist\" stems retain the aorist tense and the \"perfect\" stems retain the perfect and marginal pluperfect. The classical version of the language has elaborate rules for both voice and the tense-aspect system to emphasize clarity, and this is more elaborate than in other Indo-European languages. The evolution of these systems can be seen from the earliest layers of the Vedic literature to the late Vedic literature.\nThe three verbal moods in Sanskrit are indicative, potential (optative), and imperative.\nNominal forms.\nSanskrit recognizes three numbers\u2014singular, dual, and plural. The dual is a fully functioning category, used beyond naturally paired objects such as hands or eyes, extending to any collection of two. The elliptical dual is notable in the Vedic Sanskrit, according to Jamison, where a noun in the dual signals a paired opposition. Illustrations include \"dy\u0101v\u0101\" (literally, \"the two heavens\" for heaven-and-earth), \"m\u0101tar\u0101\" (literally, \"the two mothers\" for mother-and-father). A verb may be singular, dual or plural, while the person recognized in the language are forms of \"I\", \"you\", \"he/she/it\", \"we\" and \"they\".\nThere are three persons in Sanskrit: first, second and third. Sanskrit uses the 3\u00d73 grid formed by the three numbers and the three persons parameters as the paradigm and the basic building block of its verbal system.\nThe Sanskrit language incorporates three genders: feminine, masculine and neuter. All nouns have inherent gender. With some exceptions, personal pronouns have no gender. Exceptions include demonstrative and anaphoric pronouns. Derivation of a word is used to express the feminine. Two most common derivations come from feminine-forming suffixes, the \"-\u0101-\" (\u0906, R\u0101dh\u0101) and \"-\u012b-\" (\u0908, Rukmin\u012b). The masculine and neuter are much simpler, and the difference between them is primarily inflectional. Similar affixes for the feminine are found in many Indo-European languages, states Burrow, suggesting links of the Sanskrit to its PIE heritage.\nPronouns in Sanskrit include the personal pronouns of the first and second persons, unmarked for gender, and a larger number of gender-distinguishing pronouns and adjectives. Examples of the former include \"ah\u00e1m\" (first singular), \"vay\u00e1m\" (first plural) and \"y\u016by\u00e1m\" (second plural). The latter can be demonstrative, deictic or anaphoric. Both the Vedic and Classical Sanskrit share the \"s\u00e1/t\u00e1m\" pronominal stem, and this is the closest element to a third person pronoun and an article in the Sanskrit language, states Jamison.\nProsody, metre.\nThe Sanskrit language formally incorporates poetic metres. By the late Vedic era, this developed into a field of study; it was central to the composition of the Hindu literature, including the later Vedic texts. This study of Sanskrit prosody is called \"chandas\", and is considered one of the six Vedangas, or limbs of Vedic studies.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;poem&gt;\nThere is no word without metre,\nnor is there any metre without words.\n&lt;/poem&gt;\n\u2014 \"Natya Shastra\"\nSanskrit metres include those based on a fixed number of syllables per verse, and those based on fixed number of morae per verse. The Vedic Sanskrit employs fifteen metres, of which seven are common, and the most frequent are three (8-, 11- and 12-syllable lines). The Classical Sanskrit deploys both linear and non-linear metres, many of which are based on syllables and others based on diligently crafted verses based on repeating numbers of morae (matra per foot).\nWriting system.\nThe early history of writing Sanskrit and other languages in ancient India is a problematic topic despite a century of scholarship, states Richard Salomon \u2013 an epigraphist and Indologist specializing in Sanskrit and Pali literature. The earliest possible script from South Asia is from the Indus Valley civilization (3rd/2nd millennium BCE), but this script \u2013 if it is a script \u2013 remains undeciphered. If any scripts existed in the Vedic period, they have not survived. Scholars generally accept that Sanskrit was spoken in an oral society, and that an oral tradition preserved the extensive Vedic and Classical Sanskrit literature. Other scholars such as Jack Goody argue that the Vedic Sanskrit texts are not the product of an oral society, basing this view by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek (Greco-Sanskrit), Serbian, and other cultures. This minority of scholars argue that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without having been written down.\n\"Lipi\" is the term in Sanskrit which means \"writing, letters, alphabet\". It contextually refers to scripts, the art or any manner of writing or drawing. The term, in the sense of a writing system, appears in some of the earliest Buddhist, Hindu, and Jaina texts. P\u0101\u1e47ini's \"Astadhyayi\", composed sometime around the 5th or 4th century BCE, for example, mentions \"lipi\" in the context of a writing script and education system in his times, but he does not name the script. Several early Buddhist and Jaina texts, such as the \"Lalitavistara S\u016btra\" and \"Pannavana Sutta\" include lists of numerous writing scripts in ancient India. The Buddhist texts list the sixty four \"lipi\" that the Buddha knew as a child, with the Brahmi script topping the list. \"The historical value of this list is however limited by several factors\", states Salomon. The list may be a later interpolation. The Jain canonical texts such as the \"Pannavana Sutta\" \u2013 probably older than the Buddhist texts \u2013 list eighteen writing systems, with the Brahmi topping the list and Kharotthi (Kharoshthi) listed as fourth. The Jaina text elsewhere states that the \"Brahmi is written in 18\u00a0different forms\", but the details are lacking. However, the reliability of these lists has been questioned and the empirical evidence of writing systems in the form of Sanskrit or Prakrit inscriptions dated prior to the 3rd\u00a0century\u00a0BCE has not been found. If the ancient surfaces for writing Sanskrit were palm leaves, tree bark and cloth \u2013 the same as those in later times \u2013 these have not survived. According to Salomon, many find it difficult to explain the \"evidently high level of political organization and cultural complexity\" of ancient India without a writing system for Sanskrit and other languages.\nThe oldest datable writing systems for Sanskrit are the Br\u0101hm\u012b script, the related Kharo\u1e63\u1e6dh\u012b script and the Brahmi derivatives. The Kharosthi was used in the northwestern part of South Asia and it became extinct, while the Brahmi was used all over the subcontinent along with regional scripts such as Old Tamil. Of these, the earliest records in the Sanskrit language are in Brahmi, a script that later evolved into numerous related Indic scripts for Sanskrit, along with Southeast Asian scripts (Burmese, Thai, Lao, Khmer, others) and many extinct Central Asian scripts such as those discovered along with the Kharosthi in the Tarim Basin of western China and in Uzbekistan. The most extensive inscriptions that have survived into the modern era are the rock edicts and pillar inscriptions of the 3rd\u00a0century\u00a0BCE Mauryan emperor Ashoka, but these are not in Sanskrit.\nScripts.\nOver the centuries, and across countries, a number of scripts have been used to write Sanskrit.\nBrahmi script.\nThe Brahmi script for writing Sanskrit is a \"modified consonant-syllabic\" script. The graphic syllable is its basic unit, and this consists of a consonant with or without diacritic modifications. Since the vowel is an integral part of the consonants, and given the efficiently compacted, fused consonant cluster morphology for Sanskrit words and grammar, the Brahmi and its derivative writing systems deploy ligatures, diacritics and relative positioning of the vowel to inform the reader how the vowel is related to the consonant and how it is expected to be pronounced for clarity. This feature of Brahmi and its modern Indic script derivatives makes it difficult to classify it under the main script types used for the writing systems for most of the world's languages, namely logographic, syllabic and alphabetic.\nNagari script.\nMany modern era manuscripts are written and available in the Nagari script, whose form is attestable to the 1st millennium\u00a0CE. The Nagari script is the ancestor of Devanagari (north India), Nandinagari (south India) and other variants. The N\u0101gar\u012b script was in regular use by 7th century\u00a0CE, and had fully evolved into Devanagari and Nandinagari scripts by about the end of the first millennium of the common era. The Devanagari script, states Banerji, became more popular for Sanskrit in India since about the 18th century. However, Sanskrit does have special historical connection to the Nagari script as attested by the epigraphical evidence.\nThe Nagari script used for Classical Sanskrit has the fullest repertoire of characters consisting of fourteen vowels and thirty three consonants. For Vedic Sanskrit, it has two more allophonic consonantal characters (the intervocalic \u0933 \"\u1e37a\", and \u0933\u094d\u0939 \"\u1e37ha\"). To communicate phonetic accuracy, it also includes several modifiers such as the \"anusvara\" dot and the \"visarga\" double dot, punctuation symbols and others such as the \"halanta\" sign.\nOther writing systems.\nOther scripts such as Gujarati, Bangla-Assamese, Odia and major south Indian scripts, states Salomon, \"have been and often still are used in their proper territories for writing Sanskrit\". These and many Indian scripts look different to the untrained eye, but the differences between Indic scripts is \"mostly superficial and they share the same phonetic repertoire and systemic features\", states Salomon. They all have essentially the same set of eleven to fourteen vowels and thirty-three consonants as established by the Sanskrit language and attestable in the Brahmi script. Further, a closer examination reveals that they all have the similar basic graphic principles, the same \"varnamala\" (literally, \"garland of letters\") alphabetic ordering following the same logical phonetic order, easing the work of historic skilled scribes writing or reproducing Sanskrit works across South Asia.\nIn the south, where Dravidian languages predominate, scripts used for Sanskrit include the Kannada, Telugu, Malayalam and Grantha alphabets.\nTransliteration and romanisation schemes.\nSince the late 18th century, Sanskrit has been transliterated using the Latin alphabet. The system most commonly used today is the IAST (International Alphabet of Sanskrit Transliteration), which has been the academic standard since 1888. ASCII-based transliteration schemes have also evolved because of difficulties representing Sanskrit characters in computer systems. These include Harvard-Kyoto and ITRANS, a transliteration scheme that is used widely on the Internet, especially in Usenet and in email, for considerations of speed of entry as well as rendering issues. With the wide availability of Unicode-aware web browsers, IAST has become common online. It is also possible to type using an alphanumeric keyboard and transliterate to Devanagari using software like Mac OS X's international support.\nLiterature.\nLiterature in Sanskrit can be broadly divided into texts composed in Vedic Sanskrit and the later Classical Sanskrit. Vedic Sanskrit is the language of the extensive liturgical works of the Vedic religion, which aside from the four Vedas, include the Br\u0101hma\u1e47as and the S\u016btras.\nThe Vedic literature that survives is entirely of a religious form, whereas works in Classical Sanskrit exist in a wide variety of fields including epics, lyric, drama, romance, fairytale, fables, grammar, civil and religious law, the science of politics and practical life, the science of love and sex, philosophy, medicine, astronomy, astrology and mathematics, and is largely secular in subject-matter.\nWhile Vedic literature is essentially optimistic in spirit, portraying man as strong and powerful capable of finding fulfilment both here and in the afterworld, the later literature is pessimistic, portraying humans as controlled by the forces of fate with worldly pleasures deemed the cause of misery. These fundamental differences in psychology are attributed to the absence of the doctrines of Karma and reincarnation in the Vedic period, notions which are very prevalent in later times.\nWorks.\nSanskrit has been written in various scripts on a variety of media such as palm leaves, cloth, paper, rock and metal sheets, from ancient times.\nLexicon.\nAs an Indo-European language, Sanskrit's core lexicon is inherited from Proto-Indo-European. Over time however, the language exhibits a tendency to shed many of these inherited words and borrow others in their place from other sources.\nIn the oldest Vedic literature, there are few such non-Indo-European words, but these progressively grow in volume.\nThe following are some of the old Indo-European words that eventually fade out of use in Sanskrit:\nDravidian lexical influence.\nThe sources of these new loanwords are many, and vary across the different regions of the Indian subcontinent. But of all influences on the lexicon of Sanskrit, the most important is Dravidian.\nThe following is a list of Dravidian entrants into Sanskrit lexicon, although some may have been contested:\nNominal-form preference.\nWhile Vedic and epic form of speech is largely cognate to that of other Indo-European languages such as Greek and Latin, later Sanskrit shows a tendency to move away from using verbal forms to nominal ones. Examples of nominal forms taking the place of conventional conjugation are:\nHowever the most notable development is the prolific use of word-compounding to express ideas normally conveyed by verbal forms and sub-clauses introduced by conjunctions.\nClassical Sanskrit's pre-eminent playwright K\u0101lid\u0101sa uses:\nInfluence on other languages.\nFor nearly 2,000 years, Sanskrit was the language of a cultural order that exerted influence across South Asia, Inner Asia, Southeast Asia, and to a certain extent East Asia. A significant form of post-Vedic Sanskrit is found in the Sanskrit of Indian epic poetry\u2014the \"Ramayana\" and \"Mahabharata\". The deviations from in the epics are generally considered to be on account of interference from Prakrits, or innovations, and not because they are pre-Paninian. Traditional Sanskrit scholars call such deviations \"\u0101r\u1e63a\" (\u0906\u0930\u094d\u0937), meaning 'of the \u1e5b\u1e63is', the traditional title for the ancient authors. In some contexts, there are also more \"prakritisms\" (borrowings from common speech) than in Classical Sanskrit proper. Buddhist Hybrid Sanskrit is a literary language heavily influenced by the Middle Indo-Aryan languages, based on early Buddhist Prakrit texts which subsequently assimilated to the Classical Sanskrit standard in varying degrees.\nIndian subcontinent.\nSanskrit has greatly influenced the languages of India that grew from its vocabulary and grammatical base; for instance, Hindi is a \"Sanskritised register\" of Hindustani. All modern Indo-Aryan languages, as well as Munda and Dravidian languages have borrowed many words either directly from Sanskrit (\"tatsama\" words), or indirectly via middle Indo-Aryan languages (\"tadbhava\" words). Words originating in Sanskrit are estimated at fifty percent of the vocabulary of modern Indo-Aryan languages, as well as the literary forms of Malayalam and Kannada. Literary texts in Telugu are lexically Sanskrit or Sanskritised to an enormous extent, perhaps seventy percent or more. Marathi is another prominent language in Western India, that derives most of its words and Marathi grammar from Sanskrit. Sanskrit words are often preferred in the literary texts in Marathi over corresponding colloquial Marathi word.\nThere has been a profound influence of Sanskrit on the lexical and grammatical systems of Dravidian languages. As per Dalby, India has been a single cultural area for about two millennia which has helped Sanskrit influence on all the Indic languages. Emeneau and Burrow mention the tendency \"for all four of the Dravidian literary languages in South to make literary use of total Sanskrit lexicon indiscriminately\". There are a large number of loanwords found in the vocabulary of the three major Dravidian languages Malayalam, Kannada and Telugu. Tamil also has significant loanwords from Sanskrit. Krishnamurthi mentions that although it is not clear when the Sanskrit influence happened on the Dravidian languages, it might have been around the 5th century BCE at the time of separation of Tamil and Kannada from a common ancestral stage. \u200cThe borrowed words are classified into two types based on phonological integration \u2013 \"tadbhava\" \u2013 those words derived from Prakrit and \"tatsama\" \u2013 unassimilated loanwords from Sanskrit.\nStrazny mentions that \"so massive has been the influence that it is hard to utter Sanskrit words have influenced Kannada from the early times\". The first document in Kannada, the Halmidi inscription has a large number of Sanskrit words. As per Kachru, the influence has not only been on single lexical items in Kannada but also on \"long nominal compounds and complicated syntactic expressions\". New words have been created in Kannada using Sanskrit derivational prefixes and suffixes like \"vik\u0113ndr\u012bkara\u1e47a, anil\u012bkara\u1e47a, bah\u012bskru\u1e6da\". Similar stratification is found in verb morphology. Sanskrit words readily undergo verbalization in Kannada, verbalizing suffixes as in: \"ch\u0101pisu, dau\u1e0d\u0101yisu, rav\u0101nisu.\"\nGeorge mentions that \"No other Dravidian language has been so deeply influenced by Sanskrit as Malayalam\". According to Lambert, Malayalam is so immensely Sanskritised that every Sanskrit word can be used in Malayalam by integrating \"prosodic phonological\" changes as per Grant. Loanwords have been integrated into Malayalam by \"prosodic phonological\" changes as per Grant. These phonological changes are either by replacement of a vowel as in \"sant-\"am coming from Sanskrit \"santa\", \"s\u0101gar\"-am from \"s\u0101gara\", or addition of prothetic vowel as in \"aracan\" from \"r\u0101j\u0101-\", \"uruvam\" from \"r\u016bpa\", \"codyam\" from \"sodhya\".\nHans Henrich et al. note that, the language of the pre-modern Telugu literature was also highly influenced by Sanskrit and was standardized between 11th and 14th\u00a0centuries. Aiyar has shown that in a class of \"tadbhavas\" in Telugu the first and second letters are often replaced by the third and fourth letters and fourth again replaced often by h. Examples of the same are: Sanskrit \"artha\" becomes \"ardhama\", \"v\u012bthi\" becomes \"vidhi\", \"putra\" becomes \"bidda\", \"mukham\" becomes \"muhamu\".\nTamil also has been influenced by Sanskrit. Hans Henrich et al. mention that propagation of Jainism and Buddhism into south India had its influence. Shulman mentions that although contrary to the views held by Tamil purists, modern Tamil has been significantly influenced from Sanskrit, further states that \"Indeed, there may well be more Sanskrit in Tamil than in the Sanskrit derived north-Indian vernaculars\". Sanskrit words have been Tamilized through the \"Tamil phonematic grid\".\nBeyond the Indian subcontinent.\nSanskrit was a language for religious purposes and for the political elite in parts of medieval era Southeast Asia, Central Asia and East Asia, having been introduced in these regions mainly along with the spread of Buddhism. In some cases, it has competed with P\u0101li for prominence.\nEast Asia.\nBuddhist Sanskrit has had a considerable influence on Sino-Tibetan languages such as Chinese, state William Wang and Chaofen Sun. Many words have been adopted from Sanskrit into Chinese, both in its historic religious discourse and everyday use. This process likely started about 200\u00a0CE and continued through about 1400\u00a0CE, with the efforts of monks such as Yuezhi, Anxi, Kangju, Tianzhu, Yan Fodiao, Faxian, Xuanzang and Yijing.\nFurther, as the Chinese languages and culture influenced the rest of East Asia, the ideas in Sanskrit texts and some of its linguistic elements migrated further.\nMany terms were transliterated directly and added to the Chinese vocabulary. Chinese words like \"ch\u00e0n\u00e0\" (Devanagari: \u0915\u094d\u0937\u0923 \"\" 'instantaneous period') were borrowed from Sanskrit. Many Sanskrit texts survive only in Tibetan collections of commentaries to the Buddhist teachings, the Tengyur.\nSanskrit has also influenced the religious register of Japanese mostly through transliterations. These were borrowed from Chinese transliterations. In particular, the Shingon (lit.\u2009'True Words') sect of esoteric Buddhism has been relying on Sanskrit and original Sanskrit mantras and writings, as a means of realizing Buddhahood.\nSoutheast Asia.\nA large number of inscriptions in Sanskrit across Southeast Asia testify the influence the language held in these regions.\nLanguages such as Indonesian, Thai and Lao contain many loanwords from Sanskrit, as does Khmer. Many Sanskrit loanwords are also found in Austronesian languages, such as Javanese, particularly the older form in which nearly half the vocabulary is borrowed.\nOther Austronesian languages, such as Malay (descended into modern Malaysian and Indonesian standards) also derive much of their vocabulary from Sanskrit. Similarly, Philippine languages such as Tagalog have some Sanskrit loanwords, although more are derived from Spanish.\nA Sanskrit loanword encountered in many Southeast Asian languages is the word \"bh\u0101\u1e63\u0101\", or spoken language, which is used to refer to the names of many languages.\nTo this day, Southeast Asian languages such as Thai are known to draw upon Sanskrit for technical vocabulary.\nIndonesia.\nThe earliest Sanskrit text which was founded in the Indonesian Archipelago was at Eastern Borneo dating back to 400 CE known as the Mulavarman inscription. This is one of the reason of strong influence of Indian culture that entered the Malay Archipelago during the Indianization era, and since then, Indian culture has been absorbed towards Indonesian culture and language. Thus, the Sanskrit culture in Indonesia exists not as a religious aspect but more towards a cultural aspect that has been present for generations, resulting in a more cultural rather than Hinduistic value of the Indonesian people. As a result, it is common to find Muslim or Christian Indonesians with names that have Indian or Sanskrit nuances. Unlike names derived from Sanskrit in Thai and Khmer, the pronunciation of Sanskrit names in Indonesia is more similar to the original Indian pronunciation, except that \"v\" is changed to \"w\", for example, \"Vishnu\" in India will be spelled \"Wisnu\" in Indonesia.\nRest of the world.\nIn ancient and medieval times, several Sanskrit words in the field of food and spices made their way into European languages including Greek, Latin and later English. Some of these are \"pepper\", \"ginger\" and \"sugar\". English today has several words of Sanskrit origin, most of them borrowed during the British Raj or later. Some of these words have in turn been borrowed by other European or world languages.\nModern era.\nLiturgy, ceremonies and meditation.\nSanskrit is the sacred language of various Hindu, Buddhist, and Jain traditions. It is used during worship in Hindu temples. In Newar Buddhism, it is used in all monasteries. Sanskrit mantras and Sanskrit as a ritual language was commonplace among Jains throughout their medieval history.\nMany Hindu rituals and rites-of-passage such as the \"giving away the bride\" and mutual vows at weddings, a baby's naming or first solid food ceremony and the goodbye during a cremation invoke and chant Sanskrit hymns. Major festivals such as the \"Durga Puja\" ritually recite entire Sanskrit texts such as the \"Devi Mahatmya\" every year particularly among the numerous communities of eastern India. In the south, Sanskrit texts are recited at many major Hindu temples such as the Meenakshi Temple. In India and beyond, recitations of the \"Bhagavad Gita\" occur in diverse settings, including \"simple private household readings, to family and neighborhood recitation sessions, to holy men reciting in temples or at pilgrimage places for passersby, to public Gita discourses held almost nightly at halls and auditoriums in every Indian city\".\nLiterature and arts.\nMore than 3,000 Sanskrit works have been composed since India's independence in 1947. Much of this work has been judged of high quality, in comparison to both classical Sanskrit literature and modern literature in other Indian languages. In 2009, Satya Vrat Shastri became the first Sanskrit author to win the Jnanpith Award, India's highest literary award.\nSanskrit is used extensively in the Carnatic and Hindustani branches of classical music. Kirtanas, bhajans, stotras, and shlokas of Sanskrit are popular throughout India. The Samaveda uses musical notations in several of its recessions.\nIn Mainland China, musicians such as Sa Dingding have written pop songs in Sanskrit.\nNumerous loan Sanskrit words are found in other major Asian languages. For example, Filipino, Cebuano, Lao, Khmer, Thai and its alphabets, Malay (including Malaysian and Indonesian), Javanese (old Javanese-English dictionary by P.J. Zoetmulder contains over 25,500\u00a0entries), and even in English.\nMedia.\nSince 1974, there has been a short daily news broadcast on All India Radio. These broadcasts are also made available on the internet on AIR's website. Sanskrit news is broadcast on TV and on the internet through the DD National channel. Over 90 weeklies, fortnightlies and quarterlies are published in Sanskrit. \"Sudharma\", a daily printed newspaper in Sanskrit, has been published out of Mysore, India, since 1970. It was started by K.N. Varadaraja Iyengar, a Sanskrit scholar from Mysore.\nSchools and contemporary status.\nSanskrit has been taught in schools from time immemorial in India. In modern times, the first Sanskrit University was Sampurnanand Sanskrit University, established in 1791 in the Indian city of Varanasi. Sanskrit is taught in 5,000\u00a0traditional schools (Pathashalas), and 14,000\u00a0schools in India. Sanskrit is one of the 22\u00a0scheduled languages of India. Despite it being a studied school subject in contemporary India, Sanskrit has not been spoken as a native language in centuries.\nIn India, Sanskrit is offered as a language in central and several state education board schools and is also taught in traditional gurukulas across the country. A number of colleges and universities in India have dedicated departments for Sanskrit studies. In March 2020, the Indian Parliament passed the \"Central Sanskrit Universities Act, 2020\" which upgraded three universities, National Sanskrit University, Central Sanskrit University and Shri Lal Bahadur Shastri National Sanskrit University, from the deemed to be university status to a central university status.\nDmitri Mendeleev used the Sanskrit numbers of one, two and three (\"eka-\", \"dvi-\" or \"dwi-\", and \"tri-\" respectively) to give provisional names to his predicted elements, like eka-boron being Gallium or eka-Francium being Ununennium.\nIn the province of Bali in Indonesia, a number of educational and scholarly institutions have also been conducting Sanskrit lessons for Hindu locals.\nIn the West.\nSt James Junior School and Avanti Schools Trust in London, England, offer Sanskrit as part of the curriculum. Since September 2009, US high school students have been able to receive credits as Independent Study or toward Foreign Language requirements by studying Sanskrit as part of the \"SAFL: Samskritam as a Foreign Language\" program coordinated by Samskrita Bharati. In Australia, the private boys' high school Sydney Grammar School offers Sanskrit from years\u00a07 through to 12, including for the Higher School Certificate. Other schools that offer Sanskrit include the Ficino School in Auckland, New Zealand; St James Preparatory Schools in Cape Town, Durban and Johannesburg, South Africa; John Colet School, Sydney, Australia; Erasmus School, Melbourne, Australia.\nEuropean studies and discourse.\nWhile European scholarship in Sanskrit, begun by Heinrich Roth (1620\u20131668) and Johann Ernst Hanxleden (1681\u20131731), is considered responsible for the discovery of an Indo-European language family by Sir William Jones (1746\u20131794) (with this research playing an important role in the development of Western philology, or historical linguistics), the first scholar to suggest this was the Delhi-based Mughal Empire scholar Siraj-ud-Din Ali Khan Arzu (1687-1756) in his Persian-language philological treatise \"Muzmir\" (\"Fruitful\").\nThe 18th- and 19th-century speculations about the possible links of Sanskrit to ancient Egyptian language were later proven to be wrong, but it fed an orientalist discourse both in the form Indophobia and Indophilia, states Trautmann. Sanskrit writings, when first discovered, were imagined by Indophiles to potentially be \"repositories of the primitive experiences and religion of the human race, and as such confirmatory of the truth of Christian scripture\", as well as a key to \"universal ethnological narrative\". The Indophobes imagined the opposite, making the counterclaim that there is little of any value in Sanskrit, portraying it as \"a language fabricated by artful [Brahmin] priests\", with little original thought, possibly copied from the Greeks who came with Alexander or perhaps the Persians.\nScholars such as William Jones and his colleagues felt the need for systematic studies of Sanskrit language and literature. This launched the Asiatic Society, an idea that was soon transplanted to Europe starting with the efforts of Henry Thomas Colebrooke in Britain, then Alexander Hamilton who helped expand its studies to Paris and thereafter his student Friedrich Schlegel who introduced Sanskrit to the universities of Germany. Schlegel nurtured his own students into influential European Sanskrit scholars, particularly through Franz Bopp and Friedrich Max M\u00fcller. As these scholars translated the Sanskrit manuscripts, the enthusiasm for Sanskrit grew rapidly among European scholars, states Trautmann, and chairs for Sanskrit \"were established in the universities of nearly every German statelet\" creating a competition for Sanskrit experts.\nSymbolic usage.\nIn India, Indonesia, Nepal, Bangladesh, Sri Lanka, and Southeast Asia, Sanskrit phrases are widely used as mottoes for various national, educational and social organisations:\nIn November 2020, Gaurav Sharma, a New Zealand politician of Indian origin swore into parliament using Sanskrit alongside M\u0101ori; the decision was made as a \"homage to all Indian languages\" compromising between his native Pahari and Punjabi.\nIn popular culture.\nThe song \"My Sweet Lord\" by George Harrison includes The Hare Krishna mantra, also referred to reverentially as the Maha Mantra, a 16-word Vaishnava mantra which is mentioned in the Kali-Santarana Upanishad. \"Satyagraha\", an opera by Philip Glass, uses texts from the \"Bhagavad Gita\", sung in Sanskrit. In 1996, English psychedelic rock band Kula Shaker released \"Govinda\", a song entirely sung in Sanskrit. The closing credits of \"The Matrix Revolutions\" has a prayer from the \"Brihadaranyaka Upanishad\". The song \"Cyber-raga\" from Madonna's album \"Music\" includes Sanskrit chants, and \"Shanti/Ashtangi\" from her 1998 album \"Ray of Light\", which won a Grammy, is the ashtanga vinyasa yoga chant. The lyrics include the mantra \"Om shanti\". Composer John Williams featured choirs singing in Sanskrit for \"Indiana Jones and the Temple of Doom\" and in \"\". The theme song of \"Battlestar Galactica 2004\" is the Gayatri Mantra, taken from the Rigveda. The lyrics of \"The Child in Us\" by Enigma also contain Sanskrit verses. In 2006, Mexican singer Paulina Rubio was influenced in Sanskrit for her concept album \"Ananda\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "27699", "revid": "48787195", "url": "https://en.wikipedia.org/wiki?curid=27699", "title": "Sign language", "text": "Language that uses manual communication and body language to convey meaning\nSign languages (also known as signed languages) are languages that use the visual-manual modality to convey meaning, instead of spoken words. Sign languages are expressed through manual articulation in combination with non-manual markers. Sign languages are full-fledged natural languages with their own grammar and lexicon. Sign languages are not universal and are usually not mutually intelligible, although there are similarities among different sign languages.\nWherever communities of people with hearing challenges or people who experience deafness exist, sign languages have developed as useful means of communication and form the core of local deaf cultures. Although signing is used primarily by the deaf and hard of hearing, it is also used by hearing individuals, such as those with deaf family members including children of deaf adults.\nSign language should not be confused with body language, a type of nonverbal communication. Linguists also distinguish natural sign languages from other systems that are precursors to them or obtained from them, such as constructed manual codes for spoken languages, home sign, \"baby sign\", and signs learned by non-human primates.\nThe number of sign languages worldwide is not precisely known. Each country generally has its own native sign language and some have more than one. The 2021 edition of \"Ethnologue\" lists 150 sign languages, while the SIGN-HUB Atlas of Sign Language Structures lists over 200 and notes that there are more that have not been documented or discovered yet. As of 2021, Indo-Pakistani Sign Language is the most-used sign language in the world, and \"Ethnologue\" ranks it as the 151st most \"spoken\" language in the world.\nSome sign languages have obtained some form of legal recognition.\nHistory.\nGroups of deaf people have used sign languages throughout history. One of the earliest written records of a sign language is from the fifth century BC, in Plato's \"Cratylus\", where Socrates says: \"If we hadn't a voice or a tongue, and wanted to express things to one another, wouldn't we try to make signs by moving our hands, head, and the rest of our body, just as dumb people do at present?\" Most of what is known about pre-19th-century sign languages is limited to the manual alphabets (fingerspelling systems) that were invented to facilitate the transfer of words from a spoken language to a sign language, rather than documentation of the language itself.\nDebate around European monastic sign languages developed in the Middle Ages has come to regard them as gestural systems rather than true sign languages. Monastic sign languages were the basis for the first known manual alphabet used in deaf schools, developed by Pedro Ponce de Le\u00f3n.\nThe earliest records of contact between Europeans and Indigenous peoples of the Gulf Coast region in what is now Texas and northern Mexico note a fully formed sign language already in use by the time of the Europeans' arrival there. These records include the accounts of Cabeza de Vaca in 1527 and Coronado in 1541.\nIn 1620, Juan Pablo Bonet published ('Reduction of letters and art for teaching mute people to speak') in Madrid. It is considered the first modern treatise of sign language phonetics, setting out a method of oral education for deaf people and a manual alphabet.\nIn Britain, manual alphabets were also in use for a number of purposes, such as secret communication, public speaking, or communication by or with deaf people. In 1648, John Bulwer described \"Master Babington\", a deaf man proficient in the use of a manual alphabet, \"contryved on the joynts of his fingers\", whose wife could converse with him easily, even in the dark through the use of tactile signing.\nIn 1680, George Dalgarno published \"Didascalocophus, or, The deaf and dumb mans tutor\", in which he presented his own method of deaf education, including an \"arthrological\" alphabet, where letters are indicated by pointing to different joints of the fingers and palm of the left hand. Arthrological systems had been in use by hearing people for some time; some have speculated that they can be traced to early Ogham manual alphabets.\nThe vowels of this alphabet have survived in the modern alphabets used in British Sign Language, Auslan and New Zealand Sign Language. The earliest known printed pictures of consonants of the modern two-handed alphabet appeared in 1698 with (Latin for \"Language\" [or \"Tongue\"] \"of the Finger\"), a pamphlet by an anonymous author who was himself unable to speak. He suggested that the manual alphabet could also be used by mutes, for silence and secrecy, or purely for entertainment. Nine of its letters can be traced to earlier alphabets, and 17 letters of the modern two-handed alphabet can be found among the two sets of 26 handshapes depicted.\nCharles de La Fin published a book in 1692 describing an alphabetic system where pointing to a body part represented the first letter of the part (e.g. Brow=B), and vowels were located on the fingertips as with the other British systems. He described such codes for both English and Latin.\nBy 1720, the British manual alphabet had found more or less its present form. Descendants of this alphabet have been used by deaf communities, at least in education, in the former British colonies India, Australia, New Zealand, Uganda and South Africa, as well as the republics and provinces of the former Yugoslavia, Grand Cayman Island in the Caribbean, Indonesia, Norway, Germany and the United States.\nDuring the Polygar Wars against the British, Veeran Sundaralingam communicated with Veerapandiya Kattabomman's mute younger brother, Oomaithurai, by using their own sign language.\nFrenchman Charles-Michel de l'\u00c9p\u00e9e published his manual alphabet in the 18th century, which has survived largely unchanged in France and North America until the present time. In 1755, Abb\u00e9 de l'\u00c9p\u00e9e founded the first school for deaf children in Paris; Laurent Clerc was arguably its most famous graduate. Clerc went to the United States with Thomas Hopkins Gallaudet to found the American School for the Deaf in Hartford, Connecticut, in 1817. Gallaudet's son, Edward Miner Gallaudet, founded a school for the deaf in 1857 in Washington, D.C., which in 1864 became the National Deaf-Mute College. Now called Gallaudet University, it is still the only liberal arts university for deaf people in the world.\nInternational Sign, formerly known as Gestuno, is used mainly at international deaf events such as the Deaflympics and meetings of the World Federation of the Deaf. While recent studies claim that International Sign is a kind of a pidgin, they conclude that it is more complex than a typical pidgin and indeed is more like a full sign language.\nLinguistics.\nSign languages have capability and complexity equal to spoken languages; their study as part of the field of linguistics has demonstrated that they exhibit the fundamental properties that exist in all languages. Such fundamental properties include duality of patterning and recursion. Duality of patterning means that languages are composed of smaller, meaningless units which can be combined into larger units with meaning (see below). The term recursion means that languages exhibit grammatical rules and the output of such a rule can be the input of the same rule. It is, for example, possible in sign languages to create subordinate clauses and a subordinate clause may contain another subordinate clause.\nSign languages are not mime\u2014in other words, signs are conventional, often arbitrary and do not necessarily have a visual relationship to their referent, much as most spoken language is not onomatopoeic. While iconicity is more systematic and widespread in sign languages than in spoken ones, the difference is not categorical. The visual modality allows the human preference for close connections between form and meaning, to be more fully expressive, where as this is more suppressed in spoken language.\nSign languages, like spoken languages, organize elementary, meaningless units into meaningful semantic units. This type of organization in natural language is often called duality of patterning. As in spoken languages, these meaningless units are represented as (combinations of) features, although coarser descriptions are often also made in terms of five \"parameters\": handshape (or \"handform\"), orientation, location (or \"place of articulation\"), movement, and non-manual expression. These meaningless units in sign languages were initially called cheremes, from the Greek word for \"hand\", by analogy to the phonemes, from Greek for \"voice\", of spoken languages. Now they are sometimes called phonemes when describing sign languages too, since the function is essentially the same, but more commonly discussed in terms of \"features\" or \"parameters\". More generally, both sign and spoken languages share the characteristics that linguists have found in all natural human languages, such as transitoriness, semanticity, arbitrariness, productivity, and cultural transmission.\nCommon linguistic features of many sign languages are the occurrence of classifier constructions, a high degree of inflection by means of changes of movement, and a topic-comment syntax. More than spoken languages, sign languages can convey meaning by simultaneous means, e.g. by the use of space, two manual articulators, and the signer's face and body. Though there is still much discussion on the topic of iconicity in sign languages, classifiers are generally considered to be highly iconic, as these complex constructions \"function as predicates that may express any or all of the following: motion, position, stative-descriptive, or handling information\". The term classifier is not used by everyone working on these constructions. Across the field of sign language linguistics the same constructions are also referred with other terms such as depictive signs.\nToday, linguists study sign languages as true languages, part of the field of linguistics. However, the category \"sign languages\" was not added to the \"Linguistic Bibliography/Bibliographie Linguistique\" until the 1988 volume, when it appeared with 39 entries.\nRelationships with spoken languages.\nThere is a common misconception that sign languages are spoken language expressed in signs, or that they were invented by hearing people. Similarities in language processing in the brain between signed and spoken languages further perpetuated this misconception. Hearing teachers in deaf schools, such as Charles-Michel de l'\u00c9p\u00e9e or Thomas Hopkins Gallaudet, are often incorrectly referred to as \"inventors\" of sign language. Instead, sign languages, like all natural languages, are developed by the people who use them, in this case, deaf people, who may have little or no knowledge of any spoken language.\nAs a sign language develops, it sometimes borrows elements from spoken languages, just as all languages borrow from other languages that they are in contact with. Sign languages vary in how much they borrow from spoken languages. In many sign languages, a manual alphabet (\"fingerspelling\") may be used in signed communication to borrow a word from a spoken language. This is most commonly used for proper names of people and places; it is also used in some languages for concepts for which no sign is available at that moment, particularly if the people involved are to some extent bilingual in the spoken language. Fingerspelling can sometimes be a source of new signs, such as initialized signs, in which the handshape represents the first letter of a spoken word with the same meaning.\nOn the whole, though, sign languages are independent of spoken languages and follow their own paths of development. For example, British Sign Language (BSL) and American Sign Language (ASL) are quite different and mutually unintelligible, even though the hearing people of the United Kingdom and the United States share the same spoken language. The grammars of sign languages do not usually resemble those of spoken languages used in the same geographical area; in fact, in terms of syntax, ASL shares more with spoken Japanese than it does with English.\nSimilarly, countries which use a single spoken language throughout may have two or more sign languages, or an area that contains more than one spoken language might use only one sign language. South Africa, which has 11 official spoken languages and a similar number of other widely used spoken languages, is a good example of this. It has only one sign language with two variants due to its history of having two major educational institutions for the deaf which have served different geographic areas of the country.\nSpatial grammar and simultaneity.\nSign languages exploit the unique features of the visual medium (sight), but may also exploit tactile features (tactile sign languages). Spoken language is by and large linear; only one sound can be made or received at a time. Sign language, on the other hand, is visual and, hence, can use a simultaneous expression, although this is limited articulatorily and linguistically. Visual perception allows processing of simultaneous information.\nOne way in which many sign languages take advantage of the spatial nature of the language is through the use of classifiers. Classifiers allow a signer to spatially show a referent's type, size, shape, movement, or extent.\nThe possible simultaneity of sign languages in contrast to spoken languages is sometimes exaggerated. The use of two manual articulators is subject to motor constraints, resulting in a large extent of symmetry or signing with one articulator only. Further, sign languages, just like spoken languages, depend on linear sequencing of signs to form sentences; the greater use of simultaneity is mostly seen in the morphology (internal structure of individual signs).\nNon-manual elements.\nSign languages convey much of their prosody through non-manual elements. Postures or movements of the body, head, eyebrows, eyes, cheeks, and mouth are used in various combinations to show several categories of information, including lexical distinction, grammatical structure, adjectival or adverbial content, and discourse functions.\nAt the lexical level, signs can be lexically specified for non-manual elements in addition to the manual articulation. For instance, facial expressions may accompany verbs of emotion, as in the sign for \"angry\" in Czech Sign Language. Non-manual elements may also be lexically contrastive. For example, in ASL (American Sign Language), facial components distinguish some signs from other signs. An example is the sign translated as \"not yet\", which requires that the tongue touch the lower lip and that the head rotate from side to side, in addition to the manual part of the sign. Without these features the sign would be interpreted as \"late\". Mouthings, which are (parts of) spoken words accompanying lexical signs, can also be contrastive, as in the manually identical signs for \"doctor\" and \"battery\" in Sign Language of the Netherlands.\nWhile the content of a signed sentence is produced manually, many grammatical functions are produced non-manually (i.e., with the face and the torso). Such functions include questions, negation, relative clauses and topicalization. ASL and BSL use similar non-manual marking for yes/no questions, for example. They are shown through raised eyebrows and a forward head tilt.\nSome adjectival and adverbial information is conveyed through non-manual elements, but what these elements are varies from language to language. For instance, in ASL a slightly open mouth with the tongue relaxed and visible in the corner of the mouth means \"carelessly\", but a similar non-manual in BSL means \"boring\" or \"unpleasant\".\nDiscourse functions such as turn taking are largely regulated through head movement and eye gaze. Since the addressee in a signed conversation must be watching the signer, a signer can avoid letting the other person have a turn by not looking at them, or can indicate that the other person may have a turn by making eye contact.\nIconicity.\nIconicity is similarity or analogy between the form of a sign (linguistic or otherwise) and its meaning, as opposed to arbitrariness. The first studies on iconicity in ASL were published in the late 1970s and early 1980s. Many early sign language linguists rejected the notion that iconicity was an important aspect of sign languages, considering most perceived iconicity to be extralinguistic. However, mimetic aspects of sign language (signs that imitate, mimic, or represent) are found in abundance across a wide variety of sign languages. For example, when deaf children learning sign language try to express something but do not know the associated sign, they will often invent an iconic sign that displays mimetic properties. Though it never disappears from a particular sign language, iconicity is gradually weakened as forms of sign languages become more customary and are subsequently grammaticized. As a form becomes more conventional, it becomes disseminated in a methodical way phonologically to the rest of the sign language community. Nancy Frishberg concluded that though originally present in many signs, iconicity is degraded over time through the application of natural grammatical processes.\nIn 1978, psychologist Roger Brown was one of the first to suggest that the properties of ASL give it a clear advantage in terms of learning and memory. In his study, Brown found that when a group of six hearing children were taught signs that had high levels of iconic mapping they were significantly more likely to recall the signs in a later memory task than another group of six children that were taught signs that had little or no iconic properties. In contrast to Brown, linguists Elissa Newport and Richard Meier found that iconicity \"appears to have virtually no impact on the acquisition of American Sign Language\".\nA central task for the pioneers of sign language linguistics was trying to prove that ASL was a real language and not merely a collection of gestures or \"English on the hands\". One of the prevailing beliefs at this time was that \"real languages\" must consist of an arbitrary relationship between form and meaning. Thus, if ASL consisted of signs that had iconic form-meaning relationship, it could not be considered a real language. As a result, iconicity as a whole was largely neglected in research of sign languages for a long time. However, iconicity also plays a role in many spoken languages. Spoken Japanese for example exhibits many words mimicking the sounds of their potential referents (see Japanese sound symbolism). Later researchers, thus, acknowledged that natural languages do not need to consist of an arbitrary relationship between form and meaning. The visual nature of sign language simply allows for a greater degree of iconicity compared to spoken languages as most real-world objects can be described by a prototypical shape (e.g., a table usually has a flat surface), but most real-world objects do not make prototypical sounds that can be mimicked by spoken languages (e.g., tables do not make prototypical sounds). However, sign languages are not fully iconic. On the one hand, there are also many arbitrary signs in sign languages and, on the other hand, the grammar of a sign language puts limits to the degree of iconicity: All known sign languages, for example, express lexical concepts via manual signs. From a truly iconic language one would expect that a concept like smiling would be expressed by mimicking a smile (i.e., by performing a smiling face). All known sign languages, however, do not express the concept of smiling by a smiling face, but by a manual sign.\nThe cognitive linguistics perspective rejects a more traditional definition of iconicity as a relationship between linguistic form and a concrete, real-world referent. Rather it is a set of selected correspondences between the form and meaning of a sign. In this view, iconicity is grounded in a language user's mental representation (\"construal\" in cognitive grammar). It is defined as a fully grammatical and central aspect of a sign language rather than a peripheral phenomenon.\nThe cognitive linguistics perspective allows for some signs to be fully iconic or partially iconic given the number of correspondences between the possible parameters of form and meaning. In this way, the Israeli Sign Language (ISL) sign for \"ask\" has parts of its form that are iconic (\"movement away from the mouth\" means \"something coming from the mouth\"), and parts that are arbitrary (the handshape, and the orientation).\nMany signs have metaphoric mappings as well as iconic or metonymic ones. For these signs there are three-way correspondences between a form, a concrete source and an abstract target meaning. The ASL sign LEARN has this three-way correspondence. The abstract target meaning is \"learning\". The concrete source is putting objects into the head from books. The form is a grasping hand moving from an open palm to the forehead. The iconic correspondence is between form and concrete source. The metaphorical correspondence is between concrete source and abstract target meaning. Because the concrete source is connected to two correspondences linguistics refer to metaphorical signs as \"double mapped\".\nClassification.\nSign languages may be classified by how they arise.\nIn non-signing communities, home sign is not a full language, but closer to a pidgin. Home sign is amorphous and generally idiosyncratic to a particular family, where a deaf child does not have contact with other deaf children and is not educated in sign. Such systems are not generally passed on from one generation to the next. Where they are passed on, creolization would be expected to occur, resulting in a full language. However, home sign may also be closer to full language in communities where the hearing population has a gestural mode of language; examples include various Australian Aboriginal sign languages and gestural systems across West Africa, such as Mofu-Gudur in Cameroon.\nA village sign language is a local indigenous language that typically arises over several generations in a relatively insular community with a high incidence of deafness, and is used both by the deaf and by a significant portion of the hearing community, who have deaf family and friends. The most famous of these is probably the extinct Martha's Vineyard Sign Language of the U.S., but there are also numerous village languages scattered throughout Africa, Asia, and America.\nDeaf-community sign languages, on the other hand, arise where deaf people come together to form their own communities. These include school sign, such as Nicaraguan Sign Language, which develop in the student bodies of deaf schools which do not use sign as a language of instruction, as well as community languages such as Bamako Sign Language, which arise where generally uneducated deaf people congregate in urban centers for employment. At first, Deaf-community sign languages are not generally known by the hearing population, in many cases not even by close family members. However, they may grow, in some cases becoming a language of instruction and receiving official recognition, as in the case of ASL.\nBoth contrast with speech-taboo languages such as the various Aboriginal Australian sign languages, which are developed by the hearing community and only used secondarily by the deaf. It is doubtful whether most of these are languages in their own right, rather than manual codes of spoken languages, though a few such as Yolngu Sign Language are independent of any particular spoken language. Hearing people may also develop sign to communicate with users of other languages, as in Plains Indian Sign Language; this was a contact signing system or pidgin that was evidently not used by deaf people in the Plains nations, though it presumably influenced home sign.\nLanguage contact and creolization is common in the development of sign languages, making clear family classifications difficult\u2013 it is often unclear whether lexical similarity is due to borrowing or a common parent language, or whether there was one or several parent languages, such as several village languages merging into a Deaf-community language. Contact occurs between sign languages, between sign and spoken languages (contact sign, a kind of pidgin), and between sign languages and gestural systems used by the broader community. For example, Adamorobe Sign Language, a village sign language of Ghana, may be related to the \"gestural trade jargon used in the markets throughout West Africa\", in vocabulary and areal features including prosody and phonetics.\nThe only comprehensive classification along these lines going beyond a simple listing of languages dates back to 1991. The classification is based on the 69 sign languages from the 1988 edition of Ethnologue that were known at the time of the 1989 conference on sign languages in Montreal and 11 more languages the author added after the conference.\nIn his classification, the author distinguishes between primary and auxiliary sign languages as well as between single languages and names that are thought to refer to more than one language. The prototype-A class of languages includes all those sign languages that seemingly cannot be derived from any other language. Prototype-R languages are languages that are remotely modelled on a prototype-A language (in many cases thought to have been French Sign Language) by a process Kroeber (1940) called \"stimulus diffusion\". The families of BSL, DGS, JSL, LSF (and possibly LSG) were the products of creolization and relexification of prototype languages. Creolization is seen as enriching overt morphology in sign languages, as compared to reducing overt morphology in spoken languages.\nTypology.\nSign languages vary in word-order typology. For example, Austrian Sign Language, Japanese Sign Language and Indo-Pakistani Sign Language are Subject-object-verb while ASL is Subject-verb-object. Influence from the surrounding spoken languages is plausible.\nSign languages tend to be incorporating classifier languages, where a classifier handshape representing the object is incorporated into those transitive verbs which allow such modification. For a similar group of intransitive verbs (especially motion verbs), it is the subject which is incorporated. Only in a very few sign languages (for instance Japanese Sign Language) are agents ever incorporated. In this way, since subjects of intransitives are treated similarly to objects of transitives, incorporation in sign languages can be said to follow an ergative pattern.\nBrentari classifies sign languages as a whole group determined by the medium of communication (visual instead of auditory) as one group with the features monosyllabic and polymorphemic. That means, that one syllable (i.e. one word, one sign) can express several morphemes, e.g., subject and object of a verb determine the direction of the verb's movement (inflection).\nAnother aspect of typology that has been studied in sign languages is their systems for cardinal numbers. Typologically significant differences have been found between sign languages.\nAcquisition.\nChildren who are exposed to a sign language from birth will acquire it, just as hearing children acquire their native spoken language. In a study done at McGill University, they found that American Sign Language users who acquired the language natively (from birth) performed better when asked to copy videos of ASL sentences than ASL users who acquired the language later in life. They also found that there are differences in the grammatical morphology of ASL sentences between the two groups, all suggesting that there is an important critical period in learning signed languages.\nThe acquisition of non-manual features follows an interesting pattern: When a word that always has a particular non-manual feature associated with it (such as a wh-question word) is learned, the non-manual aspects are attached to the word but do not have the flexibility associated with adult use. At a certain point, the non-manual features are dropped and the word is produced with no facial expression. After a few months, the non-manuals reappear, this time being used the way adult signers would use them.\nWritten forms.\nSign languages do not have a traditional or formal written form. Many deaf people do not see a need to write their own language.\nSeveral ways to represent sign languages in written form have been developed.\nSo far, there is no consensus regarding the written form of sign language. Except for SignWriting, none are widely used. Maria Galea writes that SignWriting \"is becoming widespread, uncontainable and untraceable. In the same way that works written in and about a well developed writing system such as the Latin script, the time has arrived where SW is so widespread, that it is impossible in the same way to list all works that have been produced using this writing system and that have been written about this writing system.\" For example, in 2015 at the Federal University of Santa Catarina, Jo\u00e3o Paulo Ampessan wrote his linguistics master's dissertation in Brazilian Sign Language using Sutton SignWriting. In his dissertation, \"The Writing of Grammatical Non-Manual Expressions in Sentences in LIBRAS Using the SignWriting System\", Ampessan states that \"the data indicate the need for [non-manual expressions] usage in writing sign language\".\nSign perception.\nFor a native signer, sign perception influences how the mind makes sense of their visual language experience. For example, a handshape may vary based on the other signs made before or after it, but these variations are arranged in perceptual categories during its development. The mind detects handshape contrasts but groups similar handshapes together in one category. Different handshapes are stored in other categories. The mind ignores some of the similarities between different perceptual categories, at the same time preserving the visual information within each perceptual category of handshape variation.\nIn society.\nDeaf communities and Deaf culture.\nWhen deaf people constitute a relatively small proportion of the general population, deaf communities often develop that are distinct from the surrounding hearing community.\nThese deaf communities are very widespread in the world, associated especially with sign languages used in urban areas and throughout a nation, and the cultures they have developed are very rich.\nOne example of sign language variation in the deaf community is Black ASL. This sign language was developed in the black deaf community as a variant during the American era of segregation and racism, where young black deaf students were forced to attend separate schools than their white deaf peers.\nUse of sign languages in hearing communities.\nOn occasion, where the prevalence of deaf people is high enough, a deaf sign language has been taken up by an entire local community, forming what is sometimes called a \"village sign language\" or \"shared signing community\". Typically this happens in small, tightly integrated communities with a closed gene pool. Famous examples include:\nIn such communities deaf people are generally well-integrated in the general community and not socially disadvantaged, so much so that it is difficult to speak of a separate \"Deaf\" community.\nMany Australian Aboriginal sign languages arose in a context of extensive speech taboos, such as during mourning and initiation rites. They are or were especially highly developed among the Warlpiri, Warumungu, Dieri, Kaytetye, Arrernte, and Warlmanpa, and are based on their respective spoken languages.\nA sign language arose among tribes of American Indians in the Great Plains region of North America (see Plains Indian Sign Language) before European contact. It was used by hearing people to communicate among tribes with different spoken languages, as well as by deaf people. There are especially users today among the Crow, Cheyenne, and Arapaho.\nSign language is also used as a form of alternative or augmentative communication by people who can hear but have difficulties using their voices to speak.\nIncreasingly, hearing schools and universities are expressing interest in incorporating sign language. In the U.S., enrollment for ASL (American Sign Language) classes as part of students' choice of second language is on the rise. In New Zealand, one year after the passing of NZSL Act 2006 in parliament, a NZSL curriculum was released for schools to take NZSL as an optional subject. The curriculum and teaching materials were designed to target intermediate schools from Years 7 to 10, (https://, 2007).\nLegal recognition.\nSome sign languages have obtained some form of legal recognition, while others have no status at all. Sarah Batterbury has argued that sign languages should be recognized and supported not merely as an accommodation for those with disabilities, but as the communication medium of language communities.\nLegal requirements covering sign language accessibility in media vary from country to country. In the United Kingdom, the Broadcasting Act 1996 addressed the requirements for blind and deaf viewers, but has since been replaced by the Communications Act 2003.\nInterpretation.\nIn order to facilitate communication between deaf and hearing people, sign language interpreters are often used. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own syntax, different from any spoken language.\nThe interpretation flow is normally between a sign language and a spoken language that are customarily used in the same country, such as French Sign Language (LSF) and spoken French in France, Spanish Sign Language (LSE) to spoken Spanish in Spain, British Sign Language (BSL) and spoken English in the U.K., and American Sign Language (ASL) and spoken English in the U.S. and most of anglophone Canada (since BSL and ASL are distinct sign languages both used in English-speaking countries), etc. Sign language interpreters who can translate between signed and spoken languages that are not normally paired (such as between LSE and English), are also available, albeit less frequently.\nSign language is sometimes provided for television programmes that include speech. The signer usually appears in the bottom corner of the screen, with the programme being broadcast full size or slightly shrunk away from that corner. Typically for press conferences such as those given by the Mayor of New York City, the signer appears to stage left or right of the public official to allow both the speaker and signer to be in frame at the same time. Live sign interpretation of important televised events is increasingly common but still an informal industry In traditional analogue broadcasting, some programmes are repeated outside main viewing hours with a signer present. Some emerging television technologies allow the viewer to turn the signer on and off in a similar manner to subtitles and closed captioning.\nTechnology.\nOne of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&amp;T's videophone (trademarked as the Picturephone) was introduced to the public at the 1964 New York World's Fair\u2013 two deaf users were able to freely communicate with each other between the fair and another city. However, video communication did not become widely available until sufficient bandwidth for the high volume of video data became available in the early 2000s.\nThe Internet now allows deaf people to talk via a video link, either with a special-purpose videophone designed for use with sign language or with \"off-the-shelf\" video services designed for use with broadband and an ordinary computer webcam. The special videophones that are designed for sign language communication may provide better quality than 'off-the-shelf' services and may use data compression methods specifically designed to maximize the intelligibility of sign languages. Some advanced equipment enables a person to remotely control the other person's video camera, in order to zoom in and out or to point the camera better to understand the signing.\nInterpreters may be physically present with both parties to the conversation but, since the technological advancements in the early 2000s, provision of interpreters in remote locations has become available. In video remote interpreting (VRI), the two clients (a sign language user and a hearing person who wish to communicate with each other) are in one location, and the interpreter is in another. The interpreter communicates with the sign language user via a video telecommunications link, and with the hearing person by an audio link. VRI can be used for situations in which no on-site interpreters are available.\nHowever, VRI cannot be used for situations in which all parties are speaking via telephone alone. With video relay service (VRS), the sign language user, the interpreter, and the hearing person are in three separate locations, thus allowing the two clients to talk to each other on the phone through the interpreter.\nWith recent developments in artificial intelligence in computer science, some recent deep learning based machine translation algorithms have been developed which automatically translate short videos containing sign language directly to written language.\nSign language has been incorporated into film; for example, all movies shown in Brazilian movie theaters must have a Brazilian Sign Language video track available to play alongside the film via a second screen. \nSign Union flag.\nThe Sign Union flag was designed by Arnaud Balard. After studying flags around the world and vexillology principles for two years, Balard revealed the design of the flag, featuring the stylized outline of a hand. The three colors which make up the flag design are representative of Deafhood and humanity (dark blue), sign language (turquoise), and enlightenment and hope (yellow). Balard intended the flag to be an international symbol which welcomes deaf people.\nLanguage endangerment and extinction.\nAs with any spoken language, sign languages are also vulnerable to becoming endangered. For example, a sign language used by a small community may be endangered and even abandoned as users shift to a sign language used by a larger community, as has happened with Hawai'i Sign Language, which is almost extinct except for a few elderly signers. Even nationally recognised sign languages can be endangered; for example, New Zealand Sign Language is losing users. Methods are being developed to assess the language vitality of sign languages.\n&lt;templatestyles src=\"Col-float/styles.css\" /&gt;\nCommunication systems similar to sign language.\nThere are a number of communication systems that are similar in some respects to sign languages, while not having all the characteristics of a full sign language, particularly its grammatical structure. Many of these are either precursors to natural sign languages or are derived from them.\nManual codes for spoken languages.\nWhen Deaf and Hearing people interact, signing systems may be developed that use signs drawn from a natural sign language but used according to the grammar of the spoken language. In particular, when people devise one-for-one sign-for-word correspondences between spoken words (or even morphemes) and signs that represent them, the system that results is a manual code for a spoken language, rather than a natural sign language. Such systems may be invented in an attempt to help teach Deaf children the spoken language, and generally are not used outside an educational context.\n\"Baby sign language\" with hearing children.\nSome hearing parents teach signs to young hearing children. Since the muscles in babies' hands grow and develop quicker than their mouths, signs are seen as a beneficial option for better communication. Babies can usually produce signs before they can speak. This reduces the confusion between parents when trying to figure out what their child wants. When the child begins to speak, signing is usually abandoned, so the child does not progress to acquiring the grammar of the sign language.\nThis is in contrast to hearing children who grow up with Deaf parents, who generally acquire the full sign language natively, the same as Deaf children of Deaf parents.\nHome sign.\nInformal, rudimentary sign systems are sometimes developed within a single family. For instance, when hearing parents with no sign language skills have a deaf child, the child may develop a system of signs naturally, unless repressed by the parents. The term for these mini-languages is home sign (sometimes \"kitchen sign\").\nHome sign arises due to the absence of any other way to communicate. Within the span of a single lifetime and without the support or feedback of a community, the child naturally invents signs to help meet his or her communication needs, and may even develop a few grammatical rules for combining short sequences of signs. Still, this kind of system is inadequate for the intellectual development of a child and it comes nowhere near meeting the standards linguists use to describe a complete language. No type of home sign is recognized as a full language.\nPrimate use.\nThere have been several notable examples of scientists teaching signs to non-human primates in order to communicate with humans, such as chimpanzees, gorillas and orangutans. However, linguists generally point out that this does not constitute knowledge of a human \"language\" as a complete system, rather than simply signs/words. Notable examples of animals who have learned signs include:\nGestural theory of human language origins.\nOne theory of the evolution of human language states that it developed first as a gestural system, which later shifted to speech. An important question for this gestural theory is what caused the shift to vocalization.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n\"Note: the articles for specific sign languages (e.g. ASL or BSL) may contain further external links, e.g. for learning those languages.\""}
{"id": "27701", "revid": "44524978", "url": "https://en.wikipedia.org/wiki?curid=27701", "title": "String (computer science)", "text": "Sequence of characters, data type\nIn computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. More general, \"string\" may also denote a sequence (or list) of data other than just characters.\nDepending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.\nWhen a string appears literally in source code, it is known as a string literal or an anonymous string.\nIn formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.\nPurpose.\nA primary purpose of strings is to store human-readable text, like words and sentences. Strings are used to communicate information from a computer program to the user of the program. A program may also accept string input from its user. Further, strings may store data expressed as characters yet not intended for human reading.\nExample strings and their purposes:\nThe term string may also designate a sequence of data or computer records other than characters \u2014 like a \"string of bits\" \u2014 but when used without qualification it refers to strings of characters.\nHistory.\nUse of the word \"string\" to mean any items arranged in a line, series or succession dates back centuries. In 19th-century typesetting, compositors used the term \"string\" to denote a length of type printed on paper; the string would be measured to determine the compositor's pay.\nUse of the word \"string\" to mean \"a sequence of symbols or linguistic elements in a definite order\" emerged from mathematics, symbolic logic, and linguistic theory to speak about the formal behavior of symbolic systems, setting aside the symbols' meaning.\nFor example, logician C. I. Lewis wrote in 1918:\nA mathematical system is any set of strings of recognisable marks in which some of the strings are taken initially and the remainder derived from these by operations performed according to rules which are independent of any meaning assigned to the marks. That a system should consist of 'marks' instead of sounds or odours is immaterial.\nAccording to Jean E. Sammet, \"the first realistic string handling and pattern matching language\" for computers was COMIT in the 1950s, followed by the SNOBOL language of the early 1960s.\nString datatypes.\nA string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a \"literal\" or \"string literal\".\nString length.\nAlthough formal strings can have an arbitrary finite length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: \"fixed-length strings\", which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and \"variable-length strings\", whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time (see Memory management). Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length by the amount of available memory. The string length can be stored as a separate integer (which may put another artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero such as in C programming language. See also \"Null-terminated\" below.\nCharacter encoding.\nString datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC. If text in one encoding was displayed on a system using a different encoding, text was often mangled, though often somewhat readable and some computer users learned to read the mangled text.\nLogographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not \"self-synchronizing\", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string.\nUnicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode's preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different from the \"characters\", the main difficulty currently is incorrectly designed APIs that attempt to hide this difference (UTF-32 does make \"code points\" fixed-sized, but these are not \"characters\" due to composing codes).\nImplementations.\nSome languages, such as C++, Perl and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed \"mutable\" strings. In other languages, such as Java, JavaScript, Lua, Python, and Go, the value is fixed and a new string must be created if any alteration is to be made; these are termed \"immutable\" strings. Some of these languages with immutable strings also provide another type that is mutable, such as Java and .NET's , the thread-safe Java , and the Cocoa codice_5. Immutability brings advantages and disadvantages: while immutable strings may require inefficiently creating many copies, they are simpler and fully thread-safe.\nStrings are typically implemented as arrays of bytes, characters, or code units, to allow fast access to individual units or substrings, including characters when they have a fixed length. A few languages such as Haskell implement them as linked lists instead.\nMany high-level languages provide strings as a primitive data type, such as JavaScript and PHP, while most others provide them as a composite data type, some with special language support in writing literals, for example, Java and C#.\nSome languages, such as C, Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes. Even in programming languages having a dedicated string type, string can usually be iterated as a sequence character codes, like lists of integers or other values.\nRepresentations.\nRepresentations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.\nThe term \"byte string\" usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.\nMost string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the physical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.\nDope vectors.\nThe length of a string can be stored in a dope vector, separate from the storage holding the actual characters. The IBM PL/I (F) compiler used a string dope vector (SDV) for variable-length strings and for passing string parameters. The SDV contains a current length and a maximum length, and is not adjacent to the string proper. After PL/I (F), IBM dropped the SDV in favor of length-prefixed strings. \nNull-terminated.\nThe length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an \"n\"-character string takes \"n\" + 1 space (1 for the terminator), and is thus an implicit data structure.\nIn terminated strings, the terminating code is not an allowable character in any string. Strings with \"length\" field do not have this limitation and can also store arbitrary binary data.\nAn example of a \"null-terminated string\" stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:\nThe length of the string in the above example, \"codice_6\", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of other data or just garbage. (Strings of this form are sometimes called \"ASCIZ strings\", after the original assembly language directive used to declare them.)\nByte- and bit-terminated.\nUsing a special byte other than null for terminating strings has historically appeared in both hardware and software, though sometimes with a value that was also a printing character. codice_7 was used by many assembler systems, codice_8 used by CDC systems (this character had a value of zero), and the ZX80 used codice_9 since this was the string delimiter in its BASIC language.\nSomewhat similar, \"data processing\" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This bit had to be clear in all other parts of the string. This meant that, while the IBM 1401 had a seven-bit word, almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.\nEarly microcomputer software relied upon the fact that ASCII codes do not use the high-order bit, and set it to indicate the end of a string. It must be reset to 0 prior to output.\nLength-prefixed.\nThe length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value. This convention is used in many Pascal dialects; as a consequence, some people call such a string a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the \"length\" field covers the address space, strings are limited only by the available memory.\nIf the length is bounded, then it can be encoded in constant space, typically a machine word, thus leading to an implicit data structure, taking \"n\" + \"k\" space, where \"k\" is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).\nIf the length is not bounded, encoding a length \"n\" takes log(\"n\") space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length \"n\" in log(\"n\") + \"n\" space.\nIn the latter case, the length-prefix field itself does not have fixed length, therefore the actual string data needs to be moved when the string grows such that the length field needs to be increased.\nHere is a Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:\nStrings as records.\nMany languages, including object-oriented ones, implement strings as records with an internal structure like:\npublic final class String {\n private unsigned long length; // string length\n private UniquePointer&lt;char[]&gt; text; // explicit ownership\n // public methods...\nHowever, since the implementation is usually hidden, the string must be accessed and modified through member functions. codice_10 is a pointer to a dynamically allocated memory area, which might be expanded as needed. See also string (C++).\nOther representations.\nBoth character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.\nBoth of these limitations can be overcome by clever programming.\nIt is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.\nWhile these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.\nThe core data structure in a text editor is the one that manages the string (sequence of characters) that represents the current state of the file being edited.\nWhile that state could be stored in a single long consecutive array of characters, a typical text editor instead uses an alternative representation as its sequence data structure\u2014a gap buffer, a linked list of lines, a piece table, or a rope\u2014which makes certain string operations, such as insertions, deletions, and undoing previous edits, more efficient.\nSecurity concerns.\nThe differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.\nString data is frequently obtained from user input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user input can cause a program to be vulnerable to code injection attacks.\nLiteral strings.\nSometimes, strings need to be embedded inside a text file that is both human-readable and intended for consumption by a machine. This is needed in, for example, source code of programming languages, or in configuration files. In this case, the NUL character does not work well as a terminator since it is normally invisible (non-printable) and is difficult to input via a keyboard. Storing the string length would also be inconvenient as manual computation and tracking of the length is tedious and error-prone.\nTwo common representations are:\nNon-text strings.\nWhile character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A bit string or byte string, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit clean, data corruption may ensue.\nC programmers draw a sharp distinction between a \"string\", aka a \"string of characters\", which by definition is always null terminated, vs. an \"array of characters\" which may be stored in the same array but is often not null terminated.\nUsing C string handling functions on such an array of characters often seems to work, but later leads to security problems.\nString processing algorithms.\nThere are many algorithms for processing strings, each with various trade-offs. Competing algorithms can be analyzed with respect to run time, storage requirements, and so forth. The name stringology was coined in 1984 by computer scientist Zvi Galil for the theory of algorithms and data structures used for string processing.\nSome categories of algorithms include:\nAdvanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite-state machines.\nCharacter string-oriented languages and utilities.\nCharacter strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:\nMany Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.\nSome APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.\nMany scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.\nSome languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.\nCharacter string functions.\nString functions are used to create strings or change the contents of a mutable string. They also are used to query information about a string. The set of functions and their names varies depending on the computer programming language.\nThe most basic example of a string function is the string length function \u2013 the function that returns the length of a string (not counting any terminator characters or any of the string's internal structural information) and does not modify the string. This function is often named codice_13, codice_14, or codice_15. For example, codice_16 would return 11. Another common function is concatenation, where a new string is created by appending two strings, often this is the + addition operator.\nSome microprocessor's instruction set architectures contain direct support for string operations, such as block copy (e.g. In intel x86m codice_17).\nFormal theory.\nLet formula_1 be a finite set of distinct, unambiguous symbols (alternatively called characters), called the alphabet. A string (or word or expression) over formula_1 is any finite sequence of symbols from formula_1. For example, if formula_4, then formula_5 is a string over formula_1.\nThe \"length\" of a string formula_7 is the number of symbols in formula_7 (the length of the sequence) and can be any non-negative integer; it is often denoted as formula_9. The \"empty string\" is the unique string over formula_1 of length formula_11, and is denoted formula_12 or formula_13.\nThe set of all strings over formula_1 of length formula_15 is denoted formula_16. For example, if formula_4, then formula_18. We have formula_19 for every alphabet formula_1.\nThe set of all strings over formula_1 of any length is the Kleene closure of formula_1 and is denoted formula_23. In terms of formula_16,\nformula_25\nFor example, if formula_4, then formula_27. Although the set formula_23 itself is countably infinite, each element of formula_23 is a string of finite length.\nA set of strings over formula_1 (i.e. any subset of formula_23) is called a \"formal language\" over formula_1. For example, if formula_4, the set of strings with an even number of zeros, formula_34, is a formal language over formula_1.\nConcatenation and substrings.\n\"Concatenation\" is an important binary operation on formula_23. For any two strings formula_7 and formula_38 in formula_23, their concatenation is defined as the sequence of symbols in formula_7 followed by the sequence of characters in formula_38, and is denoted formula_42. For example, if formula_43 (i.e. the lowercase English alphabet), formula_44, and formula_45, then formula_46 and formula_47.\nString concatenation is an associative, but non-commutative operation. The empty string formula_12 serves as the identity element; for any string formula_7, formula_50. Therefore, the set formula_23 and the concatenation operation form a monoid, the free monoid generated by formula_1. In addition, the length function defines a monoid homomorphism from formula_23 to the non-negative integers (that is, a function formula_54, such that formula_55).\nA string formula_7 is said to be a \"substring\" or \"factor\" of formula_38 if there exist (possibly empty) strings formula_58 and formula_59 such that formula_60. The relation \"is a substring of\" defines a partial order on formula_23, the least element of which is the empty string.\nPrefixes and suffixes.\nA string formula_7 is said to be a prefix of formula_38 if there exists a string formula_7 such that formula_65. If formula_58 is nonempty, formula_7 is said to be a \"proper\" prefix of formula_38. Symmetrically, a string formula_7 is said to be a suffix of formula_38 if there exists a string formula_58 such that formula_72. If formula_58 is nonempty, formula_7 is said to be a \"proper\" suffix of formula_38. Suffixes and prefixes are substrings of formula_38. Both the relations \"is a prefix of\" and \"is a suffix of\" are prefix orders.\nReversal.\nThe reverse of a string is a string with the same symbols but in reverse order. For example, if formula_77 (where formula_78, formula_79, and formula_80 are symbols of the alphabet), then the reverse of formula_7 is formula_82. A string that is the reverse of itself (e.g., formula_83) is called a palindrome, which also includes the empty string and all strings of length formula_84.\nRotations.\nA string formula_85 is said to be a rotation of formula_38 if formula_87. For example, if formula_4 the string formula_89 is a rotation of formula_90, where formula_91 and formula_92. As another example, the string formula_93 has three different rotations, viz. formula_93 itself (with formula_95, formula_96), formula_97 (with formula_98), and formula_99 (with formula_100).\nLexicographical ordering.\nIt is often useful to define an ordering on a set of strings. If the alphabet formula_1 has a total order (cf. alphabetical order) one can define a total order on formula_23 called lexicographical order. The lexicographical order is total if the alphabetical order is, but is not well-founded for any nontrivial alphabet, even if the alphabetical order is. For example, if formula_4 and formula_104, then the lexicographical order on formula_23 includes the relationships formula_106 With respect to this ordering, e.g. the infinite set formula_107 has no minimal element.\nSee Shortlex for an alternative string ordering that preserves well-foundedness.\nFor the example alphabet, the shortlex order is formula_108\nString operations.\nA number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.\nTopology.\nStrings admit the following interpretation as nodes on a graph, where formula_109 is the number of symbols in formula_1:\nThe natural topology on the set of fixed-length strings or variable-length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the \"p\"-adic numbers and some constructions of the Cantor set, and yields the same topology.\nIsomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "27702", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=27702", "title": "Stack-based", "text": ""}
