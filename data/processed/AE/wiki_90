{"id": "55922", "revid": "7431829", "url": "https://en.wikipedia.org/wiki?curid=55922", "title": "Prostatitis", "text": "Medical condition&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nProstatitis is an umbrella term for a variety of medical conditions that incorporate bacterial and non-bacterial origin illnesses in the pelvic region. In contrast with the plain meaning of the word (which means \"inflammation of the prostate\"), the diagnosis may not always include inflammation. Prostatitis is classified into acute, chronic, asymptomatic inflammatory prostatitis, and chronic pelvic pain syndrome.\nIn the United States, prostatitis is diagnosed in 8% of all male urologist visits and 1% of all primary care physician visits for male genitourinary symptoms.\nClassification.\nThe term \"prostatitis\" refers to inflammation of the tissue of the prostate gland. It may occur as an appropriate physiological response to an infection, or it may occur in the absence of infection, or there may be no inflammation of the prostate at all.\nIn 1999, the National Institutes of Health devised a new classification system. For more specifics about each type of prostatitis, including information on symptoms, treatment, and prognosis, follow the links to the relevant full articles.\nIn 1968, Meares and Stamey determined a classification technique based on the culturing of bacteria. This classification is no longer used.\nThe conditions are distinguished by the different presentation of pain, white blood cells (WBCs) in the urine, duration of symptoms and bacteria cultured from the urine. To help express prostatic secretions that may contain WBCs and bacteria, prostate massage is sometimes used.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55924", "revid": "765510", "url": "https://en.wikipedia.org/wiki?curid=55924", "title": "Prostate", "text": "Gland of the male reproductive system\nThe prostate is an accessory gland of the male reproductive system and a muscle-driven mechanical switch between urination and ejaculation. It is found in all male mammals. It differs between species anatomically, chemically, and physiologically. Anatomically, the prostate is found below the bladder, with the urethra passing through it. It is described in gross anatomy as consisting of lobes and in microanatomy by zone. It is surrounded by an elastic, fibromuscular capsule and contains glandular and connective tissue.\nThe prostate produces and contains fluid that forms part of semen, the substance emitted during ejaculation as part of the male sexual response. This prostatic fluid is slightly alkaline, and milky or white in appearance. The alkalinity of semen helps neutralize the acidity of the vaginal tract, prolonging the lifespan of sperm. The prostatic fluid is expelled in the first part of ejaculate, together with most of the sperm, because of the action of smooth muscle tissue within the prostate. In comparison with the few spermatozoa expelled together with mainly seminal vesicular fluid, those in prostatic fluid have better motility, longer survival, and better protection of genetic material.\nDisorders of the prostate include enlargement, inflammation, infection, and cancer. The word \"prostate\" is derived from Ancient Greek (), meaning \"one who stands before\", \"protector\", \"guardian\", with the term originally used to describe the seminal vesicles.\nStructure.\nThe prostate is a exocrine gland of the male reproductive system. In adults, it is about the size of a walnut, and has an average weight of about , usually ranging between . The prostate is located in the pelvis. It sits below the urinary bladder and surrounds the urethra. The part of the urethra passing through it is called the prostatic urethra, which joins with the two ejaculatory ducts. The prostate is covered in a surface called the \"prostatic capsule\" or \"prostatic fascia\".\nThe internal structure of the prostate has been described using both lobes and zones. Because of the variation in descriptions and definitions of lobes, the zone classification is used more predominantly.\nThe prostate has been described as consisting of three or four zones. Zones are more typically able to be seen on histology, or in medical imaging, such as ultrasound or MRI.\nThe \"lobe\" classification describes lobes that, while originally defined in the fetus, are also visible in gross anatomy, including dissection and when viewed endoscopically. The five lobes are the anterior lobe or isthmus, the posterior lobe, the right and left lateral lobes, and the middle or median lobe.\nInside of the prostate, adjacent and parallel to the prostatic urethra, there are two longitudinal muscle systems. On the front side (ventrally) runs the urethral dilator (\"musculus dilatator urethrae\"), on the backside (dorsally) runs the muscle switching the urethra into the ejaculatory state (\"musculus ejaculatorius\").\nBlood and lymphatic vessels.\nThe prostate receives blood through the inferior vesical artery, internal pudendal artery, and middle rectal arteries. These vessels enter the prostate on its outer surface where it meets the bladder, and travel forward to the apex of the prostate. Both the inferior vesical and the middle rectal arteries often arise together directly from the internal iliac arteries. On entering the bladder, the inferior vesical artery splits into a urethral branch, supplying the urethral prostate; and a capsular branch, which travels around the capsule and has smaller branches, which perforate into the prostate.\nThe veins of the prostate form a network \u2013 the prostatic venous plexus, primarily around its front and outer surface. This network also receives blood from the deep dorsal vein of the penis, and is connected via branches to the vesical plexus and internal pudendal veins. Veins drain into the vesical and then internal iliac veins.\nThe lymphatic drainage of the prostate depends on the positioning of the area. Vessels surrounding the vas deferens, some of the vessels in the seminal vesicle, and a vessel from the posterior surface of the prostate drain into the external iliac lymph nodes. Some of the seminal vesicle vessels, prostatic vessels, and vessels from the anterior prostate drain into internal iliac lymph nodes. Vessels of the prostate itself also drain into the obturator and sacral lymph nodes.\nMicroanatomy.\nThe prostate consists of glandular and connective tissue. Tall column-shaped cells form the lining (the epithelium) of the glands. These form one layer or may be pseudostratified. The epithelium is highly variable and areas of low cuboidal or flat cells can also be present, with transitional epithelium in the outer regions of the longer ducts. Basal cells surround the luminal epithelial cells in benign glands. The glands are formed as many follicles, which drain into canals and subsequently 12\u201320 main ducts, These in turn drain into the urethra as it passes through the prostate. There are also a small amount of flat cells, which sit next to the basement membranes of glands, and act as stem cells.\nThe connective tissue of the prostate is made up of fibrous tissue and smooth muscle. The fibrous tissue separates the gland into lobules. It also sits between the glands and is composed of randomly orientated smooth-muscle bundles that are continuous with the bladder.\nOver time, thickened secretions called corpora amylacea accumulate in the gland.\nGene and protein expression.\nAbout 20,000 protein-coding genes are expressed in human cells and almost 75% of these genes are expressed in the normal prostate. About 150 of these genes are more specifically expressed in the prostate, with about 20 genes being highly prostate specific. The corresponding specific proteins are expressed in the glandular and secretory cells of the prostatic gland and have functions that are important for the characteristics of semen, including prostate-specific proteins, such as the prostate specific antigen (PSA), and the prostatic acid phosphatase.\nDevelopment.\nIn the developing embryo, at the hind end lies an inpouching called the cloaca. This, over the fourth to the seventh week, divides into a urogenital sinus and the beginnings of the anal canal, with a wall forming between these two inpouchings called the urorectal septum. The urogenital sinus divides into three parts, with the middle part forming the urethra; the upper part is largest and becomes the urinary bladder, and the lower part then changes depending on the biological sex of the embryo.\nThe prostatic part of the urethra develops from the middle, pelvic, part of the urogenital sinus, which is of endodermal origin. Around the end of the third month of embryonic life, outgrowths arise from the prostatic part of the urethra and grow into the surrounding mesenchyme. The cells lining this part of the urethra differentiate into the glandular epithelium of the prostate. The associated mesenchyme differentiates into the dense connective tissue and the smooth muscle of the prostate.\nCondensation of mesenchyme, urethra, and Wolffian ducts gives rise to the adult prostate gland, a composite organ made up of several tightly fused glandular and non-glandular components. To function properly, the prostate needs male hormones (androgens), which are responsible for male sex characteristics. The main male hormone is testosterone, which is produced mainly by the testicles. It is dihydrotestosterone (DHT), a metabolite of testosterone, that predominantly regulates the prostate. The prostate gland enlarges over time, until the fourth decade of life.\nFunction.\nIn ejaculation.\nThe prostate secretes fluid, which becomes part of the semen. Its secretion forms up to 30% of the semen. Semen is the fluid emitted (ejaculated) through the male urethra during the sexual response. Sperm are emitted from the vas deferens into the male urethra via the ejaculatory duct, which lies within the prostate gland. Semen is moved into the urethra following contractions of the smooth muscle of the vas deferens and seminal vesicles, following stimulation, primarily of the glans penis. Stimulation sends nerve signals via the internal pudendal nerves to the upper lumbar spine; the nerve signals causing contraction act via the hypogastric nerves. After traveling into the urethra, the seminal fluid is ejaculated by contraction of the bulbocavernosus muscle. The secretions of the prostate include proteolytic enzymes, prostatic acid phosphatase, fibrinolysin, zinc, and prostate-specific antigen. Together with the secretions from the seminal vesicles, these form the major fluid part of semen. The prostate contains various metals, including zinc, and is known to be the primary source of most metals found in semen, which are released during ejaculation.\nIn urination.\nThe prostate's changes of shape, which facilitate the mechanical switch between urination and ejaculation, are mainly driven by the two longitudinal muscle systems running along the prostatic urethra. These are the \"urethral dilator\" (\"musculus dilatator urethrae\") on the urethra's front side, which contracts during urination and thereby shortens and tilts the prostate in its vertical dimension thus widening the prostatic section of the urethral tube, and the muscle switching the urethra into the ejaculatory state (\"musculus ejaculatorius\") on its backside.\nIn case of an operation, e.g. because of benign prostatic hyperplasia (BPH), damaging or sparing of these two muscle systems varies considerably depending on the choice of operation type and details of the procedure of the chosen technique. The effects on postoperational urination and ejaculation vary correspondingly.\nIn stimulation.\nIt is possible for some men to achieve orgasm solely through stimulation of the prostate gland, such as via prostate massage or anal intercourse. This has led to the area of the rectal wall adjacent to the prostate to be popularly referred to as the \"male G-spot\".\nClinical significance.\nInflammation.\nProstatitis is inflammation of the prostate gland. It can be caused by infection with bacteria, or other noninfective causes. Inflammation of the prostate can cause painful urination or ejaculation, groin pain, difficulty passing urine, or constitutional symptoms such as fever or tiredness. When inflamed, the prostate becomes enlarged and is tender when touched during digital rectal examination. The bacteria responsible for the infection may be detected by a urine culture.\nAcute prostatitis and chronic bacterial prostatitis are treated with antibiotics. Chronic non-bacterial prostatitis, or male chronic pelvic pain syndrome is treated by a large variety of modalities including the medications alpha blockers, non-steroidal anti-inflammatories and amitriptyline, antihistamines, and other anxiolytics. Other treatments that are not medications may include physical therapy, psychotherapy, nerve modulators, and surgery. More recently, a combination of trigger point and psychological therapy has proved effective for category III prostatitis as well.\nProstate enlargement.\nAn enlarged prostate is called prostatomegaly, with benign prostatic hyperplasia (BPH) being the most common cause. BPH refers to an enlargement of the prostate due to an increase in the number of cells that make up the prostate () from a cause that is not a malignancy. It is very common in older men. It is often diagnosed when the prostate has enlarged to the point where urination becomes difficult. Symptoms include needing to urinate often (urinary frequency) or taking a while to get started (urinary hesitancy). If the prostate grows too large, it may constrict the urethra and impede the flow of urine, making urination painful and difficult, or in extreme cases completely impossible, causing urinary retention. Over time, chronic retention may cause the bladder to become larger and cause a backflow of urine into the kidneys (hydronephrosis).\nBPH can be treated with medication, a minimally invasive procedure or, in extreme cases, surgery that removes the prostate. In general, treatment often begins with an alpha-1 adrenergic receptor antagonist medication such as tamsulosin, which reduces the tone of the smooth muscle found in the urethra that passes through the prostate, making it easier for urine to pass through. For people with persistent symptoms, procedures may be considered. The surgery most often used in such cases is transurethral resection of the prostate, in which an instrument is inserted through the urethra to remove prostate tissue that is pressing against the upper part of the urethra and restricting the flow of urine. Minimally invasive procedures include transurethral needle ablation of the prostate and transurethral microwave thermotherapy. These outpatient procedures may be followed by the insertion of a temporary stent, to allow normal voluntary urination, without exacerbating irritative symptoms.\nCancer.\nProstate cancer is one of the most common cancers affecting older men in the UK, US, Northern Europe and Australia, and a significant cause of death for elderly men worldwide. Often, a person does not have symptoms; when they do occur, symptoms may include urinary frequency, urgency, hesitation and other symptoms associated with BPH. Uncommonly, such cancers may cause weight loss, retention of urine, or symptoms such as back pain due to lesions that have spread outside of the prostate.\nA digital rectal examination and the measurement of a prostate-specific antigen (PSA) level are usually the first investigations done to check for prostate cancer. PSA values are difficult to interpret, because a high value might be present in a person without cancer, and a low value can be present in someone with cancer. The next form of testing is often the taking of a prostate biopsy to assess for tumour activity and invasiveness. Because of the significant risk of overdiagnosis with widespread screening in the general population, prostate cancer screening is controversial. If a tumour is confirmed, medical imaging such as an MRI or bone scan may be done to check for the presence of tumour in other parts of the body.\nProstate cancer that is only present in the prostate is often treated with either surgical removal of the prostate or with radiotherapy or by the insertion of small radioactive particles of iodine-125 or palladium-103, called brachytherapy. Cancer that has spread to other parts of the body is usually treated also with hormone therapy, to deprive a tumour of sex hormones (androgens) that stimulate proliferation. This is often done through the use of GnRH analogues or agents (such as bicalutamide) that block the receptors that androgens act on; occasionally, surgical removal of the testes may be done instead. Cancer that does not respond to hormonal treatment, or that progresses after treatment, might be treated with chemotherapy such as docetaxel. Radiotherapy may also be used to help with pain associated with bony lesions.\nSometimes, the decision may be made not to treat prostate cancer. If a cancer is small and localised, the decision may be made to monitor for cancer activity at intervals (\"active surveillance\") and defer treatment. If a person, because of frailty or other medical conditions or reasons, has a life expectancy less than ten years, then the impacts of treatment may outweigh any perceived benefits.\nSurgery.\nSurgery to remove the prostate is called prostatectomy, and is usually done as a treatment for cancer limited to the prostate, or for prostatic enlargement. When it is done, it may be done as open surgery or as laparoscopic (keyhole) surgery. These are done under general anaesthetic. Usually the procedure for cancer is a radical prostatectomy, which means that the seminal vesicles are removed and the vasa deferentia are also tied off. Part of the prostate can also be removed from within the urethra, called transurethral resection of the prostate (TURP). Open surgery may involve a cut that is made in the perineum, or via an approach that involves a cut down the midline from the belly button to the pubic bone. Open surgery may be preferred if there is a suspicion that lymph nodes are involved and they need to be removed or biopsied during a procedure. A perineal approach will not involve lymph node removal and may result in less pain and a faster recovery following an operation. A TURP procedure uses a tube inserted into the urethra via the penis and some form of heat, electricity or laser to remove prostate tissue.\nThe whole prostate can be removed. Complications that might develop because of surgery include urinary incontinence and erectile dysfunction because of damage to nerves during the operation, particularly if a cancer is very close to nerves. Ejaculation of semen will not occur during orgasm if the vasa deferentia are tied off and seminal vesicles removed, such as during a radical prosatectomy. This will mean a man becomes infertile. Sometimes, orgasm may not be able to occur or may be painful. The penis length may shorten slightly if the part of the urethra within the prostate is also removed. General complications due to surgery can also develop, such as infections, bleeding, inadvertent damage to nearby organs or within the abdomen, and the formation of blood clots.\nTransmasculine individuals.\nNumerous studies have recorded the proliferation of prostate tissue in transmasculine individuals undergoing testosterone therapy. A 2022 study concluded that \"one hundred percent of vaginal specimens obtained from transmasculine individuals on testosterone therapy (21/21 cases) demonstrated prostatic metaplasia\" and notes, congruously with similar studies, that further research in this area is warranted so as to contribute to higher standards of care for transgender patients, and so that the presence of this tissue does not register to healthcare providers as an abnormality.\nHistory.\nThe prostate was first formally identified by Venetian anatomist Niccol\u00f2 Massa in \"Anatomiae libri introductorius\" (Introduction to Anatomy) in 1536 and illustrated by Flemish anatomist Andreas Vesalius in \"Tabulae anatomicae sex\" (six anatomical tables) in 1538. Massa described it as a \"glandular flesh upon which rests the neck of the bladder,\" and Vesalius as a \"glandular body\". The first time a word similar to \"prostate\" was used to describe the gland is credited to Andr\u00e9 du Laurens in 1600, who described it as a term already in use by anatomists at the time. The term was however used at least as early as 1549 by French surgeon Ambroise Pare.\nAt the time, Du Laurens was describing what was considered to be a pair of organs (not the single two-lobed organ), and the Latin term \"prostatae\" that was used, meaning \"one who stands before,\" \"leader\" or \"guardian,\" was a mistranslation of the term for the Ancient Greek word used to describe the seminal vesicles, \"parastatai adenoeides\", meaning \"glandular assistants\". Some have argued that surgeons in Ancient Greece and Rome must have at least seen the prostate as an anatomical entity, but other authors state that because prostatic anatomy varies greatly among species, and almost all anatomical dissection before the Renaissance was performed on animals, it is likely that the prostate was not recognized as a distinct organ. The term \"prostatae\" was taken rather than the grammatically correct \"prostator\" (singular) and \"prostatores\" (plural) because the gender of the Ancient Greek term was taken as female, when it was in fact male.\nThe fact that the prostate was one and not two organs was an idea popularised throughout the early 18th century, as was the English language term used to describe the organ, \"prostate\", attributed to William Cheselden. A monograph, \"Practical observations on the treatment of the diseases of the prostate gland\" by Everard Home in 1811, was important in the history of the prostate by describing and naming anatomical parts of the prostate, including the median lobe. The idea of the five lobes of the prostate was popularized following anatomical studies conducted by American urologist Oswald Lowsley in 1912. John E. McNeal first proposed the idea of \"zones\" in 1968; McNeal found that the relatively homogeneous cut surface of an adult prostate in no way resembled \"lobes\" and thus led to the description of \"zones\".\nProstate cancer was first described in a speech to the Medical and Chiurgical Society of London in 1853 by surgeon John Adams and increasingly described by the late 19th century. Prostate cancer was initially considered a rare disease, probably because of shorter life expectancies and poorer detection methods in the 19th century. The first treatments of prostate cancer were surgeries to relieve urinary obstruction. Samuel David Gross has been credited with the first mention of a prostatectomy, as \"too absurd to be seriously entertained\" The first removal for prostate cancer (radical perineal prostatectomy) was first performed in 1904 by Hugh H. Young at Johns Hopkins Hospital; partial removal of the gland was conducted by Theodore Billroth in 1867.\nTransurethral resection of the prostate (TURP) replaced radical prostatectomy for symptomatic relief of obstruction in the middle of the 20th century because it could better preserve penile erectile function. Radical retropubic prostatectomy was developed in 1983 by Patrick Walsh. In 1941, Charles B. Huggins published studies in which he used estrogen to oppose testosterone production in men with metastatic prostate cancer. This discovery of \"chemical castration\" won Huggins the 1966 Nobel Prize in Physiology or Medicine.\nThe role of the gonadotropin-releasing hormone (GnRH) in reproduction was determined by Andrzej W. Schally and Roger Guillemin, who both won the 1977 Nobel Prize in Physiology or Medicine for this work. GnRH receptor agonists, such as leuprorelin and goserelin, were subsequently developed and used to treat prostate cancer. Radiation therapy for prostate cancer was first developed in the early 20th century and initially consisted of intraprostatic radium implants. External beam radiotherapy became more popular as stronger X-ray radiation sources became available in the middle of the 20th century. Brachytherapy with implanted seeds (for prostate cancer) was first described in 1983. Systemic chemotherapy for prostate cancer was first studied in the 1970s. The initial regimen of cyclophosphamide and 5-fluorouracil was quickly joined by multiple regimens using a host of other systemic chemotherapy drugs.\nOther animals.\nThe prostate is found only in mammals. The prostate glands of male marsupials are proportionally larger than those of placental mammals. The presence of a functional prostate in monotremes is controversial, and if monotremes do possess functional prostates, they may not make the same contribution to semen as in other mammals.\nThe structure of the prostate varies, ranging from tubuloalveolar (as in humans) to branched tubular. The gland is particularly well developed in carnivorans and boars, though in other mammals, such as bulls, it can be small and inconspicuous. In other animals, such as marsupials and small ruminants, the prostate is disseminate, meaning not specifically localisable as a distinct tissue, but present throughout the relevant part of the urethra; in other animals, such as red deer and American elk, it may be present as a specific organ and in a disseminate form. In some marsupial species, the size of the prostate gland changes seasonally. The prostate is the only accessory gland that occurs in male dogs. Dogs can produce in one hour as much prostatic fluid as a human can in a day. They excrete this fluid along with their urine to mark their territory. Additionally, dogs are the only species apart from humans seen to have a significant incidence of prostate cancer. The prostate is the only male accessory gland that occurs in cetaceans, consisting of diffuse urethral glands surrounded by a very powerful compressor muscle.\nThe prostate gland originates with tissues in the urethral wall. This means the urethra, a compressible tube used for urination, runs through the middle of the prostate; enlargement of the prostate can constrict the urethra so that urinating becomes slow and painful.\nProstatic secretions vary among species. They are generally composed of simple sugars and are often slightly alkaline. In eutherian mammals, these secretions usually contain fructose. The prostatic secretions of marsupials usually contain N-Acetylglucosamine or glycogen instead of fructose.\nSkene's gland.\nBecause the Skene's gland and the male prostate act similarly by secreting prostate-specific antigen (PSA), which is an ejaculate protein produced in males, and of prostate-specific acid phosphatase, the Skene's gland is sometimes referred to as the \"female prostate\". Although homologous to the male prostate (developed from the same embryological tissues), various aspects of its development in relation to the male prostate are widely unknown and a matter of research.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55927", "revid": "7226930", "url": "https://en.wikipedia.org/wiki?curid=55927", "title": "Urinary system", "text": "Anatomical system consisting of the kidneys, ureters, urinary bladder, and the urethra\nThe urinary system, also known as the urinary tract or renal system, is a part of the excretory system of vertebrates. In humans and placental mammals, it consists of the kidneys, ureters, bladder, and the urethra. The purpose of the urinary system is to eliminate urine from the body, regulate blood volume and blood pressure, control levels of electrolytes and metabolites, and regulate blood pH. The kidneys have an extensive blood supply via the renal arteries which leave the kidneys via the renal vein. Each kidney consists of functional units called nephrons. Following filtration of blood and further processing, the ureters carry urine from the kidneys into the urinary bladder. The urethra carries urine from the bladder through the penis or vulva during urination. The female and male urinary system are very similar, differing only in the length of the urethra.\n800\u20132,000 milliliters (mL) of urine are normally produced every day in a healthy human. This amount varies according to fluid intake and kidney function.\nStructure.\nThe urinary system refers to the structures that produce and excrete urine. In the human urinary system there are two kidneys that are located between the dorsal body wall and parietal peritoneum on both the left and right sides. \nThe formation of urine begins within the functional unit of the kidney, the nephrons. Urine then flows through the nephrons, through a system of converging tubules called collecting ducts. These collecting ducts then join to form the minor calyces, followed by the major calyces that ultimately join the renal pelvis. Urine flows from the renal pelvis into the ureter, which transports urine into the urinary bladder. The anatomy of the human urinary bladder and urethra differs between males and females. In males, the urethra begins at the internal urethral orifice in the trigone of the bladder, and then becomes the prostatic, membranous, bulbar, and penile urethra. Urine exits the male urethra through the urinary meatus in the glans penis. The female urethra is much shorter, beginning at the bladder neck and terminating in the vulval vestibule.\nMicroanatomy.\nUnder microscopy, the urinary system is covered in a unique lining called urothelium, a type of transitional epithelium. Unlike the epithelial lining of most organs, transitional epithelium can flatten and distend. Urothelium covers most of the urinary system, including the renal pelvis, ureters, and bladder.\nFunction.\nThe main functions of the urinary system and its components are to:\nUrine formation.\nAverage urine production in adult humans is about 1\u20132 litres (L) per day, depending on state of hydration, activity level, environmental factors, weight, and the individual's health. Producing too much or too little urine requires medical attention. Polyuria is a condition of excessive urine production (&gt; 2.5 L/day). Conditions involving low output of urine are oliguria (&lt; 400 mL/day) and anuria (&lt; 100 mL/day).\nThe first step in urine formation is the filtration of blood in the kidneys. In a healthy human, the kidney receives between 12 and 30% of cardiac output, but it averages about 20% or about 1.25 L/min.\nThe basic structural and functional unit of the kidney is the nephron. Its chief function is to regulate the concentration of water and soluble substances like sodium by filtering the blood, reabsorbing what is needed and excreting the rest as urine.\nIn the first part of the nephron, Bowman's capsule filters blood from the circulatory system into the tubules. Hydrostatic and osmotic pressure gradients facilitate filtration across a semipermeable membrane. The filtrate includes water, small molecules, and ions that easily pass through the filtration membrane. However, larger molecules such as proteins and blood cells are prevented from passing through the filtration membrane. The amount of filtrate produced every minute is called the glomerular filtration rate or GFR and amounts to 180 litres per day. About 99% of this filtrate is reabsorbed as it passes through the nephron and the remaining 1% becomes urine.\nThe urinary system is regulated by the endocrine system by hormones such as antidiuretic hormone, aldosterone, and parathyroid hormone.\nRegulation of concentration and volume.\nThe urinary system is under influence of the circulatory system, nervous system, and endocrine system.\nAldosterone plays a central role in regulating blood pressure through its effects on the kidney. It acts on the distal tubules and collecting ducts of the nephron and increases reabsorption of sodium from the glomerular filtrate. Reabsorption of sodium results in retention of water, which increases blood pressure and blood volume. Antidiuretic hormone (ADH), is a neurohypophysial hormone found in most mammals. Its two primary functions are to retain water in the body and vasoconstriction. Vasopressin regulates the body's retention of water by increasing water reabsorption in the collecting ducts of the kidney nephron. Vasopressin increases water permeability of the kidney's collecting duct and distal convoluted tubule by inducing translocation of aquaporin-CD water channels in the kidney nephron collecting duct plasma membrane.\nUrination.\nUrination, also sometimes referred to as micturition, is the ejection of urine from the urinary bladder to the outside of the body. Urine is ejected through the urethra from the penis or vulva in placental mammals and through the cloaca in other vertebrates. In healthy humans (and many other animals), the process of urination is under voluntary control. In infants, some elderly individuals, and those with neurological injury, urination may occur as an involuntary reflex. Physiologically, micturition involves coordination between the central, autonomic, and somatic nervous systems. Brain centers that regulate urination include the pontine micturition center, periaqueductal gray, and the cerebral cortex.\nClinical significance.\nUrologic disease can involve congenital or acquired dysfunction of the urinary system. As an example, urinary tract obstruction is a urologic disease that can cause urinary retention.\nDiseases of the kidney tissue are normally treated by nephrologists, while diseases of the urinary tract are treated by urologists. Gynecologists may also treat female urinary incontinence.\nDiseases of other bodily systems also have a direct effect on urogenital function. For instance, it has been shown that protein released by the kidneys in diabetes mellitus sensitizes the kidney to the damaging effects of hypertension.\nDiabetes also can have a direct effect in urination due to peripheral neuropathies, which occur in some individuals with poorly controlled blood sugar levels.\nUrinary incontinence can result from a weakening of the pelvic floor muscles caused by factors such as pregnancy, childbirth, aging, and being overweight. Findings recent systematic reviews demonstrate that behavioral therapy generally results in improved urinary incontinence outcomes, especially for stress and urge UI, than medications alone. Pelvic floor exercises known as Kegel exercises can help in this condition by strengthening the pelvic floor. There can also be underlying medical reasons for urinary incontinence which are often treatable. In children, the condition is called enuresis.\nSome cancers also target the urinary system, including bladder cancer, kidney cancer, ureteral cancer, and urethral cancer. Due to the role and location of these organs, treatment is often complicated.\nHistory.\nKidney stones have been identified and recorded about as long as written historical records exist. The urinary tract including the ureters, as well as their function to drain urine from the kidneys, has been described by Galen in the second century AD.\nThe first to examine the ureter through an internal approach, called ureteroscopy, rather than surgery was Hampton Young in 1929. This was improved on by VF Marshall who is the first published use of a flexible endoscope based on fiber optics, which occurred in 1964. The insertion of a drainage tube into the renal pelvis, bypassing the ureters and urinary tract, called nephrostomy, was first described in 1941. Such an approach differed greatly from the open surgical approaches within the urinary system employed during the preceding two millennia.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55929", "revid": "50330604", "url": "https://en.wikipedia.org/wiki?curid=55929", "title": "Governor-general", "text": "Vice-regal or colonial office\nGovernor-general (plural governors-general), or governor general (plural governors general), is the title of an official, most prominently associated with the British Empire and the Commonwealth. In the context of the governors-general and former British colonies, governors-general continue to be appointed as viceroy to represent the monarch of a personal union in any sovereign state over which the monarch does not normally reign in person (non-UK Commonwealth realm). In the British Empire, governors-general were appointed on the advice of the government of the United Kingdom and were often British aristocracy, but in the mid-twentieth century they began to be appointed on the advice of the independent government of each realm and were citizens of each independent state. \nGovernors-general have also previously been appointed in respect of major colonial states or other territories held by either a monarchy or republic, such as Japan, Korea, Taiwan and France in Indochina.\nCurrent uses.\nIn modern usage, in the context of governors-general and former British colonies, the term \"governor-general\" originated in those British colonies that became self-governing within the British Empire. Before World War I, the title was used only in federated colonies in which its constituents had had \"governors\" prior to federating, namely Canada, Australia, and the Union of South Africa. In these cases, the Crown's representative in the federated Dominion was given the superior title of \"governor-general\". The first exception to this rule was New Zealand, which was granted Dominion status in 1907, but it was not until 28 June 1917 that Arthur Foljambe, 2nd Earl of Liverpool, was appointed the first governor-general of New Zealand.\nSince the 1950s, the title \"governor-general\" has been given to all representatives of the sovereign in independent non-UK Commonwealth realms. In these cases, the former office of colonial governor was altered (sometimes for the same incumbent) to become governor-general upon independence, as the nature of the office became an entirely independent constitutional representative of the monarch rather than a symbol of previous colonial rule. In these countries the governor-general acts as the monarch's representative, performing the ceremonial and constitutional functions of a head of state.\nThe only other nation which currently uses the governor-general designation is Iran, which has no connection with any monarchy or the Commonwealth. In Iran, the provincial authority is headed by a governor general (Persian: \u0627\u0633\u062a\u0627\u0646\u062f\u0627\u0631 \"ost\u0101nd\u0101r\"), who is appointed by the minister of the interior.\nBritish Empire and the Commonwealth.\nBritish Empire.\nUntil the 1920s, governors-general were British subjects, appointed on the advice of the British government, who acted as agents of the British government in each Dominion, as well as being representatives of the monarch. As such they notionally held the prerogative powers of the monarch, and also held the executive power of the country to which they were assigned. The governor-general could be instructed by the colonial secretary on the exercise of some of his functions and duties, such as the use or withholding of royal assent from legislation; history shows many examples of governors-general using their prerogative and executive powers. The monarch or imperial government could overrule any governor-general, though this could often be cumbersome, due to the remoteness of the territories from London.\nThe governor-general was also usually the commander-in-chief of the armed forces in their territory and, because of the governor-general's control of the military, the post was as much a military appointment as a civil one. The governors-general are entitled to wear a unique uniform, which is not generally worn today. If of the rank of major general, equivalent or above, they were entitled to wear that military uniform.\nCommonwealth realms.\nFollowing the Imperial Conference, and subsequent issuing of the Balfour Declaration in 1926, the role and responsibilities of the governor-general began to shift, reflecting the increased independence of the Dominions (which were in 1952 renamed \"realms\"; a term which includes the UK itself). As the sovereign came to be regarded as monarch of each territory independently, and, as such, advised only by the ministers of each country in regard to that country's national affairs (as opposed to a single British monarch ruling all the Dominions as a conglomerate and advised only by an imperial parliament), so too did the governor-general become a direct representative of the national monarch only, who no longer answered to the British government. The report resulting from the 1926 Imperial Conference stated: \"It is an essential consequence of the equality of status existing among the members of the British Commonwealth of Nations that the Governor General of a Dominion is the representative of the Crown, holding in all essential respects the same position in relation to the administration of public affairs in the Dominion as is held by His Majesty the King in Great Britain, and that he is not the representative or agent of His Majesty's Government in Great Britain or of any Department of that Government.\" These concepts were entrenched in legislation with the enactment of the Statute of Westminster in 1931, and governmental relations with the United Kingdom were placed in the hands of a British High Commissioner in each country.\nIn other words, the political reality of a self-governing Dominion within the British Empire with a governor-general answerable to the sovereign became clear. British interference in the Dominion was not acceptable and independent country status was clearly displayed. Canada, Australia, and New Zealand were clearly not controlled by the United Kingdom. The monarch of these countries (Charles III) is in law King of Canada, King of Australia, and King of New Zealand and only acts on the advice of the ministers in each country and is ostensibly in no way influenced by the British government. Today, therefore, in former British colonies that are now independent Commonwealth realms, the governor-general is constitutionally the representative of the monarch in their state and may exercise the reserve powers of the monarch according to their own constitutional authority. The governor-general, however, is still appointed by the monarch and takes an oath of allegiance to the monarch in right of their own country. Executive authority is also vested in the monarch, though much of it can be exercisable only by the governor-general on behalf of the sovereign of the independent realm. Letters of credence or letters of recall are in some realms received or issued in the name of the monarch, but in others (such as Canada and Australia) are issued in the name of the governor-general alone.\nAt diplomatic functions where the governor-general is present, the visiting diplomat or head of state toasts \"The King\" or \"The Queen\" of the relevant realm, not the governor-general, with any reference to the governor-general being subsidiary in later toasts if featuring at all, and will involve a toast to the governor-general by name, not office. (e.g., \"Mrs. Smith\", not \"Her Excellency, the Governor-General\". Sometimes a toast might be made using name and office, e.g., \"Governor-General Smith\".)\nExcept in rare cases (for example, a constitutional crisis), the governor-general usually acts in accordance with constitutional convention and upon the advice of the national prime minister (who is head of the nation's government). The governors-general are still the local representatives of the sovereign and perform the same duties as they carried out historically, though their role is for the most part ceremonial (or partly ceremonial). Rare and controversial exceptions occurred in 1926, when Canadian governor general the Viscount Byng of Vimy refused Prime Minister Mackenzie King's request for a dissolution of parliament; in 1953 and 1954 when the governor-general of Pakistan, Ghulam Mohammad, staged a constitutional coup against the prime minister and then the Constituent Assembly; and in 1975, when the governor-general of Australia, Sir John Kerr, dismissed the prime minister, Gough Whitlam (to name a few). It should be remembered that while governors-general do not normally take drastic action, he or she still has a responsibility to ensure that the constitution is respected and followed at all times. In many ways, the governor-general acts as an umpire/mediator (who must remain independent/non-partisan and objective) in the political scene. In some realms, the monarch could in principle overrule a governor-general, as governors-general are representatives of the monarch rather than holding power in their own right, but this has not happened in recent times.\nIn Australia, the present king is generally assumed to be the head of state, since the governor-general and the state governors are defined as his \"representatives\". However, since the governor-general performs almost all national regal functions, the governor-general has occasionally been referred to as the head of state in political and media discussion. To a lesser extent, uncertainty has been expressed in Canada as to which officeholder\u2014the monarch, the governor-general, or both\u2014can be considered the head of state.\nA governor-general is usually a person with a distinguished record of public service, often a retired politician, judge or military commander; however, some countries have also appointed prominent academics, members of the clergy, philanthropists, or figures from the news media to the office.\nTraditionally, the governor-general's official attire was a unique uniform, but this practice has been abandoned except on occasions when it is appropriate to be worn (and in some countries abandoned altogether). In South Africa, the governor-general of the Union of South Africa nominated by the Afrikaner Nationalist government chose not to wear uniform on any occasion. Most governors-general continue to wear appropriate medals on their clothing when required.\nThe governor-general's official residence is usually called \"Government House.\" The governor-general of the Irish Free State resided in the then Viceregal Lodge in Phoenix Park, Dublin, but the government of \u00c9amon de Valera sought to downgrade the office and the last governor-general, Domhnall Ua Buachalla, did not reside there. The office was abolished there in 1936.\nIn most Commonwealth realms, the flag of the governor-general has been the standard pattern of a blue field with the royal crest (a lion standing on a crown) above a scroll with the name of the jurisdiction. In Canada, however, this was replaced with a crowned lion clasping a maple leaf. In the Solomon Islands, the scroll was replaced with a two-headed frigate bird motif, while in Fiji, the former governor general's flag featured a whale's tooth. In New Zealand, the flag was replaced in 2008 with the shield of the coat of arms of New Zealand surmounted by a crown on a blue field.\nGovernors-general are accorded the style of \"His/Her Excellency\". This style is also extended to their spouses, whether male or female.\nAppointment.\nUntil the 1920s, governors general were British, and appointed on the advice of the British Government.\nFollowing the changes to the structure of the Commonwealth in the late 1920s, in 1929, the Australian prime minister James Scullin established the right of a Dominion prime minister to advise the monarch directly on the appointment of a governor-general, by insisting that his choice (Isaac Isaacs, an Australian) prevail over the recommendation of the British government. The convention was gradually established throughout the Commonwealth that the governor-general would be a citizen of the country concerned, and would be appointed on the advice of the government of that country, with no input from the British government; governor general of Canada since 1952 and governor-general of New Zealand since 1967. Since 1931 as each former Dominion has patriated its constitution from the UK, the convention has become law, or, since 1947, when the first realms established with a patriated constitution, India and Pakistan, were established, was always law, and no government of any realm can advise the Monarch on any matter pertaining to another realm, including the appointment of a governor-general. The monarch appoints a governor-general (in Canada: \"governor general\") as a personal representative only on the advice of the prime minister of each realm; for example, the governor-general of New Zealand is appointed by the king of New Zealand on the advice of the New Zealand prime minister, the governor-general of Tuvalu is appointed by the king of Tuvalu on the advice of the Tuvaluan prime minister, and the governor-general of Jamaica is appointed by the king of Jamaica on the advice of the Jamaican prime minister. In Papua New Guinea and the Solomon Islands, the prime minister's advice is based on the result of a vote in the national parliament.\nThe formalities for appointing governors-general are not the same in all realms. For example: When appointed, a governor-general of Australia issues a proclamation in his own name, countersigned by the head of government and under the Great Seal of Australia, formally announcing that he has been appointed by the monarch's commission, previously issued also under the Great Seal of Australia. The practice in Canada is to include in the governor general's proclamation of appointment, issued under the Great Seal of Canada, the monarch's commission naming the governor general as commander-in-chief of the Canadian Forces. Also dissimilar among the realms are the powers of governors-general. The Belizean constitution provides the governor-general with the power to assent or to withhold assent to laws, while Papua New Guinea has no requirement for royal assent at all, with laws entering into force when certified as having been passed in Parliament by the Speaker.\nTemporary replacement.\nDifferent realms have different constitutional arrangements governing who acts in place of the governor-general in the event of their death, resignation, or incapacity.\nList of countries with a governor-general.\nFormer.\nThe title has been used in many former British colonies or other territories, which became independent realms and then later became republics. Each of these realms had a governor-general.\nOther colonial and similar usage.\nFrance.\nThe equivalent term in French is \"gouverneur g\u00e9n\u00e9ral\", used in the following colonies:\nFurthermore, in Napoleonic Europe successive French governors-general were appointed by Napoleon I in:\nItaly.\nDuring the fascist period, Libya and East Africa were ruled by governor-generals, the former from 1934 to 1943, and the latter from 1936 to 1941 (who was also the Viceroy of Ethiopia).\nJapan.\nFrom 1895 to 1945, Japanese-administered Taiwan had a governor-general. From 1910 to 1945, Japanese-administered Korea had a governor-general. From 1905 to 1910, Japan had a resident-general in Korea.\nNetherlands.\nFrom 1610 to 1942 the Dutch appointed a \"gouverneur-generaal\" (\"governor-general\") to govern the Netherlands East Indies, now Indonesia. Between the capitulation of the Dutch East Indies in 1942 and the formal end of colonial rule over Indonesia by the Dutch in 1949, no governor-general was appointed.\nWhile in the Caribbean, various other titles were used, Cura\u00e7ao had three governors-general between 1816 and 1820:\nPhilippines.\nThe Philippines from the 16th through the 20th century had a series of governors-general during the Spanish and American colonial periods, as well as the brief Japanese occupation during World War II.\nSpain.\nBeginning 21 November 1564, the Spanish East Indies had a governor-general, which was under the Viceroy of New Spain based in Mexico. After the successful Mexican War of Independence in 1821, the governor-general reported directly to Spain.\nUnited States.\nFrom 1899 to 1935 under initial military rule then Insular Government, the Philippines was administered by a series of governors-general, first military and then civilian, appointed by the federal government of the United States.\nPortugal.\nThe equivalent word in Portuguese is \"governador-geral\". This title was only used for the governors of the major colonies, indicating that they had, under their authority, several subordinate governors. In most of the colonies, lower titles, mainly \"governador\" (governor) or formerly captain-major (\"capit\u00e3o-mor\"), prevailed\nBrazil.\nIn Brazil, after a few governors, from 1578 until its promotion in 1763 to a viceroyalty (though various members of the nobility since 1640 had assumed, without sovereign authority, the title of Viceroy).\nOther Western usage.\nGreece.\nThe Balkan Wars of 1912\u201313 led to the Greek acquisition of the so-called \"New Lands\" (Epirus, Macedonia, Crete and the islands of the eastern Aegean), almost doubling the country's territory. Instead of fully incorporating these new lands into Greece by dividing them into prefectures, the Ottoman administrative system continued in existence for a while, and Law \u0394\u03a1\u039b\u0394\u0384 of 1913 established five governorates-general (\u0393\u03b5\u03bd\u03b9\u03ba\u03b1\u1f76 \u0394\u03b9\u03bf\u03b9\u03ba\u03ae\u03c3\u03b5\u03b9\u03c2, sing. \u0393\u03b5\u03bd\u03b9\u03ba\u03ae \u0394\u03b9\u03bf\u03af\u03ba\u03b7\u03c3\u03b9\u03c2): Epirus, Macedonia, Crete, Aegean and Samos\u2013Ikaria. The governors-general had wide-ranging authority in their territories, and were almost autonomous of the government in Athens.\nLaw 524 in 1914 abolished the governorates-general and divided the New Lands into regular prefectures, but in 1918 Law 1149 re-instated them as a superordinate administrative level above the prefectures, with Macedonia now divided in two governorates-general, those of Thessaloniki and Kozani\u2013Florina. The governors-general of Thessaloniki, Crete and Epirus were also given ministerial rank. To these was added the Governorate-General of Thrace in 1920\u201322, comprising Western Thrace and Eastern Thrace (returned to Turkey in the Armistice of Mudanya in 1922). The extensive but hitherto legally rather undefined powers of the governors-general created friction and confusion with other government branches, until their remit was exactly delineated in 1925. The governorates-general, except for that of Thessaloniki, were abolished in 1928, but re-established in December 1929\u2014for Crete, Epirus, Thrace, and Macedonia\u2014and delegated practically all ministerial authorities for their respective areas. Over the next decade, however, in a see-saw of legislative measures that in turn gave and took away authority, they gradually lost most of their powers in favour of the prefectures and the central government in Athens.\nFollowing liberation from the Axis occupation, in 1945 the Governorate-General of Northern Greece was established, initially with subordinate governorates for West Macedonia, Central Macedonia, East Macedonia, and Thrace, the first three of which were then grouped anew into a new Governorate-General of Macedonia, albeit still subject to the Governorate-General of Northern Greece. This awkward arrangement lasted until 1950, when the administration of Macedonia was streamlined, the junior governorates abolished and only the Governorate-General of Northern Greece retained. Finally, in 1955, the Governorate-General of Northern Greece was transformed into the Ministry of Northern Greece, and all other governorates-general elsewhere in Greece were abolished.\nSweden.\nFrom 1636 to 1815, the king of Sweden typically appointed the governors-general of Sweden for the Swedish Dominions on the eastern side of the Baltic Sea and in northern Germany, but occasionally also for Scania.\nNotes and references.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "55930", "revid": "341042", "url": "https://en.wikipedia.org/wiki?curid=55930", "title": "History of Ancient Greece timeline", "text": ""}
{"id": "55931", "revid": "46272477", "url": "https://en.wikipedia.org/wiki?curid=55931", "title": "Chronology", "text": "Science of arranging events in order of occurrence\nChronology (from Latin , from Ancient Greek , , 'time'; and , \"-logia\") is the science of arranging events in their order of occurrence in time, such as in a timeline or other sequence of events. It is also \"the determination of the actual temporal sequence of past events\".\nChronology is a part of periodization. It is also a part of the discipline of history including earth history, the earth sciences, and study of the geologic time scale.\nRelated fields.\nChronology is the science of locating historical events in time. It relies mostly upon chronometry, which is also known as timekeeping, and historiography, which examines the writing of history and the use of historical methods. Radiocarbon dating estimates the age of formerly living things by measuring the proportion of carbon-14 isotope in their carbon content. Dendrochronology estimates the age of trees by correlation of the various growth rings in their wood to known year-by-year reference sequences in the region to reflect year-to-year climatic variation. Dendrochronology is used in turn as a calibration reference for radiocarbon dating curves.\nCalendar and era.\nThe familiar terms \"calendar\" and \"era\" (within the meaning of a coherent system of numbered calendar years) concern two complementary fundamental concepts of chronology. For example, during eight centuries the calendar belonging to the Christian era, which era was taken in use in the 8th century by Bede, was the Julian calendar, but after the year 1582 it was the Gregorian calendar. Dionysius Exiguus (about the year 500) was the founder of that era, which is nowadays the most widespread dating system on earth. An epoch is the date (year usually) when an era begins.\nAb Urbe condita era.\n\"Ab Urbe condita\" is Latin for \"from the founding of the City (Rome)\", traditionally set in 753\u00a0BC. It was used to identify the Roman year by a few Roman historians. Modern historians use it much more frequently than the Romans themselves did; the dominant method of identifying Roman years was to name the two consuls who held office that year. Before the advent of the modern critical edition of historical Roman works, AUC was indiscriminately added to them by earlier editors, making it appear more widely used than it actually was.\nIt was used systematically for the first time only about the year 400, by the Iberian historian Orosius. Pope Boniface IV, in about the year 600, seems to have been the first who made a connection between this era and Anno Domini. (AD 1 = AUC 754.)\nAstronomical era.\nDionysius Exiguus' Anno Domini era (which contains only calendar years \"AD\") was extended by Bede to the complete Christian era (which contains, in addition all calendar years \"BC\", but no \"year zero\"). Ten centuries after Bede, the French astronomers Philippe de la Hire (in the year 1702) and Jacques Cassini (in the year 1740), purely to simplify certain calculations, put the Julian Dating System (proposed in the year 1583 by Joseph Scaliger) and with it an astronomical era into use, which contains a leap year zero, which precedes the year 1 (AD).\nPrehistory.\nWhile of critical importance to the historian, methods of determining chronology are used in most disciplines of science, especially astronomy, geology, paleontology and archaeology.\nIn the absence of written history, with its chronicles and , late 19th century archaeologists found that they could develop relative chronologies based on pottery techniques and styles. In the field of Egyptology, William Flinders Petrie pioneered sequence dating to penetrate pre-dynastic Neolithic times, using groups of contemporary artefacts deposited together at a single time in graves and working backwards methodically from the earliest historical phases of Egypt. This method of dating is known as seriation.\nKnown wares discovered at strata in sometimes quite distant sites, the product of trade, helped extend the network of chronologies. Some cultures have retained the name applied to them in reference to characteristic forms, for lack of an idea of what they called themselves: \"The Beaker People\" in northern Europe during the 3rd millennium BCE, for example. The study of the means of placing pottery and other cultural artifacts into some kind of order proceeds in two phases, classification and typology: Classification creates categories for the purposes of description, and typology seeks to identify and analyse changes that allow artifacts to be placed into sequences.\nLaboratory techniques developed particularly after mid-20th century helped constantly revise and refine the chronologies developed for specific cultural areas. Unrelated dating methods help reinforce a chronology, an axiom of corroborative evidence. Ideally, archaeological materials used for dating a site should complement each other and provide a means of cross-checking. Conclusions drawn from just one unsupported technique are usually regarded as unreliable.\nSynchronism.\nThe fundamental problem of chronology is to synchronize events. By synchronizing an event it becomes possible to relate it to the current time and to compare the event to other events. Among historians, a typical need is to synchronize the reigns of kings and leaders in order to relate the history of one country or region to that of another. For example, the Chronicon of Eusebius (325 A.D.) is one of the major works of historical synchronism. This work has two sections. The first contains narrative chronicles of nine different kingdoms: Chaldean, Assyrian, Median, Lydian, Persian, Hebrew, Greek, Peloponnesian, Asian, and Roman. The second part is a long table synchronizing the events from each of the nine kingdoms in parallel columns.\nBy comparing the parallel columns, the reader can determine which events were contemporaneous, or how many years separated two different events. To place all the events on the same time scale, Eusebius used an Anno Mundi (A.M.) era, meaning that events were dated from the supposed beginning of the world as computed from the Book of Genesis in the Hebrew Pentateuch. According to the computation Eusebius used, this occurred in 5199 B.C. The Chronicon of Eusebius was widely used in the medieval world to establish the dates and times of historical events. Subsequent chronographers, such as George Syncellus (died circa 811), analyzed and elaborated on the Chronicon by comparing with other chronologies. The last great chronographer was Joseph Justus Scaliger (1540-1609) who reconstructed the lost Chronicon and synchronized all of ancient history in his two major works, \"De emendatione temporum\" (1583) and \"Thesaurus temporum\" (1606). Much of modern historical datings and chronology of the ancient world ultimately derives from these two works. Scaliger invented the concept of the Julian Day which is still used as the standard unified scale of time for both historians and astronomers.\nIn addition to the literary methods of synchronism used by traditional chronographers such as Eusebius, Syncellus and Scaliger, it is possible to synchronize events by archaeological or astronomical means. For example, the Eclipse of Thales, described in the first book of Herodotus can potentially be used to date the Lydian War because the eclipse took place during the middle of an important battle in that war. Likewise, various eclipses and other astronomical events described in ancient records can be used to astronomically synchronize historical events. Another method to synchronize events is the use of archaeological findings, such as pottery, to do sequence dating.\nSee also.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55932", "revid": "901", "url": "https://en.wikipedia.org/wiki?curid=55932", "title": "Packet switched", "text": ""}
{"id": "55933", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=55933", "title": "Integrated Development Environment", "text": ""}
{"id": "55934", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=55934", "title": "Eric Corley", "text": "American magazine publisher and hacker (born 1959)\nEric Gordon Corley (born December 16, 1959), also frequently referred to by his pen name of Emmanuel Goldstein, is a figure in the hacker community. He directs the non-profit organization 2600 Enterprises, Inc., publishes a magazine called \"\" (which has associated monthly meet-ups around the world), and hosts the hacker convention Hackers on Planet Earth (HOPE). His pseudonym is derived from the fictional opposition leader in George Orwell's dystopian novel \"Nineteen Eighty-Four\".\nIn 1993, Corley testified before the United States House of Representatives Subcommittee on Telecommunications. Corley was questioned in relation to the content of \"2600\" as part of discussions concerning the Digital Telephony Bill; also known as the Communications Assistance for Law Enforcement Act.\nCorley is the editor of \"The Best of 2600: A Hacker Odyssey\" which was released July 2008. The book consists of articles from the magazine \"\" set in chronological order to show the evolution of the internet and technology. A follow-up book, \"Dear Hacker. Letters to the Editor of 2600\", was published in 2010.\nCorley is the host of both the weekly radio programs \"Off the Hook\" on WBAI-FM and \"Off the Wall\" on WUSB-FM. While \"Off the Hook\" often includes a panel of guests and is frequently centered on technological topics, \"Off the Wall\" is usually narrated by Eric Corley himself and has covered a wide range of topics. \"Off the Hook\" has been on the air since 1988.\nFilmography.\nCorley directed the 2001 film \"Freedom Downtime\", a documentary about the incarcerations of Kevin Mitnick and Bernie S that also examines alleged distortions in mainstream media coverage of Mitnick's case.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "55937", "revid": "38056903", "url": "https://en.wikipedia.org/wiki?curid=55937", "title": "Glossary of communication disorders", "text": "This is a glossary of medical terms related to communication disorders which are psychological or medical conditions that could have the potential to affect the ways in which individuals can hear, listen, understand, speak and respond to others.\n&lt;templatestyles src=\"Hlist/styles.css\"/&gt;\n * References\nA.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nB.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nC.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nD.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nE.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nG.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nH.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nI.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nK.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nL.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nM.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nN.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nO.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nP.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nR.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nS.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nT.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nU.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nV.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nW.\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55940", "revid": "26325918", "url": "https://en.wikipedia.org/wiki?curid=55940", "title": "Nulla poena sine lege", "text": "Latin doctrine meaning \"no penalty without law\"\nNulla poena sine lege (Latin for \"no penalty without law\", Anglicized pronunciation: ) is a legal formula which, in its narrow interpretation, states that one can only be punished for doing something if a penalty for this behavior is fixed in criminal law. As some laws are unwritten (e.g. in oral law or customary law) and laws can be interpreted broadly, it does not necessarily mean that an action will not be punished simply because a specific rule against it is not codified.\nThe variant \"nullum crimen sine lege\" (\"no crime without law\") establishes that conduct is not criminal if not found among the behavior/circumstance combinations of a statute. The other interpretations of the formula include the rules prohibiting retroactive criminalization and prescribing laws to be strictly construed.\nDespite the use of Latin language and brocard-like appearance, the formula was mostly born in 18th century liberalism (some elements of non-retroactivity of laws and limiting the punishment to the one prescribed in the statute date back to Roman times). This principle is accepted and codified in modern democratic states as a basic requirement of the rule of law. It has been described as \"one of the most 'widely held value-judgement[s] in the entire history of human thought'\".\nRequirements.\nIn modern European criminal law, e.g. of the Constitutional Court of Germany, the principle of \"nulla poena sine lege\" has been found to consist of four separate requirements:\nIn common law.\nOne complexity is the lawmaking power of judges under common law. Even in civil law systems that do not admit judge-made law, it is not always clear when the function of interpretation of the criminal law ends and judicial lawmaking begins.\nIn English criminal law there are offences of common law origin. For example, murder is still a common law offence and lacks a statutory definition. The Homicide Act 1957 did not include a statutory definition of murder (or any other homicidal offence). Therefore, the definition of murder was the subject of no fewer than six appeals to the House of Lords within the following 40 years (\"Director of Public Prosecutions v. Smith\" [1961] A.C. 290; \"Hyam v. Director of Public Prosecutions\" [1975] A.C. 55; \"Regina v. Cunningham\" [1982] A.C. 566; \"Regina v. Moloney\" [1985] A.C. 905; \"Regina v. Hancock\" [1986] A.C. 455; \"Regina v. Woollin\" [1998] 4 A11 E.R. 103 (H.L.)).\nIn natural law.\nThe legal principle \"nulla poena sine lege\" as principle in natural law is due to the contention of scholars of the Scholasticism about the preconditions of a guilty conscience. In relation to the Ezekiel-commentary of Jerome, Thomas Aquinas and Francisco Su\u00e1rez analysed the formal conditions of the punishment of conscience. Thomas located the conditions within the synderesis. For him it is a formal and active part of the human soul. Understanding of activity, which is in accordance with the human nature, is formal possible due to the synderesis. Hence the synderesis contains in the works of patristic authors a law which commands how the human as human has to act. In the individual case this law is contentual definite. For the scholastic this is shown in the action of the intellect. This action is named since Thomas \"conscientia\". A possible content of the \"conscientia\" is the punishment in concordance with the content of the synderesis, in case the human has had not act in concordance with the human nature. An example for the punishment is madness, which since antiquity is a punishment of conscience. The Oresteia is a famous example for this.\nAccording Su\u00e1rez the punishment of conscience is the insight in an obligation to act in concordance with the human nature to undo a past misdeed. This insight obligates to impossible actions due to the fact that the misdeed is in the past and hence it is unchangeable. Therefore the \"conscientia\" obligates in concordance with the synderesis to do an impossible action. Hence the \"conscientia\" restricts conscientious persons by doing a limitation on their own will. For they are unable to think about any other action than to fulfil their obligation. Inasmuch the conscientia restricts the intellect the scholastic speak of it as a \"malum\" or \"malum metaphysicum\", because the limitation is related to a metaphysical quality of a human. The law is constituted by the human nature itself from what the \"malum metaphysicum\" is inflicted. Therefore the punishment of the conscience is executed because of a violation of natural law.\nWhen coming to terms with the Nazi crimes after World War II in Austria, the Austrian legal scholar and judge Wilhelm Malaniuk justified the admissibility of the non-application of the \"nulla poena sine lege\" with regard to the Austrian Verbotsgesetz 1947: \"Because these are crimes that are so grossly violate the laws of humanity!\" Regarding war crimes law and war crimes related to command structures, Malaniuk said: \u201cIn the war instigated by the National Socialists, the requirements of humanity as well as the principles of international law and martial law were violated to such an extent that it was no longer just the government that was believed to be responsible for this, but also the individual citizens, because they knew had to that their actions grossly violate the principles, compliance with which must be demanded from every member of the occidental culture.\"\nIn cases of universal jurisdiction.\nThe question of jurisdiction may sometimes come to contradict this principle. For example, customary international law allows the prosecution of pirates by any country (applying universal jurisdiction), even if they did not commit crimes at the area that falls under this country's law. A similar principle has appeared in the recent decades with regard to crimes of genocide (see genocide as a crime under domestic law); and UN Security Council Resolution 1674 \"reaffirms the provisions of paragraphs 138 and 139 of the 2005 World Summit Outcome Document regarding the responsibility to protect populations from genocide, war crimes, ethnic cleansing and crimes against humanity\" even if the State in which the population is being assaulted does not recognise these assaults as a breach of domestic law. However, it seems that universal jurisdiction is not to be expanded substantially to other crimes, so as to satisfy \"Nulla poena sine lege\".\nSince the Nuremberg Trials, penal law is taken to include the prohibitions of international criminal law, in addition to those of domestic law. Thus, prosecutions have been possible of such individuals as Nazi war criminals and officials of the German Democratic Republic responsible for the Berlin Wall, even though their deeds may have been allowed or even ordered by domestic law. Also, courts when dealing with such cases will tend to look to the letter of the law at the time, even in regimes where the law as it was written was generally disregarded in practice by its own authors.\nHowever, some legal scholars criticize this, because generally, in the legal systems of mainland Europe where the maxim was first developed, \"penal law\" was taken to mean statutory penal law, so as to create a guarantee to the individual, considered as a fundamental right, that he would not be prosecuted for an action or omission that was not considered a crime according to the statutes passed by the legislators in force at the time of the action or omission, and that only those penalties that were in place when the infringement took place would be applied. Also, even if one considers that certain actions are prohibited under general principles of international law, critics point out that a prohibition in a general principle does not amount to the establishment of a crime, and that the rules of international law also do not stipulate specific penalties for the violations.\nIn an attempt to address those criticisms, the statute of the recently established International Criminal Court provides for a system in which crimes and penalties are expressly set out in written law, that shall only be applied to future cases. See Article 22 of the Rome Statute, however this is under the proviso, in Article 22(3) that this only applies to the ICC, and \"doesn't affect the characterization of any conduct as criminal under international law independently of [the Rome Statute]\".\nThe principle of nulla poena sine lege, insofar as it applies to general criminal law, is enshrined in several national constitutions, and international instruments, see European Convention on Human Rights, article 7(1). However, when applied to international criminal/humanitarian law, the same legal instruments often allow for ex post facto application of the law. See ECHR, article 7(2), which states that article 7(1) \"shall not prejudice the trial and punishment of any person for any act or omission which, at the time when it was committed, was criminal according to the general principles of law recognised by civilised nations.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "55941", "revid": "431183", "url": "https://en.wikipedia.org/wiki?curid=55941", "title": "Jabber", "text": "To jabber means to babble incoherently. \nJabber may also refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "55942", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=55942", "title": "Special Air Service", "text": "Special forces of the British Army\nMilitary unit\nThe Special Air Service (SAS) is a special forces unit of the British Army. It was founded as a regiment in 1941 by David Stirling, and in 1950 it was reconstituted as a corps. The unit specialises in a number of roles including counter-terrorism, hostage rescue, direct action and special reconnaissance. Much of the information about the SAS is highly classified, and the unit is not commented on by either the British government or the Ministry of Defence due to the secrecy and sensitivity of its operations.\nThe corps consists of the 22 Special Air Service Regiment, which is the regular component, as well as the 21 Special Air Service Regiment (Artists) (Reserve) and the 23 Special Air Service Regiment (Reserve), which are reserve units, all under the operational command of United Kingdom Special Forces (UKSF). Its sister unit is the Royal Navy's Special Boat Service, which specialises in maritime counter-terrorism. Both units are under the operational control of the Director Special Forces.\nThe Special Air Service traces its origins to 1941 during the Second World War. It was reformed as part of the Territorial Army in 1947, named the 21st Special Air Service Regiment (Artists Rifles). The 22nd Special Air Service Regiment, which is part of the regular army, gained fame and recognition worldwide after its televised rescue of all but two of the hostages held during the 1980 Iranian Embassy siege.\nHistory.\nSecond World War.\nThe Special Air Service was a unit of the British Army during the Second World War that was formed in July 1941 by David Stirling and originally called \"L\" Detachment, Special Air Service Brigade\u00a0\u2013 the \"L\" designation and Air Service name being a tie-in to a British disinformation campaign, trying to deceive the Axis into thinking there was a paratrooper regiment with numerous units operating in the area (the real SAS would \"prove\" to the Axis that the fake one existed). It was conceived as a commando force to operate behind enemy lines in the North African Campaign and initially consisted of five officers and 60 other ranks. Its first mission, in November 1941, was a parachute drop in support of the Operation Crusader offensive, codenamed Operation Squatter. Due to German resistance and adverse weather conditions, the mission was a disaster, with only 22 men, a third of the unit, making it back to base. The rest were either killed or captured. Its second mission was a major success. Transported by the Long Range Desert Group, it attacked three airfields in Libya, destroying 60 aircraft without loss. In September 1942, it was renamed 1st SAS, consisting at that time of four British squadrons, one Free French, one Greek, and the .\nIn January 1943, Colonel David Stirling was captured in Tunisia and Paddy Mayne replaced him as commander. In April 1943, the 1st SAS was reorganised into the Special Raiding Squadron under Mayne's command and the Special Boat Squadron was placed under the command of George Jellicoe. The Special Raiding Squadron fought in Sicily and Italy along with the 2nd SAS, which had been formed in North Africa in 1943 in part by the renaming of the Small Scale Raiding Force under the command of Bill Stirling (brother of David). The Special Boat Squadron fought in the Aegean Islands and Dodecanese until the end of the war. In 1944 the SAS Brigade was formed. The unit was formed from:\nIt was tasked with parachute operations behind the German lines in France and carried out operations supporting the Allied advance through France (Operations Houndsworth, Bulbasket, Loyton, Kipling and Wallace-Hardy), Belgium, the Netherlands (Operation Pegasus), and eventually into Germany (Operation Archway and Operation Howard). As a result of Hitler's issuing of the Commando Order on 18 October 1942, the members of the unit faced the additional danger that they would be summarily executed if captured by the Germans. In July 1944, following Operation Bulbasket, 34 captured SAS commandos were indeed summarily executed by the Germans; in October 1944, in the aftermath of Operation Loyton, another 31 captured SAS commandos were summarily executed by the Germans.\nThe last original member of the Special Air Service and the last survivor of the Long Range Desert Group, Mike Sadler, died on 4 January 2024, at the age of 103.\nPost-war.\nAt the end of the war the British government saw no further need for the force and disbanded it on 8 October 1945.\nThe following year it was decided there was a need for a long-term deep-penetration commando unit and a new SAS regiment was to be raised as part of the Territorial Army. Ultimately, the Artists Rifles, raised in 1860 and headquartered at Dukes Road, Euston, took on the SAS mantle as 21st SAS Regiment (V) on 1 January 1947.\nJohn Woodhouse was chosen to assist with establishing a reformed selection process for the SAS. The rigorous systems he assisted in developing over three years provided the basis of selection and training of the modern SAS.\nMalayan Scouts.\nIn 1950, a 21 SAS squadron was raised to fight in the Korean War. After three months of training in Britain, it was informed that the squadron would no longer be required in Korea and so it instead volunteered to fight in the Malayan Emergency. Upon arrival in Malaya, it came under the command of Mike \"Mad Mike\" Calvert who was forming a new unit called the Malayan Scouts (SAS). Calvert had already formed one squadron from 100 volunteers in the Far East, which became A Squadron; the 21 SAS squadron then became B Squadron; and after a recruitment visit to Rhodesia by Calvert, C Squadron was formed from 100 Rhodesian volunteers. The Rhodesians returned home after three years' service and were replaced by a New Zealand squadron. By this time the need for a regular army SAS regiment had been recognised; the 22 SAS Regiment was formally added to the army list in 1952 and has been based at Hereford since 1960. In 1959 the third regiment, the 23 SAS Regiment, was formed by renaming the Reserve Reconnaissance Unit, which had succeeded MI9 and whose members were experts in escape and evasion.\n22 SAS Regiment.\nSince serving in Malaya, men from the regular army 22 SAS Regiment have taken part in reconnaissance patrols and large scale raiding missions in the Jebel Akhdar War in Oman and conducted covert reconnaissance and surveillance patrols and some larger scale raiding missions in Borneo during the Indonesia\u2013Malaysia confrontation. They returned to Oman in operations against Communist-backed rebels in the Dhofar Rebellion including the Battle of Mirbat. They have also taken part in operations in the Aden Emergency, Northern Ireland, and Gambia. Their Special projects team assisted the West German counterterrorism group GSG 9 at Mogadishu, with Lufthansa Flight 181. The SAS counter terrorist wing famously took part in a hostage rescue operation during the Iranian Embassy Siege in London. SAS were involved throughout Britain's covert involvement in the Soviet\u2013Afghan War; they acted through private military contractor Keenie Meenie Services (or KMS Ltd), training the Afghan Mujaheddin in weapons, tactics and using explosives. They trained the Mujaheddin in Afghanistan and sent them to be trained in Pakistan, Oman and parts of the UK. During the Falklands War B squadron were prepared for Operation Mikado before it was subsequently cancelled while D and G squadrons were deployed and participated in the raid on Pebble Island. Operation Flavius was a controversial operation in Gibraltar against the Provisional Irish Republican Army (PIRA). 22 SAS directed NATO aircraft onto Serb positions and hunted war criminals in Bosnia. They were involved in the Kosovo War helping KLA guerillas behind Serbian lines. According to Albanian sources one SAS sergeant was killed by Serbian special forces.\nThe Gulf War, in which A, B and D squadrons deployed, was the largest SAS mobilisation since the Second World War, also notable for the failure of the Bravo Two Zero mission. In Sierra Leone it took part in Operation Barras, a hostage rescue operation, to extract members of the Royal Irish Regiment.\nFollowing the September 11 attacks on the United States by al-Qaeda in 2001, two squadrons of 22 SAS, later reinforced by members of both the Territorial SAS units, deployed to Afghanistan as part of the Coalition invasion at the start of the War in Afghanistan, to dismantle and destroy al-Qaeda and to deny it a safe base of operations in Afghanistan by removing the Taliban from power in the war on terror. The Regiment carried out Operation Trent, the largest operation in its history, which included its first wartime HALO parachute jump. Following the invasion, the Regiment continued to operate in Afghanistan against the Taliban and other insurgents until 2006, when its deployment to Iraq became its focus of operations, until 2009 when the SAS redeployed to Afghanistan.\nThe regiment took part in the Iraq War, notably carrying out operations in Iraq before the 2003 invasion. Following the invasion, it formed part of Task Force Black/Knight to combat the post invasion insurgency; in late 2005/early 2006, the SAS were integrated into JSOC and focused its counterinsurgency efforts on combating al-Qaeda in Iraq and the Sunni insurgency alongside Delta Force. The counter-insurgency was successful, and the UKSF mission in Iraq ended in May 2009. Overall, more than 3,500 terrorists were \"taken off the streets\" of Baghdad by 22 SAS.\nVarious British newspapers have speculated on SAS involvement in Operation Ellamy and the 2011 Libyan civil war. The \"Daily Telegraph\" reports that \"defence sources have confirmed that the SAS has been in Libya for several weeks, and played a key role in coordinating the fall of Tripoli.\" While \"The Guardian\" reports \"They have been acting as forward air controllers\u00a0\u2013 directing pilots to targets\u00a0\u2013 and communicating with NATO operational commanders. They have also been advising rebels on tactics.\"\nMembers of the Special Air Service were deployed to Northern Iraq in late August 2014, and according to former SIS chief Richard Barrett, would also be sent to Syria, tasked with trying to track down the Islamic State of Iraq and the Levant (ISIL) terrorist group that the press labelled the Beatles. In 2024 it was acknowledged that five SAS members had been arrested by the Royal Military Police on suspicion of committing war crimes in Syria, though details have not been disclosed.\nSince the 1990s SAS officers have risen to senior appointments in the British Armed Forces. General Peter de la Billi\u00e8re was the commander in chief of the British forces in the 1990 Gulf War. General Michael Rose became commander of the United Nations Protection Force in Bosnia in 1994. In 1997 General Charles Guthrie became Chief of the Defence Staff the head of the British armed forces. Lieutenant-General Cedric Delves was appointed commander of the Field Army and deputy commander in chief NATO Regional Headquarters Allied Forces North in 2002\u20132003.\n21 and 23 SAS.\nFor much of the Cold War, the role of 21 SAS and 23 SAS was to provide stay-behind parties in the event of a Warsaw Pact invasion of western Europe, forming together I Corps' Corps Patrol Unit. In the case of an invasion, this Special Air Service Group would have let themselves be bypassed and remained behind in order to collect intelligence behind Warsaw Pact lines, conduct target acquisition, and thus try to slow the enemy's advance.\nIn early 2003, a squadron of about 60 soldiers from 21 SAS and 23 SAS, were deployed to Afghanistan. In 2005, for the first time since the Malayan Emergency a whole Reserve squadron deployed from one of the regiments to Afghanistan to conduct reconnaissance of Helmand province in preparation for the establishment of a Task Force based around 16 Air Assault Brigade.\nInfluence on other special forces.\nFollowing the post-war reconstitution of the Special Air Service, other countries in the Commonwealth recognised their need for similar units. The Canadian Special Air Service Company was formed in 1947, being disbanded in 1949. The New Zealand Special Air Service squadron was formed in June 1955 to serve with the British SAS in Malaya, which became a full regiment in 2011. Australia formed the 1st SAS Company in July 1957, which became a full regiment of the Special Air Service Regiment (SASR) in 1964. On its return from Malaya, the C (Rhodesian) Squadron formed the basis for creation of the Rhodesian Special Air Service in 1961. It retained the name \"C Squadron (Rhodesian) Special Air Service\" within the Rhodesian Security Forces until 1978, when it became 1 (Rhodesian) Special Air Service Regiment.\nNon-Commonwealth countries have also formed units based on the SAS. The Belgian Army's Special Forces Group, which wears the same capbadge as the British SAS, traces its ancestry partly from the 5th Special Air Service of the Second World War. The French 1st Marine Infantry Parachute Regiment (1er RPIMa) can trace its origins to the Second World War 3rd and 4th SAS, adopting its \"who dares wins\" motto. The American unit, 1st Special Forces Operational Detachment-Delta, was formed by Colonel Charles Alvin Beckwith, who served with 22 SAS as an exchange officer, and recognised the need for a similar type of unit in the United States Army. The Israeli Sayeret Matkal and Shaldag units have also been modelled after the SAS, sharing its motto. Ireland's Army Ranger Wing (ARW) also trains with the SAS. The Philippine National Police's Special Action Force was formed along the lines of the SAS.\nThe former Royal Afghan Army's 666th Commando Brigade was formed by Colonel Ramatullah Safi in the 1970s after he received his training with the SAS before it was disbanded through purges after the coups in 1973 and 1978.\nOrganisation.\nLittle publicly verifiable information exists on the contemporary SAS, as the British government usually does not comment on special forces matters due to the nature of their work. The Special Air Service comprises three units: one Regular and two Army Reserve (AR) units. The regular army unit is 22 SAS Regiment and the reserve units are 21 Special Air Service Regiment (Artists) (Reserve) (21 SAS(R)) and 23 Special Air Service Regiment (23 SAS (R)), collectively, the Special Air Service (Reserve) (SAS(R)).\nSpecial Forces Parachute Support Squadron (Para Sp Sqn) is a sub-unit of the Airborne Delivery Wing (ADW) based at RAF Brize Norton.\nSupplementary to the SAS, together with the Special Boat Service and the Special Reconnaissance Regiment is 18 (UKSF) Signal Regiment.\nSquadrons.\n22 SAS normally has a strength of 400 to 600. The regiment has four operational squadrons: A, B, D and G. Each squadron consists of approximately 65 members commanded by a major, divided into four troops (each troop being commanded by a captain) and a small headquarters section. Troops usually consist of 16 members. Members of the SAS are variously known as \"blade\" or \"operator\". Each patrol within a troop consists of four members, with each member possessing a particular skill e.g. signals, demolition, medic or linguist in addition to basic skills learned during the course of his training. The term \"squadron\" dates back to the unit's earliest days when the unit's name was intended to confuse German intelligence. The four troops specialise in four different areas:\nIn 1980 R Squadron (which has since been renamed L Detachment) was formed; its members are all ex-regular SAS regiment soldiers who have a commitment to reserve service.\n22 SAS squadron duty rotations are set up as such that one squadron is maintained on Counter-Terrorism duty in the UK; a second will be on a deployment; a third will be preparing for deployment whilst conducting short term training; and the fourth will be preparing for long-term overseas training such as jungle or desert exercises. In times of war, such as the 2003 invasion of Iraq, it is not uncommon for two squadrons to be deployed.\nSquadron Structure:\nCounter Terrorist Wing.\nThe SAS has a subunit called the Counter Terrorist Wing (CTW) that fulfils its counterterrorism (CT) role. It has previously been known as the Counter Revolutionary Warfare (CRW) Wing and special projects team. The SAS receives aviation support from No. 658 Squadron AAC to carry out their CT role.\nThe CTW is trained in Close Quarter Battle (CQB), sniper techniques and specialises in hostage rescue in buildings or on public transport. The team was formed in the early 1970s after the Prime Minister, Edward Heath, asked the Ministry of Defence to prepare for any possible terrorist attack similar to the massacre at the 1972 Summer Olympics therefore ordering that the SAS Counter Revolutionary Warfare (CRW) wing be raised.\nSquadrons refresh their training every 16 months, on average. The CRW's first deployment was during the Balcombe Street siege. The Metropolitan Police had trapped a PIRA unit; it surrendered when it heard on the BBC that the SAS were being sent in. The first documented action abroad by the CRW wing was assisting the West German counter-terrorism group GSG 9 at Mogadishu.\nThe CT role was shared amongst the squadrons, initially on a 12-month and later six-month rotation basis to ensure that all members are eventually trained in CT and CQB techniques. The SAS train for the CT role at Pontrilas Army Training Area in a facility that includes the Killing House (officially known as Close Quarter Battle House) and part of a Boeing 747 airliner that can be reconfigured to match the internal layouts of virtually any commercial aircraft. The on-call CT squadron is split into four troops, two of which are on immediate notice to move and are restricted to the Hereford-Credenhill area, whilst the other two conduct training and exercises across the UK, but are available for operational deployment should the need arise.\nOperational command.\nRegular.\n22 SAS is under the operational command of the Director Special Forces (DSF), a major-general grade post. Previously ranked as a brigadier, the DSF was promoted from brigadier to major-general in recognition of the significant expansion of the United Kingdom Special Forces (UKSF).\nReserve.\nOn 1 September 2014, 21 and 23 SAS were moved from UKSF. They were placed under command of 1st Intelligence, Surveillance and Reconnaissance Brigade. In 2019 they were moved back to UKSF.\nRecruitment and training.\nThe first version of the SAS selection course was created by John Woodhouse in 1952. The United Kingdom Special Forces do not recruit directly from the general public. All current members of the UK Armed Forces can apply for Special Forces selection, but the majority of candidates have historically come from a Royal Marines or Parachute Regiment background. Selections are held twice a year, once in summer and again in winter.\nTypically only 10% of candidates make it through the initial selection process. Between 2014 and 2022 there were more deaths in training and exercises than in combat against armed threats. In a group of approximately 200 candidates, most will drop out within the first few days, and fewer than 30 will remain by the end. Those who complete all phases of selection are transferred to an operational squadron.\nFor applicants to the reserve component, 21 SAS and 23 SAS, the pathway involves comparable elements, apart from jungle training, but taken in blocks, spread out over a longer period, to fit in with the demands of participants' civilian careers. In October 2018, recruitment policy changed to allow women to become members of the SAS for the first time. In August 2021, two women became the first to pass the pre-selection course, making them eligible for the full course.\nThe first phase of selection, aptitude phase, lasts 4 weeks and takes place in the Brecon Beacons. This phase also involves training in Sennybridge, and normally starts with approximately 200 potential candidates. Candidates complete a Personal Fitness Test (PFT) upon arrival, which consists of at least 50 sit-ups in two minutes, 60 press-ups in two minutes, and a run in 10 minutes and 30 seconds. They then complete an Annual Fitness Test (AFT), which consists of marching in two hours while carrying of equipment. Candidates then march cross-country against the clock, increasing the distance covered each day; this culminates in an endurance test known as the \"Endurance\", in which candidates march with full equipment before climbing up and down the mountain Pen y Fan (886 m; 2,907\u00a0ft) in 20 hours. By the end of this phase, candidates must then be able to run in 30 minutes or less and swim in 90 minutes or less.\nAfter completing aptitude phase, officer candidates are required to spend a week assessing their ability to carry out planning for UKSF operations while fatigued and stressed. Following mountain training, the jungle phase takes place in Belize, Brunei, or Malaysia. Candidates are taught navigation, patrol formation and movement, and jungle survival skills. Candidates then return to the UK to begin training in battle plans and foreign weapons, and then take part in combat survival exercises, ending in week-long escape and evasion training. Candidates are formed into patrols and, with nothing more than a tin can filled with survival equipment, are dressed in World War II-era uniforms and told to head for a particular destination by sunrise. The final selection test, resistance to interrogation (RTI), lasts for 36 hours.\nUniform distinctions.\nNormal barracks headdress is the sand-coloured beret, its cap badge is a downward pointing Excalibur, wreathed in flames (often incorrectly referred to as a winged dagger) worked into the cloth of a Crusader shield with the motto \"Who Dares Wins\". SAS pattern parachute wings, designed by Lieutenant Jock Lewes and based on the stylised sacred ibis of Isis of Egyptian iconography depicted in the d\u00e9cor of Shepheard's Hotel in Cairo, are worn on the right shoulder. Its is distinguished by a light-blue stripe on the trousers. Its stable belt is a shade of blue similar to the blue stripe on the No 1 dress uniform.\nBattle honours.\nIn the British Army, battle honours are awarded to regiments that have seen active service in a significant engagement or campaign, generally with a victorious outcome. The Special Air Service Regiment has been awarded the following battle honours:\nMemorials.\nThe names of those members of the Regular SAS who have died on duty were inscribed on the regimental clock tower at Stirling Lines. Originally funded by contributions of a day's pay by members of the regiment and a donation from Handley Page in memory of Cpl. R.K. Norry who was killed in a freefall parachuting accident, this was rebuilt at the new barracks at Credenhill. Those whose names are inscribed are said by surviving members to have \"failed to beat the clock\". At the suggestion of the then Commanding Officer, Dare Wilson, inscribed on the base of the clock is a verse from \"The Golden Journey to Samarkand\" by James Elroy Flecker:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe other main memorial is the SAS and Airborne Forces memorial in the cloisters at Westminster Abbey. The SAS Brigade Memorial at Sennecey-le-Grand in France commemorates the wartime dead of the Belgian, British and French SAS and recently a memorial plaque was added to the David Stirling Memorial in Scotland. There are other smaller memorials \"scattered throughout Europe and in the Far East\".\nThe local church of St Martin's, Hereford has part of its graveyard set aside as an SAS memorial, over twenty SAS soldiers are buried there. There is also a wall of remembrance displaying memorial plaques to some who could not be buried, including the 18 SAS men who lost their lives in the Sea King helicopter crash during the Falklands Campaign on 19 May 1982 and a sculpture and stained glass window dedicated to the SAS.\nOn 17 October 2017 \"Ascension\", a new sculpture and window honouring the Special Air Service Regiment in Hereford Cathedral, was dedicated by the Bishop of Hereford at a service attended by Prince William.\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "55943", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=55943", "title": "Gouraud Shading", "text": ""}
{"id": "55944", "revid": "50911835", "url": "https://en.wikipedia.org/wiki?curid=55944", "title": "Eh", "text": "Spoken interjection in English\nEh ( or ) is a spoken interjection used in many varieties of English. The oldest \"Oxford English Dictionary\" defines \"eh\" as an \"interjectional interrogative particle often inviting assent to the sentiment expressed.\" Today, while \"eh\" has many different uses, it is most popularly used in a manner similar in meaning to \"Excuse me?\", \"Please repeat that\", \"Huh?\", or to otherwise mark a question. It is also commonly used as an alternative to the question tag \"right?\", as a method for inciting a reply, as in \"Don't you think?\", \"You agree with me, right?\", as in, \"It's nice here, eh?\" (instead of \"It's nice here, right?\"). In the Americas, it is most commonly associated with Canada and Canadian English, though it is also common in England, Scotland, and New Zealand. It is also known in some American regions bordering Canada, including the area stretching from northern Wisconsin up to Michigan's Upper Peninsula. Similar interjections exist in many other languages, such as Azerbaijani, Italian, and Dutch.\nThe spelling of this sound in English is quite different from the common usage of these letters. The vowel is sounded in one of the continental manners (as in French, only missing the apostrophe), and the letter \"h\" is used to indicate it is long, as though the origin of the spelling were German.\nWhile evidence suggests that \"eh\" initially may have been considered as an onomatopoeic sound, the earliest uses of \"eh\" found so far, date back to Early Modern English in 1662, but first mentions of it are found in Middle English. In 1707, it was first used in a play, functioning \"to create or confirm agreement.\" Later, in 1773, its earliest quotation, s.v. \"eh\" was in a play by Irish playwright Oliver Goldsmith. \nIt can also convey a lack of strong emotion and a neutral response. For example, if when asked how a movie was one replies with \"Eh,\" this indicates that they did not find it particularly great or terrible. In this example, \"eh\" is used as a way to convey a middle-ground feeling or invite further discussion.\nEnglish.\nUnited States.\n\"Eh\" is also used in situations to describe something bad or mediocre. In which, it is often pronounced with a short \"e\" sound and the \"h\" may even be noticeable.\nIt is quite prevalent in the New York area to use the term \"ey\" as a general substitute for basic greetings, such as \"hey\" or \"hello\".\nIn the Upper Midwest, it is used to end sentences.\nCanada.\nHistory.\nThe first clear evidence of \"eh\"'s usage in Canada was in 1836, through the writings of Thomas Chandler Haliburton, a Nova-Scotian district judge and comical writer. \"Eh\" was first recognized as being a marker of being Canadian in 1959 by Harold B. Allen; he stated that \"eh\" is \"so exclusively a Canadian feature that immigration officials use it as an identifying clue.\" However, despite mainly being perceived as a stereotypical marker of Canadian identity, \"eh\" was not recognized initially as a Canadianism in the \"Dictionary of Canadianisms on Historical Principles\" (DCHP-1). Chief editor of the DCHP-1, Walter Avis, argued that it should not be included due to its historical use in British English and its frequency in American, Australian, and New Zealand English. However, despite \"eh\"'s origins, it has become more frequently used in Canada than in the UK and the US, and in a broader variety of contexts. Due to this frequency, it has since been included in the DCHP-2 as a Preservation of British English that is Culturally Significant.\nUses.\nAccording to the DCHP-2, there are five main uses of \"eh\" with four subtypes. The first is used to elicit confirmation (1a), which can be used in sentences like \"So that's what he thinks, eh?\" A subtype of this use is to elicit acknowledgement (1b). This applies to the acknowledgment of a fact in contrast to belief or opinion. For example, one could say \"I have a new dog, eh?\" The second subtype (1c) is to confirm agreement. This is used to increase the chance of acceptance of a suggestion, toning down statements. The fourth, (1d), is used as an exclamative over a shared experience, for example \"What a great game, eh?\" The final (1e) is to confirm compliance, like asking \"Will you?\" The belief is that this tones down a command or request. \nThe second main use of \"eh\" is as an expression of disbelief to express one's surprise over the offered information (2). Use 3 is to elicit repetition, and is referred to as the \"Pardon \"eh\".\" It is used synonymously with \"I beg your pardon?\" in the sense of asking for a repetition of what was said. The fourth use is a distinctly Canadian use, identified as the narrative \"eh\". It is a rarer form, and is claimed to be found primarily in oral evidence of Canadian origin. The final use of \"eh\" is as a metalinguistic commentary to express a link with Canada or rural Canada (5). This form is commentary on the Canadian status of \"eh\" and has contributed its share to the registration of \"eh\" and commodification of the form in association with Canada. A popular example of its use is in the phrase \"How's it goin', \"eh\"?\"\nDue to English and French being Canada's official languages, the popularity of \"eh\"'s usage in Canada is believed to be influenced by French. The French Canadian sounds similar to a nasalized Canadian \"eh\", and the two share similar functions. Due to this, the increased use of \"eh\" in Canada may have been influenced by the frequent use of \"\" in Canadian French.\nThe term is used most frequently among blue-collar workers, and the most popular form used is for opinions and exclamations. While there is a prevalent stereotype that men use \"eh\" more than women, survey results suggest similar use frequencies. Overall, between both men and women, the pardon-\"eh\" is used much less than the observation-\"eh\". The most positively viewed usage of \"eh\" is the imperative \"I know, \"eh\"?\" form with the exclamation-\"eh\" and opinion-\"eh\" close behind. The most negatively viewed usage is the anecdotal, narrative-\"eh\". This perception is due to opinions surrounding the speakers of the narrative-\"eh\", who are categorized as uneducated, lower-class, rural, and male, akin to the McKenzie brothers from the comedy sketch \"Great White North,\" which first appeared during Second City Television's (SCTV) third season.\nRegionally, while usage is similar across the ten provinces, with the use of \"eh\" not having changed significantly over the past 25 years, there is some variation. For example, in Quebec, respondents use \"eh\" for 'pardon' more than other Canadians. While usage has not changed significantly across Canada, the overall frequency of \"eh\" has declined among speakers born in the 1960s or later. This decrease has been prevalent in big cities such as Vancouver and Toronto. Despite this decline, there have been high recognition rates and uptake of the Canadian \"eh\" among immigrant populations.\nIconography.\n\"Eh\" has gained such recognition among Canadians that it is used consciously and frequently by newspaper journalists and others in informal articles and reports. Also, \"eh\" is attributed freely in reported conversations with all men, including athletes, professors, and politicians, such as Pierre Trudeau.\nThe prevalence of \"eh\" in Canadian iconography is strongly associated with its recognition as part of the Canadian national or regional identity. In print, it is used primarily to signify 'Canadian,' with many websites incorporating \"eh\" into their URLs to indicate a Canadian connection. It is also popularly incorporated into Canadian-targeted marketing campaigns, such as when Smarties' Canadian-themed packaging was labelled \"SMARTIES \"eh\"?\"\nThe usage of \"eh\" in Canada is occasionally mocked in the United States, where some view its use as a stereotypical Canadianism. Such stereotypes have been reinforced in popular culture and were famously lampooned in '. Singer Don Freed, in his song \"Saskatchewan,\" declares, \"What is this Eh\"?'-nonsense? I wouldn't speak like that if I were paid to\". There are many products displaying the phrase, such as T-shirts and coffee mugs.\nFuture usage.\nThe future of \"eh\" in Canada is vague but promising. Three critical factors that will shape the future of this expression include speaker attitudes, the possible replacement of the expressions by young speakers, and new Canadians' adoption of \"eh\". Students account for a large percentage of \"eh\" users and continue to contribute to the growing community. Because of this projected increase in the usage of \"eh\", the previous negative connotation surrounding the narrative-\"eh\" will most likely dwindle. The future of \"eh\" is quite optimistic and there is room for expansion due to the various uses possible. In addition to the popularity amongst students, immigrants are essential to the future of \"eh\". Survey results on immigrant recognition of \"eh\" show that immigrants had high rates of recognition for most types of \"eh\", with opinion-\"eh\" and exclamation-\"eh\" at the top. The data shows that while the usage of \"eh\" in immigrant countries is different, it is still common. This shows that even though native speakers still use \"eh\" more frequently, the future of \"eh\" is still optimistic. Altogether, Canada's link with bilingualism has contributed to \"eh\"'s common usage, and its recognition amongst immigrants shows that \"eh\" will continue to be prevalent in Canadian culture.\nNew Zealand.\nWhile not as commonly lampooned as the Canadian \"eh\", there are few features that are more eagerly recognized by New Zealanders as a marker of their identity than the tag particle \"eh\" (commonly spelt as \"ay\", although this has been contentious). New Zealanders use \"eh\" much more than Canadians, who are more famous for the word. This commonly used and referenced feature of New Zealand English (NZE) is one of great controversy to many communication scholars as it is both a mark of cultural identity and simultaneously a means to parody those of a lower socioeconomic status. The use of \"eh\" in New Zealand is very common among all demographics.\nCommunications scholar Miriam Meyerhoff describes \"eh\" as a \"validation checker\" to create connections between speakers. She says that there are two main uses of the phrase: to signify a question, such as \"You went to school in Christchurch, eh?\"; or to confirm that the listener understands new information, such as \"He was way bigger than me, eh\". It is believed that \"eh\" became common in New Zealand due to similarity with the M\u0101ori word , which has a similar use.\nA 1994 study by Meyerhoff sought to examine the function of \"eh\" in New Zealand culture. She hypothesized that \"eh\" did not function as a clarification device as frequently believed, but instead served as a means of establishing solidarity between individuals of similar ethnic descent. In her research, Meyerhoff analyzed conversations between an interviewer and an interviewee of either P\u0101keh\u0101 or M\u0101ori descent and calculated the frequency of \"eh\" in the conversation. In order to yield the most natural speech, Meyerhoff instructed the interviewers to introduce themselves as a \"friend of a friend\", to their respective interviewees. Her results showed M\u0101ori men as the most frequent users of \"eh\" in their interviews. As M\u0101ori are typically of a lower socio-economic status, Meyerhoff proposed that \"eh\" functioned as a verbal cue that one reciprocated by another individual signified both shared identity and mutual acceptance. Therefore, in the context of Meyerhoff's research, \"eh\" can be equated as a device to establish and maintain a group identity. This phenomenon sheds light on the continuous scholarly debate questioning if language determines culture or culture determines language. In New Zealand \"eh\" is used more often by males than females, more by younger generations than older generations, and more by the middle class than the working class. M\u0101ori use \"eh\" about twice as much as P\u0101keh\u0101, irrespective of their gender, age or class.\nEngland, Scotland and Ireland.\nThe usage of the word is widespread throughout much of the UK, particularly in Eastern Scotland, the north of England, Northern Ireland, and Wales. It is normally used to mean 'what?'. In Scotland, mainly around the Tayside region, \"eh\" is also used as a shortened term for 'yes'. For example, \"Are you going to the disco?\" \"Eh\". In Aberdeen and the wider Doric Scots speaking area of Grampian, \"eh\" is often used to end a sentence, as a continuation or sometimes, inflection is added and it's used as a confirmation, or with different inflection, a question. For example, \"I was walking home, eh, and I saw a badger, eh\", \"It was a big car, eh\" or \"We're going to the co-op, eh?\".\nRest of the world.\n\"Eh?\" used to solicit agreement or confirmation is also heard regularly amongst speakers in Australia, Trinidad and Tobago and the United Kingdom (where it is sometimes spelled on the assumption that \"eh\" would rhyme with or ). In the Caribbean island of Barbados the word \"nuh\" acts similarly, as does \"noh\" in Surinamese Dutch and Sranantongo. The usage in New Zealand is similar, and is more common in the North Island. It is also heard in the United States, especially Minnesota, Wisconsin, the Upper Peninsula of Michigan (although the Scandinavian-based Yooperism \"ya\" is more common), Oklahoma, and the New England region. In New England and Oklahoma, it is also used as a general exclamation as in Scotland and the Channel Islands of Jersey and Guernsey.\nIt is occasionally used to express indifference, in a similar way to \"meh\".\nSince usage of the word \"eh\" is not as common in the United States as it is in Canada, it is often used by Americans, and indeed Canadians themselves, to parody Canadian English.\nThe equivalent in South African English is \"hey\". This usage is also common in Western Canada.\n\"Eh\" is also used in Guernsey English and Jersey English.\n\"Eh\" is very common in the English spoken in the Seychelles.\nIn Singapore, the use of medium Singlish often includes \"eh\" as an interjection, but it is not as popularly used as \"lah\". An example of a sentence that uses \"eh\" is \"Dis guy Singlish damn good eh\", meaning \"this guy's Singlish is very good\".\nSimilar to Singapore, Malaysia also uses \"eh\" in Manglish as an interjection. It is also used as an exclamation to express surprise, depending on the length and context of the \"eh\". It also depends how one sounds uses it as a short \"eh\" can be a sarcastic shock or a genuine one. Sometimes it can be used as the equivalent as \"oi\" when the speaker is being angry to the listener such as \"Eh, hello!?\" or \"Eh, can you not!?\". A long \"eeeh\" can be a disgusted shock, annoyance, or greater surprise. The \"eh\" usage here is similar to the Japanese usage. It is used by all Malaysians regardless of what language they are using.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55945", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=55945", "title": "Special air service", "text": ""}
{"id": "55946", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=55946", "title": "Security Enhanced Linux", "text": ""}
{"id": "55947", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=55947", "title": "SE Linux", "text": ""}
{"id": "55949", "revid": "394292", "url": "https://en.wikipedia.org/wiki?curid=55949", "title": "System functional testing", "text": ""}
{"id": "55951", "revid": "47555821", "url": "https://en.wikipedia.org/wiki?curid=55951", "title": "Instant messaging", "text": "Form of computer communication over the internet or locally\nInstant messaging (IM) technology is a type of synchronous computer-mediated communication involving the immediate (real-time) transmission of messages between two or more parties over the Internet or another computer network. Originally involving simple text message exchanges, modern instant messaging applications and services (also variously known as instant messenger, messaging app, chat app, chat client, or simply a messenger) tend to also feature the exchange of multimedia, emojis, file transfer, VoIP (voice calling), and video chat capabilities.\nInstant messaging systems facilitate connections between specified known users (often using a contact list also known as a \"buddy list\" or \"friend list\") or in chat rooms, and can be standalone apps or integrated into a wider social media platform, or in a website where it can, for instance, be used for conversational commerce. Originally the term \"instant messaging\" was distinguished from \"text messaging\" by being run on a computer network instead of a cellular/mobile network, being able to write longer messages, real-time communication, presence (\"status\"), and being free (only cost of access instead of per SMS message sent).\nInstant messaging was pioneered in the early Internet era; the IRC protocol was the earliest to achieve wide adoption. Later in the 1990s, ICQ was among the first closed and commercialized instant messengers, and several rival services appeared afterwards as it became a popular use of the Internet. Beginning with its first introduction in 2005, BlackBerry Messenger became the first popular example of mobile-based IM, combining features of traditional IM and mobile SMS. Instant messaging remains very popular today; IM apps are the most widely used smartphone apps: in 2018 for instance there were 980 million monthly active users of WeChat and 1.3 billion monthly users of WhatsApp, the largest IM network.\nOverview.\nInstant messaging (IM), sometimes also called \"messaging\" or \"texting\", consists of computer-based human communication between two users (private messaging) or more (chat room or \"group\") in real-time, allowing immediate receipt of acknowledgment or reply. This is in direct contrast to email, where conversations are not in real-time, and the perceived quasi-synchrony of the communications by the users (although many systems allow users to send offline messages that the other user receives when logging in).\nEarlier IM networks were limited to text-based communication, not dissimilar to mobile text messaging. As technology has moved forward, IM has expanded to include voice calling using a microphone, videotelephony using webcams, file transfer, location sharing, image and video transfer, voice notes, and other features.\nIM is conducted over the Internet or other types of networks (see also LAN messenger). Depending on the IM protocol, the technical architecture can be peer-to-peer (direct point-to-point transmission) or client\u2013server (when all clients have to first connect to the central server). Primary IM services are controlled by their corresponding companies and usually follow the client-server model.\nAt one point, the term \"Instant Messenger\" was a service mark of AOL Time Warner and could not be used in software not affiliated with AOL in the United States. For this reason, in April 2007, the instant messaging client formerly named Gaim (or gaim) announced that they would be renamed \"Pidgin\".\nClients.\nModern IM services generally provide their own client, either a separately installed application or a browser-based client. They are normally centralised networks run by the servers of the platform's operators, unlike peer-to-peer protocols like XMPP. These usually only work within the same IM network, although some allow limited function with other services (see #Interoperability). Third-party client software applications exist that will connect with most of the major IM services. There is the class of instant messengers that uses the serverless model, which doesn't require servers, and the IM network consists only of clients. There are several serverless messengers: RetroShare, Tox, Bitmessage, Ricochet, Ring. See also: LAN messenger.\nSome examples of popular IM services today include Signal, Telegram, WhatsApp Messenger, WeChat, QQ Messenger, Viber, Line, and Snapchat. The popularity of certain apps greatly differ between different countries. Certain apps have an emphasis on certain uses - for example, Skype focuses on video calling, Slack focuses on messaging and file sharing for work teams, and Snapchat focuses on image messages. Some social networking services offer messaging services as a component of their overall platform, such as Facebook's Facebook Messenger, who also own WhatsApp. Others have a direct IM function as an additional adjunct component of their social networking platforms, like Instagram, Reddit, Tumblr, TikTok, Clubhouse and Twitter; this also includes for example dating websites, such as OkCupid or Plenty of Fish, and online gaming chat platforms.\nFeatures.\nPrivate and group messaging.\nPrivate chat allows users to converse privately with another person or a group. Privacy can also be enhanced in several ways, such as end-to-end encryption by default. Public and group chat features allow users to communicate with multiple people simultaneously.\nCalling.\nMany major IM services and applications offer a call feature for user-to-user voice calls, conference calls, and voice messages. The call functionality is useful for professionals who utilize the application for work purposes and as a hands-free method. Videotelephony using a webcam is also possible by some.\nGames and entertainment.\nSome IM applications include in-app games for entertainment. Yahoo! Messenger, for example, introduced these where users could play a game and viewed by friends in real-time. MSN Messenger featured a number of playable games within the interface. Facebook's Messenger has had a built-in option to play games with people in a chat, including games like Tetris and Blackjack. Discord features multiple games built inside the \"activities\" tab in voice channels.\nPayments.\nA relatively new feature to instant messaging, peer-to-peer payments are available for financial tasks on top of communication. The lack of a service fee also makes these advantageous to financial applications. IM services such as Facebook Messenger and the WeChat 'super-app' for example offer a payment feature.\nHistory.\nEarly systems.\nThough the term dates from the 1990s, instant messaging predates the Internet, first appearing on multi-user operating systems like Compatible Time-Sharing System (CTSS) and Multiplexed Information and Computing Service (Multics) in the mid-1960s. Initially, some of these systems were used as notification systems for services like printing, but quickly were used to facilitate communication with other users logged into the same machine. CTSS facilitated communication via text message for up to 30 people.\nParallel to instant messaging were early online chat facilities, the earliest of which was Talkomatic (1973) on the PLATO system, which allowed 5 people to chat simultaneously on a 512 x 512 plasma display (5 lines of text + 1 status line per person). During the bulletin board system (BBS) phenomenon that peaked during the 1980s, some systems incorporated chat features which were similar to instant messaging; Freelancin' Roundtable was one prime example. The first such general-availability commercial online chat service (as opposed to PLATO, which was educational) was the CompuServe CB Simulator in 1980, created by CompuServe executive Alexander \"Sandy\" Trevor in Columbus, Ohio.\nAs networks developed, the protocols spread with the networks. Some of these used a peer-to-peer protocol (e.g. talk, ntalk and ytalk), while others required peers to connect to a server (see talker and IRC). The Zephyr Notification Service (still in use at some institutions) was invented at MIT's Project Athena in the 1980s to allow service providers to locate and send messages to users.\nEarly instant messaging programs were primarily real-time text, where characters appeared as they were typed. This includes the Unix \"talk\" command line program, which was popular in the 1980s and early 1990s. Some BBS chat programs (i.e. Celerity BBS) also used a similar interface. Modern implementations of real-time text also exist in instant messengers, such as AOL's Real-Time IM as an optional feature.\nIn the latter half of the 1980s and into the early 1990s, the Quantum Link online service for Commodore 64 computers offered user-to-user messages between concurrently connected customers, which they called \"On-Line Messages\" (or OLM for short), and later \"FlashMail.\" Quantum Link later became America Online and made AOL Instant Messenger (AIM, discussed later). While the Quantum Link client software ran on a Commodore 64, using only the Commodore's PETSCII text-graphics, the screen was visually divided into sections and OLMs would appear as a yellow bar saying \"Message From:\" and the name of the sender along with the message across the top of whatever the user was already doing, and presented a list of options for responding. As such, it could be considered a type of graphical user interface (GUI), albeit much more primitive than the later Unix, Windows and Macintosh based GUI IM software. OLMs were what Q-Link called \"Plus Services\" meaning they charged an extra per-minute fee on top of the monthly Q-Link access costs.\nDevelopment of the Internet Relay Chat (IRC) protocol began in 1989, and this would become the Internet's first widespread instant messaging standard.\nGraphical messengers.\nModern, Internet-wide, GUI-based messaging clients as they are known today, began to take off in the mid-1990s with PowWow, ICQ, and AOL Instant Messenger (AIM). Similar functionality was offered by CU-SeeMe in 1992; though primarily an audio/video chat link, users could also send textual messages to each other. AOL later acquired Mirabilis, the authors of ICQ; establishing dominance in the instant messaging market. A few years later ICQ (then owned by AOL) was awarded two patents for instant messaging by the U.S. patent office. Meanwhile, other companies developed their own software; (Excite, Microsoft (MSN), Ubique, and Yahoo!), each with its own proprietary protocol and client; users therefore had to run multiple client applications if they wished to use more than one of these networks. However, the open protocol IRC continued to be popular by the millennium, and its most popular graphical app was mIRC.\nWhile instant messaging was mainly in use for consumer recreational purposes, in 1998, IBM launched their Lotus Sametime instant messenger software, the first popular example of enterprise-grade instant messaging. In 2000, an open-source application and open standards-based protocol called Extensible Messaging and Presence Protocol (XMPP) was launched, initially branded as \"Jabber\". XMPP servers could act as gateways to other IM protocols, reducing the need to run multiple clients.\nVideo calling using a webcam also started taking off during this time. Microsoft's NetMeeting, which was focused on business \"web conferencing\", was one of the earliest; the company then launched Windows Messenger, coming preloaded on Windows XP, featuring video capabilities. Yahoo! Messenger added video capabilities in 2001; by 2005, such features were built-in also in AIM, MSN Messenger, and Skype.\nThere were a reported 100 million users of instant messaging in 2001. As of 2003, AIM was the globally most popular instant messenger with 195 million users and exchanges of 1.6 billion messages daily. By 2006, AIM controlled 52 percent of the instant messaging market, but rapidly declined shortly thereafter as the company struggled to compete with other services.\nIntegrated IM and mobile.\nInstant messaging integrated in other services started picking up pace in the late 2000s. Myspace, the then-largest social networking service, launched Myspace IM in 2006, shortly after Google's Gtalk, which was integrated into its Gmail webmail interface. Facebook Chat launched in 2008, providing IM to users of the social network. By 2010, traditional instant messaging was in sharp decline in favor of these new messaging features on wider social networks, which at the time were not normally called IM. For instance, AIM's userbase had declined by more than half throughout the year 2011.\nStandalone instant messenger services were revived, evolving into becoming primarily being used on mobile due to the increasing use of Internet-enabled cell phones and smartphones. Often called \"chat apps\", to distinguish it from cellular-based SMS and MMS \"texting\" services, these newer services were specially designed to be run on mobile platforms, as opposed to older services like AIM and MSN; BlackBerry Messenger, released in 2005, was one of the influential pioneers of mobile IM, and led to other companies launching services with proprietary protocols, such as WhatsApp. Mobile instant messaging surpassed SMS in global message volume by 2013. While SMS relied on traditional paid telephone services, IM apps on mobile were available for free or a minor data charge.\nOlder IM services were eventually shut, including AIM and Yahoo! Messenger, and also Windows Live Messenger, which merged into Skype in 2013. In 2014, it was reported that instant messaging had more users than social networks. Concurrently, rising use of instant messaging at workplaces led to the creation of new services (enterprise application integration (EAI)) often integrated with other enterprise applications such as workflow systems, for example in Skype for Business, Slack and Microsoft Teams. Meanwhile, the launch of Discord in 2015 has marked a notable new example of traditional IM originally designed for desktops.\nInteroperability.\nMost IM protocols are proprietary and are not designed to be interoperable with others, meaning that many IM networks have been incompatible and users have been unable to reach users on other networks. As of 2024, fragmentation of IM services means that a typical user is likely to have to use more networks than ever, including the need to download the apps and signing up, to stay in touch with all their contacts. However, there had been attempts for solutions.\nMulti-protocol clients can use any of the IM protocols by using additional local libraries for each protocol. Examples of multi-protocol instant messenger software include Pidgin and Trillian, and more recently Beeper. These third-party clients have often been unable to keep up due to proprietary protocol restrictions and getting locked out of it. For instance, in 2015, WhatsApp started banning users who were using unofficial clients. Major IM providers usually cite the need for formal agreements, and security concerns as reasons for making changes.\nAttempted open standards.\nThere have been several attempts in the past to create a unified standard for instant messaging, including:\nHistory and agreements.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nCritics say AOL's slowness in embracing interoperability has caused setbacks to other companies trying to grow their businesses. AOL has said it supports the development of an interoperable system for all IM networks but has cited privacy and security concerns as the reasons it's taking its time. Competitors have labeled that argument a \"smoke screen.\"\nEvan Hansen, CNET, January 2001\nIn the early 2000s, when instant messaging was quickly growing, most attempts at producing a unified standard for the-then major IM providers (AOL, Yahoo!, Microsoft) had failed. There was a \"bitter row\" between AOL and its rivals regarding the opening up of their networks. In 2000, U.S. regulatory Federal Communications Commission (FCC) proposed, and supported by Microsoft chairman Bill Gates, that AOL providing interoperability of its AIM and ICQ instant messengers with Microsoft's MSN Messenger was a condition for the forthcoming AOL-Time Warner merger.\nHowever, in 2004, Microsoft, Yahoo! and AOL agreed to a deal in which Microsoft's enterprise IM server Live Communications Server 2005 would have the possibility to talk to their rival counterparts and vice versa. On October 13, 2005, Microsoft and Yahoo! announced that their IM networks would soon be interoperable, using SIP/SIMPLE. This was finally rolled out to Windows Live Messenger and Yahoo! Messenger users in July 2006. Additionally, in December 2005 by the AOL and Google strategic partnership deal, it was announced that AIM and ICQ users would be able to communicate with Google Talk users. However this feature took until December 2007 to roll out. XMPP provided the best example of open protocol interoperability, having had gateways that connected to Google Talk, Lotus Sametime and others.\nLater, RCS was developed by telecommunication companies as an instant messaging protocol to replace SMS under a unified standard. In 2022, the European Union passed the Digital Markets Act, which largely came into effect in early 2023. Among other things, the legislation mandates certain interoperability between the largest IM platforms in use in Europe. As a result, in March 2024, Meta Platforms opened up its WhatsApp and Messenger networks to be interoperable.\nTechnical.\nThere are two ways to combine the many disparate protocols:\nSome approaches allow organizations to deploy their own, private instant messaging network by enabling them to restrict access to the server (often with the IM network entirely behind their firewall) and administer user permissions. Other corporate messaging systems allow registered users to also connect from outside the corporation LAN, by using an encrypted, firewall-friendly, HTTPS-based protocol. Usually, a dedicated corporate IM server has several advantages, such as pre-populated contact lists, integrated authentication, and better security and privacy.\nEffects of IM on communication.\nWorkplace communication.\nInstant messaging has changed how people communicate in the workplace. Enterprise messaging applications like Slack, Symphony, Teamnote and Yammer allow companies to enforce policies on how employees message at work and ensure secure storage of sensitive data. They allow employees to separate work information from their personal emails and texts.\nMessaging applications may make workplace communication efficient, but they can also have consequences on productivity. A study at Slack showed on average, people spend 10 hours a day on Slack, which is about 67% more time than they spend using email.\nInstant messaging is implemented in many video-conferencing tools. A study of chat use during work-related videoconferencing found that chat during meetings allows participants to communicate without interrupting the meeting, plan action around common resources, and enables greater inclusion. The study also found that chat can cause distractions and information asymmetries between participants.\nLanguage.\nUsers sometimes make use of internet slang or text speak to abbreviate common words or expressions to quicken conversations or reduce keystrokes. The language has become widespread, with well-known expressions such as 'lol' translated over to face-to-face language.\nEmotions are often expressed in shorthand, such as the abbreviation LOL, BRB and TTYL; respectively laugh(ing) out loud, be right back, and talk to you later. Some, however, attempt to be more accurate with emotional expression over IM. Real time reactions such as (\"chortle\") (\"snort\") (\"guffaw\") or (\"eye-roll\") have been popular at one point. Also there are certain standards that are being introduced into mainstream conversations including, '#' indicates the use of sarcasm in a statement and '*' which indicates a spelling mistake and/or grammatical error in the prior message, followed by a correction.\nBusiness application.\nInstant messaging products can usually be categorised into two types: Enterprise Instant Messaging (EIM) and Consumer Instant Messaging (CIM). Enterprise solutions use an internal IM server, however this is not always feasible, particularly for smaller businesses with limited budgets. The second option, using a CIM provides the advantage of being inexpensive to implement and has little need for investing in new hardware or server software. IM is increasingly becoming a feature of enterprise software rather than a stand-alone application.\nInstant messaging has proven to be similar to personal computers, email, and the World Wide Web, in that its adoption for use as a business communications medium was driven primarily by individual employees using consumer software at work, rather than by formal mandate or provisioning by corporate information technology departments. Tens of millions of the consumer IM accounts in use are being used for business purposes by employees of companies and other organizations. The adoption of IM across corporate networks outside of the control of IT organizations creates risks and liabilities for companies who do not effectively manage and support IM use. IM was initially shunned by the corporate world partly due to security concerns, but by 2003 many had started embracing these new services.\nSoftware.\nIn response to the demand for business-grade IM and the need to ensure security and legal compliance, a new type of instant messaging, called \"Enterprise Instant Messaging\" (\"EIM\") was created when Lotus Software launched IBM Lotus Sametime in 1998. Microsoft followed suit shortly thereafter with Microsoft Exchange Instant Messaging, later created a new platform called Microsoft Office Live Communications Server, and released Office Communications Server 2007 in October 2007. Oracle Corporation also jumped into the market with its Oracle Beehive unified collaboration software.\nBoth IBM Lotus and Microsoft have introduced federation between their EIM systems and some of the public IM networks so that employees may use one interface to both their internal EIM system and their contacts on AOL, MSN, and Yahoo. As of 2010, leading EIM platforms include IBM Lotus Sametime, Microsoft Office Communications Server, Jabber XCP and Cisco Unified Presence. Industry-focused EIM platforms such as Reuters Messaging and Bloomberg Messaging also provide IM abilities to financial services companies.\nSecurity and archiving.\nCrackers (malicious or black hat hackers) have consistently used IM networks as vectors for delivering phishing attempts, drive-by URLs, and virus-laden file attachments, with over 1100 discrete attacks listed by the IM Security Center in 2004\u20132007. Hackers use two methods of delivering malicious code through IM: delivery of viruses, trojan horses, or spyware within an infected file, and the use of \"socially engineered\" text with a web address that entices the recipient to click on a URL connecting him or her to a website that then downloads malicious code.\nIM connections sometimes occur in plain text, making them vulnerable to eavesdropping. Also, IM client software often requires the user to expose open UDP ports to the world, raising the threat posed by potential security vulnerabilities.\nIn the early 2000s, a new class of IT security providers emerged to provide remedies for the risks and liabilities faced by corporations who chose to use IM for business communications. The IM security providers created new products to be installed in corporate networks for the purpose of archiving, content-scanning, and security-scanning IM traffic moving in and out of the corporation. Similar to the e-mail filtering vendors, the IM security providers focus on the risks and liabilities described above.\nWith the rapid adoption of IM in the workplace, demand for IM security products began to grow in the mid-2000s. By 2007, the preferred platform for the purchase of security software had become the \"computer appliance\", according to IDC, who estimated that by 2008, 80% of network security products would be delivered via an appliance.\nBy 2014, however, instant messengers' safety level was still extremely poor. According to a scorecard by the Electronic Frontier Foundation, only 7 out of 39 instant messengers received a perfect score. In contrast, the most popular instant messengers at the time only attained a score of 2 out of 7. A number of studies have shown that IM services are quite vulnerable for providing user privacy.\nIn 2023, cybersecurity researchers discovered that numerous malicious \"mods\" exist of the Telegram instant messenger, which is freely available for download from Google Play.\nMessage history.\nInstant messages are often logged in a local message history, similar to emails' persistent nature. IM networks may store messages with either local-based device storage (e.g. WhatsApp, Viber, Line, WeChat, Signal etc. software) or cloud-based server storage provided by the service (e.g. Telegram, Skype, Facebook Messenger, Google Meet/Chat, Discord, Slack etc.). Although cloud-based storage is advertised to offer encrypted messages, it poses an increased risk that the IM provider may have access to the decryption keys and view the user's saved messages.\nThis requires users to trust IM servers and providers because messages can generally be accessed by the company. Companies may be compelled to reveal their user's communication and suspend user accounts for any reason.\nTracking and spying.\nNews reports from 2013 revealed that the NSA is not only collecting emails and IM messages but also tracking relationships between senders and receivers of those chats and emails in a process known as metadata collection. Metadata refers to the data concerned about the chat or email as opposed to contents of messages. It may be used to collect valuable information.\nIn January 2014, Matthew Campbell and Michael Hurley filed a class-action lawsuit against Facebook for breaching the Electronic Communications Privacy Act. They alleged that the information in their supposedly private messages was being read and used to generate profit, specifically \"for purposes including but not limited to data mining and user profiling\".\nIn corporate use of IM, organizational offerings have become very sophisticated in their security and logging measures. An employee or organization member must be granted login credentials and permission to use the messaging system. Creating a specific account for each user allows the organization to identify, track and record all use of their messenger system on their servers.\nEncryption.\nEncryption is the primary method that instant messaging apps use to protect user's data privacy and security. For corporate use, encryption and conversation archiving are usually regarded as important features due to security concerns. There are also a bunch of open source encrypting messengers.\nIM does hold potential advantages over SMS. SMS messages are not encrypted, making them insecure, as the content of each SMS message is visible to mobile carriers and governments and can be intercepted by a third party, may leak metadata (such as phone numbers), or be spoofed and the sender of the message can be edited to impersonate another person.\nCurrent instant messaging networks that use end-to-end encryption include Signal, WhatsApp, Wire and iMessage. Signal and iMessage have started using Post-quantum cryptography in September 2023 and April 2024 respectively. Applications that have been criticized for lacking or poor encryption methods include Telegram and Confide, as both are prone to error or not having encryption enabled by default.\nCompliance risks.\nIn addition to the malicious code threat, using instant messaging at work creates a risk of non-compliance with laws and regulations governing electronic communications in businesses. In the United States alone, there are over 10,000 laws and regulations related to electronic messaging and records retention. The better-known of these include the Sarbanes\u2013Oxley Act, HIPAA, and SEC 17a-3.\nClarification from the Financial Industry Regulatory Authority (FINRA) was issued to member firms in the financial services industry in December 2007, noting that \"electronic communications\", \"email\", and \"electronic correspondence\" may be used interchangeably and can include such forms of electronic messaging as \"instant messaging\" and text messaging. Changes to Federal Rules of Civil Procedure, effective December 1, 2006, created a new category for electronic records which may be requested during discovery in legal proceedings.\nMost nations also regulate electronic messaging and records retention similarly to the United States. The most common regulations related to IM at work involve producing archived business communications to satisfy government or judicial requests under law. Many instant messaging communications fall into the category of business communications that must be archived and retrievable.\nCurrent user base.\nAs of May 2025, the most used instant messaging apps and services worldwide include: Signal with 100 million, Line with 197 million, Viber with 260 million, QQ with 562 million, Snapchat with 900 million, Telegram with 1 billion, Facebook Messenger with 1.3 billion, WeChat with 1.39 billion, and WhatsApp with 3 billion users. \nThere are 25 countries in the world where WhatsApp messenger is not the market leader in IM, such as the United States, Canada, Australia, New Zealand, Denmark, Norway, Sweden, Hungary, Lithuania, Poland, Slovakia, Philippines, and China.\nIM apps have varying levels of adoption in different countries. As of April 2022:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55954", "revid": "34413366", "url": "https://en.wikipedia.org/wiki?curid=55954", "title": "Command-line", "text": ""}
{"id": "55955", "revid": "51044738", "url": "https://en.wikipedia.org/wiki?curid=55955", "title": "Version control", "text": "Managing versions of source code or data\n \nVersion control (also known as revision control, source control, and source code management) is the software engineering practice of controlling, organizing, and tracking different versions in history of computer files; primarily source code text files, but generally any type of file.\nVersion control is a component of software configuration management.\nA \"version control system\" is a software tool that automates version control. Alternatively, version control is embedded as a feature of some systems such as word processors, spreadsheets, collaborative web docs, and content management systems, such as . \nVersion control includes options to view old versions and to revert a file to a previous version.\nOverview.\nAs teams develop software, it is common to deploy multiple versions of the same software, and for different developers to work on one or more different versions simultaneously. Bugs or features of the software are often only present in certain versions (because of the fixing of some problems and the introduction of others as the program develops). Therefore, for the purposes of locating and fixing bugs, it is vitally important to be able to retrieve and run different versions of the software to determine in which version(s) the problem occurs. It may also be necessary to develop two versions of the software concurrently: for instance, where one version has bugs fixed, but no new features (branch), while the other version is where new features are worked on (trunk).\nAt the simplest level, developers could simply retain multiple copies of the different versions of the program, and label them appropriately. This simple approach has been used in many large software projects. While this method can work, it is inefficient as many near-identical copies of the program have to be maintained. This requires a lot of self-discipline on the part of developers and often leads to mistakes. Since the code base is the same, it also requires granting read-write-execute permission to a set of developers, and this adds the pressure of someone managing permissions so that the code base is not compromised, which adds more complexity. Consequently, systems to automate some or all of the revision control process have been developed. This abstracts most operational steps (hides them from ordinary users).\nMoreover, in software development, legal and business practice, and other environments, it has become increasingly common for a single document or snippet of code to be edited by a team, the members of which may be geographically dispersed and may pursue different and even contrary interests. Sophisticated revision control that tracks and accounts for ownership of changes to documents and code may be extremely helpful or even indispensable in such situations.\nRevision control may also track changes to configuration files, such as those typically stored in codice_1 or codice_2 on Unix systems. This gives system administrators another way to easily track changes made and a way to roll back to earlier versions should the need arise.\nMany version control systems identify the version of a file as a number or letter, called the \"version number\", \"version\", \"revision number\", \"revision\", or \"revision level\". For example, the first version of a file might be version \"1\". When the file is changed the next version is \"2\". Each version is associated with a timestamp and the person making the change. Revisions can be compared, restored, and, with some types of files, merged.\nHistory.\nIBM's OS/360 IEBUPDTE software update tool dates back to 1962, arguably a precursor to version control system tools. Two source management and version control packages that were heavily used by IBM 360/370 installations were The Librarian and Panvalet.\nA full system designed for source code control was started in 1972: the Source Code Control System (SCCS), again for the OS/360. SCCS's user manual, especially the introduction, which was published on December 4, 1975, implied that it was the first deliberate revision control system. The Revision Control System (RCS) followed in 1982 and, later, Concurrent Versions System (CVS) added network and concurrent development features to RCS. After CVS, a dominant successor was Subversion, followed by the rise of distributed version control tools such as Git.\nStructure.\nRevision control manages changes to a set of data over time. These changes can be structured in various ways.\nOften the data is thought of as a collection of many individual items, such as files or documents, and changes to individual files are tracked. This is in line with the notion of keeping files separate but causes problems when identity changes, which happens when files are renamed, split or merged. Accordingly, some systems such as Git, consider changes to the data as a whole instead, which is less intuitive for simple changes but simplifies more complex changes.\nWhen data that is under revision control is modified, after being retrieved by \"checking out,\" this is not in general immediately reflected in the revision control system (in the \"repository\"), but must instead be \"checked in\" or \"committed.\" A copy outside of revision control is known as a \"working copy\". As a simple example, when editing a computer file, the data stored in memory by the editing program is the working copy, which is committed by saving the file. Concretely, one may print out a document, edit it by hand, and only later manually input the changes into a computer and save it. For source code control, the working copy is instead a copy of all files in a particular revision, generally stored locally on the developer's computer; in this case saving the file only changes the working copy, and checking into the repository is a separate step.\nIf multiple people are working on a single data set or document, they are implicitly creating branches of the data (in their working copies), and thus issues of merging arise, as discussed below. For simple collaborative document editing, this can be prevented by using file locking or simply avoiding working on the same document that someone else is working on.\nRevision control systems are often centralized, with a single authoritative data store, the \"repository,\" and check-outs and check-ins done with reference to this central repository. Alternatively, in distributed revision control, no single repository is authoritative, and data can be checked out and checked into any repository. When checking into a different repository, this is interpreted as a merge or patch.\nGraph structure.\nIn terms of graph theory, revisions are generally thought of as a line of development (the \"trunk\") with branches off of this, forming a directed tree, visualized as one or more parallel lines of development (the \"mainlines\" of the branches) branching off a trunk. In reality the structure is more complicated, forming a directed acyclic graph, but for many purposes \"tree with merges\" is an adequate approximation.\nRevisions occur in sequence over time, and thus can be arranged in order, either by revision number or timestamp. Revisions are based on past revisions, though it is possible to largely or completely replace an earlier revision, such as \"delete all existing text, insert new text\". In the simplest case, with no branching or undoing, each revision is based on its immediate predecessor alone, and they form a simple line, with a single latest version, the \"HEAD\" revision or \"tip\". In graph theory terms, drawing each revision as a point and each \"derived revision\" relationship as an arrow (conventionally pointing from older to newer, in the same direction as time), this is a linear graph. If there is branching, so multiple future revisions are based on a past revision, or undoing, so a revision can depend on a revision older than its immediate predecessor, then the resulting graph is instead a directed tree (each node can have more than one child), and has multiple tips, corresponding to the revisions without children (\"latest revision on each branch\"). In principle the resulting tree need not have a preferred tip (\"main\" latest revision) \u2013 just various different revisions \u2013 but in practice one tip is generally identified as HEAD. When a new revision is based on HEAD, it is either identified as the new HEAD, or considered a new branch. The list of revisions from the start to HEAD (in graph theory terms, the unique path in the tree, which forms a linear graph as before) is the \"trunk\" or \"mainline.\" Conversely, when a revision can be based on more than one previous revision (when a node can have more than one \"parent\"), the resulting process is called a \"merge,\" and is one of the most complex aspects of revision control. This most often occurs when changes occur in multiple branches (most often two, but more are possible), which are then merged into a single branch incorporating both changes. If these changes overlap, it may be difficult or impossible to merge, and require manual intervention or rewriting.\nIn the presence of merges, the resulting graph is no longer a tree, as nodes can have multiple parents, but is instead a rooted directed acyclic graph (DAG). The graph is acyclic since parents are always backwards in time, and rooted because there is an oldest version. Assuming there is a trunk, merges from branches can be considered as \"external\" to the tree \u2013 the changes in the branch are packaged up as a \"patch,\" which is applied to HEAD (of the trunk), creating a new revision without any explicit reference to the branch, and preserving the tree structure. Thus, while the actual relations between versions form a DAG, this can be considered a tree plus merges, and the trunk itself is a line.\nIn distributed revision control, in the presence of multiple repositories these may be based on a single original version (a root of the tree), but there need not be an original root - instead there can be a separate root (oldest revision) for each repository. This can happen, for example, if two people start working on a project separately. Similarly, in the presence of multiple data sets (multiple projects) that exchange data or merge, there is no single root, though for simplicity one may think of one project as primary and the other as secondary, merged into the first with or without its own revision history.\nSpecialized strategies.\nEngineering revision control developed from formalized processes based on tracking revisions of early blueprints or bluelines. This system of control implicitly allowed returning to an earlier state of the design, for cases in which an engineering dead-end was reached in the development of the design. A revision table was used to keep track of the changes made. Additionally, the modified areas of the drawing were highlighted using revision clouds.\nIn business and law.\nVersion control is widespread in business and law. Indeed, \"contract redline\" and \"legal blackline\" are some of the earliest forms of revision control, and are still employed in business and law with varying degrees of sophistication. The most sophisticated techniques are beginning to be used for the electronic tracking of changes to CAD files (see product data management), supplanting the \"manual\" electronic implementation of traditional revision control.\nIn game development.\nGame development often involves large binary files and teams working together across different disciplines. As a result, game studios use version control systems with good support for large binary files, file locking, and fast synchronization. Common tools include Perforce and several newer cloud-based systems.\nSource-management models.\nTraditional revision control systems use a centralized model where all the revision control functions take place on a shared server. If two developers try to change the same file at the same time, without some method of managing access the developers may end up overwriting each other's work. Centralized revision control systems solve this problem in one of two different \"source management models\": file locking and version merging.\nAtomic operations.\nAn operation is \"atomic\" if the system is left in a consistent state even if the operation is interrupted. The \"commit\" operation is usually the most critical in this sense. Commits tell the revision control system to make a group of changes final, and available to all users. Not all revision control systems have atomic commits; Concurrent Versions System lacks this feature.\nFile locking.\nThe simplest method of preventing \"concurrent access\" problems involves locking files so that only one developer at a time has write access to the central \"repository\" copies of those files. Once one developer \"checks out\" a file, others can read that file, but no one else may change that file until that developer \"checks in\" the updated version (or cancels the checkout).\nFile locking has both merits and drawbacks. It can provide some protection against difficult merge conflicts when a user is making radical changes to many sections of a large file (or group of files). If the files are left exclusively locked for too long, other developers may be tempted to bypass the revision control software and change the files locally, forcing a difficult manual merge when the other changes are finally checked in. In a large organization, files can be left \"checked out\" and locked and forgotten about as developers move between projects - these tools may or may not make it easy to see who has a file checked out.\nVersion merging.\nMost version control systems allow multiple developers to edit the same file at the same time. The first developer to \"check in\" changes to the central repository always succeeds. The system may provide facilities to merge further changes into the central repository, and preserve the changes from the first developer when other developers check in.\nMerging two files can be a very delicate operation, and usually possible only if the data structure is simple, as in text files. The result of a merge of two image files might not result in an image file at all. The second developer checking in files will need to take care with the merge, to make sure that the changes are compatible and that the merge operation does not introduce its own logic errors within the files. These problems limit the availability of automatic or semi-automatic merge operations mainly to simple text-based documents, unless a specific merge plugin is available for the file types.\nThe concept of a \"reserved edit\" can provide an optional means to explicitly lock a file for exclusive write access, even when a merging capability exists.\nBaselines, labels and tags.\nMost revision control tools will use only one of these similar terms (baseline, label, tag) to refer to the action of identifying a snapshot (\"label the project\") or the record of the snapshot (\"try it with baseline \"X\"\"). Typically only one of the terms \"baseline\", \"label\", or \"tag\" is used in documentation or discussion; they can be considered synonyms.\nIn most projects, some snapshots are more significant than others, such as those used to indicate published releases, branches, or milestones.\nWhen both the term \"baseline\" and either of \"label\" or \"tag\" are used together in the same context, \"label\" and \"tag\" usually refer to the mechanism within the tool of identifying or making the record of the snapshot, and \"baseline\" indicates the increased significance of any given label or tag.\nMost formal discussion of configuration management uses the term \"baseline\".\nDistributed revision control.\nDistributed revision control systems (DRCS) take a peer-to-peer approach, as opposed to the client\u2013server approach of centralized systems. Rather than a single, central repository on which clients synchronize, each peer's working copy of the codebase is a bona-fide repository.\nDistributed revision control conducts synchronization by exchanging patches (change-sets) from peer to peer. This results in some important differences from a centralized system:\nRather, communication is only necessary when pushing or pulling changes to or from other peers.\nBest practices.\nFollowing best practices is necessary to obtain the full benefits of version control. Best practice may vary by version control tool and the field to which version control is applied. The generally accepted best practices in software development include: making small/incremental changes; making commits which involve only one task or fix -- a corollary to this is to commit only code which works and does not knowingly break existing functionality; using branching to complete functionality before release; writing clear and descriptive commit messages, making what, why, and how clear in either the commit description or the code; and using a consistent branching strategy. Other best software development practices such as code review and automated regression testing may assist in the following of version control best practices.\nCosts and benefits.\nCosts and benefits will vary dependent upon the version control tool chosen and the field in which it is applied. This section speaks to the field of software development, where version control is widely applied.\nCosts.\nIn addition to the costs of licensing the version control software, using version control requires time and effort. The concepts underlying version control must be understood and the technical particulars required to operate the version control software chosen must be learned. Version control best practices must be learned and integrated into the organization's existing software development practices. Management effort may be required to maintain the discipline needed to follow best practices in order to obtain useful benefit.\nBenefits.\nAllows for reverting changes.\nA core benefit is the ability to keep history and revert changes, allowing the developer to easily undo changes. This gives the developer more opportunity to experiment, eliminating the fear of breaking existing code.\nBranching simplifies deployment, maintenance and development.\nBranching assists with deployment and release management. Branching and merging, the production, packaging, and labeling of source code patches and the easy application of patches to code bases, simplifies the maintenance and concurrent development of the multiple code bases associated with the various stages of the deployment process; development, testing, staging, production, etc.\nDamage mitigation, accountability and process and design improvement.\nThere can be damage mitigation, accountability, process and design improvement, and other benefits associated with the record keeping provided by version control, the tracking of who did what, when, why, and how.\nWhen bugs arise, knowing what was done when helps with damage mitigation and recovery by assisting in the identification of what problems exist, how long they have existed, and determining problem scope and solutions. Previous versions can be installed and tested to verify conclusions reached by examination of code and commit messages.\nSimplifies debugging.\nVersion control can greatly simplify debugging. The application of a test case to multiple versions can quickly identify the change which introduced a bug. The developer need not be familiar with the entire code base and can focus instead on the code that introduced the problem.\nImproves collaboration and communication.\nVersion control enhances collaboration in multiple ways. Since version control can identify conflicting changes, i.e. incompatible changes made to the same lines of code, there is less need for coordination among developers. \nThe packaging of commits, branches, and all the associated commit messages and version labels, improves communication between developers, both in the moment and over time. Better communication, whether instant or deferred, can improve the code review process, the testing process, and other critical aspects of the software development process.\nIntegration.\nSome of the more advanced revision-control tools offer many other facilities, allowing deeper integration with other tools and software-engineering processes. \nIntegrated development environment.\nPlugins are often available for IDEs such as Oracle JDeveloper, IntelliJ IDEA, Eclipse, Visual Studio, Delphi, NetBeans IDE, Xcode, and GNU Emacs (via vc.el). Advanced research prototypes generate appropriate commit messages.\nCommon terminology.\nTerminology can vary from system to system, but some terms in common usage include:\nBaseline.\nAn approved revision of a document or source file to which subsequent changes can be made. See baselines, labels and tags.\nBlame.\nA search for the author and revision that last modified a particular line.\nBranch.\nA set of files under version control may be \"branched\" or \"forked\" at a point in time so that, from that time forward, two copies of those files may develop at different speeds or in different ways independently of each other.\nChange.\nA \"change\" (or \"diff\", or \"delta\") represents a specific modification to a document under version control. The granularity of the modification considered a change varies between version control systems.\nChange list.\nOn many version control systems with atomic multi-change commits, a \"change list\" (or \"CL\"), \"change set\", \"update\", or \"patch\" identifies the set of \"changes\" made in a single commit. This can also represent a sequential view of the source code, allowing the examination of source as of any particular changelist ID.\nCheckout.\nTo \"check out\" (or \"co\") is to create a local working copy from the repository. A user may specify a specific revision or obtain the latest. The term 'checkout' can also be used as a noun to describe the working copy. When a file has been checked out from a shared file server, it cannot be edited by other users. Think of it like a hotel, when you check out, you no longer have access to its amenities.\nClone.\n\"Cloning\" means creating a repository containing the revisions from another repository. This is equivalent to \"push\"ing or \"pull\"ing into an empty (newly initialized) repository. As a noun, two repositories can be said to be \"clone\"s if they are kept synchronized, and contain the same revisions.\nCommit (verb).\nTo \"commit\" (\"check in\", \"ci\" or, more rarely, \"install\", \"submit\" or \"record\") is to write or merge the changes made in the working copy back to the repository. A commit contains metadata, typically the author information and a commit message that describes the change.\nCommit message.\nA short note, written by the developer, stored with the commit, which describes the commit. Ideally, it records why the modification was made, a description of the modification's effect or purpose, and non-obvious aspects of how the change works.\nConflict.\nA conflict occurs when different parties make changes to the same document, and the system is unable to reconcile the changes. A user must \"resolve\" the conflict by combining the changes, or by selecting one change in favour of the other.\nDelta compression.\nMost revision control software uses delta compression, which retains only the differences between successive versions of files. This allows for more efficient storage of many different versions of files.\nDynamic stream.\nA stream in which some or all file versions are mirrors of the parent stream's versions.\nExport.\n\"Exporting\" is the act of obtaining the files from the repository. It is similar to \"checking out\" except that it creates a clean directory tree without the version-control metadata used in a working copy. This is often used prior to publishing the contents, for example.\nFetch.\nSee \"pull\".\nForward integration.\nThe process of merging changes made in the main \"trunk\" into a development (feature or team) branch.\nHead.\nAlso sometimes called \"tip\", this refers to the most recent commit, either to the trunk or to a branch. The trunk and each branch have their own head, though HEAD is sometimes loosely used to refer to the trunk.\nImport.\n\"Importing\" is the act of copying a local directory tree (that is not currently a working copy) into the repository for the first time.\nInitialize.\nTo create a new, empty repository.\nInterleaved deltas.\nSome revision control software uses Interleaved deltas, a method that allows storing the history of text based files in a more efficient way than by using Delta compression.\nLabel.\nSee \"tag\".\nLocking.\nWhen a developer \"locks\" a file, no one else can update that file until it is unlocked. Locking can be supported by the version control system, or via informal communications between developers (aka \"social locking\").\nMainline.\nSimilar to \"trunk\", but there can be a mainline for each branch.\nMerge.\nA \"merge\" or \"integration\" is an operation in which two sets of changes are applied to a file or set of files. Some sample scenarios are as follows:\nPromote.\nThe act of copying file content from a less controlled location into a more controlled location. For example, from a user's workspace into a repository, or from a stream to its parent.\nPull, push.\nCopy revisions from one repository into another. \"Pull\" is initiated by the receiving repository, while \"push\" is initiated by the source. \"Fetch\" is sometimes used as a synonym for \"pull\", or to mean a \"pull\" followed by an \"update\".\nResolve.\nThe act of user intervention to address a conflict between different changes to the same document.\nReverse integration.\nThe process of merging different team branches into the main trunk of the versioning system.\nRevision and version.\nA version is any change in form. In SVK, a Revision is the state at a point in time of the entire tree in the repository.\nShare.\nThe act of making one file or folder available in multiple branches at the same time. When a shared file is changed in one branch, it is changed in other branches.\nStream.\nA container for branched files that has a known relationship to other such containers. Streams form a hierarchy; each stream can inherit various properties (like versions, namespace, workflow rules, subscribers, etc.) from its parent stream.\nTag.\n A \"tag\" or \"label\" refers to an important snapshot in time, consistent across many files. These files at that point may all be tagged with a user-friendly, meaningful name or revision number. See baselines, labels and tags.\nTrunk.\nThe trunk is the unique line of development that is not a branch (sometimes also called Baseline, Mainline or Master)\nUpdate.\nAn \"update\" (or \"sync\", but \"sync\" can also mean a combined \"push\" and \"pull\") merges changes made in the repository (by other people, for example) into the local \"working copy\". \"Update\" is also the term used by some CM tools (CM+, PLS, SMS) for the change package concept (see \"changelist\"). Synonymous with \"checkout\" in revision control systems that require each repository to have exactly one working copy (common in distributed systems)\nUnlocking.\nReleasing a lock.\nWorking copy.\nThe \"working copy\" is the local copy of files from a repository, at a specific time or revision. All work done to the files in a repository is initially done on a working copy, hence the name. Conceptually, it is a \"sandbox\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55959", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=55959", "title": "Politician", "text": "Person active in politics\nA politician is a person who participates in policy-making processes, usually holding a position in a political party or an elective position in government. Politicians make decisions, and influence the formulation of public policy. The roles or duties that politicians perform vary depending on the level of government, whether local, state, or national. The ideological orientation that politicians adopt often stems from their previous experience, education, beliefs, the political parties they belong to. They try to shape public opinion accordingly.\nPoliticians sometimes face many challenges and mistakes that may affect their credibility and ability to persuade. These mistakes include political corruption resulting from their use and exploitation of power to achieve self interest. Ideally they prioritize the public interest over their own profit. Challenges include how to keep up with the development of social media and confronting opposition media, in addition to discrimination for or against them on the basis of gender or race.\nDefinitions.\nStandard dictionary definitions include a range of political activists under the definition of \"politician.\" \"Merriam Webster Dictionary\" (2025) states: Politician: 1: a person experienced in the art or science of government especially: one actively engaged in conducting the business of a government. 2a : a person engaged in party politics as a profession. \n\"Oxford English Dictionary\" (1989 edition) states: Politician 2b: One keenly interested in politics; one who engages in party politics, or in political strife, or who makes politics his profession or business; also (esp. in U.S.), in a sinister sense, one who lives by politics as a trade. \nIdentity.\nPoliticians are people who participate in policy-making, in a multifaceted variety of positions of responsibility both domestically and internationally.\nIn ancient Greece, Pericles of Athens played an important role in politics, both in public discussions and in decision-making as depicted in Philip Foltz's 19th-century painting.\nOver time the figure of the politician has evolved to include many forms and functions. In the United States, George Washington played a pivotal role as a politician because he served as the first President of the United States, shaping the official role and setting many precedents. Today, political offices take many forms, held by people called (for example) premiers, ministers, mayors, governors, senators, or presidents, each with its own duties.\nThough popular discourse may equate politicians with \"leaders\", politicians may primarily conduct themselves in the political process without engaging in leading \u2014 the class of politicians may include: representative delegates,\nexecutive functionaries or administrators,\ndiligent bureaucratic cadres, and party hacks\nsubmissively propping up a voting majority.\nWhile government leaders are generally considered politicians,\nnot all politicians are subject to voters: autocratic and dictatorial regimes remain extant.\nThe identity of politicians is influenced by their social and work environments, by their ideology, and by any parties with which they may associate; furthermore, the development of means of communication and social media have increased public participation in policy-making, leading to a reformation of politicians' identities and increasing the complexity of political work.\nMedia and rhetoric.\nPoliticians are influential people who use rhetoric to impact people as in speeches or campaign advertisements. They are especially known for using common themes, and media platforms that allow them to develop their political positions, developing communication between them and the people.\nPoliticians of necessity become expert users of the media. Politicians in the 19th century made heavy use of newspapers, magazines, and pamphlets, as well as posters to disseminate their messages to appeal to voters' emotions and beliefs in their campaigns. In the 20th century, the scope of media expanded out into radio and television, and a major change occurred as speech was now presented visually as well as verbally as evidenced by the Kennedy-Nixon debates, marking a new era where visual media became crucial to campaigns. The twenty-first century has provided wide and diverse media platforms represented by Facebook, and Twitter, which has now become X, Instagram, YouTube, and others. This development has made their rhetorical messages faster, shorter more efficient, and characterized by the speed of spread and interaction.\nPoliticians, who rarely meet voters in person, seek to use the media as a means of communicating with people, winning votes, and obtaining political roles. Some research confirms that the media increases the popularity of a politician, and indicates that negative news has a stronger effect on popularity than positive news.\nSome research has suggested that politicians tend to use social media more than traditional media because their perception of the traditional media's influence as a public informant greatly affects their satisfaction with democratic processes. So they prefer to use social media and communicate directly with people in order to have greater control over their message and easier communication.\nThis continuous evolution in media has made politicians adapt their discourse to these diverse and evolving platforms for greater communication and effectiveness.\nSalary.\nHigher salaries of politicians can improve governance and decrease political corruption. The list of heads of state and government salaries shows large differences in the salaries of politicians. \nCareers and biographies.\nMattozzi and Merlo argue that politicians typically follow two main career paths in modern democracies. The first is career politicians who remain in government until retirement. The second is political careerists, who have gained a reputation for their experience at various levels of government such as international, federal, state, and local governments, they often leave politics and start a new business venture using their political connections.\nThe personal histories of politicians have been frequently studied, as it is presumed that their experiences and characteristics shape their beliefs and behaviors. There are four pathways by which a politician's biography could influence their leadership style and abilities. First, a politician's biography may shape their core beliefs, which are essential to shaping their worldview. The second pathway is those personal experiences that influence a politician's skills and competence, and which determine where politicians focus their resources and attention as leaders. The third pathway refers to biographical characteristics that influence a politician's resource allocation and responses based on characteristics such as race or gender. The fourth pathway is how a politician's biography affects his public perception, which affects politicians' leadership style and their strategy for gaining people's respect.\nCharacteristics.\nNumerous scholars have studied the characteristics of politicians and in economic class to explain characteristics impact on politicians' effectiveness and electoral success, comparing politicians involves different dimensions such as level of government (the local and national levels), political ideology (liberal or the more conservative), economic class, and comparing the more successful and less successful in terms of elections.\nDemographic factors such as age, gender, education, income, and race/ethnicity, play a role in shaping shape voter behavior and political preferences\nAlso, educational background in politics also plays an important role in shaping the political awareness of politicians and plays a major role in increasing people's confidence in them.\nPoliticians were found to have on average a political bias compared to median voters in many countries.\nIn some countries politicians are banned from having multiple citizenships to avoid dual loyalty.\nChallenges.\nIn this century of advanced communications, politicians face challenges and difficulties while communicating with people through various social media platforms. The implicit importance of social media for politics stems from the virtual space these platforms have created for expressing ideas and spreading mutual messages without restrictions. Misinformation, rumors, and discrimination complicate their political behavior and communication with people.\nFace-to-face contact with constituents is important to elected politicians. Face-to-face meetings force politicians to be directly aware of the issues that matter most to their constituents and to have a ready response for them. They pay attention to tome of voice, looking for unspoken priorities, hopes and anxieties. Politicians with good memories will ask whether such and such a problem is resolved satisfactorily, or how the children are doing. Some politicians have the knack to remember thousands of names and faces. Presidents George W. Bush and Bill Clinton were renowned for their memories.\nPolitical polarization created by the media plays a role in influencing politicians' behavior and communications, which reinforces negative campaigns. They also play a role in legislative gridlock and negatively impact public perception, which negatively impacts politicians' interests.\nAdditionally, research highlighted that politicians, especially populist politicians, may create a challenge for themselves by increasingly accusing the media of spreading misinformation or \"fake news.\" Such accusations can undermine the credibility of media platforms, even though trust in the accused politicians remains largely unaffected. They will therefore have a negative impact on the credibility of media platforms, and this distrust may extend to the media institutions as a whole that politicians use to communicate with people.\nRegarding the challenges of gender dynamics, particularly the role of women in politics, some recent research focuses on the life path of women in the political field and the challenges surrounding them. For example, there are studies on the \"supermader\" model in politics in Latin America, which illustrate the difficulties women face and how to balance their home and work and the distinction between women and men that negatively affects their acceptance in political work.\nPolitical corruption.\nHistorically, in patronage-based systems, especially in the 19th century, winning politicians replaced civil servants and government employees who were not protected by the rules of government service with their supporters, a so-called \"spoils system.\" In response to the corruption this system fostered, government job reforms were introduced. These reforms required elected politicians to work with existing civil servants and officials to pursue long-term public interest goals, rather than simply rewarding their supporters. This shift aimed to reduce corruption and prioritize the integrity of government positions.\nThe Pendleton Civil Service Reform Act of 1883 passed by the U.S. Congress to combat corruption, favoritism in hiring, and the spoils system. It advocated hiring based on merit and protected civil servants from political influence.\nIn the modern century, many laws have been put in place to protect employees and reduce political corruption and favoritism in employment, for example, the Mexican government introduced the Federal Law on Administrative Responsibilities of Public Officials (2002) which establishes professional and accountable standards for officials against corruption and the spoils system. \nAlso, the Whistleblower Protection Enhancement Act of 2012 in the USA has established corruption to protect federal employees who report corruption, fraud, or other illegal activities within the government.\nCriticism.\nSome critics often accuse politicians of not communicating with the public. They accuse politicians' speeches of being sometimes overly formal, filled with many euphemisms and metaphors, and generally seen as an attempt to \"obscure, mislead, and confuse\".\nLack of awareness, selfishness, manipulation, political corruption and dishonesty are perceptions that people often accuse politicians of, and many see them as prioritizing personal interests over the public interests. Politicians in many countries are seen as the \"most hated professionals,\" and the least trustworthy, leading to public skepticism and constant criticism.\nIn addition, some politicians tend to be negative, this strategy, although it does not enhance their chances of being re-elected or gaining public support, politicians see this negativity as consistent with negative media bias, which increases their chances of securing media access and public attention.\nAlso, lack of accountability and the immunity from prosecution they receive as politicians results in further corruption and evasion from legal punishment, as represented by the immunity bath depiction by J.J. Hanberg.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "55961", "revid": "48789554", "url": "https://en.wikipedia.org/wiki?curid=55961", "title": "Chaco War", "text": "War between Bolivia and Paraguay (1932 to 1935)\nThe Chaco War (, ) was fought from 1932 to 1935. It was between Bolivia and Paraguay over control of the northern part of the Gran Chaco region (known in Spanish as the \"Chaco Boreal\"), which was thought to be rich in petroleum. The war is also referred to as \"La Guerra de la Sed\" (Spanish for \"The War of Thirst\"), since it was fought in the semi-arid Chaco. It was the first South America war in which modern weapons (such as machine guns, armoured fighting vehicles and airplanes) were used, and also the bloodiest South America war of the 20th century \u2014 around 2% of the Bolivian population and 3% of Paraguayans were killed during the conflict.\nDuring the war, both landlocked countries faced difficulties moving arms and supplies through neighbouring countries. Despite its income from mining and a larger and better-equipped army, problems with international trade and poor internal communications ultimately turned the tide against Bolivia. The war concluded at the Chaco Peace Conference in Buenos Aires in July 1938, at which both countries signed a peace treaty awarding three-quarters of the Gran Chaco to Paraguay.\nOrigins.\nThe origins of the war are attributed to a long-standing territorial dispute and to the discovery of oil deposits on the eastern Andes range. After losing territory to neighboring countries in the late 19th century, both Bolivia and Paraguay had become landlocked countries. The Chaco was sparsely populated, but control of the Paraguay River running through it would provide access to the Atlantic Ocean. Paraguay had lost almost half of its claimed territory to Brazil and Argentina as a consequence of the War of the Triple Alliance (1864\u20131870), and its economic viability depended on retaining control of the Paraguay River. In 1879, Bolivia lost its Pacific coast to Chile during the War of the Pacific. The 1929 Treaty of Lima ended the hopes of the Bolivian government of recovering a land corridor to the Pacific Ocean, so Bolivia viewed access to the Atlantic Ocean via the Paraguay River as imperative to international trade and further economic development.\nIn international arbitration, Bolivia argued that the region had been part of the original Spanish colonial province of Moxos and Chiquitos to which Bolivia was heir. Meanwhile, Paraguay based its case on the occupation of the land. Indeed, both Paraguayan and Argentine ranchers were already breeding cattle and exploiting the quebracho forests in the area, and the small population of nomadic indigenous Guaran\u00ed-speaking people was related to Paraguay's own Guaran\u00ed heritage. As of 1919, Argentine banks owned of land in the eastern Chaco, and the Casado family, a powerful part of the Argentine oligarchy, held 141,000. The presence of Mennonite colonies in the Chaco, who settled there in the 1920s under the auspices of the Paraguayan government, was another factor in favour of Paraguay's claim.\nThe impetus for war was exacerbated by a conflict between oil companies jockeying for exploration and drilling rights, with Royal Dutch Shell backing Paraguay and Standard Oil Company supporting Bolivia. The discovery of oil in the Andean foothills sparked speculation that the Chaco might prove to be a rich source of petroleum. Standard Oil was already producing oil from wells in the hills of eastern Bolivia, around Villamontes. In addition to the interests of these companies, Argentina's goal of importing petroleum from the Chaco also contributed to the instigation of the war. In opposition to the \"dependency theory\" of the war's origins, the British historian Matthew Hughes argued against the thesis that Bolivian and Paraguayan governments were the \"puppets\" of Standard Oil and Royal Dutch Shell respectively by writing: \"In fact, there is little hard evidence available in the company and government archives to support the theory that oil companies had anything to do with causing the war or helping one side or the other during the war\". The historian Bret Gustafson, on the other hand, argues that \"the blurred lines between the bank and the oil industry show that [Standard Oil] did indeed finance the Bolivian build-up, even if instigating the war was left to Bolivian generals.\"\nPrelude to war.\nThe first confrontation between the two countries dates back to 1885, when Bolivian president Gregorio Pacheco founded Puerto Pacheco, a port on the Upper Paraguay River, asserting that this new settlement was well inside Bolivian territory. In 1888, the Paraguayan government sent the gunboat \"Pirap\u00f3\", commanded by Paraguayan War veteran Domingo Antonio Ortiz, which forcibly evicted the Bolivians from the settlement. Two agreements followed, in 1894 and 1907, which neither the Bolivian nor the Paraguayan government approved. Meanwhile, in 1905, Bolivia \u2014 ignoring Paraguayan official protests \u2014 founded Ballivi\u00e1n and Guachalla, two new outposts along the Pilcomayo River in the Chaco.\nBolivian penetration in the region went unopposed until 1927, when the first blood was shed over the Gran Chaco. On 27 February, members of a Paraguayan Army foot patrol were taken prisoner near the Pilcomayo River and held in the Bolivian outpost of Fort\u00edn Sorpresa, where the commander of the Paraguayan detachment, Lieutenant Adolfo Rojas Silva, was shot and killed in suspicious circumstances. \"Fort\u00edn\" (Spanish for \"little fort\") was the name used for the small pillbox and trench-like garrisons constructed by the military forces in the Chaco. The Bolivian government formally regretted the death of Rojas Silva, but Paraguayan public opinion called it \"murder\". After subsequent talks in Buenos Aires failed to produce any agreement in January 1928, the dispute grew more violent. On 5 December 1928, a Paraguayan cavalry unit overran Fort\u00edn Vanguardia, an advance outpost established by the Bolivian army a few kilometres northwest of Bah\u00eda Negra. The Paraguayans captured 21 Bolivian soldiers and burned their huts to the ground.\nOn 14 December 1928, Bolivia retaliated by capturing Fort\u00edn Boquer\u00f3n (which would later be the site of the first major battle of the campaign), killing 15 Paraguayans. The Bolivians also conducted an air strike on Bah\u00eda Negra on 15 December, which caused few casualties and little damage. A return to the \"status quo ante bellum\" was eventually agreed on 12 September 1929 in Washington, DC, under pressure from the Pan American League, but an arms race had already begun, and both countries were on a collision course. The regular border clashes might have led to war in the 1920s if either side had been capable of waging war. However, neither Paraguay nor Bolivia had an arms industry, and both countries had to import vast quantities of arms from Europe and the United States to arm themselves for the coming conflict. It was this lack of sufficient arms that delayed the outbreak of the war until 1932.\nArmies.\nBolivian infantry forces were armed with the latest in foreign weapons, including DWM Maxim M1904 and M1911 machine guns, Czechoslovak ZB vz. 26 and Vickers-Berthier light machine guns, Mauser-type Czechoslovak Vz. 24 7.65\u00a0mm rifles (\"mosquetones\") and Schmeisser MP-28 II 9\u00a0mm submachine guns. At the outset, the Paraguayan troops used a motley collection of small arms, including the German Maxim, the British Vickers, the Browning MG38 water-cooled machine guns, and the Danish Madsen light machine gun. The primary service rifle was the M1927 7.65\u00a0mm Paraguayan Long Rifle, a Mauser design based on the M1909 Argentine Long Rifle and manufactured by the Oviedo arsenal in Spain. The M1927 rifle, which tended to overheat in rapid fire, proved highly unpopular with the Paraguayan soldiers. Some M1927 rifles experienced catastrophic receiver failures, a fault that was later traced to faulty ammunition. After the commencement of hostilities, Paraguay captured sufficient numbers of Bolivian VZ-24 rifles and MP 28 submachine guns (nicknamed \"piripipi\") to equip all of its front-line infantry forces.\nParaguay had a population only a third as large as that of Bolivia (880,000 vs. 2,150,000). However, Paraguay gained the upper hand because of its innovative style of fighting, centered on rapid marches and flanking encirclements, compared to Bolivia's more conventional strategy. In June 1932, the Paraguayan Army totaled about 4,026 men (355 combat officers, 146 surgeons and non-combatant officers, 200 cadets, 690 NCOs and 2,653 soldiers). Both racially and culturally, the Paraguayan Army was practically homogeneous. Almost all of its soldiers were European-Guaran\u00ed \"mestizos\". Bolivia's army, however, were mostly descended from the Altiplano's aboriginals of Quechua or Aymar\u00e1 (90% of the infantry troops), while the lower-ranking officers were of Spanish or other European ancestry, and the army commander-in-chief, Hans Kundt, was German. Although the Bolivian army had more manpower, it never mobilized more than 60,000 men, and no more than two-thirds of its army were ever on the Chaco. Paraguay, on the other hand, mobilized its entire army. A British diplomat reported in 1932 that the average Bolivian had never been anywhere close to the Chaco and \"had not the slightest expectation of visiting it in the course of his life.\" Most Bolivians had little interest in fighting, let alone dying, for the Chaco. Furthermore, the typical Bolivian soldier was a Quechua or Aymara peasant conscript accustomed to life high in the Andes Mountains and did not fare well in the low-lying, hot, and humid land of the Chaco.\nMany Paraguayan Army commanders had gained combat experience as volunteers with the French Army in World War I. Its army commander, Colonel (later General and then Marshal) Jos\u00e9 F\u00e9lix Estigarribia, soon rose to the top of the combat command. Estigarribia capitalized on the native Guarani knowledge of the forest and ability to live off the land to gain valuable intelligence on conducting his military campaigns. Estigarribia preferred to bypass Bolivian garrisons, and his subordinates, such as Colonel Rafael Franco, proved adept at infiltrating enemy lines often by encircling Bolivian strongholds (Paraguay held over 21,000 prisoners-of-war when the war ended, Bolivia some 2,500). Both sides resorted to entrenched strongpoints and used barbed wire, mortars, machineguns, and mines with interlocking fields of fire.\nParaguay's war effort was total. Buses were commandeered to transport troops, wedding rings were donated to buy weapons, and Paraguay had by 1935 widened conscription to include 17-year-olds and policemen. Perhaps the most important advantage enjoyed by Paraguay was that the Paraguayans had a rail network running to the Chaco with five narrow-gauge railroads totaling some running from the ports on the Paraguay River to the Chaco, which allowed the Paraguayan Army to bring men and supplies to the front far more effectively than the Bolivians ever managed. In 1928, the British legation in La Paz reported to London that it took the Bolivian Army two weeks to march their men and supplies to the Chaco and that Bolivia's \"inordinately long lines of communication\" would help Paraguay if war broke out. Furthermore, the drop in altitude from in the Andes to in the Chaco imposed further strain on Bolivia's efforts to supply its soldiers in the Chaco. Bolivia's railroads did not run to the Chaco, and all Bolivian supplies and soldiers had to travel to the front on badly-maintained dirt roads. Hughes wrote that the Bolivian elite was well aware of the logistical problems but that throughout the war, Bolivia's leaders had a \"fatalistic\" outlook. It took for granted that the fact that the Bolivian Army had been trained by a German military mission whilst the Paraguayan Army had been trained by a French military mission, together with the tough nature of their Quechua and Aymara Indian conscripts and the country's will to win and determination, would give them the edge in the war.\nBoth armies deployed a significant number of cavalry regiments, but they actually served as infantry since it was soon learned that the dry Chaco could not provide enough water and forage for horses. Only a relatively few mounted squadrons carried out reconnaissance missions at the divisional level.\nArmor, artillery, and motorized forces.\nAt the insistence of the Minister of War General Hans Kundt, Bolivia purchased several light tanks and tankettes for the support of infantry forces. German instructors provided training to the mostly Bolivian crews, who received eight weeks' training. The Vickers light tanks bought by Bolivia were the Vickers Type A and Type B, commissioned into the Bolivian army in December 1932, and were originally painted in camouflage patterns.\nHampered by the geography and difficult terrain of the Gran Chaco, combined with scarce water sources and inadequate logistical preparations, the Bolivian superiority in vehicles (water-cooled), tanks, and towed artillery did not prove decisive in the end. Thousands of truck and vehicle engines succumbed to the thick Chaco dust, which also jammed the heavy water-cooled machine guns employed by both sides. Having relatively few artillery pieces of its own, Paraguay purchased a quantity of Stokes-Brandt Model 1931 mortars. Highly portable (each of three parts could be carried by a soldier) and accurate, with a range of 3,000 yards, the \"angu'as\" (\"corn-mashers\" or \"mortar\" in Guarani) caused many casualties among Bolivian troops. In the course of the conflict, Paraguayan factories developed their own type of pyrotechnic-ignater hand grenade, the pineapple shaped \"carumbe'i\" (Guaran\u00ed for \"little turtle\") and produced trailers, mortar tubes, artillery grenades, and aerial bombs. The Paraguayan war effort was centralized and led by the state-owned national dockyards, managed by Jos\u00e9 Bozzano. The Paraguayan Army received its first consignment of \"carumbe'i\" grenades in January 1933.\nLogistics, communications, and intelligence.\nThe Paraguayans took advantage of their ability to communicate over the radio in Guaran\u00ed, a language not spoken by the average Bolivian soldier. Paraguay had little trouble in transporting its army in large barges and gunboats on the Paraguay River to Puerto Casado and from there directly to the front lines by railway, but most Bolivian troops had to come from the western highlands, some 800\u00a0km away and with little or no logistic support. It took a Bolivian soldier 14 days to cross the distance, as opposed to a Paraguayan soldier's four. The heavy equipment used by the Bolivian Army made things even worse. The poor water supply and the dry climate of the region played a key role during the conflict. There were thousands of non-combat casualties from dehydration, mostly by the Bolivian troops.\nAir and naval assets.\nThe Chaco War is also important as the first instance of large-scale aerial warfare to take place in the Americas. Both sides used obsolete single-engined biplane fighter-bombers. The Paraguayans deployed 14 Potez 25s, and the Bolivians made extensive use of at least 20 CW-14 Ospreys. Despite an international arms embargo imposed by the League of Nations, Bolivia in particular went to great lengths in trying to import a small number of Curtiss T-32 Condor II twin-engined bombers, disguised as civil transport planes, but they were stopped in Peru before they could be delivered.\nThe valuable aerial reconnaissance produced by Bolivia's superior air force in spotting approaching Paraguayan encirclements of Bolivian forces was largely ignored by Kundt and other Bolivian Army generals, who tended to dismiss such reports as exaggerations by overzealous airmen.\nFour Junkers Ju 52s were purchased by Bolivia, which used the German transports mainly for medical evacuation and air supply. The Ju 52s alone delivered more than 4,400 tons of cargo to the front.\nThe Paraguayan Navy played a key role in the conflict by carrying thousands of troops and tons of supplies to the front lines via the Paraguay River, as well as by providing anti-aircraft support to transport ships and port facilities.\nThe \"Humait\u00e1\" and \"Paraguay\", two Italian-built gunboats, ferried troops to Puerto Casado. On 22 December 1932, three Bolivian Vickers Vespas attacked the Paraguayan riverine outpost of Bah\u00eda Negra, on the Paraguay River, and killed an army colonel, but one of the aircraft was shot down by the gunboat \"Tacuary\". Both surviving Vespas met another gunboat, the \"Humait\u00e1\", while they were flying downriver. Paraguayan sources claim that one of them was damaged. Conversely, the Bolivian army reported that the \"Humait\u00e1\" limped back to Asunci\u00f3n seriously damaged. The Paraguayan Navy admitted that \"Humait\u00e1\" was struck by machine gun fire from the aircraft but claimed that its armor shield averted damage.\nShortly before 29 March 1933, a Bolivian Osprey was shot down over the Paraguay River, and on 27 April, a strike force of six Ospreys launched a successful mission from the base at Mu\u00f1oz against the logistic riverine base and town of Puerto Casado, but the strong diplomatic reaction of Argentina prevented any further strategic attacks on targets along the Paraguay River. On 26 November 1934, the Brazilian steamer \"Paraguay\" was strafed and mistakenly bombed by Bolivian aircraft while it was sailing the Paraguay River near Puerto Mihanovich. The Brazilian government sent 11 naval planes to the area, and its navy began to convoy shipping on the river.\nThe Paraguayan navy air service was also very active in the conflict by harassing Bolivian troops deployed along the northern front with flying boats. The aircraft were moored at Bah\u00eda Negra Naval Air Base, and consisted of two Macchi M.18s. The seaplanes carried out the first night air attack in South America when they raided the Bolivian outposts of Vitriones and San Juan, on 22 December 1934. The Paraguayan Navy has celebrated ever since the annual \"Day of the Naval Air Service\" on the anniversary of the action.\nThe Bolivian Army deployed at least 10 locally-built patrol boats and transport vessels during the conflict, mostly to ship military supplies to the northern Chaco through the Mamor\u00e9-Madeira system. The transport ships \"Presidente Saavedra\" and \"Presidente Siles\" steamed on the Paraguay River from 1927 to the beginning of the war, when both units were sold to private companies. The 50-ton armed launch \"Tahuamanu\", based in the Mamor\u00e9-Madeira fluvial system, was briefly transferred to Laguna C\u00e1ceres to ferry troops downriver from Puerto Su\u00e1rez and challenged for eight months the Paraguayan naval presence in Bah\u00eda Negra. She was withdrawn to the Itenez River, in northern Bolivia, after Bolivian aerial reconnaissance revealed the actual strength of the Paraguayan Navy in the area.\nConflict.\nPitiantuta Lake incident.\nOn 15 June 1932, a Bolivian detachment captured and burned to the ground the Fort\u00edn Carlos Antonio L\u00f3pez at Pitiantut\u00e1 Lake. The captain in charge had disobeyed explicit orders by Bolivian President Daniel Salamanca to avoid provocations in the Chaco region. One month later, on 16 July, a Paraguayan detachment drove the Bolivian troops from the area. The lake had been discovered by Paraguayan explorers in March 1931, but the Bolivian High Command was unaware of that when one of its aircraft spotted the lake in April 1932.\nAfter the initial incident, Salamanca changed his status quo policy over the disputed area and ordered the outposts of Corrales, Toledo, and Fort\u00edn Boquer\u00f3n to be captured. All three were soon taken, and in response, Paraguay called for a Bolivian withdrawal. Salamanca instead demanded that they were included in a \"zone of dispute\". On a memorandum directed to Salamanca on 30 August, Bolivian General Filiberto Osorio expressed his concerns over the lack of a plan of operations and attached one that focused on an offensive from the north. Quintanilla also asked for permission to capture two additional Paraguayan garrisons: Nanawa and Rojas Silva. In August, Bolivia slowly reinforced its 4,000-man First Bolivian Army, which was already in the conflict zone, with 6,000 men.\nThe breaking of the fragile \"status quo\" in the disputed areas of the Chaco by Bolivia convinced Paraguay that a diplomatic solution on agreeable terms was impossible. Paraguay gave its general staff orders to recapture the three forts. In August, Paraguay mobilized over 10,000 troops and sent them into the Chaco region. Paraguayan Lieutenant Colonel Jos\u00e9 F\u00e9lix Estigarribia prepared for a large offensive before the Bolivians had mobilized their whole army.\nFirst Paraguayan offensive.\nFort\u00edn Boquer\u00f3n was the first target of the Paraguayan offensive. The Boquer\u00f3n compound was guarded by 619 Bolivian troops and resisted a 22-day siege by a 5,000-man Paraguayan force. An additional 2,500 Bolivians attempted to relieve the siege from the southwest but were beaten back by 2,200 Paraguayans, who defended the accesses to the siege area. A few Bolivian units managed to enter Fort\u00edn Boquer\u00f3n with supplies, and the Bolivian Air Force dropped food and ammunition to the besieged soldiers. Having begun on 9 September, the siege ended when Fort\u00edn Boquer\u00f3n finally fell on 29 September 1932.\nAfter the fall of Fort\u00edn Boquer\u00f3n, the Paraguayans continued their offensive and executed a pincer movement, which forced some of the Bolivians to surrender. The Paraguayans had expected to lay a new siege on Fort\u00edn Arce, the most advanced Bolivian outpost in the Chaco, but when they got there, they found it in ruins. The 4,000 Bolivians who were defending Arce had retreated to the southeast to Fort\u00edn Alihuat\u00e1 and Saveedra.\nBolivian offensive.\nIn December 1932, Bolivia's war mobilization had concluded. In terms of weaponry and manpower, its army was ready to overpower the Paraguayans. General Hans Kundt, a former German officer who had fought on the Eastern Front of World War I, was called by Salamanca to lead the Bolivian counteroffensive. Kundt had served intermittently as military advisor to Bolivia since the beginning of the century and had established good relationships with officers of the Bolivian Army and the country's political elites.\nThe Paraguayan Fort\u00edn Nanawa was chosen as the main target of the Bolivian offensive and was to be followed by the command centre at Isla Po\u00ed. Their capture would allow Bolivia to reach the Paraguay River and to endanger the Paraguayan city of Concepci\u00f3n. The capture of the fortresses of Corrales, Toledo, and Fern\u00e1ndez by the Bolivian Second Corps were also part of Kundt's offensive plan.\nIn January 1933, the Bolivian First Corps began its attack on Fort\u00edn Nanawa. The stronghold was considered by the Paraguayans to be the backbone of their defenses. It had zig-zag trenches; kilometres of barbed wire; and many machine-gun nests, some of which were embedded in tree trunks. The Bolivian troops had stormed the nearby Paraguayan outpost of Mariscal L\u00f3pez, which isolated Nanawa from the south. On 20 January 1933, Kundt, who personally commanded the Bolivian force, launched six to nine aircraft and 6,000 unhorsed cavalry supported by 12 Vickers machine guns. However, the Bolivians failed to capture the fort but formed a defensive amphitheater in front of it. The Second Corps managed to capture Fort\u00edn Corrales and Fort\u00edn Platanillos but failed to take Fort\u00edn Fern\u00e1ndez and Fort\u00edn Toledo. After a siege that lasted from 26 February to 11 March 1933, the Second Corps aborted its attack on Fort\u00edn Toledo and withdrew to a defensive line, built 15\u00a0km from Fort\u00edn Corrales.\nAfter the ill-fated attack on Nanawa and the failures at Fern\u00e1ndez and Toledo, Kundt ordered an assault on Fort\u00edn Alihuat\u00e1. The attack on the fort overwhelmed its few defenders. The capture of Alihuat\u00e1 allowed the Bolivians to cut the supply route of the Paraguayan First Division. When the Bolivians were informed of the isolation of the First Division, they launched an attack on it. The attack led to the Battle of Campo Jord\u00e1n, which concluded in the retreat of the Paraguayan First Division to Gondra.\nIn July 1933, Kundt, still focusing on capturing Nanawa, launched a massive frontal attack on the fort in what came to be known as the Second Battle of Nanawa. Kundt had prepared for the second attack in detail by using artillery, airplanes, tanks, and flamethrowers to overcome Paraguayan fortifications. The Paraguayans, however, had improved existing fortifications and built new ones since the First Battle of Nanawa. The Bolivian two-pronged attack managed to capture parts of the defensive complex but was soon retaken by Paraguayan counterattacks by reserves. The Bolivians lost more than 2,000 men, who were injured or killed in the Second battle of Nanawa, but Paraguay lost only 559 men who were injured or killed. The failure to capture Nanawa and the heavy loss of life led Salamanca to criticize the Bolivian high command and to order it to spare more men. The defeat seriously damaged Kundt's prestige. In September, his resignation of his position as commander-in-chief was not accepted by the president. Nanawa was a major turning point in the war since the Paraguayan Army regained the strategic initiative, which had belonged to the Bolivians since early 1933.\nSecond Paraguayan offensive.\nIn September, Paraguay began a new offensive in the form of three separate encirclement movements in the Alihuat\u00e1 area, which was chosen since its Bolivian forces had been weakened by the transfer of soldiers to attack Fort\u00edn Gondra. As a result of the encirclement campaign, the Bolivian regiments Loa and Ballivi\u00e1n, totaling 509 men, surrendered. The Jun\u00edn regiment suffered the same fate, but the Chacaltaya regiment escaped encirclement because of the intervention of two other Bolivian regiments.\nThe success of the Paraguayan Army led Paraguayan President Eusebio Ayala to travel to the Chaco to promote Jos\u00e9 F\u00e9lix Estigarribia to the rank of general. In that meeting, Ayala approved Estigarribia's new offensive plan. On the other side, the Bolivians gave up their initial plan of reaching the Paraguayan capital, Asunci\u00f3n, and switched to defensive and attrition warfare.\nThe Paraguayan Army executed a large-scale pincer movement against Fort\u00edn Alihuat\u00e1 and repeated the previous success of those operations; 7000 Bolivian troops had to evacuate Fort\u00edn Alihuat\u00e1. On 10 December 1933, the Paraguayans finished their encirclement of the 9th and 4th divisions of the Bolivian Army. After attempts had been made to break through Paraguayan lines and 2,600 of their men had been killed, 7,500 Bolivian soldiers surrendered. Only 900 Bolivian troops, led by Major Germ\u00e1n Busch, managed to slip away. The Paraguayans obtained 8,000 rifles, 536 machine guns, 25 mortars, two tanks, and 20 artillery pieces from the captured Bolivian forces. By then, Paraguayan forces had captured so many Bolivian tanks and armored vehicles that Bolivia was forced to purchase Steyr Solothurn 15\u00a0mm anti-tank rifles to fend off its own armor. The remaining Bolivian troops withdrew to their headquarters at Mu\u00f1oz, which was set on fire and evacuated on 18 December. Kundt resigned as chief of staff of the Bolivian Army.\nTruce.\nThe massive defeat at Campo de V\u00eda forced the Bolivian troops near Fort\u00edn Nanawa to withdraw northwest to form a new defensive line. Paraguayan Colonel Rafael Franco proposed to launch a new attack against Ballivi\u00e1n and Villa Montes but was turned down, as Ayala thought that Paraguay had already won the war. A 20-day ceasefire was agreed upon between the warring parties on 19 December 1933. On 6 January 1934, when the armistice expired, Bolivia had reorganized its eroded army and had assembled a larger force than the one involved in its first offensive.\nThird Paraguayan offensive.\nBy early 1934, Estigarribia was planning an offensive against the Bolivian garrison at Puerto Su\u00e1rez, 145\u00a0km upriver from Bah\u00eda Negra. The marshes of the Pantanal and the lack of canoes to navigate through them convinced the Paraguayan commander to abandon the idea and to turn his attention to the main front. After the armistice had ended, the Paraguayan Army continued its advance by capturing the outposts of Platanillos, Loa, Esteros, and Jayucub\u00e1s. After the Battle of Campo de V\u00eda in December, the Bolivian Army built up a defensive line at Magari\u00f1os-La China. The line, carefully built, was considered to be one of the finest defensive lines of the war. However, a small Paraguayan attack on 11 February 1934, managed to breach the line to the surprise of the Paraguayan command, which forced the abandonment of the whole line. A Paraguayan offensive towards Ca\u00f1ada Tarija managed to surround and neutralize 1,000 Bolivian troops on 27 March.\nIn May 1934, the Paraguayans discovered a gap in the Bolivian defenses, which would allow them to isolate the Bolivian stronghold of Ballivi\u00e1n and to force its surrender. The Paraguayans worked all night to open a new route in the forests to make the attack possible. When Bolivian reconnaissance aircraft noticed the new path being opened in the forest, a plan was made to let the Paraguayans enter halfway up the path and then to attack them from the rear. The Bolivian operation resulted in the Battle of Ca\u00f1ada Strongest between 18 and 25 May. The Bolivians managed to capture 67 Paraguayan officers and 1,389 soldiers. After their defeat at Ca\u00f1ada Strongest, the Paraguayans continued their attempts to capture Ballivi\u00e1n. It was considered to be a key stronghold by the Bolivians, mostly for its symbolic position, since it was the most southeastern Bolivian position that remained after the second Paraguayan offensive.\nIn November 1934, Paraguayan forces once again managed to surround and to neutralize two Bolivian divisions at El Carmen. The disaster forced the Bolivians to abandon Ballivi\u00e1n and to form a new defensive line at Villa Montes. On 27 November 1934, Bolivian generals confronted Salamanca while he was visiting their headquarters in Villa Montes and forced him to resign. They replaced him with Vice President Jos\u00e9 Luis Tejada. On 9 November 1934, the 12,000-man-strong Bolivian Cavalry Corps managed to capture Yrendag\u00fc\u00e9 and to put the Paraguayan Army on the run. Yrendag\u00fc\u00e9 was one of the few places with fresh water in that part of the Chaco. Although the Bolivian cavalry was marching towards La Faye from Yrendag\u00fc\u00e9, a Paraguayan force recaptured all of the wells in Yrendague. Therefore, upon their return, the exhausted and thirsty Bolivian troops found themselves without water. The already-weakened force fell apart. Many were taken prisoner, and many of those who had avoided capture died of thirst and exposure after they had wandered aimlessly through the hot, dry forest. The Bolivian Cavalry Corps had been considered one of the best units of the new army that was formed after the armistice.\nEspionage and counterespionage.\nIn February 1934, Emilio Sfeir\u2014a Lebanese-Bolivian merchant residing in Jujuy, Argentina\u2014masterminded the planning and execution of the capture, in Argentine territory, of Juan Valori, the most important Paraguayan spy of the Chaco War.\nLast battles.\nAfter the collapse of the northern and the northeastern fronts, the Bolivian defenses focused on the south to avoid the fall of their war headquarters and supply base at Villamontes. The Paraguayans launched an attack on Ybybob\u00f3 and isolated some of the Bolivian forces on the Pilcomayo River. The battle began on 28 December 1934 and lasted until early January 1935. It caused 200 Bolivian troops to be killed and 1,200 to surrender, but the Paraguayans lost only a few dozen men. Some fleeing Bolivian soldiers were reported to have jumped into the fast-flowing waters of the Pilcomayo River to avoid capture.\nAfter that defeat, the Bolivian Army prepared for a last stand at Villamontes. The loss of that base would allow the Paraguayans to reach the proper Andes. Colonel Bernardino Bilbao Rioja and Oscar Moscoso were left in charge of the defenses after other high-ranking officers had declined. On 11 January 1935, the Paraguayans encircled and forced the retreat of two Bolivian regiments. The Paraguayans also managed in January to cut off the road between Villa Montes and Santa Cruz.\nThe Paraguayan commander-in-chief, Estigarribia, decided to launch a final assault on Villa Montes. On 7 February 1935, around 5,000 Paraguayans attacked the heavily fortified Bolivian lines near Villa Montes with the aim of capturing the oilfields at Nancarainza, but they were beaten back by the Bolivian First Cavalry Division. The Paraguayans lost 350 men and were forced to withdraw north toward Boyuib\u00e9. Estigarribia claimed that the defeat was largely because of the mountainous terrain in which his forces were not used to fighting. On 6 March, Estigarribia again focused all his efforts on the Bolivian oilfields, this time at Camiri, 130\u00a0km north of Villa Montes. The commander of the Paraguayan 3rd Corps, General Franco, found a gap between the Bolivian 1st and 18th Infantry regiments and ordered his troops to attack through it, but they became stuck in a salient with no hope of further progress. The Bolivian Sixth Cavalry forced the hasty retreat of Franco's troops to avoid being cut off. The Paraguayans lost 84 troops who were taken prisoner, and more than 500 dead were left behind. The Bolivians lost almost 200 men, but unlike their exhausted enemies, they could afford a long battle of attrition. On 15 April, the Paraguayans punched through the Bolivian lines on the Parapet\u00ed River and took over the city of Charagua. The Bolivian command launched a counteroffensive, which forced the Paraguayans back. Although the Bolivians' plan fell short of its target of encircling an entire enemy division, they managed to take 475 prisoners on 25 April. On 4 June 1935, a Bolivian regiment was defeated and forced to surrender at Ingavi, on the northern front, after a last attempt had been made to reach the Paraguay River. On 12 June, the day that the ceasefire agreement was signed, Paraguayan troops were entrenched only 15\u00a0km away from the Bolivian oil fields in Cordillera Province.\nThe military conflict ended with a comprehensive Paraguayan victory, but from a wider point of view, it was a disaster for both sides. Bolivia's Criollo elite had forcibly pressed large numbers of the male indigenous population into the army even though they felt little or no connection to the nation, and Paraguay fomented nationalist fervor among its predominantly mixed population. On both sides, especially Bolivia, soldiers were ill-prepared for the dearth of water and the harsh conditions of terrain and weather that they encountered. The effects of the lower-altitude climate had seriously impaired the effectiveness of the Bolivian Army. Most of its indigenous soldiers lived on the cold \"altiplano\", at elevations of over , and they found themselves at a physical disadvantage when they were called upon to fight in tropical conditions at almost sea level.\nForeign involvement.\nArms embargo and commerce.\nThe Bolivian Army was dependent on food supplies that entered southeastern Bolivia from Argentina through Yacu\u00edba. The army had great difficulty importing arms purchased at Vickers since both Argentina and Chile were reluctant to let war material pass through their ports. The only remaining options were the port of Mollendo, in Peru, and Puerto Su\u00e1rez, on the Brazilian border. Eventually, Bolivia achieved partial success since Vickers had persuaded the British government to request Argentina and Chile to ease the import restrictions imposed on Bolivia. Internationally, the neighboring countries of Peru, Chile, Brazil, and Argentina tried to avoid being accused of fueling the conflict and so limited the imports of arms to both Bolivia and Paraguay, but Argentina supported Paraguay behind the neutrality fa\u00e7ade. Paraguay received military supplies, economic assistance, and daily intelligence from Argentina throughout the war.\nThe Argentine Army established a special detachment along the border with Bolivia and Paraguay at Formosa Province in September 1932, called \"Destacamento Mixto Formosa\", to deal with deserters from both sides trying to cross into Argentine territory and to prevent any boundary crossing by the warring armies, but the cross-border exchange with the Bolivian army was banned only in early 1934, after a formal protest by the Paraguayan government. By the end of the war, 15,000 Bolivian soldiers had deserted to Argentina. Some native tribes living on the Argentine bank of the Pilcomayo, like the Wich\u00ed and Toba people, were often fired at from the other side of the frontier or strafed by Bolivian aircraft, and a number of members of the Mak\u00e1 tribe from Paraguay, led by deserters, who had looted a farm on the border and killed some of its inhabitants, were engaged by Argentine forces in 1933. The Mak\u00e1 had been trained and armed by the Paraguayans for reconnaissance missions. After the defeat of the Bolivian Army at Campo V\u00eda, at least one former Bolivian border outpost, Fort\u00edn Sorpresa Viejo, was occupied by Argentine troops in December 1933, which led to a minor incident with Paraguayan forces.\nAdvisers and volunteers.\nA number of volunteers and hired personnel from different countries participated in the war on both sides. The high command staff of both countries was at times dominated by Europeans. In Bolivia, General Hans Kundt, a German World War I Eastern Front veteran, was in command from the beginning of the war to December 1933, when he was relieved after a series of military setbacks. Apart from Kundt, Bolivia had also been advised in the last years of the war from a Czechoslovak military mission of World War I veterans. The Czechoslovak military mission assisted the Bolivian military after the defeat of Campo V\u00eda. Paraguay was getting input from 80 former White Russian officers, including two generals, Nikolai Ern and Ivan Belaieff, the latter of whom was part of General Pyotr Nikolayevich Wrangel's staff during the Russian Civil War. In the later phase of the war, Paraguay would receive training from a large-scale Italian mission.\nBolivia had more than 107 Chileans fighting on its side. Three died from different causes in the last year of the conflict. The Chileans involved in the war enrolled privately and were mostly military and police officers. They were motivated by the unemployment caused by both the Great Depression and by the political turbulence in Chile in the early 1930s (after the Chaco War ended, some of the Chilean officers went on to fight in the International Brigades during the Spanish Civil War). The enrollment of Chilean military personnel in the Bolivian Army caused surprise in Paraguay since former Chilean President General Carlos Ib\u00e1\u00f1ez del Campo in 1928 had supported Paraguay after Bolivian reprisals for the destruction of Fort\u00edn Vanguardia. The Paraguayan press denounced the Chilean government as not being neutral and went on to claim that the Chilean soldiers were mercenaries. On 12 August 1934, the Chilean ambassador in Asunci\u00f3n was recalled to Santiago in response to official Paraguayan support of the accusations against the Chilean government in the press. Early in the war, however, a few Chilean officers had joined the Paraguayan Army. Pressure from Asuncion led the Chilean Congress on 7 September 1934 to approve a law that made it illegal to join the armies of countries at war. That did not, however, stop the enrollment of Chileans in the Bolivian Army, and it has been argued that Chilean President Arturo Alessandri Palma secretly approved of the practice to get rid of potentially-troublesome elements of the military.\nAt least two Uruguayan military pilots, Benito S\u00e1nchez Leyton and Luis Tuya, volunteered for some of the most daring missions carried out by Paraguayan Air Force Potez 25s, like the resupply of besieged forces during the Battle of Ca\u00f1ada Strongest and the mass air strike on the Bolivian stronghold of Ballivi\u00e1n on 8 July 1934. During the relief mission on Ca\u00f1ada Strongest, Leyton's Potez n\u00ba 7 managed to come back home although it had been hit by almost 200 rounds.\nArgentina was a source of arms and ammunition for Paraguay. The Argentine military attach\u00e9 in Asuncion, Colonel Schweizer, continued to advise the Paraguayan command well after the start of hostilities. However, the more valuable contribution to the Paraguayan cause came from Argentine military intelligence (G2), led by Colonel Esteban Vacareyza, which provided nightly reports on Bolivian movements and supply lines running along the border with Argentina. The G2 had broken the Bolivian Army code, reading almost all the encrypted messages for the benefit of the Paraguayan General Staff. Argentine First World War veteran pilot Vicente Almandoz Almonacid was appointed Director of Military Aviation by the Paraguayan government from 1932 to 1933.\nThe open Argentine support for Paraguay was also reflected on the battlefield when several Argentine citizens, largely from Corrientes and Entre R\u00edos, volunteered for the Paraguayan Army. Most of them served in the 7th Cavalry Regiment \"General San Mart\u00edn\" as infantrymen. They fought against the Bolivian Regiments \"Ingavi\" and \"Warnes\" at the outpost of Corrales on 1 January 1933 and had a narrow escape there after they had been outnumbered by the Bolivians. The commander of the \"Warnes\" Regiment, Lieutenant-Colonel S\u00e1nchez, was killed in an ambush set up by the retreating forces, while the Argentine volunteers lost seven trucks. The greatest achievement of \"San Mart\u00edn\" took place on 10 December 1933, when the First Squadron, led by Second Lieutenant Javier Gustavo Schreiber, ambushed and captured the two surviving Bolivian Vickers six-ton tanks on the Alihuat\u00e1-Savedra road during the Battle of Campo V\u00eda.\nA major supporter of Paraguay was US Senator Huey Long. In a speech on the Senate floor on 30 May 1934, Long, a radical populist, claimed the war was the work of \"the forces of imperialistic finance\" and maintained that Paraguay was the rightful owner of the Chaco but that Standard Oil, which Long called \"promoter of revolutions in Central America, South America and Mexico,\" had \"bought\" the Bolivian government and started the war because Paraguay had been unwilling to grant it oil concessions. Because Long believed that Standard Oil was supporting Bolivia, he greatly supported Paraguay, and in a speech about the war on the Senate floor on 7 June 1934, he called Standard Oil \"domestic murder[re]s\", \"foreign murder[er]s\", \"international conspirators\", and \"rapacious thieves and robbers\". As a result, Long became a national hero in Paraguay, and in the summer of 1934, when the Paraguayans captured a Bolivian fort, it was renamed Fort Long in his honor.\nAftermath.\nWhen a ceasefire was negotiated for noon 12 June 1935, Paraguay had controlled most of the region. The last half-hour had a senseless shootout between the armies. That was recognized in a 1938 truce, signed in Buenos Aires and approved in a referendum in Paraguay by which Paraguay was awarded three-fourths of the Gran Chaco, . Bolivia was awarded navigation rights on the Paraguay and Paran\u00e1 Rivers despite having been provided with such access before the conflict. Bolivia retained the remaining territory, which bordered Puerto Busch.\nThe war cost both nations dearly. Bolivia lost between 56,000 and 65,000 people, 2% of its population, and Paraguay lost about 36,000, or 3% of its population. Paraguay captured 21,000 Bolivian soldiers and 10,000 civilians (1% of the Bolivian population); many of the captured civilians chose to remain in Paraguay after the war. In addition, 10,000 Bolivian troops, many of them ill-trained and ill-equipped conscripts, deserted to Argentina or injured or mutilated themselves to avoid combat. By the end of hostilities, Paraguay had captured 42,000 rifles, 5,000 machine guns and submachine guns, and 25\u00a0million rounds of ammunition from Bolivian forces.\nBolivia's stunning military blunders during the Chaco War led to a mass movement, known as the \"Generaci\u00f3n del Chaco\", away from the traditional order, which was epitomised by the Revolutionary Nationalist Movement, which led to the Bolivian National Revolution of 1952.\nA final document to demarcate the border based on the 1938 border settlement was signed on 28 April 2009 in Buenos Aires.\nOver the succeeding 77 years, no commercial amounts of oil or gas were discovered in the portion of the Chaco awarded to Paraguay. However, on 26 November 2012, Paraguayan President Federico Franco announced the discovery of oil in the area of the Pirity River. He said that \"in the name of the 30,000 Paraguayans who died in the war,\" the Chaco would soon be \"the richest oil zone in South America\" and \"the area with the largest amount of oil\". In 2014, Paraguay made its first major oil discovery in the Chaco Basin, with the discovery of light oil in the Lapacho X-1 well.\nOil and gas resources extend also from the Villa Montes area and the portion of the Chaco awarded to Bolivia northward along the foothills of the Andes. Today, the fields give Bolivia the second-largest resources of natural gas in South America, after Venezuela.\nHistoriography.\nProfessor Bret Gustafson describes popular memory of the Chaco War, still \"intensely felt\" among Bolivians today, as one in which Bolivians are the \"heroes of the Chaco\" mobilized to \"'defend the oil' from foreign usurpers.'\" Other versions of this narrative blame Standard Oil or Argentina for provoking the conflict to rob Bolivia of its oil deposits. Stephen Cote, however, convincingly argues that at the time of the conflict, no known oil deposits existed in the disputed Chaco region. Instead, landlocked Bolivia was hoping to gain control of a river port that might be navigable to the Atlantic.\nThe war has been widely ignored in the English-speaking world, with the British historian Matthew Hughes noting that one bibliography of books and articles on the war listed some 450 publications, of which only 14 were in English and only 2 were military, as opposed to diplomatic, histories of the war. In contrast, there is an enormous historical literature on the war in Spanish with the subject of the conflict being of lively interest in both Bolivia and Paraguay. Almost all of the historical work done in both nations is dominated by the \"heroic\" interpretation with the war being presented simply as a matter of willpower, with neither Bolivian nor Paraguayan historians having any interest in other aspects of the war like logistics as a determinate factor. Very typical of the Spanish language histories was Marshal Estigarribia's remark in his memoirs: \"But to this organized and arrogant power we intended to oppose the virile tradition of our people and the discipline of our courage\" That for him was a sufficient explanation of Paraguay's victory. Paraguayan historians tend to take the view that their nation won because the Paraguayan will to win was stronger, and likewise, Bolivian historians tend to argue that if only the Bolivian Army had fought harder, their nation somehow would have been victorious. Hughes stated that most of the historical work on the Chaco War was not of the highest quality: \"It is difficult in the Spanish language literature to discern any clear trends in terms of a conceptual, analytical or theoretical framework with a critical, objective core that unpacks the main military aspects of the conflict, precisely the sort of approach that is becoming commonplace for military studies of other major wars of the contemporary period\".\nAs part of the \"heroic\" interpretation, writing on the war in both Bolivia and Paraguay tends to be taken up with reminiscences of veterans, with little attempt to link up the experiences of the ordinary soldiers to the broader story of the war and in a way that many outsiders find off-putting to be obsessively making their nations' case for ownership of the Chaco.\nCultural references.\nAugusto C\u00e9spedes, the Bolivian ambassador to UNESCO, and one of the most important Bolivian writers of the 20th century, wrote several books describing different aspects of the conflict. As a war reporter for the newspaper \"El Universal\", C\u00e9spedes witnessed the penuries of the war, which he described in \"Cr\u00f3nicas heroicas de una guerra est\u00fapida\" (\"Heroic Chronicles of a Stupid War\") among other books. Several of his fiction works, which are considered masterworks of the genre, used the Chaco War as a setting. Another diplomat and important figure of Bolivian literature, Adolfo Costa du Rels, wrote about the conflict, and his novel \"Laguna H3\", published in 1938, was also set during the Chaco War.\nOne of the masterpieces of the Paraguayan writer Augusto Roa Bastos, the 1960 novel \"Hijo de hombre\", described in one of its chapters the carnage and harsh war conditions during the Siege of Boquer\u00f3n. The author himself took part in the conflict by joining the Paraguayan Navy's medical service on board the transport ship \"Holanda\" at the age of 17. The Argentine film \"Hijo de Hombre\" or \"Thirst\", directed by Lucas Demare in 1961, was based on this part of the novel.\nIn Pablo Neruda's poem, \"Standard Oil Company\", he referred to the Chaco War in the context of the role that oil companies had played in the war.\nThe Chaco War, particularly the brutal Battle of Nanawa, played an important role in the adventure novel \"Wings of Fury\", by R. N. Vick.\nThe Paraguayan polka, \"Regimiento 13 Tuyut\u00ed\", composed by Ram\u00f3n Vargas Colman and written in Guaran\u00ed by Emiliano R. Fern\u00e1ndez, remembered the Paraguayan Fifth Division and its exploits in the battles around Nanawa in which Fern\u00e1ndez fought and was injured. On the other side, the Siege of Boquer\u00f3n inspired \"Boquer\u00f3n abandonado\", a Bolivian \"tonada\" recorded by the Bolivian folk singer and politician Zulma Yugar in 1982.\n\"The Broken Ear\", one of \"The Adventures of Tintin\" series of comic stories by the Belgian author Herg\u00e9 (Georges R\u00e9mi), is set during a fictionalized account of the war between the invented nations of San Theodoros and Nuevo Rico.\n\"Barrage of Fire\" by the Bolivian novelist Oscar Cerruto narrated the cruel realities of life in Bolivia during the war through the experiences of a young protagonist.\nLester Dent, as Kenneth Robeson, wrote \"Dust of Death\" (1935), one of his Doc Savage pulp fiction novels. In it, Doc Savage finds himself caught in the middle of two embattled South American republics (obviously Bolivia and Paraguay) that have found a new and deadly foe in the form of an evil hooded figure, known as The Inca in Gray. When The Inca deploys a deadly \"Dust of Death\" to slaughter citizens on both sides of the fighting, Doc Savage and his team rush into the battle to try to save the day and to avoid the firing line.\nThe film \"Chaco\" (2020), directed by Diego Mondaca, follows a company of Bolivian soldiers wandering around the bush during the Chaco War in 1934.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55962", "revid": "1063872", "url": "https://en.wikipedia.org/wiki?curid=55962", "title": "Kemal Attaturk", "text": ""}
{"id": "55964", "revid": "14208764", "url": "https://en.wikipedia.org/wiki?curid=55964", "title": "Lennox Lewis", "text": "British-Canadian boxer (born 1965)\nLennox Claudius Lewis (born 2 September 1965) is a boxing commentator and former professional boxer who competed in the heavyweight division from 1989 to 2003. He was a three-time world champion, a two-time lineal champion, and held the undisputed championship. Holding dual British and Canadian citizenship, Lewis represented Canada as an amateur at the 1984 and 1988 Olympics, winning the super-heavyweight gold medal in 1988. Lewis is regarded by many as one of the greatest heavyweight boxers of all time, and is considered among the greatest British boxers and among the greatest Canadian boxers in history.\nIn his first three years as a professional, Lewis won several regional heavyweight championships, including the European, British, and Commonwealth titles. After winning his first 21 fights, he defeated Donovan Ruddock in 1992 to take over the number one position in the World Boxing Council (WBC) rankings. He was declared WBC heavyweight champion later that year after Riddick Bowe gave up the title, refusing to defend it against Lewis. He defended the title three times before an upset knockout loss to Oliver McCall in 1994. Lewis avenged the loss in a 1997 rematch to regain the vacant WBC title.\nTwo fights against Evander Holyfield in 1999 (the first ending in a controversial draw while the rematch was won via unanimous decision) saw Lewis become undisputed heavyweight champion by unifying his WBC title with Holyfield's World Boxing Association (WBA) and International Boxing Federation (IBF) titles. In 2000, the WBA stripped Lewis of his title when he chose to face Michael Grant in April instead of mandatory challenger John Ruiz. Similarly, the IBF stripped Lewis of their title in 2002 when he chose not to face their mandatory challenger Chris Byrd.\nLewis was knocked out by Hasim Rahman in an upset in 2001, but this defeat was avenged later in the year, with Lewis regaining the WBC and IBF titles. In 2002, Lewis defeated Mike Tyson in one of the most highly anticipated fights in boxing history. Prior to the event, Lewis was awarded the \"Ring\" magazine heavyweight title, which had been discontinued in the late 1980s. In what would be his final fight, Lewis defeated Vitali Klitschko by stoppage in 2003. He eventually vacated his remaining titles and retired from boxing in February 2004.\nEarly life.\nLewis was born on 2 September 1965 in West Ham, London, to Jamaican immigrant parents and according to his mother, he would often fight with other children growing up. At birth he weighed 4.8\u00a0kg (10\u00a0lb 10 oz), and was given the name Lennox by the doctor, who said \"he looked like a Lennox.\"\nLewis moved to Kitchener, Ontario, Canada with his mother in 1977 at the age of 12. He attended Cameron Heights Collegiate Institute for high school, where he excelled in Canadian football, soccer, and basketball. In the 1982\u201383 school year, he helped the school's AAA basketball team win the Ontario provincial championship.\nAmateur career.\nLewis eventually decided that his favourite sport was boxing. He took up boxing circa 1978. He became a dominant amateur boxer and won the gold medal at the Junior World Championships in 1983. At age 18, Lewis represented Canada in the super-heavyweight division at the 1984 Summer Olympics in Los Angeles. By that time he was ranked #6 in the world by the AIBA. He advanced to the quarter-finals, where he lost by decision to Tyrell Biggs of the US, who went on to win the gold medal. Despite being 6'5\" tall, and having a very strong punch, his coaches admitted they had to pressure him to convert size and raw talent into aggression. His amateur boxing coaches were Arnie Boehm and Adrian Teodorescu, who guided Lewis to the Olympic title in 1988.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"I think in the first fight I was just trying to knock him out, trying to prove my stuff because a lot of people thought the Cubans were unbeatable. I didn't think so at all. I just wanted to go out there and prove it by knocking him out. I guess that was a bit too much. I should have stuck to my natural talent and boxed.\"\n\u2014Lennox Lewis on his two fights versus Jorge Luis Gonz\u00e1lez in August 1987\n Lewis chose not to turn professional after the Olympics, and instead fought four more years as an amateur, hoping for a second chance to win a gold medal. At the 1986 World Championships, he lost in the preliminary round to Petar Stoimenov of Bulgaria. Later that year, Lewis won gold at the Commonwealth Games. He had a close fight against Cuban Jorge Luis Gonz\u00e1lez at the 1987 Pan American Games super-heavyweight finals: the American judge scored the bout in favour of Lewis 60\u201357, while the judges from the Dominican Republic, Venezuela and Uruguay scored the bout 59\u201358 for Gonz\u00e1lez. He avenged the loss shortly thereafter, boxing for the North American amateur title eight days later.\nAfter winning several more amateur titles in the following years, he travelled to Seoul, South Korea, for the 1988 Summer Olympics and achieved his goal. In the gold medal final, Lewis defeated Riddick Bowe with a second-round referee stopped contest (RSC). Lewis became the first super-heavyweight gold medallist to become world heavyweight champion as a professional. In the Games' closing ceremony, Lewis was Canada's flag bearer. Lewis became the first Canadian to win boxing gold in 56 years.\nLewis, upon turning professional, had registered an amateur record of 85\u20139. HBO Boxing credited him with a shorter amateur record of 75 wins (58 by knockout) and 7 losses. Of all losses on the record, Valeriy Abadzhyan of the Soviet Union was the only opponent to stop Lewis in amateurs, in October 1986.\nAfter winning the Olympic gold, Lewis was approached immediately by big-time American boxing promoters, including Bob Arum. However, he was not overly impressed by their contract offers and thought about signing a professional contract with a Toronto-based promotion group. \"I feel like a basketball player being scouted by scouts down in the States. I don't want anyone controlling me. These (offers) coming to me after the Olympics are mainly because I won the gold.\"\nProfessional career.\nEarly career.\nHaving achieved his goal, Lewis declared himself a professional and moved back to his native England. He claimed he had always considered himself British, but one article reported that many British fans regarded him as \"a Canadian at heart and a Briton for convenience.\" In 2015 Lewis explained \"When I turned pro, I had to go to the United Kingdom in order to pursue my career. The infrastructure to develop boxers wasn't in Canada then.\"\nLewis signed with boxing promoter Frank Maloney and his early professional career was filled with knockouts of journeymen, as well as fighters such as Osvaldo Ocasio.\nBritish, Commonwealth and European champion.\nAfter he signed with American promoter Main Events, he won the European heavyweight title in 1990 against Frenchman Jean Maurice Chanet. In his next fight in March 1991, Lewis won the British title against undefeated, world-ranked Gary Mason, and in April 1992 won the Commonwealth title against Derek Williams. Lewis was a top-five world heavyweight, and during this period he also defeated former WBA heavyweight champion Mike Weaver, 1984 Olympic Gold medalist Tyrell Biggs, former world cruiserweight title holders Glenn McCrory and trial horses Levi Billups and Mike Dixon.\nOn 31 October 1992, Lewis knocked out Canadian Donovan \"Razor\" Ruddock in two rounds for the number one contender's position in the WBC rankings. It was Lewis's most impressive win to date and established him as one of the world's best heavyweights. Sportscaster Larry Merchant declared, \"We have a great new heavyweight.\"\nFirst reign as WBC heavyweight champion.\nThe win over Ruddock made Lewis the mandatory challenger for Riddick Bowe's heavyweight championship. Bowe held a press conference during which he threw his WBC title belt in a rubbish bin, relinquishing it to avoid a mandatory defence against Lewis. On 14 December 1992, the WBC declared Lewis its champion, making him the first world heavyweight titleholder from Britain in the 20th century.\nLewis defended the belt three times, defeating Tony Tucker, whom he knocked down for the first time in Tucker's career, and Frank Bruno and Phil Jackson by knockout. The Lennox Lewis vs. Frank Bruno fight was the first time two British-born boxers fought for a version of the world heavyweight title in the modern era.\nLewis vs. McCall.\nLewis lost his WBC title to Oliver McCall on 24 September 1994 in a huge upset at the Wembley Arena in London. In the second round, McCall landed a powerful right cross, putting Lewis on his back. Lewis returned to his feet at the count of six, but stumbled forward into the referee in a daze. Referee Jose Guadalupe Garcia felt Lewis was unable to continue and ended the fight, giving McCall the title by technical knockout. Lewis and others argued the stoppage was premature and that a champion should be given the benefit of the doubt. In spite of the Lewis camp protests, \"Boxing Monthly\" editor Glynn Leach pointed out that Lewis \"only seemed to recover his senses once the fight was waved off\", and that \"in the opinions of everyone I spoke to at ringside, the decision was correct.\"\nAfter the fight, Lewis decided he needed a new trainer to replace Pepe Correa, who had become increasingly difficult to work with. Correa denounced Lewis in public after being fired. Renowned trainer Emanuel Steward, who had been McCall's trainer during their fight, was Lewis's choice. Even before the fight with McCall, Steward had seen much potential in Lewis and immediately expressed a desire to work with him. He corrected several of Lewis's technical flaws, which included maintaining a more balanced stance, less reliance on his cross, and a focus on using a strong, authoritative jab; the latter of which would become a hallmark of Lewis's style throughout the rest of his career. Their partnership lasted until Lewis's retirement.\nSecond reign as WBC heavyweight champion.\nIn his first comeback fight, Lewis was given a chance to fight for the mandatory challenger position within the WBC and won it by knocking out American contender Lionel Butler. However, at the behest of promoter Don King, the WBC bypassed him and gave Mike Tyson the first chance at the title recently won by Briton Frank Bruno from Oliver McCall. Bruno had previously lost to both Lewis and Tyson.\nLewis had the number 1 contender's slot in the WBC rankings when he knocked out Australian Justin Fortune, then defeated former WBO Champion Tommy Morrison in October 1995, winning the minor IBC title. This was followed by a close majority decision win over Olympic gold medallist and former WBO champion Ray Mercer in May 1996. Lewis successfully sued to force Tyson to make a mandatory defence of the WBC title against him. Lewis was offered a $13.5 million guarantee to fight Tyson to settle the lawsuit, but turned it down. This would have been Lewis's highest fight purse to date. Lewis accepted $4 million from Don King to step aside and allow Tyson to fight Bruce Seldon instead, with a guarantee that if Tyson defeated Seldon, he would fight Lewis next. After winning the WBA title from Seldon, Tyson relinquished the WBC title to fight Evander Holyfield instead. The WBC title was declared vacant. This set up a rematch between Lewis and McCall, who met on 7 February 1997 in Las Vegas for the WBC title. \nIn one of the strangest fights in boxing history, McCall, who had lost the first three rounds, refused to box in the fourth and fifth rounds. He then began crying in the ring, forcing the referee to stop the fight and award Lewis the victory and the title. As newly recrowned WBC champion, Lewis successfully defended the title in 1997 against fellow Briton and former WBO world champion Henry Akinwande, who was disqualified after five rounds for excessive clinching. Lewis then met Poland's Andrew Golota, whom he knocked out in the first round. Lewis retained the WBC world title in 1998 when he knocked out lineal champion Shannon Briggs, who had recently outpointed George Foreman in a controversial fight to win the lineal title in five rounds, and beat formerly undefeated European champion \u017deljko Mavrovi\u0107 from Croatia in a 12-round unanimous decision. Lewis stated in 2006 that his fight with Mavrovic was the most awkward win of his career.\nUndisputed heavyweight champion.\nLewis vs. Holyfield.\nOn 13 March 1999, Lewis faced WBA and IBF title holder Evander Holyfield in New York City in what was supposed to be a heavyweight unification bout. Lewis fought a tactical fight, keeping Holyfield off balance with a long jab and peppering him with powerful combinations throughout the bout. Although most observers in the media believed Lewis had clearly won the fight, the bout was declared a draw, to much controversy. \nThe raw statistics of the fight suggested the bout belonged to Lewis, who landed 348 punches compared to Holyfield's 130. Lewis also out-jabbed Holyfield 137 to 52. Judge Eugenia Williams, who scored the fight in Holyfield's favour, said she saw Lewis land fewer punches than Holyfield. The camps of both fighters agreed to an immediate rematch with both fighters receiving equal purses of $15 million dollars.\nLewis vs. Holyfield II.\nThe sanctioning bodies ordered a rematch. Eight months later in Las Vegas (13 November 1999), the two men fought again in a more competitive contest than the original fight, with the two boxers having some heavy exchanges from rounds six to nine. The punch stats however still clearly favoured Lewis, who landed 195 punches to Holyfield's 137, although Lewis landed 119 power shots and 76 jabs, showing a definite shift in his tactics from the first fight, when he focused more on the jab. This time the three judges scored the fight unanimously (115\u2013113, 116\u2013112 and 117\u2013111) in favour of Lewis, who became undisputed heavyweight champion of the World. The British public voted Lewis the 1999 BBC Sports Personality of the Year.\nThough not as contentiously disputed as in their first bout, the result of the rematch was also challenged as numerous media sources and observers felt Holyfield should have won a decision or kept his unified championship with a draw. Some writers and reporters speculated that the verdict for Lewis had been a form of retribution for the controversial results of the prior bout.\nLewis did not view either bout with Evander Holyfield as among his most difficult, but conceded Holyfield tested his limits more than any other boxer. &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"People seem to be genuinely surprised when I tell them Holyfield was my toughest opponent, not to be confused with my toughest fight, which was Ray Mercer, but when you really dive into why that is, it actually makes a lot of sense.\"\nFirst reign as unified heavyweight champion.\nAfter Lewis defeated Holyfield the WBA ordered Lewis to defend the title against John Ruiz of Chelsea, Massachusetts, who was then an obscure Don King fighter who had been made the WBA's number one-ranked contender. The WBA gave permission for Lewis to fight his WBC mandatory Michael Grant first if he would fight Ruiz next, to which Lewis agreed. Opposed to this, King challenged this decision in court on the basis of a clause in the Lewis-Holyfield rematch contract that said Lewis's first bout as undisputed champion would be against the WBA's number one contender. Lewis was therefore to be stripped of his WBA belt if he fought Grant first. It was because of this that the WBA instated its \"Super Champion\" title, giving unified titleholders who also hold a WBA belt more time to defend against mandatory challengers.\nLewis proceeded to fight the 203\u00a0cm (6 foot 7 inch) American Michael Grant, whom he considered the best contender available. He successfully defended his WBC, IBO and IBF titles against Grant with a second-round knockout victory in Madison Square Garden in April 2000.\nLater that same year, Lewis knocked out South African Francois Botha in two rounds in London, before winning a 12-round decision against New Zealander and IBF mandatory opponent, David Tua in Las Vegas.\nLewis vs. Rahman.\nOn 21 April 2001, Lewis was knocked out by 20-to-1 underdog Hasim Rahman in a bout at Carnival City Casino in South Africa. The main event actually took place on Sunday 22 April 2001 at 05:00 local time in order to accommodate HBOs significant United States\u2013based audience at a reasonable hour on the Saturday night. Before the bout, Lewis had a role in the film \"Ocean's Eleven\" in which he \"boxed\" against Wladimir Klitschko.\nSecond reign as unified heavyweight champion.\nLewis vs. Rahman II.\nLewis immediately sought a rematch with the new champion; Rahman, however, now being promoted by Don King, tried to secure another opponent for his inaugural title defence. Lewis took Rahman to court to honour the rematch clause in their contract. Rahman was ordered to honour the clause and give Lewis a rematch in his first title defence. While promoting the rematch with Rahman on ESPN's Up Close, the fighters got into a brawl similar to the one between Muhammad Ali and Joe Frazier in front of Howard Cosell on \"Wide World of Sports\". Lewis regained the title on 17 November by outclassing and then knocking out Hasim Rahman in the fourth round of their rematch.\nLewis vs. Tyson.\nOn 8 June 2002, Lewis defended his title against Mike Tyson. Ticket sales were slow because they were priced as high as US$2,400, but a crowd of 15,327 turned up to see boxing's then biggest event at the Pyramid Arena in Memphis, Tennessee. Tyson also had to pay Lewis $335,000 out of his purse for biting him at the news conference announcing the fight, which was originally scheduled for 6 April 2002 in Las Vegas. Las Vegas, however, rejected the fight because of Tyson's licensing problems and several other states refused Tyson a licence before Memphis finally bid US$12\u00a0million to land it.\nBy the end of the seventh round Tyson was tired and sluggish, his face swollen and his eyes cut. He was knocked out in the eighth by a right cross. After the fight, George Foreman declared, \"He [Lewis] is, no doubt, the best heavyweight of all time. What he's done clearly puts him on top of the heap.\"\nThis was the highest-grossing event in pay-per-view history, generating US$106.9\u00a0million from 1.95\u00a0million buys in the US, until it was surpassed by De La Hoya-Mayweather in 2007. Both fighters were guaranteed US$17.5 million.\nLewis vs. Klitschko.\nLewis was forced to vacate the IBF title in 2002 after refusing to face mandatory challenger Chris Byrd. In May 2003, Lewis sued boxing promoter Don King for US$385\u00a0million, claiming that King used threats and bribery to have Tyson pull out of a rematch with Lewis and a fight on the card of a Lewis title defence.\nLewis scheduled a fight with Kirk Johnson for June, but when Johnson suffered an injury in training, Lewis fought Vitali Klitschko, the WBC's No. 1 contender and former WBO champion. Lewis had planned to fight him in December, but since Klitschko had been on the undercard of the Johnson fight anyway, they agreed to square off on 21 June. Lewis entered the ring at a career high 116\u00a0kg (256&lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20442 pounds). Lewis was dominated in the early rounds and was wobbled in round two by solid Klitschko punches. Lewis opened a cut above Klitschko's eye with a right cross in the third round and gave a better showing from the fourth round onwards. With both fighters looking tired before the start of round seven, the doctor advised that the fight should be stopped because of a severe cut above Klitschko's left eye, awarding Lewis victory by TKO. Klitschko was leading 58\u201356 on all three judges' scorecards when the fight was stopped. Lewis was guaranteed US$7 million and Klitschko US$1.4 million. The gate was US$2,523,384 from an attendance of 15,939 at the Staples Center in California. The fight aired live on HBO's \"World Championship Boxing\" with approximately 7 million viewers.\nInterviewed about the fight by HBO, Dr. Paul Wallace explained his decision to stop the fight:\nWhen he raised his head up, his upper eyelid covered his field of vision. At that point I had no other option but to stop the fight. If he had to move his head to see me, there was no way he could defend his way against a punch.\nKlitschko's face required sixty stitches.\nBecause Klitschko had fought so bravely against Lewis, boxing fans soon began calling for a rematch. The WBC agreed, and kept the Ukrainian as its No. 1 contender. Lewis initially was in favour of a rematch:\nI want the rematch, I enjoyed that fight. It was just a fight. We went at it. You have to play dollars and cents but I'm opting more for the rematch.\nNegotiations for the rematch followed but Lewis changed his mind. Instead, Klitschko fought and defeated Kirk Johnson on 6 December in WBC Eliminator, setting up a mandatory rematch with Lewis. Lewis announced his retirement shortly thereafter in February 2004, to pursue other interests, including sports management and music promotion, and vacated the title. Lewis said he would not return to the ring. At his retirement, Lewis's record was 41 wins, two losses and one draw, with 32 wins by knockout.\nRetirement.\nIn 2008 when asked about a potential bout after being antagonised by Riddick Bowe, Lewis quipped \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"He waits until I am in retirement to call out my name, I will come out of retirement to beat up that guy. I'll beat him up for free.\"\nIn 2011, Bowe again confronted Lewis, this time over Twitter, demanding he \"put [his] gold medal on and let's fight for that!!\", where Lewis remarked \"I thought we already did.\"\nLewis worked as a boxing analyst for HBO on \"Boxing After Dark\" from 2006 until 2010.\nFighting style.\nLewis was a classic upright boxer, who beat opponents from the outside with his dominant 213\u00a0cm (84 inch) reach. His jab, which was often a pawing shot early in his career, became a formidable weapon under the tutelage of Emmanuel Steward, which Lewis used to set up his signature punch, the straight right hand. Under Steward, Lewis became less reliant on his right hand and displayed a more complete skill-set. Criticised at times for being too patient and for his lack of infighting skills, Lewis was at his most effective when boxing from range. Known for his physical strength, Lewis was able to maneuver opponents into punching range and was especially effective against taller opponents. Lewis eventually developed into one of the more complete heavyweights, able to box at range or fight aggressively when necessary, as well as being a hard puncher.\nLegacy.\nLewis was the seventh Olympic gold medallist to become world heavyweight champion after Floyd Patterson, Muhammad Ali, Joe Frazier, George Foreman, Leon Spinks, and Michael Spinks. He holds the distinction of being the first professional heavyweight champion to win a gold medal in the super-heavyweight category, which was not created until the 1984 Summer Olympics. He is also the only boxer to represent Canada at the Summer Olympics and subsequently win a professional world title. Lewis was the first boxer to hold the British heavyweight title and subsequently win a world title. Although three fighters have since repeated this feat (Herbie Hide, Tyson Fury, and Anthony Joshua), only Lewis also won the Lonsdale belt outright.\nWhile struggling to achieve popularity and respect earlier in his professional career, Lewis's standing has increased since his retirement in 2003, and he is now considered one of the greatest heavyweights of all time. Struggling to win the affection of the British public and facing indifference from an American audience, Lewis's body of work eventually established him as the dominant heavyweight of his time. He was the last undisputed heavyweight champion, until May 2024, when Oleksandr Usyk defeated Tyson Fury.\nLewis became one of only two boxers in history, and the first since Ken Norton in 1978, to have been awarded the heavyweight championship without actually winning a championship bout when the WBC awarded him their title in 1992. This was due to Riddick Bowe relinquishing the title after failing to agree to defend the title against Lewis, who had become the mandatory challenger by defeating Donovan Ruddock a few weeks earlier. In 2001, Lewis became the fourth boxer (after Muhammad Ali, Evander Holyfield and Michael Moorer) to have held the world heavyweight championship on three occasions.\nLewis defeated 15 boxers for the world heavyweight title, the fifth-most in history. His combined three reigns tally 3,086 days (8 years, 5 months and 13 days), which ranks as the fourth-longest cumulative time spent as world heavyweight champion. His total of fourteen successful defences ranks as the fifth-highest in heavyweight history. At four years, two months and fifteen days, Lewis has the twelfth-longest reign in heavyweight championship history. As of December 2024, BoxRec ranks Lennox as the 55th greatest fighter of all time, pound for pound.\nIn 2018, \"Boxing News\" ranked Lewis as the third-greatest heavyweight of all time, behind Muhammad Ali and Joe Louis. While acknowledging that he could occasionally be vulnerable, the magazine stated that at his best, Lewis was as unbeatable as any heavyweight in history. In 2017, \"Boxing News\" also ranked Lewis as the second best British fighter of all time, after Jimmy Wilde. In the same year, \"The Ring\" magazine ranked Lewis as both the greatest heavyweight of the last thirty years and the joint-eleventh greatest heavyweight of all time (alongside Evander Holyfield), describing him as \"a giant who fought with finesse\" who beat every available contender. Thomas Hauser stated that the idea of Lewis having no chin was a myth, citing his rising from the powerful punch from Oliver McCall which floored Lewis for the first knockdown of his career, and suggesting that he was perhaps stopped prematurely. He also contended that the knockout punch from Hasim Rahman in their first fight would have knocked out anyone. In 2003, \"The Ring\" ranked Lewis 33rd in their list of greatest punchers of all time.\nAlong with Ingemar Johansson and Rocky Marciano, Lewis is one of three world heavyweight champions to have retired with victories over every opponent he faced as a professional. Unlike Johansson, who lost twice to Floyd Patterson after winning their first bout, Lewis is the only heavyweight to have avenged all his in-ring defeats. He is also, along with Gene Tunney, Marciano and Vitali Klitschko, one of four heavyweight champions to have ended his career as world champion, and with a world title fight victory in his final fight.\nIn 1999, he was named Fighter of the Year by the Boxing Writers Association of America, as well as BBC Sports Personality of the Year. In 2008, Lewis was inducted into Canada's Sports Hall of Fame. In 2009, in his first year of eligibility, Lewis was inducted into the International Boxing Hall of Fame. He was inducted into the Ontario Sports Hall of Fame in 2012.\nLife outside boxing.\nIn 2000, Lewis appeared on Reflection Eternal's debut album \"Train of Thought\", giving a shout out on the track \"Down for the Count.\"\nIn 2001, Lewis had a role in the film \"Ocean's Eleven\" in which he \"boxed\" against Wladimir Klitschko.\nIn 2002, Lewis was reportedly offered \u00a35m by World Wrestling Entertainment (WWE) chairman Vince McMahon to take up professional wrestling in his industry. His camp held discussions over a possible match with Brock Lesnar in February 2003, at the No Way Out pay-per-view event. Prior to the offer Lewis was familiar with wrestling; he was part of the famous match held in the old Wembley Stadium between The British Bulldog and Bret \"The Hitman\" Hart for the Intercontinental Championship at SummerSlam in 1992, representing the Bulldog during his entrance while bearing a Union Flag.\nIn 2002, Lewis played himself on an episode of \"The Jersey\" called \"It's a Mad Mad Mad Mad Jersey\".\nIn 2003, Lewis made a brief cameo appearance in the Jennifer Lopez and LL Cool J video \"All I Have\".\nIn 2006, he appeared in the movie \"Johnny Was\" with Vinnie Jones.\nLewis played in the World Series of Poker in both 2006 and 2007, and was knocked out without winning any money.\nLewis appeared on NBC's \"Celebrity Apprentice\" in 2008. He came in fourth place (out of 14).\nLewis made a public service announcement against domestic violence for Do Something.\nIn 2011, he was awarded an honorary Doctorate from Wilfrid Laurier University in Waterloo, Ontario. He also has his own charitable foundation called the Lennox Lewis foundation which helps disadvantaged children in Canada, Jamaica, the United Kingdom, and the United States.\nLewis is a supporter of his home town football club, West Ham United.\nOn 24 May 2018, Lewis was part of an Oval Office ceremony to announce the pardon of boxer Jack Johnson.\nIn 2024, Lewis joined fellow London boxers \u2013 Frank Bruno, Nigel Benn, and Chris Eubank \u2013 for a mini documentary, \"Four Kings\".\nPersonal life.\nLewis is a Christian. He is an avid amateur chess player, and funded an after-school chess programme for disadvantaged youths, one of whom earned a university chess scholarship at Tennessee Tech. He holds both British and Canadian citizenship. He married Violet Chang in 2005 after his retirement from boxing and has four children.\nAmateur bouts and tournaments.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55967", "revid": "35586814", "url": "https://en.wikipedia.org/wiki?curid=55967", "title": "Communication disorder", "text": "Any disorder affecting the ability to comprehend or use language and speech\nMedical condition&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nA communication disorder is any disorder that affects an individual's ability to comprehend, detect, or apply language and speech to engage in dialogue effectively with others. This also encompasses deficiencies in verbal and non-verbal communication styles. The delays and disorders can range from simple sound substitution to the inability to understand or use one's native language. This article covers subjects such as diagnosis, the DSM-IV, the DSM-V, and examples like sensory impairments, aphasia, learning disabilities, and speech disorders.\nDiagnosis.\nDisorders and tendencies included and excluded under the category of communication disorders may vary by source. For example, the definitions offered by the American Speech\u2013Language\u2013Hearing Association differ from those of the Diagnostic Statistical Manual 4th edition (DSM-IV).\nGleason (2001) defines a communication disorder as a speech and language disorder which refers to problems in communication and in related areas such as oral motor function. The delays and disorders can range from simple sound substitution to the inability to understand or use one's native language. In general, communication disorders commonly refer to problems in speech (comprehension and/or expression) that significantly interfere with an individual's achievement and/or quality of life. Knowing the operational definition of the agency performing an assessment or giving a diagnosis may help.\nPersons who speak more than one language or are considered to have an accent in their location of residence do not have a speech disorder if they are speaking in a manner consistent with their home environment or that is a blending of their home and foreign environment.\nOther conditions, as specified in the Cincinnati Children's Health Library (2019), that may increase the risk of developing a communication disorder include:\nDSM-IV.\nAccording to the DSM-IV-TR (no longer used), communication disorders were usually first diagnosed in childhood or adolescence, though they are not limited as childhood disorders and may persist into adulthood. They may also occur with other disorders.\nDiagnosis involved testing and evaluation during which it is determined if the scores/performance are \"substantially below\" developmental expectations and if they \"significantly\" interfere with academic achievement, social interactions, and daily living. This assessment might have also determined if the characteristic is deviant or delayed. Therefore, it may have been possible for an individual to have communication challenges but not meet the criteria of being \"substantially below\" criteria of the DSM IV-TR.\nThe DSM diagnoses did not comprise a complete list of all communication disorders, for example, auditory processing disorder is not classified under the DSM or ICD-10.\nThe following diagnoses were included as communication disorders:\nDSM-5.\nThe DSM-5 diagnoses for communication disorders completely rework the ones stated above. The diagnoses are made more general in order to capture the various aspects of communications disorders in a way that emphasizes their childhood onset and differentiate these communications disorders from those associated with other disorders (e.g. autism spectrum disorders).\nExamples.\nExamples of disorders that may include or create challenges in language and communication and/or may co-occur with the above disorders:\nAphasia.\nAphasia is loss of the ability to produce or comprehend language. There are acute aphasias which result from stroke or brain injury, and primary progressive aphasias caused by progressive illnesses such as dementia.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55968", "revid": "4626", "url": "https://en.wikipedia.org/wiki?curid=55968", "title": "Analog to digital convertor", "text": ""}
{"id": "55970", "revid": "4626", "url": "https://en.wikipedia.org/wiki?curid=55970", "title": "Analog to Digital Convertors", "text": ""}
{"id": "55971", "revid": "49971062", "url": "https://en.wikipedia.org/wiki?curid=55971", "title": "Inner ear", "text": "Innermost part of the vertebrate ear\nThe inner ear (internal ear, auris interna) is the innermost part of the vertebrate ear. In vertebrates, the inner ear is mainly responsible for sound detection and balance. In mammals, it consists of the bony labyrinth, a hollow cavity in the temporal bone of the skull with a system of passages comprising two main functional parts:\nThe inner ear is found in all vertebrates, with substantial variations in form and function. The inner ear is innervated by the eighth cranial nerve in all vertebrates.\nStructure.\nThe labyrinth can be divided by layer or by region.\nBony and membranous labyrinths.\nThe bony labyrinth, or osseous labyrinth, is the network of passages with bony walls lined with periosteum. The three major parts of the bony labyrinth are the vestibule of the ear, the semicircular canals, and the cochlea. The membranous labyrinth runs inside of the bony labyrinth, and creates three parallel fluid filled spaces. The two outer are filled with perilymph and the inner with endolymph.\nVestibular and cochlear systems.\nIn the middle ear, the energy of pressure waves is translated into mechanical vibrations by the three auditory ossicles. Pressure waves move the tympanic membrane which in turns moves the malleus, the first bone of the middle ear. The malleus articulates to incus which connects to the stapes. The footplate of the stapes connects to the oval window, the beginning of the inner ear. When the stapes presses on the oval window, it causes the perilymph, the liquid of the inner ear, to move. The middle ear thus serves to convert the energy from sound pressure waves to a force upon the perilymph of the inner ear. The oval window has only approximately 1/18 the area of the tympanic membrane and thus produces a higher pressure. The cochlea propagates these mechanical signals as waves in the fluid and membranes and then converts them to nerve impulses which are transmitted to the brain.\nThe vestibular system is the region of the inner ear where the semicircular canals converge, close to the cochlea. The vestibular system works with the visual system to keep objects in view when the head is moved. Joint and muscle receptors are also important in maintaining balance. The brain receives, interprets, and processes the information from all these systems to create the sensation of balance.\nThe vestibular system of the inner ear is responsible for the sensations of balance and motion. It uses the same kinds of fluids and detection cells (hair cells) as the cochlea uses, and sends information to the brain about the attitude, rotation, and linear motion of the head. The type of motion or attitude detected by a hair cell depends on its associated mechanical structures, such as the curved tube of a semicircular canal or the calcium carbonate crystals (otolith) of the saccule and utricle.\nDevelopment.\nThe human inner ear develops during week 4 of embryonic development from the auditory placode, a thickening of the ectoderm which gives rise to the bipolar neurons of the cochlear and vestibular ganglions. As the auditory placode invaginates towards the embryonic mesoderm, it forms the auditory vesicle or \"otocyst\".\nThe auditory vesicle will give rise to the utricular and saccular components of the membranous labyrinth. They contain the sensory hair cells and otoliths of the macula of utricle and of the saccule, respectively, which respond to linear acceleration and the force of gravity. The utricular division of the auditory vesicle also responds to angular acceleration, as well as the endolymphatic sac and duct that connect the saccule and utricle.\nBeginning in the fifth week of development, the auditory vesicle also gives rise to the cochlear duct, which contains the spiral organ of Corti and the endolymph that accumulates in the membranous labyrinth. The vestibular wall will separate the cochlear duct from the perilymphatic scala vestibuli, a cavity inside the cochlea. The basilar membrane separates the cochlear duct from the scala tympani, a cavity within the cochlear labyrinth. The lateral wall of the cochlear duct is formed by the spiral ligament and the stria vascularis, which produces the endolymph. The hair cells develop from the lateral and medial ridges of the cochlear duct, which together with the tectorial membrane make up the organ of Corti.\nMicroanatomy.\nRosenthal's canal or the spiral canal of the cochlea is a section of the bony labyrinth of the inner ear that is approximately 30\u00a0mm long and makes 2\u00be turns about the modiolus, the central axis of the cochlea that contains the spiral ganglion.\nSpecialized inner ear cell include: hair cells, pillar cells, Boettcher's cells, Claudius' cells, spiral ganglion neurons, and Deiters' cells (phalangeal cells).\nThe hair cells are the primary auditory receptor cells and they are also known as auditory sensory cells, acoustic hair cells, auditory cells or cells of Corti. The organ of Corti is lined with a single row of inner hair cells and three rows of outer hair cells. The hair cells have a hair bundle at the apical surface of the cell, which consists of an array of actin-based stereocilia. Each stereocilium inserts as a rootlet into a dense filamentous actin mesh known as the cuticular plate. Disruption of these bundles results in hearing impairments and balance defects.\nInner and outer pillar cells in the organ of Corti support hair cells. Outer pillar cells are unique because they are free standing cells which only contact adjacent cells at the bases and apices. Both types of pillar cell have thousands of cross linked microtubules and actin filaments in parallel orientation. They provide mechanical coupling between the basement membrane and the mechanoreceptors on the hair cells.\nBoettcher's cells are found in the organ of Corti where they are present only in the lower turn of the cochlea. They lie on the basilar membrane beneath Claudius' cells and are organized in rows, the number of which varies between species. The cells interdigitate with each other, and project microvilli into the intercellular space. They are supporting cells for the auditory hair cells in the organ of Corti. They are named after German pathologist Arthur B\u00f6ttcher (1831\u20131889).\nClaudius' cells are found in the organ of Corti located above rows of Boettcher's cells. Like Boettcher's cells, they are considered supporting cells for the auditory hair cells in the organ of Corti. They contain a variety of aquaporin water channels and appear to be involved in ion transport. They also play a role in sealing off endolymphatic spaces. They are named after the German anatomist Friedrich Matthias Claudius (1822\u20131869).\nDeiters' cells (phalangeal cells) are a type of neuroglial cell found in the organ of Corti and organised in one row of inner phalangeal cells and three rows of outer phalangeal cells. They are the supporting cells of the hair cell area within the cochlea. They are named after the German pathologist Otto Deiters (1834\u20131863) who described them.\nHensen's cells are high columnar cells that are directly adjacent to the third row of Deiters' cells.\nHensen's stripe is the section of the tectorial membrane above the inner hair cell.\nNuel's spaces refer to the fluid-filled spaces between the outer pillar cells and adjacent hair cells and also the spaces between the outer hair cells.\nHardesty's membrane is the layer of the tectoria closest to the reticular lamina and overlying the outer hair cell region.\nReissner's membrane is composed of two cell layers and separates the scala media from the scala vestibuli.\nHuschke's teeth are the tooth-shaped ridges on the spiral limbus that are in contact with the tectoria and separated by interdental cells.\nBlood supply.\nThe bony labyrinth receives its blood supply from three arteries:\n1 \u2013 Anterior tympanic branch (from maxillary artery).\n2 \u2013 Petrosal branch (from middle meningeal artery).\n3 \u2013 Stylomastoid branch (from posterior auricular artery).\nThe membranous labyrinth is supplied by the labyrinthine artery.\nVenous drainage of the inner ear is through the labyrinthine vein, which empties into the sigmoid sinus or inferior petrosal sinus.\nFunction.\nNeurons within the ear respond to simple tones, and the brain serves to process other increasingly complex sounds. An average adult is typically able to detect sounds ranging between 20 and 20,000\u00a0Hz. The ability to detect higher pitch sounds decreases in older humans.\nThe human ear has evolved with two basic tools to encode sound waves; each is separate in detecting high and low-frequency sounds. Georg von B\u00e9k\u00e9sy (1899\u20131972) employed the use of a microscope in order to examine the basilar membrane located within the inner-ear of cadavers. He found that movement of the basilar membrane resembles that of a traveling wave; the shape of which varies based on the frequency of the pitch. In low-frequency sounds, the tip (apex) of the membrane moves the most, while in high-frequency sounds, the base of the membrane moves most.\nDisorders.\nInterference with or infection of the labyrinth can result in a syndrome of ailments called labyrinthitis. The symptoms of labyrinthitis include temporary nausea, disorientation, vertigo, and dizziness. Labyrinthitis can be caused by viral infections, bacterial infections, or physical blockage of the inner ear.\nAnother condition has come to be known as autoimmune inner ear disease (AIED). It is characterized by idiopathic, rapidly progressive, bilateral sensorineural hearing loss. It is a fairly rare disorder while at the same time, a lack of proper diagnostic testing has meant that its precise incidence cannot be determined.\nOther animals.\nBirds have an auditory system similar to that of mammals, including a cochlea. Reptiles, amphibians, and fish do not have cochleas but hear with simpler auditory organs or vestibular organs, which generally detect lower-frequency sounds than the cochlea. The cochlea of birds is also similar to that of crocodiles, consisting of a short, slightly curved bony tube within which lies the basilar membrane with its sensory structures.\nCochlear system.\nIn reptiles, sound is transmitted to the inner ear by the stapes (stirrup) bone of the middle ear. This is pressed against the oval window, a membrane-covered opening on the surface of the vestibule. From here, sound waves are conducted through a short perilymphatic duct to a second opening, the round window, which equalizes pressure, allowing the incompressible fluid to move freely. Running parallel with the perilymphatic duct is a separate blind-ending duct, the lagena, filled with endolymph. The lagena is separated from the perilymphatic duct by a basilar membrane, and contains the sensory hair cells that finally translate the vibrations in the fluid into nerve signals. It is attached at one end to the saccule.\nIn most reptiles the perilymphatic duct and lagena are relatively short, and the sensory cells are confined to a small basilar papilla lying between them. However, in mammals, birds, and crocodilians, these structures become much larger and somewhat more complicated. In birds, crocodilians, and monotremes, the ducts are simply extended, together forming an elongated, more or less straight, tube. The endolymphatic duct is wrapped in a simple loop around the lagena, with the basilar membrane lying along one side. The first half of the duct is now referred to as the scala vestibuli, while the second half, which includes the basilar membrane, is called the scala tympani. As a result of this increase in length, the basilar membrane and papilla are both extended, with the latter developing into the organ of Corti, while the lagena is now called the cochlear duct. All of these structures together constitute the cochlea.\nIn therian mammals, the lagena is extended still further, becoming a coiled structure (cochlea) in order to accommodate its length within the head. The organ of Corti also has a more complex structure in mammals than it does in other amniotes.\nThe arrangement of the inner ear in living amphibians is, in most respects, similar to that of reptiles. However, they often lack a basilar papilla, having instead an entirely separate set of sensory cells at the upper edge of the saccule, referred to as the papilla amphibiorum, which appear to have the same function.\nAlthough many fish are capable of hearing, the lagena is, at best, a short diverticulum of the saccule, and appears to have no role in sensation of sound. Various clusters of hair cells within the inner ear may instead be responsible; for example, bony fish contain a sensory cluster called the macula neglecta in the utricle that may have this function. Although fish have neither an outer nor a middle ear, sound may still be transmitted to the inner ear through the bones of the skull, or by the swim bladder, parts of which often lie close by in the body.\nVestibular system.\nBy comparison with the cochlear system, the vestibular system varies relatively little between the various groups of jawed vertebrates. The central part of the system consists of two chambers, the saccule and utricle, each of which includes one or two small clusters of sensory hair cells. All jawed vertebrates also possess three semicircular canals arising from the utricle, each with an ampulla containing sensory cells at one end.\nAn endolymphatic duct runs from the saccule up through the head and ending close to the brain. In cartilaginous fish, this duct actually opens onto the top of the head, and in some teleosts, it is simply blind-ending. In all other species, however, it ends in an endolymphatic sac. In many reptiles, fish, and amphibians this sac may reach considerable size. In amphibians the sacs from either side may fuse into a single structure, which often extends down the length of the body, parallel with the spinal canal.\nThe primitive lampreys and hagfish, however, have a simpler system. The inner ear in these species consists of a single vestibular chamber, although in lampreys, this is associated with a series of sacs lined by cilia. Lampreys have only two semicircular canals, with the horizontal canal being absent, while hagfish have only a single, vertical, canal.\nEquilibrium.\nThe inner ear is primarily responsible for balance, equilibrium and orientation in three-dimensional space. The inner ear can detect both static and dynamic equilibrium. Three semicircular ducts and two chambers, which contain the saccule and utricle, enable the body to detect any deviation from equilibrium. The macula sacculi detects vertical acceleration while the macula utriculi is responsible for horizontal acceleration. These microscopic structures possess stereocilia and one kinocilium which are located within the gelatinous otolithic membrane. The membrane is further weighted with otoliths. Movement of the stereocilia and kinocilium enable the hair cells of the saccula and utricle to detect motion. The semicircular ducts are responsible for detecting rotational movement.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55972", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=55972", "title": "Middle ear", "text": "Portion of the ear\nThe middle ear is the portion of the ear medial to the eardrum, and distal to the oval window of the cochlea (of the inner ear). \nThe mammalian middle ear contains three ossicles (malleus, incus, and stapes), which transfer the vibrations of the eardrum into waves in the fluid and membranes of the inner ear. The hollow space of the middle ear is also known as the tympanic cavity and is surrounded by the tympanic part of the temporal bone. The auditory tube (also known as the Eustachian tube or the pharyngotympanic tube) joins the tympanic cavity with the nasal cavity (nasopharynx), allowing pressure to equalize between the middle ear and throat.\nThe primary function of the middle ear is to efficiently transfer acoustic energy from compression waves in air to fluid\u2013membrane waves within the cochlea.\nStructure.\nOssicles.\nThe middle ear contains three tiny bones known as the ossicles: \"malleus\", \"incus\", and \"stapes\". The ossicles were given their Latin names for their distinctive shapes; they are also referred to as the \"hammer\", \"anvil\", and \"stirrup\", respectively. The ossicles directly couple sound energy from the eardrum to the oval window of the cochlea. While the stapes is present in all tetrapods, the malleus and incus evolved from lower and upper jaw bones present in reptiles.\nThe ossicles are classically supposed to mechanically convert the vibrations of the eardrum into amplified pressure waves in the fluid of the cochlea (or inner ear), with a lever arm factor of 1.3. Since the effective vibratory area of the eardrum is about 14 fold larger than that of the oval window, the sound pressure is concentrated, leading to a pressure gain of at least 18.1. The eardrum is merged to the malleus, which connects to the incus, which in turn connects to the stapes. Vibrations of the stapes footplate introduce pressure waves in the inner ear. There is a steadily increasing body of evidence that shows that the lever arm ratio is actually variable, depending on frequency. Between 0.1 and 1\u00a0kHz it is approximately 2, it then rises to around 5 at 2\u00a0kHz and then falls off steadily above this frequency. The measurement of this lever arm ratio is also somewhat complicated by the fact that the ratio is generally given in relation to the tip of the malleus (also known as the umbo) and the level of the middle of the stapes. The eardrum is actually attached to the malleus handle over about a 0.5\u00a0cm distance. In addition, the eardrum itself moves in a very chaotic fashion at frequencies &gt;3\u00a0kHz. The linear attachment of the eardrum to the malleus actually smooths out this chaotic motion and allows the ear to respond linearly over a wider frequency range than a point attachment. The auditory ossicles can also reduce sound pressure (the inner ear is very sensitive to overstimulation), by uncoupling each other through particular muscles.\nThe middle ear efficiency peaks at a frequency of around 1\u00a0kHz. The combined transfer function of the outer ear and middle ear gives humans a peak sensitivity to frequencies between 1\u00a0kHz and 3\u00a0kHz.\nMuscles.\nThe movement of the ossicles may be stiffened by two muscles. The stapedius muscle, the smallest skeletal muscle in the body, connects to the stapes and is controlled by the facial nerve; the tensor tympani muscle is attached to the upper end of the medial surface of the handle of malleus and is under the control of the medial pterygoid nerve which is a branch of the mandibular nerve of the trigeminal nerve. These muscles contract in response to loud sounds, thereby reducing the transmission of sound to the inner ear. This is called the acoustic reflex.\nNerves.\nOf surgical importance are two branches of the facial nerve that also pass through the middle ear space. These are the horizontal portion of the facial nerve and the chorda tympani. Damage to the horizontal branch during ear surgery can lead to paralysis of the face (same side of the face as the ear). The chorda tympani is the branch of the facial nerve that carries taste from the ipsilateral half (same side) of the tongue.\nFunction.\nSound transfer.\nOrdinarily, when sound waves in air strike liquid, most of the energy is reflected off the surface of the liquid. The middle ear allows the impedance matching of sound traveling in air to acoustic waves traveling in a system of fluids and membranes in the inner ear. This system should not be confused, however, with the propagation of sound as compression waves in liquid.\nThe acoustic impedance of air is about formula_1, while the impedance of cochlear fluids (formula_2) is approximately equal to that of sea water. Because of this high impedance, only formula_3 of incident energy could be directly transmitted from the air to cochlear fluids. \nThe middle ear's impedance matching mechanism increases the efficiency of sound transmission. Two processes are involved: \nTogether, they amplify pressure by 26 times, or about 30 dB. The actual value is around 20 dB across 200 to 10000 Hz.\nThe middle ear couples sound from air to the fluid via the oval window, using the principle of \"mechanical advantage\" in the form of the \"hydraulic principle\" and the \"lever principle\". The vibratory portion of the tympanic membrane (eardrum) is many times the surface area of the footplate of the stapes (the third ossicular bone which attaches to the oval window); furthermore, the shape of the articulated ossicular chain is a complex lever, the long arm being the long process of the malleus, the fulcrum being the body of the incus, and the short arm being the lenticular process of the incus. The collected pressure of sound vibration that strikes the tympanic membrane is therefore concentrated down to this much smaller area of the footplate, increasing the force but reducing the velocity and displacement, and thereby coupling the acoustic energy.\nThe middle ear is able to dampen sound conduction substantially when faced with very loud sound, by noise-induced reflex contraction of the middle-ear muscles.\nClinical significance.\nThe middle ear is hollow in the tympanic cavity and Eustachian tube. In a high-altitude environment or on diving into water, there will be a pressure difference between the middle ear and the outside environment. This pressure will pose a risk of bursting or otherwise damaging the tympanum (eardrum) if it is not relieved. If middle ear pressure remains low, the eardrum (tympanic membrane) may become retracted into the middle ear. One of the functions of the Eustachian tubes that connect the middle ear to the nasopharynx is to help keep middle ear pressure the same as air pressure. The Eustachian tubes are normally pinched off at the nose end, to prevent being clogged with mucus, but they may be opened by lowering and protruding the jaw; this is why yawning or chewing helps relieve the pressure felt in the ears when on board an aircraft. Eustachian tube obstruction may result in fluid build up in the middle ear, which causes a conductive hearing loss. Otitis media is an inflammation of the middle ear.\nInjuries.\nThe middle ear is well protected from most minor external injuries by its internal location, but is vulnerable to pressure injury (barotrauma).\nInfections.\nRecent findings indicate that the middle ear mucosa could be subjected to human papillomavirus infection. Indeed, DNAs belonging to oncogenic HPVs, i.e., HPV16 and HPV18, have been detected in normal middle ear specimens, thereby indicating that the normal middle ear mucosa could potentially be a target tissue for HPV infection.\nDiversity and evolution.\nThe middle ear of tetrapods is analogous with the spiracle of fishes, an opening from the pharynx to the side of the head in front of the main gill slits. In fish embryos, the spiracle forms as a pouch in the pharynx, which grows outward and breaches the skin to form an opening; in most tetrapods, this breach is never quite completed, and the final vestige of tissue separating it from the outside world becomes the eardrum. The inner part of the spiracle, still connected to the pharynx, forms the eustachian tube.\nIn reptiles, birds, and early fossil tetrapods, there is a single auditory ossicle, the columella which is homologous with the stapes, or \"stirrup\" of mammals. This is connected indirectly with the eardrum via a mostly cartilaginous extracolumella and medially to the inner-ear spaces via a widened footplate in the fenestra ovalis. The columella is an evolutionary derivative of the bone known as the hyomandibula in fish ancestors, a bone that supported the skull and braincase.\nAmphibians.\nThe structure of the middle ear in living amphibians varies considerably and is often degenerate. In most frogs and toads, it is similar to that of reptiles, but in other amphibians, the middle ear cavity is often absent. In these cases, the stapes either is also missing or, in the absence of an eardrum, connects to the quadrate bone in the skull, although, it is presumed, it still has some ability to transmit vibrations to the inner ear. In many amphibians, there is also a second auditory ossicle, the \"operculum\" (not to be confused with the structure of the same name in fishes). This is a flat, plate-like bone, overlying the fenestra ovalis, and connecting it either to the stapes or, via a special muscle, to the scapula. It is not found in any other vertebrates.\nMammals.\nMammals are unique in having evolved a three-ossicle middle-ear independently of the various single-ossicle middle ears of other land vertebrates, all during the Triassic period of geological history. Functionally, the mammalian middle ear is very similar to the single-ossicle ear of non-mammals, except that it responds to sounds of higher frequency, because these are better taken up by the inner ear (which also responds to higher frequencies than those of non-mammals). The malleus, or \"hammer\", evolved from the articular bone of the lower jaw, and the incus, or \"anvil\", from the quadrate. In other vertebrates, these bones form the primary jaw joint, but the expansion of the dentary bone in mammals led to the evolution of an entirely new jaw joint, freeing up the old joint to become part of the ear. For a period of time, both jaw joints existed together, one medially and one laterally. The evolutionary process leading to a three-ossicle middle ear was thus an \"accidental\" byproduct of the simultaneous evolution of the new, secondary jaw joint. In many mammals, the middle ear also becomes protected within a cavity, the auditory bulla, not found in other vertebrates. A bulla evolved late in time and independently numerous times in different mammalian clades, and it can be surrounded by membranes, cartilage or bone. The bulla in humans is part of the temporal bone.\nRecently found fossils such as Morganucodon show intermediary steps of middle ear evolution. A new morganucodontan-like species, \"Dianoconodon youngi\", shows parts of the mandible (= dentary) that permit an auditory function, although these bones are still attached to the mandible.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55973", "revid": "50389618", "url": "https://en.wikipedia.org/wiki?curid=55973", "title": "Outer ear", "text": "Outer part of the ear\nThe outer ear, external ear, or auris externa is the external part of the ear, which consists of the auricle (also pinna) and the ear canal. It gathers sound energy and focuses it on the eardrum (tympanic membrane).\nStructure.\nAuricle.\nThe visible part is called the auricle, also known as the pinna, especially in other animals. It is composed of a thin plate of yellow elastic cartilage, covered with integument, and connected to the surrounding parts by ligaments and muscles; and to the commencement of the ear canal by fibrous tissue. Many mammals can move the pinna (with the auriculares muscles) in order to focus their hearing in a certain direction in much the same way that they can turn their eyes. Most humans do not have this ability.\nEar canal.\nFrom the pinna, the sound waves move into the ear canal (also known as the \"external acoustic meatus\") a simple tube running through to the middle ear. This tube leads inward from the bottom of the auricula and conducts the vibrations to the tympanic cavity and amplifies frequencies in the range 2\u00a0kHz to 5\u00a0kHz.\nAuricular muscles.\nIntrinsic muscles.\nThe intrinsic auricular muscles are:\nThe intrinsic muscles contribute to the topography of the auricle, while also function as a sphincter of the external auditory meatus. It has been suggested that during prenatal development in the womb, these muscles exert forces on the cartilage which in turn affects the shaping of the ear.\nExtrinsic muscles.\nThe extrinsic auricular muscles are the three muscles surrounding the \"auricula\" or outer ear:\nThe superior muscle is the largest of the three, followed by the posterior and the anterior.\nIn some mammals these muscles can adjust the direction of the pinna. In humans these muscles possess very little action.\nThe auricularis anterior draws the auricula forward and upward, the auricularis superior slightly raises it, and the auricularis posterior draws it backward. The superior auricular muscle also acts as a stabilizer of the occipitofrontalis muscle and as a weak brow lifter. The presence of auriculomotor activity in the posterior auricular muscle causes the muscle to contract and cause the pinna to be pulled backwards and flatten when exposed to sudden, surprising sounds.\nFunction.\nOne consequence of the configuration of the outer ear is selectively to boost the sound pressure 30- to 100-fold for frequencies around 3\u00a0kHz. This amplification makes humans most sensitive to frequencies in this range\u2014and also explains why they are particularly prone to acoustical injury and hearing loss near this frequency. Most human speech sounds are also distributed in the bandwidth around 3\u00a0kHz.\nAn important function of the pinna and concha is to selectively filter different sound frequencies in order to provide cues about the elevation of the sound source. The convolutions of the pinna are shaped so that the outer ear transmits more high-frequency components from an elevated source than from the same source at ear level.\nClinical significance.\nMalformations of the external ear can be a consequence of hereditary disease, or exposure to environmental factors such as radiation, infection. Such defects include:\nSurgery.\nUsually, malformations are treated with surgery, although artificial prostheses are also sometimes used.\nIf malformations are accompanied by hearing loss amenable to correction, then the early use of hearing aids may prevent complete hearing loss.\nEvolution.\nThe outer ear's cartilage is homologous to the cartilage in gills of amphibians, fishes, and invertebrates such as the horseshoe crab. The extracolumella cartilage of reptiles is likely also homologous.\nReferences.\n \"This article incorporates text in the public domain from the 20th edition of\" Gray's Anatomy \"(1918)\"\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "55974", "revid": "50849756", "url": "https://en.wikipedia.org/wiki?curid=55974", "title": "Federal Communications Commission", "text": "Independent U.S. government agency\nThe Federal Communications Commission (FCC) is an independent agency of the United States federal government that regulates communications by radio, television, wire, internet, Wi-Fi, satellite, and cable across the United States. The FCC maintains jurisdiction over the areas of broadband access, fair competition, radio frequency use, media responsibility, public safety, and homeland security.\nThe FCC was established pursuant to the Communications Act of 1934 to replace the radio regulation functions of the previous Federal Radio Commission. The FCC took over wire communication regulation from the Interstate Commerce Commission. The FCC's mandated jurisdiction covers the 50 states, the District of Columbia, and the territories of the United States. The FCC also provides varied degrees of cooperation, oversight, and leadership for similar communications bodies in other countries in North America. The FCC is funded entirely by regulatory fees. It has an estimated fiscal-2022 budget of $388 million. It employs 1,433 federal personnel \nMission and agency objectives.\nAs specified in Section 1 of the Communications Act of 1934 and amended by the Telecommunications Act of 1996 (amendment to 47 U.S.C. \u00a7151), the mandate of the FCC is, \"to make available so far as possible, to all the people of the United States, without discrimination on the basis of race, color, religion, national origin, or sex, rapid, efficient, nationwide, and world-wide wire and radio communication services with adequate facilities at reasonable charges.\"\nFurthermore, the Act provides that the FCC was created, \"for the purpose of the national defense,\" and, \"for the purpose of promoting safety of life and property through the use of wire and radio communications.\"\nOrganization and procedures.\nCommissioners.\nThe FCC is directed by five commissioners appointed by the president of the United States and confirmed by the United States Senate for five-year terms, except when filling an unexpired term. The U.S. president designates one of the commissioners to serve as chairman. No more than three commissioners may be members of the same political party. None of them may have a financial interest in any FCC-related business.\nAfter their terms expire, commissioners may continue serving until the appointment of their replacements. However, they may not serve beyond the end of the \"next\" session of Congress following term expiration. In practice, this means that commissioners may serve up to &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1+1\u20442 years beyond the official term expiration listed above if no replacement is appointed. This would end on the date that Congress adjourns its annual session, generally no later than noon on January\u00a03.\nBureaus.\nThe FCC is organized into seven bureaus, each headed by a \"chief\" that is appointed by the chair of the commission. Bureaus process applications for licenses and other filings, analyze complaints, conduct investigations, develop and implement regulations, and participate in hearings.\nOffices.\nThe FCC has twelve staff offices.\nThe FCC's offices provide support services to the bureaus.\nHeadquarters.\nThe FCC leases space in the Sentinel Square III building in northeast Washington, D.C.\nPrior to moving to its new headquarters in October 2020, the FCC leased space in the Portals building in southwest Washington, D.C. Construction of the Portals building was scheduled to begin on March 1, 1996. In January 1996, the General Services Administration signed a lease with the building's owners, agreeing to let the FCC lease of space in Portals for 20 years, at a cost of $17.3\u00a0million per year in 1996 dollars. Prior to the Portals, the FCC had space in six buildings at and around 19th Street NW and M Street NW. The FCC first solicited bids for a new headquarters complex in 1989. In 1991 the GSA selected the Portals site. The FCC had wanted to move into a more expensive area along Pennsylvania Avenue.\nHistory.\nCommunications Act of 1934.\nOn February 26, 1934, President Franklin Roosevelt recommended the creation of the Federal Communications Commission.\nIn 1934, Congress passed the Communications Act, which abolished the Federal Radio Commission and transferred jurisdiction over radio licensing to a new Federal Communications Commission, including in it also the telecommunications jurisdiction previously handled by the Interstate Commerce Commission.\nTitle II of the Communications Act focused on telecommunications using many concepts borrowed from railroad legislation and Title III contained provisions very similar to the Radio Act of 1927.\nThe initial organization of the FCC was effected July 17, 1934, in three divisions, Broadcasting, Telegraph, and Telephone. Each division was led by two of the seven commissioners, with the FCC chairman being a member of each division. The organizing meeting directed the divisions to meet on July 18, July 19, and July 20, respectively.\nReport on Chain Broadcasting.\nIn 1941, the Federal Communications Commission issued the \"Report on Chain Broadcasting\" which was led by new FCC chairman James Lawrence Fly (and Telford Taylor as general counsel). The major point in the report was the breakup of the National Broadcasting Company (NBC), which ultimately led to the creation of the American Broadcasting Company (ABC), but there were two other important points. One was network option time, the culprit here being the Columbia Broadcasting System (CBS). The report limited the amount of time during the day and at what times the networks may broadcast. Previously a network could demand any time it wanted from a Network affiliate. The second concerned artist bureaus. The networks served as both agents and employers of artists, which was a conflict of interest the report rectified.\nFreeze of 1948.\nIn assigning television stations to various cities after World War II, the FCC found that it placed many stations too close to each other, resulting in interference. At the same time, it became clear that the designated VHF channels, 2 through 13, were inadequate for nationwide television service. As a result, the FCC stopped giving out construction permits for new licenses in October 1948, under the direction of Chairman Rosel H. Hyde. Most expected this \"Freeze\" to last six months, but as the allocation of channels to the emerging UHF technology and the eagerly awaited possibilities of color television were debated, the FCC's re-allocation map of stations did not come until April 1952, with July 1, 1952, as the official beginning of licensing new stations.\nOther FCC actions hurt the fledgling DuMont and ABC networks. American Telephone and Telegraph (AT&amp;T) forced television coaxial cable users to rent additional radio long lines, discriminating against DuMont, which had no radio network operation. DuMont and ABC protested AT&amp;T's television policies to the FCC, which regulated AT&amp;T's long-line charges, but the commission took no action. The result was that financially marginal DuMont was spending as much in long-line charge as CBS or NBC while using only about 10 to 15 percent of the time and mileage of either larger network.\nThe FCC's Sixth Report and Order ended the Freeze in 1952. It took five years for the US to grow from 108 stations to more than 550. New stations came online slowly, only five by the end of November 1952. The Sixth Report and Order required some existing television stations to change channels, but only a few existing VHF stations were required to move to UHF, and a handful of VHF channels were deleted altogether in smaller media markets like Peoria, Fresno, Bakersfield and Fort Wayne, Indiana to create markets which were UHF \"islands\". The report also set aside a number of channels for the newly emerging field of educational television, which hindered struggling ABC and DuMont's quest for affiliates in the more desirable markets where VHF channels were reserved for non-commercial use.\nThe Sixth Report and Order also provided for the \"intermixture\" of VHF and UHF channels in most markets; UHF transmitters in the 1950s were not yet powerful enough, nor receivers sensitive enough (if they included UHF tuners at all; they were not formally required until the 1960s All-Channel Receiver Act), to make UHF viable against entrenched VHF stations. In markets where there were no VHF stations and UHF was the only TV service available, UHF survived. In other markets, which were too small to financially support a television station, too close to VHF outlets in nearby cities, or where UHF was forced to compete with more than one well-established VHF station, UHF had little chance for success.\nDenver had been the largest U.S. city without a TV station by 1952. Senator Edwin Johnson (D-Colorado), chair of the Senate's Interstate and Foreign Commerce Committee, had made it his personal mission to make Denver the first post-Freeze station. The senator had pressured the FCC, and proved ultimately successful as the first new station (a VHF station) came on-line a remarkable ten days after the commission formally announced the first post-Freeze construction permits. KFEL (now KWGN-TV)'s first regular telecast was on July 21, 1952.\nTelecommunications Act of 1996.\nIn 1996, Congress enacted the Telecommunications Act of 1996, in the wake of the breakup of AT&amp;T resulting from the U.S. Department of Justice's antitrust suit against AT&amp;T. The legislation attempted to create more competition in local telephone service by requiring Incumbent Local Exchange Carriers to provide access to their facilities for Competitive Local Exchange Carriers. This policy has thus far had limited success and much criticism.\nThe development of the Internet, cable services and wireless services has raised questions whether new legislative initiatives are needed as to competition in what has come to be called 'broadband' services. Congress has monitored developments but as of 2009 has not undertaken a major revision of applicable regulation. The Local Community Radio Act in the 111th Congress has gotten out of committee and will go before the house floor with bi-partisan support, and unanimous support of the FCC.\nBy passing the Telecommunications Act of 1996, Congress also eliminated the cap on the number of radio stations any one entity could own nationwide and also substantially loosened local radio station ownership restrictions. Substantial radio consolidation followed. Restrictions on ownership of television stations were also loosened. Public comments to the FCC indicated that the public largely believed that the severe consolidation of media ownership had resulted in harm to diversity, localism, and competition in media, and was harmful to the public interest.\nModernization of the FCC's information technology systems.\nDavid A. Bray joined the commission in 2013 as chief information officer and quickly announced goals of modernizing the FCC's legacy information technology (IT) systems, citing 200 different systems for only 1750 people a situation he found \"perplexing\". These efforts later were documented in a 2015 Harvard Case Study. In 2017, Christine Calvosa replaced Bray as the acting CIO of FCC.\n2023 reorganization and Space Bureau establishment.\nOn January 4, 2023, the FCC voted unanimously to create a newly formed Space Bureau and Office of International Affairs within the agency, replacing the existing International Bureau. FCC chairwoman Jessica Rosenworcel explained that the move was done to improve the FCC's \"coordination across the federal government\" and to \"support the 21st-century satellite industry.\" The decision to establish the Space Bureau was reportedly done to improve the agency's capacity to regulate Satellite Internet access. The new bureau officially launched on April 11, 2023.\nFCC Review of Rules on Network Mergers (2025-present).\nOn September 30, 2025, The FCC launched a new review of its media ownership limits, amid broadcasters\u2019 lobbying push to modernize the restrictions in the face of competition from tech giants. The agency voted to take public comment, including on a rule that limits a company from owning more than two stations in a market, and a restriction on mergers between any two of the four major broadcast networks.\nOn November 19, 2025, FCC Chairman Brendan Carr announced a review of the relationships between broadcast networks and their affiliates, calling into question contractural restrictions that penalize stations for pre-empting shows and those that prevent them from airing rival programming. The FCC will take comments on a host of issues, rooted in Carr\u2019s belief that networks have gained too much leverage at the expense of local stations. He wrote on X, \u201cThe FCC has an obligation to ensure that local broadcast TV stations meet their public interest obligations. Yet National Programmers operating out of New York &amp; Hollywood are reportedly preventing those broadcasters from serving their local communities\u2014including by punishing them for exercising their right to preempt national programming.\u201d\nCommissioners.\nThe commissioners of the FCC as of \u00a023, 2025[ [update]]:\nThe initial group of FCC commissioners after establishment of the commission in 1934 comprised the following seven members:\nThe complete list of commissioners is available on the FCC website. Frieda B. Hennock (D-NY) was the first female commissioner of the FCC in 1948.\nMedia policy.\nBroadcast radio and television.\nThe FCC regulates broadcast stations, repeater stations as well as commercial broadcasting operators who operate and repair certain radiotelephone, radio and television stations. Broadcast licenses are to be renewed if the station meets the \"public interest, convenience, or necessity\". The FCC's enforcement powers include fines and broadcast license revocation (see FCC MB Docket 04-232). Burden of proof would be on the complainant in a petition to deny.\nCable and satellite.\nThe FCC first promulgated rules for cable television in 1965, with cable and satellite television now regulated by the FCC under Title VI of the Communications Act. Congress added Title VI in the Cable Communications Policy Act of 1984, and made substantial modifications to Title VI in the Cable Television and Consumer Protection and Competition Act of 1992. Further modifications to promote cross-modal competition (telephone, video, etc.) were made in the Telecommunications Act of 1996, leading to the current regulatory structure.\nContent regulation and indecency.\nBroadcast television and radio stations are subject to FCC regulations including restrictions against indecency or obscenity. The Supreme Court has repeatedly held, beginning soon after the passage of the Communications Act of 1934, that the inherent scarcity of radio spectrum allows the government to impose some types of content restrictions on broadcast license holders notwithstanding the First Amendment. Cable and satellite providers are also subject to some content regulations under Title VI of the Communications Act such as the prohibition on obscenity, although the limitations are not as restrictive compared to broadcast stations.\nThe 1981 inauguration of Ronald Reagan as President of the United States accelerated an already ongoing shift in the FCC towards a decidedly more market-oriented stance. A number of regulations felt to be outdated were removed, most controversially the Fairness Doctrine in 1987.\nIn terms of indecency fines, the FCC took no action on the case FCC v. Pacifica until 1987, about ten years after the landmark United States Supreme Court decision that defined the power of the FCC over indecent material as applied to broadcasting.\nAfter the 1990s had passed, the FCC began to increase its censorship and enforcement of indecency regulations in the early 2000s to include a response to the Janet Jackson \"wardrobe malfunction\" that occurred during the halftime show of Super Bowl XXXVIII.\nThen on June 15, 2006, President George W. Bush signed into law the Broadcast Decency Enforcement Act of 2005 sponsored by then-Senator Sam Brownback, a former broadcaster himself, and endorsed by Congressman Fred Upton of Michigan who authored a similar bill in the United States House of Representatives. The new law stiffened the penalties for each violation of the Act. The Federal Communications Commission was empowered to fines in the amount of $325,000 for each violation by each station that violated decency standards. The legislation raised the fine ten times over the previous maximum of $32,500 per violation.\nMedia ownership.\nThe FCC has established rules limiting the national share of media ownership of broadcast radio or television stations. It has also established cross-ownership rules limiting ownership of a newspaper and broadcast station in the same market, in order to ensure a diversity of viewpoints in each market and serve the needs of each local market.\nDiversity.\nIn the second half of 2006, groups such as the National Hispanic Media Coalition, the National Latino Media Council, the National Association of Hispanic Journalists, the National Institute for Latino Policy, the League of United Latin American Citizens (LULAC) and others held town hall meetings in California, New York and Texas on media diversity as its effects Latinos and minority communities. They documented widespread and deeply felt community concerns about the negative effects of media concentration and consolidation on racial-ethnic diversity in staffing and programming. At these Latino town hall meetings, the issue of the FCC's lax monitoring of obscene and pornographic material in Spanish-language radio and the lack of racial and national-origin diversity among Latino staff in Spanish-language television were other major themes.\nPresident Barack Obama appointed Mark Lloyd to the FCC in the newly created post of associate general counsel/chief diversity officer.\nLocalism.\nNumerous controversies have surrounded the city of license concept as the internet has made it possible to broadcast a single signal to every owned station in the nation at once, particularly when Clear Channel, now IHeartMedia, became the largest FM broadcasting corporation in the US after the Telecommunications Act of 1996 became law - owning over 1,200 stations at its peak. As part of its license to buy more radio stations, Clear Channel was forced to divest all TV stations.\nDigital television transition.\nTo facilitate the adoption of digital television, the FCC issued a second digital TV (DTV) channel to each holder of an analog TV station license. All stations were required to buy and install all new equipment (transmitters, TV antennas, and even entirely new broadcast towers), and operate for years on both channels. Each licensee was required to return one of their two channels following the end of the digital television transition.\nAfter delaying the original deadlines of 2006, 2008, and eventually February 17, 2009, on concerns about elderly and rural folk, on June 12, 2009, all full-power analog terrestrial TV licenses in the U.S. were terminated as part of the DTV transition, leaving terrestrial television available only from digital channels and a few low-power LPTV stations. To help U.S. consumers through the conversion, Congress established a federally sponsored DTV Converter Box Coupon Program for two free converters per household.\nWireline policy.\nThe FCC regulates telecommunications services under Title II of the Communications Act of 1934. Title II imposes common carrier regulation under which carriers offering their services to the general public must provide services to all customers and may not discriminate based on the identity of the customer or the content of the communication. This is similar to and adapted from the regulation of transportation providers (railroad, airline, shipping, etc.) and some public utilities. Wireless carriers providing telecommunications services are also generally subject to Title II regulation except as exempted by the FCC.\nTelephone.\nThe FCC regulates interstate telephone services under Title II. The Telecommunications Act of 1996 was the first major legislative reform since the 1934 act and took several steps to de-regulate the telephone market and promote competition in both the local and long-distance marketplace.\nFrom monopoly to competition.\nThe important relationship of the FCC and the American Telephone and Telegraph (AT&amp;T) Company evolved over the decades. For many years, the FCC and state officials agreed to regulate the telephone system as a natural monopoly. The FCC controlled telephone rates and imposed other restrictions under Title II to limit the profits of AT&amp;T and ensure nondiscriminatory pricing.\nIn the 1960s, the FCC began allowing other long-distance companies, namely MCI, to offer specialized services. In the 1970s, the FCC allowed other companies to expand offerings to the public. A lawsuit in 1982 led by the Justice Department after AT&amp;T underpriced other companies, resulted in the breakup of the Bell System from AT&amp;T. Beginning in 1984, the FCC implemented a new goal that all long-distance companies had equal access to the local phone companies' customers. Effective January 1, 1984, the Bell System's many member-companies were variously merged into seven independent \"Regional Holding Companies\", also known as Regional Bell Operating Companies (RBOCs), or \"Baby Bells\". This divestiture reduced the book value of AT&amp;T by approximately 70%.\nInternet.\nThe FCC initially exempted \"information services\" such as broadband Internet access from regulation under Title II. The FCC held that information services were distinct from telecommunications services that are subject to common carrier regulation.\nHowever, Section 706 of the Telecommunications Act of 1996 required the FCC to help accelerate deployment of \"advanced telecommunications capability\" which included high-quality voice, data, graphics, and video, and to regularly assess its availability. In August 2015, the FCC said that nearly 55\u00a0million Americans did not have access to broadband capable of delivering high-quality voice, data, graphics and video offerings.\nOn February 26, 2015, the FCC reclassified broadband Internet access as a telecommunications service, thus subjecting it to Title II regulation, although several exemptions were also created. The reclassification was done in order to give the FCC a legal basis for imposing net neutrality rules (see below), after earlier attempts to impose such rules on an \"information service\" had been overturned in court.\nNet neutrality.\nIn 2005, the FCC formally established the following principles: To encourage broadband deployment and preserve and promote the open and interconnected nature of the public Internet, Consumers are entitled to access the lawful Internet content of their choice; Consumers are entitled to run applications and use services of their choice, subject to the needs of law enforcement; Consumers are entitled to connect their choice of legal devices that do not harm the network; Consumers are entitled to competition among network providers, application and service providers, and content providers. However, broadband providers were permitted to engage in \"reasonable network management\".\nOn August 1, 2008, the FCC formally voted 3-to-2 to uphold a complaint against Comcast, the largest cable company in the US, ruling that it had illegally inhibited users of its high-speed Internet service from using file-sharing software. The FCC imposed no fine, but required Comcast to end such blocking in 2008. FCC chairman Kevin J. Martin said the order was meant to set a precedent that Internet providers, and indeed all communications companies, could not prevent customers from using their networks the way they see fit unless there is a good reason. In an interview Martin stated that \"We are preserving the open character of the Internet\" and \"We are saying that network operators can't block people from getting access to any content and any applications.\" Martin's successor, Julius Genachowski has maintained that the FCC has no plans to regulate the internet, saying: \"I've been clear repeatedly that we're not going to regulate the Internet.\" The Comcast case highlighted broader issues of whether new legislation is needed to force Internet providers to maintain net neutrality, i.e. treat all uses of their networks equally. The legal complaint against Comcast related to BitTorrent, software that is commonly used for downloading larger files.\nIn December 2010, the FCC revised the principles from the original Internet policy statement and adopted the Open Internet Order consisting of three rules regarding the Internet: Transparency. Fixed and mobile broadband providers must disclose the network management practices, performance characteristics, and terms and conditions of their broadband services; No blocking. Fixed broadband providers may not block lawful content, applications, services, or non-harmful devices; mobile broadband providers may not block lawful websites, or block applications that compete with their voice or video telephony services; and No unreasonable discrimination.\nOn January 14, 2014, Verizon won its lawsuit over the FCC in the United States Court of Appeals for the District of Columbia Court. Verizon was suing over increased regulation on internet service providers on the grounds that \"even though the commission has general authority to regulate in this arena, it may not impose requirements that contravene express statutory mandates. Given that the commission has chosen to classify broadband providers in a manner that exempts them from treatment as common carriers, the Communications Act expressly prohibits the commission from nonetheless regulating them as such.\"\nAfter these setbacks in court, in April 2014 the FCC issued a Notice of Proposed Rulemaking regarding a path forward for The Open Internet Order. On November 10, 2014, President Obama created a YouTube video recommending that the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality.\nOn February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet.\nThe rules prompted debate about the applicability of First Amendment protections to Internet service providers and edge providers. Republican commissioner Ajit Pai said the Open Internet Order \"posed a special danger\" to \"First Amendment speech, freedom of expression, [and] even freedom of association.\" Democratic member and then-Chairman Tom Wheeler said in response that the rules were \"no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept.\" According to a Washington Post poll, 81% of Americans supported net neutrality in 2014, with 81% of Democrats and 85% of Republicans saying they opposed allowing Internet providers to charge websites for faster speeds.\nOn March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new \"Net Neutrality\" regulations.\nOn April 27, 2017, FCC chairman Ajit Pai released a draft Notice of Proposed Rulemaking that would revise the legal foundation for the agency's Open Internet regulations. The NPRM was voted on at the May 18th Open Meeting. On December 14, the commission voted 3\u20132 in favor of passing the repeal of the 2015 rules. The repeal formally took effect on June 11, 2018, when the 2015 rules expired.\nHowever, in April 2024, the FCC re-adopted the net neutrality rules by 3\u20132 vote, prohibiting internet service providers from blocking or limiting user access, reviving the regulations repealed in 2017. On January 2, 2025, the United States Court of Appeals for the Sixth Circuit ruled that the FCC lacked the authority to adopt net neutrality regulations. The court held that the regulations violated federal law because broadband Internet service providers are classified as information service providers, not telecommunications service providers.\nNSA wiretapping.\nWhen it emerged in 2006 that AT&amp;T, BellSouth and Verizon may have broken U.S. laws by aiding the National Security Agency in possible illegal wiretapping of its customers, Congressional representatives called for an FCC investigation into whether or not those companies broke the law. The FCC declined to investigate, however, claiming that it could not investigate due to the classified nature of the program \u2013 a move that provoked the criticism of members of Congress.\n\"Today the watchdog agency that oversees the country's telecommunications industry refused to investigate the nation's largest phone companies' reported disclosure of phone records to the NSA\", said Rep. Edward Markey (D-Mass.) in response to the decision. \"The FCC, which oversees the protection of consumer privacy under the Communications Act of 1934, has taken a pass at investigating what is estimated to be the nation's largest violation of consumer privacy ever to occur. If the oversight body that monitors our nation's communications is stepping aside then Congress must step in.\"\nWireless policy.\nThe FCC regulates all non-Federal uses of radio frequency spectrum in the United States under Title III of the Communications Act of 1934. In addition to over-the-air broadcast television and radio stations, this includes commercial mobile (i.e., mobile phone) services, amateur radio, citizen's band radio, theatrical wireless microphone installations, and a very wide variety of other services. Use of radio spectrum by U.S. federal government agencies is coordinated by the National Telecommunications and Information Administration, an agency within the Department of Commerce.\nCommercial mobile service.\nCommercial mobile radio service (CMRS) providers, including all mobile phone carriers, are subject to spectrum and wireless regulations under Title III (similar to broadcasters) as well as common carrier regulations under Title II (similar to wireline telephone carriers), except as provided by the FCC.\nSpectrum auctions.\nBeginning in 1994, the FCC has usually assigned commercial spectrum licenses through the use of competitive bidding, i.e., spectrum auctions. These auctions have raised tens of billions of dollars for the U.S. Treasury, and the FCC's auction approach is now widely emulated throughout the world. The FCC typically obtains spectrum for auction that has been reclaimed from other uses, such as spectrum returned by television broadcasters after the digital television transition, or spectrum made available by federal agencies able to shift their operations to other bands.\nUnlicensed spectrum.\nNormally, any intentional radio transmission requires an FCC license pursuant to Title III. However, in recent decades the FCC has also opened some spectrum bands for unlicensed operations, typically restricting them to low power levels conducive to short-range applications. This has facilitated the development of a very wide range of common technologies from wireless garage door openers, cordless phones, and baby monitors to Wi-Fi and Bluetooth among others. However, unlicensed devices \u2014 like most radio transmission equipment \u2014 must still receive technical approval from the FCC before being sold into the marketplace, including ensuring that such devices cannot be modified by end users to increase transmit power above FCC limits.\nWhite spaces.\n\"White spaces\" are radio frequencies that went unused after the federally mandated transformation of analog TV signals to digital. On October 15, 2008, FCC Chairman Kevin Martin announced his support for the unlicensed use of white spaces. Martin said he was \"hoping to take advantage of utilizing these airwaves for broadband services to allow for unlicensed technologies and new innovations in that space.\"\nGoogle, Microsoft and other companies are vying for the use of this white-space to support innovation in Wi-Fi technology. Broadcasters and wireless microphone manufacturers fear that the use of white space would \"disrupt their broadcasts and the signals used in sports events and concerts.\" Cell phone providers such as T-Mobile US have mounted pressure on the FCC to instead offer up the white space for sale to boost competition and market leverage.\nOn November 4, 2008, the FCC commissioners unanimously agreed to open up unused broadcast TV spectrum for unlicensed use.\nAmateur radio.\nAmateur radio operators in the United States must be licensed by the FCC before transmitting. While the FCC maintains control of the written testing standards, it no longer administers the exams, having delegated that function to private volunteer organizations. No amateur license class requires examination in Morse code; neither the FCC nor the volunteer organizations test code skills for amateur licenses.\nBroadcasting tower database.\nAn FCC database provides information about the height and year built of broadcasting towers in the US. It does not contain information about the structural types of towers or about the height of towers used by Federal agencies, such as most NDBs, LORAN-C transmission towers or VLF transmission facilities of the US Navy, or about most towers not used for transmission like the BREN Tower. These are instead tracked by the Federal Aviation Administration as obstructions to air navigation.\nCriticism for use of proprietary standards.\nIn 2023, Andrew Tisinger criticized the FCC for ignoring international open standards, and instead choosing proprietary closed standards, or allowing communications companies to do so and implement the anticompetitive practice of vendor lock-in.\nIn the case of digital TV, it chose the ATSC standard, even though DVB was already in use around the world, including DVB-S satellite TV in the U.S. Unlike competing standards, the ATSC system is encumbered by numerous patents, and therefore royalties that make TV sets and DTV converters much more expensive than in the rest of the world. Additionally, the claimed benefit of better reception in rural areas is more than negated in urban areas by multipath interference, which other systems are nearly immune to. It also cannot be received while in motion for this reason, while all other systems can, even without dedicated mobile TV signals or receivers.\nFor digital radio, the FCC chose proprietary HD Radio, which crowds the existing FM broadcast band and even AM broadcast band with in-band adjacent-channel sidebands, which create noise in other stations. This is in contrast to worldwide DAB, which uses unused TV channels in the VHF band III range. This too has patent fees, while DAB does not. While there has been some effort by iBiquity to lower them, the fees for HD Radio are still an enormous expense when converting each station, and this fee structure presents a potentially high cost barrier to entry for community radio and other non-commercial educational stations when entering the HD Radio market. (Under the subsidiary communications authority principle, FM stations could in theory use any in-band on-channel digital system of their choosing; a competing service, FMeXtra, briefly gained some traction in the early 21st century but has since been discontinued.)\nSatellite radio (also called SDARS by the FCC) uses two proprietary standards instead of DAB-S, which requires users to change equipment when switching from one provider to the other, and prevents other competitors from offering new choices as stations can do on terrestrial radio. Had the FCC picked DAB-T for terrestrial radio, no separate satellite receiver would have been needed at all, and the only difference from DAB receivers in the rest of the world would be the need to tune S band instead of L band.\nIn mobile telephony, the FCC abandoned the \"any lawful device\" principle decided against AT&amp;T landlines, and has instead allowed each mobile phone company to dictate what its customers can use.\nPublic consultation.\nAs the public interest standard has always been important to the FCC when determining and shaping policy, so too has the relevance of public involvement in U.S. communication policy making. The \"FCC Record\" is the comprehensive compilation of decisions, reports, public notices, and other documents of the FCC, published since 1986.\nHistory of the issue.\n1927 Radio Act.\nIn the 1927 Radio Act, which was formulated by the predecessor of the FCC (the Federal Radio Commission), section 4(k) stipulated that the commission was authorized to hold hearings for the purpose of developing a greater understanding of the issues for which rules were being crafted. Section 4(k) stated that:\nExcept as otherwise provided in this Act, the commission, from time to time, as public convenience, interest, or necessity requires, shall... have the authority to hold hearings, summon witnesses, administer oaths, compel the production of books, documents, and papers and to make such investigations as may be necessary in the performance of its duties.\nThus, it is clear that public consultation, or at least consultation with outside bodies was regarded as central to the commission's job from early on. Though it should not be surprising, the act also stipulated that the commission should verbally communicate with those being assigned licenses. Section 11 of the act noted:\nIf upon examination of any application for a station license or for the renewal or modification of a station license the licensing authority shall determine that public interest, convenience, or necessity would be served by the granting thereof, it shall authorize the issuance, renewal, or modification thereof in accordance with said finding. In the event the licensing authority upon examination of any such application does not reach such decision with respect thereto, it shall notify the applicant thereof, shall fix and give notice of a time and place for hearing thereon, and shall afford such applicant an opportunity to be heard under such rules and regulations as it may prescribe.\nPublic hearings.\nAs early as 1927, there is evidence that public hearings were indeed held; among them, hearings to assess the expansion of the radio broadcast band. At these early hearings, the goal of having a broad range of viewpoints presented was evident, as not only broadcasters, but also radio engineers and manufacturers were in attendance. Numerous groups representing the general public appeared at the hearings as well, including amateur radio operators and inventors as well as representatives of radio listeners' organizations.\nWhile some speakers at the 1927 hearings referred to having received \"invitations\", Herbert Hoover's assistant observed in a letter at the time that \"the Radio Commission has sent out a blanket invitation to all people in the country who desire either to appear in person or to submit their recommendations in writing. I do not understand that the commission has sent for any particular individuals, however\" [Letter from George Akerson, assistant to Sec. Hoover, to Mrs. James T. Rourke, Box 497, Commerce Period Papers, Herbert Hoover Presidential Library (March 29, 1927)] (FN 14)\nIncluding members of the general public in the discussion was regarded (or at least articulated) as very important to the commission's deliberations. In fact, FCC commissioner Bellows noted at the time that \"it is the radio listener we must consider above everyone else.\" Though there were numerous representatives of the general public at the hearing, some expressing their opinions to the commission verbally, overall there was not a great turnout of everyday listeners at the hearings.\nThough not a constant fixture of the communications policy-making process, public hearings were occasionally organized as a part of various deliberation processes as the years progressed. For example, seven years after the enactment of the Radio Act, the Communications Act of 1934 was passed, creating the FCC. That year the federal government's National Recovery Agency (associated with the New Deal period) held public hearings as a part of its deliberations over the creation of new broadcasting codes.\nA few years later , the FCC held hearings to address early cross-ownership issues; specifically, whether newspaper companies owning radio stations was in the public interest. These \"newspaper divorcement hearings\" were held between 1941 and 1944, though it appears that these hearings were geared mostly towards discussion by industry stakeholders. Around the same time, the commission held hearings as a part of its evaluation of the national television standard, and in 1958 held additional hearings on the television network broadcasting rules. Though public hearings were organized somewhat infrequently, there was an obvious public appeal. In his now famous \"vast wasteland\" speech in 1961, FCC chairman Newton Minow noted that the commission would hold a \"well advertised public hearing\" in each community to assure broadcasters were serving the public interest, clearly a move to reconnect the commission with the public interest (at least rhetorically).\nOn September 5, 2023, commissioner Nathan Simington held a public forum on the tech-focused social news site, Hacker News.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55976", "revid": "47392020", "url": "https://en.wikipedia.org/wiki?curid=55976", "title": "Makefile", "text": ""}
{"id": "55978", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=55978", "title": "Disassembly", "text": ""}
{"id": "55980", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=55980", "title": "Mae Carol Jemison", "text": ""}
{"id": "55982", "revid": "46451761", "url": "https://en.wikipedia.org/wiki?curid=55982", "title": "Dirty bomb", "text": "Type of radiological weapon\nA dirty bomb or radiological dispersal device is a radiological weapon that combines radioactive material with conventional explosives. The purpose of the weapon is to contaminate the area around the dispersal agent/conventional explosion with radioactive material, serving primarily as an area denial device against civilians. It is not to be confused with a nuclear explosion, such as a fission bomb, which produces blast effects far in excess of what is achievable by the use of conventional explosives. Unlike the rain of radioactive material from a typical fission bomb, a dirty bomb's radiation can be dispersed only within a few hundred meters or a few miles of the explosion.\nDirty bombs have never been used, only tested. They are designed to disperse radioactive material over a certain area. They act through the effects of radioactive contamination on the environment and related health effects of radiation poisoning in the affected populations. The containment and decontamination of victims, as well as decontamination of the affected area require considerable time and expenses, rendering areas partly unusable and causing economic damage. Dirty bombs might be used to create mass panic as a weapon of terror.\nEffect of a dirty bomb explosion.\nWhen dealing with the implications of a dirty bomb attack, there are two main areas to be addressed: the civilian impact, not only dealing with immediate casualties and long term health issues, but also the psychological effect; and the economic impact. With no prior event of a dirty bomb detonation, it is considered difficult to predict the impact. Several analyses have predicted that radiological dispersal devices will neither sicken nor kill many people.\nSource: Adapted from Levi MA, Kelly HC. \"Weapons of mass disruption\". \"Sci Am.\" 2002 Nov;287(5):76-81.\nAccidents with radioactives.\nThe effects of uncontrolled radioactive contamination have been reported several times.\nOne example is the radiological accident occurring in Goi\u00e2nia, Brazil, between September 1987 and March 1988: Two metal scavengers broke into an abandoned radiotherapy clinic and removed a teletherapy source capsule containing powdered caesium-137 with an activity of 50 TBq. They brought it back to the home of one of the men to take it apart and sell as scrap metal. Later that day both men were showing acute signs of radiation illness with vomiting and one of them had a swollen hand and diarrhea. A few days later one of the men punctured the thick window of the capsule, allowing the caesium chloride powder to leak out and when realizing the powder glowed blue in the dark, brought it back home to his family and friends to show it off. After two weeks of spread by contact contamination causing an increasing number of adverse health effects, the correct diagnosis of acute radiation sickness was made at a hospital and proper precautions could be put into procedure. By this time 249 people were contaminated, 151 exhibited both external and internal contamination, of whom 20 people were seriously ill and five people died.\nThe Goi\u00e2nia incident to some extent predicts the contamination pattern if it is not immediately realized that the explosion spread radioactive material, but also how fatal even very small amounts of ingested radioactive powder can be. This raises worries of terrorists using powdered alpha emitting material, that if ingested can pose a serious health risk, as in the case of Alexander Litvinenko, who was poisoned by tea with polonium-210. \"Smoky bombs\" based on alpha emitters might be just as dangerous as beta or gamma emitting dirty bombs.\nPublic perception of risks.\nAlthough the exposure might be minimal, many people find radiation exposure especially frightening because it is something they cannot see or feel, and it therefore becomes an unknown source of danger. When United States Attorney General John Ashcroft on June 10, 2002, announced the arrest of Jos\u00e9 Padilla, allegedly plotting to detonate such a weapon, he said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[A] radioactive \"dirty bomb\"\u00a0... spreads radioactive material that is highly toxic to humans and can cause mass death and injury.\nThis public fear of radiation also plays a big role in why the costs of a radiological dispersal device impact on a major metropolitan area (such as Lower Manhattan) might be equal to or even larger than that of the 9/11 attacks. Assuming the radiation levels are not too high and the area does not need to be abandoned such as the town of Pripyat near the Chernobyl reactor, an expensive and time-consuming cleanup procedure will begin. This will mainly consist of tearing down highly contaminated buildings, digging up contaminated soil and quickly applying sticky substances to remaining surfaces so that radioactive particles adhere before radioactivity penetrates the building materials. These procedures are the current state of the art for radioactive contamination cleanup, but some experts say that a complete cleanup of external surfaces in an urban area to current decontamination limits may not be technically feasible. Loss of working hours will be vast during cleanup, but even after the radiation levels reduce to an acceptable level, there might be residual public fear of the site including possible unwillingness to conduct business as usual in the area. Tourist traffic is likely never to resume.\nDirty bombs and terrorism.\nSince the 9/11 attacks, the fear of terrorist groups using dirty bombs has increased, which has been frequently reported in the media. The meaning of terrorism used here, is described by the U.S. Department of Defense's definition, which is \"the calculated use of unlawful violence or threat of unlawful violence to inculcate fear; intended to coerce or to intimidate governments or societies in the pursuit of goals that are generally political, religious, or ideological.\"\nConstructing and obtaining material for a dirty bomb.\nIn order for a terrorist organization to construct and detonate a dirty bomb, it must acquire radioactive material. Possible radiological dispersal device material could come from the millions of radioactive sources used worldwide in the industry, for medical purposes and in academic applications mainly for research. Of these sources, only nine reactor-produced isotopes stand out as being suitable for radiological terror: americium-241, californium-252, caesium-137, cobalt-60, iridium-192, plutonium-238, polonium-210, radium-226 and strontium-90, and even from these it is possible that radium-226 and polonium-210 do not pose a significant threat. Of these sources the U.S. Nuclear Regulatory Commission has estimated that within the U.S., approximately one source is lost, abandoned or stolen every day of the year. Within the European Union the annual estimate is 70. There exist thousands of such \"orphan\" sources scattered throughout the world, but of those reported lost, no more than an estimated 20 percent can be classified as potential high security concerns if used in a radiological dispersal device. Russia is believed to house thousands of orphan sources, which were lost following the collapse of the Soviet Union. A large but unknown number of these sources probably belong to the high security risk category. These include the beta-emitting strontium-90 sources used as radioisotope thermoelectric generators for beacons in lighthouses in remote areas of Russia. In December 2001, three Georgian woodcutters stumbled over such a power generator and dragged it back to their camp site to use it as a heat source. Within hours they suffered from acute radiation sickness and sought hospital treatment. The International Atomic Energy Agency (IAEA) later stated that it contained approximately of strontium, equivalent to the amount of radioactivity released immediately after the Chernobyl accident (though the total radioactivity release from Chernobyl was 2500 times greater at around ).\nAlthough a terrorist organization might obtain radioactive material through the \"black market\", and there has been a steady increase in illicit trafficking of radioactive sources from 1996 to 2004, these recorded trafficking incidents mainly refer to rediscovered orphan sources without any sign of criminal activity, and it has been argued that there is no conclusive evidence for such a market. In addition to the hurdles of obtaining usable radioactive material, there are several conflicting requirements regarding the properties of the material the terrorists need to take into consideration: First, the source should be \"sufficiently\" radioactive to create direct radiological damage at the explosion or at least to perform societal damage or disruption. Second, the source should be transportable with enough shielding to protect the carrier, but not so much that it will be too heavy to maneuver. Third, the source should be sufficiently dispersible to effectively contaminate the area around the explosion.\nPossibility of use by terrorist groups.\nThe first attempt of radiological terror was reportedly carried out in November 1995 by a group of Chechen separatists, who buried a caesium-137 source wrapped in explosives at the Izmaylovsky Park in Moscow. A Chechen rebel leader alerted the media, the bomb was never activated, and the incident amounted to a mere publicity stunt. In December 1998, a second attempt was announced by the Chechen Security Service, who discovered a container filled with radioactive materials attached to an explosive mine. The bomb was hidden near a railway line in the suburban area Argun, ten miles east of the Chechen capital of Grozny. The same Chechen separatist group was suspected to be involved.\nOn 8 May 2002, Jos\u00e9 Padilla (a.k.a. Abdulla al-Muhajir) was arrested on suspicion that he was an al-Qaeda terrorist planning to detonate a dirty bomb in the U.S. This suspicion was raised by information obtained from an arrested terrorist in U.S. custody, Abu Zubaydah, who under interrogation revealed that the organization was close to constructing a dirty bomb. Although Padilla had not obtained radioactive material or explosives at the time of arrest, law enforcement authorities uncovered evidence that he was on reconnaissance for usable radioactive material and possible locations for detonation. It has been doubted whether Jos\u00e9 Padilla was preparing such an attack, and it has been claimed that the arrest was highly politically motivated, given the pre-9/11 security lapses by the CIA and FBI.\nIn 2006, Dhiren Barot from North London pleaded guilty of conspiring to murder people in the United Kingdom and United States using a radioactive dirty bomb. He planned to target underground car parks within the UK and buildings in the U.S. such as the International Monetary Fund, World Bank buildings in Washington D.C., the New York Stock Exchange, Citigroup buildings and the Prudential Financial buildings in Newark, New Jersey. He also faces 12 other charges including, conspiracy to commit public nuisance, seven charges of making a record of information for terrorist purposes and four charges of possessing a record of information for terrorist purposes. Experts say if the plot to use the dirty bomb was carried out \"it would have been unlikely to cause deaths, but was designed to affect about 500 people\".\nIn January 2009, a leaked FBI report described the results of a search of the Maine home of James G. Cummings, a white supremacist who had been shot and killed by his wife. Investigators found four one-gallon containers of 35 percent hydrogen peroxide, uranium, thorium, lithium metal, aluminium powder, beryllium, boron, black iron oxide and magnesium as well as literature on how to build dirty bombs and information about caesium-137, strontium-90 and cobalt-60, radioactive materials. Officials confirmed the veracity of the report but stated that the public was never at risk.\nIn July 2014, ISIS militants seized of uranium compounds from Mosul University. The material was unenriched and so could not be used to build a conventional fission bomb, but a dirty bomb was a theoretical possibility. Nonetheless, uranium's relatively low radioactivity makes it a poor candidate for use in a dirty bomb.\nTerrorist organizations may also capitalize on the fear of radiation to create weapons of mass disruption rather than weapons of mass destruction. A fearful public response may in itself accomplish the goals of a terrorist organization to gain publicity or destabilize society. Even simply stealing radioactive materials may trigger a panic reaction from the general public. Similarly, a small-scale release of radioactive materials or a threat of such a release may be considered sufficient for a terror attack. Particular concern is directed towards the medical sector and healthcare sites, which are \"intrinsically more vulnerable than conventional licensed nuclear sites\". Opportunistic attacks may range to even kidnapping patients whose treatment involve radioactive materials. In the Goi\u00e2nia accident, over 100,000 people admitted themselves to monitoring, while only 49 were admitted to hospitals. Other benefits to a terrorist organization of a dirty bomb include economic disruption in the area affected, abandonment of affected assets (such a buildings, subways) due to public concern, and international publicity useful for recruitment.\nTests.\nIsrael carried out a four-year series of tests on nuclear explosives to measure the effects were hostile forces ever to use them against Israel, \"Haaretz\" reported in 2015. According to the report, high-level radiation was measured only at the center of the explosions, while the level of dispersal of radiation by particles carried by the wind (fallout) was low. The bombs reportedly did not pose a significant danger beyond their psychological effect.\nDetection and prevention.\nDirty bombs may be prevented by detecting illicit radioactive materials in shipping with tools such as a Radiation Portal Monitor. Similarly, unshielded radioactive materials may be detected at checkpoints by Geiger counters, gamma-ray detectors, and even Customs and Border Patrol (CBS) pager-sized radiation detectors. Hidden materials may also be detected by x-ray inspection and heat emitted may be picked up by infrared detectors. Such devices, however, may be circumvented by simply transporting materials across unguarded stretches of coastline or other barren border areas.\nOne proposed method for detecting shielded Dirty Bombs is Nanosecond Neutron Analysis (NNA). Designed originally for the detection of explosives and hazardous chemicals, NNA is also applicable to fissile materials. NNA determines what chemicals are present in an investigated device by analyzing emitted \u03b3-emission neutrons and \u03b1-particles created from a reaction in the neutron generator. The system records the temporal and spatial displacement of the neutrons and \u03b1-particles within separate 3D regions. A prototype dirty-bomb detection device created with NNA is demonstrated to be able to detect uranium from behind a 5\u00a0cm-thick lead wall. Other radioactive material detectors include Radiation Assessment and Identification (RAID) and Sensor for Measurement and Analysis of Radiation Transients, both developed by Sandia National Laboratories. Sodium iodide scintillator based aerial radiation detection systems are capable to detect International Atomic Energy Agency (IAEA) defined dangerous quantities of radioactive material and have been deployed by the New York City Police Department (NYPD) Counterterrorism Bureau.\nThe IAEA recommends certain devices be used in tandem at country borders to prevent transfer of radioactive materials, and thus the building of dirty bombs. They define the four main goals of radiation detection instruments as detection, verification, assessment and localization, and identification as a means to escalate a potential radiological situation. The IAEA also defines the following types of instruments:\nLegislative and regulatory actions can also be used to prevent access to materials needed to create a dirty bomb. Examples include the 2006 U.S. Dirty Bomb Bill, the Yucca Flats proposal, and the Nunn-Lungar act. Similarly, close monitoring and restrictions of radioactive materials may provide security for materials in vulnerable private-sector applications, most notably in the medical sector where such materials are used for treatments. Suggestions for increased security include isolation of materials in remote locations and strict limitation of access.\nOne way to mitigate a major effect of a radiological weapons may also be to educate the public on the nature of radioactive materials. As one of the major concerns of a dirty bomb is the public panic proper education may prove a viable counter-measure. Education on radiation is considered by some to be \"the most neglected issue related to radiological terrorism\".\nPersonal safety.\nThe dangers of a dirty bomb come from the initial blast and the radioactive materials To mitigate the risk of radiation exposure, FEMA suggests the following guidelines:\nTreatment.\nAs of 2023[ [update]], research is under way to find radioactive decontanimation drugs to remove radioactive elements from the body. One drug candidate under investigation is HOPO 14-1.\nOther uses of the term.\nThe term has also been used historically to refer to certain types of nuclear weapons. Due to the inefficiency of early nuclear weapons, only a small amount of the nuclear material would be consumed during the explosion. Little Boy had an efficiency of only 1.4%. Fat Man, which used a different design and a different fissile material, had an efficiency of 14%. Thus, they tended to disperse large amounts of unused fissile material, and the fission products, which are on average much more dangerous, in the form of nuclear fallout. During the 1950s, there was considerable debate over whether \"clean\" bombs could be produced and these were often contrasted with \"dirty\" bombs. \"Clean\" bombs were often a stated goal and scientists and administrators said that high-efficiency nuclear weapon design could create explosions that generated almost all of their energy in the form of nuclear fusion, which does not create harmful fission products.\nBut the \"Castle Bravo\" accident of 1954, in which a thermonuclear weapon produced a large amount of fallout that was dispersed among human populations, suggested that this was not what was actually being used in modern thermonuclear weapons, which derive around half of their yield from a final fission stage of the fast fissioning of the uranium tamper of the secondary. While some proposed producing \"clean\" weapons, other theorists noted that one could make a nuclear weapon intentionally \"dirty\" by \"salting\" it with a material, which would generate large amounts of long-lasting fallout when irradiated by the weapon core. These are known as salted bombs; a specific subtype often noted is a cobalt bomb.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55983", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=55983", "title": "Black rat", "text": "Species of rodent\n&lt;templatestyles src=\"Template:Taxobox/core/styles.css\" /&gt;\nThe black rat (Rattus rattus), also known as the roof rat, ship rat, or house rat, is a common long-tailed rodent of the stereotypical rat genus \"Rattus\", in the subfamily Murinae. It likely originated in the Indian subcontinent, but is now found worldwide.\nThe black rat is black to light brown in colour with a lighter underside. It is a generalist omnivore and a serious pest to farmers because it feeds on a wide range of agricultural crops. It is sometimes kept as a pet. In parts of India, it is considered sacred and respected in the Karni Mata Temple in Deshnoke.\nTaxonomy.\n\"Mus rattus\" was the scientific name proposed by Carl Linnaeus in 1758 for the black rat.\nThree subspecies were once recognized, but today are considered invalid and are now known to be actually colour morphs:\nCharacteristics.\nA typical adult black rat is long, not including a tail, and weighs , depending on the subspecies. Black rats typically live for about one year in the wild and up to four years in captivity. Despite its name, the black rat exhibits several colour forms. It is usually black to light brown in colour with a lighter underside. In England during the 1920s, several variations were bred and shown alongside domesticated brown rats. This included an unusual green-tinted variety.\nOrigin.\nThe black rat was present in prehistoric Europe and in the Levant during postglacial periods. The black rat in the Mediterranean region differs genetically from its South Asian ancestor by having 38 instead of 42 chromosomes. Its closest relative is the Asian house rat (\"R. tanezumi\") from Southeast Asia. The two diverged about 120,000 years ago in southwestern Asia. It is unclear how the rat made its way to Europe due to insufficient data, although a land route seems more likely based on the distribution of European haplogroup \"A\". The black rat spread throughout Europe with the Roman conquest, but declined around the 6th century, possibly due to collapse of the Roman grain trade, climate cooling, or the Justinianic Plague. A genetically different rat population of haplogroup A replaced the Roman population in the medieval times in Europe.\nIt is a resilient vector for many diseases because of its ability to hold so many infectious bacteria in its blood. It was formerly thought to have played a primary role in spreading bacteria contained in fleas on its body, such as the plague bacterium (\"Yersinia pestis\") which is responsible for the Plague of Justinian and the Black Death. However, recent studies have called this theory into question and instead posit humans themselves as the vector, as the movements of the epidemics and the black rat populations do not show historical or geographical correspondence. A study published in 2015 indicates that other Asiatic rodents served as plague reservoirs, from which infections spread as far west as Europe via trade routes, both overland and maritime. Although the black rat was certainly a plague vector in European ports, the spread of the plague beyond areas colonized by rats suggests that the plague was also circulated by humans after reaching Europe.\nDistribution and habitat.\nThe black rat originated in India and Southeast Asia, and spread to the Near East and Egypt, and then throughout the Roman Empire, reaching Great Britain as early as the 1st century AD. Europeans subsequently spread it throughout the world. The black rat is again largely confined to warmer areas, having been supplanted by the brown rat (\"Rattus norvegicus\") in cooler regions and urban areas. In addition to the brown rat being larger and more aggressive, the change from wooden structures and thatched roofs to bricked and tiled buildings favoured the burrowing brown rats over the arboreal black rats. In addition, brown rats eat a wider variety of foods, and are more resistant to weather extremes.\nBlack rat populations can increase exponentially under certain circumstances, perhaps having to do with the timing of the fruiting of the bamboo plant, and cause devastation to the plantings of subsistence farmers; this phenomenon is known as \"mautam\" in parts of India.\nBlack rats are thought to have arrived in Australia with the First Fleet, and subsequently spread to many coastal regions in the country.\nBlack rats adapt to a wide range of habitats. In urban areas they are found around warehouses, residential buildings, and other human settlements. They are also found in agricultural areas, such as in barns and crop fields. In urban areas, they prefer to live in dry upper levels of buildings, so they are commonly found in wall cavities and false ceilings. In the wild, black rats live in cliffs, rocks, the ground, and trees. They are great climbers and prefer to live in palms and trees, such as pine trees. Their nests are typically spherical and made of shredded material, including sticks, leaves, other vegetation and cloth. In the absence of palms or trees, they can burrow into the ground. Black rats are also found around fences, ponds, riverbanks, streams, and reservoirs.\nBehaviour and ecology.\nIt is thought that male and female rats have similarly sized home ranges during the winter, but male rats increase the size of their home range during the breeding season. Along with differing between rats of different sex, home range also differs depending on the type of forest in which the black rat inhabits. For example, home ranges in the southern beech forests of the South Island, New Zealand appear to be much larger than the non-beech forests of the North Island. Due to the limited number of rats that are studied in home range studies, the estimated sizes of rat home ranges in different rat demographic groups are inconclusive.\nDiet and foraging.\nBlack rats are considered omnivores and eat a wide range of foods, including seeds, fruit, stems, leaves, fungi, and a variety of invertebrates and vertebrates. They are generalists, and thus not very specific in their food preferences, which is indicated by their tendency to feed on any meal provided for cows, swine, chickens, cats and dogs. They are similar to the tree squirrel in their preference of fruits and nuts. They eat about per day and drink about per day. Their diet is high in water content. They are a threat to many natural habitats because they feed on birds and insects. They are also a threat to many farmers, since they feed on a variety of agricultural-based crops, such as cereals, sugar cane, coconuts, cocoa, oranges, and coffee beans.\nThe black rat displays flexibility in its foraging behaviour. It is a predatory species and adapts to different micro-habitats. It often meets and forages together in close proximity within and between sexes. It tends to forage after sunset. If the food cannot be eaten quickly, it searches for a place to carry and hoard to eat at a later time. Although it eats a broad range of foods, it is a highly selective feeder; only a restricted selection of the foods is dominating. When offered a wide diversity of foods, it eats only a small sample of each. This allows it to monitor the quality of foods that are present year round, such as leaves, as well as seasonal foods, such as herbs and insects. This method of operating on a set of foraging standards ultimately determines the final composition of its meals. Also, by sampling the available food in an area, it maintains a dynamic food supply, balance its nutrient intake, and avoids intoxication by secondary compounds.\nNesting behaviour.\nThrough the usage of tracking devices such as radio transmitters, rats have been found to occupy dens located in trees, as well as on the ground. In Puketi Forest in the Northland Region of New Zealand, rats have been found to form dens together. Rats appear to den and forage in separate areas in their home range depending on the availability of food resources. Research shows that, in New South Wales, the black rat prefers to inhabit lower leaf litter of forest habitat. There is also an apparent correlation between the canopy height and logs and the presence of black rats. This correlation may be a result of the distribution of the abundance of prey as well as available refuges for rats to avoid predators. As found in North Head, New South Wales, there is positive correlation between rat abundance, leaf litter cover, canopy height, and litter depth. All other habitat variables showed little to no correlation. While this species' relative, the brown (Norway) rat, prefers to nest near the ground of a building the black rat will prefer the upper floors and roof. Because of this habit they have been given the common name roof rat.\nDiseases.\nBlack rats (or their ectoparasites) can carry a number of pathogens, of which bubonic plague (via the Oriental rat flea), typhus, Weil's disease, toxoplasmosis and trichinosis are the best known. It has been hypothesized that the displacement of black rats by brown rats led to the decline of the Black Death. This theory has, however, been deprecated, as the dates of these displacements do not match the increases and decreases in plague outbreaks.\nRats serve as outstanding vectors for transmittance of diseases because they can carry bacteria and viruses in their systems. A number of bacterial diseases are common to rats, and these include \"Streptococcus pneumoniae\", \"Corynebacterium kutsheri,\" \"Bacillus piliformis\", \"Pasteurella pneumotropica\", and \"Streptobacillus moniliformis\", to name a few. All of these bacteria are disease causing agents in humans. In some cases, these diseases are incurable.\nPredators.\nThe black rat is prey to cats and owls in domestic settings. In less urban settings, rats are preyed on by weasels, foxes and coyotes. These predators have little effect on the control of the black rat population because black rats are agile and fast climbers. In addition to agility, the black rat also uses its keen sense of hearing to detect danger and quickly evade mammalian and avian predators.\nAs an invasive species.\nDamage caused.\nAfter \"Rattus rattus\" was introduced into the northern islands of New Zealand, they fed on the seedlings, adversely affecting the ecology of the islands. Even after eradication of \"R. rattus\", the negative effects may take decades to reverse. When consuming these seabirds and seabird eggs, these rats reduce the pH of the soil. This harms plant species by reducing nutrient availability in soil, thus decreasing the probability of seed germination. For example, research conducted by Hoffman et al. indicates a large impact on 16 indigenous plant species directly preyed on by \"R. rattus\". These plants displayed a negative correlation in germination and growth in the presence of black rats.\nRats prefer to forage in forest habitats. In the Ogasawara islands, they prey on the indigenous snails and seedlings. Snails that inhabit the leaf litter of these islands showed a significant decline in population on the introduction of \"Rattus rattus\". The black rat shows a preference for snails with larger shells (greater than 10\u00a0mm), and this led to a great decline in the population of snails with larger shells. A lack of prey refuges makes it more difficult for the snail to avoid the rat.\nComplex pest.\nThe black rat is a complex pest, defined as one that influences the environment in both harmful and beneficial ways. In many cases, after the black rat is introduced into a new area, the population size of some native species declines or goes extinct. This is because the black rat is a good generalist with a wide dietary niche and a preference for complex habitats; this causes strong competition for resources among small animals. This has led to the black rat completely displacing many native species in Madagascar, the Galapagos, and the Florida Keys. In a study by Stokes \"et al.\", habitats suitable for the native bush rat, \"Rattus fuscipes\", of Australia are often invaded by the black rat and are eventually occupied by only the black rat. When the abundances of these two rat species were compared in different micro-habitats, both were found to be affected by micro-habitat disturbances, but the black rat was most abundant in areas of high disturbance; this indicates it has a better dispersal ability.\nDespite the black rat's tendency to displace native species, it can also aid in increasing species population numbers and maintaining species diversity. The bush rat, a common vector for spore dispersal of truffles, has been extirpated from many micro-habitats of Australia. In the absence of a vector, the diversity of truffle species would be expected to decline. In a study in New South Wales, Australia it was found that, although the bush rat consumes a diversity of truffle species, the black rat consumes as much of the diverse fungi as the natives and is an effective vector for spore dispersal. Since the black rat now occupies many of the micro-habitats that were previously inhabited by the bush rat, the black rat plays an important ecological role in the dispersal of fungal spores. By eradicating the black rat populations in Australia, the diversity of fungi would decline, potentially doing more harm than good.\nControl methods.\nLarge-scale rat control programs have been taken to maintain a steady level of the invasive predators in order to conserve the native species in New Zealand such as kokako and mohua. Pesticides, such as pindone and 1080 (sodium fluoroacetate), are commonly distributed via aerial spray by helicopter as a method of mass control on islands infested with invasive rat populations. Bait, such as brodifacoum, is also used along with coloured dyes (used to deter birds from eating the baits) in order to kill and identify rats for experimental and tracking purposes. Another method to track rats is the use of wired cage traps, which are used along with bait, such as rolled oats and peanut butter, to tag and track rats to determine population sizes through methods like mark-recapture and radio-tracking. Tracking tunnels (coreflute tunnels containing an inked card) are also commonly used monitoring devices, as are chew-cards containing peanut butter. Poison control methods are effective in reducing rat populations to nonthreatening sizes, but rat populations often rebound to normal size within months. Besides their highly adaptive foraging behaviour and fast reproduction, the exact mechanisms for their rebound is unclear and are still being studied.\nIn 2010, the Sociedad Ornitol\u00f3gica Puertorrique\u00f1a (Puerto Rican Bird Society) and the Ponce Yacht and Fishing Club launched a campaign to eradicate the black rat from the Isla Ratones (Mice Island) and Isla Cardona (Cardona Island) islands off the municipality of Ponce, Puerto Rico.\nDecline in population.\nEradication projects have eliminated black rats from Lundy in the Bristol Channel (2006) and from the Shiant Islands in the Outer Hebrides (2016). Populations probably survive on other islands (e.g. Inchcolm) and in localised areas of the British mainland. Recent National Biodiversity Network data shows a very sparse populations around the U.K., and the Mammal Society has no records in the major port towns of Manchester, Liverpool or Glasgow for over 40 years, areas which were historically population strongholds.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55986", "revid": "45959324", "url": "https://en.wikipedia.org/wiki?curid=55986", "title": "Xanana Gusm\u00e3o", "text": "Prime Minister of Timor-Leste (2007\u20132015; since 2023)\nJos\u00e9 Alexandre \"Xanana\" Gusm\u00e3o (; born 20 June 1946) is an East Timorese politician. He has served as the tenth prime minister of Timor-Leste since 2023, previously serving as the sixth prime minister from 2007 to 2015. A former rebel, he also served as East Timor's first president since its re-establishment of independence from 2002 to 2007.\nEarly life and career.\nGusm\u00e3o was born in Laleia, Manatuto, in what was then Portuguese Timor, as the second son in a large family. His parents, both of whom were school teachers, were of mixed Portuguese-Timorese ancestry, and his family were \"assimilados\". He attended a Jesuit school in Dare, just outside Dili, and Dili High School. After leaving high-school for financial reasons at the age of 15 in 1961, he held a variety of unskilled jobs, while continuing his education at night school.\nIn 1965, aged 19, Gusm\u00e3o met Emilia Batista, who was later to become his wife. His nickname, \"Xanana\", was taken from the name of the American rock and roll band \"Sha Na Na\", (which is pronounced the same as \"Xanana\" which is spelled according to Portuguese and Tetum spelling rules) who in turn were named after a lyric from the doo-wop song \"Get a Job\" written and recorded in 1957 by the Silhouettes.\nIn 1966, Gusm\u00e3o obtained a position with the public service, which allowed him to continue his education. This was interrupted in 1968 when Gusm\u00e3o was recruited by the Portuguese Army for national service. He served for three years, rising to the rank of corporal. During this time, he married Emilia Batista, with whom he had a son Eugenio, and a daughter Zenilda. He has since divorced Emilia, and in 2000, he married Australian Kirsty Sword, with whom he had three sons: Alexandre, Kay Olok and Daniel. In 1971, Gusm\u00e3o completed his national service, his son was born, and he became involved with a nationalist organisation headed by Jos\u00e9 Ramos-Horta. For the next three years he was actively involved in peaceful protests directed at the colonial system.\nThe 1974 Carnation Revolution in Portugal resulted in the beginning of decolonisation for Portuguese Timor, and shortly afterwards the Governor M\u00e1rio Lemos Pires announced plans to grant the colony independence. Plans were drawn up to hold general elections with a view to independence in 1978. During most of 1975 a bitter internal struggle occurred between two rival factions in Portuguese Timor. Gusm\u00e3o became deeply involved with the Fretilin faction, and as a result he was arrested and imprisoned by the rival faction the Timorese Democratic Union (UDT) in mid-1975. Taking advantage of the internal disorder, and with an eye to absorbing the colony, Indonesia immediately began a campaign of destabilisation, and frequent raids into Portuguese Timor were staged from Indonesian West Timor. By late 1975 the Fretilin faction had gained control of Portuguese Timor and Gusm\u00e3o was released from prison. He was given the position of Press Secretary within the Fretilin organisation. On 28 November 1975, Fretilin declared the independence of Portuguese Timor as \"The Democratic Republic of East Timor\", and Gusm\u00e3o was responsible for filming the ceremony. Nine days later, Indonesia invaded East Timor. At the time Gusm\u00e3o was visiting friends outside of Dili and he witnessed the invasion from the hills. For the next few days he searched for his family.\nIndonesian occupation.\nAfter the appointment of the Provisional Government of East Timor by Indonesia, Gusm\u00e3o became heavily involved in resistance activities. Gusm\u00e3o was largely responsible for the level of organisation that evolved in the resistance, which ultimately led to its success. The early days featured Gusm\u00e3o walking from village to village to obtain support and recruits. In 1977, Gusm\u00e3o was the aide-de-camp to Fretilin political commissar Abel Larisina and organised supplies for civilians at the resistance base at Matebian. In November 1978, the base was destroyed by the Indonesians. But after Fretilin suffered some major setbacks in the early 1980s, including a failed 1984 coup attempt against Gusm\u00e3o led by four senior Falintil officers, including Mauk Moruk, Gusm\u00e3o left Fretilin and supported various centrist coalitions, eventually becoming a leading opponent of Fretilin. In March 1981, a secret national conference in Lacluta elected him head of Falintil, succeeding the slain Nicolau dos Reis Lobatos. \nIn 1988, Gusm\u00e3o became leader of the newly formed National Council of Resistance (CNRT). To avoid being seen as partisan, Gusm\u00e3o left Fretilin for this. Under his leadership, FALINTIL relied more on clandestine underground networks and used small groups to attack Indonesian targets. By the mid-1980s, he was a major leader. During the early 1990s, Gusm\u00e3o became deeply involved in diplomacy and media management, and was instrumental in alerting the world to the massacre in Dili that occurred in Santa Cruz on 12 November 1991. Gusm\u00e3o was interviewed by many major media channels and obtained worldwide attention.\nAs a result of his high profile, Gusm\u00e3o became a prime target of the Indonesian government. Indonesian troops (TNI) attempted to capture Gusm\u00e3o in the Same and Ainaro area on 14 November 1990 with \"Operasi Senyum\" (\"Operation Smile\"). Four days earlier, a woman had been captured who testified during interrogation that the rebel leader was staying at a nearby mountain. Xanana Gusm\u00e3o, however, probably escaped one night before the attack. After the attack, in which twelve battalions and four helicopters were deployed, the military claimed to have found about 100 fighters. Also found was a container with Gusm\u00e3o's documents, a video camera and his typewriter. Among the documents were letters from the Pope and Bishop Carlos Belo. According to a traditional Timorese legend, some warriors were able to transform themselves into dogs to escape their captors. Picking up on this myth, the legend spread that Gusm\u00e3o could also turn into a white dog and thus run around the village unnoticed while the Indonesian soldiers were looking for him.\nIn November 1992, a campaign for his capture was finally successful in a large-scale operation by the Indonesian military with Gusm\u00e3o apprehended in a tunnel under the family home of Alian\u00e7a Ara\u00fajo in Lahane near Dili and taken to Bali. In May 1993, Gusm\u00e3o was tried, convicted and sentenced to life imprisonment by the Indonesian government. He was found guilty under Article 108 of the Indonesian Penal Code (rebellion), Law no. 12 of 1951 (illegal possession of firearms) and Article 106 (attempting to separate part of the territory of Indonesia). He spoke in his own defence and he was appointed with defence lawyers before the commencement of his trial. The sentence was commuted to 20 years by the Indonesian President Suharto in August 1993. He was taken to Jakarta's maximum security prison, Cipinang. Although not released until late 1999, Gusm\u00e3o successfully led the resistance from within prison with the help of Kirsty Sword. Prior to his release, the United Kingdom offered Gusm\u00e3o political asylum to ensure his safety. The \"Xanana Room\" at the British Embassy in Jakarta commemorates this today. By the time of his release, he was regularly visited by United Nations representatives, and dignitaries such as Nelson Mandela.\nTransition to independence.\nOn 30 August 1999, a referendum was held in East Timor and an overwhelming majority voted for independence. The Indonesian military commenced a campaign of terror as a result, with terrible consequences. Although the Indonesian government denied ordering this offensive, they were widely condemned for failing to prevent it. As a result of overwhelming diplomatic pressure from the United Nations, promoted by Portugal since the late 1970s and also by the United States and Australia in the 1990s, a UN-sanctioned, Australian-led international peace-keeping force (INTERFET) entered East Timor.\nGusm\u00e3o was secretly flown into East Timor by INTERFET on Thursday, 21 October 1999, flying from Darwin, Australia, to Baucau, before moving onwards to Dili. His presence in Dili was revealed by loudspeaker trucks announcing he would make a speech. This 25-minute speech urged reconciliation and rebuilding. At this time, Gusm\u00e3o was 53, and was already expected to become the first President, despite some criticism over his lack of action against the post-referendum terror campaign.\nIn 1999, Xanana Gusm\u00e3o was elected speaker of the National Consultative Council (NCC), a kind of transitional parliament during the UN administration of East Timor. On 23 October 2000, Gusm\u00e3o also became spokesman for the subsequent National Council (NC). Gusm\u00e3o was appointed to a senior role in the UN administration that governed East Timor until 20 May 2002. During this time he continually campaigned for unity and peace within East Timor, and was generally regarded as the \"de facto\" leader of the emerging nation. Elections were held in late 2001 and Gusm\u00e3o, endorsed by nine parties but not by Fretilin, ran as an independent and was comfortably elected leader. \nGusm\u00e3o eventually won the presidential election on 14 April 2002 with 82.7% against his opponent Francisco Xavier do Amaral and the first president of East Timor when it became formally independent on 20 May 2002. Gusm\u00e3o has published an autobiography with selected writings entitled \"To Resist Is to Win\". He is the main narrator of the film \"A Hero's Journey\"/\"Where the Sun Rises\", a 2006 documentary about him and East Timor. According to director Grace Phan, it's an \"intimate insight into the personal transformation\" of the man who helped shape and liberate East Timor.\nIndependent East Timor.\nOn 21 June 2006, Gusm\u00e3o called for Prime Minister Mari Alkatiri to resign or else he would, as allegations that Alkatiri had ordered a hit squad to threaten and kill his political opponents led to a large backlash. Senior members of the Fretilin party met on 25 June to discuss Alkatiri's future as the prime minister, amidst a protest involving thousands of people calling for Alkatiri to resign instead of Gusm\u00e3o. Despite receiving a vote of confidence from his party, Alkatiri resigned on 26 June 2006 to end the uncertainty. In announcing this he said, \"I declare I am ready to resign my position as prime minister of the government...so as to avoid the resignation of His Excellency the President of the Republic [Xanana Gusm\u00e3o].\" The 'hit squad' accusations against Alkatiri were subsequently rejected by a UN Commission, which also criticised Gusm\u00e3o for making inflammatory statements during the crisis.\nGusm\u00e3o declined to run for another term in the April 2007 presidential election. In March 2007 he said that he would lead the new National Congress for Timorese Reconstruction (CNRT) into the parliamentary election planned to be held later in the year, and said that he would be willing to become prime minister if his party won the election. He was succeeded as president by Jos\u00e9 Ramos-Horta on 20 May 2007. The CNRT placed second in the June 2007 parliamentary election, behind Fretilin, taking 24.10% of the vote and 18 seats. He won a seat in parliament as the first name on the CNRT's candidate list. The CNRT allied with other parties to form a coalition that would hold a majority of seats in parliament. After weeks of dispute between this coalition and Fretilin over who should form the government, Ramos-Horta announced on 6 August that the CNRT-led coalition would form the government and that Gusm\u00e3o would become prime minister on 8 August. Gusm\u00e3o was sworn in at the presidential palace in Dili on 8 August.\nOn 11 February 2008, a motorcade containing Gusm\u00e3o came under gunfire one hour after President Jos\u00e9 Ramos-Horta was shot in the stomach. Gusm\u00e3o's residence was also occupied by rebels. According to the Associated Press, the incidents raised the possibility of a coup attempt; they have also described as possible assassination attempts and kidnap attempts.\nIn the 2012 parliamentary elections in East Timor, Gusm\u00e3o succeeded in re-entering parliament. With the CNRT as the strongest party, he also leads the new government as Prime Minister and Minister of Defence. Alfredo Pires took over as Minister of Petroleum and Natural Resources. Again, Gusm\u00e3o renounced his seat in parliament.\nAt the beginning of 2015, Gusm\u00e3o announced his intention to reshuffle the government and also to resign early himself. On 5 February, he informed his coalition partners that he intended to propose former Health Minister Rui Ara\u00fajo as his successor and resigned by writing to President Taur Matan Ruak. The President accepted his resignation and appointed Ara\u00fajo to form a new government. The handover of office took place on 16 February. In the new government, Gusm\u00e3o is \"Consultative Minister\" and Minister of Planning and Strategic Investment.\nIn the 2017 parliamentary elections in East Timor, Gusm\u00e3o succeeded in entering parliament as the CNRT's list leader. However, the CNRT suffered heavy losses and came a close second behind Fretilin. On 4 August 2017, Gusm\u00e3o announced his resignation as CNRT party leader. However, this resignation was not accepted at the extraordinary party congress and was later simply ignored. The CNRT went into opposition, which is why Gusm\u00e3o lost his ministerial post. He also renounced his seat in parliament after the first day of the session.\nIn the border disputes between Australia and East Timor, Gusm\u00e3o worked as East Timor's chief negotiator. After the successful conclusion of the new Timor Sea border treaty on 6 March 2018, he received a triumphant reception and a hero's welcome from thousands of East Timorese on his return to Dili. In the 2018 general elections, Gusm\u00e3o represented the CNRT in the Alliance for Change and Progress (AMP) trio and entered parliament at number one on the list. However, he renounced his mandate already for the first session on 13 June. On 5 July, Gusm\u00e3o was appointed Minister of State of the Council of the Prime Minister and Minister of Planning and Strategic Investment by President Francisco Guterres. Due to the conflict with the President of the Republic over the appointment of CNRT ministers, Gusm\u00e3o stayed away from the scheduled dates of his swearing-in ceremony and eventually renounced his position in the VIII Government. However, he continued to have responsibility for the Maritime Boundaries Office and continued negotiations with Australia until 2022. On 18 December 2019, Gusm\u00e3o was also appointed by Cabinet as the Blue Economy Representative.\nIn the 2022 East Timor presidential elections, Gusm\u00e3o ran Ramos-Horta as the CNRT candidate. Gusm\u00e3o played a central role in the election campaign, pushing Ramos-Horta into the background. In the event of an election, Gusm\u00e3o announced that Ramos-Horta would dissolve parliament and call early elections. Ramos-Horta was more cautious about the matter and instead announced that he wanted to hold talks with all parties. On 20 May 2022, Ramos-Horta took up his second term as president.\nIn October 2022, several families in Dili's Aimeti Laran and Becusi Craic neighbourhoods were to be evicted from their homes. The landowner had enforced this in court, while the families justified their right to live there by saying they had been living there for decades. A team from the District Court and the National Police had already removed the belongings of seven families in Becusi Craic when Xanana Gusm\u00e3o intervened with media attention. He ordered the police officers to carry the families' belongings back into the houses and waited until they had finished the job. As a result, Judge Zulmira Auxiliadora Barros da Silva, who had ordered the evictions, was publicly defamed. The events became known as the \"Aimeti Laran case\" and the \"Becussi Craic\". In April 2023, the Conselho Superior da Magistratura Judicial (CSMJ) issued a press release expressing regret for the judge's \"campaign of professional disparagement\" and criticising the \"total obstruction\" of the execution of the sentence with the media present. The CSMJ concluded that the judge had acted correctly, declared its solidarity with the judicial officials involved and insisted on the sovereignty of the judiciary.\nIn the 2023 East Timorese parliamentary election, the CNRT won 41% of the votes and gained 31 seats out of 65 in the National Parliament. On 1 July 2023, Gusmao was sworn in as prime minister after his party's victory in the parliamentary election.\nAwards and honours.\nAwards.\nIn 1999, Gusm\u00e3o was awarded the Sakharov Prize for Freedom of Thought.\nIn 2000, he was awarded the Sydney Peace Prize for being a \"courageous and principled leader for the independence of the East Timorese people\".\nAlso in 2000, he won the first Gwangju Prize for Human Rights, created to honour \"individuals, groups or institutions in Korea and abroad that have contributed in promoting and advancing human rights, democracy and peace through their work.\"\nIn 2002, he was awarded the North\u2013South Prize by the Council of Europe.\nGusm\u00e3o is an Eminent Member of the S\u00e9rgio Vieira de Mello Foundation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "55987", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=55987", "title": "Euphrates River", "text": ""}
{"id": "55988", "revid": "4200277", "url": "https://en.wikipedia.org/wiki?curid=55988", "title": "Tell Abu Hureyra", "text": "Archaeological site in Syria\nTell Abu Hureyra () is a prehistoric archaeological site in the Upper Euphrates valley in Syria. The tell was inhabited between 13,300 and 7,800 cal. BP in two main phases: Abu Hureyra 1, dated to the Epipalaeolithic, was a village of sedentary hunter-gatherers; Abu Hureyra 2, dated to the Pre-Pottery Neolithic, was home to some of the world's first farmers. This almost continuous sequence of occupation through the Neolithic Revolution has made Abu Hureyra one of the most important sites in the study of the origins of agriculture.\nThe site is significant because the inhabitants of Abu Hureyra started out as hunter-gatherers but gradually moved to farming, making them the earliest known farmers in the world. Cultivation started at the beginning of the Younger Dryas period at Abu Hureyra. Evidence uncovered at Abu Hureyra suggests that rye was the first cereal crop to be systematically cultivated. In light of this, it is now believed that the first systematic cultivation of cereal crops was around 13,000 years ago.\nDuring the Late Glacial Interstadial, Abu Hureyra site experienced climatic change. Due to lake level changes and aridity, the vegetation expanded into lower areas of the fields. Abu Hureyra accumulated vegetation that consisted of grasses, oaks, and \"Pistacia atlantica\" trees. The climate changed from warm and dry months to abruptly cold and dry months.\nHistory of research.\nThe site was excavated as a rescue operation before it was flooded by Lake Assad, the reservoir of the Tabqa Dam which was being built at that time. The site was excavated by Andrew M. T. Moore in 1972 and 1973. It was limited to only two seasons of fieldwork. Despite the limited time frame, a large amount of material was recovered and studied over the following decades. It was one of the first archaeological sites to use modern methods of excavation such as \"flotation\", which preserved even the tiniest and most fragile plant remains. A preliminary report was published in 1983 and a final report in 2000.\nSince around 2012 Moore and others have published several papers reporting on meltglass, nanodiamonds, microspherules, and charcoal and high concentrations of iridium, platinum, nickel, and cobalt from the site of Abu Hureyra, which they attribute to an impact event that destroyed the village around 10,800 BC. The possible connexion of this impact hypothesis with the Younger Dryas has been rejected by experts in archaeology and impact science.\nLocation and description.\nAbu Hureyra is a tell, or ancient settlement mound, in modern-day Raqqa Governorate in northern Syria. It is on a plateau near the south bank of the Euphrates, east of Aleppo. The tell is a massive accumulation of collapsed houses, debris, and lost objects accumulated over the course of the habitation of the ancient village. The mound is nearly across, deep, and contained over of archaeological deposits. Today the tell is inaccessible, submerged beneath the waters of Lake Assad.\nOccupation history.\nFirst occupation (c. 11,300-9,400 BCE).\nThe village of Abu Hureyra had two separate periods of occupation: An Epipalaeolithic settlement and a Neolithic settlement. The Epipaleolithic, or Natufian, settlement was established c. 13,300 years ago and lasted until 11,400 BP. During the first settlement, the village consisted of small round huts, cut into the soft sandstone of the terrace. The roofs were supported with wooden posts, and roofed with brushwood and reeds. Huts contained underground storage areas for food. The houses that they lived in were subterranean pit dwellings. The inhabitants are probably most accurately described as \"hunter-collectors\", as they didn't only forage for immediate consumption, but built up stores for longterm food security. They settled down around their larder to protect it from animals and other humans. From the distribution of wild food plant remains found at Abu Hureyra it seems that they lived there year-round. The population was small, housing a few hundred people at most\u2014but perhaps the largest collection of people permanently living in one place anywhere at that time.\nThe inhabitants of Abu Hureyra obtained food by hunting, fishing, and gathering of wild plants. Gazelle was hunted primarily during the summer, when vast herds passed by the village during their annual migration. These would probably be hunted communally, as mass killings also required mass processing of meat, skin, and other parts of the animal. The huge amount of food obtained in a short period was a reason for settling down permanently: it was too heavy to carry and would need to be kept protected from weather and pests.\nOther prey included large wild animals such as onager, sheep, and cattle, and smaller animals such as hare, fox, and birds, which were hunted throughout the year. Different plant species were collected, from three different eco-zones within walking distance (river, forest, and steppe). Plant foods were also harvested from \"wild gardens\" with species gathered including wild cereal grasses such as einkorn wheat, emmer wheat, and two varieties of rye. Several large stone tools for grinding grain were found at the site.\nAbu Hureyra 1 had a variety of crops that made up the system. Their resources consisted of 41% \"Rumex\" and \"Polygonum\", 43% rye and einkorn, and the remaining 16% lentils.\nDepopulation.\nAfter 1,300 years the hunter-gatherers of the first occupation mostly abandoned Abu Hureyra, probably because of the Younger Dryas, an intense and relatively abrupt return to glacial climate conditions which lasted over 1,000 years, or because of the purported impact event. The drought disrupted the migration of the gazelle and destroyed forageable plant food sources. The inhabitants might have moved to Mureybet, less than 50\u00a0km to the northeast on the other side of the Euphrates, which expanded dramatically at this time.\nSecond occupation (ca. 8,600\u20135,800 BCE).\nThe second occupation covers a period from ca. 10,600\u20137,800 cal BP in the Neolithic. In comparison to Abu Hureyra 1, Abu Hureyra 2 had a different accumulation of resources, consisting of 25% \"Rumex\"/\"Polygonum\", 3.7% rye/einkorn, 29% barley, 23.5% emmer, 9.4% wheat-free threshing, and 9.4% lentils.\nIt is from the early part of the Younger Dryas that the first indirect evidence of agriculture was detected in the excavations at Abu Hureyra, although the cereals themselves were still of the wild variety. It was during the intentional sowing of cereals in more favourable refuges like Mureybet that these first farmers developed domesticated strains during the centuries of drought and cold of the Younger Dryas. When the climate abated about 9500 BCE they spread all over the Middle East with this new bio-technology, and Abu Hureyra grew to a large village eventually with several thousand people. The second occupation grew domesticated varieties of rye, wheat and barley, and kept sheep as livestock. The hunting of gazelle decreased sharply, probably due to overexploitation that eventually left them extinct in the Middle East. At Abu Hureyra they were replaced by meat from domesticated animals. The second occupation lasted for about 2,000 years.\nAbu Hureyra 2 was broadly contemporary with K\u00f6rtik Tepe and Tell Mureybet.\nTransition from foraging to farming.\nSome evidence has been found for cultivation of rye from 11050 BCE in the sudden rise of pollen from weed plants that typically infest newly disturbed soil. Peter Akkermans and Glenn Schwartz found this claim about epipaleolithic rye, \"difficult to reconcile with the absence of cultivated cereals at Abu Hureyra and elsewhere for thousands of years afterwards\". It could have been an early experiment that didn't survive and continue. It has been suggested that drier climate conditions resulting from the beginning of the Younger Dryas caused wild cereals to become scarce, leading people to begin cultivation as a means of securing a food supply. Results of recent analysis of the rye grains from this level suggest that they may actually have been domesticated during the Epipalaeolithic. It is speculated that the permanent population of the first occupation was fewer than 200 individuals. These individuals occupied several tens of square kilometers, a rich resource base of several different ecosystems. On this land they hunted, harvested food and wood, made charcoal, and may have cultivated cereals and grains for food and fuel. \nThe first domesticated morphologic cereals came about at the Abu Hureyra site around 10,000 years ago. Amongst the species found at the site, cereals such as rye and emmer wheat are predominant.\nAgriculture.\nThe village of Abu Hureyra had impressive agricultural advances for the time period. The rapid growth of farming led to the development of two different domesticated forms of wheat, barley, rye, lentils, and more due in part to a sudden cool period in the area. The cool period affected the supply of wild animals such as gazelle, which at the time was their main source of protein. Since their food supply became scarce it was critical that they find a way to provide for the population, this led to extensive agricultural efforts as well as the domestication of sheep and goats to provide a steady protein source. Another helpful factor was the ability to grow legumes, which fix nitrogen levels in the soil. This improved the fertility of the soil and allowed for the crop plants to flourish.\nThis massive increase in agriculture had a cost. Those who lived in the village of Abu Hureyra experienced several injuries and skeletal abnormalities. These injuries mostly came from the way the crops were harvested. In order to harvest the crops the people of Abu Hureyra would kneel for several hours on end. The act of kneeling for long durations would put the individuals at risk for injuring the big toes, hips, and lower back. There was cartilage damage in the toe that was so severe the metatarsal bones would rub together. In addition to this injury another common injury was for the last dorsal vertebra to be damaged, crushed, or out of alignment due to the pressure used during the grinding of grains. These skeletal abnormalities also can be found on the teeth of the Abu Hureyra people. Since the grain was stone ground many flakes of stone would still be left in the grain which over time would wear down the teeth. In rare cases women would have large grooves in their front teeth which suggests they used their mouth as a third hand while weaving baskets. This dates basket weaving as far back as 6500 BC and the fact so few women had these grooves shows that basket weaving was a rare skill to have. These baskets were extremely important to the success of the agriculture because the baskets were used to collect or spread seeds, and were also used to collect or distribute water.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55989", "revid": "3499011", "url": "https://en.wikipedia.org/wiki?curid=55989", "title": "Radiological weapon", "text": ""}
{"id": "55992", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=55992", "title": "Tommy Cooper", "text": "Welsh comedian, magician and actor (1921\u20131984)\nThomas Frederick Cooper (19 March 1921 \u2013 15 April 1984) was a Welsh prop comedian and magician. As an entertainer, his appearance was large and lumbering at , and he habitually wore a red fez when performing. He served in the British Army for seven years before developing his conjuring skills and becoming a member of The Magic Circle. Although he spent time on tour performing his magical act, which specialised in magic tricks that appeared to fail, he rose to international prominence when his career moved into television, with programmes for London Weekend Television and Thames Television.\nBy the end of the 1970s, Cooper was smoking and drinking heavily, which affected his career and his health, effectively ending offers to front new programmes and relegating him to performing as a guest star on other entertainment shows. On 15\u00a0April 1984, Cooper died at the age of 63 after suffering a heart attack on live television.\nEarly life.\nThomas Frederick Cooper was born on 19\u00a0March 1921, at 19 Llwyn-On Street in Caerphilly, Glamorgan. He was delivered by the woman who owned the house in which the family were lodging. His parents were Thomas H. Cooper, a Welsh recruiting sergeant in the British Army and later coal miner, and Catherine Gertrude (\"n\u00e9e\" Wright), Thomas's English wife from Crediton, Devon.\nTo change from his mining role in Caerphilly, which could have had implications for his health, his father accepted the offer of a new job and the family moved to Exeter, Devon, when Cooper was three. It was in Exeter that he acquired the West Country accent that became part of his act. As an adult and on a visit to Wales to visit the house where he was born, Cooper was asked if he considered himself to be a Welshman, to which he answered, \"Well yes, my father's Welsh... and my mother's from Devon. Actually I was in Caerphilly and left here when I was about a year old, I was getting very serious with a girl\", much to the amusement of the BBC interviewer and himself.\nWhen he was eight years old an aunt bought him a magic set and he spent hours perfecting the tricks. In the 1960s his brother David (born 1930) opened D. &amp; Z. Cooper's Magic Shop at 249 High Street in Slough, Buckinghamshire. The shop later moved to Eastbourne, East Sussex and was run by David's daughter Sabrina. After leaving school, Cooper became a shipwright in Southampton, Hampshire. In 1940 he was called up as a trooper in the Royal Horse Guards, serving for seven years. He joined Montgomery's Desert Rats in Egypt. Cooper became a member of a Navy, Army and Air Force Institutes (NAAFI) entertainment party, and developed an act around his magic tricks interspersed with comedy. One evening in Cairo, during a sketch in which he was supposed to be in a costume that required a pith helmet, having forgotten the prop Cooper reached out and borrowed a fez from a passing waiter, which got huge laughs. He wore a fez when performing after that, the prop later being described as \"an icon of 20th-century comedy\".\nDevelopment of the act.\nCooper was demobilised after seven years of military service and took up show business on Christmas Eve 1947. He later developed a popular monologue about his military experience as \"Cooper the Trooper\". He worked in variety theatres around the country and at many night spots in London, performing as many as 52 shows in one week.\nCooper developed his conjuring skills and became a member of The Magic Circle, but there are various stories about how and when he developed his delivery of \"failed\" magic tricks:\nTo keep the audience on their toes Cooper threw in an occasional trick that worked when it was least expected.\nCareer.\nCooper was influenced by Laurel and Hardy, Will Hay, Max Miller, Bob Hope, and Robert Orben.\nIn 1947 Cooper was booked by Miff Ferrie, a musician, to appear in a show starring the sand dance act Marqueeze and the Dance of the Seven Veils. This was followed by a European tour and work in pantomime, and concluded with a season at the Windmill Theatre. Ferrie remained Cooper's sole agent for 37 years, until Cooper's death in 1984.\nCooper rapidly became a top-liner in variety with his turn as the conjurer whose tricks never succeeded, but it was his television work that raised him to national prominence. After his debut on the BBC talent show \"New to You\" in March 1948 he began starring in his own shows, and was popular with audiences for nearly 40 years, notably through his work with London Weekend Television from 1968 to 1972 and with Thames Television from 1973 to 1980. Thanks to his many television shows during the mid-1970s he was one of the most recognisable comedians in the world.\nJohn Fisher writes in his biography of Cooper: \"Everyone agrees that he was mean. Quite simply he was acknowledged as the tightest man in show business, with a pathological dread of reaching into his pocket.\" One of Cooper's stunts was to pay the exact taxi fare and when leaving the cab slip something into the taxi driver's pocket, saying, \"Have a drink on me.\" That something would turn out to be a tea bag.\nBy the mid-1970s alcohol had started to erode Cooper's professionalism and club owners complained that he turned up late or rushed through his show in five minutes. In addition he suffered from chronic indigestion, lumbago, sciatica, bronchitis and severe circulation problems in his legs. When Cooper realised the extent of his maladies he cut down on his drinking, and the energy and confidence returned to his act. However, he never stopped drinking and could be fallible: on an otherwise triumphant appearance with Michael Parkinson he forgot to set the safety catch on the guillotine illusion into which he had cajoled Parkinson, and only a last-minute intervention by the floor manager saved Parkinson from serious injury or worse.\nCooper was a heavy cigar smoker (up to 40 a day) as well as an excessive drinker. He suffered a heart attack on 22 April 1977 while performing a show in Rome. Three months later he was back on television in \"Night Out at the London Casino\".\nBy 1980 his drinking meant that Thames Television would not give him another starring series, and \"Cooper's Half Hour\" was his last. He did continue to appear as a guest on other television shows, however, and worked with Eric Sykes on two Thames productions in 1982.\nPersonal life.\nCooper married Gwen Henty in Nicosia, Cyprus, on 24\u00a0February 1947. She died in 2002. They had two children: Thomas, who was born in 1956, became an actor under the name Thomas Henty and died in 1988; and Victoria.\nFrom 1967 until his death, Cooper also had a relationship with his personal assistant, Mary Fieldhouse (aka Mary Kay, the wife of composer Norman Kay), who wrote about it in her book, \"For the Love of Tommy\" (1986).\nCooper's will was proved via probate on 29\u00a0August 1984, at \u00a3327,272.\nOn Christmas Day 2018, the documentary \"Tommy Cooper: In His Own Words\" was broadcast on Channel 5. The programme featured Cooper's daughter, Vicky, who gave her first television interview following years of abstaining \"because of the grief\".\nDeath.\nOn 15 April 1984, Cooper collapsed from a heart attack in front of 12\u00a0million viewers, midway through his act on the London Weekend Television variety show \"Live from Her Majesty's\", transmitted live from Her Majesty's Theatre in Westminster, London. An assistant had helped him put on a cloak for his sketch, while Jimmy Tarbuck, the host, was hiding behind the stage curtains waiting to pass him different props that he would then appear to pull from inside his gown. His last words seemed to be \"Thank you, love\", to the assistant seconds before collapsing. The assistant smiled at him as he slumped down, believing that it was part of the act. Likewise, the audience laughed as he fell backwards, as a hand (possibly Tarbuck's hand) briefly appeared from behind the curtain to reach out towards Cooper.\nAs Cooper lay dying on the floor, the audience continued to laugh at him, believing he was making a joke about how long it had taken him to button up the cloak he had on, before Tarbuck, director Alasdair MacMillan, and crew behind the curtain who witnessed the incident realised that Cooper had genuinely collapsed. The laughter from the audience began to die down as they realised Cooper was unable to get back up.\nIn the wings, show producer David Bell asked Cooper's son if the fall was part of the act. He replied that his father had a bad back, and thus would be unable to get back up if he fell on purpose. After it became apparent that Cooper was in trouble, Alasdair MacMillan cued the orchestra to play music for an unscripted commercial break (noticeable because of several seconds of blank screen while LWT's master control contacted regional stations to start transmitting advertisements) and Tarbuck's manager tried to pull Cooper back through the curtains.\nIt was decided to continue with the show. Dustin Gee and Les Dennis were the act that had to follow Cooper and performed in the limited space in front of the curtains. Two stools were positioned either side of the protrusion from behind the curtain where Cooper had collapsed, whilst efforts were being made to revive him. The following act, Howard Keel, performed as Cooper was moved (evident by the twitching of the curtains as he sang and the disappearance of the protrusion as he finished performing). After another commercial break, the curtain was removed, and he was taken by ambulance to Westminster Hospital, where he was pronounced dead on arrival at 63 years old. His death was not officially reported until the next morning, although the incident was the leading item on the news programme that followed the show.\nCooper's funeral was held at Mortlake Crematorium in London, and his son scattered his ashes in the back garden, over his father's favourite daffodils. There are memorials to Cooper, his wife Gwen, and their son Thomas, on his wife's family grave at Ocklynge Cemetery, Eastbourne, East Sussex.\nThe video of Cooper's heart attack on stage has been uploaded to numerous video-sharing websites. YouTube drew criticism from a number of sources when footage of the incident was posted on the website in May 2009. John Beyer of the pressure group Mediawatch-UK said: \"This is very poor taste. That the broadcasters have not repeated the incident shows they have a respect for him and I think that ought to apply also on YouTube.\" On 28\u00a0December 2011, segments of the \"Live from Her Majesty's\" clip, including Cooper collapsing on stage, were included in the Channel 4 programme \"The Untold Tommy Cooper\".\nLegacy and honours.\nA statue of Cooper was unveiled in his birthplace, Caerphilly, in 2008 by Sir Anthony Hopkins, who is patron of the Tommy Cooper Society. The statue, which cost \u00a345,000, was sculpted by James Done. In 2009, for Red Nose Day, a charity Red Nose was put on the statue, but the nose was stolen.\nCooper was a member of the Grand Order of Water Rats.\nIn a 2005 poll, The Comedians' Comedian, comedians and comedy insiders voted Cooper the sixth greatest comedy act ever. He has been cited as an influence by Jason Manford and John Lydon. Jerome Flynn has toured with his own tribute show to Cooper called \"Just Like That\".\nIn February 2007 \"The Independent\" reported that Andy Harries, a producer of \"The Queen\", was working on a dramatisation of the last week of Cooper's life. Harries described Cooper's death as \"extraordinary\" in that the whole thing was broadcast live on national television. The film subsequently went into production over six years later as a television drama for ITV. From a screenplay by Simon Nye, \"\" was directed by Benjamin Caron and the title role was played by David Threlfall. It was broadcast 21\u00a0April 2014.\nIn 2010 Cooper was portrayed by Clive Mantle in a stage show, \"Jus' Like That! A Night Out with Tommy Cooper\", at the Edinburgh Festival. To train for the role Mantle mastered many of Cooper's magic tricks, studying under Geoffrey Durham for several months.\nIn 2012 the British Heart Foundation ran a series of advertisements featuring Cooper to raise awareness of heart conditions. These included posters bearing his image together with radio commercials featuring classic Cooper jokes.\n\"Being Tommy Cooper\", a new play written by Tom Green and starring Damian Williams, was produced by Franklin Productions and toured the UK in 2013.\nIn 2014, with the support of The Tommy Cooper Estate and Cooper's daughter Victoria, a new tribute show, \"Just Like That! The Tommy Cooper Show\", commemorating 30 years since the comedian's death was produced by Hambledon Productions. The production moved to the Museum of Comedy in Bloomsbury, London, from September 2014 and continues to tour extensively throughout the UK.\nIn May 2016, a blue plaque in memory of Cooper was unveiled at his former home in Barrowgate Road, Chiswick. It was announced in August that the Victoria and Albert Museum had acquired 116 boxes of Cooper's papers and props, including his \"gag file\", in which the museum said he had used a system to store his jokes alphabetically \"as meticulous as an archivist\".\nOn 5 March 2021, BBC One aired the 30-minute documentary \"Tommy Cooper at the BBC\", looking at his best performances, including his appearance on the \"Parkinson\" show where he almost killed Michael Parkinson with a trick guillotine. The programme, which celebrated the centenary of his birth, was presented by Sir Lenny Henry.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55994", "revid": "2", "url": "https://en.wikipedia.org/wiki?curid=55994", "title": "M\u00e9ni\u00e8re's disease", "text": "Disorder of the inner ear\nMedical condition&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nM\u00e9ni\u00e8re's disease (MD) is a disease of the inner ear characterized by potentially severe and incapacitating episodes of vertigo, tinnitus, hearing loss, and a feeling of fullness in the ear. Typically, only one ear is affected initially, but over time, both ears may become involved. Episodes generally last from 20 minutes to a few hours, with varying time between episodes. The hearing loss and ringing in the ears can become constant over time. M\u00e9ni\u00e8re's disease was identified in the early 1800s by Prosper Meni\u00e8re.\nThe cause of M\u00e9ni\u00e8re's disease is unclear, but likely involves both genetic and environmental factors. A number of theories exist for why it occurs, including constrictions in blood vessels, viral infections, and autoimmune reactions. About 10% of cases run in families. Symptoms are believed to occur as the result of increased fluid buildup in the labyrinth of the inner ear. Diagnosis is based on the symptoms and a hearing test. Other conditions that may produce similar symptoms include vestibular migraine and transient ischemic attack.\nNo cure is known. Attacks are often treated with medications to help with the nausea and anxiety. Measures to prevent attacks are overall poorly supported by the evidence. A low-salt diet, diuretics, and corticosteroids may be tried. Physical therapy may help with balance and counselling may help with anxiety. Injections into the ear or surgery may also be tried if other measures are not effective, but are associated with risks. The use of tympanostomy tubes (ventilation tubes) to improve vertigo and hearing in people with M\u00e9ni\u00e8re's disease is not supported by definitive evidence.\nThe disease affects between 0.3 and 1.9 per 1,000 people. The onset of M\u00e9ni\u00e8re's disease is usually around 40 to 60 years old. Females are more commonly affected than males. After 5\u201315 years of symptoms, episodes that include dizziness or a sensation of spinning sometimes stop and the person is left with loss of balance, poor hearing in the affected ear, and ringing or other sounds in the affected ear or ears.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nSigns and symptoms.\nM\u00e9ni\u00e8re's is characterized by recurrent episodes of vertigo, fluctuating hearing loss, and tinnitus; episodes may be preceded by a headache and a feeling of fullness in the ears. People may also experience additional symptoms related to irregular reactions of the autonomic nervous system. These symptoms are not symptoms of M\u00e9ni\u00e8re's disease per se, but rather are side effects resulting from failure of the organ of hearing and balance, and include nausea, vomiting, and sweating, which are typically symptoms of vertigo, and not of M\u00e9ni\u00e8re's. This includes a sensation of being pushed sharply to the floor from behind. Sudden falls without loss of consciousness (drop attacks) may be experienced by some people.\nCauses.\nThe cause of M\u00e9ni\u00e8re's disease is unclear, but likely involves both genetic and environmental factors. A number of theories exist including constrictions in blood vessels, viral infections, and autoimmune reactions.\nMechanism.\nThe initial triggers of M\u00e9ni\u00e8re's disease are not fully understood, with a variety of potential inflammatory causes that lead to endolymphatic hydrops (EH), a distension of the endolymphatic spaces in the inner ear. Endolymphatic hydrops is strongly associated with developing M\u00e9ni\u00e8re's disease, but not everyone with EH develops M\u00e9ni\u00e8re's disease: \"The relationship between endolymphatic hydrops and Meniere's disease is not a simple, ideal correlation.\" Notably, mild EH can also occur in vestibular migraine which is an important differential diagnosis for M\u00e9ni\u00e8re's disease.&lt;ref name=\"DOI10.1136/jnnp-2024-334419\"&gt;V. Kirsch, Rainer Boegle, J. Gerb, E. Kierig, Birgit Ertl\u2010Wagner, Sandra Becker\u2010Bense, Thomas Brandt, Marianne Dieterich: \"Imaging endolymphatic space of the inner ear in vestibular migraine.\" In: \"Journal of Neurology Neurosurgery &amp; Psychiatry.\" 2024, S.\u00a0jnnp\u2013334419 .&lt;/ref&gt;\nAdditionally, in fully developed M\u00e9ni\u00e8re's disease, the balance system (vestibular system) and the hearing system (cochlea) of the inner ear are affected, but some cases occur where EH affects only one of the two systems enough to cause symptoms. The corresponding subtypes of the disease are called vestibular M\u00e9ni\u00e8re's disease, showing symptoms of vertigo, and cochlear M\u00e9ni\u00e8re's disease, showing symptoms of hearing loss and tinnitus.\nThe mechanism of M\u00e9ni\u00e8re's disease is not fully explained by EH, but fully developed EH may mechanically and chemically interfere with the sensory cells for balance and hearing, which can lead to temporary dysfunction and even to death of the sensory cells, which in turn can cause the typical symptoms of MD \u2013 vertigo, hearing loss, and tinnitus.\nAn estimated 30% of people with M\u00e9ni\u00e8re's disease have Eustachian tube dysfunction.\nDiagnosis.\nThe diagnostic criteria as of 2015 define definite MD and probable MD as:\nDefinite\nProbable\nA common and important symptom of MD is hypersensitivity to sounds. This hypersensitivity is easily diagnosed by measuring the loudness discomfort levels (LDLs).\nSymptoms of MD overlap with migraine-associated vertigo (MAV) in many ways, but when hearing loss develops in MAV, it is usually in both ears, and this is rare in MD, and hearing loss generally does not progress in MAV as it does in MD.\nPeople who have had a transient ischemic attack (TIA) or stroke can present with symptoms similar to MD, and in people at risk magnetic resonance imaging should be conducted to exclude TIA or stroke.\nOther vestibular conditions that should be excluded include vestibular paroxysmia, recurrent unilateral vestibulopathy, vestibular schwannoma, or a tumor of the endolymphatic sac.\nManagement.\nNo cure for M\u00e9ni\u00e8re's disease is known, but medications, diet, physical therapy, counseling, and some surgical approaches can be used to manage it. More than 85% of patients with M\u00e9ni\u00e8re's disease get better from changes in lifestyle, medical treatment, or minimally invasive surgical procedures. Those procedures include intratympanic (into the cavity behind the eardrum, known as the tympanic cavity) steroid therapy, intratympanic gentamicin therapy or endolymphatic sac surgery.\nMedications.\nDuring MD episodes, medications to reduce nausea are used, as are drugs to reduce the anxiety caused by vertigo. For longer-term treatment to stop progression, the evidence base is weak for all treatments. Although a causal relation between allergy and M\u00e9ni\u00e8re's disease is uncertain, medication to control allergies may be helpful. To assist with vertigo and balance problems, glycopyrrolate has been found to be a useful vestibular suppressant in patients with M\u00e9ni\u00e8re's disease.\nDiuretics, such as the thiazide-like diuretic chlortalidone, are widely used to manage MD on the theory that it reduces fluid buildup (pressure) in the ear. Based on evidence from multiple but small clinical trials, diuretics appear to be useful for reducing the frequency of episodes of dizziness but do not seem to prevent hearing loss.\nIn cases where hearing loss and continuing severe episodes of vertigo occur, a chemical labyrinthectomy, in which a medication such as gentamicin is injected into the middle ear and kills parts of the vestibular apparatus, may be prescribed. This treatment has the risk of worsening hearing loss.\nDiet.\nPeople with MD are often advised to reduce their sodium intake. Reducing salt intake, however, has not been well studied. Based on the assumption that MD is similar in nature to a migraine, some advise eliminating \"migraine triggers\" such as caffeine, but the evidence for this is weak. There is no high-quality evidence that changing diet by restricting salt, caffeine or alcohol improves symptoms.\nPhysical therapy.\nWhile use of physical therapy early after the onset of MD is probably not useful due to the fluctuating disease course, physical therapy to help retraining of the balance system appears to be useful to reduce both subjective and objective deficits in balance over the longer term.\nCounseling.\nThe psychological distress caused by the vertigo and hearing loss may worsen the condition in some people. Counseling may be useful to manage the distress, as may education and relaxation techniques.\nSurgery.\nIf symptoms do not improve with less invasive approaches and for cases where the condition is uncontrolled or persistent and affecting both ears, surgery may be considered.\nEndolymphatic sac surgery.\nSurgery to decompress the endolymphatic sac is one surgical approach that is sometimes suggested. Three methods of surgical endolymphatic sac decompression are sometimes suggested \u2013 simple decompression, insertion of a shunt, or removal of the sac. There is some very weak evidence that all three methods may be useful for reducing dizziness, but that the level of evidence supporting these surgical procedures is low with further higher quality investigations being suggested. There is a risk in these types of surgical procedures that the shunts used in these surgeries are at risk of becoming displaced or misplaced. For those with severe cases who are eligible for endolymphatic sac decompression, a 2014 systematic review reported that in at least 75% of people, EL sac decompression was effective at controlling vertigo in the short term (&gt;1 year of follow-up) and long term (&gt;24 months).\nVentilation tubes.\nSurgical implantation of eustachian tubes (ventilation tubes) is not strongly supported by medical studies. There are some tentative evidence of benefit from tympanostomy tubes for improvement in the unsteadiness associated with the disease, conclusions about how effective this surgery is and the potential for side effects and harms is not clear.\nOther surgical interventions.\nDestructive surgeries such as vestibular nerve labyrinthectomy are irreversible and involve removing entire functionality of most, if not all, of the affected ear; as of 2013, almost no evidence existed with which to judge whether these surgeries are effective. The inner ear itself can be surgically removed via labyrinthectomy, although hearing is always completely lost in the affected ear with this operation. The surgeon can also cut the nerve to the balance portion of the inner ear in a vestibular neurectomy. The hearing is often mostly preserved; however, the surgery involves cutting open into the lining of the brain, and a hospital stay of a few days for monitoring is required.\nPrognosis.\nM\u00e9ni\u00e8re's disease usually starts confined to one ear; it extends to both ears in about 30% of cases. People may start out with only one symptom, but in M\u00e9ni\u00e8re's disease all three appear with time. Hearing loss usually fluctuates in the beginning stages and becomes more permanent in later stages. M\u00e9ni\u00e8re's disease has a course of 5\u201315 years, and people generally end up with mild disequilibrium, tinnitus, and moderate hearing loss in one ear.\nAs of 2020, there has been no recent major breakthrough in the pathogenesis research of M\u00e9ni\u00e8re's disease.\nEpidemiology.\nFrom 3 to 11% of diagnosed dizziness in neuro-otological clinics are due to M\u00e9ni\u00e8re's disease. The annual incidence rate is estimated to be about 15 cases per 100,000 people and the prevalence rate is about 218 per 100,000, and around 15% of people with M\u00e9ni\u00e8re's disease are older than 65. In around 9% of cases, a relative also had M\u00e9ni\u00e8re's disease, indicating a genetic predisposition in some cases.\nThe odds of M\u00e9ni\u00e8re's disease are greater for people of white ethnicity, with severe obesity, and women. Several conditions are often comorbid with M\u00e9ni\u00e8re's disease, including arthritis, psoriasis, gastroesophageal reflux disease, irritable bowel syndrome, and migraine.\nHistory.\nThe condition is named after the French physician Prosper Meni\u00e8re, who in an 1861 article described the main symptoms and was the first to suggest a single disorder for all of the symptoms, in the combined organ of balance and hearing in the inner ear.\nThe American Academy of Otolaryngology\u00a0\u2013 Head and Neck Surgery Committee on Hearing and Equilibrium set criteria for diagnosing MD, as well as defining two subcategories \u2013 cochlear (without vertigo) and vestibular (without deafness).\nIn 1972, the academy defined criteria for diagnosing MD as:\nIn 1985, this list changed to alter wording, such as changing \"deafness\" to \"hearing loss associated with tinnitus, characteristically of low frequencies\" and requiring more than one attack of vertigo to diagnose. Finally in 1995, the list was again altered to allow for degrees of the disease:\nIn 2015, the International Classification for Vestibular Disorders Committee of the Barany Society published consensus diagnostic criteria in collaboration with the American Academy of Otolaryngology\u2013Head and Neck Surgery, the European Academy of Otology and Neurootology, the Japan Society for Equilibrium Research, and the Korean Balance Society.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55995", "revid": "1383976", "url": "https://en.wikipedia.org/wiki?curid=55995", "title": "K. M. Peyton", "text": "British author (1929\u20132023)\nKathleen Wendy Herald Peyton (2 August 1929 \u2013 19 December 2023), who wrote primarily as K. M. Peyton, was a British author of fiction for children and young adults in the 1960s and 1970s. \nPeyton wrote more than fifty novels in the including the \"Ruth Hollis\" series, the \"Pennington\" series, and the \"Flambards\" series, the latter about the Russell family which spanned the period before and after the First World War. For the \"Flambards\" series, Peyton won both the 1969 Carnegie Medal from the Library Association and the 1970 Guardian Children's Fiction Prize, judged by a panel of British children's writers. In 1979, the Flambards trilogy was adapted by Yorkshire Television as a 13-part TV series, \"Flambards\", starring Christine McKenna as the heroine Christina Parsons.\nPersonal life and education.\nKathleen Wendy Herald Peyton was born on 2 August 1929 in Birmingham, England. Peyton began writing when she was nine-years-old and was first published when she was fifteen. Peyton has stated that she \"never decided to become a writer...[she] just was one.\" Growing up in London, she could not own a horse, and instead developed an obsession with them\u2014all her early books are about young girls who have ponies. In 1950, Peyton published her first novel \"Sabre, the Horse from the Sea\", illustrated by British artist Lionel Edwards.\nLater, she attended Kingston Art School, and then Manchester Art School. It was there that she met another student, Mike Peyton, an ex-serviceman who had been a military artist and prisoner of war. He shared her love of walking in the Pennines. They married in 1950, and travelled around Europe. When they returned to Britain, Peyton completed a Teaching Diploma and taught for three years at Northampton High School. \nCareer.\nAs a secondary school teacher, Peyton started writing young boys' adventure stories and sold them as serials to \"The Scout\", a magazine owned by The Scout Association. These stories were later published in full-length. During this time, she began writing under the name K.M. Peyton\u2014the 'M' represented her husband Mike who helped create the plots of her stories. Peyton soon left her teaching career, in pursuit of becoming a full-time writer. \nAfter the birth of her two daughters, Peyton started writing fiction novels and returned to the topic of her \"first love\"\u2014ponies, horses and equestrianism. These ideas are explored in the two book series: \"Flambards\" and \"Ruth Hollis\". Later in life, Peyton became involved in horse racing and used her own personal experiences of owning horses as further inspiration for her writing. \nIn association with the Oxford University Press, Peyton's novels were illustrated by artist Victor G. Ambrus in the late 1960s. Peyton was an artist herself and self-illustrated a few of her own novels as well. During the 1970s, her best-selling series \"Flambards\" was published in multiple languages, such as Italian, German, Finnish, and Swedish. \nWriters who cite K.M. Peyton as an influence include Linda Newbery, whose young adult novel \"The Damage Done\" (2001, Scholastic) is dedicated \"to Kathleen Peyton, who made me want to try.\"\nDeath.\nPeyton died on 19 December 2023, at the age of 94. Peyton had two daughters, Hilary and Veronica.\nHonours and awards.\nPeyton won the Guardian Prize for the Flambards trilogy, and won the Carnegie Medal for its second book. She was also a commended runner-up for the Carnegie Medal six times in eight years during the 1960s\u2014one of the books was the first Flambards book, another was the third Flambards book in competition with the Medal-winning second. The others were \"Windfall\" (1962), \"The Maplin Bird\" (1964), \"The Plan for Birdmarsh\" (1965), and \"Thunder in the Sky\" (1966).\nPeyton was awarded Member of the Order of the British Empire (MBE) in the 2014 New Year Honours for services to children's literature.\nAdaptations.\nThe \"Flambards\" trilogy was adapted by Yorkshire Television in 1978. The TV miniseries, \"Flambards\", starring Christine McKenna as the heroine Christina Parsons, comprised 13 episodes. The miniseries was broadcast in the UK in 1979, and eventually the US in 1980. \nPeyton's \"The Right-Hand Man\" (1977), a historical novel featuring an English stagecoach driver, was adapted into a feature film. In 1985, it was shot in Australia and was later released in 1987.\n\"A Pattern of Roses\" (1972) was adapted in 1983 as a TV film, introducing Helena Bonham Carter in her first screen role. \n\"Who, Sir? Me, Sir?\" (1985) was adapted as a BBC TV series.\nWorks.\nThe bibliography of Peyton's \"pony books only\" by Jane Badger Books includes all nineteen series books and many \"other books\" (\u2021) listed here.\nFlambards Series (1967\u20131981).\nPeyton's extension of the trilogy followed its television adaptation and reversed the original ending.\nRuth Hollis Series (1968\u20131979).\nThe Pennington series continues the story of Ruth Hollis in 1971.\nJonathan Meredith Series (1977\u20131984).\nSee also the Ruth Hollis series: Jonathan Meredith is a minor character in \"The Team\".\nMinna Series (2007\u20132009).\nSet in Roman Britain.\nOther books.\n\u00a7 By age fifteen, Kathleen Herald had written \"about ten more\" novels that publishers rejected with \"very nice letters\".\n\u2021 Jane Badger Books lists these titles among Peyton's \"pony books only\" \u2013 as well as all nineteen series books listed above.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "55998", "revid": "10289486", "url": "https://en.wikipedia.org/wiki?curid=55998", "title": "Prosper Meni\u00e8re", "text": "French doctor (1799\u20131862)\nProsper Meni\u00e8re (18 June 1799 \u2013 7 February 1862) was a French medical doctor who first identified that the inner ear could be the source of a condition combining vertigo, hearing loss and tinnitus, which is now known as M\u00e9ni\u00e8re's disease.\nBiography.\nMeni\u00e8re was born in Angers, France. During his education he excelled at humanities and classics. He completed his medical studies at H\u00f4tel-Dieu de Paris in 1826, and earned his M.D. in 1828. He then assisted Guillaume Dupuytren.\nMeni\u00e8re was originally set to be an assistant professor in faculty, but political tensions disturbed his professorship and he was sent to control the spread of cholera. He received a legion of honor for his work, but never gained professorship. After securing the position of physician-in-chief at the Institute for deaf-mutes, he focused on the diseases of the ear.\nMeni\u00e8re's studies at the deaf-mute institute helped formulate his paper, \"On a particular kind of hearing loss resulting from lesions of the inner ear\" which ultimately led to the recognition of M\u00e9ni\u00e8re's disease.\nThere is debate as to how Meni\u00e8re's name is spelled. Prosper himself was known to write his name as \"Meni\u00e8re\" while his son used the spelling \"M\u00e9ni\u00e8re.\" Many people omit the accent marks.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "55999", "revid": "14703151", "url": "https://en.wikipedia.org/wiki?curid=55999", "title": "Tongue", "text": "Muscular organ in the mouth of most vertebrates\nThe tongue is a muscular organ in the mouth of a typical tetrapod. It manipulates food for chewing and swallowing as part of the digestive process, and is the primary organ of taste. The tongue's upper surface (dorsum) is covered by taste buds housed in numerous lingual papillae. It is sensitive and kept moist by saliva and is richly supplied with nerves and blood vessels. The tongue also serves as a natural means of cleaning the teeth. A major function of the tongue is to enable speech in humans and vocalization in other animals.\nThe human tongue is divided into two parts, an oral part at the front and a pharyngeal part at the back. The left and right sides are also separated along most of its length by a vertical section of fibrous tissue (the lingual septum) that results in a groove, the median sulcus, on the tongue's surface.\nThere are two groups of glossal muscles. The four intrinsic muscles alter the shape of the tongue and are not attached to bone. The four paired extrinsic muscles change the position of the tongue and are anchored to bone.\nEtymology.\nThe word tongue derives from the Old English \"tunge\", which comes from Proto-Germanic *\"tung\u014dn\". It has cognates in other Germanic languages\u2014for example \"tonge\" in West Frisian, \"tong\" in Dutch and Afrikaans, \"Zunge\" in German, \"tunge\" in Danish and Norwegian, and \"tunga\" in Icelandic, Faroese and Swedish. The \"ue\" ending of the word seems to be a fourteenth-century attempt to show \"proper pronunciation\", but it is \"neither etymological nor phonetic\". Some used the spelling \"tunge\" and \"tonge\" as late as the sixteenth century.\nIn humans.\nStructure.\nThe tongue is a muscular hydrostat that forms part of the floor of the oral cavity. The left and right sides of the tongue are separated by a vertical section of fibrous tissue known as the lingual septum. This division is along the length of the tongue save for the very back of the pharyngeal part and is visible as a groove called the median sulcus. The human tongue is divided into anterior and posterior parts by the terminal sulcus, which is a \"V\"-shaped groove. The apex of the terminal sulcus is marked by a blind foramen, the foramen cecum, which is a remnant of the median thyroid diverticulum in early embryonic development. The anterior \"oral\" part is the visible part situated at the front and makes up roughly two-thirds the length of the tongue. The posterior \"pharyngeal\" part is the part closest to the throat, roughly one-third of its length. These parts differ in terms of their embryological development and nerve supply.\nThe anterior tongue is, at its apex, thin and narrow. It is directed forward against the lingual surfaces of the lower incisor teeth. The posterior part is, at its root, directed backward, and connected with the hyoid bone by the hyoglossi and genioglossi muscles and the hyoglossal membrane, with the epiglottis by three glossoepiglottic folds of mucous membrane, with the soft palate by the glossopalatine arches, and with the pharynx by the superior pharyngeal constrictor muscle and the mucous membrane. It also forms the anterior wall of the oropharynx.\nThe average length of the human tongue from the oropharynx to the tip is 10\u00a0cm. The average weight of the human tongue from adult males is 99g and for adult females 79g.\nIn phonetics and phonology, a distinction is made between the tip of the tongue and the blade (the portion just behind the tip). Sounds made with the tongue tip are said to be apical, while those made with the tongue blade are said to be laminal.\nTongue posture is the resting position of the tongue in the mouth. Evidence demonstrates that the tongue plays a role in mouth and face development.\nUpper surface.\nThe upper surface of the tongue, the dorsal surface, is called the dorsum, and is divided by a groove into symmetrical halves by the \"median sulcus\". The \"foramen cecum\" marks the end of this division (at about 2.5\u00a0cm from the root of the tongue) and the beginning of the \"terminal sulcus\". The foramen cecum is also the point of attachment of the thyroglossal duct and is formed during the descent of the thyroid diverticulum in embryonic development.\nThe terminal sulcus is a shallow groove that runs forward as a shallow groove in a \"V\" shape from the foramen cecum, forwards and outwards to the margins (borders) of the tongue. The terminal sulcus divides the tongue into a posterior pharyngeal part and an anterior oral part. The pharyngeal part is supplied by the glossopharyngeal nerve and the oral part is supplied by the lingual nerve (a branch of the mandibular branch (V3) of the trigeminal nerve) for somatosensory perception and by the chorda tympani (a branch of the facial nerve) for taste perception.\nBoth parts of the tongue develop from different pharyngeal arches.\nUndersurface.\nOn the undersurface, the ventral surface, of the tongue is a fold of mucous membrane called the frenulum that tethers the tongue at the midline to the floor of the mouth. On either side of the frenulum are small prominences called sublingual caruncles that the major salivary submandibular glands drain into.\nMuscles.\nThe eight muscles of the human tongue are classified as either \"intrinsic\" or \"extrinsic\". The four intrinsic muscles act to change the shape of the tongue, and are not attached to any bone. The four extrinsic muscles act to change the position of the tongue, and are anchored to bone.\nExtrinsic.\nThe four extrinsic muscles originate from bone and extend to the tongue. They are the genioglossus, the hyoglossus (often including the chondroglossus) the styloglossus, and the palatoglossus. Their main functions are altering the tongue's position allowing for protrusion, retraction, and side-to-side movement.\nThe genioglossus arises from the mandible and protrudes the tongue. It is also known as the tongue's \"safety muscle\" since it is the only muscle that propels the tongue forward.\nThe hyoglossus, arises from the hyoid bone and retracts and depresses the tongue. The chondroglossus is often included with this muscle.\nThe styloglossus arises from the styloid process of the temporal bone and draws the sides of the tongue up to create a trough for swallowing.\nThe palatoglossus arises from the palatine aponeurosis, and depresses the soft palate, moves the \"palatoglossal fold\" towards the midline, and elevates the back of the tongue during swallowing.\nIntrinsic.\nFour paired intrinsic muscles of the tongue originate and insert within the tongue, running along its length. They are the superior longitudinal muscle, the inferior longitudinal muscle, the vertical muscle, and the transverse muscle. These muscles alter the shape of the tongue by lengthening and shortening it, curling and uncurling its apex and edges as in tongue rolling, and flattening and rounding its surface. This provides shape and helps facilitate speech, swallowing, and eating.\nThe superior longitudinal muscle runs along the upper surface of the tongue under the mucous membrane, and functions to shorten and curl the tongue upward. It originates near the epiglottis, at the hyoid bone, from the median fibrous septum.\nThe inferior longitudinal muscle lines the sides of the tongue, and is joined to the styloglossus muscle. It functions to shorten and curl the tongue downward.\nThe vertical muscle is located in the middle of the tongue, and joins the superior and inferior longitudinal muscles. It functions to flatten the tongue.\nThe transverse muscle divides the tongue at the middle, and is attached to the mucous membranes that run along the sides. It functions to lengthen and narrow the tongue.\nBlood supply.\nThe tongue receives its blood supply primarily from the lingual artery, a branch of the external carotid artery. The lingual veins drain into the internal jugular vein. The floor of the mouth also receives its blood supply from the lingual artery. There is also a secondary blood supply to the root of tongue from the tonsillar branch of the facial artery and the ascending pharyngeal artery.\nAn area in the neck sometimes called the Pirogov triangle is formed by the intermediate tendon of the digastric muscle, the posterior border of the mylohyoid muscle, and the hypoglossal nerve. The lingual artery is a good place to stop severe hemorrhage from the tongue.\nNerve supply.\nInnervation of the tongue consists of motor fibers, special sensory fibers for taste, and general sensory fibers for sensation.\nInnervation of taste and sensation is different for the anterior and posterior part of the tongue because they are derived from different embryological structures (pharyngeal arch 1 and pharyngeal arches 3 and 4, respectively).\nLymphatic drainage.\nThe tip of tongue drains to the submental nodes. The left and right halves of the anterior two-thirds of the tongue drains to submandibular lymph nodes, while the posterior one-third of the tongue drains to the jugulo-omohyoid nodes.\nMicroanatomy.\nThe upper surface of the tongue is covered in masticatory mucosa, a type of oral mucosa, which is of keratinized stratified squamous epithelium. Embedded in this are numerous papillae, some of which house the taste buds and their taste receptors. The lingual papillae consist of filiform, fungiform, vallate and foliate papillae, and only the filiform papillae are not associated with any taste buds.\nThe tongue can divide itself in dorsal and ventral surface. The dorsal surface is a stratified squamous keratinized epithelium, which is characterized by numerous mucosal projections called papillae. The lingual papillae covers the dorsal side of the tongue towards the front of the terminal groove. The ventral surface is stratified squamous non-keratinized epithelium which is smooth.\nDevelopment.\nThe tongue begins to develop in the fourth week of embryonic development from a median swelling \u2013 the median tongue bud (tuberculum impar) of the first pharyngeal arch.\nIn the fifth week a pair of \"lateral lingual swellings\", one on the right side and one on the left, form on the first pharyngeal arch. These lingual swellings quickly expand and cover the median tongue bud. They form the anterior part of the tongue that makes up two-thirds of the length of the tongue, and continue to develop through prenatal development. The line of their fusion is marked by the median sulcus.\nIn the fourth week, a swelling appears from the second pharyngeal arch, in the midline, called the copula. During the fifth and sixth weeks, the copula is overgrown by a swelling from the third and fourth arches (mainly from the third arch) called the hypopharyngeal eminence, and this develops into the posterior part of the tongue (the other third and the posterior most part of the tongue is developed from the fourth pharyngeal arch). The hypopharyngeal eminence develops mainly by the growth of endoderm from the third pharyngeal arch. The boundary between the two parts of the tongue, the anterior from the first arch and the posterior from the third arch is marked by the terminal sulcus. The terminal sulcus is shaped like a \"V\" with the tip of the V situated posteriorly. At the tip of the terminal sulcus is the foramen cecum, which is the point of attachment of the thyroglossal duct where the embryonic thyroid begins to descend.\nFunction.\nTaste.\nChemicals that stimulate taste receptor cells are known as tastants. Once a tastant is dissolved in saliva, it can make contact with the plasma membrane of the gustatory hairs, which are the sites of taste transduction.\nThe tongue is equipped with many taste buds on its dorsal surface, and each taste bud is equipped with taste receptor cells that can sense particular classes of tastes. Distinct types of taste receptor cells respectively detect substances that are sweet, bitter, salty, sour, spicy, or taste of umami. Umami receptor cells are the least understood and accordingly are the type most intensively under research. There is a common misconception that different sections of the tongue are exclusively responsible for different basic tastes. Although widely taught in schools in the form of the tongue map, this is incorrect; all taste sensations come from all regions of the tongue, although certain parts are more sensitive to certain tastes.\nMastication.\nThe tongue is an important accessory organ in the digestive system. The tongue is used for crushing food against the hard palate, during mastication and manipulation of food for softening prior to swallowing. The epithelium on the tongue's upper, or dorsal surface is keratinised. Consequently, the tongue can grind against the hard palate without being itself damaged or irritated.\nSpeech.\nThe tongue is one of the primary articulators in the production of speech, and this is facilitated by both the extrinsic muscles that move the tongue and the intrinsic muscles that change its shape. Specifically, different vowels are articulated by changing the tongue's height and retraction to alter the resonant properties of the vocal tract. These resonant properties amplify specific harmonic frequencies (formants) that are different for each vowel, while attenuating other harmonics. For example, [a] is produced with the tongue lowered and centered and [i] is produced with the tongue raised and fronted. Consonants are articulated by constricting airflow through the vocal tract, and many consonants feature a constriction between the tongue and some other part of the vocal tract. For example, alveolar consonants like [s] and [n] are articulated with the tongue against the alveolar ridge, while velar consonants like [k] and [g] are articulated with the tongue dorsum against the soft palate (velum). Tongue shape is also relevant to speech articulation, for example in retroflex consonants, where the tip of the tongue is curved backward.\nIntimacy.\nThe tongue plays a role in physical intimacy and sexuality. The tongue is part of the erogenous zone of the mouth and can be used in intimate contact, as in the French kiss and in oral sex.\nClinical significance.\nDisease.\nA congenital disorder of the tongue is that of ankyloglossia also known as \"tongue-tie\". The tongue is \"tied\" to the floor of the mouth by a very short and thickened frenulum and this affects speech, eating, and swallowing.\nThe tongue is prone to several pathologies including glossitis and other inflammations such as geographic tongue, and median rhomboid glossitis; burning mouth syndrome, oral hairy leukoplakia, oral candidiasis (thrush), black hairy tongue, bifid tongue (due to failure in fusion of two lingual swellings of first pharyngeal arch) and fissured tongue.\nThere are several types of oral cancer that mainly affect the tongue. Mostly these are squamous cell carcinomas.\nFood debris, desquamated epithelial cells and bacteria often form a visible tongue coating. This coating has been identified as a major factor contributing to bad breath (halitosis), which can be managed by using a tongue cleaner.\nMedication delivery.\nThe sublingual region underneath the front of the tongue is an ideal location for the administration of certain medications into the body. The oral mucosa is very thin underneath the tongue, and is underlain by a plexus of veins. The sublingual route takes advantage of the highly vascular quality of the oral cavity, and allows for the speedy application of medication into the cardiovascular system, bypassing the gastrointestinal tract. This is the only convenient and efficacious route of administration (apart from Intravenous therapy) of nitroglycerin to a patient suffering chest pain from angina pectoris.\nOther animals.\nThe muscles of the tongue evolved in amphibians from occipital somites. Most amphibians show a proper tongue after their metamorphosis. As a consequence, most tetrapod animals\u2014amphibians, reptiles, birds, and mammals\u2014have tongues (the frog family of pipids lack tongue). In mammals such as dogs and cats, the tongue is often used to clean the fur and body by licking. The tongues of these species have a very rough texture, which allows them to remove oils and parasites. Some dogs have a tendency to consistently lick a part of their foreleg, which can result in a skin condition known as a lick granuloma. A dog's tongue also acts as a heat regulator. As a dog increases its exercise the tongue will increase in size due to greater blood flow. The tongue hangs out of the dog's mouth and the moisture on the tongue will work to cool the bloodflow.\nSome animals have tongues that are specially adapted for catching prey. For example, chameleons, frogs, pangolins and anteaters have prehensile tongues.\nOther animals may have organs that are analogous to tongues, such as a butterfly's proboscis or a radula on a mollusc, but these are not homologous with the tongues found in vertebrates and often have little resemblance in function. For example, butterflies do not lick with their proboscides; they suck through them, and the proboscis is not a single organ, but two jaws held together to form a tube. Many species of fish have small folds at the base of their mouths that might informally be called tongues, but they lack a muscular structure like the true tongues found in most tetrapods.\nSociety and culture.\nFigures of speech.\nThe tongue can serve as a metonym for \"language\". For example, the New Testament of the Bible, in the Book of Acts of the Apostles, Jesus' disciples on the Day of Pentecost received a type of spiritual gift: \"there appeared unto them cloven tongues like as of fire, and it sat upon each of them. And they were all filled with the Holy Ghost, and began to speak with other tongues\u00a0...\", which amazed the crowd of Jewish people in Jerusalem, who were from various parts of the Roman Empire but could now understand what was being preached. The phrase \"mother tongue\" is used as a child's first language. Many languages have the same word for \"tongue\" and \"language\", as did the English language before the Middle Ages.\nA common temporary failure in word retrieval from memory is referred to as the \"tip-of-the-tongue\" phenomenon. The expression \"tongue in cheek\" refers to a statement that is not to be taken entirely seriously \u2013 something said or done with subtle ironic or sarcastic humour. A \"tongue twister\" is a phrase very difficult to pronounce. Aside from being a medical condition, \"tongue-tied\" means being unable to say what you want due to confusion or restriction. The phrase \"cat got your tongue\" refers to when a person is speechless. To \"bite one's tongue\" is a phrase which describes holding back an opinion to avoid causing offence. A \"slip of the tongue\" refers to an unintentional utterance, such as a Freudian slip. The \"gift of tongues\" refers to when one is uncommonly gifted to be able to speak in a foreign language, often as a type of spiritual gift. Speaking in tongues is a common phrase used to describe \"glossolalia\", which is to make smooth, language-resembling sounds that is no true spoken language itself. A deceptive person is said to have a forked tongue, and a smooth-talking person is said to have a silver tongue.\nGestures.\nSticking one's tongue out at someone is considered a childish gesture of rudeness or defiance in many countries; the act may also have sexual connotations, depending on the way in which it is done. However, in Tibet it is considered a greeting. In 2009, a farmer from Fabriano, Italy, was convicted and fined by Italy's highest court for sticking his tongue out at a neighbor with whom he had been arguing - proof of the affront had been captured with a cell-phone camera.\nBody art.\nTongue piercing and splitting have become more common in western countries in recent decades. One study found that one-fifth of young adults in Israel had at least one type of oral piercing, most commonly the tongue.\nRepresentational art.\nProtruding tongues appear in the art of several Polynesian cultures.\nAs food.\nThe tongues of some animals are consumed and sometimes prized as delicacies. Hot-tongue sandwiches frequently appear on menus in kosher delicatessens in America. Taco de lengua (\"lengua\" being Spanish for tongue) is a taco filled with beef tongue, and is especially popular in Mexican cuisine. As part of Colombian gastronomy, Tongue in Sauce (Lengua en Salsa) is a dish prepared by frying the tongue and adding tomato sauce, onions and salt. Tongue can also be prepared as birria. Pig and beef tongue are consumed in Chinese cuisine. Duck tongues are sometimes employed in Sichuan dishes, while lamb's tongue is occasionally employed in Continental and contemporary American cooking. Fried cod \"tongue\" is a relatively common part of fish meals in Norway and in Newfoundland. In Argentina and Uruguay cow tongue is cooked and served in vinegar (\"lengua a la vinagreta\"). In the Czech Republic and in Poland, a pork tongue is considered a delicacy, and there are many ways of preparing it. In Eastern Slavic countries, pork and beef tongues are commonly consumed, boiled and garnished with horseradish or jellied; beef tongues fetch a significantly higher price and are considered more of a delicacy. In Alaska, cow tongues are among the more common. Both cow and moose tongues are popular toppings on open-top-sandwiches in Norway, the latter usually amongst hunters.\nTongues of seals and whales have been eaten, sometimes in large quantities, by sealers and whalers, and in various times and places have been sold for food on shore.\nReferences.\n \"This article incorporates text in the public domain from the 20th edition of\" Gray's Anatomy \"(1918)\"\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56001", "revid": "11005874", "url": "https://en.wikipedia.org/wiki?curid=56001", "title": "Conformance testing", "text": "Type of testing for standards, contracts and regulations\nConformance testing and also known as compliance testing or type testing, is testing or other activities that determine whether a process, product, or service complies with the requirements of a specification, technical standard, contract, or regulation. It is an element of the more general conformity assessment.\nTesting is often either logical testing or physical testing. The test procedures may involve other criteria from mathematical testing or chemical testing. Beyond simple conformance, other requirements for efficiency, interoperability, or compliance may apply. \nConformance testing may be undertaken by the producer of the product or service being assessed, by a user, or by an accredited independent organization, which can sometimes be the author of the standard being used. When testing is accompanied by certification, the products or services may then be advertised as being certified in compliance with the referred technical standard. Manufacturers and suppliers of products and services rely on such certification including listing on the certification body's website, to assure quality to the end user and that competing suppliers are on the same level.\nAside from the various types of testing, related conformance testing activities may also include surveillance, inspection, auditing, certification, and accreditation.\nForms of conformance testing.\nThe UK government identifies three forms of testing or assessment:\nTypical areas of application.\nConformance testing is applied in various industries where a product or service must meet specific quality and/or regulatory standards. This includes areas such as:\nIn all such testing, the subject of test is not just the formal conformance in aspects of completeness of filed proofs, validity of referred certificates, and qualification of operating staff. Rather, it also heavily focuses on operational conditions, physical conditions, and applied test environments. By extension conformance testing leads to a vast set of documents and files that allow for reiterating all performed tests.\nSoftware engineering.\nIn software testing, conformance testing verifies that a product performs according to its specified standards. Compilers, for instance, are extensively tested to determine whether they meet the recognized standard for that language.\nElectronic and electrical engineering.\nIn electronic engineering and electrical engineering, some countries and business environments (such as telecommunication companies) require that an electronic product meet certain requirements before they can be sold. Standards for telecommunication products written by standards organizations such as ANSI, the FCC, and IEC have certain criteria that a product must meet before compliance is recognized. In countries such as Japan, China, Korea, and some parts of Europe, products cannot be sold unless they are known to meet those requirements specified in the standards. Usually, manufacturers set their own requirements to ensure product quality, sometimes with levels much higher than what the governing bodies require. Compliance is realized after a product passes a series of tests without occurring some specified mode of failure.\nCompliance testing for electronic devices include emissions tests, immunity tests, and safety tests. Emissions tests ensure that a product will not emit harmful electromagnetic interference in communication and power lines. Immunity tests ensure that a product is immune to common electrical signals and electromagnetic interference (EMI) that will be found in its operating environment, such as electromagnetic radiation from a local radio station or interference from nearby products. Safety tests ensure that a product will not create a safety risk from situations such as a failed or shorted power supply, blocked cooling vent, and powerline voltage spikes and dips.\nFor example, Ericsson's telecommunications research and development subsidiary Telcordia Technologies publishes conformance standards for telecommunication equipment to pass the following tests:\nStandards.\nSeveral international standards relating to conformance testing are published by the International Organization for Standardization (ISO) and covered in the divisions of ICS 03.120.20 for management and ICS 23.040.01 for technical. Other standalone ISO standards include:\nMutual recognition agreements.\nMany countries sign mutual recognition agreements (MRAs) with other countries in order to promote trade of and facilitate market access to goods and services, while making it easier to meet a country's conformance testing requirements. Additionally, these agreements have the advantage of increasing confidence in conformance assessment bodies (e.g., testing labs and certification bodies), and by extension, product quality. An example is the IAF MLA which is an agreement for the mutual recognition of accredited certification between IAF Accreditation Body (AB) Member signatories.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56011", "revid": "51208", "url": "https://en.wikipedia.org/wiki?curid=56011", "title": "Mao Ze Dong", "text": ""}
{"id": "56015", "revid": "10512", "url": "https://en.wikipedia.org/wiki?curid=56015", "title": "Alphonso V of Spain", "text": ""}
{"id": "56021", "revid": "877696643", "url": "https://en.wikipedia.org/wiki?curid=56021", "title": "North sea", "text": ""}
{"id": "56022", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=56022", "title": "The Chunnel", "text": ""}
{"id": "56023", "revid": "40218375", "url": "https://en.wikipedia.org/wiki?curid=56023", "title": "Chunnel", "text": ""}
{"id": "56024", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=56024", "title": "Le tunnel sous la Manche", "text": ""}
{"id": "56025", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=56025", "title": "English channel", "text": ""}
{"id": "56026", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=56026", "title": "La Manche", "text": ""}
{"id": "56028", "revid": "28414705", "url": "https://en.wikipedia.org/wiki?curid=56028", "title": "Brussel", "text": "Brussel is a surname. Notable people with the surname include:\nSee also.\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n Surname listThis page lists people with the surname . "}
{"id": "56029", "revid": "327763", "url": "https://en.wikipedia.org/wiki?curid=56029", "title": "Bruxelles", "text": ""}
{"id": "56031", "revid": "327763", "url": "https://en.wikipedia.org/wiki?curid=56031", "title": "Belgi\u00eb", "text": ""}
{"id": "56032", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=56032", "title": "Belgien", "text": ""}
{"id": "56033", "revid": "9092818", "url": "https://en.wikipedia.org/wiki?curid=56033", "title": "Belgique", "text": ""}
{"id": "56035", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=56035", "title": "Neatherlands", "text": ""}
{"id": "56039", "revid": "1261736", "url": "https://en.wikipedia.org/wiki?curid=56039", "title": "Hayden Christiansen", "text": ""}
{"id": "56040", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=56040", "title": "Local base", "text": ""}
{"id": "56041", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=56041", "title": "Preregular space", "text": ""}
{"id": "56045", "revid": "1639942", "url": "https://en.wikipedia.org/wiki?curid=56045", "title": "606 BC", "text": "Calendar year\n \nThe year 606 BC was a year of the pre-Julian Roman calendar. In the Roman Empire, it was known as year 148 \"Ab urbe condita\". The denomination 606 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56046", "revid": "38854409", "url": "https://en.wikipedia.org/wiki?curid=56046", "title": "538 BC", "text": "Calendar year\n \nThe year 538 BC was a year of the pre-Julian Roman calendar. In the Roman Empire, it was known as year 216 \"Ab urbe condita\". The denomination 538 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years. \nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56057", "revid": "981188282", "url": "https://en.wikipedia.org/wiki?curid=56057", "title": "Totally disconnected", "text": ""}
{"id": "56058", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=56058", "title": "Second category", "text": ""}
{"id": "56060", "revid": "5543430", "url": "https://en.wikipedia.org/wiki?curid=56060", "title": "Green economists", "text": ""}
{"id": "56061", "revid": "24758205", "url": "https://en.wikipedia.org/wiki?curid=56061", "title": "Discrete space", "text": "Type of topological space\nIn topology, a discrete space is a particularly simple example of a topological space or similar structure, one in which the points form a discontinuous sequence, meaning they are \"isolated\" from each other in a certain sense. The discrete topology is the finest topology that can be given on a set. Every subset is open in the discrete topology so that in particular, every singleton subset is an open set in the discrete topology.\nDefinitions.\nGiven a set formula_1:\nA metric space formula_2 is said to be \"uniformly discrete\" if there exists a \"&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;packing radius\" formula_3 such that, for any formula_4 one has either formula_5 or formula_6 The topology underlying a metric space can be discrete, without the metric being uniformly discrete: for example the usual metric on the set formula_7\n&lt;templatestyles src=\"Math_proof/styles.css\" /&gt;Proof that a discrete space is not necessarily uniformly discrete\nLet formula_8 consider this set using the usual metric on the real numbers. Then, formula_1 is a discrete space, since for each point formula_10 we can surround it with the open interval formula_11 where formula_12 The intersection formula_13 is therefore trivially the singleton formula_14 Since the intersection of an open set of the real numbers and formula_1 is open for the induced topology, it follows that formula_16 is open so singletons are open and formula_1 is a discrete space.\nHowever, formula_1 cannot be uniformly discrete. To see why, suppose there exists an formula_3 such that formula_20 whenever formula_21 It suffices to show that there are at least two points formula_22 and formula_23 in formula_1 that are closer to each other than formula_25 Since the distance between adjacent points formula_26 and formula_27 is formula_28 we need to find an formula_29 that satisfies this inequality:\nformula_30\nSince there is always an formula_29 bigger than any given real number, it follows that there will always be at least two points in formula_1 that are closer to each other than any positive formula_33 therefore formula_1 is not uniformly discrete.\nProperties.\nThe underlying uniformity on a discrete metric space is the discrete uniformity, and the underlying topology on a discrete uniform space is the discrete topology.\nThus, the different notions of discrete space are compatible with one another.\nOn the other hand, the underlying topology of a non-discrete uniform or metric space can be discrete; an example is the metric space formula_35 (with metric inherited from the real line and given by formula_36).\nThis is not the discrete metric; also, this space is not complete and hence not discrete as a uniform space.\nNevertheless, it is discrete as a topological space.\nWe say that formula_1 is \"topologically discrete\" but not \"uniformly discrete\" or \"metrically discrete\".\nAdditionally:\nAny function from a discrete topological space to another topological space is continuous, and any function from a discrete uniform space to another uniform space is uniformly continuous. That is, the discrete space formula_1 is free on the set formula_1 in the category of topological spaces and continuous maps or in the category of uniform spaces and uniformly continuous maps. These facts are examples of a much broader phenomenon, in which discrete structures are usually free on sets.\nWith metric spaces, things are more complicated, because there are several categories of metric spaces, depending on what is chosen for the morphisms. Certainly the discrete metric space is free when the morphisms are all uniformly continuous maps or all continuous maps, but this says nothing interesting about the metric structure, only the uniform or topological structure. Categories more relevant to the metric structure can be found by limiting the morphisms to Lipschitz continuous maps or to short maps; however, these categories don't have free objects (on more than one element). However, the discrete metric space is free in the category of bounded metric spaces and Lipschitz continuous maps, and it is free in the category of metric spaces bounded by 1 and short maps. That is, any function from a discrete metric space to another bounded metric space is Lipschitz continuous, and any function from a discrete metric space to another metric space bounded by 1 is short.\nGoing the other direction, a function formula_47 from a topological space formula_41 to a discrete space formula_1 is continuous if and only if it is \"locally constant\" in the sense that every point in formula_41 has a neighborhood on which formula_47 is constant.\nEvery ultrafilter formula_52 on a non-empty set formula_1 can be associated with a topology formula_54 on formula_1 with the property that every non-empty proper subset formula_56 of formula_1 is either an open subset or else a closed subset, but never both. Said differently, every subset is open or closed but (in contrast to the discrete topology) the only subsets that are both open and closed (i.e. clopen) are formula_58 and formula_1. In comparison, every subset of formula_1 is open and closed in the discrete topology.\nExamples and uses.\nA discrete structure is often used as the \"default structure\" on a set that doesn't carry any other natural topology, uniformity, or metric; discrete structures can often be used as \"extreme\" examples to test particular suppositions. For example, any group can be considered as a topological group by giving it the discrete topology, implying that theorems about topological groups apply to all groups. Indeed, analysts may refer to the ordinary, non-topological groups studied by algebraists as \"discrete groups\". In some cases, this can be usefully applied, for example in combination with Pontryagin duality. A 0-dimensional manifold (or differentiable or analytic manifold) is nothing but a discrete and countable topological space (an uncountable discrete space is not second-countable). We can therefore view any discrete countable group as a 0-dimensional Lie group.\nA product of countably infinite copies of the discrete space of natural numbers is homeomorphic to the space of irrational numbers, with the homeomorphism given by the continued fraction expansion. A product of countably infinite copies of the discrete space formula_61 is homeomorphic to the Cantor set; and in fact uniformly homeomorphic to the Cantor set if we use the product uniformity on the product. Such a homeomorphism is given by using ternary notation of numbers. (See Cantor space.) Every fiber of a locally injective function is necessarily a discrete subspace of its domain. \nIn the foundations of mathematics, the study of compactness properties of products of formula_61 is central to the topological approach to the ultrafilter lemma (equivalently, the Boolean prime ideal theorem), which is a weak form of the axiom of choice.\nIndiscrete spaces.\nIn some ways, the opposite of the discrete topology is the trivial topology (also called the \"indiscrete topology\"), which has the fewest possible open sets (just the empty set and the space itself). Where the discrete topology is initial or free, the indiscrete topology is final or cofree: every function \"from\" a topological space \"to\" an indiscrete space is continuous, etc.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56065", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=56065", "title": "Joseph Paul DiMaggio", "text": ""}
{"id": "56067", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=56067", "title": "Joe Di Maggio", "text": ""}
{"id": "56068", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=56068", "title": "Joe di Maggio", "text": ""}
{"id": "56069", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=56069", "title": "Vladivostok, Russia", "text": ""}
{"id": "56071", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=56071", "title": "List of film directors", "text": ""}
{"id": "56072", "revid": "35126782", "url": "https://en.wikipedia.org/wiki?curid=56072", "title": "Isla de Pascua", "text": ""}
{"id": "56073", "revid": "35126782", "url": "https://en.wikipedia.org/wiki?curid=56073", "title": "Rapa Nui", "text": ""}
{"id": "56075", "revid": "1216158", "url": "https://en.wikipedia.org/wiki?curid=56075", "title": "Grumman E-2 Hawkeye", "text": "Airborne early warning and control aircraft\nThe Northrop Grumman E-2 Hawkeye is an American all-weather, carrier-capable, tactical airborne early warning (AEW) aircraft. This twin-turboprop aircraft was designed and developed during the late 1950s and early 1960s by the Grumman Aircraft Company for the United States Navy as a replacement for the earlier, piston-engined E-1 Tracer, which was rapidly becoming obsolete. The aircraft's performance has been upgraded with the E-2B and E-2C versions, where most of the changes were made to the radar and radio communications due to advances in electronic integrated circuits and other electronics. The fourth major version of the Hawkeye is the E-2D, which first flew in 2007. The E-2 was the first aircraft designed specifically for AEW, as opposed to a modification of an existing airframe, such as the Boeing E-3 Sentry. Variants of the Hawkeye have been in continuous production since 1960, giving it the longest production run of any carrier-based aircraft.\nThe E-2 also received the nickname \"Super Fudd\" because it replaced the WF (later E-1) \"Willy Fudd\". In recent decades, the E-2 has been commonly referred to as the \"Hummer\" because of the distinctive sounds of its turboprop engines, quite unlike that of turbojet and turbofan jet engines. In addition to U.S. Navy service, smaller numbers of E-2s have been sold to the armed forces of Egypt, France, Israel, Japan, Mexico, Singapore, and Taiwan.\nGrumman also used the basic layout of the E-2 to produce the Grumman C-2 Greyhound cargo aircraft.\nDevelopment.\nBackground.\nContinual improvements in airborne radars through 1956 led to the construction of AEW airplanes by several different countries and several different armed forces. The functions of command and control and sea and air surveillance were also added. The first carrier-based aircraft to perform these missions for the U.S. Navy and its allies was the Douglas AD Skyraider, which was replaced in US Navy service by the Grumman E-1 Tracer, which was a modified version of the S-2 Tracker twin-engined, antisubmarine warfare aircraft, where the radar was carried in an aerofoil-shaped radome carried above the aircraft's fuselage.\nE-2A and E-2B Hawkeye.\nIn 1956, the U.S. Navy developed a requirement for an AEW aircraft where its data could be integrated into the Naval Tactical Data System aboard the Navy's ships, with a design from Grumman being selected to meet this requirement in March 1957. Its design, initially designated W2F-1, but later redesignated the E-2A Hawkeye, was the first carrier plane that had been designed from its wheels up as an AEW and command-and-control airplane. The design engineers at Grumman faced immense challenges, including the requirement that the aircraft be able to operate from the older modified s. These vessels were built during World War II and were smaller than modern carriers, being later modified to allow them to operate jet aircraft. Consequently, various height, weight, and length restrictions had to be factored into the E-2A design, resulting in some handling characteristics that were less than ideal. However, the E-2A never operated from the modified \"Essex\"-class carriers.\nThe first prototype, acting as an aerodynamic testbed only, flew on 21 October 1960. The first fully equipped aircraft followed it on 19 April 1961, and entered service with the US Navy as the E-2A in January 1964. By 1965, the project had accumulated so many development issues that it was cancelled after 59 aircraft had already been built. In particular, difficulties were being experienced due to inadequate cooling in the closely packed avionics compartment. Early computers and complex avionics systems generated considerable heat and could fail without proper ventilation. These issues continued long after the aircraft entered service. At one point, reliability was so bad that the entire fleet of aircraft was grounded.\nAfter Navy officials had been forced to explain to Congress why four production contracts had been signed before avionics testing had been completed, action was taken; Grumman and the US Navy scrambled to improve the design. The unreliable rotary drum computer was replaced by a Litton L-304 digital computer and various avionics systems were replaced \u2013 the upgraded aircraft were designated E-2Bs. In total, 49 of the 59 E-2As were upgraded to E-2B standards. These aircraft replaced the E-1B Tracers in the various US Navy AEW squadrons.\nE-2C Hawkeye and upgrades.\nAlthough the upgraded E-2B was a vast improvement on the unreliable E-2A, it was an interim measure. The US Navy knew the design had much greater capability and had yet to achieve the performance and reliability parameters set out in the original 1957 design. In April 1968, a reliability improvement program was initiated. In addition, now that the capabilities of the aircraft were starting to be realized, more were desired; 28 new E-2Cs were ordered to augment the 49 E-2Bs that would be upgraded. Improvements in the new and upgraded aircraft were concentrated in the radar and computer performance.\nTwo E-2A test machines were modified as E-2C prototypes, the first flying on 20 January 1971. Trials proved satisfactory, and the E-2C was ordered into production. The first production aircraft performed its initial flight on 23 September 1972. The original E-2C, known as Group 0, consisted of 55 aircraft; the first aircraft became operational in 1973 and began serving on carriers in the 1980s and 1990s, until they were replaced in first-line service by Group II aircraft. The US Navy Reserve used some aircraft for tracking drug smugglers. The type was commonly used in conjunction with Grumman F-14 Tomcat fighters, monitoring airspace and then vectoring Tomcats over the Link-4A datalink to destroy potential threats with long-range AIM-54C Phoenix missiles.\nThe next production run, between 1988 and 1991, had 18 aircraft built to the Group I standard, which replaced the E-2's older APS-125 radar and T56-A-425 turboprops with their successors, the APS-139 radar system and T56-A-427 turboprops. The first Group I aircraft entered service in August 1981. Upgrading the Group 0 aircraft to Group I specifications was considered, but the cost was comparable to a new production aircraft, so upgrades were not conducted. Group I aircraft were only flown by the Atlantic Fleet squadrons. This version was followed within a few years by the Group II, which had the improved APS-145 radar. Fifty Group II aircraft were delivered, 12 being upgraded Group I aircraft. This new version entered service in June 1992 and served with the Pacific and Atlantic Fleet squadrons.\nBy 1997, the US Navy intended that all front-line squadrons would be equipped, for a total of 75 Group II aircraft. Grumman merged with Northrop in 1994 and plans began on the Group II Plus, also known as the Group II/NAV upgrade. This kept the same computer and radar as the Group II while upgrading the pilot avionics, such as replacing the mechanical inertial navigation system (INS) with a more reliable and accurate laser Ring Gyroscope-driven INS, installing dual multifunction control display units (vs. one in the Group II) and integrating GPS into the weapons system. A variant of the Group II with upgrades to the mission computer and combat information center (CIC) workstations is referred to as the MCU/ACIS; these were produced in small numbers due to production of the Hawkeye 2000 soon after its introduction. All Group II aircraft had their 1960s-vintage computer processors replaced by a mission computer with the same functionality via modern computer technology, referred to as the GrIIM RePr (Group II Mission Computer Replacement Program, pronounced \"grim reaper\").\nAnother upgrade to the Group II was the Hawkeye 2000, which featured the same APS-145 radar, but incorporated an upgraded mission computer and CIC workstations (Advanced Control Indicator Set or ACIS and carries the U.S. Navy's new CEC (cooperative engagement capability) data-link system. It is also fitted with larger-capacity, vapor-cycle, avionics cooling system. Starting in 2007, a hardware and software upgrade package began to be added to existing Hawkeye 2000 aircraft. This upgrade allowed faster processing, double-current trackfile capacity, and access to satellite information networks. Hawkeye 2000 cockpits being upgraded included solid-state glass displays and a GPS-approach capability. The remaining Hawkeye Group II NAV Upgrade aircraft received GPS approach capability, but did not get the solid-state glass displays.\nIn 2004, the E-2C's propeller system was changed; a new eight-bladed propeller system named NP2000 was developed by the Hamilton-Sundstrand company to replace the old four-bladed design. Improvements included reduced vibrations and better maintainability as a result of the ability to remove prop blades individually instead of having to remove the entire prop and hub assembly. The propeller blades are of carbon fiber construction with steel leading-edge inserts and deicing boots at the root of the blade.\nE-2D Advanced Hawkeye.\nThe E-2 was once considered for replacement by the \"Common Support Aircraft\", but this concept was abandoned. The latest E-2 version is the E-2D Advanced Hawkeye, which features an entirely new avionics suite, including the new AN/APY-9 radar, radio suite, mission computer, integrated satellite communications, flight management system, improved T56-A-427A engines, a glass cockpit, and aerial refueling. The APY-9 radar features an active electronically scanned array (AESA), which adds electronic scanning to the mechanical rotation of the radar in its radome. The E-2D includes provisions for the copilot to act as a \"tactical 4th operator\" (T4O), able to reconfigure the main cockpit display to show radar, IFF, and Link 16 (JTIDS) or CEC and to access all acquired data. The E-2D's first flight occurred on 3 August 2007. On 8 May 2009, an E-2D used its Cooperative Engagement Capability system to engage an overland cruise missile with a Standard Missile SM-6 fired from another platform in an integrated fire-control system test. These two systems will form the basis of the Naval Integrated Fire Control \u2013 Counter Air (NIFC-CA) when fielded in 2015; the USN is investigating adding other systems to the NIFC-CA network in the future.\nThe APY-9 radar has been suspected of being capable of detecting fighter-sized stealth aircraft, which are typically optimized against high frequencies such as Ka, Ku, X, C, and parts of the S-bands. Small aircraft lack the size or weight allowances for all-spectrum, low-observable features, leaving a vulnerability to detection by the UHF-band APY-9 radar, potentially detecting fifth-generation fighters including the Russian Sukhoi Su-57 and the Chinese Chengdu J-20 and Shenyang J-31. Historically, UHF radars had resolution and detection issues that made them ineffective for accurate targeting and fire control; Northrop Grumman and Lockheed claim that the APY-9 has solved these shortcomings by using advanced electronic scanning and high digital computing power via space/time adaptive processing. According to the Navy's NIFC-CA concept, the E-2D could guide fleet weapons, such as AIM-120 AMRAAM and SM-6 missiles, onto targets beyond a launch platform's detection range or capabilities.\nDeliveries of initial-production E-2Ds began in 2010. On 4 February 2010, the first production E-2D Advanced, dubbed \"Delta One\", conducted the D model's first carrier landing aboard USS \"Harry S. Truman\" as a part of carrier suitability testing. On 27 September 2011, an E-2D was successfully launched by the prototype Electromagnetic Aircraft Launch System (EMALS) at Naval Air Engineering Station Lakehurst. On 12 February 2013, the Office of the Secretary of Defense approved the E-2D to enter full-rate production. The Navy planned for an initial operational capability by 2015. In June 2013, the 10th E-2D was delivered to the Navy, with an additional 10 aircraft in various stages of manufacturing and predelivery flight testing. On 18 July 2013, Northrop Grumman was awarded a $113.7 million contract for five full-rate production Lot 2 E-2D Advanced Hawkeye aircraft. On 13 August 2013, Northrop Grumman was awarded a $617 million contract for five E-2Ds until full-rate production Lot 1. On 30 June 2014, Northrop Grumman was awarded a $3.6 billion contract to supply 25 more E-2D, for a total contracted number of 50 aircraft; 13 E-2D models had been delivered by that time.\nIn December 2016, an E-2D flew for the first time with an aerial refueling capability. This feature allows the aircraft to double its time on station to five hours and increase total mission time from four to seven hours. The refueling modification began with the 46th plane (out of 75 planned) for delivery in late 2020, costing an additional $2 million per aircraft. The Navy plans to retrofit the feature on all previous Hawkeyes for $6 million per plane.\nDesign.\nThe E-2 is a high-wing airplane, with one Allison T56 turboprop engine (5250 shp rating) on each wing and retractable tricycle landing gear. As with all CATOBAR carrier-borne airplanes, the E-2 is equipped with a tail hook for recovery (landing) and the nose gear can attach to a shuttle of the aircraft carrier's catapults for launch (takeoff). A distinguishing feature of the Hawkeye is its 24-foot (7.3 m) diameter rotating radar dome (rotodome) that is mounted above its fuselage and wings. This carries the E-2's primary antennas for its long-range radar and IFF systems. No other carrier-borne aircraft possesses one of these. Land-based aircraft with rotodomes include the Boeing E-3 Sentry, a larger AWACS airplane operated by the U.S. Air Force and NATO air forces in large numbers. The similarly placed stationary radome of the E-2's piston-engined predecessor, the E-1 Tracer, also mandated the E-2's adoption of a modern version of the Grumman Sto-Wing folding wing system, preventing the folded wing panels from making contact with the E-2's rotodome.\nThe aircraft is operated by a crew of five, with the pilot and co-pilot on the flight deck and the combat information center officer, air control officer, and radar operator stations located in the rear fuselage directly beneath the rotodome.\nIn U.S. service, the E-2 Hawkeye provides all-weather airborne early warning and command and control capabilities for all aircraft-carrier battle groups. In addition, its other purposes include sea and land surveillance, the control of the aircraft carrier's fighter planes for air defense, the control of strike aircraft on offensive missions, the control of search and rescue missions for naval aviators and sailors lost at sea, relaying radio communications, air-to-air and ship-to-air. It can also serve in an air traffic control capacity in emergency situations when land-based ATC is unavailable.\nThe E-2C and E-2D Hawkeyes use advanced electronic sensors combined with digital computer signal processing, especially its radars, for early warning of enemy aircraft attacks and anti-ship missile attacks, controlling the carrier's combat air patrol (CAP) fighters, and secondarily for surveillance of the surrounding sea and land for enemy warships and guided-missile launchers and any other electronic surveillance missions as directed.\nOperational history.\nUS Navy.\nThe E-2A entered U.S. Navy service in January 1964 and in April 1964 with VAW-11 at NAS North Island. The first deployment was aboard the aircraft carrier during 1965.\nSince entering combat during the Vietnam War, the E-2 has served the US Navy around the world, acting as the electronic \"eyes of the fleet\".\nIn August 1981, a Hawkeye from VAW-124 \"Bear Aces\" directed two F-14 Tomcats from VF-41 \"Black Aces\" in an intercept mission in the Gulf of Sidra that resulted in the downing of two Libyan Sukhoi Su-22s. Hawkeyes from VAW-123 aboard the aircraft carrier directed a group of F-14 Tomcat fighters flying the Combat Air Patrol during Operation El Dorado Canyon, the joint strike of two Carrier Battle Groups in the Mediterranean Sea against Libyan targets during 1986.\nMore recently, E-2Cs provided the command and control for both aerial warfare and land-attack missions during the Persian Gulf War. Hawkeyes have supported the U.S. Coast Guard, the U.S. Customs Service, and American federal and state police forces during anti-drug operations.\nIn the mid-1980s, several U.S. Navy E-2Cs were made available to the U.S. Coast Guard and the U.S. Customs Service for counter-narcotics (CN) and maritime interdiction operations. The Coast Guard produced a small cadre of naval flight officers (NFOs), starting with the recruitment and interservice transfer of Navy NFOs with E-2 flight experience and the flight training of other junior Coast Guard officers as NFOs. A fatal aircraft mishap on 24 August 1990 involving a Coast Guard E-2C at the former Naval Station Roosevelt Roads in Puerto Rico prompted the Coast Guard to discontinue flying E-2Cs and to return its E-2Cs to the Navy. The U.S. Customs Service also returned its E-2Cs to the Navy and concentrated on the use of former U.S. Navy P-3 Orion aircraft in the CN role.\nE-2C Hawkeye squadrons played a critical role in air operations during Operation Desert Storm. In one instance, a Hawkeye crew provided critical air control direction to two F/A-18 Hornet aircrew, resulting in the shootdown of two Iraqi MiG-21s. During Operations Southern Watch and Desert Fox, Hawkeye crews continued to provide thousands of hours of air coverage, while providing air-to-air and air-to-ground command and control in a number of combat missions.\nThe E-2 Hawkeye is a crucial component of all U.S. Navy carrier air wings; each carrier is equipped with four Hawkeyes (five in some situations), allowing for continuous 24-hour-a-day operation of at least one E-2 and for one or two to undergo maintenance in the aircraft carrier's hangar deck at all times. Until 2005, the US Navy Hawkeyes were organized into East and West Coast wings, supporting the respective fleets. However, the East coast wing was disestablished, all aircraft were organized into a single wing based at Point Mugu, California. Six E-2C aircraft were deployed by the U.S. Navy Reserve for drug interdiction and homeland security operations until 9 March 2013, when the sole Reserve squadron, VAW-77 \"Nightwolves\", was decommissioned and its six aircraft sent to other squadrons.\nDuring Operation Enduring Freedom and Operation Iraqi Freedom all ten Regular Navy Hawkeye squadrons flew overland sorties. They provided battle management for attack of enemy ground targets, close-air-support coordination, combat search and rescue control, and airspace management, as well as datalink and communication relay for both land and naval forces. During the aftermath of Hurricane Katrina, three Hawkeye squadrons (two regular Navy and one Navy Reserve) were deployed in support of civilian relief efforts including Air Traffic Control responsibilities spanning three states, and the control of U.S. Army, U.S. Navy, U.S. Air Force, U.S. Marine Corps, U.S. Coast Guard, and Army National Guard and Air National Guard helicopter rescue units.\nHawkeye 2000s first deployed in 2003 aboard with VAW-117, the \"Wallbangers\" (formerly the \"Nighthawks\") and CVW-11. U.S. Navy E-2C Hawkeyes have been upgraded with eight-bladed propellers as part of the NP2000 program; the first squadron to cruise with the new propellers was VAW-124 \"Bear Aces\". The Hawkeye 2000 version can track over 2,000 targets simultaneously while also detecting 20,000 targets to a range greater than and simultaneously guide 40\u2013100 air-to-air intercepts or air-to-surface engagements.\nIn 2014, several E-2C Hawkeyes from the Bear Aces of VAW-124 were deployed from as flying command posts and air traffic controllers over Iraq during Operation Inherent Resolve against the Islamic State.\nVAW-120, the E-2C fleet replacement squadron began receiving E-2D Advanced Hawkeyes for training use in July 2010. On 27 March 2014, the first E-2Ds were delivered to the VAW-125. The E-2D achieved Initial Operational Capability (IOC) in October 2014 when VAW-125 was certified to have five operational aircraft. This began training on the aircraft for its first operational deployment, scheduled for 2015 aboard . The E-2D will play a larger role than that of the E-2C, with five E-2Ds aboard each carrier instead of the current four C-models, requiring the acquisition of 75 total E-2Ds. On 11 March 2015, the \"Theodore Roosevelt\" Carrier Strike Group departed Naval Station Norfolk and returned to port on 23 November 2015, concluding the first operational use of the E-2D.\nOther operators.\nE-2 Hawkeyes have been sold by the U.S. Federal Government under Foreign Military Sales (FMS) procedures to the armed forces of Egypt, France, Israel, Japan, Singapore and Taiwan.\nEgypt.\nEgypt purchased five E-2C Hawkeyes, that entered service in 1987 and were upgraded to Hawkeye 2000 standard. One additional upgraded E-2C was purchased. The first upgraded aircraft was delivered in March 2003 and deliveries were concluded in late 2008. Egypt requested two additional excess E-2C aircraft in October 2007; deliveries began in 2010. They all operate in 601 AEW Brigade, Cairo-West.\nEgypt used the E-2C Hawkeye in a bombing operation in 2015 against ISIL in Libya.\nFrance.\nThe French Naval Aviation (Aeronavale) operates three E-2C Hawkeyes and has been the only operator of the E-2 Hawkeye from an aircraft carrier besides the U.S. Navy. The French nuclear-powered carrier, , currently carries two E-2C Hawkeyes on her combat patrols offshore. The third French E-2C Hawkeye has been upgraded with eight-bladed propellers as part of the NP2000 program. In April 2007, France requested the purchase of an additional aircraft.\nThe Flottille 4F of the French Navy's Aeronavale was stood up on 2 July 2000 and flies its E-2C Hawkeyes from its naval air station at Lann-Bihoue, deploying to the \"Charles de Gaulle\". They took part in operations in Afghanistan and Libya.\nIn September 2019 Florence Parly, French Minister of the Armed Forces, announced that three new E-2D Advanced Hawkeyes would be purchased in 2020 to replace the E-2Cs in service.\nIn December 2024, France's first E-2D Hawkeye entered production, scheduled for delivery in 2027.\nJapan.\nOn 6 September 1976, Soviet Air Forces pilot Viktor Belenko successfully defected, landing his MiG-25 'Foxbat' at Hakodate Airport, Japan. During this incident, the Japan Self-Defense Forces' (JASDF) radar lost track of the aircraft when Belenko flew his MiG-25 at a low altitude, prompting the JASDF to consider procurement of airborne early warning aircraft.\nInitially, the E-3 Sentry airborne warning and control system aircraft was considered to be the prime candidate for the airborne early warning mission by the JASDF. However, the Japanese Defense Agency realized that the E-3 would not be readily available due to USAF needs and opted to procure E-2 Hawkeye aircraft. The Japan Air Self-Defense Force bought thirteen E-2C aircraft to improve its early warning capabilities. The E-2C was put into service with the Airborne Early Warning Group (AEWG) at Misawa Air Base in January 1987.\nOn 21 November 2014, the Japanese Ministry of Defense officially decided to procure the E-2D version of the Hawkeye, instead of the Boeing 737 AEW&amp;C design. In June 2015, the Japanese government requested to buy four E-2Ds through a Foreign Military Sale.\nIn September 2018 the Defense Security Cooperation Agency (DSCA) notified Congress of the possible sale of up to nine E-2Ds to Japan.\nA sale of up to five E-2Ds for JASDF was approved by the U.S. State Department and DSCA notified Congress on 7 March 2023. The sale includes ancillary equipment, spares and training support for an estimated $1.38 billion. The proposed five E-2Ds are in addition to the six E-2Ds Japan already has and the seven more it has on order. However, the Japanese Ministry of Defense did not reveal in its most recent proposed budget any intention to acquire more aircraft.\nMexico.\nIn 2004, three former Israel Air Force E-2C aircraft were sold to the Mexican Navy to perform maritime and shore surveillance missions. These aircraft were upgraded locally by IAI. The first Mexican E-2C was rolled out in January 2004.\nSingapore.\nThe Republic of Singapore Air Force acquired four Grumman E-2C Hawkeye airborne early warning aircraft in 1987, which are assigned to the 111 Squadron \"Jaeger\" based at Tengah Air Base.\nIn April 2007, it was announced that the four E-2C Hawkeyes were to be replaced with four Gulfstream G550s which would become the primary early warning aircraft of the Singapore Air Force. On 13 April 2012, the newer G550 AEWs officially took over duty from the former. Singapore has close ties with the Israel military which has also acquired the G550 AEW.\nIsrael.\nIsrael was the first export customer; its four Hawkeyes were delivered during 1981, complete with the folding wings characteristic of carrier-borne aircraft.\nThe four examples were soon put into active service before and during the 1982 Lebanon War during which they won a resounding victory over Syrian air defenses and fighter control. They were central to the Israeli victory in the air battles over the Bekaa Valley during which over 90 Syrian fighters were downed. The Hawkeyes were also the linchpins of the operation in which the IAF destroyed the surface-to-air missile (SAM) array in the Bekaa, coordinating the various stages of the operation, vectoring planes into bombing runs and directing intercepts. Under constant escort by F-15 Eagles, there were always two Hawkeyes on station off the Lebanese coast, controlling the various assets in the air and detecting any Syrian aircraft upon their takeoff, eliminating any chance of surprise.\nThe Israeli Air Force (IAF) operated four E-2s for its homeland AEW protection through 1994. The IAF was the first user of the E-2 to install air-to-air refueling equipment.\nThree of the four Israeli-owned Hawkeyes were sold to Mexico in 2002 after they had been upgraded with new systems; the remaining example was sent to be displayed in the Israeli Air Force Museum. In 2010, Singapore began retiring its E-2Cs as well. Both Israel and Singapore now employ the Israel Aerospace Industries (IAI) \"Eitam\", a Gulfstream G550-based platform with Elta's EL/W-2085 sensor package (a newer derivative of the airborne Phalcon system) for their national AEW programs.\nTaiwan.\nTaiwan acquired four E-2T aircraft from the US on 22 November 1995. On 15 April 2006 Taiwan commissioned two new E-2K Hawkeyes at an official ceremony at the Republic of China Air Force (ROCAF) base in Pingtung Airport in southern Taiwan.\nThe four E-2Ts were approved for upgrade to Hawkeye 2000 configuration in a 2008 arms deal. The four E-2T aircraft were upgraded to what became known as E-2K standard in two batches, the first batch of two aircraft were sent to the United States in June 2010, arriving home in late 2011; on their return the second batch of two aircraft were sent for upgrade, returning to Taiwan in March 2013.\nOffers.\nIn August 2009, the U.S. Navy and Northrop Grumman briefed the Indian Navy on the E-2D Advanced Hawkeye on its potential use to satisfy its current shore-based and future carrier-based Airborne Early Warning and Control (AEW&amp;C) requirements. The Indian Navy has reportedly expressed interest in acquiring up to six Hawkeyes.\n;E-2C Group 0\n Initial production version of E-2C, fitted with AN/APS-120 or AN/APS-125 radar. Lengthened nose compared to earlier versions\nNew radar (AN/APS-139), plus upgraded mission computer and upgraded engines. 18 new build aircraft.\nAN/APS-145 radar, further improved electronics.\nAvionics upgrade, inclusion of GPS into weapon system.\nNew mission computer, Cooperative Engagement Capability (CEC) and additional satellite communications aerial. Originally designated Group 2+.\nSpecifications (E-2D).\n\"Data from\" US Navy fact file E-2D Storybook (page 25)General characteristics* Crew: 5: pilot, copilot, radar officer (RO), combat information center officer (CICO), aircraft control officer (ACO)* Aspect ratio: 9.15* Airfoil: root: NACA 63A216; tip: NACA 63A414* Propellers: -bladed\nPerformance* Maximum speed: Mach * Endurance: 6 hours (8 hours land-based)* g limits: * Roll rate: \nAvionics\nSee also.\nRelated development\nAircraft of comparable role, configuration, and era\nRelated lists\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56079", "revid": "50418541", "url": "https://en.wikipedia.org/wiki?curid=56079", "title": "Krull dimension", "text": "In mathematics, dimension of a ring\nIn commutative algebra, the Krull dimension of a commutative ring \"R\", named after Wolfgang Krull, is the supremum of the lengths of all chains of prime ideals. The Krull dimension need not be finite even for a Noetherian ring. More generally the Krull dimension can be defined for modules over possibly non-commutative rings as the deviation of the poset of submodules.\nThe Krull dimension was introduced to provide an algebraic definition of the dimension of an algebraic variety: the dimension of the affine variety defined by an ideal \"I\" in a polynomial ring \"R\" is the Krull dimension of \"R\"/\"I\".\nA field \"k\" has Krull dimension 0; more generally, \"k\"[\"x\"1, ..., \"x\"\"n\"] has Krull dimension \"n\". A principal ideal domain that is not a field has Krull dimension 1. A local ring has Krull dimension 0 if and only if every element of its maximal ideal is nilpotent.\nThere are several other ways that have been used to define the dimension of a ring. Most of them coincide with the Krull dimension for Noetherian rings, but can differ for non-Noetherian rings.\nExplanation.\nWe say that a chain of prime ideals of the form\nformula_1\nhas length formula_2. That is, the length is the number of strict inclusions, not the number of primes; these differ by formula_3. We define the Krull dimension of formula_4 to be the supremum of the lengths of all chains of prime ideals in formula_4.\nGiven a prime ideal formula_6 in formula_4, we define the &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;height of formula_6, written formula_9, to be the supremum of the lengths of all chains of prime ideals contained in formula_6, meaning that formula_11. In other words, the height of formula_6 is the Krull dimension of the localization of formula_4 at formula_6. A prime ideal has height zero if and only if it is a minimal prime ideal. The Krull dimension of a ring is the supremum of the heights of all maximal ideals, or those of all prime ideals. The height is also sometimes called the codimension, rank, or altitude of a prime ideal.\nIn a Noetherian ring, every prime ideal has finite height. Nonetheless, Nagata gave an example of a Noetherian ring of infinite Krull dimension. A ring is called catenary if any inclusion formula_15 of prime ideals can be extended to a maximal chain of prime ideals between formula_6 and formula_17, and any two maximal chains between formula_6\nand formula_17 have the same length. A ring is called universally catenary if any finitely generated algebra over it is catenary. Nagata gave an example of a Noetherian ring which is not catenary.\nIn a Noetherian ring, a prime ideal has height at most formula_2 if and only if it is a minimal prime ideal over an ideal generated by formula_2 elements (Krull's height theorem and its converse). It implies that the descending chain condition holds for prime ideals in such a way the lengths of the chains descending from a prime ideal are bounded by the number of generators of the prime.\nMore generally, the height of an ideal formula_22 is the infimum of the heights of all prime ideals containing formula_22. In the language of algebraic geometry, this is the codimension of the subvariety of formula_24 corresponding to formula_22.\nSchemes.\nIt follows readily from the definition of the spectrum of a ring Spec(\"R\"), the space of prime ideals of \"R\" equipped with the Zariski topology, that the Krull dimension of \"R\" is equal to the dimension of its spectrum as a topological space, meaning the supremum of the lengths of all chains of irreducible closed subsets. This follows immediately from the Galois connection between ideals of \"R\" and closed subsets of Spec(\"R\") and the observation that, by the definition of Spec(\"R\"), each prime ideal formula_6 of \"R\" corresponds to a generic point of the closed subset associated to formula_6 by the Galois connection.\nOf a module.\nIf formula_4 is a commutative ring, and formula_40 is an formula_4-module, we define the Krull dimension of formula_40 to be the Krull dimension of the quotient of formula_4 making formula_40 a faithful module. That is, we define it by the formula:\nformula_45\nwhere formula_46, the annihilator, is the kernel of the natural map formula_47 of formula_4 into the ring of formula_4-linear endomorphisms of formula_40.\nIn the language of schemes, finitely generated modules are interpreted as coherent sheaves, or generalized finite rank vector bundles.\nFor non-commutative rings.\nThe Krull dimension of a module over a possibly non-commutative ring is defined as the deviation of the poset of submodules ordered by inclusion. For commutative Noetherian rings, this is the same as the definition using chains of prime ideals. The two definitions can be different for commutative rings which are not Noetherian.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56080", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=56080", "title": "Pi-theorem", "text": ""}
{"id": "56082", "revid": "32156510", "url": "https://en.wikipedia.org/wiki?curid=56082", "title": "Al Quaeda", "text": ""}
{"id": "56083", "revid": "11308236", "url": "https://en.wikipedia.org/wiki?curid=56083", "title": "Public school", "text": "Public school may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "56084", "revid": "1477759", "url": "https://en.wikipedia.org/wiki?curid=56084", "title": "Kashmir crisis", "text": ""}
{"id": "56085", "revid": "50867279", "url": "https://en.wikipedia.org/wiki?curid=56085", "title": "School uniforms in Japan", "text": "The majority of Japan's junior high and high schools require students to wear school uniforms. Female Japanese school uniforms are noted for their sailor aesthetics, a characteristic adopted in the early 20th century to imitate the popular sailor dress trend occurring in Western nations. The aesthetic also arose from a desire to imitate military style dress, particularly in the design choices for male uniforms. These school uniforms were introduced in Japan in the late 19th century, replacing the traditional kimono. Today, school uniforms are common in many Japanese public and private schools. The Japanese word for the sailor style of uniform is .\nHistory.\nThe usage of School uniforms in Japan began in the mid-19th century. Previously, students wore standard everyday clothes to school: kimono for female students, with for male students. During the Meiji period, students began to wear uniforms modelled after Western dress.\nLate 19th century: The Hakama era.\nIn the 1880s female students wore Western dress, but this came to be considered impractical. Utako Shimoda, a women's activist, educator and dress reformer, found traditional kimono to be too restrictive. She argued that the Kimono prevented women and girls from moving and taking part in physical activities, thus harming their health. While western dress was being adopted at the time, she also believed corsets to be restrictive and also harmful to women's health. Utako Shimoda had worked as lady-in-waiting to Empress Sh\u014dken from 1871 to 1879. She adapted the clothing worn by ladies-in-waiting at the Japanese imperial court, which included , to make a uniform for her Jissen Women's University. During the Meiji period (1868\u20131912) and the following Taish\u014d period (1912\u20131926), other women's schools also adopted the . It became standard wear for high schools in Japan, and is still worn by many women to their university graduations.\nDuring the Taish\u014d period, male students began to wear (matching black trousers and a tunic with a standing collar and five gold buttons, and geta). These, apart from the footwear, are still worn today.\nEarly 20th century: Introduction of the Serafuku.\nThe 1920s saw the introduction of European/American-style naval uniforms, called in Japanese. The idea was taken from scaled-down sailor suits worn by children coming from royal European families, while also drawing inspiration from American female sailor dresses that had been a popular trend amongst the Youths of America in the early 20th Century. It was relatively easy to sew and thus was easily adopted in the country. Fukuoka Jo Gakuin University adopted the serafuku in 1921, while evidence suggests Heian Jogakuin University may have adopted the serafuku as early as 1920.\nWorld War II era.\nDuring World War II, student uniforms became militarised and were altered to accommodate the fact that schoolgirls were being drafted as factory workers to replace men who had gone to the front lines. Wearing skirts was deemed impractical and so loose slacks known as (a traditional Japanese farming outfit) were issued as alternatives. Students were also required to wear large name tags () bearing their name, address, and blood type for emergency identification. Additionally, head-protecting cushions () were distributed to prepare for air raids.\nMid 20th century.\nFollowing World War II and the Occupation of Japan by the United States, many schools adopted a more Western-patterned Catholic school uniform style. Compulsory education was extended to include junior high school, and public schools were required to become coeducational. With the establishment of numerous junior high schools nationwide, uniforms for boys and girls were altered. While most public junior high schools retained the traditional gakuran for boys and sailor uniforms for girls, some parents and educators felt uncomfortable with these uniforms' military associations. Seeking a more \u201cpeaceful\u201d image, certain schools adopted uniforms inspired by those from abroad. The introduction of the blazer, especially in urban areas, became increasingly popular. Additionally, the advent of synthetic fabrics allowed for the production of durable, colourful uniforms at lower costs, fostering greater variety.\nMany home economics classes in Japan up until the 1950s gave sewing sailor outfits as assignments. Girls sewed sailor outfits for younger children in their communities.\nLate 20th century.\nDuring the counterculture era of the 1970s in Japan, many left-wing and youth-led movements called for the abolishment of school uniforms in Japan entirely. These groups characterised the Seifuku and Gakuran as manifestations of Japanese authoritarianism.\nIn the 1970s and 1980s, gangs began modifying uniforms by making skirts longer and shortening the tops, and so schools began switching to blazer or sweater vest style uniforms to try to combat the effect.\nIn 1986, the Akashi School Uniform Company published the book \"\u201cSeifuku Kakumei\u201d (School Uniform Revolution)\", which successfully advocated that high schools should have their own unique variants of the Seifuku. Two years later, in 1988, the company introduced student uniforms designed by the fashion designer Hanae Mori, sparking a trend where other uniform companies began collaborating with well-known designers to create stylish uniforms. Amongst the designers who became involved in uniform design following Hanar Mori was Kansai Yamamoto.\nDuring the 1980s and 1990s, Japan experienced a period of economic prosperity often referred to as the \"Bubble Era.\" This period brought significant cultural shifts, especially among young people, who embraced the concept of kawaii (cute) as a form of self-expression. Schoolgirls, in particular, became trendsetters by adopting playful and colourful fashion elements into their uniforms. The sailor uniform remained popular, but it was often customized with longer skirts, colourful ribbons, and loose socks. Another popular style was the burusera look, combining sailor uniforms with casual elements like cardigans and loafers. This trend coincided with the emergence of the kogal subculture, where girls tanned their skin, dyed their hair blonde or brown, and wore shortened skirts. While these styles were seen as rebellious, they were also perceived as empowering expressions of individuality. This fashion movement significantly influenced Japanese pop culture, from music videos to television dramas, and sparked international interest in Japanese youth fashion.\n21st century.\nAs Japan entered the 21st century, educational reforms and societal changes led to the gradual standardization of school uniforms. Many schools moved away from sailor suits in favour of blazer uniforms, which were considered more practical and professional. These new uniforms typically featured pleated skirts, blazers, ties, and loafers for girls, while boys wore blazers with trousers and ties. The gyaru subculture of the 2000s, with its bold fashion statements, continued to influence school uniform modifications. However, as social norms tightened and dress codes became stricter, overt customization became less common. Instead, the focus shifted towards more subtle personalisation, such as varying the style of socks or adding discreet accessories.\nAs of 2012[ [update]], 50% of Japanese junior high schools and 20% of senior high schools use sailor suit uniforms. The stated in 2012 that, \"The sailor suit is changing from adorable and cute, a look that 'appeals to the boys,' to a uniform that \"girls like to wear for themselves.\" As of that year, contemporary sailor suits have front closures with zippers or snaps and more constructed bodices. The stated that \"the form is snug to enhance the figure\u2014the small collar helps the head look smaller, for better balance.\"\nIn the 2020s many Japanese high schools are pivoting to items such as gender-neutral blazers to accommodate LGBT students.\nUsage.\nThe Japanese junior and senior-high-school uniform traditionally consists of a military-styled uniform for boys and a sailor outfit for girls. These uniforms are based on Meiji-period formal military dress, themselves modeled on American/European-style naval uniforms. The sailor outfits replace the undivided (known as ) designed by Utako Shimoda between 1920 and 1930. While this style of uniform is still in use, many schools have moved to more Western-pattern Catholic school uniform styles. These uniforms consist of a white shirt, tie, blazer with school crest, and tailored trousers (often not of the same colour as the blazer) for boys and a white blouse, tie, blazer with school crest, and tartan culottes or skirt for girls.\nRegardless of what type of uniform any particular school assigns its students, all schools have a summer version of the uniform (usually consisting of just a white dress shirt and the uniform slacks for boys and a reduced-weight traditional uniform or blouse and tartan skirt with tie for girls) and a sports-activity uniform (a polyester track suit for year-round use and a T-shirt and short pants for summer activities). Depending on the discipline level of any particular school, students may often wear different seasonal and activity uniforms within the same classroom during the day. Individual students may attempt to subvert the system of uniforms by wearing their uniforms incorrectly or by adding prohibited elements such as large loose socks or badges. Girls may shorten their skirts, permanently or by wrapping up the top to decrease length; boys may wear trousers about the hips, omit ties, or keep their shirts unbuttoned.\nSince some schools do not have sex-segregated changing- or locker-rooms, students may change for sporting activities in their classrooms. As a result, such students may wear their sports uniforms under their classroom uniforms. Certain schools also regulate student hairstyles, footwear, and book bags; but these particular rules are usually adhered to only on special occasions, such as trimester opening and closing ceremonies and school photo days.\nIt is normal for uniforms to be worn outside of school areas, but this is going out of fashion and many students wear casual dress outside of school. While not many public elementary schools in Japan require uniforms, many private schools and public schools run by the central government still do so.\nThe , also called the , is a uniform for junior high school and senior high school boys in Japan. The colour is normally black, but some schools use navy blue.\nThe top has a standing collar buttoning down from top-to-bottom. Buttons are usually decorated with the school emblem to show respect to the school. Pants are straight leg and a black or dark-coloured belt is worn with them. Boys usually wear penny loafers or sneakers with this uniform. Some schools may require the students to wear collar-pins representing the school and/or class rank.\nTraditionally, the is also worn along with a matching (usually black) student cap, although this custom is less common in modern times.\nThe is derived from the Prussian Waffenrock or the Christian clergy cassock. The term is a combination of meaning \"study\" or \"student\", and meaning the Netherlands or, historically in Japan, the West in general; thus, translates as \"Western style clothes for student (uniform)\".\nThe original model of the present day was first established in 1873 for students of all schools. During the Japanese occupation, such clothing was also brought to school in Korea, Taiwan, and Manchukuo. Nowadays, the is still worn in some South Korean conservative high schools.\nWhile the is associated solely as the boys' uniform of both most middle schools and conservative high schools in Japan nowadays, blazers began to be adopted in most number of high schools in Japan (both public and private).\nSailor.\nThe is a common style of uniform worn by female middle school students, traditionally by high school students, and occasionally, elementary school students. It was introduced as a school uniform in 1920 at Heian Jogakuin University and 1921 by the principal of Fukuoka Jo Gakuin University, Elizabeth Lee. It was modeled after the uniform used by the British Royal Navy at the time, which Lee had experienced as an exchange student in the United Kingdom, as well as the popular American Sailor dress which at the time had already been a common fashion choice amongst school girls in the United States during the time period since the start of the 20th century.\nMuch like the male uniform, the , the sailor outfits bear a similarity to various military-styled naval uniforms. The uniform generally consists of a blouse attached with a sailor-style collar and a pleated skirt. There are seasonal variations for summer and winter; sleeve length and fabric are adjusted accordingly. A ribbon is tied in the front and laced through a loop attached to the blouse. Several variations on the ribbon include neckties, bolo ties, neckerchiefs, and bows. Common colours include navy blue, white, gray, light green, and black.\nShoes, socks, and other accessories are sometimes included as part of the uniform. These socks are typically navy or white. The shoes are typically brown or black penny loafers. Although not part of the prescribed uniform, alternate forms of legwear (such as loose socks, knee-length stockings, or similar) are also commonly matched by more fashionable girls with their sailor outfits.\nThe sailor uniform today is generally associated solely with middle schools and conservative high schools, since a majority of high schools have changed to more Western-style tartan skirts or blazers, similar to the Catholic school uniform.\nGenderless uniforms.\nHistorically, school uniforms in Japan are decided on the basis of sex, with trousers for male students and skirts for female students. However, in April 2019, public junior high schools in Tokyo's Nakano Ward began allowing students to choose their uniform regardless of sex. This started with a sixth grader who did not want to wear skirts in junior high school and asked her female classmates for their opinions on uniforms. The responses showed that most of her classmates also wanted the freedom to choose their uniforms. The young student delivered the survey results to the mayor of Nakano, and all of the principals for the ward's public junior high schools agreed on the proposal, allowing students to freely choose their uniforms.\nSchools allowing trousers for female students rose to 600 in 2019 from only four in 1997, and over 400 schools adopted genderless uniforms for 2022's fiscal year. There was a lot of support from female students for the adaptation of genderless uniforms and the implementation of slacks since it allowed for more comfort by keeping their legs warm and making it easier to ride their bicycles. The decision for genderless uniforms is also in consideration of sexual minority students.\nIn addition to changes made in the uniform, schools made adaptations to the school bags and uniforms for outside-of-class activities. In 2022, genderless swimwear was introduced at a few high schools and has quickly spread to more schools throughout Japan. Genderless swimwear gradually evolved from the need to protect against sunburn to a desire to deemphasize body shape by adding more coverage.\nCultural significance.\nSchool uniform varies throughout different schools in Japan, with some schools known for their particular uniforms. School uniform can have a nostalgic characteristic for former students, and are often associated with relatively carefree youth. Uniforms are sometimes modified by students as a means of exhibiting individualism. This is done in ways such as lengthening or shortening the skirt, removing the ribbon, hiding patches or badges under the collar, etc. In past decades, brightly coloured variants of the sailor outfits were also adopted by Japanese , and biker gangs.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56086", "revid": "43473171", "url": "https://en.wikipedia.org/wiki?curid=56086", "title": "Territorial dispute", "text": "Disagreement over the possession or control of land between countries or their subdivisions\nA territorial dispute or boundary dispute is a disagreement over the possession or control of territories (land, water or airspace) between two or more political entities.\nContext and definitions.\nTerritorial disputes are often related to the possession of natural resources such as rivers, fertile farmland, mineral or petroleum resources although the disputes can also be driven by culture, religion, and ethnic nationalism. Territorial disputes often result from vague and unclear language in a treaty that set up the original boundary.\nTerritorial disputes are a major cause of wars and terrorism, as states often try to assert their sovereignty over a territory through invasion, and non-state entities try to influence the actions of politicians through terrorism. International law does not support the use of force by one state to annex the territory of another state. The https:// states, \"All Members shall refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the Purposes of the United Nations.\"\nIn some cases in which the boundary is not demarcated, such as the Taiwan Strait, and Kashmir, the parties involved define a Line of Control, which serves as the \"de facto\" international border.\nBasis in international law.\nTerritorial disputes have significant meaning in the international society, both by their relation to the fundamental right of states, sovereignty and also because they are important for international peace. International law has significant relations with territorial disputes because territorial disputes tackles the basis of international law; the state territory. International law is based on the persons of international law, which requires a defined territory, as mentioned in the 1933 Montevideo Convention on the Rights and Duties of States.\nArticle 1 of the Montevideo Convention declares that \"[t]he state as a person of international law should possess the following qualifications: (a) a permanent population; (b) a defined territory; (c) government; and (d) capacity to enter into relations with other States\" \nAlso, B. T. Sumner's article mentions, \"In international law and relations, ownership of territory is significant because sovereignty over land defines what constitutes a state.\"\nTherefore, the breach of a country's borders or territorial disputes pose a threat to a state's very sovereignty and the right as a person of international law. In addition, territorial disputes are sometimes brought to the International Court of Justice, as was the case in Costa Rica and Nicaragua (2005). Territorial disputes cannot be separated from international law, whose basis is on the law of state borders, and their potential settlement also relies on international law and the Court.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56087", "revid": "791042707", "url": "https://en.wikipedia.org/wiki?curid=56087", "title": "Economic", "text": ""}
{"id": "56090", "revid": "76", "url": "https://en.wikipedia.org/wiki?curid=56090", "title": "Territorial conflict", "text": ""}
{"id": "56092", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=56092", "title": "Trieste", "text": "City in Friuli-Venezia-Giulia, Italy\nTrieste ( , ; see more) is a city and seaport in northeast Italy. It is the capital and largest city of the autonomous region of Friuli-Venezia Giulia, as well as of the regional decentralization entity of Trieste. As of 2025, it has a population of 198,668.\nTrieste is located at the head of the Gulf of Trieste, on a narrow strip of Italian territory lying between the Adriatic Sea and Slovenia; Slovenia lies close, at approximately east and southeast of the city, while Croatia is about to the south of the city. The city has a long coastline and is surrounded by grassland, forest, and karstic areas.\nTrieste belonged, as Triest, to the Habsburg monarchy from 1382 until 1918. In the 19th century, the monarchy was one of the Great Powers of Europe and Trieste was its most important seaport. As a prosperous trading hub in the Mediterranean region, Trieste grew to become the fourth largest city of the Austro-Hungarian Empire (after Vienna, Budapest, and Prague). At the turn of the 20th century, it emerged as an important hub for literature and music. Trieste underwent an economic revival during the 1930s, and the Free Territory of Trieste became a major site of the struggle between the Eastern and Western blocs after the Second World War.\nA deep-water port, Trieste is a maritime gateway for northern Italy, Germany, Austria and Central Europe. It is considered the end point of the maritime Silk Road, with connections to the Mediterranean, Suez Canal and Atlantic Ocean. Since the 1960s, Trieste has emerged as a prominent research location in Europe because of its many international organisations and institutions. The city lies at the intersection of Latin (due to Italy), Slavic (due to Slovenia and Croatia) and Germanic (due to Austria-Hungary) cultures, where Central Europe meets the Mediterranean Sea, and is home to diverse ethnic groups and religious communities.\nA scholarly area, Trieste has the highest percentage of researchers, per capita, in Europe. (\"City of the Barcolana\"), (\"City of the bora\"), (\"City of Wind\"), \"Vienna by the sea\" and \"City of Coffee\" are epithets used to describe Trieste.\nEtymology.\nThe most likely origin is the word, \"Tergeste\" \u2013 with the \"-est-\" suffix typical of Venetic \u2013 and derived from the hypothetical Illyrian word \"*terg-\" \"market\" (etymologically cognate to the Albanian term 'market, marketplace' and reconstructed Proto-Slavic \"*t\u044arg\u044a\") Roman authors also transliterated the name as \"Tergestum\" (according to Strabo, the name of the oppidum Tergestum originated from the three battles the Roman Army had to engage in with local tribes, \"TER GESTUM [BELLUM]\").\nHistory.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nTimeline of Trieste &lt;br&gt;\n \u00a0Roman Empire, pre 395&lt;br&gt;\n\u00a0Western Roman Empire, 395\u2013476&lt;br&gt;\n\u00a0Byzantine Empire, 476\u2013567&lt;br&gt;\n\u00a0Lombards, 567\u2212788&lt;br&gt;\n\u00a0Francia, 788\u2212843&lt;br&gt;\n\u00a0Middle Francia, 843\u2212855&lt;br&gt;\n\u00a0Patriarchate of Aquileia, 855\u2013952&lt;br&gt;\n\u00a0March of Verona, 952\u20131081&lt;br&gt;\n Patria del Friuli, 1081\u20131368 \n Republic of Venice, 1368\u20131369 \n\u00a0Patriarchate of Aquileia, 1378\u20131382&lt;br&gt;\n Holy Roman Empire, 1382\u22121806&lt;br&gt;\n Austrian Empire, 1804\u20131809&lt;br&gt;\n First French Empire, 1809\u20131814&lt;br&gt;\n Austrian Empire, 1814\u20131867&lt;br&gt;\n Austria-Hungary, 1867\u22121922&lt;br&gt;\n Kingdom of Italy, 1922\u20131943&lt;br&gt;\n OZAK, 1943\u20131945&lt;br&gt;\n Allied Military Government, 1945\u20131947&lt;br&gt;\n Free Territory of Trieste, 1947\u20131954&lt;br&gt;\n Italy, 1954\u2013present\nAncient history.\nSince the second millennium BC, the location was an inhabited site. Originally an Illyrian settlement, the Veneti entered the region in the 10th\u20139th c. BC and seem to have given the town its name, \"Tergeste\", because \"terg*\" is a Venetic word meaning market (q.v. Oderzo, whose ancient name was \"Opitergium\"). Later, the town was captured by the Carni, a tribe of the Eastern Alps, before becoming part of the Roman Republic in 177 BC during the Second Istrian War.\nAfter being attacked by barbarians from the interior in 52 BC, and until 46 BC, it was granted the status of Roman colony under Julius Caesar, who recorded its name as \"Tergeste\" in \"Commentarii de Bello Gallico\" (51 BC), in which he recounts events of the Gallic Wars.\nDuring the imperial period the border of Roman Italy moved from the Timavo River to the Formione (today Risano). Roman Tergeste flourished due to its position on the road from Aquileia, the main Roman city in the area, to Istria, and as a port, some ruins of which are still visible. Emperor Augustus built a line of walls around the city in 33\u201332 BC, while Trajan built a theatre in the 2nd century. At the same time, the citizens of the town were enrolled in the tribe Pupinia. In 27 BC, Trieste was incorporated in \"Regio X\" of Augustan \"Italia\".\nIn the early Christian era Trieste continued to flourish. Between 138 and 161 AD, its territory was enlarged and nearby Carni and Catali were granted Roman citizenship by the Roman Senate and Emperor Antoninus Pius at the pleading of a leading Tergestine citizen, the \"quaestor urbanus\", Fabius Severus.\nAlready at the time of the Roman Empire there was a fishing village called Vallicula (\"small valley\") in the Barcola area. Remains of richly decorated Roman villas, including wellness facilities, piers and extensive gardens suggest that Barcola was already a place for relaxation among the Romans because of its favourable microclimate, as it was located directly on the sea and protected from the bora. At that time, Pliny the Elder mentioned the vines of the wine Pulcino (\"Vinum Pucinum\" \u2013 probably today's \"Prosecco\"), which were grown on the slopes.\nMiddle Ages.\nIn 788, Trieste submitted to Charlemagne, who placed it under the authority of the count-bishop who in turn was subject to the Duke of Fri\u00f9li.\nDuring the 13th and 14th centuries, Trieste became a maritime trade rival to the Republic of Venice, which briefly occupied it in 1283\u201387, before coming under the patronage of the Patriarchate of Aquileia. After it committed a perceived offence against Venice, the Venetian State declared war against Trieste in July 1368 and by November had occupied the city. Venice intended to keep the city and began rebuilding its defences, but was forced to leave in 1372. Due to the Peace of Turin in 1381, Venice renounced its claim to Trieste and the leading citizens of Trieste petitioned Leopold III of Habsburg, Duke of Austria, to annex Trieste to his domains. The agreement of voluntary submission (\"dedizione\") was signed at the castle of Graz on 30 September 1382.\nThe city maintained a high degree of autonomy under the Habsburgs, but was increasingly losing ground as a trade hub, both to Venice and to Ragusa. In 1463, a number of Istrian communities petitioned Venice to attack Trieste. Trieste was saved from utter ruin by the intervention of Pope Pius II who had previously been bishop of Trieste. However, Venice limited Trieste's territory to outside the city. Trieste would be assaulted again in 1468\u20131469 by Holy Roman Emperor Frederick III. His sack of the city is remembered as the \"Destruction of Trieste.\" He then restored the city walls for the fourth time. Trieste was fortunate to be spared another sack in 1470 by the Ottomans who burned the village of Prosecco, only about from Trieste, while on their way to attack Friuli.\nEarly modern period.\nFollowing an unsuccessful Habsburg invasion of Venice in the prelude to the 1508\u201316 War of the League of Cambrai, the Venetians occupied Trieste again in 1508, and were allowed to keep the city under the terms of the peace treaty. However, the Habsburg Empire recovered Trieste a little over one year later, when the conflict resumed. By the 18th century Trieste became an important port and commercial hub for the Austrians. In 1719, it was granted status as a free port within the Habsburg Empire by Emperor Charles VI, and remained a free port until 1 July 1791. The reign of his successor, Maria Theresa of Austria, marked the beginning of a very prosperous era for the city. Serbs settled Trieste largely in the 18th and 19th centuries, and they soon formed an influential and rich community within the city, as a number of Serbian traders came into ownership of many important businesses and built palaces across Trieste.\n19th century.\nIn the following decades, Trieste was briefly occupied by troops of the French Empire during the Napoleonic Wars on several occasions, in 1797, 1805 and 1809. From 1809 to 1813, Trieste was annexed into the Illyrian Provinces, interrupting its status of free port and losing its autonomy. The municipal autonomy was not restored after the return of the city to the Austrian Empire in 1813. Following the Napoleonic Wars, Trieste continued to prosper as the Free Imperial City of Trieste (), a status that granted economic freedom, but limited its political self-government. The city's role as Austria's main trading port and shipbuilding centre was later emphasised by the foundation of the merchant shipping line Austrian Lloyd in 1836, whose headquarters stood at the corner of the Piazza Grande and Sanit\u00e0 (today's Piazza Unit\u00e0 d'Italia). By 1913, Austrian Lloyd had a fleet of 62 ships totalling 236,000 tonnes. With the introduction of constitutionalism in the Austrian Empire in 1860, the municipal autonomy of the city was restored, with Trieste becoming capital of the Austrian Littoral crown land ().\nWith anti-clericalism on the rise in the rest of the Italian peninsula due to the Kingdom of Piedmont-Sardina's bellicose policies towards the church and its estates, Pope Leo XIII at times considered moving his residence to Trieste or Salzburg. However, Emperor Franz Joseph rejected the idea. Trieste, along with Rijeka (Fiume), served as an important base for the Imperial-Royal Navy, which in the first decade of the 20th century embarked on a major modernisation programme. With the construction of the Austrian Southern Railway, the first major railway in the Empire, in 1857, Trieste acquired a significant role in the trade of coal. \nTrieste had long been home to Italian irredentist sentiment, as evidenced by the activity at Caff\u00e8 Tommaseo. In 1882 this fervour culminated in an attempted assassination of Emperor Franz Joseph at the hands of Wilhem Oberdank (Guglielmo Oberdan), while His Majesty was visiting the city. The perpetrator was arrested, tried, found guilty and ultimately sentenced to death. His legacy was regarded as worthy of martyrdom status by fellow irredentists, while monarchical elements regarded his actions as ignominious. The Emperor, who went on to reign for thirty-four more years, never again visited Trieste.\n20th century.\nAt the beginning of the 20th century, Trieste was a bustling cosmopolitan city frequented by artists and philosophers. James Joyce was a long-stay tourist between 1904 and 1915. Joyce worked on \"Dubliners\" and \"Ulysses\" while in Trieste. His students included Italo Svevo and a bookshop run by Umberto Saba was near Joyce's apartment. Other authors with roots in Trieste include Claudio Magris, Jan Morris, Fulvio Tomizza, Enzo Bettiza, Susanna Tamaro, and Ernesto Nathan Rogers. Sigmund Freud, Zofka Kveder, Dragotin Kette, Ivan Cankar, and Scipio Slataper have also been associated with Trieste. The city was the major port on the \"Austrian Riviera\", a term used in tourist marketing.\nWorld War I, annexation to Italy and Fascist era.\nItaly, in return for entering World War I on the side of the Allied Powers, had been promised substantial territorial gains, which included the former Austrian Littoral and western Inner Carniola. Italy therefore annexed the city of Trieste at the end of the war, in accordance with the provisions of the 1915 Treaty of London and the Italian-Yugoslav 1920 Treaty of Rapallo.\nIn the late 1920s, following Italian fascists burning down of the Slovene cultural centre in July 1920, the Slovene militant anti-fascist organisation TIGR carried out several bomb attacks in the city centre. In 1930 and 1941, two trials of Slovene activists were held in Trieste by the fascist Special Tribunal for the Security of the State. During the 1920s and 1930s, several monumental buildings were built in the Fascist architectural style, including the University of Trieste and the almost tall Victory Lighthouse (Faro della Vittoria), which became a city landmark. The economy improved in the late 1930s, and several large infrastructure projects were carried out.\nWorld War II and aftermath.\nFollowing the trisection of Slovenia, starting from the winter of 1941, the first Slovene Partisans appeared in Trieste province, although the resistance movement did not become active in the city itself until late 1943.\nAfter the Italian armistice in September 1943, the city was occupied by Wehrmacht troops. Trieste became nominally part of the newly constituted Italian Social Republic, but it was de facto ruled by Germany, who created the Operation Zone of the Adriatic Littoral (OZAK) out of former Italian north-eastern regions, with Trieste as the administrative centre. The new administrative entity was headed by Friedrich Rainer, Gauleiter of Carinthia, named supreme commissary of the AK zone. A semblance of indigenous Italian rule was kept in the form of Cesare Pagnini, mayor of Trieste, but every civil official was assigned a representative of the supreme commissar in the form of a Deutsche Berater (German Adviser). Under German occupation, the only concentration camp with a crematorium on Italian soil was built in a suburb of Trieste, at the Risiera di San Sabba on 4 April 1944. From 20 October 1943, to the spring of 1944, around 25,000 Jews and partisans were interrogated and tortured in the Risiera. Three to four thousand of them were murdered here by shooting, beating or in gas vans. Most were imprisoned before being transferred to other concentration camps.\nThe city saw intense Italian and Yugoslav partisan activity and suffered from Allied bombings, over 20 air raids in 1944\u20131945, targeting the oil refineries, port and marshalling yard but causing considerable collateral damage to the city and 651 deaths among the population. The worst raid took place on 10 June 1944, when a hundred tons of bombs dropped by 40 USAAF bombers, targeting the oil refineries, resulted in the destruction of 250 buildings, damage to another 700 and 463 victims.\nOccupation by Yugoslav Partisans.\nOn 30 April 1945, the Slovenian and Italian anti-Fascist Osvobodilna fronta (OF) and National Liberation Committee (Comitato di Liberazione Nazionale, or CLN) of Edoardo Marzari and Antonio Fonda Savio, made up of approximately 3,500 volunteers, incited a riot against the Nazi occupiers. On 1 May Allied members of the Yugoslav Partisans' 8th Dalmatian Corps took over most of the city, except for the courts and the castle of San Giusto, where the German garrisons refused to surrender to anyone but the New Zealanders, due to the Partisans' reputation for shooting German and Italian prisoners of war. The 2nd New Zealand Division under General Freyberg continued to advance towards Trieste along Route 14 around the northern coast of the Adriatic sea and arrived in the city the following day (see official histories \"The Italian Campaign\" and \"Through the Venetian Line\"). The German forces surrendered on the evening of 2 May, but were then turned over to the Yugoslav forces.\nThe Yugoslavs held full control of the city until 12 June, a period known in Italian historiography as the \"forty days of Trieste\". During this period, hundreds of local Italians and anti-Communist Slovenes were arrested by the Yugoslav authorities, and many of them were never seen again. Some were interned in Yugoslav internment camps (in particular at Borovnica, Slovenia), while others were murdered on the Karst Plateau. British Field Marshal Harold Alexander condemned the Yugoslav military occupation, stating that \"Marshal Tito's apparent intention to establish his claims by force of arms...[is] all too reminiscent of Hitler, Mussolini and Japan. It is to prevent such actions that we have been fighting this war.\" In this most turbulent of periods, the city saw a thorough reorganisation of the political-administrative system: the Yugoslav Fourth Army, to which many figures of prominence were attached (including Edvard Kardelj, a sign of just how important the Isonzo front was in Yugoslav aims) established a provisional Military Command in the occupied areas. Fully understanding the precarious position it found itself in, the Yugoslav Command undertook great efforts to claim the success for itself, faced with the presence of the 2nd New Zealand Division under General Bernard Freyberg in Trieste, which could undermine, as it did, postwar claims of sovereignty and control over the seaport. The journalist Geoffrey Cox wrote that it was \"the first major confrontation of the Cold War\" and was \"the one corner of Europe where no demarcation line had been agreed upon in advance by the Allies.\" To this effect, a Tanjug Agency communiqu\u00e9 stated: \"The seaport of Trieste, Monfalcone and Gorizia could not be occupied by the above mentioned division [the New Zealand Division] as these cities had already been liberated...by the Yugoslav army...It is true that some Allied forces have without our permission entered into the above mentioned cities which might have undesirable consequences unless this misunderstanding is promptly settled by mutual agreement.\"\nA city in limbo (1945\u20131947).\nAfter an agreement between the Yugoslav leader Josip Broz Tito and Field Marshal Alexander, the Yugoslav forces withdrew from Trieste, which came under a joint British-U.S. military administration. The Julian March was divided by the Morgan Line between Anglo-American and Yugoslav military administration until September 1947 when the Paris Peace Treaty established the Free Territory of Trieste. The effective turning point for Trieste's fortunes had already been established, though: President Truman's stipulations, later named the Truman Doctrine, in all but name had sealed the status quo, formalised only in the above-mentioned treaty, one that proved to be a careful balancing act between Yugoslav demands, Italian claims and international aims toward the Adriatic gulf and Eastern Europe in general. Questions arose on the structure of government as soon and even earlier than the signing of the treaty, with neither Italy nor Yugoslavia willing to recognise a joint governor. Initially, the newly established Allied Military Government (AMG) found it difficult to exercise its authority over the newly administered territories (the Italian majority provinces of Trieste, Gorizia and Pola), because of a rooted communist presence, especially in the countryside. This state of affairs did not change until a formal peace treaty with Italy had been signed, granting the AMG the full powers to administer justice and re-establish law and order in those areas under its administration. Replacing the People's Militia, the AMG recruited a civilian police force from the indigenous population along the Anglo-Saxon police model. This exercise of jurisdiction was thus articulated: pursuant to Proclamation No. 1, three tiers of tribunals were established: the Summary Military Courts, with jurisdiction over petty crime, the Superior Military Courts, which could impose punishments not exceeding 10 years imprisonment, and the General Military Court, which could impose the death penalty. Civil courts, as modelled on the Kingdom of Italy's code, were, pursuant to General Order No. 6, re-established 12 July 1945, but the Slovene minority was given the right to be heard, and for proceedings to be, in their own language.\nZone A of the Free Territory of Trieste (1947\u201354).\nIn 1947, Trieste was declared an independent city state under the protection of the United Nations as the Free Territory of Trieste. The territory was divided into two zones, A and B, along the Morgan Line established in 1945.\nFrom 1947 to 1954, Zone A was occupied and governed by the Allied Military Government, composed of the American Trieste United States Troops (TRUST), commanded by Major General Bryant E. Moore, the commanding general of the American 88th Infantry Division, and the \"British Element Trieste Forces\" (BETFOR), commanded by Sir Terence Airey, who were the joint forces commander and also the military governors.\nZone A covered almost the same area of the current Italian Province of Trieste, except for four small villages south of Muggia (see below), which were given to Yugoslavia after the dissolution of the Free Territory in 1954. Occupied Zone B, which was under the administration of Milo\u0161 Stamatovi\u0107, then a colonel in the Yugoslav People's Army, was composed of the north-westernmost portion of the Istrian peninsula, between the Mirna River and the cape Debeli Rti\u010d.\nIn 1954, in accordance with the Memorandum of London, the vast majority of Zone A\u2014including the city of Trieste\u2014joined Italy, whereas Zone B and four villages from Zone A (Plavje, Spodnje \u0160kofije, Hrvatini, and Elerji) became part of Yugoslavia, divided between Slovenia and Croatia. The final border line with Yugoslavia and the status of the ethnic minorities in the areas was settled bilaterally in 1975 with the Treaty of Osimo. This line now constitutes the border between Italy and Slovenia.\nGeography.\nTrieste is located in the northernmost part of the high Adriatic, in northeastern Italy, near the border with Slovenia. The city lies on the Gulf of Trieste. Built mostly on a hillside, Trieste's urban territory lies at the foot of an imposing escarpment that comes down abruptly from the Karst Plateau towards the sea. The karst hills delimiting the city reach an elevation of above sea level. It lies at the junction point of the Italian geographical region, the Balkan Peninsula, and Mitteleuropan Area.\nClimate.\nTrieste's climate is humid subtropical (K\u00f6ppen: \"Cfa\", Trewartha: \"Cf\"), with cool winters and hot summers. On average, relative humidity is low (~ 65%), while only three months (January, March and July) receive slightly less than of precipitation.\nTrieste, like the Istrian Peninsula, has evenly distributed rainfall above in total; it is noteworthy that no true summer drought occurs. Snow occurs on average 2 days per year.\nWinter highs are lower than the average temperatures in the Mediterranean zone. Two basic weather patterns alternate \u2014 sunny, windy and often cold days frequently caused a northeastern wind called bora, and rainy days with temperatures of about .\nSummer is very warm with highs of about and lows above , with hot nights being influenced by the warm seawater. The highest temperature of the last 30 years is in 2020, whereas the absolute minimum was in 1996.\nThe Trieste area is divided into 8a\u201310a zones according to USDA hardiness zoning; Villa Opicina (320 to 420 MSL), with an 8a zone in the upper suburban area down to a 10a zone in the shielded and windproof valleys close to the Adriatic sea.\nThe climate can be severely affected by the bora, a very dry and usually cool north-to-northeast katabatic wind that can last for some days and reach speeds of up to on the piers of the port, thus sometimes lowering temperatures to subzero levels.\nGovernment.\nMayors of Trieste since 1949:\nAdministrative divisions.\nTrieste is administratively divided into seven districts, which in turn are further subdivided into parishes (\"frazioni\"):\nThe iconic city centre is Piazza Unit\u00e0 d'Italia, which is located between the large 19th-century avenues of Borgo Teresiano and the old medieval city, characterised by many narrow streets.\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nAs of 2025, Trieste has a population of 198,668, of whom 48.3% were male and 51.7% were female. Minors make up 13.0% of the population, and pensioners who make up 28.4%. This compares with the Italian average of 14.9% and 24.7% respectively. Between 2011 and 2021, the population of Trieste declined by 1.5%, while Italy as a whole declined 0.7%.\nSince the annexation to Italy after World War I, there has been a steady decline in Trieste's demographic weight compared to other cities. In 1911, Trieste was the 4th largest city in the Austro-Hungarian Empire (3rd largest in the Austrian part of the Monarchy). In 1921, Trieste was the 8th largest city in the country, in 1961 the 12th largest, in 1981 the 14th largest, while it is now in 15th place.\nThere are 24,949 foreign-born residents in Trieste, representing 12.6% of the total population. The largest native minorities are Slovenes, Croats and Serbs, but there is also a large immigrant group from Balkan nations (particularly Serbia, Romania and Croatia): 4.95%, Asia: 0.52%, and sub-saharan Africa: 0.2%. The Serbian community consists of both autochthonous and immigrant groups. Trieste is predominantly Roman Catholic.\nLanguages.\nThe particular dialect of Trieste, called tergestino, spoken until the beginning of the 19th century, was surpassed in relevance by the Triestine dialect of Venetian (a language deriving directly from Vulgar Latin) and other languages, including standard Italian, Slovene, and German. While Triestine and Italian were spoken by the largest part of the population, German was the language of the Austrian bureaucracy and Slovene was predominantly spoken in the surrounding villages. From the last decades of the 19th century, the number of speakers of Slovene grew steadily, reaching 25% of the overall population of Trieste in 1911.\nAccording to the 1911 census, the proportion of Slovene speakers grew to 12.6% in the city centre (15.9% counting only Austrian citizens), 47.6% in the suburbs (53% counting only Austrian citizens), and 90.5% in the surroundings. They were the largest ethnic group in nine of the nineteen urban neighbourhoods of Trieste, and represented a majority in seven of them. The Italian speakers, on the other hand, made up 60.1% of the population in the city centre, 38.1% in the suburbs, and 6.0% in the surroundings. They were the largest linguistic group in ten of the nineteen urban neighbourhoods, and represented the majority in seven of them (including all six in the city centre). German speakers amounted to 5% of the city's population, with the highest proportions in the city centre.\nThe city also had several other smaller ethnic communities, including Croats, Czechs, Istro-Romanians, Serbs and Greeks, who mostly assimilated either into the Italian or the Slovene-speaking communities. Altogether, in 1911, 51.83% of the population of the municipality of Trieste spoke Italian, 24.79% spoke Slovene, 5.2% spoke German, 1% spoke Croatian, 0.3% spoke \"other languages\", and 16.8% were foreigners, including a further 12.9% Italians (immigrants from the Kingdom of Italy and thus considered separately from Triestine Italians) and 1.6% Hungarians.\nBy 1971, following the emigration of Slovenes to neighbouring Slovenia and the immigration of Italians from other regions (and from Yugoslav-annexed Istria) to Trieste, the percentage of Italian speakers had risen to 91.8%, and that of Slovenian speakers had dwindled to 5.7%.\nToday, the dominant local dialect of Trieste is \"Triestine\" (\"triestin\", pronounced ), a form of Venetian. This dialect and official Italian are spoken in the city, while Slovene is spoken in some of the immediate suburbs. There are also small numbers of Serbo-Croatian, German, Greek, and Hungarian speakers.\nMain sights and vistas.\nIn 2012, Lonely Planet listed Trieste as the world's most underrated travel destination.\nCastles.\nCastello Miramare (Miramare Castle).\nThe Castello Miramare, or Miramare Castle, on the waterfront from Trieste, was built between 1856 and 1860 in a project by Carl Junker, commissioned by Archduke Maximilian. The castle gardens comprise a variety of trees, chosen by and planted on the orders of Maximilian. Features of the gardens include two ponds, one noted for its swans and the other for lotus flowers, the castle dependence (\"Castelletto\"), a bronze statue of Maximilian, and a small chapel where a cross made from the remains of the \"Novara\" is kept, the flagship on which Maximilian, brother of Emperor Franz Josef, set sail to become Emperor of Mexico.\nDuring the 1930s, the castle was also the home of Prince Amedeo, Duke of Aosta, the last commander of Italian forces in East Africa during the Second World War. During the period of the application of the Instrument for the Provisional Regime of the Free Territory of Trieste, as established in the Treaty of Peace with Italy (Paris 10/02/1947), the castle served as headquarters for the United States Army's TRUST force.\nCastel San Giusto.\nThe Castel San Giusto, or Castle of San Giusto, was built upon the remains of previous castles on the site and took almost two centuries to build. The stages of the development of the castle's defensive structures are marked by the following periods: the central part built, under Frederick III, Holy Roman Emperor (1470\u20131), the round Venetian bastion (1508\u20139), the Hoyos-Lalio bastion and the Pomis, or \"Bastione fiorito\" dated 1630.\nArchaeological remains.\nThe ruins of the temple dedicated to Zeus are next to the Forum, those of Athena's temple are under the basilica, visitors can see its basement.\nRoman theatre.\nThe Roman theatre lies at the foot of the San Giusto hill, facing the sea. The construction partially exploits the gentle slope of the hill, and much of the theatre is made of stone. The topmost portion of the steps and the stage were supposedly made of wood. The statues which adorned the theatre, brought to light in the 1930s, are now preserved in the town museum. Three inscriptions from the Trajanic period mention a certain Q. Petronius Modestus, someone closely connected to the development of the theatre, which was erected during the second half of the 1st century. This amphitheatre held tragedies, comedies, and gladiator fights. It was hidden for quite some time and was excavated in 1937\u20131938.\nCaves.\nIn the entire Province of Trieste, there are 10 speleological groups out of 24 in the whole Friuli-Venezia Giulia region. The Trieste plateau (Altopiano Triestino), called Kras or the Carso and covering an area of about within Italy has approximately 1,500 caves of various sizes (like that of Basovizza, now a monument to the Foibe massacres).\nAmong the more famous are the Grotta Gigante, the largest tourist cave in the world, with a single cavity large enough to contain St Peter's in Rome, and the Cave of Trebiciano, deep, at the bottom of which flows the Timavo River. This river dives underground at the \u0160kocjan Caves in Slovenia (on the UNESCO list and only a few kilometres from Trieste) and flows about before emerging about from the sea in a series of springs near Duino, reputed by the Romans to be an entrance to Hades (\"the world of the dead\").\nBeaches.\nMuch of Trieste lies directly on the sea. Some bathing establishments are located in the very centre, like the \"El Pedocin - Bagno marino La Lanterna\" and the \"Ausonia\". The \"Bagno Marino Ferroviario\" has been located in Viale Miramare 30 since 1925. Many locals and students use their lunch break or free time to go to Barcola, which is an urban beach, to meet friends on the famous mile-long embankment. In the evening, many locals walk there between the bars with a view of the sea, the Alpine arc, Istria and the city.\nWell-known are the 10 popular semi-circular units on the bank consisting of a viewing platform, sanitary facilities and changing rooms, which are popularly referred to as \"Topolini\". In the area of the Excelsior bathing establishment, which is located on a historic sand bank, there were elegant Roman villas and their sports and bathing facilities in antiquity. Already in the 19th century there were numerous restaurants and cafes with shady vine arbors. The sea around Miramare Castle is today a nature reserve.\nThe pine forest of Barcola is located directly on the sea and is a meeting place for the inhabitants in every season. One of the best running routes in Trieste leads from Barcola to Miramare Castle and back. The small bathing complex Bagno da Sticco is right next to Miramare Castle. Further towards Grignano and Duino there are numerous bays and natural beaches. Due to the currents in the Adriatic, the water in the area of Trieste is very pure and not polluted by suspended matter from rivers. The current is counterclockwise.\nCulture.\nThe literary-intellectual centre of Trieste is mostly located in the downtown area: \"Libreria Antiquaria Umberto Saba\" located at the ground floor of Via San Nicol\u00f2 No. 30, where James Joyce lived (where his son Giorgio was born and where Joyce wrote some of the short stories from Dubliners and Stephen Hero); the house in Via San Nicol\u00f2 No. 31, where Umberto Saba spent his breaks at the cafe-milk shop \"Walter\" and the house in Via San Nicol\u00f2 No. 32, in which the Berlitz School was located and where James Joyce came into contact with and subsequently taught Italo Svevo, are all of literary relevance. Around this area, at the end of Via San Nicol\u00f2, a life-size statue of Umberto Saba has been placed by the city government. Having Via San Nicol\u00f2 become Trieste's high street, numerous cafes and restaurants that used to be located there, most notably the Berger beer hall at No. 17, which later became the Berger Grand Restaurant, have now ceased operations. Via San Nicol\u00f2 No. 30 is also the symbolic centre of the homonymous novel by Roberto Curci. One of the most important Art Nouveau buildings in Trieste, the \"Casa Smolars\", completed in 1905, stands in Via San Nicol\u00f2 No. 36. Eppinger Caff\u00e8 has been located nearby since around 1946. The former \"Palazzo della RAS\", located in Piazza della Repubblica, has been completely renovated and is now an hotel.\nCaffe Stella Polare is located in Piazza Ponterosso. This cosmopolitan coffee house was also frequented by Saba, Joyce, Guido Voghera, Virgilio Giotti and in particular by the former German-speaking minority from Trieste. With the end of World War II and the arrival of the Anglo-Americans in the city, this caf\u00e9 became a hangout place of many soldiers and a famous ballroom to meet local young women. Trieste hosts the annual ITS (International Talent Support Awards) young fashion designer competition. The power metal band Rhapsody was founded in Trieste by the city's natives Luca Turilli and Alex Staropoli.\nHarry's Piccolo is the only Michelin-starred restaurant in Trieste. Next to Piazza Unita d\u2019Italia the streets of the Cavana district are lined with design-led cafes and delis.\nTheaters and museums.\nTrieste has a lively cultural scene with various theaters. Among these figure Teatro Lirico Giuseppe Verdi, Politeama Rossetti, the Teatro La Contrada, the Slovene theatre in Trieste (, since 1902), Teatro Miela, and several smaller ones.\nThere are also a number of museums. Among these are:\nNational monuments.\nTwo important national monuments:\nThe Slovenska gospodarsko-kulturna zveza\u2014Unione Economica-Culturale Slovena is the umbrella organisation bringing together cultural and economic associations belonging to the Slovene minority.\nChurches.\nThe Greek Orthodox Church of San Nicol\u00f2 dei Greci, which is dedicated to Saint Nicholas, the patron saint of seafarers and whose interior inspired James Joyce, is located by the sea in Piazza Tommaseo, next to the historic Caff\u00e8 Tommaseo. This coffee house, also located at the beginning of Via San Nicol\u00f2, was opened in 1830. It is the oldest coffee house still in operation in Trieste and is still a meeting place for artists and intellectuals today.\nEconomy.\nDuring the Austro-Hungarian era, Trieste became a leading European city in economy, trade and commerce, and was the fourth-largest and most important centre in the empire, after Vienna, Budapest and Prague. The economy of Trieste, however, fell into decline after the city's annexation to Italy in 1922. The Fascist government promoted several development schemes in the 1930s, with new manufacturing activities dedicated to shipbuilding and defence production (such as the \"Cantieri Aeronautici Navali Triestini (CANT)\"). Allied bombings during World War II destroyed the industrial section of the city (mainly the shipyards). However, starting from the 1970s, Trieste has experienced steady economic growth.\nSince the fall of the Iron Curtain, the accession of Slovenia, Croatia, Hungary, the Czech Republic and Slovakia to the EU and the increasing importance of the maritime Silk Road to Asia and Africa across the Suez Canal, trade has seen an increase in Trieste. The Port of Trieste is a major trade hub in the northern Mediterranean, with significant commercial shipping activity and busy container and oil terminals. The port has been included in the Silk Road scheme because of its ability to dock container ships with very large drafts. Because of this natural advantage, the Port of Hamburg (HHLA) and the State of Hungary have holdings in the port area of Trieste and the associated facilities have been expanded by the Italian state in 2021 with an investment of \u20ac400 million. The port is now being promoted as a key terminal for the India-Middle East-Europe economic Corridor (IMEC) and the Indo-Mediterranean. It is considered strategic for Europe due to its position which can connect various trade routes as well as reinforce trade with the Ukrainian port of Odesa.\nThe oil terminal is a key infrastructure in the Transalpine Pipeline, which covers 40% of Germany's energy requirements (100% of the states of Bavaria and Baden-W\u00fcrttemberg), 90% of Austria and 50% of the Czech Republic's. The sea highway connecting the ports of Trieste and Istanbul is one of the busiest RO/RO [roll on roll-off] routes in the Mediterranean. The port is also Italy's and the Mediterranean's greatest coffee port, supplying more than 40% of Italy's coffee. The city is part of the Corridor 5 project to establish closer transport connections between Western and Eastern Europe, through countries such as Slovenia, Croatia, Hungary, Ukraine and Bosnia. \nThe thriving coffee industry in Trieste began under Austria-Hungary, with the Austro-Hungarian government even awarding tax-free status to the city in order to encourage more commerce. Some evidence of Austria-Hungary's coffee-driven economic growth stimulus remain, such as the Hausbrandt Trieste coffee company. As a result, present-day Trieste is characterised by its many cafes, and is still known to this day as \"the coffee capital of Italy\". Companies active in the coffee sector have given birth to the Trieste Coffee Cluster as their main umbrella organisation, but also as an economic actor in its own right. A large part of Italian coffee imports (approx. 2\u20132.5 million sacks) are handled and processed in the city.\nTwo Fortune Global 500 companies have their global or national headquarters in the city, respectively: Assicurazioni Generali and Allianz. Other corporations based in Trieste are Fincantieri, one of the world's leading shipbuilding companies, and the Italian operations of W\u00e4rtsil\u00e4. Prominent companies from Trieste include: AcegasApsAmga (Hera Group), Adriatic Assicurazioni SpA Autamarocchi SpA, Banca Generali SpA (BIT: BGN), Genertel, Genertellife, HERA Trading, the coffee company Illy, the shipping line Italia Marittima, Modiano, Nuovo Arsenale Cartubi Srl, Jindal Steel and Power Italia SpA; Pacorini SpA, Siderurgica Triestina (Arvedi Group), TBS Groug, U-blox, Telit, and polling and marketing company SWG.\nThe real estate market in Trieste has been growing in recent years. The relevant land register law comes from old Austrian legislation and was adopted by the Italian legal system after 1918 in Trieste, as well as in the provinces of Trento, Bolzano and Gorizia as well as in some municipalities of the provinces of Udine, Brescia, Belluno and Vicenza.\nCommercial fishing.\nFishing boats anchor at Molo Veneziano near Piazza Venezia. In summer (large lamps) are used for fishing and in autumn and winter (smaller fishing nets) are used. In the Gulf of Trieste, because of the crystal-clear, nutrient-poor water with little plankton, fishing in itself is challenging. The fishing season lasts from May to July. In terms of fish reproduction, fishing is prohibited in August and restricted in winter. As of 2009, there are fewer than 200 professional fishermen in the city. There is also a small fishing port in the suburb Barcola. Some of the fish is sold directly from the boats or delivered to the town's shops and restaurants. The rare alici ([anchovies - in the local dialect: ) from the Gulf of Trieste near Barcola, which are only caught at Sirocco, are particularly sought after because of their white meat and special taste and fetch high prices for fishermen.\nEducation and research.\nThe University of Trieste, founded in 1924, is a medium-size state-supported institution with 12 faculties. It currently has about 23,000 students enrolled and 1,000 professors.\nTrieste also hosts the Scuola Internazionale Superiore di Studi Avanzati (SISSA), a leading graduate and postgraduate teaching and research institution in the study of mathematics, theoretical physics, and neuroscience, and the MIB School of Management Trieste.\nThere are three international schools offering primary and secondary education programmes in English in the greater metropolitan area: the International School of Trieste, the European School of Trieste, and the United World College of the Adriatic located in the nearby village of Duino.\nThe city also hosts numerous national and international scientific research organizations: \nTrieste is also a hub for corporate training and skills development, hosting, among others, Generali's Generali Academy and Illy's Universit\u00e0 del Caff\u00e9. This competence centre was created in 1999 to spread the culture of quality coffee through training all over the world and to carry out research and innovation.\nAs a result of the combination of research, business and funding, there are a growing number of spin-off companies in Trieste (partnerships in the production world exist with companies such as Cimolai, Danieli, Eni, Fincantieri, Generali, Illy, Mitsubishi, Vodafone) and proportionally the highest number of start-ups in Italy, the city also being referred to as Italy's Silicon Valley. Neurala, a company specialising in artificial intelligence, has chosen Trieste as its European research centre. Trieste has the highest proportion of researchers in Europe in relation to the population. They also appreciate the high quality of life and leisure time, so, as is often said, you can ski and swim by the sea in one day from Trieste.\nSports.\nThe local football club, Triestina, is one of the older clubs in Italy. Notably, it was runner-up in the 1947\u20131948 season of the Italian first division (Serie A), losing the championship to Torino.\nTrieste is notable for having had two football clubs participating in the championships of two different nations at the same time during the period of the Free Territory of Trieste, due to the schism within the city and region created by the post-war demarcation. Triestina played in the Italian first division (Serie A). Although it faced relegation after the first season after the Second World War, the FIGC modified the rules, as it was deemed important to keep the club in the league. The following year the club played its best season with a 3rd-place finish. Meanwhile, Yugoslavia bought A.S.D. Ponziana, a small team in Trieste, which under the new name Amatori Ponziana Trst, played in the Yugoslavian league for three years. Triestina went bankrupt in the 1990s, but after being re-founded, it regained a position in the Italian second division (Serie B) in 2002. Ponziana was renamed \"Circolo Sportivo Ponziana 1912\" and currently plays in Friuli-Venezia Giulia Group of Promozione, which is the 7th level of the Italian league.\nTrieste also has a well-known basketball team, Pallacanestro Trieste, which reached its zenith in the 1990s under coach Bogdan Tanjevi\u0107 when, with large financial backing from sponsors Stefanel, it was able to sign players such as Dejan Bodiroga, Fernando Gentile and Gregor Fu\u010dka, stars of European basketball. At the end of the 2017\u201318 season, the team, now trained by coach Eugenio Dalmasson and sponsored by Alma, won promotion to the Lega Basket Serie A, Italy's highest basketball league, 14 years after its last tenure.\nMany sailing clubs have roots in the city which contribute to Trieste's strong tradition in that sport. The Barcolana regatta, first held in 1969, is the world's largest sailing race by number of participants.\nLocal sporting facilities include the Stadio Nereo Rocco, a UEFA-certified stadium with seating capacity of 32,500; the Palatrieste, an indoor sporting arena sitting 7,000 people, and Piscina Bruno Bianchi, a large Olympic size swimming pool.\nOn 26 August 1985 American basketball player Michael Jordan dunked so hard that the backboard shattered during a Nike exhibition game played in Trieste. The signed jersey and shoes (including one of the tiny shards of glass in the sole of the left shoe) that the player wore during the famous shattered backboard game were later auctioned. The moment the glass broke was filmed and is often cited as a particularly important milestone in Jordan's rise.\nThe historically most successful handball club in Italy, Pallamano Trieste is based in the city.\nFilm.\nTrieste has been portrayed on screen a number of times, with films often shot on location. In 1942 the early neorealist \"Alfa Tau!\" was filmed partly in the city.\nCinematic interest in Trieste peaked during the height of the \"Free Territory\" era from 1947 to 1954, with international films such as \"Sleeping Car to Trieste\" and \"Diplomatic Courier\" portraying it as a hotbed of espionage. These films, along with \"The Yellow Rolls-Royce\" (1964), conveyed an image of the city as a cosmopolitan place of conflict between Great Powers, a portrayal which resembles \"Casablanca\" (1943). Italian filmmakers, by contrast, portrayed Trieste as unquestionably Italian in a series of patriotic films, including \"Trieste mia!\" and \"Ombre su Trieste\".\nIn 1963 the city hosted the first International Festival of Science Fiction Film (Festival internazionale del film di fantascienza), which ran until 1982. Under the name Science Plus Fiction (now Trieste Science+Fiction Festival), the festival was revived in 2000.\nAn interest in the city has been sparked by movies such as \"The Invisible Boy\" (2014), its sequel \"The Invisible Boy\u2014Second Generation\", and the TV series \"La Porta Rossa\".\nTriestine cuisine.\nLong established authentic restaurants include Buffet da Pepi, Vecio Buffet Marascutti, Buffet Siora Rosa, and Antica Trattoria Suban. \"Buffet\" means that restaurant staff assemble a platter of cold cuts and antipasto for the customer who chooses either a lunch meal or a snack. Local cuisine has been influenced by the various ethnic groups which have populated the city, mainly Central Europeans. Traditional main courses include jota, minestra de bisi spacai (pea stew), rotolo di spinaci in straza (spinach rolls), sardoni impanai (breaded anchovies, a sought-after delicacy), capuzi garbi (krauts), capuzi garbi in tecia (saut\u00e9ed krauts), vienna sausages, goulash, \u0107evapi and frito misto (fried fish). Popular desserts are presnitz, fave triestine, titola, crostoli, struccolo de pomi, kugelhupf, rigo jancsi and the Triester torte.\nCapo Triestino (also capo in B or capo in bicchiere) is considered a local coffee speciality. This miniature cappuccino in a glass cup is usually consumed at the bar. The Prosecco wine is served in hundreds of variations. The eponymous village of Prosecco is located within the limits of the Trieste municipality.\nTransport.\nMaritime.\nTrieste's maritime location and its former long-term status as part of the Austrian Empire\u2014later the Austro-Hungarian Empire\u2014made the Port of Trieste the major commercial port for much of the landlocked areas of central Europe. In the 19th century, a new port district known as the Porto Nuovo was built northeast of the city centre.\nSignificant volumes of goods pass through the container, steel works and oil terminals, all located to the south of the city centre. After many years of stagnation, a change in the leadership placed the port on a steady growth path, recording a 40% increase in shipping traffic as of 2007[ [update]].\nToday the port of Trieste is one of the largest Italian ports and next to Gioia Tauro the only deep water port in the central Mediterranean for seventh generation container ships.\nRail.\nRailways came early to Trieste, due to the importance of its port and the need to transport people and goods inland. The first railroad line to reach Trieste was the S\u00fcdbahn, built by the Austrian government in 1857. This railway stretches for to Lviv, Ukraine, via Ljubljana, Slovenia; Sopron, Hungary; Vienna, Austria; and Krak\u00f3w, Poland, crossing the backbone of the Alps mountains through the Semmering Pass near Graz. It approaches Trieste through the village of Villa Opicina, a few kilometres from the centre but over higher in elevation. Due to this, the line takes a detour to the north, gradually descending before terminating at the Trieste Centrale railway station.\nIn 1887, the Imperial Royal Austrian State Railways (German: ) opened a new railway line, the (German: ), from the new port of Trieste to Hrpelje-Kozina, on the . The intended function of the new line was to reduce the Austrian Empire's dependence on the S\u00fcdbahn network. Its opening gave Trieste a second station south of the original one, which was named Trieste Sant'Andrea (German: ). The two stations were connected by a railway line that in the initial plans was meant to be an interim solution: the (German: ), which survived until 1981, when it was replaced by the , a railway tunnel route to the east of the city.\nWith the opening of the Transalpina Railway from Vienna, Austria via Jesenice and Nova Gorica in 1906, the St. Andrea station was replaced by a new, more capacious, facility, named Trieste stazione dello Stato (German: ), later , now a railway museum, and the original station came to be identified as Trieste stazione della Meridionale or Trieste Meridionale (German: ). This railway also approached Trieste via Villa Opicina, but it took a rather shorter loop southwards towards the sea front. Freight lines from the dock area include container services to northern Italy and to Budapest, Hungary, together with rolling highway services to Salzburg, Austria and Frankfurt, Germany.\nThere are direct intercity and high-speed trains between Trieste and Venice, Verona, Turin, Milan, Rome, Florence, Naples and Bologna. Passenger trains also run between Villa Opicina and Ljubljana.\nOn special occasion, the historic ETR 252 \"Arlecchino\" runs the Venezia Santa Lucia-Trieste Centrale route, operated by Fondazionefs. This is one of four examples ever built.\nAir.\nTrieste is served by the Trieste Airport (IATA: TRS). The airport serves domestic and international destinations and is fully connected to the national railway and highway networks. The Trieste Airport railway station links the passenger terminal directly to the Venice\u2013Trieste railway thanks to a 425-metre long skybridge. A 16 platform bus terminal, a multi-storey car park with 500 lots and a car park with 1,000 lots give public and private motor vehicles rapid access to the A4 Trieste-Turin highway. At the interchange near Palmanova, the A4 branches off to Autostrada A23 linking to Austria's S\u00fcd Autobahn (A2) via Udine and Tarvisio. In the southern direction, this highway also offers seamless interconnection to Slovenia's A1 Motorway, and through that to highway networks in Croatia, Hungary, and the Balkans.\nLocal transport.\nLocal public transport is operated by Trieste Trasporti, a part of TPL FVG, which operates a network of around 60 bus routes and two ferry lines. Its also operates the Opicina Tramway, a hybrid between a tramway and funicular railway, providing a more direct link between the city centre and Opicina.\nInternational relations.\nTrieste hosts the Secretariat of the Central European Initiative, an inter-governmental organisation among Central and South-Eastern European states.\nIn recent years, Trieste has been chosen as host to a number of high level bilateral and multilateral meetings such as: the Western Balkans Summit in 2017; the Italo-Russian Bilateral Summit in 2013 (Letta-Putin) and the Italo-German Bilateral Summit in 2008 (Berlusconi-Merkel); the G7 meeting of education ministers in 2024; the G8 meetings of Foreign Affairs and Environment Ministers, respectively in 2009 and 2001. In December 2020, Trieste hosted three-party talks between the foreign ministers of Italy, Croatia, and Slovenia on the delimitation of their respective exclusive economic zone. In 2020, Trieste was nominated the European Science Capital by EuroScience. In August 2021, it hosted the G20 Meeting of Ministers of Innovation and Research.\nSister cities and twin towns.\nTrieste is twinned with:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56093", "revid": "50561691", "url": "https://en.wikipedia.org/wiki?curid=56093", "title": "Kerchief", "text": "Cloth tied around the head or neck; bandana\nA kerchief (from the Old French \"couvre-chef\", \"cover head\"), also known as a bandana or bandanna, is a triangular or square piece of cloth tied around the head, face, or neck for protective or decorative purposes.\nBandanas originated in India as bright-coloured handkerchiefs of silk and cotton with spots in white on coloured grounds, chiefly red and blue Bandhani. The silk styles were made of the finest-quality yarns and were popular The popularity of \"head kerchiefs\" may vary by culture or religion, often being used as a Christian headcovering by men and women of the Anabaptist, Eastern Orthodox, and Plymouth Brethren denominations, as well as by some Orthodox Jewish and Muslim men and women and is also considered a hat.\nThe \"neckerchief\" and \"handkerchief\" are related items.\nTypes.\nBandana.\nA bandana or bandanna (from Hindi and Urdu, ultimately from Sanskrit \u092c\u0928\u094d\u0927\u0928 or bandhana, \"a bond\") is a type of large, usually colourful kerchief, originating from the Indian subcontinent, often worn on the head or around the neck of a person. Bandanas are frequently printed in a paisley pattern and are most often used to hold hair back, either as a fashionable head accessory or for practical purposes. It is also used to tie around the neck to prevent sunburn, and around the mouth and nose to protect from dust inhalation or to hide the identity of its wearer.\nThe word bandana stems from the Hindi words 'b\u0101ndhn\u016b', or \"tie-dyeing\", and 'b\u0101ndhn\u0101', \"to tie\". These stem from Sanskrit roots 'badhn\u0101ti', \"he ties\", and Sanskrit 'bandhana' (\u092c\u0928\u094d\u0927\u0928), \"a bond\". In the 18th and 19th centuries bandanas were frequently known as bandannoes.\nBandanas originated in India as bright-coloured handkerchiefs of silk and cotton with spots in white on coloured grounds, chiefly red and blue Bandhani. The silk styles were made of the finest-quality yarns and were popular. Bandana prints for clothing were first produced in Glasgow from cotton yarns, and are now made in many qualities. The term, at present, generally means a fabric in printed styles, whether silk, silk and cotton, or all cotton.\nThe bandana found popularity in the US during the late 1700s because snuff users preferred coloured and patterned silk handkerchiefs over white ones, as the former hid tobacco stains better when the users blew their noses. In the late 18th and early 19th centuries, bandanas began to appear with political and military advertisements printed on them. Such printed bandanas were common in the early and mid-1900s during World War I and World War II. Decorative bandanas were also common gear, particularly as neckwear, for cowboys, and so for country and western entertainers such as Roy Rogers and, later, Willie Nelson. The latter singer began wearing bandanas when he moved from Nashville back to Austin, Texas, \"just in time to catch the hippie wave cresting at counterculture center the Armadillo World Headquarters\".\nAround the same time, bandanas also became popular with motorcyclists, particularly with Harley-Davidson riders and bikers. In the 1970s paisley bandanas also became popular amongst gangs in California, most notably with two well-known rival gangs, the Bloods, who wore red bandanas, and the Crips, who wore blue ones.\nGreen bandanas have become a symbol of the abortion-rights movement.\nOramal.\nThe Oramal is a traditional kerchief used in Central Asia and the Caucasus (note how it is tied, the neck is usually not covered by it). In some countries like Uzbekistan, it was traditionally used only at home, while in public the paranja was more popular. In other countries, like Kazakhstan, it was commonly used in public. In Kyrgyzstan, the white color is an indication that the woman is married.\nAs well it was widely used by men at horse riding in summertime instead of wearing a cap (cf. bandana of bikers).\nAustronesian headscarves.\nKerchiefs are also worn as headdresses by Austronesian cultures in maritime Southeast Asia. Among Malay men it is known as tengkolok and is worn during traditional occasions, such as weddings (worn by the groom) and the pesilat.\nSee also.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56095", "revid": "7770027", "url": "https://en.wikipedia.org/wiki?curid=56095", "title": "Cain", "text": "Biblical figure\nCain is a biblical figure in the Book of Genesis within Abrahamic religions. He is the elder brother of Abel, and the firstborn son of Adam and Eve, the first couple within the Bible. He was a farmer who gave an offering of his crops to God. However, God was not pleased and favored Abel's offering over Cain's. Out of jealousy, Cain killed his brother, for which he was punished by God with the curse and mark of Cain. He had several descendants, starting with his son Enoch and including Lamech.\nThe narrative is notably unclear on God's reason for rejecting Cain's sacrifice. Some traditional interpretations consider Cain to be the originator of evil, violence, or greed. \nGenesis narrative.\nAccording to the narrative in Genesis 4, Cain was the first human born (as distinct from God's creation of Adam and Eve), and the first murderer. When he and his brother made their offerings to God, the LORD \"had regard for Abel and his offering, but for Cain and his offering he had no regard. So Cain was very angry\". \nInterpretations.\nJewish and Christian interpretations.\nA question arising early in the story is why God rejected Cain's sacrifice. The text states, \"In the course of time Cain brought some of the fruits of the soil as an offering to the Lord. And Abel also brought an offering: fat portions from some of the firstborn of his flock. The Lord looked with favor on Abel and his offering, but on Cain and his offering he did not look with favor.\" Noteworthy is the difference in the type of sacrifice: fruits of the soil are renewable and bloodless, while fat portions are set apart for the Lord and taken from the firstborn, pointing to an act of faith, since it is not guaranteed there will be more. The Midrash suggests that although Abel brought the best meat from his flock, Cain did not set aside the best of his harvest for God.\nSimilar to the internalized spiritual death, God warns Adam and Eve off from eating the forbidden fruit\u2014they do not physically die immediately, but over time, their bodies age and die\u2014the Lord warns Cain that his inappropriate anger is waiting to consume him: \"If you do what is right, will you not be accepted? But if you do not do what is right, sin is crouching at your door. It desires to have you, but you must rule over it.\nCurse and mark.\nAccording to , Cain treacherously murdered his brother, Abel, and then lied about the murder to God. As a result, Cain was cursed and marked for life. With the earth left cursed to drink Abel's blood, Cain could no longer farm the land. He becomes a \"fugitive and wanderer\" and receives a mark from God - commonly referred to as the \"mark of Cain -\" so that no one can enact vengeance on him.\nExegesis of the Septuagint's narrative, \"groaning and shaking upon the earth\", has Cain suffering from body tremors. Interpretations extend Cain's curse to his descendants, where they all died in the Great Deluge as retribution for the loss of Abel's potential offspring.\nIn another biblical account, Ham discovered his father Noah drunk and naked in his tent. Because of this, Noah cursed Ham's son Canaan to be \"servants of servants\". Although the scriptures do not mention Ham's skin color, some doctrines associated the curse with black people and used it to justify slavery.\nIslamic interpretation.\nCain's name in Islamic tradition is Qabil (). His story is mentioned in the Quran, though without a name, where he and his brother Abel offer sacrifices; Abel's sacrifice was accepted while Cain's was not. Cain gets angry and threatens to murder his brother, but Abel tries to console him, saying that God only accepts sacrifices from the God-fearing and that he would not try to harm Cain. In the end, Cain kills Abel. God sends a crow searching in the ground to show Cain how to hide the disgrace of his brother. In his shame, Cain began to curse himself and became full of guilt.\nLatter-day Saint interpretation.\nThe Pearl of Great Price, a Mormon book of scripture, has been interpreted to depict the descendants of Cain as dark-skinned, and church president Brigham Young stated, \"What is the mark? You will see it on the countenance of every African you ever did see...\". However, this position was disavowed by modern leaders of The Church of Jesus Christ of Latter-day Saints.\nEtymology.\nOne popular theory regarding the name of Cain connects it to the verb \"kana\" (&lt;templatestyles src=\"Script/styles_hebrew.css\" /&gt;\u05e7\u05e0\u05d4\u200e \"qnh\"), meaning \"to get\" and used by Eve in when she says after bearing Cain, \"I have \"gotten\" a man from the Lord.\" In this viewpoint, articulated by Nachmanides in the thirteenth century, Cain's name presages his role of mastery, power, and sin. In one of the \"Legends of the Jews\", Cain is the fruit of a union between Eve and Satan, who is also the angel Samael and the serpent in the Garden of Eden, and Eve exclaims at Cain's birth, \"I have gotten a man through an angel of the Lord.\" According to the \"Life of Adam and Eve\" (c.), Cain fetched his mother a reed (\"qaneh\") which is how he received his name \"Qayin\" (Cain). The symbolism of him fetching a reed may be a nod to his occupation as a farmer, as well as a commentary to his destructive nature. He is also described as \"lustrous\", which may reflect the Gnostic association of Cain with the sun.\nCharacteristics.\nCain is described as a city-builder, and the forefather of tent-dwelling pastoralists, all lyre and pipe players, and bronze and iron smiths.\nIn an alternate translation of Genesis 4:17, endorsed by a minority of modern commentators, Cain's son Enoch builds a city and names it after \"his\" son, Irad. Such a city could correspond with Eridu, one of the most ancient cities known. Philo observes that it makes no sense for Cain, the third human on Earth, to have founded an actual city. Instead, he argues, the city symbolizes an unrighteous philosophy.\nIn the New Testament, Cain is cited as an example of unrighteousness in and . The Targumim, rabbinic sources, and later speculations supplemented background details for the daughters of Adam and Eve. Such exegesis of Genesis 4 introduced Cain's wife as being his sister, a concept that has been accepted for at least 1,800 years. This can be seen with Jubilees 4 which narrates that Cain settled down and married his sister Awan, who bore their first son, the first Enoch, approximately 196 years after the creation of Adam. Cain then establishes the first city, naming it after his son, builds a house, and lives there until it collapses on him, killing him on the same year of Adam's death.\nRelationship with the ground.\nIn this alternative reading of the text, the ground could be personified as a character. This reading is evidenced by given human qualities, like a mouth, in the scripture. The ground is also the only subject of an active verb in the verse that states, \"It opens its mouth to take the blood.\" This suggests that the ground reacted to the situation. By that logic, the ground could then potentially be an accomplice to the murder of Abel (Jordstad 708). The reaction from the ground raises the question, \"Does the intimate connection between humans and the ground mean that the ground mirrors or aids human action, regardless of the nature of that action?\"\nOther stories.\nIn Jewish tradition, Philo, \"Pirke De-Rabbi Eliezer\", and the \"Targum Pseudo-Jonathan\" asserted that Adam was not the father of Cain. Instead, Eve was subject to adultery, having been seduced by Sammael, the serpent (\"nahash\", ) in the Garden of Eden, the devil himself. Christian exegesis of the \"evil one\" in has also led some commentators, such as Tertullian, to agree that Cain was the son of the devil or a fallen angel. Thus, according to some interpreters, Cain was half-human and half-angel, one of the Nephilim (see Genesis 6). Gnostic exposition in the \"Apocryphon of John\" has Eve seduced by Yaldabaoth. However, in the \"Hypostasis of the Archons\", Eve is raped by a pair of Archons.\nPseudo-Philo, a Jewish work of the first century CE, relates that Cain murdered his brother at the age of 15. After escaping to the Land of Nod, Cain had four sons: Enoch, Olad, Lizpha, and Fosal, as well as two daughters, Citha and Maac. The latter five are not mentioned in the Bible. Cain died at the age of 730, leaving his corrupt descendants to spread evil on Earth. According to the Book of Jubilees, Cain murdered his brother with a stone. Afterward, Cain was killed by the same instrument he used against his brother: his house fell in upon him, and its stones killed him. A heavenly law was cited after the narrative of Cain's death saying:\nWith the instrument with which a man kills his neighbour with the same shall he be killed; after the manner that he wounded him, in like manner shall they deal with him.\nA Talmudic tradition says that after Cain had murdered his brother, God made a horn grow on his head. Later, Cain was killed at the hands of his great-grandson, Lamech, who mistook him for a wild beast. A Christian version of this tradition from the time of the Crusades holds that the slaying of Cain by Lamech took place on a mound called \"Cain Mons\" (i.e., Mount Cain), which is a corruption of \"Caymont\", a Crusader fort in Tel Yokneam in modern-day Israel.\nThe story of Cain and Abel is also referred to in chapter 19 of 1 Meqabyan, a book considered canonical in the Ethiopian Orthodox Tewahedo Church. In this text, Cain killed Abel because he desired Abel's wife.\nAccording to the Mandaean scriptures, including the Qulasta, the Mandaean Book of John, and the Ginza Rabba, \"Abel\" is cognate with the angelic soteriological figure Hibil Ziwa who taught John the Baptist.\nIn the book \"Aradia, or the Gospel of the Witches\" by Charles Godfrey Leland, Cain is a lunar figure.\nFamily.\nFamily tree.\nThe following family tree of the line of Cain is compiled from a variety of biblical and extra-biblical texts.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSisters/wives.\nVarious early commentators have said that Cain and Abel have sisters, usually twin sisters. According to Rabbi Joshua ben Karha as quoted in Genesis Rabbah, \"Only two entered the bed, and seven left it: Cain and his twin sister, Abel and his two twin sisters.\"\nMotives.\nThe Book of Genesis does not give a specific reason for the murder of Abel. Modern commentators typically assume that the motives were jealousy and anger due to God rejecting Cain's offering, while accepting Abel's. The First Epistle of John says the following:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Do not be like Cain, who belonged to the evil one and murdered his brother. And why did he murder him? Because his own actions were evil and his brother's were righteous.\"\nAncient exegetes, such as the Midrash and the \"Conflict of Adam and Eve with Satan\", tell that the motive involved a desire for the most beautiful woman. According to Midrashic tradition, Cain and Abel each had twin sisters; each was to marry the other's. The Midrash states that Abel's promised wife, Aclima, was more beautiful than Awan, Cain's promised wife. And so, after Cain would not consent to this arrangement, Adam suggested seeking God's blessing by means of a sacrifice. Whoever God blessed would marry Aclima. When God openly rejected Cain's sacrifice, Cain slew his brother in a fit of jealousy and anger. Rabbinical exegetes have discussed whether Cain's incestuous relationship with his sister was in violation of \"halakha\".\nLegacy and symbolism.\nA millennia-old explanation for Cain being capable of murder is that he may have been the offspring of a fallen angel or Satan himself, rather than being the son of Adam.\nA medieval legend has Cain arriving at the Moon, where he eternally settled with a bundle of twigs. This was originated by the popular fantasy of interpreting the shadows on the Moon as a face. An example of this belief can be found in Dante Alighieri's \"Inferno\" (XX, 126) where the expression \"Cain and the twigs\" is used as a kenning for \"moon\".\nIn Latter-day Saint theology, Cain is considered to be the quintessential Son of Perdition, the father of \"secret combinations\" (i.e. secret societies and organized crime), as well as the first to hold the title Master Mahan meaning \"master of [the] great secret, that [he] may murder and get gain\".\nIn Mormon folklore a second-hand account relates that an early Mormon leader, David W. Patten, encountered a very tall, hairy, dark-skinned man in Tennessee who said that he was Cain. The account states that Cain had earnestly sought death but was denied it, and that his mission was to destroy the souls of men. The recollection of Patten's story is quoted in Spencer W. Kimball's \"The Miracle of Forgiveness\", a popular book within the Church of Jesus Christ of Latter-day Saints. This widespread Mormon belief is further emphasized by an account from Salt Lake City in 1963 which stated that \"One superstition is based on the old Mormon belief that Cain is a black man who wanders the earth begging people to kill him and take his curse upon themselves (M, 24, SLC, 1963).\"\nFreud's theory of fratricide is explained by the Oedipus or Electra complex through Carl Jung's supplementation.\nThere were other, minor traditions concerning Cain and Abel, of both older and newer date. The apocryphal \"Life of Adam and Eve\" tells of Eve having a dream in which Cain drank his brother's blood. In an attempt to prevent the prophecy from happening, the two young men are separated and given different jobs.\nThe author Daniel Quinn, first in his book \"Ishmael\" and later in \"The Story of B\", proposes that the story of Cain and Abel is an account of early Semitic herdsmen observing the beginnings of what he calls totalitarian agriculture, with Cain representing the first 'modern' agriculturists and Abel the pastoralists.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56096", "revid": "49152960", "url": "https://en.wikipedia.org/wiki?curid=56096", "title": "Pioneer movement", "text": "Organization for children operated by a communist party\nA pioneer movement is an organization for children operated by a communist party. Typically children enter into the organization in elementary school and continue until adolescence. The adolescents then typically join the Young Communist League. Prior to the 1990s, there was a wide cooperation between pioneer and similar movements of about 30 countries, coordinated by the international organization, \"International Committee of Children's and Adolescents' Movements\" (, CIMEA), founded in 1958, with headquarters in Budapest, Hungary.\nOverview.\nDuring the Russian Civil War from 1917 to 1921, most of the Russian Scoutmasters and many Scouts fought in the ranks of the White Army against the Red Army. Between 1918 and 1920, the All-Russian Congresses of the Russian Union of the Communist Youth (Komsomol) decided to eradicate the Scout movement and create an organization of the communist type, that would take Soviet children and adolescents under its umbrella. This organization would resemble the Scout movement in its form but properly educate children with Communist teachings, to evince previous capitalist ideologies of private ownership and individualism.\nOn behalf of the Soviet Council of People's Commissars, Nadezhda Krupskaya (Vladimir Lenin's wife and the People's Commissar of State for Education) was one of the main contributors to the cause of the Pioneer movement. In 1922, she wrote an essay called \"Russian Union of the Communist Youth and boy-Scoutism.\" However, it was the remaining scoutmasters themselves who supported the Komsomol and the Red Army, who introduced the name \"Pioneer\" to it and convinced the Komsomol to adapt the Scout symbols and rituals.\nThe first Pioneer organization was founded in Soviet Russia in 1922. Later, similar organizations were founded in the countries of the Eastern Bloc and other Communist states.\nThe Pioneer movement was modelled in many aspects on the Scout movement. The two movements share some principles like preparedness and promotion of sports and outdoor skills. The motto \"Always prepared!\" was adopted by the pioneer movement from the Scout Motto.\nA member of the movement is known as a Pioneer, with the name stemming from the pioneering activity in Scouting. A red neckerchief\u00a0\u2013 is the traditional item of clothing worn by a pioneer. This tradition was adapted from the Scout uniform.\nBut there are some distinct differences between the two movements. Most notably, the Scout movement receives government funding but does not overtly claim to evince the ideologies of the political economy. In contrast, the Pioneer movement is fully subsidized by the government and does make overt connections to the political economy. Thus, it teaches communist principles, including community-oriented and needs-based equal-pay labouring for the larger collective.\nPioneer movements have existed and still exist in countries where the Communist Party is in power as well as in some countries where the Communist Party is in opposition, if the party is large enough to support a children's organization. In countries ruled by Communist Parties, membership of the pioneer movement is officially optional. However, membership provides many benefits, so the vast majority of children typically join the movement (although at different ages). During the existence of the Soviet Union, thousands of Young Pioneer camps and Young Pioneer Palaces were built exclusively for Young Pioneers, which were free of charge, sponsored by the government and trade unions. There were many newspapers and magazines published for Young Pioneers in millions of copies.\nA national pioneer organization is often named after a famous party member that is considered a suitable role model for young communists, such as Vladimir Lenin in the Soviet Union, Enver Hoxha in Albania, Georgi Dimitrov in Bulgaria, Jos\u00e9 Mart\u00ed in Cuba, Ernst Th\u00e4lmann in East Germany, Damdin S\u00fckhbaatar in Mongolia, and Ho Chi Minh in Vietnam.\nThe Movement of the First is a state-sponsored effort to initiate a follow-up of the pioneer movement in modern Russia.\nCountries with current Pioneer movements.\nThe Pioneer movement now exists in these countries:\nOlder children could continue in other communist organizations, but that would typically be done only by a limited number of people.\nThe communist parties in Russia and other countries continue to run a pioneer organization, but membership tends to be quite limited.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56097", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=56097", "title": "History of Styria", "text": "Account of events in Styria\nThe history of Styria concerns the region roughly corresponding to the modern Austrian state of Styria and the Slovene region of Styria (\"\u0160tajerska\") from its settlement by Germans and Slavs in the Dark Ages until the present. This mountainous and scenic region, which became a centre for mountaineering in the 19th century, is often called the \"Green March\", because half of the area is covered with forests and one quarter with meadows, grasslands, vineyards and orchards. Styria is also rich in minerals, soft coal and iron, which has been mined at Erzberg since the time of the Romans. The Slovene Hills (, ) is a famous wine-producing district, stretching between Slovenia and Austria. Styria was for long the most densely populated and productive mountain region in Europe.\nStyria's population before World War I was 68% German-speaking, 32% Slovene, bordered on (clockwise) Lower Austria, Hungary, Croatia, Carniola, Carinthia, Salzburg, and Upper Austria. In 1918 after World War I the southern, Slovene-speaking third south of the Mur River was incorporated into Slovenia in the Kingdom of Serbs, Croats and Slovenes. The remaining two-thirds became the Austrian federal state of Styria, while the Slovene-speaking third (Lower Styria) formed the informal Styria region in Slovenia, now divided into the Drava and Savinja Statistical Regions and the major part of Slovenian Carinthia. The capital both of the duchy and the Austrian state has always been Graz, which is now also the residence of the governor and the seat of the administration of the land.\nPolitical history.\nPrehistory to Charlemagne.\nThe Roman history of Styria is as part of Noricum and Pannonia, with the romanized Celtic population of the Taurisci. During the great migrations, various Germanic tribes settled and/or traversed the region using the river valleys and low passes, but about 600 CE the Slavs took possession of the area and settled assimilating the remaining autochthonous romanized population.\nWhen Styria came under the hegemony of Charlemagne as a part of Carantania (Carinthia), erected as a border territory against the Avars and Slavs, there was a large influx of Bavarii and other Christianized Germanic peoples, whom the bishops of Salzburg and the patriarchs of Aquileia kept faithful to Rome. Bishop Vergilius of Salzburg (745-84), was largely instrumental in establishing a church hierarchy in the Duchy and gained for himself the name of \"Apostle of Carantania.\" In 811 Charlemagne made the Drave River the boundary between the dioceses of Salzburg and Aquileia.\nMiddle Ages.\nThe March of Styria was created in the Duchy of Carinthia in the late 10th century as a defence against the Magyars. Long called the Carantanian or Carinthian March it was soon ruled by a margravial dynasty called the Otakars that originated from Steyr in Upper Austria thus giving the land its name: \"Steiermark\". This march was raised to become a duchy by the Emperor Frederick Barbarossa in 1180 after the fall of Henry the Lion of Bavaria.\nWith the death of Ottokar the first line of rulers of Styria became extinct; the region fell successively to the Babenberg family, rulers of Austria, as stipulated in the Georgenberg Pact; after their extinction to the control of Hungary (1254\u201360); to King Ottokar of Bohemia; in 1276 to the Habsburgs, who provided it with Habsburgs for Styrian dukes during the years 1379-1439 and 1564\u20131619.\nAt the time of the Ottoman invasions in the 16th and 17th centuries the land suffered severely and was depopulated. The Turks made incursions into Styria nearly twenty times; churches, monasteries, cities, and villages were destroyed and plundered, while the population was either killed or carried away into slavery.\nModern era.\nThe Semmering Railway, completed in 1854, was a triumph of engineering in its time, the oldest of the great European mountain railways. It was remarkable for its numerous and long tunnels and viaducts spanning mountain valleys, running from Gloggnitz in Lower Austria to M\u00fcrzzuschlag in Styria, and passing through the area's scenery. The railway brought tourists to alpine lake resorts and mineral springs at Rohitsch (today's Roga\u0161ka Slatina) and Bad Gleichenberg, the brine springs of Bad Aussee, and the thermal springs of Tuffer (today's La\u0161ko), Neuhaus am Klausenbach and Tobelbad.\nFollowing World War I, Styria was divided by the Treaty of Saint Germain. Lower Styria with the cities of Celje and Maribor became part of the Kingdom of Serbs, Croats and Slovenes, while the rest remained with Austria as the State of Styria. Other than in Carinthia, no fighting resulted from this, in spite of a German minority in Slovenia (the larger cities of Lower Styria were largely German-speaking).\nLower Styria was reattached to the Reichsgau Steiermark from 1942 to 1945, whence it was annexed by Germany. After World War II, Styria became part of the British occupation zone in Austria. The lower third was returned to Yugoslavia and today, it makes up about the eastern third of Slovenia.\nReligious history.\nThe Protestant Reformation made its way into the country about 1530. Duke Karl (ruling 1564\u201390), whose wife was the Catholic Duchess Maria of Bavaria, introduced the Counter-Reformation into the country; in 1573 he invited the Jesuits into Styria and in 1586 he founded the Catholic University of Graz. In 1598 his son and successor Ferdinand suppressed all Protestant schools and expelled the teachers and preachers: Protestant doctrines were maintained only in a few isolated mountain valleys, as in the valley of the Inn and the valley of the Mur. On a narrow reading of the Peace of Augsburg, 1555, with its principle of \"cuius regio, eius religio\", only the nobility were not forced to return to the Roman Church; each could have Protestant services privately in his own house.\nAfter Ferdinand had become Holy Roman Emperor in 1619 and had defeated his Protestant opponents in the Battle of White Mountain near Prague in 1620, he forbade all Protestant church services whatsoever (1625). In 1628 he commanded the nobility also to return to the Catholic faith. A large number of noble families, consequently, emigrated from the country. But most of them either returned, or their descendants did so, becoming Catholics and recovering their possessions.\nIn the second half of the 17th century renewed action against the Protestants in the isolated mountain valleys resulted in the expulsion of Protestant ministers with the peasants who would not give up Protestantism; about 30,000 chose compulsory emigration to Transylvania over conversion. Only an Edict of Toleration issued by Emperor Joseph II as late as 1781 put an end to religious repression. The Protestants then received the right to found parish communities and to exercise their religion in those enclaves undisturbed.\nIn 1848, all the provinces of the Habsburg monarchy received complete liberty of religion and of conscience, parity of religions, and the right to the public exercise of religion.\nEcclesiastically the province was historically divided into two Catholic prince-bishoprics, Seckau and Lavant. From the time of their foundation both were suffragans of the Archdiocese of Salzburg. The Prince-Bishopric of Seckau was established in 1218; since 1786 the see of the prince-bishop has been Graz. The Prince-Bishopric of Lavant with its bishop's seat at Sankt Andr\u00e4 in the Carinthian Lavant Valley was founded as a bishopric in 1228 and raised to a prince-bishopric in 1446. In 1847 the bishop's seat was transferred from St. Andr\u00e4 to Maribor, and after World War I the see's boundaries were adapted to the new political frontiers. A short-lived third Salzburg suffragan diocese of Leoben comprising 157 parishes in the districts of Leoben and Bruck an der Mur existed on Styrian soil from 1786 but was incorporated into the diocese of Graz-Seckau in 1856 Today the see of the bishop of Graz-Seckau is identical in territory with the Austrian State of Styria.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56098", "revid": "1319107618", "url": "https://en.wikipedia.org/wiki?curid=56098", "title": "Monte Carlo method", "text": "Probabilistic problem-solving algorithm\nMonte Carlo methods, (sometimes called Monte Carlo experiments or Monte Carlo simulations) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. The name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, mathematician Stanis\u0142aw Ulam, was inspired by his uncle's gambling habits.\nMonte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution. They can also be used to model phenomena with significant uncertainty in inputs, such as calculating the risk of a nuclear power plant failure. Monte Carlo methods are often implemented using computer simulations, and they can provide approximate solutions to problems that are otherwise intractable or too complex to analyze mathematically.\nMonte Carlo methods are widely used in various fields of science, engineering, and mathematics, such as physics, chemistry, biology, statistics, artificial intelligence, finance, and cryptography. They have also been applied to social sciences, such as sociology, psychology, and political science. Monte Carlo methods have been recognized as one of the most important and influential ideas of the 20th century, and they have enabled many scientific and technological breakthroughs.\nMonte Carlo methods also have some limitations and challenges, such as the trade-off between accuracy and computational cost, the curse of dimensionality, the reliability of random number generators, and the verification and validation of the results.\nOverview.\nMonte Carlo methods vary, but tend to follow a particular pattern:\nFor example, consider a quadrant (circular sector) inscribed in a unit square. Given that the ratio of their areas is , the value of \u03c0 can be approximated using the Monte Carlo method:\nIn this procedure, the domain of inputs is the square that circumscribes the quadrant. One can generate random inputs by scattering grains over the square, then performing a computation on each input to test whether it falls within the quadrant. Aggregating the results yields our final result, the approximation of \u03c0.\nThere are two important considerations:\nUses of Monte Carlo methods require large amounts of random numbers, and their use benefitted greatly from pseudorandom number generators, which are far quicker to use than the tables of random numbers that had been previously employed.\nApplication.\nMonte Carlo methods are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution.\nIn physics-related problems, Monte Carlo methods are useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean\u2013Vlasov processes, kinetic models of gases).\nOther examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in mathematics, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to systems engineering problems (space, oil exploration, aircraft design, etc.), Monte Carlo\u2013based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative \"soft\" methods.\nIn principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the 'sample mean') of independent samples of the variable. When the probability distribution of the variable is parameterized, mathematicians often use a Markov chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution. By the ergodic theorem, the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler.\nIn other problems, the objective is generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depend on the distributions of the current random states (see McKean\u2013Vlasov processes, nonlinear filtering equation). In other instances, a flow of probability distributions with an increasing level of sampling complexity arise (path spaces models with an increasing time horizon, Boltzmann\u2013Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain. A natural way to simulate these sophisticated nonlinear Markov processes is to sample multiple copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and MCMC methodologies, these mean-field particle techniques rely on sequential interacting samples. The terminology \"mean field\" reflects the fact that each of the \"samples\" (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.\nSimple Monte Carlo.\nSuppose one wants to know the expected value formula_1 of a population (and knows that formula_1 exists), but does not have a formula available to compute it. The simple Monte Carlo method gives an estimate for formula_1 by running formula_4 simulations and averaging the simulations' results. It has no restrictions on the probability distribution of the inputs to the simulations, requiring only that the inputs are randomly generated and are independent of each other and that formula_1 exists. A sufficiently large formula_4 will produce a value for formula_7 that is arbitrarily close to formula_1; more formally, it will be the case that, for any formula_9, formula_10.\nTypically, the algorithm to obtain formula_7 is\n \nAn example.\nSuppose we want to know how many times we should expect to throw three eight-sided dice for the total of the dice throws to be at least formula_12. We know the expected value exists. The dice throws are randomly distributed and independent of each other. So simple Monte Carlo is applicable:\n \nIf formula_4 is large enough, formula_7 will be within formula_15 of formula_1 for any formula_9.\nDetermining a sufficiently large \"n\".\nGeneral formula.\nLet formula_18. Choose the desired confidence level \u2013 the percent chance that, when the Monte Carlo algorithm completes, formula_7 is indeed within formula_15 of formula_1. Let formula_22 be the formula_22-score corresponding to that confidence level.\nLet formula_24 be the estimated variance, sometimes called the \"sample\" variance; it is the variance of the results obtained from a relatively small number formula_25 of \"sample\" simulations. Choose a formula_25; Driels and Shin observe that \"even for sample sizes an order of magnitude lower than the number required, the calculation of that number is quite stable.\"\nThe following algorithm computes formula_24 in one pass while minimizing the possibility that accumulated numerical error produces erroneous results:\n \n \n \"\" \nNote that, when the algorithm completes, formula_28 is the mean of the formula_25 results.\nThe value formula_4 is sufficiently large when\nformula_31\nIf formula_32, then formula_33; sufficient sample simulations were done to ensure that formula_28 is within formula_15 of formula_1. If formula_37, then formula_4 simulations can be run \"from scratch,\" or, since formula_25 simulations have already been done, one can just run formula_40 more simulations and add their results into those from the sample simulations:\n ;\nA formula when simulations' results are bounded.\nAn alternative formula can be used in the special case where all simulation results are bounded above and below.\nChoose a value for formula_15 that is twice the maximum allowed difference between formula_1 and formula_7. Let formula_44 be the desired confidence level, expressed as a percentage. Let every simulation result formula_45 be such that formula_46 for finite formula_47 and formula_48. To have confidence of at least formula_49 that formula_50, use a value for formula_4 such that:\nformula_52\nFor example, if formula_53, then formula_54.\nComputational costs.\nDespite its conceptual and algorithmic simplicity, the computational cost associated with a Monte Carlo simulation can be staggeringly high. In general the method requires many samples to get a good approximation, which may incur an arbitrarily large total runtime if the processing time of a single sample is high. Although this is a severe limitation in very complex problems, the embarrassingly parallel nature of the algorithm allows this large cost to be reduced (perhaps to a feasible level) through parallel computing strategies in local processors, clusters, cloud computing, GPU, FPGA, etc.\nHistory.\nBefore the Monte Carlo method was developed, simulations tested a previously understood deterministic problem, and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using probabilistic metaheuristics (see simulated annealing).\nAn early variant of the Monte Carlo method was devised to solve the Buffon's needle problem, in which \u03c0 can be estimated by dropping needles on a floor made of parallel equidistant strips. In the 1930s, Enrico Fermi first experimented with the Monte Carlo method while studying neutron diffusion, but he did not publish this work.\nIn the late 1940s, Stanis\u0142aw Ulam invented the modern version of the Markov Chain Monte Carlo method while he was working on nuclear weapons projects at the Los Alamos National Laboratory. In 1946, nuclear weapons physicists at Los Alamos were investigating neutron diffusion in the core of a nuclear weapon. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. Ulam proposed using random experiments. He recounts his inspiration as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The first thoughts and attempts I made to practice [the Monte Carlo Method] were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires. The question was what are the chances that a Canfield solitaire laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations, I wondered whether a more practical method than \"abstract thinking\" might not be to lay it out say one hundred times and simply observe and count the number of successful plays. This was already possible to envisage with the beginning of the new era of fast computers, and I immediately thought of problems of neutron diffusion and other questions of mathematical physics, and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations. Later [in 1946], I described the idea to John von Neumann, and we began to plan actual calculations.\nBeing secret, the work of von Neumann and Ulam required a code name. A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name \"Monte Carlo\", which refers to the Monte Carlo Casino in Monaco where Ulam's uncle would borrow money from relatives to gamble.\nMonte Carlo methods were central to the simulations required for further postwar development of nuclear weapons, including the design of the H-bomb, though severely limited by the computational tools at the time. Von Neumann, Nicholas Metropolis and others programmed the ENIAC computer to perform the first fully automated Monte Carlo calculations, of a fission weapon core, in the spring of 1948. In the 1950s Monte Carlo methods were used at Los Alamos for the development of the hydrogen bomb, and became popularized in the fields of physics, physical chemistry, and operations research. The Rand Corporation and the U.S. Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.\nThe theory of more sophisticated mean-field type particle Monte Carlo methods had certainly started by the mid-1960s, with the work of Henry P. McKean Jr. on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics. An earlier pioneering article by Theodore E. Harris and Herman Kahn, published in 1951, used mean-field genetic-type Monte Carlo methods for estimating particle transmission energies. Mean-field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms (a.k.a. metaheuristic) in evolutionary computing. The origins of these mean-field computational techniques can be traced to 1950 and 1954 with the work of Alan Turing on genetic type mutation-selection learning machines and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton, New Jersey.\nQuantum Monte Carlo, and more specifically diffusion Monte Carlo methods can also be interpreted as a mean-field particle Monte Carlo approximation of Feynman\u2013Kac path integrals. The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean-field particle interpretation of neutron-chain reactions, but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984. In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall N. Rosenbluth and Arianna W. Rosenbluth.\nThe use of sequential Monte Carlo in advanced signal processing and Bayesian inference is more recent. It was in 1993, that Gordon et al., published in their seminal work the first application of a Monte Carlo resampling algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. Another pioneering article in this field was Genshiro Kitagawa's, on a related \"Monte Carlo filter\", and the ones by Pierre Del Moral and Himilcon Carvalho, Pierre Del Moral, Andr\u00e9 Monin and G\u00e9rard Salut on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in 1989\u20131992 by P. Del Moral, J. C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the https:// (the Laboratory for Analysis and Architecture of Systems) on radar/sonar and GPS signal processing problems. These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism.\nFrom 1950 to 1996, all the publications on sequential Monte Carlo methodologies, including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms were written by Pierre Del Moral in 1996.\nBranching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons, and by Dan Crisan, Pierre Del Moral and Terry Lyons. Further developments in this field were described in 1999 to 2001 by P. Del Moral, A. Guionnet and L. Miclo.\nDefinitions.\nThere is no consensus on how \"Monte Carlo\" should be defined. For example, Ripley defines most probabilistic modeling as \"stochastic simulation\", with \"Monte Carlo\" being reserved for Monte Carlo integration and Monte Carlo statistical tests. Sawilowsky distinguishes between a simulation, a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to obtain the statistical properties of some phenomenon (or behavior).\nHere are some examples:\nKalos and Whitlock point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. \"Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling.\"\nConvergence of the Monte Carlo simulation can be checked with the Gelman-Rubin statistic.\nMonte Carlo and random numbers.\nThe main idea behind this method is that the results are computed based on repeated random sampling and statistical analysis. The Monte Carlo simulation is, in fact, random experimentations, in the case that, the results of these experiments are not well known.\nMonte Carlo simulations are typically characterized by many unknown parameters, many of which are difficult to obtain experimentally. Monte Carlo simulation methods do not always require truly random numbers to be useful (although, for some applications such as primality testing, unpredictability is vital). Many of the most useful techniques use deterministic, pseudorandom sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good simulations is for the pseudo-random sequence to appear \"random enough\" in a certain sense.\nWhat this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are uniformly distributed or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest and most common ones. Weak correlations between successive samples are also often desirable/necessary.\nSawilowsky lists the characteristics of a high-quality Monte Carlo simulation:\nPseudo-random number sampling algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given probability distribution.\nLow-discrepancy sequences are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called quasi-Monte Carlo methods.\nIn an effort to assess the impact of random number quality on Monte Carlo simulation outcomes, astrophysical researchers tested cryptographically secure pseudorandom numbers generated via Intel's RDRAND instruction set, as compared to those derived from algorithms, like the Mersenne Twister, in Monte Carlo simulations of radio flares from brown dwarfs. No statistically significant difference was found between models generated with typical pseudorandom number generators and RDRAND for trials consisting of the generation of 107 random numbers.\nMonte Carlo simulation versus \"what if\" scenarios.\nThere are ways of using probabilities that are definitely not Monte Carlo simulations \u2013 for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a \"best guess\" estimate. Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.\nBy contrast, Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring. For example, a comparison of a spreadsheet cost construction model run using traditional \"what if\" scenarios, and then running the comparison again with Monte Carlo simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the \"what if\" analysis. This is because the \"what if\" analysis gives equal weight to all scenarios (see quantifying uncertainty in corporate finance), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called \"rare events\".\nApplications.\nMonte Carlo methods are especially useful for simulating phenomena with significant uncertainty in inputs and systems with many coupled degrees of freedom. Areas of application include:\nPhysical sciences.\nMonte Carlo methods are very important in computational physics, physical chemistry, and related applied fields, and have diverse applications from complicated quantum chromodynamics calculations to designing heat shields and aerodynamic forms as well as in modeling radiation transport for radiation dosimetry calculations. In statistical physics, Monte Carlo molecular modeling is an alternative to computational molecular dynamics, and Monte Carlo methods are used to compute statistical field theories of simple particle and polymer systems. Quantum Monte Carlo methods solve the many-body problem for quantum systems. In radiation materials science, the binary collision approximation for simulating ion implantation is usually based on a Monte Carlo approach to select the next colliding atom. In experimental particle physics, Monte Carlo methods are used for designing detectors, understanding their behavior and comparing experimental data to theory. In astrophysics, they are used in such diverse manners as to model both galaxy evolution and microwave radiation transmission through a rough planetary surface. Monte Carlo methods are also used in the ensemble models that form the basis of modern weather forecasting.\nEngineering.\nMonte Carlo methods are widely used in engineering for sensitivity analysis and quantitative probabilistic analysis in process design. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,\nClimate change and radiative forcing.\nThe Intergovernmental Panel on Climate Change relies on Monte Carlo methods in probability density function analysis of radiative forcing.\nComputational biology.\nMonte Carlo methods are used in various fields of computational biology, for example for Bayesian inference in phylogeny, or for studying biological systems such as genomes, proteins, or membranes.\nThe systems can be studied in the coarse-grained or \"ab initio\" frameworks depending on the desired accuracy.\nComputer simulations allow monitoring of the local environment of a particular molecule to see if some chemical reaction is happening for instance. In cases where it is not feasible to conduct a physical experiment, thought experiments can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).\nComputer graphics.\nPath tracing, occasionally referred to as Monte Carlo ray tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the rendering equation, making it one of the most physically accurate 3D graphics rendering methods in existence.\nApplied statistics.\nThe standards for Monte Carlo experiments in statistics were set by Sawilowsky. In applied statistics, Monte Carlo methods may be used for at least four purposes:\nMonte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate randomization test is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice\u2014or more frequently\u2014for the efficiency of not having to track which permutations have already been selected).\nArtificial intelligence for games.\nMonte Carlo methods have been developed into a technique called Monte-Carlo tree search that is useful for searching for the best move in a game. Possible moves are organized in a search tree and many random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.\nThe Monte Carlo tree search (MCTS) method has four steps:\nThe net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.\nMonte Carlo Tree Search has been used successfully to play games such as Go, Tantrix, Battleship, Havannah, and Arimaa.\nDesign and visuals.\nMonte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in global illumination computations that produce photo-realistic images of virtual 3D models, with applications in video games, architecture, design, computer generated films, and cinematic special effects.\nSearch and rescue.\nThe US Coast Guard utilizes Monte Carlo methods within its computer modeling software SAROPS in order to calculate the probable locations of vessels during search and rescue operations. Each simulation can generate as many as ten thousand data points that are randomly distributed based upon provided variables. Search patterns are then generated based upon extrapolations of these data in order to optimize the probability of containment (POC) and the probability of detection (POD), which together will equal an overall probability of success (POS). Ultimately this serves as a practical application of probability distribution in order to provide the swiftest and most expedient method of rescue, saving both lives and resources.\nFinance and business.\nMonte Carlo simulation is commonly used to evaluate the risk and uncertainty that would affect the outcome of different decision options. Monte Carlo simulation allows the business risk analyst to incorporate the total effects of uncertainty in variables like sales volume, commodity and labor prices, interest and exchange rates, as well as the effect of distinct risk events like the cancellation of a contract or the change of a tax law.\nMonte Carlo methods in finance are often used to evaluate investments in projects at a business unit or corporate level, or other financial valuations. They can be used to model project schedules, where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project. Monte Carlo methods are also used in option pricing, default risk analysis. Additionally, they can be used to estimate the financial impact of medical interventions.\nLaw.\nA Monte Carlo approach was used for evaluating the potential value of a proposed program to help female petitioners in Wisconsin be successful in their applications for harassment and domestic abuse restraining orders. It was proposed to help women succeed in their petitions by providing them with greater advocacy thereby potentially reducing the risk of rape and physical assault. However, there were many variables in play that could not be estimated perfectly, including the effectiveness of restraining orders, the success rate of petitioners both with and without advocacy, and many others. The study ran trials that varied these variables to come up with an overall estimate of the success level of the proposed program as a whole.\nLibrary science.\nMonte Carlo approach had also been used to simulate the number of book publications based on book genre in Malaysia. The Monte Carlo simulation utilized previous published National Book publication data and book's price according to book genre in the local market. The Monte Carlo results were used to determine what kind of book genre that Malaysians are fond of and was used to compare book publications between Malaysia and Japan.\nOther.\nNassim Nicholas Taleb writes about Monte Carlo generators in his 2001 book \"Fooled by Randomness\" as a real instance of the reverse Turing test: a human can be declared unintelligent if their writing cannot be told apart from a generated one.\nUse in mathematics.\nIn general, the Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also Random number generation) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically. The most common application of the Monte Carlo method is Monte Carlo integration.\nIntegration.\nDeterministic numerical integration algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then 10100 points are needed for 100 dimensions\u2014far too many to be computed. This is called the curse of dimensionality. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to an iterated integral. 100 dimensions is by no means unusual, since in many physical problems, a \"dimension\" is equivalent to a degree of freedom.\nMonte Carlo methods provide a way out of this exponential increase in computation time. As long as the function in question is reasonably well-behaved, it can be estimated by randomly selecting points in 100-dimensional space, and taking some kind of average of the function values at these points. By the central limit theorem, this method displays formula_55 convergence\u2014i.e., quadrupling the number of sampled points halves the error, regardless of the number of dimensions.\nA refinement of this method, known as importance sampling in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling, recursive stratified sampling, adaptive umbrella sampling or the VEGAS algorithm.\nA similar approach, the quasi-Monte Carlo method, uses low-discrepancy sequences. These sequences \"fill\" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.\nAnother class of methods for sampling points in a volume is to simulate random walks over it (Markov chain Monte Carlo). Such methods include the Metropolis\u2013Hastings algorithm, Gibbs sampling, Wang and Landau algorithm, and interacting type MCMC methodologies such as the sequential Monte Carlo samplers.\nSimulation and optimization.\nAnother powerful and very popular application for random numbers in numerical simulation is in numerical optimization. The problem is to minimize (or maximize) functions of some vector that often has many dimensions. Many problems can be phrased in this way: for example, a computer chess program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the traveling salesman problem the goal is to minimize distance traveled. There are also applications to engineering design, such as multidisciplinary design optimization. It has been applied with quasi-one-dimensional models to solve particle dynamics problems by efficiently exploring large configuration space. Reference is a comprehensive review of many issues related to simulation and optimization.\nThe traveling salesman problem is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. If instead of the goal being to minimize the total distance traveled to visit each desired destination but rather to minimize the total time needed to reach each destination, this goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine the optimal path a different simulation is required: optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize the travel decisions to identify the best path to follow taking that uncertainty into account.\nInverse problems.\nProbabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines prior information with new information obtained by measuring some observable parameters (data).\nAs, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).\nWhen analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as normally information on the resolution power of the data is desired. In the general case many parameters are modeled, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the \"a priori\" distribution is available.\nThe best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex \"a priori\" information and data with an arbitrary noise distribution.\nPhilosophy.\nPopular exposition of the Monte Carlo Method was conducted by McCracken. The method's general philosophy was discussed by Elishakoff and Gr\u00fcne-Yanoff and Weirich.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "56099", "revid": "38448155", "url": "https://en.wikipedia.org/wiki?curid=56099", "title": "Red dwarf", "text": "Dim, low mass stars on the main sequence\nA red dwarf is the smallest kind of star on the main sequence. Red dwarfs are by far the most common type of fusing star in the Milky Way, at least in the neighborhood of the Sun. However, due to their low luminosity, individual red dwarfs are not easily observed. Not one star that fits the stricter definitions of a red dwarf is visible to the naked eye. Proxima Centauri, the star nearest to the Sun, is a red dwarf, as are fifty of the sixty nearest stars. According to some estimates, red dwarfs make up three-quarters of the fusing stars in the Milky Way.\nThe coolest red dwarfs near the Sun have a surface temperature of about and the smallest have radii about 9% that of the Sun, with masses about 7.5% that of the Sun. These red dwarfs have spectral types of L0 to L2. There is some overlap with the properties of brown dwarfs, since the most massive brown dwarfs at lower metallicity can be as hot as and have late M spectral types.\nDefinitions and usage of the term \"red dwarf\" vary by how inclusive they are on the hotter and more massive end. One definition is synonymous with stellar M dwarfs, yielding a maximum temperature of and 0.6\u00a0M\u2609. Another includes all stellar M-type main-sequence and all K-type main-sequence stars (K dwarf), yielding a maximum temperature of and 0.8\u00a0M\u2609. Some definitions include any stellar M dwarf and part of the K dwarf classification. Other definitions are also in use. Many of the coolest, lowest-mass M dwarfs are expected to be brown dwarfs, not true stars, and so those would be excluded from any definition of red dwarf.\nStellar models indicate that red dwarfs less than 0.35\u00a0M\u2609 are fully convective. Hence, the helium produced by the thermonuclear fusion of hydrogen is constantly remixed throughout the star, avoiding helium buildup at the core, thereby prolonging the period of fusion. A low-mass red dwarf therefore develops very slowly, maintaining a constant luminosity and spectral type for trillions of years, until its fuel is depleted and it turns into a blue dwarf. Because of the comparatively short age of the universe, no red dwarfs yet exist at advanced stages of evolution.\nDefinition.\nThe term \"red dwarf\" when used to refer to a star does not have a strict definition. One of the earliest uses of the term was in 1915, used simply to contrast \"red\" dwarf stars with hotter \"blue\" dwarf stars. It became established use, although the definition remained vague. In terms of which spectral types qualify as red dwarfs, different researchers picked different limits, for example K8\u2013M5 or \"later than K5\". \"Dwarf M star\", abbreviated dM, was also used, but sometimes it also included stars of spectral type K.\nIn modern usage, the definition of a \"red dwarf\" still varies. When explicitly defined, it typically includes late K- and early to mid-M-class stars, but in many cases it is restricted to M-class stars. In some cases all K stars are included as red dwarfs, and occasionally even earlier stars.\nThe most recent surveys place the coolest true main-sequence stars into spectral types L2 or L3. At the same time, many objects cooler than about M6 or M7 are brown dwarfs, insufficiently massive to sustain hydrogen-1 fusion. This gives a significant overlap in spectral types for red and brown dwarfs. Objects in that spectral range can be difficult to categorize.\nDescription and characteristics.\nHertzsprung\u2013Russell diagram\nSpectral type\nO\nB\nA\nF\nG\nK\nM\nL\nT\nBrown dwarfs\nWhite dwarfs\nRed dwarfs\nSubdwarfs\nMain sequence&lt;br&gt;(\"dwarfs\")\nSubgiants\nGiants\nRed giants\nBlue giants\nBright giants\nSupergiants\nRed supergiant\nHypergiants\nabsolutemagni-tude(MV)\nRed dwarfs are very-low-mass stars. As a result, they have relatively low pressures, a low fusion rate, and hence, a low temperature. The energy generated is the product of nuclear fusion of hydrogen into helium by way of the proton\u2013proton (PP) chain. Hence, these stars emit relatively little light, sometimes as little as &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u204410,000 that of the Sun, although this would still imply a power output on the order of (10 trillion gigawatts or 10 ZW). Even the largest red dwarfs (for example HD 179930, HIP 12961 and Lacaille 8760) have only about 10% of the Sun's luminosity. In general, red dwarfs less than 0.35\u00a0M\u2609 transport energy from the core to the surface by convection. Convection occurs because of the opacity of the interior, which has a high density compared with the temperature. As a result, energy transfer by radiation is decreased, and instead convection is the main form of energy transport to the surface of the star. Above this mass, a red dwarf will have a region around its core where convection does not occur.\nBecause low-mass red dwarfs are fully convective, helium does not accumulate at the core, and compared with larger stars such as the Sun, they can burn a larger proportion of their hydrogen before leaving the main sequence. As a result, red dwarfs have estimated lifespans far longer than the present age of the universe, and stars less than 0.8\u00a0M\u2609 have not had time to leave the main sequence. The lower the mass of a red dwarf, the longer the lifespan. It is believed that the lifespans of these stars exceed the expected 10-billion-year lifespan of the Sun by the third or fourth power of the ratio of the solar mass to their masses; thus, a 0.1\u00a0M\u2609 red dwarf may continue burning for 10 trillion years. As the proportion of hydrogen in a red dwarf is consumed, the rate of fusion declines and the core starts to contract. The gravitational energy released by this size reduction is converted into heat, which is carried throughout the star by convection.\nAccording to computer simulations, the minimum mass a red dwarf must have to eventually evolve into a red giant is 0.25\u00a0M\u2609; less massive objects, as they age, would increase their surface temperatures and luminosities, becoming blue dwarfs and finally white dwarfs.\nThe less massive the star, the longer this evolutionary process takes. A 0.16\u00a0M\u2609 red dwarf (approximately the mass of the nearby Barnard's Star) would stay on the main sequence for 2.5 trillion years, followed by five billion years as a blue dwarf, during which the star would have one third of the Sun's luminosity (L\u2609) and a surface temperature of 6,500\u20138,500 kelvins.\nThe fact that red dwarfs and other low-mass stars remain on the main sequence when more massive stars have moved off the main sequence allows the age of star clusters to be estimated by finding the mass at which the stars move off the main sequence. This provides a lower limit to the age of the Universe and also allows formation timescales to be placed upon the structures within the Milky Way, such as the Galactic halo and Galactic disk.\nAll observed red dwarfs contain \"metals\", defined in astronomy as elements heavier than hydrogen and helium. The Big Bang model predicts that the first generation of stars should have only hydrogen, helium, and trace amounts of lithium, and hence would be of low metallicity. With their extreme lifespans, any red dwarfs that were a part of that first generation (population III stars) should still exist today. Low-metallicity red dwarfs, however, are rare. The accepted model for the chemical evolution of the universe anticipates such a scarcity of metal-poor dwarf stars because only giant stars are thought to have formed in the metal-poor environment of the early universe. As giant stars end their short lives in supernova explosions, they spew out the heavier elements needed to form smaller stars. Therefore, dwarfs became more common as the universe aged and became enriched in metals. While the basic scarcity of ancient metal-poor red dwarfs is expected, observations have detected even fewer than predicted. The sheer difficulty of detecting objects as dim as red dwarfs was thought to account for this discrepancy, but improved detection methods have only confirmed the discrepancy.\nThe boundary between the least massive red dwarfs and the most massive brown dwarfs depends strongly on the metallicity. At solar metallicity the boundary occurs at about 0.07\u00a0M\u2609, while at zero metallicity the boundary is around 0.09\u00a0M\u2609. At solar metallicity, the least massive red dwarfs theoretically have temperatures around , while measurements of red dwarfs in the solar neighbourhood suggest the coolest stars have temperatures of about and spectral classes of about L2. Theory predicts that the coolest red dwarfs at zero metallicity would have temperatures of about . The least massive red dwarfs have radii of about 0.09\u00a0R\u2609, while both more massive red dwarfs and less massive brown dwarfs are larger.\nSpectral standard stars.\nThe spectral standards for M\u00a0type stars have changed slightly over the years, but settled down somewhat since the early 1990s. Part of this is due to the fact that even the nearest red dwarfs are fairly faint, and their colors do not register well on photographic emulsions used in the early to mid 20th century. The study of mid- to late-M\u00a0dwarfs has significantly advanced only in the past few decades, primarily due to development of new astrographic and spectroscopic techniques, dispensing with photographic plates and progressing to charged-couple devices (CCDs) and infrared-sensitive arrays.\nThe revised Yerkes Atlas system (Johnson &amp; Morgan, 1953) listed only two M\u00a0type spectral standard stars: HD\u00a0147379 (M0V)\nand HD\u00a095735/Lalande 21185 (M2V). While HD\u00a0147379 was not considered a standard by expert classifiers in later compendia of standards, Lalande 21185 is still a primary standard for M2V. Robert Garrison does not list any \"anchor\" standards among the red dwarfs, but Lalande 21185 has survived as a M2V standard through many compendia. The review on MK classification by Morgan &amp; Keenan (1973) did not contain red dwarf standards. \nIn the mid-1970s, red dwarf standard stars were published by Keenan &amp; McNeil (1976) and Boeshaar (1976), but there was little agreement among the standards. As later cooler stars were identified through the 1980s, it was clear that an overhaul of the red dwarf standards was needed. Building primarily upon the Boeshaar standards, a group at Steward Observatory (Kirkpatrick, Henry, &amp; McCarthy, 1991) filled in the spectral sequence from K5V to M9V. It is these M\u00a0type dwarf standard stars which have largely survived as the main standards to the modern day. There have been negligible changes in the red dwarf spectral sequence since 1991. Additional red dwarf standards were compiled by Henry et al. (2002), and D. Kirkpatrick has recently\nreviewed the classification of red dwarfs and standard stars in Gray &amp; Corbally's 2009 monograph. The M\u00a0dwarf primary spectral standards are: GJ 270 (M0V), GJ 229A (M1V), Lalande 21185 (M2V), Gliese 581 (M3V), Gliese 402 (M4V), GJ 51 (M5V), Wolf 359 (M6V), van Biesbroeck 8 (M7V), VB 10 (M8V), LHS 2924 (M9V).\nPlanet formation.\nGas-rich disks (protoplanetary disks) have been detected around low-mass stars and brown dwarfs with ages as high as around 45 Myrs. This is unusual as more massive stars usually don't show primordial disks beyond 10 Myrs. These old disks have been dubbed Peter Pan disks, with J0808 being the prototype. The long presence of gas in the disk could enable the formation of resonant chains, such as seen in TRAPPIST-1. It is thought that only some will reach this high age and most will dissipate after 5 Myrs. The environment can play a role in the disk lifetime, such as stellar flybys and external photoevaporation, which can result in ionized proplyds. Some edge-on protoplanetary disks around early M-stars are resolved, such as Tau 042021 and HH 30. These show jets and more recently disk winds in NIRCam and NIRSpec observations. The disk wind is an important part in removal of mass from the disk and accretion of material onto the surface of stars.\nObservations with the Mid-Infrared Instrument has advanced the study of the composition of the inner part of primordial disks around late M-dwarfs. Studies found either hydrocarbon-rich composition (e.g. 2MASS J1605\u20131933, ISO-ChaI 147, J0446B) or water-rich composition (e.g. Sz 114). The disks show a trend from oxygen-rich in younger disks to carbon-rich in older disks. Silicates are also detected for some disks. This is explained with a model of inwards drifting material. At first water-ice-rich pebbles drift inwards, increasing the amount of oxygen in the inner disk. Then carbon-rich vapour drifts inwards and increases the amount of carbon in the inner disk. This process is more efficient in very low-mass stars because the icy outer part is closer to the inner disk. This trend of carbon-rich disks is also present in brown dwarfs and planetary-mass objects. The brown dwarf 2M1207 has a disk rich in hydrocarbons, and the planetary-mass object Cha 1107\u22127626 also shows hydrocarbons in the disk. This composition could influence the composition of the planets formed within these disks, especially their atmospheres. If close-in planets accrete their atmospheres early, they could have a low C/O ratio (low amounts of carbon, high amounts of oxygen). If they accrete their atmospheres late, their atmospheres could have a high C/O ratio (similar to Titan). The removal of carbon from the solids could also result in carbon-poor composition of the soldis (core/mantle/crust) in rocky planets.\nAfter the primordial gas is removed, the system is left with a debris disk. Examples of debris disks around red dwarfs are AU Microscopii, CE Antliae and Fomalhaut C.\nPlanets.\nMany red dwarfs are orbited by exoplanets, but large Jupiter-sized planets are comparatively rare. Doppler surveys of a wide variety of stars indicate about 1 in 6\u00a0stars with twice the mass of the Sun are orbited by one or more of Jupiter-sized planets, versus 1 in 16 for Sun-like stars and the frequency of close-in giant planets (Jupiter size or larger) orbiting red dwarfs is only 1 in 40. On the other hand, microlensing surveys indicate that long-orbital-period Neptune-mass planets are found around one in three red dwarfs. Observations with HARPS further indicate 40% of red dwarfs have a \"super-Earth\" class planet orbiting in the habitable zone where liquid water can exist on the surface. Computer simulations of the formation of planets around low-mass stars predict that Earth-sized planets are most abundant, but more than 90% of the simulated planets are at least 10% water by mass, suggesting that many Earth-sized planets orbiting red dwarf stars are covered in deep oceans.\nAt least four and possibly up to six exoplanets were discovered orbiting within the Gliese\u00a0581 planetary system between 2005 and 2010. One planet has about the mass of Neptune, or 16\u00a0Earth masses (M\ud83d\udf28). It orbits just from its star, and is estimated to have a surface temperature of , despite the dimness of its star. In 2006, an even smaller exoplanet (only 5.5\u00a0M\ud83d\udf28) was found orbiting the red dwarf OGLE-2005-BLG-390L; it lies from the star and its surface temperature is .\nIn 2007, a new, potentially habitable exoplanet, Gliese 581c, was found, orbiting Gliese 581. The minimum mass estimated by its discoverers (a team led by Stephane Udry) is 5.36\u00a0M\ud83d\udf28. The discoverers estimate its radius to be 1.5\u00a0times that of Earth (R\ud83d\udf28). Since then Gliese 581d, which is also potentially habitable, was discovered.\nGliese\u00a0581c and d are within the habitable zone of the host star, and are two of the most likely candidates for habitability of any exoplanets discovered so far. Gliese 581g, detected September 2010, has a near-circular orbit in the middle of the star's habitable zone. However, the planet's existence is contested.\nOn 23\u00a0February 2017 NASA announced the discovery of seven Earth-sized planets orbiting the red dwarf star TRAPPIST-1 approximately 39\u00a0light-years away in the constellation Aquarius. The planets were discovered through the transit method, meaning we have mass and radius information for all of them. TRAPPIST-1e, f, and g appear to be within the habitable zone and may have liquid water on the surface.\nHabitability.\nModern evidence suggests that planets in red dwarf systems are extremely unlikely to be habitable. In spite of their great numbers and long lifespans, there are several factors which may make life difficult on planets around a red dwarf. First, planets in the habitable zone of a red dwarf would be so close to the parent star that they would likely be tidally locked. For a nearly circular orbit, this would mean that one side would be in perpetual daylight and the other in eternal night. This could create enormous temperature variations from one side of the planet to the other. Such conditions would appear to make it difficult for forms of life similar to those on Earth to evolve. And it appears there is a great problem with the atmosphere of such tidally locked planets: the perpetual night zone would be cold enough to freeze the main gases of their atmospheres, leaving the daylight zone bare and dry. On the other hand, a theory proposes that either a thick atmosphere or planetary ocean could potentially circulate heat around such a planet. Furthermore, even if a red dwarf's characteristics render most of its planet's surface uninhabitable, there is a chance for life to exist around a limited region, such as the planet's terminator.\nVariability in stellar energy output may also have negative impacts on the development of life. Red dwarfs are often flare stars, which can emit gigantic flares, doubling their brightness in minutes. This variability makes it difficult for life to develop and persist near a red dwarf. While it may be possible for a planet orbiting close to a red dwarf to keep its atmosphere even if the star flares, more-recent research suggests that these stars may be the source of constant high-energy flares and very large magnetic fields, diminishing the possibility of life as we know it.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "56100", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=56100", "title": "Ouagadougou", "text": "Capital of Burkina Faso\nOuagadougou or Wagadugu (; , ; ; , ) is the capital and largest city of Burkina Faso, and the administrative, communications, cultural and economic centre of the nation. It has a population of 2,415,266 in 2019. The city's name is often shortened to \"Ouaga\". The inhabitants are called \"ouagalais\". The spelling of the name \"Ouagadougou\" is derived from the French orthography common in former French African colonies.\nOuagadougou's primary industries are food processing and textiles. It is served by an international airport and is linked by rail to Abidjan in the Ivory Coast and, for freight only, to Kaya. There are several highways linking the city to Niamey, Niger, south to Ghana, and southwest to Ivory Coast. Ouagadougou has one of West Africa's largest markets, which burned down in 2003 and has since reopened with better facilities and improved fire-prevention measures. Other attractions include the National Museum of Burkina Faso, the Moro-Naba Palace (site of the Moro-Naba Ceremony), the National Museum of Music, and several craft markets.\nHistory.\nFoundation and regional importance.\nOuagadougou was founded possibly as early as 1050 by the Soninke Wangara diaspora from the Ghana Empire, also known as Wagadu. The name \"Wagadugu\" means 'home of the Wagu', the Soninke subgroup that ruled Ghana. \"Ouagadougou\" is a Francophone spelling of this name.\nThe Mossi people, moving north in the 14th century, conquered Wagadugu around the same time they raided Walata, contributing to the decline of the Mali Empire. According to legend, the city was taken by Oubri, a grandson of Ouedraogo.\nThe eponymous Wagadugu Kingdom was founded in the 15th century, which became the main center of the Mossi States around 1495. The 10th Moro Naba, Nyadfo, was the first Moro-Naba to live at Ouagadougou, in the middle of the 17th century. It became the permanent capital under the 21st Moro Naba, Zombre, a century later. The Moro-Naba Ceremony is still performed every Friday by the Moro-Naba and his court. The 24th Moro Naba, Doulougou, built the first mosque in Ouagadougou early in the nineteenth century.\nColonialism.\nOn 5 September 1896 French forces entered Ouagadougou and burned the city to the ground. In 1919 the colonial administration made Ouagadougou the capital of the Upper Volta territory, extensively rebuilding the town. In 1954 the railroad line from Ivory Coast reached the city, spurring massive population growth.\nIndependence.\nOn 15 January 2016, gunmen armed with heavy weapons attacked central Ouagadougou at the Cappuccino restaurant and the Splendid Hotel. 28 people were killed, and at least 56 wounded; after a government counterattack, a total of 176 hostages were released the morning after the initial attack. Three of the perpetrators were also killed. The jihadist insurgency continued with major attacks in 2017 and 2018.\nClimate.\nOuagadougou's climate is hot semi-arid (\"BSh\") under K\u00f6ppen-Geiger classification, and closely borders with tropical wet and dry (\"Aw\"). The city is part of the Sudano-Sahelian area, with annual rainfall of about . The rainy season stretches from May to September, with an average temperature of . The cool season runs from October to February, with a minimum average temperature of . The maximum temperature during the hot season, which runs from March to April, can reach . The harmattan (a dry wind) and the West African Monsoon are the two main factors that determine Ouagadougou's climate. Being further north, Ouagadougou's warmest months are slightly hotter and drier than those of Bobo-Dioulasso, the country's second most populous city.\nGovernment.\nOuagadougou's first municipal elections were held in 1956.\nThe city is divided into five arrondissements, consisting of 30 sectors, which are subdivided into districts. Districts of Ouagadougou include Gounghin, Kamsaoghin, Koulouba, Moemmin, Niogsin, Paspanga, Peuloghin, Bilbalogho, and Tiendpalogo.\nOuagadougou's communes have invested in huge city-management projects. This is largely because Ouagadougou constitutes a 'cultural centre' by merit of holding the SIAO (International Arts and Crafts fair) and the FESPACO (Panafrican Film and Television Festival of Ouagadougou). Moreover, the villages' growing affluence allows for such investment, and the population's rapid growth necessitates it.\nEducation.\nThough literacy in Ouagadougou is not high, there are three universities in the city. The largest is the state University of Ouagadougou, which was founded in 1974. In 2010 it had around 40,000 students (83% of the national population of university students).\nThe city's official language is French and the principal local languages are More, Dyula and Fulfulde. The bilingual program in schools (French plus one of the local languages) was established in 1994.\nInternational schools include:\nSport.\nOuagadougou's inhabitants play a wide array of sports, including association football, basketball, and volleyball. There are tournaments and activities organized by the local authorities. The Stade du 4-Ao\u00fbt is the home of \u00c9toile Filante de Ouagadougou, the city's main football team.\nHealth.\nOuagadougou has both state and private hospitals. The two state hospitals in the city are the Centre hospitalier national Yalgado Ouedraogo (CHNYO) and the Centre hospitalier national p\u00e9diatrique Charles de Gaulle (CHNP-CDG). Despite that, the local population still largely can only afford traditional local medicine and the \"pharmacop\u00e9e\".\nTransport.\nAir transport.\nThomas Sankara International Airport Ouagadougou (code OUA) serves the area with flights to West Africa and Europe. Air Burkina has its head office in the Air Burkina Storey Building () in Ouagadougou.\nRail.\nOuagadougou is connected by passenger rail service to Bobo-Dioulasso, Koudougou and Ivory Coast. As of June 2014, Sitarail operates a passenger train three times a week along the route from Ouagadougou to Abidjan. There are freight services to Kaya in north Burkina Faso and in 2014 plans were announced to revive freight services to the manganese mine at Tambao starting in 2016.\nEconomy.\nThe economy of Ouagadougou is based on industry and commerce. Some industrial facilities have relocated from Bobo-Dioulasso to Ouagadougou, which has made the city an important industrial centre of Burkina Faso. The industrial areas of Kossodo and Gounghin are home to several processing plants and factories. The industry of Ouagadougou is the sector that fuels urban growth, as people move to the city from the countryside to find employment in industry. The Copromof workshop in Ouagadougou sews cotton lingerie for the French label \"Atelier Augusti.\"\nOuagadougou is an important commercial centre. It is a centre where goods are collected and directed to rural areas. With a large consumer base, large amounts of energy sources, raw materials for buildings, agricultural products and livestock products are imported to the city.\nThe economy is dominated by the informal sector, which is characterized by petty commodity production, and workers not necessarily having salaries. Traditional, informal trade is widespread and concentrated around markets and major roads, as well as in outlets in neighborhoods. While the formal economy consists of modern economic practices with workplaces having qualified, stable labor forces, or more traditional forms of business such as family businesses.\nThe tertiary sector is also an important part of the economy. This comprises communications, banking, transport, bars, restaurants, hotels, as well as administrative jobs.\nTwin towns \u2013 sister cities.\nOuagadougou is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nParks.\nThe Bangr-Weoogo urban park (area: ), before colonialism, belonged to the Mosse chiefs. Considering it a sacred forest, many went there for traditional initiations or for refuge. The French colonists, disregarding its local significance and history, established it as a park in the 1930s. In 1985, renovations were done in the park. In January 2001, the park was renamed \"Parc Urbain Bangr-Weoogo\", meaning \"the urban park of the forest of knowledge\".\nAnother notable park in Ouagadougou is the \"L'Unit\u00e9 P\u00e9dagogique\", which shelters animals in a semi-free state. This botanic garden/biosphere system stretches over and also serves as a museum for the country's history.\n\"Jardin de l'amiti\u00e9 Ouaga-Loudun\" (Garden of Ouaga-Loudun Friendship), with a green space that was renovated in 1996, is a symbol of the twin-city relationship between Ouagadougou and Loudun in France. It is situated in the centre of the city, near the \"Nation Unies' crossroads\".\nCulture.\nThere are a number of cultural and art venues, such as the Maison du Peuple and Salle des Banquets, in addition to performances of many genres of music, including traditional folk music, modern music, and rap.\nArt and crafts.\nSeveral international festivals and activities are organized within the municipality, such as FESPACO (Panafrican Film and Television Festival of Ouagadougou), which is Africa's largest festival of this type, SIAO (International Art and Craft Fair), FESPAM (Pan-African Music Festival), FITMO (International Theatre and Marionnette Festival) and FESTIVO.\nPlaces of worship.\nThe most common places of worship are Muslim mosques. There are also numerous Christian churches: Roman Catholic Archdiocese of Ouagadougou (Catholic Church), Association of Reformed Evangelical Church of Burkina Faso (World Communion of Reformed Churches), Assemblies of God, Deeper Life Bible Church, and the International Evangelism Center.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56101", "revid": "1318138457", "url": "https://en.wikipedia.org/wiki?curid=56101", "title": "Ethnic conflict", "text": "Conflict between ethnic groups\nAn ethnic conflict is a conflict between two or more ethnic groups. While the source of the conflict may be political, social, economic or religious, the individuals in conflict must expressly fight for their ethnic group's position within society. This criterion differentiates ethnic conflict from other forms of struggle.\nAcademic explanations of ethnic conflict generally fall into one of three schools of thought: primordialist, instrumentalist or constructivist. Recently, some have argued for either top-down or bottom-up explanations for ethnic conflict. Intellectual debate has also focused on whether ethnic conflict has become more prevalent since the end of the Cold War, and on devising ways of managing conflicts, through instruments such as consociationalism and federalisation.\nTheories of causes.\nIt is argued that rebel movements are more likely to organize around ethnicity because ethnic groups are more apt to be aggrieved, better able to mobilize, and more likely to face difficult bargaining challenges compared to other groups. The causes of ethnic conflict are debated by political scientists and sociologists. Official academic explanations generally fall into one of three schools of thought: primordialist, instrumentalist, and constructivist. More recent scholarship draws on all three schools.\nPrimordialist accounts.\nProponents of primordialist accounts argue that \"[e]thnic groups and nationalities exist because there are traditions of belief and action towards primordial objects such as biological features and especially territorial location\". Primordialist accounts rely on strong ties of kinship among members of ethnic groups. Donald L. Horowitz argues that this kinship \"makes it possible for ethnic groups to think in terms of family resemblances\".\nClifford Geertz, a founding scholar of primordialism, asserts that each person has a natural connection to perceived kinsmen. In time and through repeated conflict, essential ties to one's ethnicity will coalesce and will interfere with ties to civil society. Ethnic groups will consequently always threaten the survival of civil governments but not the existence of nations formed by one ethnic group. Thus, when considered through a primordial lens, ethnic conflict in multi-ethnic society is inevitable.\nA number of political scientists argue that the root causes of ethnic conflict do not involve ethnicity \"per se\" but rather institutional, political, and economic factors. These scholars argue that the concept of ethnic war is misleading because it leads to an essentialist conclusion that certain groups are doomed to fight each other when in fact the wars between them that occur are often the result of political decisions.\nMoreover, primordial accounts do not account for the spatial and temporal variations in ethnic violence. If these \"ancient hatreds\" are always simmering under the surface and are at the forefront of people's consciousness, then ethnic groups should constantly be ensnared in violence. However, ethnic violence occurs in sporadic outbursts. For example, Varshney points out that although Yugoslavia broke up due to ethnic violence in the 1990s, it had enjoyed a long peace of decades before the USSR collapsed. Therefore, some scholars claim that it is unlikely that primordial ethnic differences alone caused the outbreak of violence in the 1990s.\nPrimordialists have reformulated the \"ancient hatreds\" hypothesis and have focused more on the role of human nature. Petersen argues that the existence of hatred and animosity does not have to be rooted in history for it to play a role in shaping human behavior and action: \"If 'ancient hatred' means a hatred consuming the daily thoughts of great masses of people, then the 'ancient hatreds' argument deserves to be readily dismissed. However, if hatred is conceived as a historically formed 'schema' that guides action in some situations, then the conception should be taken more seriously.\"\nInstrumentalist accounts.\nAnthony Smith notes that the instrumentalist account \"came to prominence in the 1960s and 1970s in the United States, in the debate about (white) ethnic persistence in what was supposed to have been an effective melting pot\". This new theory sought explained persistence as the result of the actions of community leaders, \"who used their cultural groups as sites of mass mobilization and as constituencies in their competition for power and resources, because they found them more effective than social classes\". In this account of ethnic identification, ethnicity and race are viewed as instrumental means to achieve particular ends.\nWhether ethnicity is a fixed perception or not is not crucial in the instrumentalist accounts. Moreover, the scholars of this school generally do not oppose the view that ethnic difference plays a part in many conflicts. They simply claim that ethnic difference is not sufficient to explain conflicts.\nMass mobilization of ethnic groups can only be successful if there are latent ethnic differences to be exploited, otherwise politicians would not even attempt to make political appeals based on ethnicity and would focus instead on economic or ideological appeals. For these reasons, it is difficult to completely discount the role of inherent ethnic differences. Additionally, ethnic entrepreneurs, or elites, could be tempted to mobilize ethnic groups in order to gain their political support in democratizing states. Instrumentalist theorists especially emphasize this interpretation in ethnic states in which one ethnic group is promoted at the expense of other ethnicities.\nFurthermore, ethnic mass mobilization is likely to be plagued by collective action problems, especially if ethnic protests are likely to lead to violence. Instrumentalist scholars have tried to respond to these shortcomings. For example, Russell Hardin argues that ethnic mobilization faces problems of coordination and not collective action. He points out that a charismatic leader acts as a focal point around which members of an ethnic group coalesce. The existence of such an actor helps to clarify beliefs about the behavior of others within an ethnic group.\nConstructivist accounts.\nA third, constructivist, set of accounts stress the importance of the socially constructed nature of ethnic groups, drawing on Benedict Anderson's concept of the imagined community. Proponents of this account point to Rwanda as an example because the Tutsi/Hutu distinction was codified by the Belgian colonial power in the 1930s on the basis of cattle ownership, physical measurements and church records. Identity cards were issued on this basis, and these documents played a key role in the genocide of 1994.\nSome argue that constructivist narratives of historical master cleavages are unable to account for local and regional variations in ethnic violence. For example, Varshney highlights that in the 1960s \"racial violence in the USA was heavily concentrated in northern cities; southern cities though intensely politically engaged, did not have riots\". A constructivist master narrative is often a country level variable whereas studies of incidences of ethnic violence are often done at the regional and local level.\nScholars of ethnic conflict and civil wars have introduced theories that draw insights from all three traditional schools of thought. In \"The Geography of Ethnic Violence\", Monica Duffy Toft shows how ethnic group settlement patterns, socially constructed identities, charismatic leaders, issue indivisibility, and state concern with precedent setting can lead rational actors to escalate a dispute to violence, even when doing so is likely to leave contending groups much worse off. Such research addresses empirical puzzles that are difficult to explain using primordialist, instrumentalist, or constructivist approaches alone. As Varshney notes, \"pure essentialists and pure instrumentalists do not exist anymore\".\nStudy in the post-Cold War world.\nOne of the most debated issues relating to ethnic conflict is whether it has become more or less prevalent in the post\u2013Cold War period. Even\nthough a decline in the rate of new ethnic conflicts was evident in the late 1990s, ethnic conflict remains the most common form of\narmed intrastate conflict today. At the end of the Cold War, academics including Samuel P. Huntington and Robert D. Kaplan predicted a proliferation of conflicts fueled by civilisational clashes, tribalism, resource scarcity and overpopulation.\nThe violent ethnic conflicts in Nigeria, Mali, Sudan and other countries in the Sahel region have been exacerbated by droughts, food shortages, land degradation, and population growth.\nHowever, some theorists contend that this does not represent a rise in the incidence of ethnic conflict, because many of the proxy wars fought during the Cold War as ethnic conflicts were actually hot spots of the Cold War. Research shows that the fall of Communism and the increase in the number of capitalist states were accompanied by a decline in total warfare, interstate wars, ethnic wars, revolutionary wars, and the number of refugees and displaced persons. Indeed, some scholars have questioned whether the concept of ethnic conflict is useful at all. Others have attempted to test the \"clash of civilisations\" thesis, finding it to be difficult to operationalise and that civilisational conflicts have not risen in intensity in relation to other ethnic conflicts since the end of the Cold War.\nA key question facing scholars who attempt to adapt their theories of interstate violence to explain or predict large-scale ethnic violence is whether ethnic groups could be considered \"rational\" actors.\nPrior to the end of the Cold War, the consensus among students of large-scale violence was that ethnic groups should be considered irrational actors, or semi-rational at best. If true, general explanations of ethnic violence would be impossible. In the years since, however, scholarly consensus has shifted to consider that ethnic groups may in fact be counted as rational actors, and the puzzle of their apparently irrational actions (for example, fighting over territory of little or no intrinsic worth) must therefore be explained in some other way. As a result, the possibility of a general explanation of ethnic violence has grown, and collaboration between comparativist and international-relations sub-fields has resulted in increasingly useful theories of ethnic conflict.\nPublic goods provision.\nA major source of ethnic conflict in multi-ethnic democracies is over the access to state patronage. Conflicts over state resources between ethnic groups can increase the likelihood of ethnic violence. In ethnically divided societies, demand for public goods decreases as each ethnic group derives more utility from benefits targeted at their ethnic group in particular. These benefits would be less valued if all other ethnic groups had access to them. Targeted benefits are more appealing because ethnic groups can solidify or heighten their social and economic status relative to other ethnic groups whereas broad programmatic policies will not improve their relative worth. Politicians and political parties in turn, have an incentive to favor co-ethnics in their distribution of material benefits. Over the long run, ethnic conflict over access to state benefits is likely to lead to the ethnification of political parties and the party system as a whole where the political salience of ethnic identity increase leading to a self-fulfilling equilibrium: If politicians only distribute benefits on an ethnic basis, voters will see themselves primarily belonging to an ethnic group and view politicians the same way. They will only vote for the politician belonging to the same ethnic group. In turn, politicians will refrain from providing public goods because it will not serve them well electorally to provide services to people not belonging to their ethnic group. In democratizing societies, this could lead to ethnic outbidding and lead to extreme politicians pushing out moderate co-ethnics. Patronage politics and ethnic politics eventually reinforce each other, leading to what Chandra terms a \"patronage democracy\".\nThe existence of patronage networks between local politicians and ethnic groups make it easier for politicians to mobilize ethnic groups and instigate ethnic violence for electoral gain since the neighborhood or city is already polarized along ethnic lines. The dependence of ethnic groups on their co-ethnic local politician for access to state resources is likely to make them more responsive to calls of violence against other ethnic groups. Therefore, the existence of these local patronage channels generates incentives for ethnic groups to engage in politically motivated violence.\nWhile the link between ethnic heterogeneity and under provision of public goods is generally accepted, there is little consensus around the causal mechanism underlying this relationship. To identify possible causal stories, Humphreys and Habyarimana ran a series of behavioral games in Kampala, Uganda, that involved several local participants completing joint tasks and allocating money amongst them. Contrary to the conventional wisdom, they find that participants did not favor the welfare of their co-ethnics disproportionately. It was only when anonymity was removed and everyone's ethnicity was known did co-ethnics decide to favor each other. Humphreys and Habyarimana argue that cooperation among co-ethnics is primarily driven by reciprocity norms that tend to be stronger among co-ethnics. The possibility of social sanctions compelled those who would not otherwise cooperate with their co-ethnics to do so. The authors find no evidence to suggest that co-ethnics display a greater degree of altruism towards each other or have the same preferences. Ethnic cooperation takes place because co-ethnics have common social networks and therefore can monitor each other and can threaten to socially sanction any transgressors.\nEthnic conflict amplification.\nOnline social media.\nIn the early twenty-first century, the online social networking service Facebook has played a role in amplifying ethnic violence in the Rohingya genocide that started in October 2016 and in ethnic violence in Ethiopia during 2019\u20132020.\nThe United Nations Human Rights Council described Facebook as having been \"a useful instrument for those seeking to spread hate\" and complained that Facebook was unable to provide data on the extent of its role in the genocide.\nDuring 2019\u20132020, posts on Facebook dominated the Internet in Ethiopia and played a major role in encouraging ethnic violence. An October 2019 Facebook post led to the deaths of 70 people in Ethiopia. In mid-2020, ethnic tensions in Ethiopia were amplified by online hate speech on Facebook that followed the 29 June assassination of Hachalu Hundessa. The Hachalu Hundessa riots, in which mobs \"lynched, beheaded, and dismembered their victims\", took place with \"almost-instant and widespread sharing of hate speech and incitement to violence on Facebook, which whipped up people's anger\", according to David Gilbert writing in \"Vice\". People \"call[ed] for genocide and attacks against specific religious or ethnic groups\" and \"openly post[ed] photographs of burned-out cars, buildings, schools and houses\", according to \"Network Against Hate Speech\", an Ethiopian citizens' group. Berhan Taye of Access Now stated that in Ethiopia, offline violence quickly leads to online \"calls for ethnic attacks, discrimination, and destruction of property [that] goes viral\". He stated, \"Facebook's inaction helps propagate hate and polarization in a country and has a devastating impact on the narrative and extent of the violence.\"\nEthnic conflict resolution.\nInstitutional ethnic conflict resolution.\nA number of scholars have attempted to synthesize the methods available for the resolution, management or transformation of their ethnic conflict. John Coakley, for example, has developed a typology of the methods of conflict resolution that have been employed by states, which he lists as: indigenization, accommodation, assimilation, acculturation, population transfer, boundary alteration, genocide and ethnic suicide. John McGarry and Brendan O'Leary have developed a taxonomy of eight macro-political ethnic conflict regulation methods, which they note are often employed by states in combination with each other. They include a number of methods that they note are clearly morally unacceptable.\nWith increasing interest in the field of ethnic conflict, many policy analysts and political scientists theorized potential resolutions and tracked the results of institutional policy implementation. As such, theories often focus on which Institutions are the most appropriate for addressing ethnic conflict.\nConsociationalism.\nConsociationalism is a power sharing agreement which coopts the leaders of ethnic groups into the central state's government. Each nation or ethnic group is represented in the government through a supposed spokesman for the group. In the power sharing agreement, each group has veto powers to varying degrees, dependent on the particular state. Moreover, the norm of proportional representation is dominant: each group is represented in the government in a percentage that reflects the ethnicity's demographic presence in the state. Another requirement for Arend Lijphart is that the government must be composed of a \"grand coalition\" of the ethnic group leaders which supposes a top-down approach to conflict resolution.\nIn theory, this leads to self governance and protection for the ethnic group. Many scholars maintain that since ethnic tension erupts into ethnic violence when the ethnic group is threatened by a state, then veto powers should allow the ethnic group to avoid legislative threats. Switzerland is often characterized as a successful consociationalist state.\nA recent example of a consociational government is the post-conflict Bosnian government that was agreed upon in the Dayton Accords in 1995. A tripartite presidency was chosen and must have a Croat, a Serb, and a Bosniak. The presidents take turns acting as the forefront executive in terms of 8 months for 4 years. Many have credited this compromise of a consociational government in Bosnia for the end of the violence and the following long-lasting peace.\nIn contrast to Lijphart, several political scientists and policy analysts have condemned consociationalism. One of the many critiques is that consociationalism locks in ethnic tensions and identities. This assumes a primordial stance that ethnic identities are permanent and not subject to change. Furthermore, this does not allow for any \"others\" that might want to partake in the political process. As of 2012 a Jewish Bosnian is suing the Bosnian government for precluding him from running for presidential office since only a Croat, Serb, or Bosniak can run under the consociational government. Determining ethnic identities in advance and implementing a power sharing system on the basis of these fixed identities is inherently discriminatory against minority groups that might be not be recognized. Moreover, it discriminates against those who do not choose to define their identity on an ethnic or communal basis. In power sharing-systems that are based on pre-determined identities, there is a tendency to rigidly fix shares of representation on a permanent basis which will not reflect changing demographics over time. The categorization of individuals in particular ethnic groups might be controversial anyway and might in fact fuel ethnic tensions.\nThe inherent weaknesses in using pre-determined ethnic identities to form power sharing systems has led Ljiphart to argue that adopting a constructivist approach to consociationalism can increase its likelihood of success. The self-determination of ethnic identities is more likely to be \"non-discriminatory, neutral, flexible and self-adjusting\". For example, in South Africa, the toxic legacy of apartheid meant that successful consociation could only be built on the basis of the self-determination of groups. Ljiphart claims that because ethnic identities are often \"unclear, fluid and flexible,\" self-determination is likely to be more successful than pre-determination of ethnic groups. A constructivist approach to consociational theory can therefore strengthen its value as a method to resolve ethnic conflict.\nAnother critique points to the privileging of ethnic identity over personal political choice. Howard has deemed consociationalism as a form of ethnocracy and not a path to true pluralistic democracy. Consociationalism assumes that a politician will best represent the will of his co-ethnics above other political parties. This might lead to the polarization of ethnic groups and the loss of non-ethnic ideological parties.\nHorowitz has argued that a single transferable vote system could prevent the ethnification of political parties because voters cast their ballots in order of preference. This means that a voter could cast some of his votes to parties other than his co-ethnic party. This in turn would compel political parties to broaden their manifestos to appeal to voters across the ethnic divide to hoover up second and third preference votes.\nFederalism.\nThe theory of implementing federalism in order to curtail ethnic conflict assumes that self-governance reduces \"demands for sovereignty\". Hechter argues that some goods such as language of education and bureaucracy must be provided as local goods, instead of statewide, in order to satisfy more people and ethnic groups. Some political scientists such as Stroschein contend that ethnofederalism, or federalism determined along ethnic lines, is \"asymmetric\" as opposed to the equal devolution of power found in non-ethnic federal states, such as the United States. In this sense, special privileges are granted to specific minority groups as concessions and incentives to end violence or mute conflict.\nThe Soviet Union divided its structure into ethnic federal states termed Union Republics. Each Union Republic was named after a titular ethnic group who inhabited the area as a way to Sovietize nationalist sentiments during the 1920s. Brubaker asserts that these titular republics were formed in order to absorb any potential elite led nationalist movements against the Soviet center by incentivizing elite loyalty through advancement in the Soviet political structure.\nThus, federalism provides some self-governance for local matters in order to satisfy some of the grievances which might cause ethnic conflict among the masses. Moreover, federalism brings in the elites and ethnic entrepreneurs into the central power structure; this prevents a resurgence of top-down ethnic conflict.\nNevertheless, after the dissolution of the Soviet Union many critiques of federalism as an institution to resolve ethnic conflict emerged. The devolution of power away from the central state can weaken ties to the central state. Moreover, the parallel institutions created to serve a particular nation or ethnic group might provide significant resources for secession from the central state. As most states are unwilling to give up an integral portion of their territory, secessionist movements may trigger violence.\nFurthermore, some competing elite political players may not be in power; they would remain unincorporated into the central system. These competing elites can gain access through federal structures and their resources to solidify their political power in the structure. According to V.P. Gagnon this was the case in the former Yugoslavia and its disintegration into its ethnic federal states. Ethnic entrepreneurs were able to take control of the institutionally allocated resources to wage war on other ethnic groups.\nNon-territorial autonomy.\nA recent theory of ethnic tension resolution is non-territorial autonomy or NTA. NTA has emerged in recent years as an alternative solution to ethnic tensions and grievances in places that are likely to breed conflict. For this reason, NTA has been promoted as a more practical and state building solution than consociationalism. NTA, alternatively known as non-cultural autonomy (NCA), is based on the difference of \"jus solis\" and \"jus sanguinis,\" the principles of territory versus that of personhood. It gives rights to ethnic groups to self-rule and govern matters potentially concerning but limited to: education, language, culture, internal affairs, religion, and the internally established institutions needed to promote and reproduce these facets. In contrast to federalism, the ethnic groups are not assigned a titular sub-state, but rather the ethnic groups are dispersed throughout the state unit. Their group rights and autonomy are not constrained to a particular territory within the state. This is done in order not to weaken the center state such as in the case of ethnofederalism.\nThe origin of NTA can be traced back to the Marxists works of Otto Bauer and Karl Renner. NTA was employed during the interwar period, and the League of Nations sought to add protection clauses for national minorities in new states. In the 1920s, Estonia granted some cultural autonomy to the German and Jewish populations in order to ease conflicts between the groups and the newly independent state.\nIn Europe, most notably in Belgium, NTA laws have been enacted and created parallel institutions and political parties in the same country. In Belgium, NTA has been integrated within the federal consociational system. Some scholars of ethnic conflict resolution claim that the practice of NTA will be employed dependent on the concentration and size of the ethnic group asking for group rights.\nOther scholars, such as Clarke, argue that the successful implementation of NTA rests on the acknowledgement in a state of \"universal\" principles: true rule of law, established human rights, stated guarantees to minorities and their members to use their own quotidien language, religion, and food practices, and a framework of anti-discrimination legislation in order to enforce these rights. Moreover, no individual can be forced to adhere, identify, or emphasize a particular identity (such as race, gender, sexuality, etc.) without their consent in order for NTA to function for its purpose.\nNonetheless, Clarke critiques the weaknesses of NTA in areas such as education, a balance between society wide norms and intracommunity values; policing, for criminal matters and public safety; and political representation, which limits the political choices of an individual if based solely on ethnicity. Furthermore, the challenge in evaluating the efficacy of NTA lies in the relatively few legal implementations of NTA.\nCultural rights.\nEmphasizing the limits of approaches that focus mainly on institutional answers to ethnic conflicts\u2014which are essentially driven by ethnocultural dynamics of which political and/or economic factors are but elements\u2014Gregory Paul Meyjes urges the use of intercultural communication and cultural-rights based negotiations as tools with which to effectively and sustainably address inter-ethnic strife. Meyjes argues that to fully grasp, preempt, and/or resolve such conflicts\u2014whether with or without the aid of territorial or non-territorial institutional mechanism(s) -- a cultural rights approach grounded in intercultural knowledge and skill is essential.\nEthnic conflict resolution outside formal institutions.\nInformal inter-ethnic engagement.\nInstitutionalist arguments for resolving ethnic conflict often focus on national-level institutions and do not account for regional and local variation in ethnic violence within a country. Despite similar levels of ethnic diversity in a country, some towns and cities have often found to be especially prone to ethnic violence. For example, Ashutosh Varshney, in his study of ethnic violence in India, argues that strong inter-ethnic engagement in villages often disincentivizes politicians from stoking ethnic violence for electoral gain. Informal interactions include joint participation in festivals, families from different communities eating together or allowing their children to play with one another. Every day engagement between ethnic groups at the village level can help to sustain the peace in the face of national level shocks like an ethnic riot in another part of the country. In times of ethnic tension, these communities can quell rumors, police neighborhoods and come together to resist any attempts by politicians to polarize the community. The stronger the inter-ethnic networks are, the harder it is for politicians to polarize the community even if it may be in their political interest to do so.\nFormal inter-ethnic associations.\nIn cities where the population tends to be much higher, informal interactions between ethnic groups might not be sufficient to prevent violence. This is because many more links are needed to connect everyone, and therefore it is much more difficult to form and strengthen inter-ethnic ties. In cities, formal inter-ethnic associations like trade unions, business associations and professional organizations are more effective in encouraging inter-ethnic interactions that could prevent ethnic violence in the future. These organizations force ethnic groups to come together based on shared economic interests that overcomes any pre-existing ethnic differences. For example, inter-ethnic business organizations serve to connect the business interests of different ethnic groups which would increase their desire to maintain ethnic harmony. Any ethnic tension or outbreak of violence will go against their economic interests and therefore, over time, the salience of ethnic identity diminishes.\nInteractions between ethnic groups in formal settings can also help countries torn apart by ethnic violence to recover and break down ethnic divisions. Paula Pickering, a political scientist, who studies peace-building efforts in Bosnia, finds that formal workplaces are often the site where inter-ethnic ties are formed. She claims that mixed workplaces lead to repeated inter-ethnic interaction where norms of professionalism compel everyone to cooperate and to treat each other with respect, making it easier for individuals belonging to the minority group to reach out and form relationships with everyone else. Nevertheless, Giuliano's research in Russia has shown that economic grievances, even in a mixed workplace, can be politicized on ethnic lines.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56102", "revid": "835170", "url": "https://en.wikipedia.org/wiki?curid=56102", "title": "Manneken Pis", "text": "Brass sculpture and fountain in Brussels, Belgium\n (; ) is a landmark bronze fountain sculpture in central Brussels, Belgium, depicting a puer mingens: a naked little boy urinating into the fountain's basin. Though its existence is attested as early as the mid-15th century, \"Manneken Pis\" was redesigned by the Brabantine sculptor and put in place in 1619. Its blue stone niche in rocaille style dates from 1770. The statue has been repeatedly stolen or damaged throughout its history. Since 1965, a replica has been displayed on site, with the original stored in the Brussels City Museum.\n\"Manneken Pis\" is one of the best-known symbols of Brussels and Belgium, inspiring several legends, as well as numerous imitations and similar statues, both nationally and abroad. The figure is regularly dressed up and its wardrobe consists of around one thousand different costumes. Since 2017, they have been exhibited in a dedicated museum called \"GardeRobe MannekenPis\", located on the same street. Owing to its self-derisive nature, \"Manneken Pis\" is also an example of \"belgitude\" (French; lit.\u2009'Belgianness'), as well as of folk humour (zwanze) popular in Brussels.\n\"Manneken Pis\" is approximately five minutes' walk from the Grand-Place/Grote Markt (Brussels' main square), at the junction of the / and the pedestrian /. This site is served by the \"premetro\" (underground tram) station Bourse - Grand-Place/Beurs - Grote Markt (on lines 4 and 10), as well as the bus stop / (on lines 33, 48 and 95).\nNaming.\nThe statue's original name was \"Menneke(n) Pis\" or \"Menneke(n) Pist\". In fact, in the Brabantian dialect of Brussels (known as Brusselian, and also sometimes referred to as Marols or Marollien), \"een manneke\" means a small man, whereas \"een menneke\" means a little boy (it is the diminutive of \"men\", meaning boy), though in modern Flemish (the local variant of Dutch), \"menneke\" also means a small man (it is synonymous to \"mannetje\"). Nowadays, the name \"Manneken Pis\" (Dutch, ), usually translated as \"Little Pissing Man\" or \"Little Peeing Man\" in English, is official in both French and Dutch.\n\"Manneken Pis\" is sometimes given the nickname of in French or in Dutch (both meaning \"Little Julien\"), which in fact refers to a now-disappeared fountain of the \"Little Julien\" (\"Juliaenkensborre\"). This stems from a confusion by the 19th-century historians Alexandre Henne and Alphonse Wauters, who mistook the two distinct fountains because of their proximity. Due to its long history, the statue is also sometimes dubbed in French or in Dutch (\"the oldest bourgeois of Brussels\").\nHistory.\nOrigins of \"Manneken Pis\".\nThe earliest mention of the existence of \"Manneken Pis\" dates from the mid-15th century, and can be found in an administrative document from 1451\u201352 about the water lines supplying the fountains of Brussels. From the beginning, the fountain played an essential role in the distribution of drinking water. It stood on a column and poured water into a double rectangular basin of stone. The only representations of this first statue can be found, very schematically, on a map by the cartographers Georg Braun and Frans Hogenberg, in which the fountain appeared to be installed directly on the street and not on a corner as it is today. \"Manneken Pis\" is depicted again in a painting from 1616 by the court painters Denis Van Alsloot and Antoon Sallaert representing Brussels' Ommegang of 1615, as well as in a preparatory drawing to this painting, in which it is dressed as a shepherd.\nThe first statue was replaced with a new bronze version, commissioned in 1619 by the Brussels City Council. This bronze statue, on the corner of the / and the /, was conceived by the Brabantine sculptor , father of the architect and sculptor J\u00e9r\u00f4me Duquesnoy the Younger and the famous sculptor Fran\u00e7ois Duquesnoy. It was probably cast and installed in 1620. During that time, the column supporting the statue and the double rectangular basin collecting water were completely remodelled by the stone cutter Daniel Raessens.\n17th\u201319th centuries.\nDuring its history, \"Manneken Pis\" faced many hazards. It survived undamaged the bombardment of Brussels of 1695 by the French army, but the pipes having been affected, it could not deliver its water for some time. A pamphlet published the same year recounts this episode. This text is the oldest attesting that \"Manneken Pis\" had become \"an object of glory appreciated by all and renowned throughout the world\". It is also the first time that it served as a symbol for the people of Brussels. It is also traditionally said that after the bombardment, it was triumphantly placed again on its pedestal. On that occasion, the following passage from the Bible was inscribed above its head: (\"The Lord placed me on a stone base, and now I raise my head above my enemies\").\nAs shown by an engraving by Jacobus Harrewijn, dating from 1697, the fountain was no longer located on the street, but in a recess at the corner of the / and the / and was protected by a gate. In 1770, the column and the double rectangular basin disappeared; the statue was integrated into a new decor, in the form of a blue stone niche in rocaille style, originating from another dismantled fountain of Brussels. The water simply flowed through a grating in the ground, which was replaced with a basin in the 19th century. In its new setting, \"Manneken Pis\" gives the impression of being smaller than in its original layout.\nThe whole structure is protected by wrought iron railings, the last version of which dates from 1851. The latter prevented access to water, relegating the fountain to a decorative and symbolic role. It is also the case, around the same time, of the other fountains in Brussels. This correlates with efforts by the City of Brussels, starting in 1855, to allow for the distribution of drinking water in homes.\nThe figure has repeatedly been the object of theft or attempted theft. Legend has it that the statue was removed in 1745 by English soldiers and found in the Flemish town of Geraardsbergen (). As a sign of their appreciation, the people of Brussels gave this city a replica of the statue. A second attempted theft was allegedly made in 1747 by a group of French grenadiers stationed in Brussels. The population rebelled against this deed and threatened a bloody revenge. To calm things down, the King of France, Louis XV, offered a gentleman's gown of brocade, embroidered with gold, to \"Manneken Pis\". He also authorised the statue to carry the sword, and decorated it with the Cross of St. Louis.\nThe statue was stolen in 1817 by the freed convict Antoine Licas or Lycas. The perpetrator was heavily punished; he was condemned to forced labour for life, and was first tied for an hour to stocks on the Grand-Place/Grote Markt. The original statue was broken into eleven pieces during this abduction and was restored by a specialised welder, under the supervision of sculptor Gilles-Lambert Godecharle. The pieces were matched and used to make a mould in which the bronze statue was poured. The statue was then screwed onto a new copper base marked \"1620 \u2013 REST 1817\".\n20th century\u2013present.\n\"Manneken Pis\" experienced similar misadventures in the 20th century. Two attempted thefts occurred in 1955 and 1957. Some accounts say that it has been stolen up to seven times. Notably, in January 1963, students of the Antwerp student's association \"De Wikings\" of the Sint-Ignatius Handelshogeschool (Higher Business Education), now part of the University of Antwerp, \"hijacked\" the statue for five days before handing it over to the Antwerp authorities. The local and international press covered the story, contributing to the students' collection of funds donated to two orphanages. The case did go further, however, and the base was replaced identically by the \"Compagnie des Bronzes de Bruxelles\", to which the statue was anchored by a reinforced bronze attachment.\nThings were more serious when it disappeared in 1965; the statue had been broken by the thief and only the feet and ankles remained. In June 1966, the Antwerp magazine \"De Post\" received an anonymous phone call, signalling that the \"body\" was in the Charleroi Canal. It was found there by divers, sent by the magazine, and was brought back to Brussels on 27 June. Restored once again, the statue was sheltered and the original version is now kept and displayed on the second floor of the Brussels City Museum, at the King's House, on the Grand-Place. In the meantime, a replica of the statue had already been commissioned by Brussels' authorities and cast by the \"Compagnie des Bronzes\". The new statue was thus installed in place of the old one and this version still adorns the niche on the Rue du Ch\u00eane to this day.\nIn late 2018, city technician R\u00e9gis Callens discovered that the basin of the statue had developed a leak, leading to a reported of water being used per day. The leak occurred for an unknown number of years, unnoticed among the several hundred water features in the City of Brussels and was only later discovered with the help of Shayp water monitoring technology. The statue received a temporary fix in March 2019, with a permanent recirculating system set to be installed. The solution was announced during Brussels Water Week where city officials cited the situation as motivation to check for similar problems in other fountains.\nLegends.\nThere are several legends behind \"Manneken Pis\", but the most famous is one involving Duke Godfrey III of Leuven. In 1142, the troops of this two-year-old lord were battling against the troops of the Berthouts, the lords of Grimbergen, in Ransbeek (now Neder-Over-Heembeek, a northern part of the City of Brussels). To give themselves courage, the soldiers placed the infant lord in a basket which they hung from a large oak tree overlooking the battlefield. While his men were in dire straits, the little duke rose up in the basket, and from his perch, urinated onto the troops of the Berthouts, who eventually lost the battle. The fountain perpetuates the memory of this victory. The name of the / (\"Oak Tree Street\"), at the corner of which the statue is located, recalls the famous tree.\nTraditions.\nCostumes and wardrobe.\n\"Manneken Pis\" is dressed in costumes, several times each week, according to a published schedule, which is posted on the railings around the fountain. Since 1954, the costumes are managed by the non-profit organisation \"The Order of the Friends of Manneken Pis\", who review hundreds of designs submitted each year, and select a small number to be produced and used. His wardrobe consists of around one thousand different costumes, many of which could previously be viewed in a permanent exhibition inside the Brussels City Museum, located on the Grand-Place, immediately opposite the Town Hall. In February 2017, a specially designed museum, called \"GardeRobe MannekenPis\", opened its doors at 19, /.\nAlthough the proliferation of costumes is of 20th-century origin, their occasional use dates back almost to the date of casting. The oldest evidence of the tradition of dressing \"Manneken Pis\" dates from 1615; during the Ommegang of Brussels organised that year in honour of Archduchess Isabella, sovereign of the Spanish Netherlands, \"Manneken Pis\" was dressed in a shepherd's costume. He received his first costume on 1 May 1698 from the Governor of the Austrian Netherlands, Maximilian II Emanuel of Bavaria, during the festivities of one of the Guilds of Brussels. The oldest costume on display in the City Museum, the gentleman's gown offered by King Louis XV, is of 18th-century origin. In 1756, an inventory indicates that \"Manneken Pis\" had five complete costumes. From 1918 to 1940, he was offered some thirty costumes. But it was especially after 1945 that the movement took on an exceptional dimension; he had more than 400 costumes in 1994, more than 750 in 2005, and more than 950 in 2016. In 2018, \"Manneken Pis\" received his 1000th costume, created by fashion designer Jean-Paul Lespagnard.\nThe costume change on the figure is a colourful ceremony, often accompanied by brass band music. Many costumes represent the national dress of nations whose citizens come to Brussels as tourists; others are the uniforms of assorted trades, professions, associations, and branches of the civil and military services. As well as historical clothing, the wardrobe also contains modern costumes, such as ones of Dracula, Mickey Mouse and Santa Claus. In the past, the costume was cut without a cutting pattern. The sleeves were padded with cotton wool and ended with gloves. It is only since 1945 that a pattern allowed the making of more fitted costumes.\nFolklore.\nThe \"Order of the Friends of Manneken Pis\" was founded in 1954 and has more than 150 members. The objective of the Order is to stimulate the cultural, tourist, philanthropic and commercial development of Belgium in general, and more particularly to preserve the traditions linked to \"Manneken Pis\". The Order is always present during the ceremonies surrounding the presentation of new costumes and during the statue's official greetings and anniversaries.\nOn certain folkloric occasions (e.g. Saint-Verhaegen, Meyboom plantation), \"Manneken Pis\" is hooked up to a keg of beer. Cups are filled up with the beer flowing from the statue and given out to passers-by.\nUse in campaigns.\nIn June 2025, \"Manneken Pis\" was temporarily turned off to mark World Continence Week as part of an initiative by the Belgian charity PlasPraat vzw and the Dutch charity Bekkenbodem4all to awareness about incontinence, and to break down stigmas around people discussing the issue with medical professionals.\nReplicas and similar statues.\nIn Belgium.\nAlthough Brussels' \"Manneken Pis\" is the best known, others exist all across the country. As early as the 17th century, the statue was the subject of decorative replicas. The Brussels City Museum exhibits a copy which was crafted by Jacques Van den Broeck in 1630, probably from a cast of Duquesnoy's statue. Another local copy, from 1636, in a private American collection, is attributed to the German founder Daniel Haneman. Similar statues can also be found in the Flemish cities of Koksijde, Hasselt, Ghent, and Bruges, as well as in the Walloon municipality of Braine-l'Alleud (where it is called \"El Gamin qui piche\", meaning \"The Peeing Kid\" in Walloon).\nFeud with Geraardsbergen.\nThere is an ongoing dispute over which \"Manneken Pis\" is the oldest; the one in Brussels or the one in Geraardsbergen. According to tradition, Geraardsbergen was in a rebuilding phase after the devastating passage of Jean II de Cro\u00ff's troops in the spring of 1452, during a war opposing the city of Ghent and the Burgundian duke Philip the Good. On that occasion, the spout from one of the city's fountains, in the shape of a copper lion's head, was taken by the people of Ghent. The aldermen of Geraardsbergen thus had their own \"Manneken Pis\" made in 1459 to replace it. The statue was cast in brass by Reinier Van Tienen, based on a model designed by Gillis Vander Jeught.\nIt can be assumed that the first version of Brussels' \"Manneken Pis\", dating from before 1451, served as inspiration to Geraardsbergen's statue. Looking at the ages of the statues, both of them are replicas; the one in Brussels dating from 1965 and the one in Geraardsbergen from 1985. The design of Geraarsbergen's original statue, however, which dates from 1459, antedates that of Duquesnoy's statue, from 1619. It is on this basis that Geraardsbergen asserts that its \"Manneken Pis\" is the oldest, but since there was probably already a \"Manneken Pis\" in 1452 in Brussels, the tradition might be slightly older there.\nInternationally.\nSince the 20th century, numerous copies or imitations of \"Manneken Pis\" have been created abroad. It is necessary to distinguish the official copies offered by the City of Brussels from copies and imitations carried out privately by admirers of the little statue. Official copies were offered to: Colmar, France (1922); Osaka, Japan (1928); Monaco (1951); London, United Kingdom (1959); Broxeele, France (1979); Benalmadena, Spain (1991); and Nagoya, Japan (2015).\nA replica of \"Manneken Pis\" has pride of place in the lobby of the police station of Poitiers, France, commemorating the fact that this city was, for 26 days, the seat of the Belgian government during World War II. Similar copies of the statue exist in the Bulgarian city of Stara Zagora, in the Danish town of Bogense, as well as in Chiayi Park in Taiwan. Another working replica stands on the platform of Tokyo's Hamamatsuch\u014d Station. There, the statue is a great source of pride for station workers who dress it in various costumes\u2014traditional and otherwise\u2014at different times of year.\nIn September 2002, a Belgian-born waffle-maker set up a replica in front of his waffle stand in the Orlando Fashion Square mall, in Orlando, Florida. He recalled the legend as \"the boy who saved Brussels from fire by extinguishing it with his urine\" (perhaps confusing the legend with an incident in \"Gulliver's Travels\"). Some shocked shoppers made a formal complaint. Mall officials said that the waffle-shop owner did not follow procedures when he put up the statue and was therefore in violation of his lease.\nIn contrast, there is a similar statue known as \"Manequinho\" in Rio de Janeiro, Brazil, made in 1908. It used to be in front of the National Library, but after complaints about immorality, it was moved to the headquarters of Botafogo de Futebol e Regatas, a famous Brazilian football club, where it has been adopted as a mascot by the club. Fans usually dress it with the club's jersey after important wins.\nStatues inspired by \"Manneken Pis\".\n\"Jeanneke Pis\".\n\"Manneken Pis\" is not the only peeing statue in Brussels. Since 1987, it has had a female equivalent, \"Jeanneke Pis\" (\"Little Pissing Joan\"), located on the eastern side of the / (\"Fidelity Alley\"), a narrow cul-de-sac some long leading northwards off the restaurant-packed /, in central Brussels. The bronze sculpture depicts a naked little girl with short pigtails, squatting and urinating on a blue-grey limestone base. It feeds a small fountain and is now protected from vandalism by iron bars. It is, however, less illustrious than its masculine counterpart.\n\"Het Zinneke\".\n\"Het Zinneke\", sometimes called \"Zinneke Pis\", another bronze sculpture in central Brussels, depicting a dog urinating against a bollard, can also be seen as a reference to \"Manneken Pis\". It is, however, not associated with a fountain. \"Zinneke\" is a nickname chosen to represent a person from Brussels who was not born there. The word means \"mutt\" or \"bastard\" in Brusselian dialect, and originally referred to the city's stray dogs that hung around the streets by the Lesser Senne (a tangent canal of the river Senne, which circumnavigated Brussels along the city walls) until the end of the 19th century. It is located at the junction of the / and the /, not far from the Halles Saint-G\u00e9ry/Sint-Gorikshallen.\nIn popular culture.\nBeing of prominent symbolic nature to Brussels and Belgium in general, \"Manneken Pis\" is widely used to represent both the city and country (as well as its people) in advertising, branding, tourism and as a national personification. The statue's self-derisive nature also embodies the typical Belgian identity referred to as \"belgitude\" (French; lit.\u2009'Belgianness'), as well as a type of folk humour specific to Brussels (called \"zwanze\" in Brusselian dialect).\nSurrounded by souvenir shops, the fountain has become a major tourist attraction. Figurine-sized replicas of \"Manneken Pis\" in brass, fiberglass, or even Belgian chocolate, are commonly sold there. \"Manneken Pis\" has also been adapted into such \"risqu\u00e9\" souvenir items as ashtrays and corkscrews.\nIn 2001, the Irish low-cost airline Ryanair used an image of \"Manneken Pis\" in an advert attacking the Belgian national airline Sabena, with the slogan \"Pissed off with Sabena's high fares?\" Sabena sued Ryanair, claiming the comparison was misleading, and Ryanair was ordered to apologise. Their apology read \"We're Sooooo Sorry Sabena!\" and listed further price comparisons. Sabena went bankrupt later in 2001.\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "56103", "revid": "753665", "url": "https://en.wikipedia.org/wiki?curid=56103", "title": "Stevie Smith", "text": "English poet and novelist (1902\u20131971)\nFlorence Margaret Smith (20 September 1902 \u2013 7 March 1971), known as Stevie Smith, was an English poet and novelist. She won the Cholmondeley Award and was awarded the Queen's Gold Medal for Poetry. \"Stevie\", a play by Hugh Whitemore based on her life, was adapted into a film starring Glenda Jackson.\nLife.\nStevie Smith, born Florence Margaret Smith at number 34 De La Pole Avenue in Kingston upon Hull, was the second daughter of Charles Ward Smith (1872\u20131949) and Ethel Rahel (1876\u20131919), daughter of a maritime engineer, John Spear. She was called \"Peggy\" within her family, but acquired the nickname \"Stevie\" as a young woman when she was riding in the park with a friend who said that she reminded him of the jockey Steve Donoghue.\nHer father was a shipping agent, running a business that he had inherited from his father. As the company and his marriage both began to fall apart, he ran away to sea and Smith saw very little of him after that. He appeared occasionally on 24-hour shore leave and sent very brief postcards, one of which read, \"Off to Valparaiso, Love Daddy.\"\nWhen Stevie Smith was three years old she moved with her mother and sister to Palmers Green in North London, where she lived until her death in 1971. She resented the fact that her father had abandoned his family. Later, when her mother became ill, her aunt Madge Spear, whom Smith called \"The Lion Aunt\", came to live with them, raised Smith and her elder sister Molly, and became the most important person in Smith's life. Spear was a feminist who claimed to have \"no patience\" with men and, as Smith wrote, \"she also had 'no patience' with Hitler\". Smith and Molly, raised in a family of women, became attached to their own independence, in contrast to what Smith described as the typical Victorian family atmosphere of \"Father knows best.\"\nWhen Smith was five she developed tuberculous peritonitis and was sent to a sanatorium near Broadstairs, Kent, where she remained for three years. She related that her preoccupation with death began when she was seven, at a time when she was very distressed at being sent away from her mother. Death and fear fascinated her, and provide the subjects of many of her poems. Her mother died when Smith was 16.\nWhen she was suffering from the depression to which she was sporadically subject throughout her life Smith was so consoled by the thought of death as a release that, as she put it, she did not have to commit suicide. She wrote in several poems that death was \"the only god who must come when he is called\". Smith also suffered throughout her life from an acute nervousness, described as a mix of shyness and intense sensitivity.\nIn the poem \"A House of Mercy\" she wrote of her childhood house in North London:\n&lt;poem&gt;\nIt was a house of female habitation,\nTwo ladies fair inhabited the house,\nAnd they were brave. For although Fear knocked loud\nUpon the door, and said he must come in,\nThey did not let him in.\n&lt;/poem&gt;\nSmith was educated at Palmers Green High School, the North London Collegiate School for Girls, and Mrs Hoster's Secretarial College. She spent the remainder of her life with her aunt, and worked as private secretary to Sir Neville Pearson at Newnes Publishing Company in London from 1923 to 1953. She corresponded and socialised widely with other writers and creative artists, including Elisabeth Lutyens, Sally Chilver, Inez Holden, Naomi Mitchison, Isobel English and Anna Kallin.\nAfter she retired from Sir Neville Pearson's service following a nervous breakdown, she gave poetry readings and broadcasts on the BBC that gained her new friends and readers among a younger generation. Sylvia Plath became a fan of her poems and sent Smith a letter in 1962, describing herself as \"a desperate Smith-addict\". Plath expressed interest in meeting in person, but took her own life soon after she sent the letter.\nSmith was described by her friends as being naive and selfish in some ways but formidably intelligent in others, having been raised by her aunt as both a spoiled child and a resolutely autonomous woman. Likewise, her political views vacillated between her aunt's Toryism and her friends' left-wing tendencies. Smith was celibate for most of her life, although she rejected the idea that she was lonely as a result, alleging that she had a number of intimate relationships with friends and family that kept her fulfilled. She never entirely abandoned or accepted the High Church Anglican faith of her childhood, describing herself as a \"lapsed atheist\", and wrote sensitively about theological puzzles;\"There is a God in whom I do not believe/Yet to this God my love stretches.\" Her 14-page essay of 1958, \"The Necessity of Not Believing\", concludes: \"There is no reason to be sad, as some people are sad when they feel religion slipping off from them. There is no reason to be sad, it is a good thing.\" The essay was unveiled at a meeting of the Cambridge Humanist Society.\nSmith died of a brain tumour on 7 March 1971. Her last collection, \"Scorpion and Other Poems\", was published posthumously in 1972, and the \"Collected Poems\" followed in 1975. Her three novels were republished and there was a successful play based on her life, \"Stevie\", written by Hugh Whitemore. It was filmed in 1978 by Robert Enders and starred Glenda Jackson and Mona Washbourne.\nFiction.\nSmith wrote three novels, the first of which, \"Novel on Yellow Paper\", was published in 1936. Apart from death, common subjects in her writing include loneliness; myth and legend; absurd vignettes, usually drawn from middle-class British life; war; human cruelty; and religion. All her novels are lightly fictionalised accounts of her own life, which got her into trouble at times as people recognised themselves. Smith said that two of the male characters in her last book are different aspects of George Orwell, who was close to Smith. There were rumours that they were lovers; he was married to his first wife at the time.\n\"Novel on Yellow Paper\" (Cape, 1936).\nSmith's first novel is structured as the random typings of a bored secretary, Pompey. She plays word games, retells stories from classical and popular culture, remembers events from her childhood, gossips about her friends and describes her family, particularly her beloved Aunt. As with all Smith's novels, there is an early scene where the heroine expresses feelings and beliefs for which she will later feel significant, although ambiguous, regret. In \"Novel on Yellow Paper\" that belief is anti-Semitism, where she feels elation at being the \"only Goy\" at a Jewish party. This apparently throwaway scene acts as a timebomb, which detonates at the centre of the novel when Pompey visits Germany as the Nazis are gaining power. With horror, she acknowledges the continuity between her feeling \"Hurray for being a Goy\" at the party and the madness that is overtaking Germany. The German scenes stand out in the novel, but perhaps equally powerful is her dissection of failed love. She describes two unsuccessful relationships, first with the German Karl and then with the suburban Freddy. The final section of the novel describes with unusual clarity the intense pain of her break-up with Freddy.\n\"Over the Frontier\" (Cape, 1938).\nSmith herself dismissed her second novel as a failed experiment, but its attempt to parody popular genre fiction to explore profound political issues now seems to anticipate post-modern fiction. If anti-Semitism was one of the key themes of \"Novel on Yellow Paper\", \"Over the Frontier\" is concerned with militarism. In particular, she asks how the necessity of fighting Fascism can be achieved without descending into the nationalism and dehumanisation that fascism represents. After a failed romance the heroine, Pompey, suffers a breakdown and is sent to Germany to recuperate. At this point the novel changes style radically, as Pompey becomes part of an adventure/spy yarn in the style of John Buchan or Dornford Yates. As the novel becomes increasingly dreamlike, Pompey crosses over the frontier to become a spy and soldier. If her initial motives are idealistic, she becomes seduced by the intrigue and, ultimately, violence. The vision Smith offers is a bleak one: \"Power and cruelty are the strengths of our lives, and only in their weakness is there love.\"\n\"The Holiday\" (Chapman and Hall, 1949).\nSmith's final novel was her own favourite, and is her most fully realised. It is concerned with personal and political malaise in the immediate post-war period. Most of the characters are employed in the army or the civil service in post-war reconstruction, and its heroine, Celia, works for the Ministry as a cryptographer and propagandist. \"The Holiday\" describes a series of hopeless relationships. Celia and her cousin Caz are in love, but cannot pursue their affair since it is believed that, because of their parents' adultery, they are half-brother and half-sister. Celia's other cousin Tom is in love with her, Basil is in love with Tom, Tom is estranged from his father, Celia's beloved Uncle Heber, who pines for a reconciliation; and Celia's best friend Tiny longs for the married Vera. These unhappy, futureless but intractable relationships are mirrored by the novel's political concerns. The unsustainability of the British Empire and the uncertainty over Britain's post-war role are constant themes, and many of the characters discuss their personal and political concerns as if they were seamlessly linked. Caz is on leave from Palestine and is deeply disillusioned, Tom goes mad during the war, and it is telling that the family scandal that blights Celia and Caz's lives took place in India. Just as Pompey's anti-Semitism is tested in \"Novel on Yellow Paper\", so Celia's traditional nationalism and sentimental support for colonialism are challenged throughout \"The Holiday\".\nPoetry.\nSmith's first volume of poetry, the self-illustrated \"A Good Time Was Had By All\", was published in 1937 and established her as a poet. Soon her poems were found in periodicals. Her style was often very dark; her characters were perpetually saying \"goodbye\" to their friends or welcoming death. At the same time her work has an eerie levity and can be very funny though it is neither light nor whimsical. \"Stevie Smith often uses the word 'peculiar' and it is the best word to describe her effects\" (Hermione Lee). She was never sentimental, undercutting any pathetic effects with the ruthless honesty of her humour.\n\"A good time was had by all\" \u2013 the title of Smith's first collection \u2013 itself became a catch phrase, still occasionally used to this day. Smith said she got the phrase from parish magazines, where descriptions of church picnics often included this phrase. This saying has become so familiar that it is recognised even by those who are unaware of its origin. Variations appear in pop culture, including \"Being for the Benefit of Mr. Kite!\" by the Beatles.\nThough her poems were remarkably consistent in tone and quality throughout her life, their subject matter changed over time, with less of the outrageous wit of her youth and more reflection on suffering, faith and the end of life. Her best-known poem is \"Not Waving but Drowning\". She was awarded the Cholmondeley Award for Poets in 1966 and won the Queen's Gold Medal for poetry in 1969. She published nine volumes of poems in her lifetime (three more were released posthumously).\nAs an occasional work, Smith wrote the text of the coffee-table book \"Cats in Colour\" (1959), for which she wrote a humorous series of captions to photographs imagining the inner lives of cats.\nSmith's poems have been the focus of writers and critics around the world. James Antoniou writes in \"The Australian\" that her 'apparent innocence masks such fierce complexities, such ambition and startling originality, that many people baulk at her work', while Michael Dirda affirms in \"The Washington Post\" that, 'certainly, an outward charm is part of Smith's aesthetic strategy, though there\u2019s nothing naive or whimsical beneath her surface.' Carol Rumens writes in \"The Guardian\" that Smith 'skewered formality, though formally deft, and challenged, with a Victorian school marm's brisk tartness, the lingering shades of late-Victorian social hypocrisy.'\nIn 2023, newly declassified UK government files revealed that Smith was considered as a candidate to be the new Poet Laureate of the United Kingdom in 1967 following the death of John Masefield. She was rejected after appointments secretary John Hewitt consulted with Dame Helen Gardner, the Merton Professor of English at the University of Oxford (who stated that Smith \"wrote 'little girl poetry' about herself mostly\") and Geoffrey Handley-Taylor, chair of The Poetry Society (who stated that Smith was \"unstable\").\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56104", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=56104", "title": "Nature of God", "text": ""}
{"id": "56106", "revid": "1728614", "url": "https://en.wikipedia.org/wiki?curid=56106", "title": "Wildfire", "text": "Uncontrolled fires in forests or open spaces\nA wildfire, forest fire, or a bushfire is an unplanned and uncontrolled fire in an area of combustible vegetation. Some natural forest ecosystems depend on wildfire. Modern forest management often engages in prescribed burns to mitigate fire risk and promote natural forest cycles. However, controlled burns can turn into wildfires by mistake.\nWildfires can be classified by cause of ignition, physical properties, combustible material present, and the effect of weather on the fire. Wildfire severity results from a combination of factors such as available fuels, physical setting, and weather. Climatic cycles with wet periods that create substantial fuels, followed by drought and heat, often precede severe wildfires. These cycles have been intensified by climate change, and can be exacerbated by curtailment of mitigation measures (such as budget or equipment funding), or sheer enormity of the event.\nWildfires are a common type of disaster in some regions, including Siberia (Russia); California, Washington, Oregon, Texas, Florida (United States); British Columbia (Canada); and Australia. Areas with Mediterranean climates or in the taiga biome are particularly susceptible. Wildfires can severely impact humans and their settlements. Effects include for example the direct health impacts of smoke and fire, as well as destruction of property (especially in wildland\u2013urban interfaces), and economic losses. There is also the potential for contamination of water and soil.\nAt a global level, human practices have made the impacts of wildfire worse, with a doubling in land area burned by wildfires compared to natural levels. Humans have impacted wildfire through climate change (e.g. more intense heat waves and droughts), land-use change, and wildfire suppression. The carbon released from wildfires can add to carbon dioxide concentrations in the atmosphere and thus contribute to the greenhouse effect. This creates a climate change feedback.\nNaturally occurring wildfires can have beneficial effects on those ecosystems that have evolved with fire. In fact, many plant species depend on the effects of fire for growth and reproduction.\nIgnition.\nThe ignition of a fire takes place through either natural causes or human activity (deliberate or not).\nNatural causes.\nNatural occurrences that can ignite wildfires without the involvement of humans include lightning, volcanic eruptions, sparks from rock falls, and spontaneous combustions.\nHuman activity.\nSources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming. In the tropics, farmers often practice the slash-and-burn method of clearing fields during the dry season.\nIn middle latitudes, the most common human causes of wildfires are equipment generating sparks (chainsaws, grinders, mowers, etc.), overhead power lines, and arson.\nArson may account for over 20% of human caused fires, although human activities, including campfires, power line failures, and equipment use, are responsible for approximately 85% of wildfires. The combination of these ignition sources with dry conditions leads to more frequent and severe fires. However, in the 2019\u201320 Australian bushfire season \"an independent study found online bots and trolls exaggerating the role of arson in the fires.\" In the 2023 Canadian wildfires false claims of arson gained traction on social media; however, arson is generally not the main cause of wildfires in Canada. In California, generally 6\u201310% of wildfires annually are arson.\nCoal seam fires burn in the thousands around the world, such as those in Burning Mountain, New South Wales; Centralia, Pennsylvania; and several coal-sustained fires in China. They can also flare up unexpectedly and ignite nearby flammable material.\nSpread.\nThe spread of wildfires varies based on the flammable material present, its vertical arrangement and moisture content, and weather conditions. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows:\nPhysical properties.\nWildfires occur when all the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation that is subjected to enough heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are needed to evaporate any water in the material and heat the material to its fire point.\nDense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, often as a consequence of droughts, plants dry out and are therefore more flammable.\nA wildfire \"front\" is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of . Next, the pyrolysis of wood at releases flammable gases. Finally, wood can smolder at or, when heated sufficiently, ignite at . Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to , which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or \"torching\": the drying of tree canopies and their subsequent ignition from below.\n Wildfires have a rapid \"forward rate of spread\" (FROS) when burning through dense uninterrupted fuels. They can move as fast as in forests and in grasslands. Wildfires can advance tangentially to the main front to form a \"flanking\" front, or burn in the opposite direction of the main front by \"backing\". They may also spread by \"jumping\" or \"spotting\" as winds and vertical convection columns carry \"firebrands\" (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels around a wildfire are especially vulnerable to ignition from firebrands. Spotting can create spot fires as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as from the fire front.\nEspecially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than . Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.\nIntensity variations during day and night.\nIntensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00\u00a0p.m. and 2:00\u00a0p.m. Wildfire suppression operations in the United States revolve around a 24-hour \"fire day\" that begins at 10:00\u00a0a.m. due to the predictable increase in intensity resulting from the daytime warmth.\nClimate change effects.\nIncreasing risks due to climate change.\nClimate change promotes the type of weather that makes wildfires more likely. In some areas, an increase of wildfires has been attributed directly to climate change. Evidence from Earth's past also shows more fire in warmer periods. Climate change increases potential evapotranspiration. This can cause vegetation and soils to dry out when potential evaporation exceeds precipitation and available moisture from the given ecosystem. The vapor pressure deficit also contributes to increasing wildfire risk and has been worsening in the warming climate. When a fire starts in an area with very dry vegetation, it can spread rapidly. Higher temperatures can also lengthen the fire season. This is the time of year in which severe wildfires are most likely, particularly in regions where snow is disappearing.\nWeather conditions are raising the risks of wildfires. But the total area burnt by wildfires has decreased. This is mostly because savanna has been converted to cropland, so there are fewer trees to burn.\nClimate variability including heat waves, droughts, and El Ni\u00f1o, and regional weather patterns, such as high-pressure ridges, can increase the risk and alter the behavior of wildfires dramatically. Years of high precipitation can produce rapid vegetation growth, which when followed by warmer periods can encourage more widespread fires and longer fire seasons. High temperatures dry out the fuel loads and make them more flammable, increasing tree mortality and posing significant risks to global forest health. Since the mid-1980s, in the Western US, earlier snowmelt and associated warming have also been associated with an increase in length and severity of the wildfire season, or the most fire-prone time of the year. A 2019 study indicates that the increase in fire risk in California may be partially attributable to human-induced climate change.\nIn the summer of 1974\u20131975 (southern hemisphere), Australia suffered its worst recorded wildfire, when 15% of Australia's land mass suffered \"extensive fire damage\". Fires that summer burned up an estimated . In Australia, the annual number of hot days (above ) and very hot days (above ) has increased significantly in many areas of the country since 1950. The country has always had bushfires but in 2019, the extent and ferocity of these fires increased dramatically. For the first time catastrophic bushfire conditions were declared for Greater Sydney. New South Wales and Queensland declared a state of emergency but fires were also burning in South Australia and Western Australia.\nIn 2019, extreme heat and dryness caused massive wildfires in Siberia, Alaska, Canary Islands, Australia, and in the Amazon rainforest. The fires in the latter were caused mainly by illegal logging. The smoke from the fires expanded over a huge territory including major cities, dramatically reducing air quality.\nAs of August 2020, the wildfires in that year were 13% worse than in 2019 due primarily to climate change, deforestation and agricultural burning. The Amazon rainforest's existence is threatened by fires. Record-breaking wildfires in 2021 occurred in Turkey, Greece and Russia, thought to be linked to climate change.\nCarbon dioxide and other emissions from fires.\nThe carbon released from wildfires can add to greenhouse gas concentrations. Climate models do not yet fully reflect this feedback.\nWildfires release large amounts of carbon dioxide, black and brown carbon particles, and ozone precursors such as volatile organic compounds and nitrogen oxides (NOx) into the atmosphere. These emissions affect radiation, clouds, and climate on regional and even global scales. Wildfires also emit substantial amounts of semi-volatile organic species that can partition from the gas phase to form secondary organic aerosol (SOA) over hours to days after emission. In addition, the formation of the other pollutants as the air is transported can lead to harmful exposures for populations in regions far away from the wildfires. While direct emissions of harmful pollutants can affect first responders and residents, wildfire smoke can also be transported over long distances and impact air quality across local, regional, and global scales.The health effects of wildfire smoke, such as worsening cardiovascular and respiratory conditions, extend beyond immediate exposure, contributing to nearly 16,000 annual deaths, a number expected to rise to 30,000 by 2050. The economic impact is also significant, with projected costs reaching $240 billion annually by 2050, surpassing other climate-related damages.\nOver the past century, wildfires have accounted for 20\u201325% of global carbon emissions, the remainder from human activities. Global carbon emissions from wildfires through August 2020 equaled the average annual emissions of the European Union. In 2020, the carbon released by California's wildfires was significantly larger than the state's other carbon emissions.\nForest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57\u00a0gigatonnes (0.89 and 2.83\u00a0billion short tons) of CO2 into the atmosphere, which is between 13\u201340% of the annual global carbon dioxide emissions from burning fossil fuels.\nIn June and July 2019, fires in the Arctic emitted more than 140 megatons of carbon dioxide, according to an analysis by CAMS. To put that into perspective this amounts to the same amount of carbon emitted by 36 million cars in a year. The recent wildfires and their massive CO2 emissions mean that it will be important to take them into consideration when implementing measures for reaching greenhouse gas reduction targets accorded with the Paris climate agreement. Due to the complex oxidative chemistry occurring during the transport of wildfire smoke in the atmosphere, the toxicity of emissions was indicated to increase over time.\nAtmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%. The Amazon is estimated to hold around 90 billion tons of carbon. As of 2019, the earth's atmosphere has 415 parts per million of carbon, and the destruction of the Amazon would add about 38 parts per million.\nSome research has shown wildfire smoke can have a cooling effect.\nResearch in 2007 stated that black carbon in snow changed temperature three times more than atmospheric carbon dioxide. As much as 94 percent of Arctic warming may be caused by dark carbon on snow that initiates melting. The dark carbon comes from fossil fuels burning, wood and other biofuels, and forest fires. Melting can occur even at low concentrations of dark carbon (below five parts per billion).\nPrevention and mitigation.\nWildfire prevention refers to the preemptive methods aimed at reducing the risk of fires as well as lessening its severity and spread. Prevention techniques aim to manage air quality, maintain ecological balances, protect resources, and to affect future fires. Prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement.\nWildfire prevention programs around the world may employ techniques such as \"wildland fire use\" (WFU) and \"prescribed or controlled burns\". \"Wildland fire use\" refers to any fire of natural causes that is monitored but allowed to burn. \"Controlled burns\" are fires ignited by government agencies under less dangerous weather conditions. Other objectives can include maintenance of healthy forests, rangelands, and wetlands, and support of ecosystem diversity.\nStrategies for wildfire prevention, detection, control and suppression have varied over the years. One common and inexpensive technique to reduce the risk of uncontrolled wildfires is controlled burning: intentionally igniting smaller less-intense fires to minimize the amount of flammable material available for a potential wildfire. Vegetation may be burned periodically to limit the accumulation of plants and other debris that may serve as fuel, while also maintaining high species diversity. While other people claim that controlled burns and a policy of allowing some wildfires to burn is the cheapest method and an ecologically appropriate policy for many forests, they tend not to take into account the economic value of resources that are consumed by the fire, especially merchantable timber. Some studies conclude that while fuels may also be removed by logging, such thinning treatments may not be effective at reducing fire severity under extreme weather conditions.\nBuilding codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Continued residential development in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life.\nGoat grazing programs.\nAs climate change drives more frequent and more intense wildfires, more effort is being given to mitigation of fire potential by active measures such as managing fire fuels (ground cover, weeds, small shrubs, coyote brush, etc). In Northern California, for example, goat herds have been used in many communities to reduce the amount of fire fuels on the outskirts of some communities. It is estimated that 60 to 80,000 goats were thus employed by 2024.\nDetection.\nThe demand for timely, high-quality fire information has increased in recent years. Fast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.\nPublic hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.\nLocal sensor networks.\nA small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or \"tree-rechargeable\": able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.\nThe Department of Natural Resources signed a contract with PanoAI for the installation of 360 degree 'rapid detection' cameras around the Pacific northwest, which are mounted on cell towers and are capable of continuous monitoring of a radius. Additionally, Sensaio Tech, based in Brazil and Toronto, has released a sensor device that continuously monitors 14 different variables common in forests, ranging from soil temperature to salinity. This information is connected live back to clients through dashboard visualizations, while mobile notifications are provided regarding dangerous levels.\nSatellite and aerial monitoring.\nSatellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than . The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from for MODIS and AVHRR data and up to for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution may also limit the effectiveness of satellite imagery. Global Forest Watch provides detailed daily updates on fire alerts.\nIn 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions.\nIn 2014, an international campaign was organized in South Africa's Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375\u00a0m fire product, put it to use during several large wildfires in Kruger.\nSince 2021 NASA has provided active fire locations in near real-time via the Fire Information for Resource Management System (FIRMS).\nThe increased prevalence of wildfires has led to proposals deploy technologies based on artificial intelligence for early detection, prevention, and prediction of wildfires.\nSuppression.\nWildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires under extreme weather conditions are difficult to suppress without a change in the weather. Wildfires in Canada and the US burn an average of per year.\nAbove all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, United States, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.\nCosts of wildfire suppression.\nThe suppression of wild fires takes up a large amount of a country's gross domestic product which directly affects the country's economy. While costs vary wildly from year to year, depending on the severity of each fire season, in the United States, local, state, federal and tribal agencies collectively spend tens of billions of dollars annually to suppress wildfires. In the United States, it was reported that approximately $6 billion was spent between 2004\u20132008 to suppress wildfires in the country. In California, the U.S. Forest Service spends about $200 million per year to suppress 98% of wildfires and up to $1 billion to suppress the other 2% of fires that escape initial attack and become large.\nWildland firefighting safety.\nWildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis. Between 2000 and 2016, more than 350\u00a0wildland firefighters died on-duty.\nEspecially in hot weather conditions, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.\nSmoke, ash, and debris can also pose serious respiratory hazards for wildland firefighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.\nFirefighters are also at risk of cardiac events including strokes and heart attacks. Firefighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland firefighters face include slips, trips, falls, burns, scrapes, and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.\nFire retardants.\nFire retardants are used to slow wildfires by inhibiting combustion. They are aqueous solutions of ammonium phosphates and ammonium sulfates, as well as thickening agents. The decision to apply retardant depends on the magnitude, location and intensity of the wildfire. In certain instances, fire retardant may also be applied as a precautionary fire defense measure.\nTypical fire retardants contain the same agents as fertilizers. Fire retardants may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant's effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.\nImpacts on the natural environment.\nOn the atmosphere.\nMost of Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about . The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot (black carbon), and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach over wildfires. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding . Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.\nWildfires can affect local atmospheric pollution, and release carbon in the form of carbon dioxide. Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Increased fire byproducts in the troposphere can increase ozone concentrations beyond safe levels.\nOn ecosystems.\nWildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin.\nSome ecosystems are adapted to low-severity fires, where trees can survive but underbrush is cleared. Human suppression of lightning-caused fires in areas like Canada and the United States has created a buildup of fuel compared to more frequent fires before the 20th century. This has resulted in fewer but higher-severity fires which can kill mature trees.\nHigh-severity wildfire creates complex early seral forest habitat (also called \"snag forest habitat\"), which often has higher species richness and diversity than unburned old forest. Plant and animal species in most types of North American forests evolved with fire, and many of these species depend on wildfires, and particularly high-severity fires, to reproduce and grow. Fire helps to return nutrients from plant matter back to the soil. The heat from fire is necessary to the germination of certain types of seeds, and the snags (dead trees) and early successional forests created by high-severity fire create habitat conditions that are beneficial to wildlife. Early successional forests created by high-severity fire support some of the highest levels of native biodiversity found in temperate conifer forests. Post-fire logging has no ecological benefits and many negative impacts; the same is often true for post-fire seeding. The exclusion of wildfires can contribute to vegetation regime shifts, such as woody plant encroachment.\nAlthough some ecosystems rely on naturally occurring fires to regulate growth, some ecosystems suffer from too much fire, such as the chaparral in southern California and lower-elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, damaged native plant communities, and encouraged the growth of non-native weeds. Invasive species, such as \"Lygodium microphyllum\" and \"Bromus tectorum\", can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further alters native vegetation communities.\nIn the Amazon rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO2. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by 2030. Wildfires generate ash, reduce the availability of organic nutrients, and cause an increase in water runoff, eroding other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors burned off of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating to 10,000\u00a0BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants. Burn severity can be measured by satellites to calculate the Normalized Burn Ratio.\nOn waterways.\nDebris and chemical runoff into waterways after wildfires can make drinking water sources unsafe. Though it is challenging to quantify the impacts of wildfires on surface water quality, research suggests that the concentration of many pollutants increases post-fire. The impacts occur during active burning and up to years later. Increases in nutrients and total suspended sediments can happen within a year while heavy metal concentrations may peak 1\u20132 years after a wildfire.\nBenzene is one of many chemicals that have been found in drinking water systems after wildfires. Benzene can permeate certain plastic pipes and thus require long times to be removed from the water distribution infrastructure. Researchers estimated that, in worst case scenarios, more than 286 days of constant flushing of a contaminated HDPE service line were needed to reduce benzene below safe drinking water limits. Temperature increases caused by fires, including wildfires, can cause plastic water pipes to generate toxic chemicals such as benzene.\nImpacts on humans.\nWildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human-induced geographic and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which contributes to an increase in wildfire risk.\nAirborne hazards.\nThe most noticeable adverse effect of wildfires is the destruction of property. However, hazardous chemicals released also significantly impact human health.\nWildfire smoke is composed primarily of carbon dioxide and water vapor. Other common components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small airborne particulates (in solid form or liquid droplets) are also present in smoke and ash debris. 80\u201390% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller.\nCarbon dioxide in smoke poses a low health risk due to its low toxicity. Rather, carbon monoxide and fine particulate matter, particularly 2.5\u00a0\u03bcm in diameter and smaller, have been identified as the major health threats. High levels of heavy metals, including lead, arsenic, cadmium, and copper were found in the ash debris following the 2007 Californian wildfires. A national clean-up campaign was organised in fear of the health effects from exposure. In the devastating California Camp Fire (2018) that killed 85 people, lead levels increased by around 50 times in the hours following the fire at a site nearby (Chico). Zinc concentration also increased significantly in Modesto, away. Heavy metals such as manganese and calcium were found in numerous California fires as well. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.\nThe degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract through inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies.\nThe U.S. Environmental Protection Agency (EPA) developed the air quality index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use it to determine their exposure to hazardous air pollutants based on visibility range.\nHealth effects.\nWildfire smoke contains particulates that may have adverse effects upon the human respiratory system. Evidence of the health effects should be relayed to the public so that exposure may be limited. The evidence can also be used to influence policy to promote positive health outcomes.\nInhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is composed of combustion products i.e. carbon dioxide, carbon monoxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principal health concern is the inhalation of particulate matter and carbon monoxide. \nParticulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into three categories based on particle diameter: coarse PM, fine PM, and ultrafine PM. Coarse particles are between 2.5 micrometers and 10 micrometers, fine particles measure 0.1 to 2.5 micrometers, and ultrafine particle are less than 0.1 micrometer. lmpact on the body upon inhalation varies by size. Coarse PM is filtered by the upper airways and can accumulate and cause pulmonary inflammation. This can result in eye and sinus irritation as well as sore throat and coughing. Coarse PM is often composed of heavier and more toxic materials that lead to short-term effects with stronger impact.\nSmaller PM moves further into the respiratory system creating issues deep into the lungs and the bloodstream. In asthma patients, PM2.5 causes inflammation but also increases oxidative stress in the epithelial cells. These particulates also cause apoptosis and autophagy in lung epithelial cells. Both processes damage the cells and impact cell function. This damage impacts those with respiratory conditions such as asthma where the lung tissues and function are already compromised. Particulates less than 0.1 micrometer are called ultrafine particle (UFP). It is a major component of wildfire smoke. UFP can enter the bloodstream like PM2.5\u20130.1 however studies show that it works into the blood much quicker. The inflammation and epithelial damage done by UFP has also shown to be much more severe. PM2.5 is of the largest concern in regards to wildfire. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly associated with exposure to fine PM from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.\nAsthma exacerbation.\nSeveral epidemiological studies have demonstrated a close association between air pollution and respiratory allergic diseases such as bronchial asthma.\nAn observational study of smoke exposure related to the 2007 San Diego wildfires revealed an increase both in healthcare utilization and respiratory diagnoses, especially asthma among the group sampled. Projected climate scenarios of wildfire occurrences predict significant increases in respiratory conditions among young children. PM triggers a series of biological processes including inflammatory immune response, oxidative stress, which are associated with harmful changes in allergic respiratory diseases.\nAlthough some studies demonstrated no significant acute changes in lung function among people with asthma related to PM from wildfires, a possible explanation for these counterintuitive findings is the increased use of quick-relief medications, such as inhalers, in response to elevated levels of smoke among those already diagnosed with asthma.\nThere is consistent evidence between wildfire smoke and the exacerbation of asthma.\nAsthma is one of the most common chronic disease among children in the United States, affecting an estimated 6.2 million children. Research on asthma risk focuses specifically on the risk of air pollution during the gestational period. Several pathophysiology processes are involved in this. Considerable airway development occurs during the 2nd and 3rd trimesters and continues until 3 years of age. It is hypothesized that exposure to these toxins during this period could have consequential effects, as the epithelium of the lungs during this time could have increased permeability to toxins. Exposure to air pollution during parental and pre-natal stage could induce epigenetic changes which are responsible for the development of asthma. Studies have found significant association between PM2.5, NO2 and development of asthma during childhood despite heterogeneity among studies. Furthermore, maternal exposure to chronic stressors is most likely present in distressed communities, and as this can be correlated with childhood asthma, it may further explain links between early childhood exposure to air pollution, neighborhood poverty, and childhood risk.\nCarbon monoxide danger.\nCarbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. Thus, it is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headaches, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma, and even death. Even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990 to 2006 found that 21.9% of the deaths occurred from heart attacks.\nAnother important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from various countries who were directly and indirectly affected by wildfires were found to demonstrate different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.\nEpidemiology.\nThe Western US has seen an increase in both the frequency and intensity of wildfires over the last several decades. This has been attributed to the arid climate of there and the effects of global warming. An estimated 46 million people were exposed to wildfire smoke from 2004 to 2009 in the Western US. Evidence has demonstrated that wildfire smoke can increase levels of airborne particulate.\nThe EPA has defined acceptable concentrations of PM in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.\nAn increase in PM smoke emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD. Looking at the wildfires in Southern California in 2003, investigators have shown an increase in hospital admissions due to asthma symptoms while being exposed to peak concentrations of PM in smoke. Another epidemiological study found a 7.2% (95% confidence interval: 0.25%, 15%) increase in risk of respiratory related hospital admissions during smoke wave days with high wildfire-specific particulate matter 2.5 compared to matched non-smoke-wave days.\nChildren participating in the Children's Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide, it is estimated that 339,000 people die due to the effects of wildfire smoke each year.\nBesides the size of PM, their chemical composition should also be considered. Antecedent studies have demonstrated that the chemical composition of PM2.5 from wildfire smoke can yield different estimates of human health outcomes as compared to other sources of smoke such as solid fuels.\nPost-fire risks.\nAfter a wildfire, hazards remain. Residents returning to their homes may be at risk from falling fire-weakened trees. Humans and pets may also be harmed by falling into ash pits. The Intergovernmental Panel on Climate Change (IPCC) also reports that wildfires cause significant damage to electric systems, especially in dry regions.\nChemically contaminated drinking water, at levels of hazardous waste concern, is a growing problem. In particular, hazardous waste scale chemical contamination of buried water systems was first discovered in the U.S. in 2017, and has since been increasingly documented in Hawaii, Colorado, and Oregon after wildfires. In 2021, Canadian authorities adapted their post-fire public safety investigation approaches in British Columbia to screen for this risk, but have not found it as of 2023. Another challenge is that private drinking wells and the plumbing within a building can also become chemically contaminated and unsafe. Households experience a wide-variety of significant economic and health impacts related to this contaminated water. Evidence-based guidance on how to inspect and test wildfire impacted wells and building water systems was developed for the first time in 2020. In Paradise, California, for example, the 2018 Camp Fire caused more than $150 million dollars' worth of damage. This required almost a year of time to decontaminate and repair the municipal drinking water system from wildfire damage.\nThe source of this contamination was first proposed after the 2018 Camp Fire in California as originating from thermally degraded plastics in water systems, smoke and vapors entering depressurized plumbing, and contaminated water in buildings being sucked into the municipal water system. In 2020, it was first shown that thermal degradation of plastic drinking water materials was one potential contamination source. In 2023, the second theory was confirmed where contamination could be sucked into pipes that lost water pressure.\nOther post-fire risks, can increase if other extreme weather follows. For example, wildfires make soil less able to absorb precipitation, so heavy rainfall can result in more severe flooding and damages like mud slides.\nAt-risk groups.\nFirefighters.\nFirefighters are at greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Some of the most common health conditions that firefighters acquire from prolonged smoke inhalation include cardiovascular and respiratory diseases. For example, wildland firefighters can become hypoxic as a result of oxygen deprivation. Due to firefighters' occupational duties, they are frequently exposed to hazardous chemicals at close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters shows that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA-permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5\u201310% are overexposed.\nBetween 2001 and 2012, over 200 fatalities occurred among wildland firefighters. In addition to heat and chemical hazards, firefighters are also at risk for electrocution from power lines; injuries from equipment; slips, trips, and falls; injuries from vehicle rollovers; heat-related illness; insect bites and stings; stress; and rhabdomyolysis. Wildfires that reach urban environments create additional toxic fumes and carcinogenic particles from burning metals, plastics, electronics, paints, and other common materials.\nResidents.\nResidents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0\u20134), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to their already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods. They are also at risk for future wildfires and may move away to areas they consider less risky.\nWildfires affect large numbers of people in Western Canada and the United States. In California alone, more than 350,000 people live in towns and cities in \"very high fire hazard severity zones\".\nDirect risks to building residents in fire-prone areas can be moderated through design choices such as choosing fire-resistant vegetation, maintaining landscaping to avoid debris accumulation and to create firebreaks, and by selecting fire-retardant roofing materials. Potential compounding issues with poor air quality and heat during warmer months may be addressed with MERV 11 or higher outdoor air filtration in building ventilation systems, mechanical cooling, and a provision of a refuge area with additional air cleaning and cooling, if needed.\nHistory.\nThe first evidence of wildfires is fossils of the giant fungi \"Prototaxites\" preserved as charcoal, discovered in South Wales and Poland, dating to the Silurian period (about https://\u00a0million years ago). Smoldering surface fires started to occur sometime before the Early Devonian period https://\u00a0million years ago. Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30\u201331% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.\nWildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. \"Lepidodendron\" forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C4-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera \"Eucalyptus\", \"Pinus\" and \"Sequoia\", which have thick bark to withstand fires and employ pyriscence.\nHuman involvement.\nThe human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered pre-existing landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle Ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices. In the mid-19th century, explorers from observed Aboriginal Australians using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in lands protected by Kakadu National Park to encourage biodiversity.\nWildfires typically occur during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times more land burned annually in California before 1800 compared to recent decades (1,800,000 hectares/year compared to 102,000 hectares/year).\nAccording to a paper published in the journal \"Science\", the number of natural and human-caused fires decreased by 24.3% between 1998 and 2015. Researchers explain this as a transition from nomadism to settled lifestyle and intensification of agriculture that lead to a drop in the use of fire for land clearing.\nIncreases of certain tree species (i.e. conifers) over others (i.e. deciduous trees) can increase wildfire risk, especially if these trees are also planted in monocultures.\nSome invasive species, moved in by humans (i.e., for the pulp and paper industry) have in some cases also increased the intensity of wildfires. Examples include species such as Eucalyptus in California and gamba grass in Australia.\nSociety and culture.\nWildfires have a place in many cultures. \"To spread like wildfire\" is a common idiom in English, meaning something that \"quickly affects or becomes known by more and more people\".\nWildfire activity has been attributed as a major factor in the development of Ancient Greece. In modern Greece, as in many other regions, it is the most common disaster caused by a natural hazard and figures prominently in the social and economic lives of its people.\nIn 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, characters from the Disney movie \"Bambi\", and the official mascot of the U.S. Forest Service, Smokey Bear. The Smokey Bear fire prevention campaign has yielded one of the most popular characters in the United States; for many years there was a living Smokey Bear mascot, and it has been commemorated on postage stamps.\nThere are also significant indirect or second-order societal impacts from wildfire, such as demands on utilities to prevent power transmission equipment from becoming ignition sources, and the cancelation or nonrenewal of homeowners insurance for residents living in wildfire-prone areas.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\nAttribution"}
{"id": "56107", "revid": "43056943", "url": "https://en.wikipedia.org/wiki?curid=56107", "title": "Metropolis\u2013Hastings algorithm", "text": "Monte Carlo algorithm\nIn statistics and statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. New samples are added to the sequence in two steps: first a new sample is proposed based on the previous sample, then the proposed sample is either added to the sequence or rejected depending on the value of the probability distribution at that point. The resulting sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value).\nMetropolis\u2013Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, there are usually other methods (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and these are free from the problem of autocorrelated samples that is inherent in MCMC methods.\nHistory.\nThe algorithm is named in part for Nicholas Metropolis, the first coauthor of a 1953 paper, entitled \"Equation of State Calculations by Fast Computing Machines\", with Arianna W. Rosenbluth, Marshall Rosenbluth, Augusta H. Teller and Edward Teller. For many years the algorithm was known simply as the \"Metropolis algorithm\". The paper proposed the algorithm for the case of symmetrical proposal distributions, but in 1970, W.K. Hastings extended it to the more general case. The generalized method was eventually identified by both names, although the first use of the term \"Metropolis-Hastings algorithm\" is unclear.\nSome controversy exists with regard to credit for development of the Metropolis algorithm. Metropolis, who was familiar with the computational aspects of the method, had coined the term \"Monte Carlo\" in an earlier article with Stanis\u0142aw Ulam, and led the group in the Theoretical Division that designed and built the MANIAC I computer used in the experiments in 1952. However, prior to 2003 there was no detailed account of the algorithm's development. Shortly before his death, Marshall Rosenbluth attended a 2003 conference at LANL marking the 50th anniversary of the 1953 publication. At this conference, Rosenbluth described the algorithm and its development in a presentation titled \"Genesis of the Monte Carlo Algorithm for Statistical Mechanics\". Further historical clarification is made by Gubernatis in a 2005 journal article recounting the 50th anniversary conference. Rosenbluth makes it clear that he and his wife Arianna did the work, and that Metropolis played no role in the development other than providing computer time.\nThis contradicts an account by Edward Teller, who states in his memoirs that the five authors of the 1953 article worked together for \"days (and nights)\". In contrast, the detailed account by Rosenbluth credits Teller with a crucial but early suggestion to \"take advantage of statistical mechanics and take ensemble averages instead of following detailed kinematics\". This, says Rosenbluth, started him thinking about the generalized Monte Carlo approach \u2013 a topic which he says he had discussed often with John Von Neumann. Arianna Rosenbluth recounted (to Gubernatis in 2003) that Augusta Teller started the computer work, but that Arianna herself took it over and wrote the code from scratch. In an oral history recorded shortly before his death, Rosenbluth again credits Teller with posing the original problem, himself with solving it, and Arianna with programming the computer.\nDescription.\nThe Metropolis\u2013Hastings algorithm can draw samples from any probability distribution with probability density formula_1, provided that we know a function formula_2 proportional to the density formula_3 and the values of formula_2 can be calculated. The requirement that formula_2 must only be proportional to the density, rather than exactly equal to it, makes the Metropolis\u2013Hastings algorithm particularly useful, because it removes the need to calculate the density's normalization factor, which is often extremely difficult in practice.\nThe Metropolis\u2013Hastings algorithm generates a sequence of sample values in such a way that, as more and more sample values are produced, the distribution of values more closely approximates the desired distribution. These sample values are produced iteratively in such a way, that the distribution of the next sample depends only on the current sample value, which makes the sequence of samples a Markov chain. Specifically, at each iteration, the algorithm proposes a candidate for the next sample value based on the current sample value. Then, with some probability, the candidate is either accepted, in which case the candidate value is used in the next iteration, or it is rejected in which case the candidate value is discarded, and the current value is reused in the next iteration. The probability of acceptance is determined by comparing the values of the function formula_2 of the current and candidate sample values with respect to the desired distribution.\nThe method used to propose new candidates is characterized by the probability distribution formula_7 (sometimes written formula_8) of a new proposed sample formula_9 given the previous sample formula_10. This is called the \"proposal density\", \"proposal function\", or \"jumping distribution\". A common choice for formula_7 is a Gaussian distribution centered at formula_10, so that points closer to formula_10 are more likely to be visited next, making the sequence of samples into a Gaussian random walk. In the original paper by Metropolis et al. (1953), formula_7 was suggested to be a uniform distribution limited to some maximum distance from formula_10. More complicated proposal functions are also possible, such as those of Hamiltonian Monte Carlo, Langevin Monte Carlo, or preconditioned Crank\u2013Nicolson.\nFor the purpose of illustration, the Metropolis algorithm, a special case of the Metropolis\u2013Hastings algorithm where the proposal function is symmetric, is described below.\nLet formula_2 be a function that is proportional to the desired probability density function formula_1 (a.k.a. a target distribution).\nThis algorithm proceeds by randomly attempting to move about the sample space, sometimes accepting the moves and sometimes remaining in place. formula_1 at specific point formula_9 is proportional to the iterations spent on the point by the algorithm. Note that the acceptance ratio formula_33 indicates how probable the new proposed sample is with respect to the current sample, according to the distribution whose density is formula_1. If we attempt to move to a point that is more probable than the existing point (i.e. a point in a higher-density region of formula_1 corresponding to an formula_36), we will always accept the move. However, if we attempt to move to a less probable point, we will sometimes reject the move, and the larger the relative drop in probability, the more likely we are to reject the new point. Thus, we will tend to stay in (and return large numbers of samples from) high-density regions of formula_1, while only occasionally visiting low-density regions. Intuitively, this is why this algorithm works and returns samples that follow the desired distribution with density formula_1.\nCompared with an algorithm like adaptive rejection sampling that directly generates independent samples from a distribution, Metropolis\u2013Hastings and other MCMC algorithms have a number of disadvantages:\nOn the other hand, most simple rejection sampling methods suffer from the \"curse of dimensionality\", where the probability of rejection increases exponentially as a function of the number of dimensions. Metropolis\u2013Hastings, along with other MCMC methods, do not have this problem to such a degree, and thus are often the only solutions available when the number of dimensions of the distribution to be sampled is high. As a result, MCMC methods are often the methods of choice for producing samples from hierarchical Bayesian models and other high-dimensional statistical models used nowadays in many disciplines.\nIn multivariate distributions, the classic Metropolis\u2013Hastings algorithm as described above involves choosing a new multi-dimensional sample point. When the number of dimensions is high, finding the suitable jumping distribution to use can be difficult, as the different individual dimensions behave in very different ways, and the jumping width (see above) must be \"just right\" for all dimensions at once to avoid excessively slow mixing. An alternative approach that often works better in such situations, known as Gibbs sampling, involves choosing a new sample for each dimension separately from the others, rather than choosing a sample for all dimensions at once. That way, the problem of sampling from potentially high-dimensional space will be reduced to a collection of problems to sample from small dimensionality. This is especially applicable when the multivariate distribution is composed of a set of individual random variables in which each variable is conditioned on only a small number of other variables, as is the case in most typical hierarchical models. The individual variables are then sampled one at a time, with each variable conditioned on the most recent values of all the others. Various algorithms can be used to choose these individual samples, depending on the exact form of the multivariate distribution: some possibilities are the adaptive rejection sampling methods, the adaptive rejection Metropolis sampling algorithm, a simple one-dimensional Metropolis\u2013Hastings step, or slice sampling.\nFormal derivation.\nThe purpose of the Metropolis\u2013Hastings algorithm is to generate a collection of states according to a desired distribution formula_1. To accomplish this, the algorithm uses a Markov process, which asymptotically reaches a unique stationary distribution formula_41 such that formula_42.\nA Markov process is uniquely defined by its transition probabilities formula_43, the probability of transitioning from any given state formula_9 to any other given state formula_22. It has a unique stationary distribution formula_41 when the following two conditions are met:\nThe Metropolis\u2013Hastings algorithm involves designing a Markov process (by constructing transition probabilities) that fulfills the two above conditions, such that its stationary distribution formula_41 is chosen to be formula_1. The derivation of the algorithm starts with the condition of detailed balance:\n formula_58\nwhich is re-written as\n formula_59\nThe approach is to separate the transition in two sub-steps; the proposal and the acceptance-rejection. The proposal distribution formula_60 is the conditional probability of proposing a state formula_22 given formula_9, and the acceptance distribution formula_63 is the probability to accept the proposed state formula_22. The transition probability can be written as the product of them:\n formula_65\nInserting this relation in the previous equation, we have\n formula_66\nThe next step in the derivation is to choose an acceptance ratio that fulfills the condition above. One common choice is the Metropolis choice:\n formula_67\nFor this Metropolis acceptance ratio formula_68, either formula_69 or formula_70 and, either way, the condition is satisfied.\nThe Metropolis\u2013Hastings algorithm can thus be written as follows:\nProvided that specified conditions are met, the empirical distribution of saved states formula_82 will approach formula_1. The number of iterations (formula_84) required to effectively estimate formula_1 depends on the number of factors, including the relationship between formula_1 and the proposal distribution and the desired accuracy of estimation. For distribution on discrete state spaces, it has to be of the order of the autocorrelation time of the Markov process. An accessible account of the convergence theory for Metropolis\u2013Hastings is given in.\nIt is important to notice that it is not clear, in a general problem, which distribution formula_60 one should use or the number of iterations necessary for proper estimation; both are free parameters of the method, which must be adjusted to the particular problem in hand.\nUse in numerical integration.\nA common use of Metropolis\u2013Hastings algorithm is to compute an integral. Specifically, consider a space formula_88 and a probability distribution formula_1 over formula_90, formula_91. Metropolis\u2013Hastings can estimate an integral of the form of\n formula_92\nwhere formula_93 is a (measurable) function of interest.\nFor example, consider a statistic formula_94 and its probability distribution formula_95, which is a marginal distribution. Suppose that the goal is to estimate formula_95 for formula_97 on the tail of formula_95. Formally, formula_95 can be written as\n formula_100\nand, thus, estimating formula_95 can be accomplished by estimating the expected value of the indicator function formula_102, which is 1 when formula_103 and zero otherwise.\nBecause formula_97 is on the tail of formula_95, the probability to draw a state formula_9 with formula_94 on the tail of formula_95 is proportional to formula_95, which is small by definition. The Metropolis\u2013Hastings algorithm can be used here to sample (rare) states more likely and thus increase the number of samples used to estimate formula_95 on the tails. This can be done e.g. by using a sampling distribution formula_41 to favor those states (e.g. formula_112 with formula_113).\nStep-by-step instructions.\nSuppose that the most recent value sampled is formula_18. To follow the Metropolis\u2013Hastings algorithm, we next draw a new proposal state formula_22 with probability density formula_74 and calculate a value\n formula_117\nwhere\n formula_118\nis the probability (e.g., Bayesian posterior) ratio between the proposed sample formula_22 and the previous sample formula_18, and\n formula_121\nis the ratio of the proposal density in two directions (from formula_18 to formula_22 and conversely).\nThis is equal to 1 if the proposal density is symmetric.\nThen the new state formula_124 is chosen according to the following rules.\n If formula_125\n formula_126\n else:\n formula_127\nThe Markov chain is started from an arbitrary initial value formula_71, and the algorithm is run for many iterations until this initial state is \"forgotten\". These samples, which are discarded, are known as \"burn-in\". The remaining set of accepted values of formula_9 represent a sample from the distribution formula_1.\nThe algorithm works best if the proposal density matches the shape of the target distribution formula_1, from which direct sampling is difficult, that is formula_132.\nIf a Gaussian proposal density formula_20 is used, the variance parameter formula_134 has to be tuned during the burn-in period.\nThis is usually done by calculating the \"acceptance rate\", which is the fraction of proposed samples that is accepted in a window of the last formula_135 samples.\nThe desired acceptance rate depends on the target distribution, however it has been shown theoretically that the ideal acceptance rate for a one-dimensional Gaussian distribution is about 50%, decreasing to about 23% for an formula_135-dimensional Gaussian target distribution. These guidelines can work well when sampling from sufficiently regular Bayesian posteriors as they often follow a multivariate normal distribution as can be established using the Bernstein\u2013von Mises theorem.\nIf formula_134 is too small, the chain will \"mix slowly\" (i.e., the acceptance rate will be high, but successive samples will move around the space slowly, and the chain will converge only slowly to formula_1). On the other hand,\nif formula_134 is too large, the acceptance rate will be very low because the proposals are likely to land in regions of much lower probability density, so formula_140 will be very small, and again the chain will converge very slowly. One typically tunes the proposal distribution so that the algorithms accepts on the order of 30% of all samples \u2013 in line with the theoretical estimates mentioned in the previous paragraph.\nBayesian inference.\nMCMC can be used to draw samples from the posterior distribution of a statistical model.\nThe acceptance probability is given by:\nformula_141\nwhere formula_142 is the likelihood, formula_143 the prior probability density and formula_144 the (conditional) proposal probability.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56108", "revid": "30699314", "url": "https://en.wikipedia.org/wiki?curid=56108", "title": "Penrose triangle", "text": "Impossible object\nThe Penrose triangle, also known as the Penrose tribar, the impossible tribar, or the impossible triangle, is a triangular impossible object, an optical illusion consisting of an object which can be depicted in a perspective drawing. It cannot exist as a solid object in ordinary three-dimensional Euclidean space, although its surface can be embedded isometrically (bent but not stretched) in five-dimensional Euclidean space. It was first created by the Swedish artist Oscar Reutersv\u00e4rd in 1934. Independently from Reutersv\u00e4rd, the triangle was devised and popularized in the 1950s by psychiatrist Lionel Penrose and his son, the mathematician and Nobel Prize laureate Roger Penrose, who described it as \"impossibility in its purest form\". It is featured prominently in the works of artist M. C. Escher, whose earlier depictions of impossible objects partly inspired it.\nDescription.\nThe tribar/triangle appears to be a solid object, made of three straight beams of square cross-section which meet pairwise at right angles at the vertices of the triangle they form. The beams may be broken, forming cubes or cuboids.\nThis combination of properties cannot be realized by any three-dimensional object in ordinary Euclidean space. Such an object can exist in certain Euclidean 3-manifolds. A surface with the same geodesic distances as the depicted surface of the tribar, but without its flat shape and right angles, are to be preserved, can also exist in 5-dimensional Euclidean space, which is the lowest-dimensional Euclidean space within which this surface can be isometrically embedded. There also exist three-dimensional solid shapes each of which, when viewed from a certain angle, appears the same as the 2-dimensional depiction of the Penrose triangle, such as the sculpture \"Impossible Triangle\" in East Perth, Australia. The term \"Penrose Triangle\" can refer to the 2-dimensional depiction or the impossible object itself.\nIf a line is traced around the Penrose triangle, a 4-loop M\u00f6bius strip is formed.\nCreation of the Penrose triangle from partial figures.\nIf you move the left part of the figure parallel to the right until its upper horizontal edge coincides with the upper horizontal edge of the middle part of the figure, the Penrose triangle (right) is created by overlapping the two parts.\nThe first two partial views of the Penrose triangle are individually perceptible, whereas the resulting tribar represents an impossible figure.\nDepictions.\nM.C. Escher's lithograph \"Waterfall\" (1961) depicts a watercourse that flows in a zigzag along the long sides of two elongated Penrose triangles, so that it ends up two stories higher than it began. The resulting waterfall, forming the short sides of both triangles, drives a water wheel. Escher points out that in order to keep the wheel turning, some water must occasionally be added to compensate for evaporation. A third Penrose triangle lies between the other two, formed by two segments of waterway and a support tower.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56109", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=56109", "title": "Brown rat", "text": "Species of common rat\n&lt;templatestyles src=\"Template:Taxobox/core/styles.css\" /&gt;\nThe brown rat (Rattus norvegicus), also known as the common rat, street rat, sewer rat, wharf rat, Hanover rat, Norway rat and Norwegian rat, is a widespread, common species of rat. One of the largest muroids, it is a brown or grey rodent with a body length of up to long, and a tail slightly shorter than that. It weighs between . Thought to have originated in northern China and neighbouring areas, it has now spread to all continents except Antarctica, and is the dominant rat in Europe and much of North America, having become naturalised across the world. With rare exceptions, the brown rat lives wherever humans live, particularly in urban areas. They are omnivorous, reproduce rapidly, and can be a vector for several human diseases.\nSelective breeding of the brown rat has produced the fancy rat (rats kept as pets), as well as the laboratory rat (rats used as model organisms in biological research). Both fancy rats and laboratory rats are of the domesticated subspecies \"Rattus norvegicus domestica\". Studies of wild rats in New York City have shown that populations living in different neighborhoods can evolve distinct genomic profiles over time, by slowly accruing different traits.\nNaming and etymology.\nThe brown rat was originally called the \"Hanover rat\" by people wishing to link problems in 18th-century England with the House of Hanover. It is not known for certain why the brown rat is named \"Rattus norvegicus\" (Norwegian rat), as it did not originate from Norway. However, the English naturalist John Berkenhout, author of the 1769 book \"Outlines of the Natural History of Great Britain\", is most likely responsible for popularizing the misnomer. Berkenhout gave the brown rat the binomial name \"Mus norvegicus\", believing it had migrated to England from Norwegian ships in 1728.\nBy the early to the middle part of the 19th century, British academics believed that the brown rat was not native to Norway, hypothesizing (incorrectly) that it may have come from Ireland, Gibraltar or across the English Channel with William the Conqueror. As early as 1850, however, a new hypothesis of the rat's origins was beginning to develop. The British novelist Charles Dickens acknowledged this in his weekly journal, \"All the Year Round,\" writing:\nIt is frequently called, in books and otherwise, the 'Norway rat', and it is said to have been imported into this country in a ship-load of timber from Norway. Against this hypothesis stands the fact that when the brown rat had become common in this country, it was unknown in Norway, although there was a small animal like a rat, but really a lemming, which made its home there.\nAcademics began to prefer this etymology of the brown rat towards the end of the 19th century, as seen in the 1895 text \"Natural History\" by American scholar Alfred Henry Miles:\nThe brown rat is the species common in England, and best known throughout the world. It is said to have travelled from Persia to England less than two hundred years ago and to have spread from thence to other countries visited by English ships.\nThough the assumptions surrounding this species' origins were not yet the same as modern ones, by the 20th century, it was believed among naturalists that the brown rat did not originate in Norway, rather the species came from central Asia and (likely) China.\nDescription.\nThe brown rat's fur is usually brown or dark grey, while the underparts are lighter grey or brown. The brown rat is a rather large murid and can weigh twice as much as a black rat (\"R. rattus\") and many times more than a house mouse (\"Mus musculus\"). The head and body length ranges from while the tail ranges in length from , therefore being shorter than the head and body. Adult weight ranges from . Large individuals can reach but are not expected outside of domestic specimens. Stories of rats attaining sizes as big as cats are exaggerations, or misidentifications of larger rodents. It is common for breeding wild brown rats to weigh less than .\nThe heaviest live brown rat on record is and they can reach a maximum length of .\nThe brown rat has acute hearing, is sensitive to ultrasound, and possesses a very highly developed olfactory sense. Its average heart rate is 300 to 400 beats per minute, with a respiratory rate of around 100 per minute. The vision of a pigmented rat is poor, around 20/600, while a non-pigmented with no melanin in its eyes has both around 20/1200 vision and a terrible scattering of light within its vision. It is a dichromat which perceives colours rather like a human with red-green colorblindness, and its colour saturation may be quite faint. The blue perception, however, also has UV receptors, allowing it to see ultraviolet lights that humans and some other species cannot.\nBiology and behavior.\nThe brown rat is nocturnal and is a good swimmer, both on the surface and underwater, and has been observed climbing slim round metal poles several feet in order to reach garden bird feeders. Brown rats dig well, and often excavate extensive burrow systems. A 2007 study found brown rats to possess metacognition, a mental ability previously only found in humans and some other primates, but further analysis suggested they may have been following simple operant conditioning principles.\nCommunication.\nBrown rats are capable of producing ultrasonic vocalizations. As pups, young rats use different types of ultrasonic cries to elicit and direct maternal search behavior, as well as to regulate their mother's movements in the nest. Although pups produce ultrasounds around any other rats at the age of 7 days, by 14 days old they significantly reduce ultrasound production around male rats as a defensive response. Adult rats will emit ultrasonic vocalizations in response to predators or perceived danger; the frequency and duration of such cries depends on the sex and reproductive status of the rat. The female rat also emit ultrasonic vocalizations during mating.\nRats may also emit short, high frequency, ultrasonic, socially induced vocalization during rough and tumble play, before receiving morphine, or mating, and when tickled. The vocalization, described as a distinct \"chirping\", has been likened to laughter, and is interpreted as an expectation of something rewarding. Like most rat vocalizations, the chirping is too high in pitch for humans to hear without special equipment. Bat detectors are often used by pet owners for this purpose.\nIn research studies, the chirping is associated with positive emotional feelings, and social bonding occurs with the tickler, resulting in the rats becoming conditioned to seek the tickling. However, as the rats age, the tendency to chirp appears to decline.\nBrown rats also produce communicative noises capable of being heard by humans. The most commonly heard in domestic rats is bruxing, or teeth-grinding, which is most usually triggered by happiness, but can also be 'self-comforting' in stressful situations, such as a visit to the vet. The noise is best described as either a quick clicking or 'burring' sound, varying from animal to animal. Vigorous bruxing can be accompanied by boggling, where the eyes of the rat rapidly bulge and retract due to movement of the lower jaw muscles behind the eye socket.\nIn addition, they commonly squeak along a range of tones from high, abrupt pain squeaks to soft, persistent 'singing' sounds during confrontations.\nDiet.\nThe brown rat is a true omnivore and consumes almost anything, but cereals form a substantial part of its diet. The most-liked foods of brown rats include scrambled eggs, raw carrots, and cooked corn kernels. The least-liked foods are raw beets, peaches and raw celery.\nForaging behavior is often population-specific, and varies by environment and food source. Brown rats living near a hatchery in West Virginia catch fingerling fish. \nSome colonies along the banks of the Po River in Italy dive for mollusks, a practice demonstrating social learning among members of this species. Rats on the island of Norderoog in the North Sea stalk and kill sparrows and ducks.\nAlso preyed upon by brown rats are chicks, mice and small lizards. Examination of a wild brown rat stomachs in Germany revealed 4,000 food items, most of which were plants, although studies have shown that brown rats prefer meat when given the option. In metropolitan areas, they survive mainly on discarded human food and anything else that can be eaten without negative consequences.\nReproduction and life cycle.\nThe brown rat can breed throughout the year if conditions are suitable, with a female producing up to five litters a year. The gestation period is only 21 days, and litters can number up to 14, although seven is common. They weigh an average of at birth. They reach sexual maturity in about five weeks. Under ideal conditions (for the rat), this means that the population of females could increase by a factor of three and a half (half a litter of 7) in 8 weeks (5 weeks for sexual maturity and 3 weeks of gestation), corresponding to a population growing by a factor of 10 in just 15 weeks. As a result, the population can grow from 2 to 15,000 in a year. The maximum life span is three years, although most barely manage one. A yearly mortality rate of 95% is estimated, with predators and interspecies conflict as major causes.\nWhen lactating, female rats display a 24-hour rhythm of maternal behavior, and will usually spend more time attending to smaller litters than large ones.\nBrown rats live in large, hierarchical groups, either in burrows or subsurface places, such as sewers and cellars. When food is in short supply, the rats lower in social order are the first to die. If a large fraction of a rat population is exterminated, the remaining rats will increase their reproductive rate, and quickly restore the old population level.\nThe female is capable of becoming pregnant immediately after giving birth, and can nurse one litter while pregnant with another. She is able to produce and raise two healthy litters of normal size and weight without significantly changing her own food intake. However, when food is restricted, she can extend pregnancy by over two weeks, and give birth to litters of normal number and weight.\nMating behaviors.\nMales can ejaculate multiple times in a row, and this increases the likelihood of pregnancy as well as decreases the number of stillborns. Multiple ejaculation also means that males can mate with multiple females, and they exhibit more ejaculatory series when there are several oestrous females present. Males also copulate at shorter intervals than females. In group mating, females often switch partners.\nDominant males have higher mating success and also provide females with more ejaculate, and females are more likely to use the sperm of dominant males for fertilization.\nIn mating, female rats show a clear mating preference for unknown males versus males that they have already mated with (also known as the Coolidge effect), and will often resume copulatory behavior when introduced to a novel sexual partner.\nFemales also prefer to mate with males who have not experienced social stress during adolescence, and can determine which males were stressed even without any observed difference in sexual performance of males experiencing stress during adolescence and not.\nSocial behavior.\nRats commonly groom each other and sleep together. Rats are said to establish an order of hierarchy, so one rat will be dominant over another one. Groups of rats tend to \"play fight\", which can involve any combination of jumping, chasing, tumbling, and \"boxing\". Play fighting involves rats going for each other's necks, while serious fighting involves strikes at the others' back ends. If living space becomes limited, rats may turn to aggressive behavior, which may result in the death of some animals, reducing the burden over the living space.\nRats, like most mammals, also form family groups of a mother and her young. This applies to both groups of males and females. However, rats are territorial animals, meaning that they usually act aggressively towards or scared of strange rats. Rats will fluff up their hair, hiss, squeal, and move their tails around when defending their territory. Rats will chase each other, groom each other, sleep in group nests, wrestle with each other, have dominance squabbles, communicate, and play in various other ways with each other. Huddling is an additional important part of rat socialization. Huddling, an extreme form of herding and like chattering or \"bruxing\" is often used to communicate that they are feeling threatened and not to come near. The common rat has been more successful at inhabiting and building communities on 6 continents and are the only species to have occupied more land than humans.\nDuring the wintry months, rats will huddle into piles \u2013 usually cheek-to-cheek \u2013 to control humidity and keep the air warm as a heat-conserving function. Just like elderly rats are commonly groomed and nursed by their companions, nestling rats especially depend on heat from their mother, since they cannot regulate their own temperature. Other forms of interaction include: crawling under, which is literally the act of crawling underneath one another (this is common when the rat is feeling ill and helps them breathe); walking over to find a space next to their closest friend, also explained in the name; allo-grooming, so-called to distinguish it from self-grooming; and nosing, where a rat gently pushes with its nose at another rat near the neck.\nBurrowing.\nRats are known to burrow extensively, both in the wild and in captivity, if given access to a suitable substrate. Rats generally begin a new burrow adjacent to an object or structure, as this provides a sturdy \"roof\" for the section of the burrow nearest to the ground's surface. Burrows usually develop to eventually include multiple levels of tunnels, as well as a secondary entrance. Older male rats will generally not burrow, while young males and females will burrow vigorously.\nBurrows provide rats with shelter and food storage, as well as safe, thermo-regulated nest sites. Rats use their burrows to escape from perceived threats in the surrounding environment; for example, rats will retreat to their burrows following a sudden, loud noise or while fleeing an intruder. Burrowing can therefore be described as a \"pre-encounter defensive behavior\", as opposed to a \"post-encounter defensive behavior\", such as flight, freezing, or avoidance of a threatening stimulus. \nEvolution.\nBrown rats' ancestors diverged from the black rat lineage approximately 2.9 million years ago.\u00a0 Evidence collected from mitochondrial genomes suggests that they emerged as a separate species anywhere from 0.5 to 2.9 million years ago. Brown rats have two sister species within the \"norvegicus\" group. The Himalayan field rat (\"Rattus nitidus\") has recently been identified as closely related, with mitochondrial DNA suggesting divergence around 700 thousand years ago. However, genetic intermixing between these species is believed to have continued after this separation. Re-encounters between Himalayan and brown rat populations led to the introgression of genes from the former into the latter. This allowed for adaptations in olfactory receptors to be spread to brown rats. Another sister species, the Turkestan rat, has been found close to the brown rat's ancestral range in Nepal. No such cases of genetic mixing between Turkestan and brown rats have been documented thus far. \nAdaptations in laboratory populations.\nBrown rats' interactions with human environments have resulted in a number of identifiable changes in traits. This is especially well documented in laboratory rats. Despite significant levels of inbreeding through artificial selection, domesticated laboratory rats maintain higher genetic diversity than laboratory mice. Population structure among strains of domesticated rats is so powerful that distinctions are detectable between rats bred in different rooms of the same facilities. The earliest evidence of differentiation from wild populations is the early proliferation of color variants among domesticated strains. This was documented in the 18th century in Japan and by the 19th century in Europe.\nToday's laboratory rats exhibit physical and behavioral adaptations. Rats bred for laboratory use develop smaller testes and have smaller neocortexes. They struggle significantly more with digging and swimming than wild rats when placed under identical conditions.[17] This latter difference results in an aversion in domesticated rats that allows researchers to test memory using the Morris water maze.[19] Rats will recall an unseen platform's location in a pool and swim to it to avoid swimming.\u00a0 Other differences contribute to testing in more direct fashions; domesticated rats are able to learn faster and have lower resting levels of stress hormones.[19] On the other hand, laboratory rats' increased agonistic behavior is less beneficial and not intentionally selected for.[19] A mixture of artificial selection and random variation through genetic drift is likely responsible for these differences. \nUnique adaptations have been observed in albino strains of laboratory rats. Albinos have smaller visual cortices and are less active during the day than their pigmented counterparts. Both of these adaptations are believed to be connected to their diminished visual acuity.\nDifferences between laboratory rats and wild populations have led to increasing concerns over the representativeness of psychological studies using inbred strains. Some researchers point to the effects of selective breeding on fear response and brain size as warping the results' applicability to human fear mechanisms.\nAdaptations in wild populations.\nLike laboratory strains, wild populations in human-occupied environments show significant genetic variation. Urban environments present substantial barriers to movement that may restrict brown rat populations to single city blocks. These lead to differentiated population structures between regions of the same city, as demonstrated in New York, Vancouver, and Salvador. Roadways and districts with low levels of garbage were found to separate populations and restrict gene flow among groups.\nParticular attention has been given to the adaptations found in the New York City population. New York rats have longer noses and shorter molar rows than the Chinese population; these are hypothesized to be adaptations to a colder climate and a diet including human food, respectively. Genetic markers also show differences in regions associated with the metabolism and diet of New York rats. However, metabolic differences have also been connected to migrating populations prior to relocation in urban settlements. Immune response changes from this period are suspected to have enabled the eventual domestication of brown rats in Europe.\nPopulation bottlenecks are a significant source of adaptation. Such a bottleneck is theorized to have occurred 20,000 years ago in the ancestral Chinese population, and similar reductions due to founder effects have been observed as invasive populations spread to new areas. A notable recent instance of bottleneck-induced adaptation is the rise of rodenticide resistance among wild brown rats. Resistance to warfarin was discovered in urban populations in the mid-20th century, prompting the synthesis of new forms of rodenticide. Some populations in the United Kingdom have also been found to resist the second-generation rodenticides developed. Behavioral adaptations have also made effective rodenticide more difficult to provide; fear of new stimuli in wild populations has been linked to the widespread presence of rodenticide. This fear is markedly reduced in domesticated populations.\nDistribution and habitat.\nPossibly originating from the plains of northern China and Mongolia, the brown rat spread to other parts of the world sometime in the Middle Ages. The question of when brown rats became commensal with humans remains unsettled, but as a species, they have spread and established themselves along routes of human migration and now live almost everywhere humans are.\nThe brown rat may have been present in Europe as early as 1553, a conclusion drawn from an illustration and description by Swiss naturalist Conrad Gesner in his book \"Historiae animalium\", published 1551\u20131558. Though Gesner's description could apply to the black rat, his mention of a large percentage of albino specimens\u2014not uncommon among wild populations of brown rats\u2014adds credibility to this conclusion. Reliable reports dating to the 18th century document the presence of the brown rat in Ireland in 1722, England in 1730, France in 1735, Germany in 1750, and Spain in 1800, becoming widespread during the Industrial Revolution. It did not reach North America until around 1750\u20131755.\nAs it spread from Asia, the brown rat generally displaced the black rat in areas where humans lived. In addition to being larger and more aggressive, the change from wooden structures and thatched roofs to bricked and tiled buildings favored the burrowing brown rats over the arboreal black rats. In addition, brown rats eat a wider variety of foods, and are more resistant to weather extremes.\nIn the absence of humans, brown rats prefer damp environments, such as river banks. However, the great majority are now linked to man-made environments, such as sewage systems.\nIt is often said that there are as many rats in cities as people, but this varies from area to area depending on climate, living conditions, etc. Brown rats in cities tend not to wander extensively, often staying within of their nest if a suitable concentrated food supply is available, but they will range more widely where food availability is lower. It is difficult to determine the extent of their home range because they do not utilize a whole area but rather use regular runways to get from one location to another. There is great debate over the size of the population of rats in New York City, with estimates from almost 100 million rats to as few as 250,000. Experts suggest that New York is a particularly attractive place for rats because of its aging infrastructure and high poverty rates. In 2023, the city appointed Kathleen Corradi as the first Rat Czar, a position created to address the city's rat population. The position focuses on instituting policies measures to curb the population such as garbage regulation and additional rat trapping. In addition to sewers, rats are very comfortable living in alleyways and residential buildings, as there is usually a large and continuous food source in those areas.\nIn the United Kingdom, some figures show that the rat population has been rising, with estimations that 81 million rats reside in the UK Those figures would mean that there are 1.3 rats per person in the country. High rat populations in the UK are often attributed to the mild climate, which allow them higher survival rates during the winter. With the increase in global temperature and glacier retreat, it is estimated that brown rat populations will see an increase.\nIn tropical and desert regions, brown rat occurrence tends to be limited to human-modified habitats. Contiguous rat-free areas in the world include the continent of Antarctica, the Arctic, some isolated islands, the Canadian province of Alberta, and certain conservation areas in New Zealand. Most of Australia apart from the eastern and south-eastern coastal areas does not have reports of substantial rat occurrences.\nAntarctica is uninhabitable by rats. The Arctic has extremely cold winters that rats cannot survive outdoors, and the human population density is extremely low, making it difficult for rats to travel from one habitation to another, although they have arrived in many coastal areas by ship. When the occasional rat infestation is found and eliminated, the rats are unable to re-infest it from an adjacent one. Isolated islands are also able to eliminate rat populations because of low human population density and the geographic distance from other rat populations.\nRats as invasive species.\nMany parts of the world have been populated by rats secondarily, where rats are now important invasive species that compete with and threaten local fauna. For instance, Norway rats reached North America between 1750 and 1775 and even in the early 20th century, from 1925 to 1927, 50% of ships entering the port of New York were rat infested.\nFaroe Islands.\nThe brown rat was first observed on the Faroe Islands in 1768. It is thought that the first individuals arrived on the southernmost island, Su\u00f0uroy, via the wreck of a Norwegian ship that had stranded on the Scottish Isle of Lewis on its way from Trondheim to Dublin. The drifting wreck, carrying brown rats, drifted northwards until it reached the village of Hvalba. Dispersion afterwards appears to have been fast, including all of Su\u00f0uroy within a year. In 1769, they were observed in T\u00f3rshavn on the southern part of Streymoy, and a decade later, in the villages in the northern part of this island.\nFrom here, they crossed the strait and occupied Eysturoy during the years 1776 to 1779. In 1779, they reached Vagar. Whether the rats dispersed from the already established population in Su\u00f0uroy, or they were brought to the Faroe Islands with other ships is unknown. The Northern islands were invaded by the brown rat more than 100 years later, after Norwegians built and operated a whaling station in the village of Hvannasund on Bor\u00f0oy from 1898 to 1920. From there, the brown rat spread to the neighbouring islands of Vi\u00f0oy and Kunoy. A recent genomic analysis reveals three independent introductions of the invasive brown rat to the Faroe Islands.\nToday the brown rat is found on seven of the eighteen Faroese islands, and is common in and around human habitations as well as in the wild. Although the brown rat is now common on all of the largest Faroese islands, only sparse information on the population is available in the literature. An investigation for infection with the spirochaete \"Leptospira interrogans\" did not find any infected animals, suggesting that \"Leptospira\" prevalence rates on the Faroe Islands may be among the lowest recorded worldwide.\nAlaska.\nHawadax Island (formerly known as Rat Island) in Alaska is thought to have been the first island in the Aleutians to be invaded by Norway rats (the Brown rat) when a Japanese ship went aground in the 1780s. They had a devastating effect on the native bird life. An eradication program was started in 2007 and the island was declared rat-free in June 2009.\nAlberta.\nAlberta is the largest rat-free, human-populated area in the world. Rat invasions of Alberta were stopped and rats were eliminated by very aggressive government rat control measures, starting during the 1950s.\nThe only \"Rattus\" species that is capable of surviving the climate of Alberta is the brown rat, which can only survive in the prairie region of the province, and even then must overwinter in buildings. Although it is a major agricultural area, Alberta is far from any seaport and only a portion of its eastern boundary with Saskatchewan provides a favorable entry route for rats. Brown rats cannot survive in the wild boreal forest to the north, the Rocky Mountains to the west, nor can they safely cross the semiarid High Plains of Montana to the south. The first brown rat did not reach Alberta until 1950, and in 1951, the province launched a rat-control program that included shooting, poisoning, and gassing rats, and bulldozing or burning down some rat-infested buildings. The effort was backed by legislation that required every person and every municipality to destroy and prevent the establishment of designated pests. If they failed, the provincial government could carry out the necessary measures and charge the costs to the landowner or municipality.\nIn the first year of the rat control program, of arsenic trioxide were spread throughout 8,000 buildings on farms along the Saskatchewan border. However, in 1953 the much safer and more effective rodenticide warfarin was introduced to replace arsenic. Warfarin is an anticoagulant that was approved as a drug for human use in 1954 and is much safer to use near humans and other large animals than arsenic. By 1960, the number of rat infestations in Alberta had dropped to below 200 per year. In 2002, the province finally recorded its first year with zero rat infestations, and from 2002 to 2007 there were only two infestations found. After an infestation of rats in the Medicine Hat landfill was found in 2012, the province's rat-free status was questioned, but provincial government rat control specialists brought in excavating machinery, dug out, shot, and poisoned 147 rats in the landfill, and no live rats were found thereafter. In 2013, the number of rat infestations in Alberta dropped to zero again. Alberta defines an infestation as two or more rats found at the same location, since a single rat cannot reproduce. About a dozen single rats enter Alberta in an average year and are killed by provincial rat control specialists before they can reproduce.\nOnly zoos, universities, and research institutes are allowed to keep caged rats in Alberta, and possession of unlicensed rats, including fancy rats by anyone else is punishable by a penalty of up to C$5,000 or up to 60 days in jail.\nThe adjacent and similarly landlocked province of Saskatchewan initiated a rat control program in 1972, and has managed to reduce the number of rats in the province substantially, although they have not been eliminated. The Saskatchewan rat control program has considerably reduced the number of rats trying to enter Alberta.\nNew Zealand.\nFirst arriving before 1800 (perhaps on James Cook's vessels), brown rats pose a serious threat to many of New Zealand's native wildlife. Rat eradication programmes within New Zealand have led to rat-free zones on offshore islands and even on fenced \"ecological islands\" on the mainland. Before an eradication effort was launched in 2001, the sub-Antarctic Campbell Island had the highest population density of brown rats in the world.\nDiseases.\nSimilar to other rodents, brown rats may carry a number of pathogens, which can result in disease, including Weil's disease, rat bite fever, cryptosporidiosis, viral hemorrhagic fever, Q fever and hantavirus pulmonary syndrome. In the United Kingdom, brown rats are an important reservoir for \"Coxiella burnetii,\" the bacterium that causes Q fever, with seroprevalence for the bacteria found to be as high as 53% in some wild populations.\nThis species can also serve as a reservoir for \"Toxoplasma gondii\", the parasite that causes toxoplasmosis, though the disease usually spreads from rats to humans when domestic cats feed on infected brown rats. The parasite has a long history with the brown rat, and there are indications that the parasite has evolved to alter an infected rat's perception to cat predation, making it more susceptible to predation and increasing the likelihood of transmission.\nSurveys and specimens of brown rat populations throughout the world have shown this species is often associated with outbreaks of trichinosis, but the extent to which the brown rat is responsible in transmitting \"Trichinella\" larvae to humans and other synanthropic animals is at least somewhat debatable. \"Trichinella pseudospiralis\", a parasite previously not considered to be a potential pathogen in humans or domestic animals, has been found to be pathogenic in humans and carried by brown rats.\nThey can also be responsible for transmitting \"Angiostrongylus\" larvae to humans by eating raw or undercooked snails, slugs, molluscs, crustaceans, water and/or vegetables contaminated with them.\nBrown rats are sometimes mistakenly thought to be a major reservoir of bubonic plague, a possible cause of the Black Death. However, the bacterium responsible, \"Yersinia pestis\", is commonly endemic in only a few rodent species and is usually transmitted zoonotically by rat fleas\u2014common carrier rodents today include ground squirrels and wood rats. However, brown rats may suffer from plague, as can many nonrodent species, including dogs, cats, and humans. During investigations of the plague epidemic in San Francisco in 1907, &gt;1% of collected rats were infected with \"Y. pestis.\" The original carrier for the plague-infected fleas thought to be responsible for the Black Death was the black rat, and it has been hypothesized that the displacement of black rats by brown rats led to the decline of bubonic plague. This theory has, however, been deprecated, as the dates of these displacements do not match the increases and decreases in plague outbreaks.\nDuring the COVID-19 pandemic, one study of New York City sewer rats showed that 17 percent of the city's brown rat population had become infected with SARS-CoV-2.\nIn captivity.\nUses in science.\nSelective breeding of white-marked rats rescued from being killed in a now-outlawed sport called rat baiting has produced the pink-eyed white laboratory rat. Like mice, these rats are frequently subjects of medical, psychological and other biological experiments, and constitute an important model organism. This is because they grow quickly to sexual maturity and are easy to keep and to breed in captivity. When modern biologists refer to \"rats\", they almost always mean \"Rattus norvegicus\".\nAs pets.\nThe brown rat is kept as a pet in many parts of the world. Australia, the United Kingdom, and the United States are just a few of the countries that have formed fancy rat associations similar in nature to the American Kennel Club, establishing standards, orchestrating events, and promoting responsible pet ownership.\nThe many different types of domesticated brown rats include variations in coat patterns, as well as the style of the coat, such as Hairless or Rex, and more recently developed variations in body size and structure, including dwarf and tailless fancy rats.\nWorking rats.\nA working rat is a rat trained for specific tasks as a working animal. In many cases, working rats are domesticated brown rats. However, other species, notably the Gambian pouched rat, have been trained to assist humans.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nOverviews\n\"Rattus norvegicus\" genome and use as model animal"}
{"id": "56110", "revid": "32990417", "url": "https://en.wikipedia.org/wiki?curid=56110", "title": "Impossible object", "text": "Type of optical illusion\nAn impossible object (also known as an impossible figure or an undecidable figure) is a type of optical illusion that consists of a two-dimensional figure which is instantly and naturally understood as representing a projection of a three-dimensional object but cannot exist as a solid object. Impossible objects are of interest to psychologists, mathematicians and artists without falling entirely into any one discipline.\nNotable examples.\nNotable impossible objects include:\nExplanations.\nImpossible objects can be unsettling because of our natural desire to interpret 2D drawings as three-dimensional objects. This is why a drawing of a Necker cube would most likely be seen as a cube, rather than \"two squares connected with diagonal lines\", \"a square surrounded by irregular planar figures\" or any other planar figure. Looking at different parts of an impossible object makes one reassess the 3D nature of the object, which confuses the mind.\nIn most cases the impossibility becomes apparent after viewing the figure for a few seconds. However, the initial impression of a 3D object remains even after it has been contradicted. There are also more subtle examples of impossible objects where the impossibility does not become apparent spontaneously and it is necessary to consciously examine the geometry of the implied object to determine that it is impossible.\nRoger Penrose wrote about describing and defining impossible objects mathematically using the algebraic topology concept of cohomology.\nHistory.\nAn early example of an impossible object comes from \"Apolin\u00e8re Enameled\", a 1916 advertisement painted by Marcel Duchamp. It depicts a girl painting a bed-frame with white enamelled paint, and deliberately includes conflicting perspective lines, to produce an impossible object. To emphasise the deliberate impossibility of the shape, a piece of the frame is missing.\nSwedish artist Oscar Reutersv\u00e4rd was one of the first to deliberately design many impossible objects. He has been called \"the father of impossible figures\". In 1934, he drew the Penrose triangle, some years before the Penroses. In Reutersv\u00e4rd's version, the sides of the triangle are broken up into cubes.\nIn 1956, British psychiatrist Lionel Penrose and his son, mathematician Roger Penrose, submitted a short article to the \"British Journal of Psychology\" titled \"Impossible Objects: A Special Type of Visual Illusion\". This was illustrated with the Penrose triangle and Penrose stairs. The article referred to Escher, whose work had sparked their interest in the subject, but not Reutersv\u00e4rd, of whom they were unaware. The article was published in 1958.\nFrom the 1930s onwards, Dutch artist M. C. Escher produced many drawings featuring paradoxes of perspective gradually working towards impossible objects. In 1957, he produced his first drawing containing a true impossible object: \"Cube with Magic Ribbons\". He produced many further drawings featuring impossible objects, sometimes with the entire drawing being an impossible object. \"Waterfall\" and \"Belvedere\" are good examples of impossible constructions. His work did much to draw the attention of the public to impossible objects.\nSome contemporary artists are also experimenting with impossible figures, for example, Jos de Mey, Shigeo Fukuda, Sandro del Prete, Istv\u00e1n Orosz (Utisz), Guido Moretti, Tam\u00e1s F. Farkas, Mathieu Hamaekers, and Kokichi Sugihara.\nConstructed impossible objects.\nAlthough possible to represent in two dimensions, it is not geometrically possible for such an object to exist in the physical world. However, some models of impossible objects have been constructed, such that when they are viewed from a very specific point, the illusion is maintained. Rotating the object or changing the viewpoint breaks the illusion, and therefore many of these models rely on forced perspective or having parts of the model appearing to be further or closer than they actually are.\nThe notion of an \"interactive impossible object\" is an impossible object that can be viewed from any angle without breaking the illusion.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56111", "revid": "1005449", "url": "https://en.wikipedia.org/wiki?curid=56111", "title": "David Stirling", "text": "Scottish World War II officer, and founder of the Special Air Service\nLieutenant-Colonel Sir Archibald David Stirling, (15 November 1915 \u2013 4 November 1990) was a Scottish officer in the British Army and the founder and creator of the Special Air Service (SAS). Under his leadership, the SAS carried out hit-and-run raids behind the Axis lines of the North African campaign. He saw active service during the Second World War until he was captured in January 1943. He spent the rest of the war in captivity, despite making several attempts to escape.\nStirling left the Regular Army in 1947. He settled in Rhodesia (now Zimbabwe) and founded the Capricorn Africa Society, which aimed to fight racial discrimination in Africa, but Stirling's preference for a limited, elitist voting franchise over universal suffrage limited the movement's appeal. He subsequently formed various private military companies and was linked with a failed attempt to overthrow the Libyan leader Muammar Gaddafi in the early 1970s. He also attempted to organise efforts to undermine trades unionism and to overthrow the British government, none of which made significant headway. He was made a Knight Bachelor in 1990, and died later the same year.\nEarly life.\nThe National Army Museum records that Stirling was born and raised in Keir House, Perthshire, into an aristocratic Scottish family with a proud military heritage. However, \"The Scotsman\" newspaper of 20 November 1915 and \"The Stirling Observer\" of 23 November 1915 both published announcements that \"The Hon Mrs Keir gave birth to a son on Monday 15th at 15 Cambridge Square, London\". He was the son of Brigadier-General Archibald Stirling, of Keir, and Margaret Fraser, daughter of Simon Fraser, the Lord Lovat (a descendant of Charles II). Simon Fraser, 15th Lord Lovat was a first cousin. His paternal grandparents were Sir William Stirling-Maxwell, 9th Baronet and Lady Anna Maria Leslie-Melville.\nStirling was educated in England at the Catholic boarding school Ampleforth College alongside his elder brother Bill Stirling. He was part of the Ampleforth Officer Training Corps. He briefly attended Trinity College, Cambridge, before being \"sent down\" (i.e. expelled) for 28 transgressions of which the master of the college asked him to select three which would be the \"least offensive to his mother\". He then went to Paris to unsuccessfully attempt to become an artist.\nSecond World War.\nStirling was commissioned into the Scots Guards on 24 July 1937. When the Second World War broke out in September 1939, Stirling was in Montana, USA, working as a cattle-rancher until returning to Britain on SS \"Manhattan\", from New York City to Southampton on 16 September 1939.\nFounding of the SAS.\nIn June 1940, he volunteered for the new No. 8 (Guards) Commando under Lieutenant-Colonel Robert Laycock, which became part of Force Z (later named \"Layforce\"). On 1 February 1941, Layforce sailed for the Middle East, in support of the capture of Rhodes, but were soon disbanded after suffering heavy casualties in the Battle of Crete and the Battle of the Litani River. Stirling remained convinced that due to the mechanised nature of war, a small team of highly trained soldiers with the advantage of surprise could attack several targets from the desert in a single night.\nBelieving that taking his idea up the chain of command was unlikely to work, Stirling decided to go straight to the top. On crutches following a parachuting accident, he stealthily entered Middle East headquarters in Cairo (under, through, or over a fence) in an effort to see Commander-in-Chief, Middle East Command General Sir Claude Auchinleck. Spotted by guards, Stirling abandoned his crutches and entered the building, only to come face-to-face with an officer with whom he had previously fallen out. Retreating rapidly, he entered the office of the deputy chief of staff, Major General Neil Ritchie. Stirling explained his plan to Ritchie, immediately after which Ritchie persuaded Auchinleck to allow Stirling to form a new special operations unit. The unit was given the deliberately misleading name \"L Detachment, Special Air Service Brigade\" to reinforce Dudley Clarke's deception of a parachute brigade existing in North Africa.\nSAS operations.\nStirling's new special operations unit was, at the outset, short of equipment (particularly tents and related gear) when the unit set up at Kibrit Air Base. The first operation of the new SAS was to steal from a nearby well-equipped New Zealand regiment various supplies including tents, bedding, tables, chairs and a piano. After at least four trips, they had a well-stocked camp.\nAfter a brief period of training, an initial attempt at attacking a German airfield by parachute landing on 16 November 1941 in support of Operation Crusader proved to be disastrous for the unit. Of the original 55 men, some 34 were killed, wounded or captured far from the target, after being blown off course or landing in the wrong area, during one of the biggest storms to hit the region. Escaping only with the help of the Long Range Desert Group (LRDG) \u2013 who were designated to pick up the unit after the attack \u2013 Stirling agreed that approaching by land under the cover of night would be safer and more effective than parachuting. As quickly as possible he organised raids on ports using this simple method, bluffing through checkpoints at night using the language skills of some of his soldiers.\nUnder Stirling's leadership, the Lewes bomb, the first hand-held dual explosive and incendiary device, was invented by Jock Lewes. American Jeeps, which were able to deal with the harsh desert terrain better than other transport, were cut down, adapted and fitted with Vickers K machine guns fore and aft. Stirling also pioneered the use of small groups to escape detection. Finding it difficult to lead from the rear, Stirling often led from the front, his SAS units driving through enemy airfields in the Jeeps to shoot up aircraft and crew.\nThe first Jeep-borne airfield raid occurred soon after acquiring the first batch of Jeeps in June 1942, when Stirling's SAS group attacked the Italian-held Bagush airfield along with two other Axis airfields all in the same night. After returning to Cairo, Stirling collected a consignment of more Jeeps for further airfield raids. His biggest success was on the night of 26\u201327 July 1942 when his SAS squadron, armed with 18 jeeps, raided the Sidi Haneish landing strip and destroyed 37 Axis aircraft (mostly bombers and heavy transport) for the loss of two men killed. After a drive through the desert, evading enemy patrols and aircraft, Stirling and his men reached the safety of their advance camp at Qaret Tartura on the edge of the Qattara Depression. He was promoted to lieutenant-colonel in September 1942.\nIn North Africa, in the 15 months before Stirling's capture, the SAS had destroyed over 250 aircraft on the ground, dozens of supply dumps, wrecked railways and telecommunications, and had put hundreds of enemy vehicles out of action. \nField Marshal Bernard Montgomery said \"The boy Stirling is quite mad, quite, quite mad. However, in a war there is often a place for mad people.\".\nCapture by Germans.\nThese hit-and-run operations eventually proved Stirling's undoing; he was captured during one in Tunisia by the Germans in January 1943 having been dubbed \"The Phantom Major\" by Field Marshal Erwin Rommel. Although Stirling escaped from the Germans, he was subsequently re-captured by the Italian III Armored Group \"Cavalleggeri di Monferrato\" and the Italians took great delight in the embarrassment this caused their German allies. He made four further escape attempts, before he was sent to Colditz Castle, where he remained as a prisoner for the rest of the war. He arrived on 20 August 1944 and was given the task of setting up the Colditz British Intelligence Unit by a Stay-Behind Order (SBO) which was in place in the area. Following Stirling's capture, Paddy Mayne took command of the SAS.\nPost-war activities.\nStirling transferred to the Regular Army Reserve of Officers in 1947. Stirling was granted the honorary rank of Lt. Col as a reservist, a rank that he retained on his retirement in 1965. \nIn Africa.\nStirling was the founder of the Capricorn Africa Society, promoting freedom from racial discrimination in Africa. Founded in 1949, while much of Africa was still under colonial rule, it had its high point at the 1956 conference at Salima, a social event which sought to enable whites \"to relate to Africans on the basis of something approximating social equality\". However, because of his opposition to universal suffrage, preferring a qualified and very elitist voting franchise, educated Africans were divided on it and it attracted insufficient support. Consequently, the society's attempt to deal with the problem of different levels of social development in a non-racial way was ineffective, although it received surprising validation when the South African Communist Party used Stirling's multi-racial elitist model for its 1955 \"Congress Alliance\" with the African National Congress of South Africa. Stirling resigned as Chairman of the Society in 1959.\nLibel action.\nIn September 1967 Len Deighton wrote an article in \"The Sunday Times Magazine\" about Operation Snowdrop, a raid led by Stirling. The following year Stirling was awarded \"substantial damages\" in a libel action about the article.\nMercenary and arms dealer.\nAfter the war, Stirling organised deals to provide British weapons and military personnel to other countries, such as Saudi Arabia, for various privatised foreign policy operations. Along with several associates, Stirling formed Watchguard International Ltd, initially with offices in Sloane Street (where the Chelsea Hotel later opened), latterly in South Audley Street in Mayfair.\nBusiness was chiefly with the Gulf States. He was linked, along with Denys Rowley, to a failed attempt to overthrow the Libyan ruler Muammar Gaddafi in 1970 or 1971. Stirling was the founder of \u201cprivate military company\u201d KAS International, also known as KAS Enterprises.\nWatchguard International Ltd was a private military company, registered in Jersey in 1965 by Stirling and John Woodhouse. Woodhouse's first assignment was to go to Yemen to report on the state of the royalist forces when a cease-fire was declared. At the same time Stirling was cultivating his contacts in the Iranian government and exploring the chances of obtaining work in Africa. The company operated in Zambia and in Sierra Leone, providing training teams and advising on security matters, but its founders' maverick ways of doing business caused its eventual downfall. Woodhouse resigned as Director of Operations after a series of disagreements and Stirling ceased to take an active part in 1972.\nGreat Britain 75.\nIn mid-1970s, Stirling became increasingly worried that an \"undemocratic event\" would occur and decided to organise a private army to overthrow the government. He created an organisation called Great Britain 75 and recruited members from the aristocratic clubs in Mayfair; these were mainly ex-military men, and often former SAS members. The plan was that in the event of civil unrest resulting in the breakdown of normal Government operations, they would take over its running. He described this in detail in an interview from 1974, part of which is featured in Adam Curtis's documentary \"The Mayfair Set\", episode 1: \"Who Pays Wins\".\nIn August 1974, before Stirling was ready to go public with GB75, the pacifist magazine \"Peace News\" obtained and published his plans. His biographer Alan Hoe disputed the newspaper's disparaging portrayal of Stirling as a right-wing 'Colonel Blimp'.\nUndermining trade unionism.\nDuring the mid to late 1970s, Stirling created a secret organisation designed to undermine trade unionism from within. He recruited like-minded individuals from within the trade union movement, with the express intention that they should cause as much trouble during conferences as permissible. One such member was Kate Losinska, who was Head of the Civil and Public Services Association. Funding for this \"operation\" came primarily from his friend Sir James Goldsmith.\nHonours.\nStirling was awarded the Distinguished Service Order in recognition of gallant and distinguished service in the Middle East on 24 February 1942, appointed an Officer of the Order of the British Empire in recognition of gallant and distinguished service in the field on 14 November 1946 and appointed a Knight Bachelor in the 1990 New Year Honours for services to the military.\nIn 1984 the new base of the SAS was renamed Stirling Lines (from Bradbury Lines) in his honour.\nIn 2002 the SAS memorial, a statue of Stirling standing on a rock, was unveiled on the Hill of Row near his family's estate at Park of Keir. Two bronze plaques were stolen from the statue sometime around the end of May 2014. The current Laird of the Keir estate is his nephew Archie Stirling, a millionaire businessman and former Scots Guards officer.\nIn popular culture.\nStirling was depicted by Connor Swindells in the 2022 television historical drama \"\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Data related to at Wikidata"}
{"id": "56112", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=56112", "title": "Necker cube", "text": "Form of perceptual phenomena\nThe Necker cube is an optical illusion that was first published as a rhomboid in 1832 by Swiss crystallographer Louis Albert Necker. It is a simple wire-frame, two dimensional drawing of a cube with no visual cues as to its orientation, so it can be interpreted to have either the lower-left or the upper-right square as its front side.\nAmbiguity.\nThe Necker cube is an ambiguous drawing.\nEach part of the picture is ambiguous by itself, yet the human visual system picks an interpretation of each part that makes the whole consistent. The Necker cube is sometimes used to test computer models of the human visual system to see whether they can arrive at consistent interpretations of the image the same way humans do.\nHumans do not usually see an inconsistent interpretation of the cube. A cube whose edges cross in an inconsistent way is an example of an impossible object, specifically an impossible cube.\nWith the cube on the left, most people see the lower-left face as being in front most of the time. This is possibly because people view objects from above, with the top side visible, far more often than from below, with the bottom visible, so the brain \"prefers\" the interpretation that the cube is viewed from above.\nThere is evidence that by focusing on different parts of the figure, one can force a more stable perception of the cube. The intersection of the two faces that are parallel to the observer forms a rectangle, and the lines that converge on the square form a \"y-junction\" at the two diagonally opposite sides. If an observer focuses on the upper \"y-junction\" the lower left face will appear to be in front. The upper right face will appear to be in front if the eyes focus on the lower junction. Blinking while being on the second perception will probably cause you to switch to the first one.\nIt is possible to cause the switch to occur by focusing on different parts of the cube. If one sees the first interpretation on the right it is possible to cause a switch to the second by focusing on the base of the cube until the switch occurs to the second interpretation. Similarly, if one is viewing the second interpretation, focusing on the left side of the cube may cause a switch to the first.\nThe orientation of the Necker cube can also be altered by shifting the observer's point of view. When seen from apparent above, one face tends to be seen closer; and in contrast, when seen from a subjective viewpoint that is below, a different face comes to the fore.\nThe Necker cube has shed light on the human visual system. The phenomenon has served as evidence of the human brain being a neural network with two distinct equally possible interchangeable stable states. Sidney Bradford, blind from the age of ten months but regaining his sight following an operation at age 52, did not perceive the ambiguity that normal-sighted observers do, but rather perceived only a flat image.\nDuring the 1970s, undergraduates in the Psychology Department of City University, London, were provided with assignments to measure their Introversion-Extroversion orientations by the time it took for them to switch between the Front and Back perceptions of the Necker Cube.\nReferences in academia and popular culture.\nThe Necker cube is discussed to such extent in Robert J. Sawyer's 1998 science fiction novel \"Factoring Humanity\" that \"Necker\" becomes a verb, meaning to impel one's brain to switch from one perspective or perception to another.\nThe Necker cube is also used to illustrate how vampires in Peter Watts' science fiction novels \"Blindsight\" (2006) and \"Echopraxia\" (2014) have superior pattern recognition skills. One of the pieces of evidence is that vampires can see both interpretations of the Necker Cube simultaneously, which sets them apart from baseline humanity.\nCultural critic Benjamin Kirbach uses the figure of the Necker cube as the basis for what he calls \"neckerology\". Kirbach draws on concepts ranging from object-oriented ontology, speculative realism, and new materialism to show that even the average everyday objects we encounter only ever appear to us through partial aspects and profiles (what phenomenologists call \"adumbration\"). Like a Necker cube, these aspects and profiles can only become perceptible through the occlusion of other aspects and profiles that remain hidden from view. Kirbach also reveals that Necker himself, whose full name is Louis Albert Necker \"de Saussure\", is the first-cousin once-removed of renowned linguist Ferdinand de Saussure. The latter Saussure's division of the linguistic sign into \"signifier\" versus \"signified\"\u2014what he called a \"two-sided psychological entity [\"une entit\u00e9 psychique \u00e0 deux faces\"]\"\u2014is perhaps not unlike the perpetual push-and-pull of a Necker cube. \"[B]y dividing it into signifier and signified,\" Kirbach writes, \"we might say that Saussure himself 'neckerized' the sign\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nSources"}
{"id": "56114", "revid": "25212220", "url": "https://en.wikipedia.org/wiki?curid=56114", "title": "Urbanization", "text": "Process of population movement to cities\nUrbanization (or urbanisation in British English) is the population shift from rural to urban areas, the corresponding decrease in the proportion of people living in rural areas, and the ways in which societies adapt to this change. It can also mean population growth in urban areas instead of rural ones. It is predominantly the process by which towns and cities are formed and become larger as more people begin to live and work in central areas.\nAlthough the two concepts are sometimes used interchangeably, urbanization should be distinguished from urban growth. Urbanization refers to the \"proportion\" of the total national population living in areas classified as urban, whereas urban growth strictly refers to the \"absolute\" number of people living in those areas. It is predicted that by 2050, about 64% of the developing world and 86% of the developed world will be urbanized. This is predicted to generate artificial scarcities of land, lack of drinking water, playgrounds and other essential resources for most urban dwellers. The predicted urban population growth is equivalent to approximately 3 billion urbanites by 2050, much of which will occur in Africa and Asia. Notably, the United Nations has also recently projected that nearly all global population growth from 2017 to 2030 will take place in cities, with about 1.1 billion new urbanites over the next 10 years. In the long term, urbanization is expected to significantly impact the quality of life in negative ways.\nUrbanization is relevant to a range of disciplines, including urban planning, geography, sociology, architecture, economics, education, statistics, and public health. The phenomenon has been closely linked to globalization, modernization, industrialization, marketization, administrative/institutional power, and the sociological process of rationalization. Urbanization can be seen as a specific condition at a set time (e.g. the proportion of total population or area in cities or towns), or as an increase in that condition over time. Therefore, urbanization can be quantified either in terms of the level of urban development relative to the overall population, or as the rate at which the urban proportion of the population is increasing. Urbanization creates enormous social, economic and environmental challenges, which provide an opportunity for sustainability with the \"potential to use resources much less or more efficiently, to create more sustainable land use and to protect the biodiversity of natural ecosystems.\" However, current urbanization trends have shown that massive urbanization has led to unsustainable ways of living. Developing urban resilience and urban sustainability in the face of increased urbanization is at the centre of international policy in Sustainable Development Goal 11 \"Sustainable cities and communities.\"\nUrbanization is not merely a modern phenomenon, but a rapid and historic transformation of human social roots on a global scale, whereby predominantly rural culture is being rapidly replaced by predominantly urban culture. The first major change in settlement patterns was the accumulation of hunter-gatherers into villages many thousands of years ago. Village culture is characterized by common bloodlines, intimate relationships, and communal behaviour, whereas urban culture is characterized by distant bloodlines, unfamiliar relations, and competitive behaviour. This unprecedented movement of people is forecast to continue and intensify during the next few decades, mushrooming cities to sizes unthinkable only a century ago. As a result, the world urban population growth curve has up till recently followed a quadratic-hyperbolic pattern.\nHistory.\nFrom the development of the earliest cities in Indus valley civilization, Mesopotamia and Egypt until the 18th century, an equilibrium existed between the vast majority of the population who were engaged in subsistence agriculture in a rural context, and small centres of populations in the towns where economic activity consisted only of trade at markets and manufactures on a small scale. Due to the primitive and relatively stagnant state of agriculture throughout this period, the ratio of rural to urban population remained at a fixed equilibrium. However, a significant increase in the percentage of the global urban population can be traced in the 1st millennium BCE.\nWith the onset of the British Agricultural Revolution and Industrial Revolution in the late 18th century, this relationship was finally broken and an unprecedented growth in urban population took place over the course of the 19th century, both through continued migration from the countryside and due to the tremendous demographic expansion that occurred at that time. In England and Wales, the proportion of the population living in cities with more than 20,000 people jumped from 17% in 1801 to 54% in 1891. Moreover, and adopting a broader definition of urbanization, while the urbanized population in England and Wales represented 72% of the total in 1891, for other countries the figure was 37% in France, 41% in Prussia and 28% in the United States.\nAs labourers were freed up from working the land due to higher agricultural productivity they converged on the new industrial cities like Manchester and Birmingham which were experiencing a boom in commerce, trade, and industry. Growing trade around the world also allowed cereals to be imported from North America and refrigerated meat from Australasia and South America. Spatially, cities also expanded due to the development of public transport systems, which facilitated commutes of longer distances to the city centre for the working class.\nUrbanization rapidly spread across the Western world and, since the 1950s, it has begun to take hold in the developing world as well. At the turn of the 20th century, just 15% of the world's population lived in cities. According to the UN, the year 2007 witnessed the turning point when more than 50% of the world population was living in cities, for the first time in human history.\nYale University in June 2016 published urbanization data from the time period 3700 BC to 2000 AD, The data was used to make a video showing the development of cities around the world during the time period. The origins and spread of urban centres around the world were also mapped by archaeologists.\nCauses.\nUrbanization occurs either organically or planned as a result of individual, collective and state action. Living in a city can be culturally and economically beneficial since it can provide greater opportunities for access to the labour market, better education, housing, and safety conditions, and reduce the time and expense of commuting and transportation. Conditions like density, proximity, diversity, and marketplace competition are elements of an urban environment that are deemed beneficial. However, there are also harmful social phenomena that arise: alienation, stress, increased cost of living, and mass marginalization that are connected to an urban way of living. Suburbanization, which is happening in the cities of the largest developing countries, may be regarded as an attempt to balance these harmful aspects of urban life while still allowing access to a large extent of shared resources.\nIn cities, money, services, wealth and opportunities are centralized. Many rural inhabitants come to the city to seek their fortune and alter their social position. Businesses, which provide jobs and exchange capital, are more concentrated in urban areas. Whether the source is trade or tourism, it is also through the ports or banking systems, commonly located in cities, that foreign money flows into a country.\nMany people move into cities for economic opportunities, but this does not fully explain the very high recent urbanization rates in places like China and India. Rural flight is a contributing factor to urbanization. In rural areas, often on small family farms or collective farms in villages, it has historically been difficult to access manufactured goods, though the relative overall quality of life is very subjective, and may certainly surpass that of the city. Farm living has always been susceptible to unpredictable environmental conditions, and in times of drought, flood or pestilence, survival may become extremely problematic.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nThai farmers are seen as poor, stupid, and unhealthy. As young people flee the farms, the values and knowledge of rice farming and the countryside are fading, including the tradition of long kek, helping neighbours plant, harvest, or build a house. We are losing what we call Thai-ness, the values of being kind, helping each other, having mercy and gratefulness.\n\u2013 Iam Thongdee, Professor of Humanities, Mahidol University in Bangkok\nIn a New York Times article concerning the acute migration away from farming in Thailand, life as a farmer was described as \"hot and exhausting\". \"Everyone says the farmer works the hardest but gets the least amount of money\". In an effort to counter this impression, the Agriculture Department of Thailand is seeking to promote the impression that farming is \"honorable and secure\".\nHowever, in Thailand, urbanization has also resulted in massive increases in problems such as obesity. Shifting from a rural environment to an urbanized community also caused a transition to a diet that was mainly carbohydrate-based to a diet higher in fat and sugar, consequently causing a rise in obesity. City life, especially in modern urban slums of the developing world, is not immune to pestilence or climatic disturbances such as floods, yet continues to strongly attract migrants. Examples of this were the 2011 Thailand floods and 2007 Jakarta flood. Urban areas are also far more prone to violence, drugs, and other urban social problems. In the United States, industrialization of agriculture has negatively affected the economy of small and middle-sized farms and strongly reduced the size of the rural labor market.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nThese are the costs of participating in the urban economy. Your increased income is canceled out by increased expenditure. In the end, you have even less left for food. \n\u2013 Madhura Swaminathan, economist at Kolkata's Indian Statistical Institute\nParticularly in the developing world, conflict over land rights due to the effects of globalization has led to less politically powerful groups, such as farmers, losing or forfeiting their land, resulting in obligatory migration into cities. In China, where land acquisition measures are forceful, there has been far more extensive and rapid urbanization (54%) than in India (36%), where peasants form militant groups (e.g. Naxalites) to oppose such efforts. Obligatory and unplanned migration often results in the rapid growth of slums.\nThis is also similar to areas of violent conflict, where people are driven off their land due to violence.\nCities offer a larger variety of services, including specialist services not found in rural areas. These services require workers, resulting in more numerous and varied job opportunities. Elderly people may be forced to move to cities where there are doctors and hospitals that can cater to their health needs. Varied and high-quality educational opportunities are another factor in urban migration, as well as the opportunity to join, develop, and seek out social communities.\nUrbanization also creates opportunities for women that are not available in rural areas. This creates a gender-related transformation where women are engaged in paid employment and have access to education. This may cause fertility to decline. However, women are sometimes still at a disadvantage due to their unequal position in the labour market, their inability to secure assets independently from male relatives and exposure to violence.\nPeople in cities are more productive than in rural areas. An important question is whether this is due to agglomeration effects or whether cities simply attract those who are more productive. Urban geographers have shown that there exists a large productivity gain due to locating in dense agglomerations. It is thus possible that agents locate in cities in order to benefit from these agglomeration effects.\nDominant conurbation.\nThe dominant conurbation(s) of a country can get more benefits from the same things cities offer, attracting the rural population and urban and suburban populations from other cities. Dominant conurbations are quite often disproportionately large cities, but do not have to be. For instance Greater Manila is a conurbation instead of a city. Its total population of 20 million (over 20% national population) make it a primate city, but Quezon City (2.7 million), the largest municipality in Greater Manila, and Manila (1.6 million), the capital, are normal cities instead. A conurbation's dominance can be measured by output, wealth, and especially population, each expressed as a percentage of the entire country's. Greater Seoul is one conurbation that dominates South Korea. It is home to 50% of the entire national population.\nThough Greater Busan-Ulsan (15%, 8 million) and Greater Osaka (14%, 18 million) dominate their respective countries, their populations are moving to their even more dominant rivals, Seoul and Tokyo respectively.\nEconomic effects.\nAs cities develop, costs will skyrocket. This often takes the working class out of the market, including officials and employees of the local districts. For example, Eric Hobsbawm's book \"The age of revolution: 1789\u20131848\" (published 1962 and 2005) chapter 11, stated \"Urban development in our period was a gigantic process of class segregation, which pushed the new labouring poor into great morasses of misery outside the centres of government, business, and the newly specialized residential areas of the bourgeoisie. The almost universal European division into a 'good' west end and a 'poor' east end of large cities developed in this period.\" This is probably caused by the south-west wind which carries coal smoke and other pollutants down, making the western edges of towns better than the eastern ones.\nSimilar problems now affect less developed countries, as rapid development of cities makes inequality worse. The drive to grow quickly and be efficient can lead to less fair urban development. Think tanks such as the Overseas Development Institute have proposed policies that encourage labour-intensive to make use of the migration of less skilled workers. One problem these migrant workers are involved with is the growth of slums. In many cases, the rural-urban unskilled migrant workers are attracted by economic opportunities in cities. Unfortunately, they cannot find a job and or pay for houses in urban areas and have to live in slums.\nUrban problems, along with developments in their facilities, are also fuelling suburb development trends in less developed nations, though the trend for core cities in said nations tends to continue to become ever denser. Development of cities is often viewed negatively, but there are positives in cutting down on transport costs, creating new job opportunities, providing education and housing, and transportation. Living in cities permits individuals and families to make use of their closeness to workplaces and diversity. While cities have more varied markets and goods than rural areas, facility congestion, domination of one group, high overhead and rental costs, and the inconvenience of trips across them frequently combine to make marketplace competition harsher in cities than in rural areas.\nIn many developing countries where economies are growing, the growth is often random and based on a small number of industries. Youths in these nations lack access to financial services and business advisory services, cannot get credit to start a business, and have no entrepreneurial skills. Therefore, they cannot seize opportunities in these industries. Making sure adolescents have access to excellent schools and infrastructure to work in such industries and improve schools is compulsory to promote a fair society.\nEnvironmental effects.\nFurthermore, urbanization improves environmental eminence through superior facilities and standards in urban areas as compared to rural areas. Lastly, urbanization curbs pollution emissions by increasing innovations. In his 2009 book \"Whole Earth Discipline\", Stewart Brand argues that the effects of urbanization are primarily positive for the environment. First, the birth rate of new urban dwellers falls immediately to replacement rate and keeps falling, reducing environmental stresses caused by population growth. Secondly, emigration from rural areas reduces destructive subsistence farming techniques, such as improperly implemented slash and burn agriculture. Alex Steffen also speaks of the environmental benefits of increasing the urbanization level in \"Carbon Zero: Imagining Cities that can save the planet\".\nHowever, existing infrastructure and city planning practices are not sustainable. In July 2013 a report issued by the United Nations Department of Economic and Social Affairs warned that with 2.4 billion more people by 2050, the amount of food produced will have to increase by 70%, straining food resources, especially in countries already facing food insecurity due to changing environmental conditions. The mix of changing environmental conditions and the growing population of urban regions, according to UN experts, will strain basic sanitation systems and health care, and potentially cause a humanitarian and environmental disaster.\nUrban heat island.\nUrban heat islands have become a growing concern over the years. An urban heat island is formed when industrial areas absorb and retain heat. Much of the solar energy reaching rural areas is used to evaporate water from plants and soil. In cities, there are less vegetation and exposed soil. Most of the sun's energy is instead absorbed by buildings and asphalt; leading to higher surface temperatures. Vehicles, factories, and heating and cooling units in factories and homes release even more heat. As a result, cities are often warmer than other areas near them. Urban heat islands also make the soil drier and absorb less carbon dioxide from emissions. A Qatar University study found that land-surface temperatures in Doha increased annually by 0.65\u00a0\u00b0C from 2002 to 2013 and 2023.\nWater quality.\nUrban runoff, polluted water created by rainfall on impervious surfaces, is a common effect of urbanization. Precipitation from rooftops, roads, parking lots and sidewalks flows to storm drains, instead of percolating into groundwater. The contaminated stormwater in the drains is typically untreated and flows to nearby streams, rivers or coastal bays.\nEutrophication in water bodies is another effect large populations in cities have on the environment. When rain occurs in these large cities, it filters CO2 and other pollutants in the air onto the ground. These chemicals are washed directly into rivers, streams, and oceans, making water worse and damaging ecosystems in them.\nEutrophication is a process which causes low levels of oxygen in water and algal blooms that may harm aquatic life. Harmful algal blooms make dangerous toxins. They live best in nitrogen- and phosphorus-rich places which include the oceans contaminated by the aforementioned chemicals. In these ideal conditions, they choke surface water, blocking sunlight and nutrients from other life forms. Overgrowth of algal blooms makes water worse overall and disrupts the natural balance of aquatic ecosystems. Furthermore, as algal blooms die, CO2 is produced. This makes the ocean more acidic, a process called acidification.\nThe ocean's surface can absorb CO2 from the Earth's atmosphere as emissions increase with the rise in urban development. In fact, the ocean absorbs a quarter of the CO2 produced by humans. This helps to lessen the harmful effects of greenhouse gases, but it also makes the ocean more acidic. A drop in pH the prevents the proper formation of calcium carbonate, which sea creatures need to build or keep shells or skeletons. This is especially true for many species of molluscs and coral. However, some species have been able to thrive in a more acidic environment.\nFood waste.\nRapid growth of communities creates new challenges in the developed world and one such challenge is an increase in food waste also known as urban food waste. Food waste is the disposal of food products that can no longer be used due to unused products, expiration, or spoilage. The increase of food waste can raise environmental concerns such as increase production of methane gases and attraction of disease vectors. Landfills are the third leading cause of the release of methane, causing a concern on its impact to our ozone and on the health of individuals. Accumulation of food waste causes increased fermentation, which increases the risk of rodent and bug migration. An increase in migration of disease vectors creates greater potential of disease spreading to humans.\nWaste management systems vary on all scales from global to local and can also be influenced by lifestyle. Waste management was not a primary concern until after the Industrial Revolution. As urban areas continued to grow along with the human population, proper management of solid waste became an apparent concern. To address these concerns, local governments sought solutions with the lowest economic impacts which meant implementing technical solutions at the very last stage of the process. Current waste management reflects these economically motivated solutions, such as incineration or unregulated landfills. Yet, a growing increase for addressing other areas of life cycle consumption has occurred from initial stage reduction to heat recovery and recycling of materials. For example, concerns for mass consumption and fast fashion have moved to the forefront of the urban consumers' priorities. Aside from environmental concerns (e.g. climate change effects), other urban concerns for waste management are public health and land access.\nHabitat fragmentation.\nUrbanization can have a large effect on biodiversity by causing a division of habitats and thereby alienation of species, a process known as habitat fragmentation. Habitat fragmentation does not destroy the habitat, as seen in habitat loss, but rather breaks it apart with things like roads and railways. This change may affect a species ability to sustain life by separating it from the environment in which it is able to easily access food, and find areas that they may hide from predation. With proper planning and management, fragmentation can be avoided by adding corridors that aid in the connection of areas and allow for easier movement around urbanized regions.\nDepending on the various factors, such as level of urbanization, both increases or decreases in \"species richness\" can be seen. This means that urbanization may be detrimental to one species but also help facilitate the growth of others. In instances of housing and building development, many times vegetation is completely removed immediately in order to make it easier and less expensive for construction to occur, thereby obliterating any native species in that area. Habitat fragmentation can filter species with limited dispersal capacity. For example, aquatic insects are found to have lower species richness in urban landscapes. The more urbanized the surrounding of habitat is, the fewer species can reach the habitat. The negative effects of urbanisation on aquatic insects can be long-lasting from the temporal perspective. Other times, such as with birds, urbanization may allow for an increase in richness when organisms are able to adapt to the new environment. This can be seen in species that may find food while scavenging developed areas or vegetation that has been added after urbanization has occurred i.e. planted trees in city areas.\nHealth and social effects.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nWhen cities don't plan for increases in population it drives up house and land prices, creating rich (ghettos) and poor ghettos. \"You get a very unequal society and that inequality is manifested where people live, in our neighbourhoods, and it means there can be less capacity for empathy and less development for all society.\"\n\u2013 Jack Finegan, Urban Programme Specialist at UN-Habitat\nIn the developing world, urbanization does not translate into a significant increase in life expectancy. Rapid urbanization has led to increased mortality from non-communicable diseases associated with lifestyle, including cancer and heart disease. Differences in mortality from contagious diseases vary depending on the particular disease and location.\nUrban health levels are on average better in comparison to rural areas. However, residents in poor urban areas such as slums and informal settlements suffer \"disproportionately from disease, injury, premature death, and the combination of ill-health and poverty entrenches disadvantage over time.\" Many of the urban poor have difficulty accessing health services due to their inability to pay for them; so they resort to less qualified and unregulated providers.\nWhile urbanization is associated with improvements in public hygiene, sanitation and access to health care, it also entails changes in occupational, dietary, and exercise patterns. It can have mixed effects on health patterns, alleviating some problems, and accentuating others.\nNutrition.\nTraditionally, rural populations have tended to eat plant-based diets rich in grains, fruits and vegetables, and with low fat content. However, rural people migrating to urban areas often shift towards diets that rely more on processed foods characterized by a higher content of meat, sugars, refined grains and fats. Urban residents typically have reduced time available for at-home food preparation combined with increased disposable income, facilitating access to convenience foods and ready-to-eat meals.\nOne such effect is the formation of food deserts. Nearly 23.5 million people in the United States lack access to supermarkets within one mile of their home. Several studies suggest that long distances to a grocery store are associated with higher rates of obesity and other health disparities.\nFood deserts in developed countries often correspond to areas with a high density of fast food chains and convenience stores that offer little to no fresh food. Urbanization has been shown to be associated with the consumption of less fresh fruits, vegetables, and whole grains and a higher consumption of processed foods and sugar-sweetened beverages. Poor access to healthy food and high intakes of fat, sugar and salt are associated with a greater risk for obesity, diabetes and related chronic disease. Overall, body mass index and cholesterol levels increase sharply with national income and the degree of urbanization.[40]\nFood deserts in the United States are most commonly found in low-income and predominately African American neighbourhoods. One study on food deserts in Denver, Colorado found that, in addition to minorities, the affected neighbourhoods also had a high proportion of children and new births. In children, urbanization is associated with a lower risk of under-nutrition but a higher risk of being overweight.\nInfections.\nUrbanization has also been linked to the spread of communicable diseases, which can spread more rapidly in the favourable environment with more people living in a smaller area. Such diseases can be respiratory infections and gastrointestinal infections. Other infections could be infections, which need a vector to spread to humans. An example of this could be dengue fever.\nAsthma.\nUrbanization has also been associated with an increased risk of asthma as well. Throughout the world, as communities transition from rural to more urban societies, the number of people affected by asthma increases. The odds of reduced rates of hospitalization and death from asthmas has decreased for children and young adults in urbanized municipalities in Brazil. This finding indicates that urbanization may have a negative impact on population health particularly affecting people's susceptibility to asthma.\nIn low and middle income countries many factors contribute to the high numbers of people with asthma. Similar to areas in the United States with increasing urbanization, people living in growing cities in low income countries experience high exposure to air pollution, which increases the prevalence and severity of asthma among these populations. Links have been found between exposure to traffic-related air pollution and allergic diseases. Children living in poor, urban areas in the United States now have an increased risk of morbidity due to asthma in comparison to other low-income children in the United States. In addition, children with croup living in urban areas have higher hazard ratios for asthma than similar children living in rural areas. Researchers suggest that this difference in hazard ratios is due to the higher levels of air pollution and exposure to environmental allergens found in urban areas.\nExposure to elevated levels of ambient air pollutants such as nitrogen dioxide (NO2), carbon monoxide (CO), and particulate matter with a diameter of less than 2.5 micrometres (PM2.5), can cause DNA methylation of CpG sites in immune cells, which increases children's risk of developing asthma. Studies have shown a positive correlation between \"Foxp3\" methylation and children's exposure to NO2, CO, and PM2.5. Furthermore, any amount of exposure to high levels of air pollution have shown long term effects on the \"Foxp3\" region.\nDespite the increase in access to health services that usually accompanies urbanization, the rise in population density negatively affects air quality ultimately mitigating the positive value of health resources as more children and young adults develop asthma due to high pollution rates. However, urban planning, as well as emission control, can lessen the effects of traffic-related air pollution on allergic diseases such as asthma.\nCrime.\nHistorically, crime and urbanization have gone hand in hand. The simplest explanation is that areas with a higher population density are surrounded by greater availability of goods. Committing crimes in urbanized areas is also more feasible. Modernization has led to more crime as well, as the modern media has raised greater awareness of the income gap between the rich and the poor. This leads to feelings of deprivation, which in turn can lead to crime. In some regions where urbanization happens in wealthier areas, a rise in property crime and a decrease in violent crime is seen.\nData shows that there is an increase in crime in urbanized areas. Some factors include per capita income, income inequality, and overall population size. There is also a smaller association between unemployment rate, police expenditures and crime. The presence of crime also has the ability to produce more crime. These areas have less social cohesion and therefore less social control. This is evident in the geographical regions that crime occurs in. As most crime tends to cluster in city centres, the further the distance from the centre of the city, the lower the occurrence of crimes are.\nMigration is also a factor that can increase crime in urbanized areas. People from one area are displaced and forced to move into an urbanized society. Here they are in a new environment with new norms and social values. This can lead to less social cohesion and more crime.\nPhysical activity.\nAlthough urbanization tends to produce more negative effects, one positive effect that urbanization has impacted is an increase in physical activity in comparison to rural areas. Residents of rural areas and communities in the United States have higher rates of obesity and engage in less physical activity than urban residents. Rural residents consume a higher percent of fat calories and are less likely to meet the guidelines for physical activity and more likely to be physically inactive. In comparison to regions within the United States, the west has the lowest prevalence of physical \"inactivity\" and the south has the highest prevalence of physical \"inactivity\". Metropolitan and large urban areas across all regions have the highest prevalence of physical activity among residents.\nBarriers such as geographic isolation, busy and unsafe roads, and social stigmas lead to decreased physical activity in rural environments. Faster speed limits on rural roads prohibits the ability to have bike lanes, sidewalks, footpaths, and shoulders along the side of the roads. Less developed open spaces in rural areas, like parks and trails, suggest that there is lower walkability in these areas in comparison to urban areas. Many residents in rural settings have to travel long distances to utilize exercise facilities, taking up too much time in the day and deterring residents from using recreational facilities to obtain physical activity. Additionally, residents of rural communities are traveling further for work, decreasing the amount of time that can be spent on leisure physical activity and significantly decreases the opportunity to partake in active transportation to work.\nNeighbourhoods and communities with nearby fitness venues, a common feature of urbanization, have residents that partake in increased amounts of physical activity. Communities with sidewalks, street lights, and traffic signals have residents participating in more physical activity than communities without those features. Having a variety of destinations close to where people live, increases the use of active transportation, such as walking and biking. Active transportation is also enhanced in urban communities where there is easy access to public transportation due to residents walking or biking to transportation stops.\nIn a study comparing different regions in the United States, opinions across all areas were shared that environmental characteristics like access to sidewalks, safe roads, recreational facilities, and enjoyable scenery are positively associated with participation in leisure physical activity. Perceiving that resources are nearby for physical activity increases the likelihood that residents of all communities will meet the guidelines and recommendations for appropriate physical activity. Specific to rural residents, the safety of outdoor developed spaces and convenient availability to recreational facilities matters most when making decisions on increasing physical activity. In order to combat the levels of inactivity in rural residents, more convenient recreational features, such as the ones discussed in this paragraph, need to be implemented into rural communities and societies.\nMental health.\nUrbanization factors that contribute to mental health can be thought of as factors that affect the individual and factors that affect the larger social group. At the macro, social group level, changes related to urbanization are thought to contribute to social disintegration and disorganization. These macro factors contribute to social disparities which affect individuals by creating perceived insecurity. Perceived insecurity can be due problems with the physical environment, such as issues with personal safety, or problems with the social environment, such as a loss of positive self-concepts from negative events. Increased stress is a common individual psychological stressor that accompanies urbanization and is thought to be due to perceived insecurity. Changes in social organization, a consequence of urbanization, are thought to lead to reduced social support, increased violence, and overcrowding. It is these factors that are thought to contribute to increased stress.\nA 2004 study of 4.4 million Swedish residents found that .\nChanging forms.\nDifferent forms of urbanization can be classified depending on the style of architecture and planning methods as well as the historic growth of areas.\nIn cities of the developed world urbanization traditionally exhibited a concentration of human activities and settlements around the downtown area, the so-called \"in-migration\". In-migration refers to migration from former colonies and similar places. The fact that many immigrants settle in impoverished city centres led to the notion of the \"peripheralization of the core\", which simply describes that people who used to be at the periphery of the former empires now live right in the centre.\nRecent developments, such as inner-city redevelopment schemes, mean that new arrivals in cities no longer necessarily settle in the centre. In some developed regions, the reverse effect, originally called counter urbanization has occurred, with cities losing population to rural areas, and is particularly common for richer families. This has been possible because of improved communications and has been caused by factors such as the fear of crime and poor urban environments. It has contributed to the phenomenon of shrinking cities experienced by some parts of the industrialized world.\nRural migrants are attracted by the possibilities that cities can offer, but often settle in shanty towns and experience extreme poverty. The inability of countries to provide adequate housing for these rural migrants is related to overurbanization, a phenomenon in which the rate of urbanization grows more rapidly than the rate of economic development, leading to high unemployment and high demand for resources. In the 1980s, this was attempted to be tackled with the urban bias theory which was promoted by Michael Lipton.\nMost of the urban poor in developing countries unable to find work can spend their lives in insecure, poorly paid jobs. According to research by the Overseas Development Institute pro-poor urbanization will require labour-intensive growth, supported by labour protection, flexible land use regulation and investments in basic services.'\nSuburbanization.\nWhen the residential area shifts outward, this is called suburbanization. A number of researchers and writers suggest that suburbanization has gone so far to form new points of concentration outside the downtown both in developed and developing countries such as India. This networked, poly-centric form of concentration is considered by some emerging pattern of urbanization. It is called variously edge city (Garreau, 1991), network city (Batten, 1995), postmodern city (Dear, 2000), or exurb, though the latter term now refers to a less dense area beyond the suburbs. Los Angeles is the best-known example of this type of urbanization. In the United States, this process has reversed as of 2011, with \"re-urbanization\" occurring as \"suburban flight\" due to chronically high transport costs.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n...the most important class conflict in the poor countries of the world today is not between labour and capital. Nor is it between foreign and national interests. It is between rural classes and urban classes. The rural sector contains most of the poverty and most of the low-cost sources of potential advance; but the urban sector contains most of the articulateness, organization, and power. So the urban classes have been able to win most of the rounds of the struggle with the countryside... \n\u2013 Michael Lipton, author of urban bias theory\nPlanned urbanization.\nUrbanization can be planned urbanization or organic. Planned urbanization, i.e.: planned community or the garden city movement, is based on an advance plan, which can be prepared for military, aesthetic, economic or urban design reasons. Examples can be seen in many ancient cities; although with exploration came the collision of nations, which meant that many invaded cities took on the desired planned characteristics of their occupiers. Many ancient organic cities experienced redevelopment for military and economic purposes, new roads carved through the cities, and new parcels of land were cordoned off serving various planned purposes giving cities distinctive geometric designs. UN agencies prefer to see urban infrastructure installed before urbanization occurs. Landscape planners are responsible for landscape infrastructure (public parks, sustainable urban drainage systems, greenways etc.) which can be planned before urbanization takes place, or afterwards to revitalize an area and create greater livability within a region. Concepts of control of the urban expansion are considered in the American Institute of Planners.\nAs population continues to grow and urbanize at unprecedented rates, new urbanism and smart growth techniques are implemented to create a transition into developing environmentally, economically, and socially sustainable cities. Additionally, a more well-rounded approach articulates the importance to promote participation of non-state actors, which could include businesses, research and non-profit organizations and, most importantly, local citizens. Smart Growth and New Urbanism's principles include walkability, mixed-use development, comfortable high-density design, land conservation, social equity, and economic diversity. Mixed-use communities work to fight gentrification with affordable housing to promote social equity, decrease automobile dependency to lower use of fossil fuels, and promote a localized economy. Walkable communities have a 38% higher average GDP per capita than less walkable urban metros (Leinberger, Lynch). By combining economic, environmental, and social sustainability, cities will become equitable, resilient, and more appealing than urban sprawl that overuses land, promotes automobile use, and segregates the population economically.\nUrbanization throughout the world.\nPresently, most countries in the world are urbanized, with the global urbanization average numbering 56.2% in 2020. However, there are great differences between some regions; the nations of Europe, the Middle East, the Americas and East Asia are predominantly urbanized. Meanwhile, two large belts (from central to eastern Africa, and from central to southeast Asia) of very lowly urbanized countries exist, as seen on the map here. These labeled countries are among the least urbanized.\nAs of 2022, urbanization rates are over 80% in the United States, Canada, Mexico, Brazil, Argentina, Chile, Japan, Australia, the United Kingdom, France, Finland, Denmark, Israel, Spain and South Korea. South America is the most urbanized continent in the world, accounting for more than 80% of its total population living in urban areas. It is also the only continent where the urbanization rate is over 80%.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56115", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=56115", "title": "Necker cube illusion", "text": ""}
{"id": "56116", "revid": "45200258", "url": "https://en.wikipedia.org/wiki?curid=56116", "title": "Salting the earth", "text": "Ancient symbolic ritual\nSalting the earth, or sowing with salt, is the ritual of spreading salt on the sites of cities razed by conquerors. It originated as a curse on re-inhabitation in the ancient Near East and became a well-established folkloric motif in the Middle Ages. The best-known example is the salting of Shechem as narrated in the Biblical Book of Judges 9:45. The supposed salting of Carthage is not supported by historical evidence.\nCities.\nThe custom of purifying or consecrating a destroyed city with salt and cursing anyone who dared to rebuild it was widespread in the ancient Near East, but historical accounts are unclear as to what the sowing of salt meant in that process. In the case of Shechem, various commentaries explain it as:\n...a covenantal curse, a means of ensuring desolation, a ritual to avert the vengeance of the shades of the slaughtered, a purification of the site preparatory to rebuilding, or a preparation for final destruction under the herem ritual.\nAncient Near East.\nVarious Hittite and Assyrian texts speak of ceremonially strewing salt, minerals, or plants (weeds, \"cress\", or \"kudimmu\", which are associated with salt and desolation) over destroyed cities, including Hattusa, Taidu, Arinna, Hunusa, Irridu, and Susa. The Book of Judges (9:45) says that Abimelech, the judge of the Israelites, sowed his own capital, Shechem, with salt, c.\u20091050 BC, after quelling a revolt against him. This may have been part of a \u1e25\u0113rem ritual (see Salt in the Bible).\nCarthage.\nAt least as early as 1863, various texts have claimed that the Roman general Scipio Aemilianus plowed over and sowed the city of Carthage with salt after defeating it in the Third Punic War (146 BC), sacking it, and enslaving the survivors. The salting was probably modeled on the story of Shechem. Though ancient sources mention symbolically drawing a plow over various cities and salting them, none mention Carthage in particular. The salting story entered the academic literature in Bertrand Hallward's chapter in the first edition of the \"Cambridge Ancient History\" (1930), and was widely accepted as factual. However, there are no ancient sources for it and it is now considered apocryphal.\nPalestrina.\nWhen Pope Boniface VIII destroyed Palestrina in 1299, he ordered that it be plowed \"following the old example of Carthage in Africa\", and salted. \"I have run the plough over it, like the ancient Carthage of Africa, and I have had salt sown upon it\u00a0...\" The text is not clear as to whether he thought Carthage was salted. Later accounts of other saltings in the destructions of medieval Italian cities are now rejected as unhistorical: Padua by Attila (452), perhaps in a parallel between Attila and the ancient Assyrians; Milan by Frederick Barbarossa (1162); and Semifonte by the Florentines (1202).\nJerusalem.\nThe English epic poem \"Siege of Jerusalem\" (c.\u20091370) recounts that Titus commanded the sowing of salt on the Temple, but this episode is not found in Josephus's account.\nSpanish Empire.\nIn Spain and the Spanish Empire, salt was poured onto the land owned by a convicted traitor (often one who was executed and his head placed on a \"picota\", or pike, afterwards) after his house was demolished.\nPortugal.\nThis was done in Portugal as well. The last known event of this sort was the destruction of the Duke of Aveiro's palace in Lisbon in 1759, due to his participation in the T\u00e1vora affair (a conspiracy against King Joseph I of Portugal). His palace was demolished and his land was salted. A stone memorial now perpetuates the memory of the shame of the Duke, where it is written:\nIn this place were put to the ground and salted the houses of Jos\u00e9 Mascarenhas, stripped of the honours of Duque de Aveiro and others\u00a0... Put to Justice as one of the leaders of the most barbarous and execrable upheaval that\u00a0... was committed against the most royal and sacred person of the Lord Joseph I. In this infamous land nothing may be built for all time.\nBrazil.\nIn the Portuguese colony of Brazil, the leader of the Inconfid\u00eancia Mineira, Tiradentes, was sentenced to death and his house was \"razed and salted, so that never again be built up on the floor,\u00a0... and even the floor will rise up a standard by which the memory is preserved (preserving) the infamy of this heinous offender\u00a0...\" He suffered further indignities, being hanged and quartered, his body parts carried to various parts of the country where his fellow revolutionaries had met, and his children deprived of their property and honor.\nLegends.\nAn ancient legend recounts that Odysseus feigned madness by yoking a horse and an ox to his plow and sowing salt.\nFootnotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56117", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=56117", "title": "Shareholder rights plan", "text": "Defense against a takeover of a company\nA shareholder rights plan, colloquially known as a \"poison pill\", is a type of defensive tactic used by a corporation's board of directors against a takeover.\nIn the field of mergers and acquisitions, shareholder rights plans were devised in the early 1980s to prevent takeover bids by limiting a shareholder's right to negotiate a price for the sale of shares directly.\nTypically, such a plan gives shareholders the right to buy more shares at a discount if one shareholder buys a certain percentage or more of the company's shares. The plan could be triggered, for instance, if any one shareholder buys 20% of the company's shares, at which point every other shareholder will have the right to buy a new issue of shares at a discount. If all other shareholders can buy more shares at a discount, such purchases would dilute the bidder's interest, and the bid cost would rise substantially. Knowing that such a plan could be activated, the bidder could be discouraged from taking over the corporation without the board's approval, and would first negotiate with the board to revoke the plan.\nThe plan can be issued by the board of directors as an \"option\" or a \"warrant\" attached to existing shares, and it can only be revoked at the board's discretion.\nHistory.\nThe poison pill was invented by mergers and acquisitions lawyer Martin Lipton of Wachtell, Lipton, Rosen &amp; Katz in 1982, as a response to tender-based hostile takeovers. Poison pills became popular during the early 1980s in response to the wave of takeovers by corporate raiders such as T. Boone Pickens and Carl Icahn. The term \"poison pill\" derives its original meaning from a poison pill physically carried by various spies throughout history, a pill which was taken by the spies if they were discovered to eliminate the possibility of being interrogated by an enemy.\nIt was reported in 2001 that since 1997, for every company with a poison pill which successfully resisted a hostile takeover, there were 20 companies with poison pills that accepted takeover offers. The trend since the early 2000s has been for shareholders to vote against poison pill authorization since poison pills are designed to resist takeovers, whereas from the point of view of a shareholder, takeovers can be financially rewarding.\nSome have argued that poison pills are detrimental to shareholder interests because they perpetuate existing management. For instance, Microsoft originally made an unsolicited bid for Yahoo!, but subsequently dropped the bid after Yahoo! CEO Jerry Yang threatened to make the takeover as difficult as possible unless Microsoft raised the price to US$37 per share. One Microsoft executive commented, \"They are going to burn the furniture if we go hostile. They are going to destroy the place.\" Yahoo has had a shareholders rights plan in place since 2001. Analysts suggested that Microsoft's raised offer of $33 per share was already too expensive, and that Yang was not bargaining in good faith, which later led to several shareholder lawsuits and an aborted proxy fight from Carl Icahn. Yahoo's stock price plunged after Microsoft withdrew the bid, and Jerry Yang faced a backlash from stockholders that eventually led to his resignation.\nPoison pills saw a resurgence of popularity in 2020 as a result of the coronavirus pandemic. As stock prices plummeted due to the pandemic, various companies turned to shareholder rights plans to defend against opportunistic takeover offers. In March 2020, 10 U.S. companies adopted new poison pills, setting a new record.\nThe Twitter Board of Directors unanimously enacted a shareholder rights plan in 2022 following an unsolicited purchase offer from Elon Musk. The purchase took place regardless in October 2022.\nOverview.\nIn publicly held companies, there are various \"poison pill\" methods to deter takeover bids. Takeovers by soliciting proxies against the board or by acquiring a controlling block of shares and using the associated votes to get elected to the board. Once in control of the board, the bidder can manage the target. Currently, the most common type of takeover defence is a shareholder rights plan. Because the board of directors of the company can redeem or otherwise eliminate a standard poison pill, it does not typically preclude a proxy fight or other takeover attempts not accompanied by an acquisition of a significant block of the company's stock. It can, however, prevent shareholders from entering into certain agreements that can assist in a proxy fight, such as an agreement to pay another shareholder's expenses. In combination with a staggered board of directors, however, a shareholder rights plan can be a defense.\nThe goal of a shareholder rights plan is to force a bidder to negotiate with the target's board and not directly with the shareholders. The effects are twofold:\nCommon types of poison pills.\nPreferred stock plan.\nThe target issues a large number of new shares, often preferred shares, to existing shareholders. These new shares usually have severe redemption provisions, such as allowing them to be converted into a large number of common shares if a takeover occurs. This immediately dilutes the percentage of the target owned by the acquirer and makes it more expensive to acquire 50% of the target's stock.\nFlip-in.\nA \"flip-in\" permits shareholders, except for the acquirer, to purchase additional shares at a discount. This provides investors with instantaneous profits. Using this type of poison pill also dilutes shares held by the acquiring company, making the takeover attempt more expensive and more difficult.\nFlip-over.\nA \"flip-over\" enables stockholders to purchase the acquirer's shares after the merger at a discounted rate. For example, a shareholder may gain the right to buy the stock of its acquirer, in subsequent mergers, at a two-for-one rate.\nBack-end rights plan.\nUnder this scenario, the target company re-phases all its employees' stock-option grants to ensure they immediately become vested if the company is taken over. Many employees can then exercise their options and then dump the stocks. With the release of the \"golden handcuffs\", many discontented employees may quit immediately after having cashed in their stock options. This poison pill is designed to create an exodus of talented employees, reducing the corporate value as a target. In many high-tech businesses, attrition of talented human resources may result in a diluted or empty shell being left behind for the new owner.\nFor instance, PeopleSoft guaranteed its customers in June 2003 that if it were acquired within two years, presumably by its rival Oracle, and product support were reduced within four years, its customers would receive a refund of between two and five times the fees they had paid for their PeopleSoft software licenses. While the acquisition ultimately prevailed, the hypothetical cost to Oracle was valued at as much as US$1.5 billion.\nVoting plan.\nIn a voting plan, a company will charter preferred stock with superior voting rights over that of common shareholders. If an unfriendly bidder acquired a substantial quantity of the target firm's voting common stock, it then still would not be able to exercise control over its purchase. For example, ASARCO established a voting plan in which 99% of the company's common stock would only harness 16.5% of the total voting power.\nIn addition to these pills, a \"dead-hand\" provision allows only the directors who introduce the poison pill to remove it (for a set period after they have been replaced), thus potentially delaying a new board's decision to sell a company.\nConstraints and legal status.\nThe legality of poison pills had been unclear when they were first put to use in the early 1980s. However, the Delaware Supreme Court upheld poison pills as a valid instrument of takeover defense in its 1985 decision in \"Moran v. Household International, Inc.\" However, many jurisdictions other than the U.S. have held the poison pill strategy as illegal, or place restraints on their use.\nCanada.\nIn Canada, almost all shareholder's rights plans are \"chewable,\" meaning they contain a permitted bid concept such that a bidder who is willing to conform to the requirements of a permitted bid can acquire the company by take-over bid without triggering a flip-in event. Shareholder rights plans in Canada are also weakened by the ability of a hostile acquirer to petition the provincial securities regulators to have the company's pill overturned. Generally, the courts will overturn the pill to allow shareholders to decide whether they want to tender to a bid for the company. However, the company may be allowed to maintain it for long enough to run an auction to see if a white knight can be found. A notable Canadian case before the securities regulators in 2006 involved the poison pill of Falconbridge Ltd. which at the time was the subject of a friendly bid from Inco and a hostile bid from Xstrata plc, which was a 20% shareholder of Falconbridge. Xstrata applied to have Falconbridge's pill invalidated, citing among other things that the Falconbridge had had its pill in place without shareholder approval for more than nine months and that the pill stood in the way of Falconbridge shareholders accepting Xstrata's all-cash offer for Falconbridge shares. Despite similar facts with previous cases in which securities regulators had promptly taken down pills, the Ontario Securities Commission ruled that Falconbridge's pill could remain in place for a further limited period as it had the effect of sustaining the auction for Falconbridge by preventing Xstrata increasing its ownership and potentially obtaining a blocking position that would prevent other bidders from obtaining 100% of the shares.\nUnited Kingdom.\nIn the United Kingdom, poison pills are not allowed under the Takeover Panel rules. The rights of public shareholders are protected by the Panel on a case-by-case, principles-based regulatory regime. Raids have helped bidders win targets such as BAA plc and AWG plc when other bidders were considering emerging at higher prices. If these companies had poison pills, they could have prevented the raids by threatening to dilute the positions of their hostile suitors if they exceeded the statutory levels (often 10% of the outstanding shares) in the rights plan. The London Stock Exchange itself is another example of a company that has seen significant stakebuilding by a hostile suitor, in this case the NASDAQ. The LSE's ultimate fate is currently up in the air, but NASDAQ's stake is sufficiently large that it is essentially impossible for a third party bidder to make a successful offer to acquire the LSE.\nEurope.\nTakeover law is still evolving in continental Europe, as individual countries slowly fall in line with requirements mandated by the European Commission. Stakebuilding is commonplace in many continental takeover battles such as Scania AB. Formal poison pills are quite rare in continental Europe, but national governments hold golden shares in many \"strategic\" companies such as telecom monopolies and energy companies. Governments have also served as \"poison pills\" by threatening potential suitors with negative regulatory developments if they pursue the takeover. Examples of this include Spain's adoption of new rules for the ownership of energy companies after E.ON of Germany made a hostile bid for Endesa and France's threats to punish any potential acquiror of Groupe Danone.\nOther takeover defenses.\nPoison pill is sometimes used more broadly to describe other types of takeover defenses that involve the target taking some action. Although the broad category of takeover defenses (more commonly known as \"shark repellents\") includes the traditional shareholder rights plan poison pill. Other anti-takeover protections include:\nShareholder input.\nA minuscule number of companies are giving shareholders a say on poison pills. As of June 15, 2009, 21 companies that had adopted or extended a poison pill had publicly disclosed they plan to put the poison pill to a shareholder vote within a year. That was up from 2008's full year total of 18, and was the largest number ever reported since the early 1980s, when the pill was invented.\nEffect.\nWhile there is some evidence that takeover protections allow managers to negotiate a higher purchase price, overall, they reduce firm productivity.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56118", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=56118", "title": "Tierra Amarilla, New Mexico", "text": "Tierra Amarilla is a census-designated place in and the county seat of Rio Arriba County, New Mexico, United States.\n\"Tierra Amarilla\" is Spanish for \"Yellow Earth\". The name refers to clay deposits found in the Chama River Valley and used by Native American peoples. Tewa and Navajo toponyms for the area also refer to the yellow clay.\nHistory.\nThere is evidence of 5000 years of habitation in the Chama River Valley including pueblo sites south of Abiquiu. The area served as a trade route for peoples in the present-day Four Corners region and the Rio Grande Valley. Navajos later used the valley as a staging area for raids on Spanish settlements along the Rio Grande. Written accounts of the Tierra Amarilla locality by pathfinding Spanish friars in 1776 described it as suitable for pastoral and agricultural use. The route taken by the friars from Santa Fe to California became the Spanish Trail. During the Californian Gold Rush the area became a staging point for westward fortune seekers.\nTierra Amarilla Grant.\nThe Tierra Amarilla Land Grant was created in 1832 by the Mexican government for Manuel Martinez and settlers from Abiquiu. The land grant encompassed a more general area than the contemporary community known as \"Tierra Amarilla\". The grant holders were unable to maintain a permanent settlement due to \"raids by Utes, Navajos and Jicarilla Apaches\" until early in the 1860s. In 1860 the United States Congress confirmed the land grant as a private grant, rather than a community grant, due to mistranslated and concealed documents. Although a land patent for the grant required the completion of a geographical survey before issuance, some of Manuel Martinez' heirs began to sell the land to Anglo speculators. In 1880 Thomas Catron sold some of the grant to the Denver and Rio Grande Railway for the construction of their San Juan line and a service center at Chama. By 1883 Catron had consolidated the deeds he held for the whole of the grant sans the original villages and their associated fields. In 1950, the descendants of the original grant holders' court petitions to reclaim communal land were rebuked.\nRio Arriba's county seat.\nIn 1866 the United States Army established Camp Plummer just south of Los Ojos (established in 1860) to rein in already decreased Native American activity on the grant. The military encampment was deserted in 1869. Las Nutrias, the site of the contemporary community, was founded nearby c.1862. The first post office in Las Nutrias was established in 1866 and bore the name \"Tierra Amarilla\", as did the present one which was established in 1870 after an approximately two-year absence. In 1877 a U.S. Army lieutenant described the village as \"the center of the Mexican population of northwestern New Mexico\". The territorial legislature located Rio Arriba's county seat in Las Nutrias and renamed the village in 1880. The Denver and Rio Grande Railway's 1881 arrival at Chama, about ten miles to the north, had profound effects on the development of the region by bringing the area out of economic and cultural isolation.\nWhen Tierra Amarilla was designated as the county seat the villagers set about building a courthouse. This structure was demolished to make way for the present one, which was built in 1917 and gained notoriety fifty years later when it was the location of a gunfight between land rights activists and authorities. The neoclassical design by Isaac Rapp is now on the National Register of Historic Places.\nCourthouse raid.\nThe Alianza Federal de Mercedes, led by Reies Tijerina, raided the Rio Arriba County Courthouse in 1967. Attempting to make a citizen's arrest of the district attorney \"to bring attention to the unscrupulous means by which government and Anglo settlers had usurped Hispanic land grant properties,\" an armed struggle in the courthouse ensued resulting in Tijerina and his group fleeing to the south with two prisoners as hostages. Eulogio Salazar, a prison guard, was shot and Daniel Rivera, a sheriff's deputy, was badly injured. The National Guard, FBI and New Mexico State Police successfully pursued Tijerina, who was sentenced to less than three years.\nGeography.\nThe Brazos Cliffs are a prominent nearby landmark and attraction. Also nearby are the artificial Heron Lake and El Vado Lake.\nTierra Amarilla's elevation is 7,524 feet above sea level.\nLayout.\nThe settlement is situated in a cluster of villages along U.S. Route 84 and the Chama River. The layout of the villages, including the one that became Tierra Amarilla, do not follow the urban planning principles of the Laws of the Indies.\nClimate.\nTierra Amarilla has a humid continental climate (K\u00f6ppen \"Dfb\") with very cold, snowy, though generally sunny winters, and summers featuring very warm to hot afternoons and cold to cool mornings. During the winter, mornings are frigid, with as many as 26.7 falling to or below , although maxima top freezing on all but nineteen afternoons during an average winter. The coldest temperature has been on January 6, 1971. Snowfall is much heavier than in more populated parts of New Mexico as Tierra Amarilla is located on a western slope rather than in a valley: the annual average is with a maximum of in January 1997 and a maximum annual total of between July 1996 and June 1997. The maximum snow depth has been on 30 November 1983.\nThe spring season sees the sunniest weather of all and steadily warming temperatures, although over the year as a whole 224.9 mornings fall to or below freezing, with four freezes to be expected as late as June. The summer, although seeing diurnal temperature ranges of over , is the wettest period due to frequent monsoonal thunderstorms. The wettest months have been September 1927 and August 1967 which each saw of precipitation, the wettest calendar year 1986 with , and the driest 1956 with .\nDemographics.\n&lt;templatestyles src=\"US Census population/styles.css\"/&gt;\nTierra Amarilla has the ZIP code of 87575. The ZIP Code Tabulation Area for ZIP Code 87575 had a population of 750 at the 2000 census.\nEducation.\nIt is within the Chama Valley Independent Schools school district. The two schools in the community are: Tierra Amarilla Elementary School (PreK-6) and Escalante Middle/High School (7\u201312).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56119", "revid": "9444589", "url": "https://en.wikipedia.org/wiki?curid=56119", "title": "Takeover", "text": "Purchase of a company by another company\nIn business, a takeover is the purchase of one company (the \"target\") by another (the \"acquirer\" or \"bidder\"). In the UK, the term refers to the acquisition of a public company whose shares are publicly listed, in contrast to the acquisition of a private company.\nManagement of the target company may or may not agree with a proposed takeover, and this has resulted in the following takeover classifications: friendly, hostile, reverse or back-flip. Financing a takeover often involves loans or bond issues which may include junk bonds as well as a simple cash offer. It can also include shares in the new company.\nTakeover types.\nFriendly takeover.\nA \"friendly takeover\" is an acquisition which is approved by the management of the target company. Before a bidder makes an offer for another company, it usually first informs the company's board of directors.\nIn a private company, because the shareholders and the board are usually the same people or closely connected with one another, private acquisitions are usually friendly. If the shareholders agree to sell the company, then the board is usually of the same mind or sufficiently under the orders of the equity shareholders to cooperate with the bidder. This point is not relevant to the UK concept of takeovers, which always involve the acquisition of a public company.\nA \"bear hug\" is an unsolicited takeover bid which is so generous that the shareholders of the target company are very likely to submit, accepting the offer.\nHostile takeover.\nA \"hostile takeover\" allows a bidder to take over a target company whose management is unwilling to agree to a merger or takeover. The party who initiates a hostile takeover bid approaches the shareholders directly, as opposed to seeking approval from officers or directors of the company. A takeover is considered \"hostile\" if the target company's board rejects the offer, and if the bidder continues to pursue it, or the bidder makes the offer directly after having announced its firm intention to make an offer. Development of the hostile takeover is attributed to Louis Wolfson. Hostile takeovers are relatively rare; by one estimate, only 40 takeovers (out of 3,300) in 1986 were hostile.\nA hostile takeover can be conducted in several ways. A tender offer can be made where the acquiring company makes a public offer at a fixed price above the current market price. An acquiring company can also engage in a proxy fight, whereby it tries to persuade enough shareholders, usually a simple majority, to replace the management with a new one which will approve the takeover. Another method involves quietly purchasing enough stock on the open market, known as a \"creeping tender offer\" or \"dawn raid\", to effect a change in management. In all of these ways, management resists the acquisition, but it is carried out anyway.\nIn the United States, a common defense tactic against hostile takeovers is to use section 16 of the Clayton Act to seek an injunction, arguing that section 7 of the act, which prohibits acquisitions where the effect may be substantially to lessen competition or to tend to create a monopoly, would be violated if the offeror acquired the target's stock.\nThe main consequence of a bid being considered hostile is practical rather than legal. If the board of the target cooperates, the bidder can conduct extensive due diligence into the affairs of the target company, providing the bidder with a comprehensive analysis of the target company's finances. In contrast, a hostile bidder will only have more limited, publicly available information about the target company available, rendering the bidder vulnerable to hidden risks regarding the target company's finances. Since takeovers often require loans provided by banks in order to service the offer, banks are often less willing to back a hostile bidder because of the relative lack of target information which is available to them. Under Delaware law, boards must engage in defensive actions that are proportional to the hostile bidder's threat to the target company.\nA well-known example of an extremely hostile takeover was Oracle's bid to acquire PeopleSoft. As of 2018, about 1,788 hostile takeovers with a total value of US$28.86\u00a0billion had been announced.\nReverse takeover.\nA \"reverse takeover\" is a type of takeover where a private company acquires a public company. This is usually done at the instigation of the private company, the purpose being for the private company to effectively float itself while avoiding some of the expense and time involved in a conventional IPO. However, in the UK under AIM rules, a reverse takeover is an acquisition or acquisitions in a twelve-month period which for an AIM company would:\nAn individual or organization, sometimes known as a corporate raider, can purchase a large fraction of the company's stock and, in doing so, get enough votes to replace the board of directors and the CEO. With a new agreeable management team, the stock is, potentially, a much more attractive investment, which might result in a price rise and a profit for the corporate raider and the other shareholders. A well-known example of a reverse takeover in the United Kingdom was Darwen Group's 2008 takeover of Optare plc. This was also an example of a back-flip takeover (see below) as Darwen was rebranded to the more well-known Optare name.\nBackflip takeover.\nA \"backflip takeover\" is any sort of takeover in which the acquiring company turns itself into a subsidiary of the purchased company. This type of takeover can occur when a larger but less well-known company purchases a struggling company with a very well-known brand. Examples include:\nTakeover financing.\nFunding.\nOften a company acquiring another pays a specified amount for it. This money can be raised in a number of ways. Although the company may have sufficient funds available in its account, remitting payment entirely from the acquiring company's cash on hand is unusual. More often, it will be borrowed from a bank, or raised by an issue of bonds. Acquisitions financed through debt are known as leveraged buyouts, and the debt will often be moved down onto the balance sheet of the acquired company. The acquired company then has to pay back the debt. This is a technique often used by private equity companies. The debt ratio of financing can go as high as 80% in some cases. In such a case, the acquiring company would only need to raise 20% of the purchase price.\nLoan note alternatives.\nCash offers for public companies often include a \"loan note alternative\" that allows shareholders to take a part or all of their consideration in loan notes rather than cash. This is done primarily to make the offer more attractive in terms of taxation. A conversion of shares into cash is counted as a disposal that triggers a payment of capital gains tax, whereas if the shares are converted into other securities, such as loan notes, the tax is rolled over.\nTakeover deals.\nAll-share deals.\nA takeover, particularly a reverse takeover, may be financed by an all-share deal. The bidder does not pay money, but instead issues new shares in itself to the shareholders of the company being acquired. In a reverse takeover the shareholders of the company being acquired end up with a majority of the shares in, and so control of, the company making the bid. The company has managerial rights.\nAll-cash deals.\nIf a takeover of a company consists of simply an offer of an amount of money per share (as opposed to all or part of the payment being in shares or loan notes), then this is an all-cash deal.\nThe purchasing company can source the necessary cash in a variety of ways, including existing cash resources, loans, or a separate issue of company shares.\nMechanics.\nIn the United Kingdom.\nTakeovers in the UK (meaning acquisitions of public companies only) are governed by the City Code on Takeovers and Mergers, also known as the 'City Code' or 'Takeover Code'. The rules for a takeover can be found in what is primarily known as 'The Blue Book'. The Code used to be a non-statutory set of rules that was controlled by city institutions on a theoretically voluntary basis. However, as a breach of the Code brought such reputational damage and the possibility of exclusion from city services run by those institutions, it was regarded as binding. In 2006, the Code was put onto a statutory footing as part of the UK's compliance with the European Takeover Directive (2004/25/EC).\nThe Code requires that all shareholders in a company should be treated equally. It regulates when and what information companies must and cannot release publicly in relation to the bid, sets timetables for certain aspects of the bid, and sets minimum bid levels following a previous purchase of shares.\nIn particular:\nThe Rules Governing the Substantial Acquisition of Shares, which used to accompany the Code and which regulated the announcement of certain levels of shareholdings, have now been abolished, though similar provisions still exist in the Companies Act 1985.\nStrategies.\nThere are a variety of reasons why an acquiring company may wish to purchase another company. Some takeovers are \"opportunistic\" \u2013 the target company may simply be very reasonably priced for one reason or another and the acquiring company may decide that in the long run, it will end up making money by purchasing the target company. The large holding company Berkshire Hathaway has profited well over time by purchasing many companies opportunistically in this manner.\nOther takeovers are \"strategic\" in that they are thought to have secondary effects beyond the simple effect of the profitability of the target company being added to the acquiring company's profitability. For example, an acquiring company may decide to purchase a company that is profitable and has good distribution capabilities in new areas which the acquiring company can use for its own products as well. A target company might be attractive because it allows the acquiring company to enter a new market without having to take on the risk, time and expense of starting a new division. An acquiring company could decide to take over a competitor not only because the competitor is profitable, but in order to eliminate competition in its field and make it easier, in the long term, to raise prices. Also a takeover could fulfill the belief that the combined company can be more profitable than the two companies would be separately due to a reduction of redundant functions.\nExecutive compensation.\nTakeovers may also benefit from a principal-agent problem associated with top executive compensation. For example, it is fairly easy for a top executive to reduce the price of their company's stock due to information asymmetry. The executive can accelerate accounting of expected expenses, delay accounting of expected revenue, engage in off-balance-sheet transactions to make the company's profitability appear temporarily poorer, or simply promote and report severely conservative (i.e. pessimistic) estimates of future earnings. Such seemingly adverse earnings news will be likely to (at least temporarily) reduce the company's stock price. (This is again due to information asymmetries since it is more common for top executives to do everything they can to window dress their company's earnings forecasts.) There are typically very few legal risks to being 'too conservative' in one's accounting and earnings estimates.\nA reduced share price makes a company an easier takeover target. When the company gets bought out (or taken private)\u00a0\u2013 at a dramatically lower price\u00a0\u2013 the takeover artist gains a windfall from the former top executive's actions to surreptitiously reduce the company's stock price. This can represent tens of billions of dollars (questionably) transferred from previous shareholders to the takeover artist. The former top executive is then rewarded with a golden handshake for presiding over the fire sale that can sometimes be in the hundreds of millions of dollars for one or two years of work. This is nevertheless an excellent bargain for the takeover artist, who will tend to benefit from developing a reputation of being very generous to parting top executives. This is just one example of a principal-agent problem, otherwise regarded as perverse incentive.\nSimilar issues occur when a publicly held asset or non-profit organization undergoes privatization. Top executives often reap tremendous monetary benefits when a government owned or non-profit entity is sold to private hands. Just as in the example above, they can facilitate this process by making the entity appear to be in financial crisis. This perception can reduce the sale price (to the profit of the purchaser) and make non-profits and governments more likely to sell. It can also contribute to a public perception that private entities are more efficiently run, reinforcing the political will to sell off public assets.\nDebt for equity.\nTakeovers also tend to substitute debt for equity. In a sense, any government tax policy of allowing for deduction of interest expenses but not of dividends, has essentially provided a substantial subsidy to takeovers. It can punish more-conservative or prudent management that does not allow their companies to leverage themselves into a high-risk position. High leverage will lead to high profits if circumstances go well but can lead to catastrophic failure if they do not. This can create substantial negative externalities for governments, employees, suppliers and other stakeholders.\nGolden share.\nCorporate takeovers occur frequently in the United States, Canada, United Kingdom, France and Spain. They happen only occasionally in Italy because larger shareholders (typically controlling families) often have special board voting privileges designed to keep them in control. They do not happen often in Germany because of the dual board structure, nor in Japan because companies have interlocking sets of ownerships known as keiretsu, nor in the People's Republic of China because many publicly listed companies are state owned.\nTactics against hostile takeover.\nThere are quite a few tactics or techniques which can be used to deter a hostile takeover.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56120", "revid": "50654061", "url": "https://en.wikipedia.org/wiki?curid=56120", "title": "Hispanic", "text": "Spanish-speaking cultures and persons\nThe term Hispanic () refers to people, cultures, or countries related to Spain, the Spanish language, or broadly. In some contexts, especially within the United States, \"Hispanic\" is used as an ethnic or meta-ethnic term.\nThe term commonly applies to Spaniards and Spanish-speaking (Hispanophone) populations and countries in Hispanic America (the continent) and Hispanic Africa (Equatorial Guinea and the disputed territory of Western Sahara), which were formerly part of the Spanish Empire due to colonization mainly between the 16th and 20th centuries. The cultures of Hispanophone countries outside Spain have been influenced as well by the local pre-Hispanic cultures or other foreign influences.\nThere was also Spanish influence in the former Spanish East Indies, including the Philippines, Marianas, and other nations. However, Spanish is not a predominant language in these regions and, as a result, their inhabitants are not usually considered Hispanic.\nHispanic culture is a set of customs, traditions, beliefs, and art forms in music, literature, dress, architecture, cuisine, and other cultural fields that are generally shared by peoples in Hispanic regions, but which can vary considerably from one country or territory to another. The Spanish language is the main cultural element shared by Hispanic peoples.\nTerminology.\nThe term Hispanic derives from the Latin word , the adjectival derivation of \"\", which means of the Iberian Peninsula and possibly Celtiberian origin. In English the word is attested from the 16th century (and in the late 19th century in American English).\nThe words \"Spain\", \"Spanish\", and \"Spaniard\" are of the same etymology as \"\", ultimately.\n was the Latin name given to a person from Hispania during Roman rule. The ancient Roman Hispania, which roughly comprised what is currently called the Iberian Peninsula, included the contemporary states of Spain, Portugal, parts of France, Andorra, and the British Overseas Territory of Gibraltar. In English, the term \"Hispano-Roman\" is sometimes used. The Hispano-Romans were composed of people from many different Indigenous tribes, in addition to colonists from Italia. Some famous (plural of \"\") and were the emperors Trajan, Marcus Aurelius, Hadrian, Theodosius I and Magnus Maximus, the poets Marcus Annaeus Lucanus, Martial and Prudentius, the philosophers Seneca the Elder and Seneca the Younger, and the usurper Maximus of Hispania. A number of these men, such as Trajan, Hadrian and others, were in fact descended from Roman colonial families.\nHere follows a comparison of several terms related to \"Hispanic\":\n\"Hispania\" was divided into two provinces: Hispania Citerior and Hispania Ulterior. In 27 BC, Hispania Ulterior was divided into two new provinces, Hispania Baetica and Hispania Lusitania, while Hispania Citerior was renamed Hispania Tarraconensis. This division of Hispania explains the usage of the singular and plural forms (Spain, and The Spains) used to refer to the peninsula and its kingdoms in the Middle Ages.\nBefore the marriage of Queen Isabella I of Castile and King Ferdinand II of Aragon in 1469, the four Christian kingdoms of the Iberian Peninsula\u2014the Kingdom of Portugal, the Crown of Aragon, the Crown of Castile, and the Kingdom of Navarre\u2014were collectively called The Spains. This revival of the old Roman concept in the Middle Ages appears to have originated in Proven\u00e7al, and was first documented at the end of the 11th century. In the Council of Constance, the four kingdoms shared one vote.\nThe terms \"Spain\" and \"the Spains\" were not interchangeable. Spain was a geographic territory, home to several kingdoms (Christian and Muslim), with separate governments, laws, languages, religions, and customs, and was the historical remnant of the Hispano-Gothic unity. Spain was not a political entity until much later, and when referring to the Middle Ages, one should not be confounded with the nation-state of today. The term \"The Spains\" referred specifically to a collective of juridico-political units, first the Christian kingdoms, and then the different kingdoms ruled by the same king. Illustrative of this fact is the historical ecclesiastical title of Primate of the Spains, traditionally claimed by the Archbishop of Braga, a Portuguese prelate.\nWith the \"Decretos de Nueva Planta\", Philip V started to organize the fusion of his kingdoms that until then were ruled as distinct and independent, but this unification process lacked a formal and juridic proclamation.\nAlthough colloquially and literally the expression \"King of Spain\" or \"King of the Spains\" was already widespread, it did not refer to a unified nation-state. It was only in the constitution of 1812 that was adopted the name \"Espa\u00f1as\" (Spains) for the Spanish nation and the use of the title of \"king of the Spains\". The constitution of 1876 adopts for the first time the name \"Spain\" for the Spanish nation and from then on the kings would use the title of \"king of Spain\".\nThe expansion of the Spanish Empire between 1492 and 1898 brought thousands of Spanish migrants to the conquered lands, who established settlements, mainly in the Americas, but also in other distant parts of the world (as in the Philippines, the lone Spanish territory in Asia), producing a number of multiracial populations. Today, the varied populations of these places, including those with Spanish ancestry, are also designated as Hispanic.\nDefinitions in ancient Rome.\nThe Latin gentile adjectives that belong to Hispania are \"Hispanus, Hispanicus,\" and \"Hispaniensis.\" A Hispanus is someone who is a native of Hispania with no foreign parents, while children born in Hispania of Roman parents were \"Hispanienses\". \"Hispaniensis\" means 'connected in some way to Hispania', as in \"Exercitus Hispaniensis\" ('the Spanish army') or \"mercatores Hispanienses\" ('Spanish merchants'). \"Hispanicus\" implies 'of' or 'belonging to' Hispania or the Hispanus or of their fashion as in \"gladius Hispanicus\". The gentile adjectives were not ethnolinguistic but derived primarily on a geographic basis, from the toponym Hispania as the people of Hispania spoke different languages, although Titus Livius (Livy) said they could all understand each other, not making clear if they spoke dialects of the same language or were polyglots.\nThe first recorded use of an anthroponym derived from the toponym Hispania is attested in one of the five fragments, of Ennius in 236 BC who wrote \"Hispane, non Romane memoretis loqui me\" (\"Remember that I speak like a Hispanic not a Roman\") as having been said by a native of Hispania.\nDefinitions in Portugal, Spain, the rest of Europe.\nIn Portugal, Hispanic refers to something historical related to ancient Hispania (especially the terms Hispano-Roman and Hispania) or the Spanish language and cultures shared by all the Spanish-speaking countries. Although sharing the etymology for the word (pt: ', es: '), the definition for Hispanic is different between Portugal and Spain.\nThe Royal Spanish Academy (Spanish: Real Academia Espa\u00f1ola, RAE), the official royal institution responsible for regulating the Spanish language defines the terms \"'\" and \"'\" (which in Spain have slightly different meanings) as:\n\"Hispano\":\n\"Hisp\u00e1nico\"\nThe modern term to identify Portuguese and Spanish territories under a single nomenclature is \"Iberian\", and the one to refer to cultures derived from both countries in the Americas is \"Iberian-American\". These designations can be mutually recognized by people in Portugal and Brazil. \"Hispanic\" is totally void of any self-identification in Brazil, and quite to the contrary, serves the purpose of marking a clear distinction in relation to neighboring countries' culture. Brazilians may identify as Latin Americans, but refute being considered Hispanics because their language and culture are neither part of the Hispanic cultural sphere, nor Spanish-speaking world.\nIn Spanish, the term \"'\", as in \"'\", refers to the people of Spanish origin who live in the Americas and to a relationship to Spain or to the Spanish language. There are people in Hispanic America that are not of Spanish origin, such as Amerindians- the original people of these areas, as well as Africans and people with origins from other parts of Europe.\nLike in Portugal, in the rest of Europe (and wider world) the concept of 'Hispanic' refers to historical ancient Hispania (especially the term Hispano-Roman and Hispania during the Roman Empire) or the Spanish language and cultures shared by all the Spanish-speaking countries.\nDefinitions in the United States.\nBoth \"Hispanic\" and \"Latino\" are widely used in American English for Spanish-speaking people and their descendants in the United States. While \"Hispanic\" refers to Spanish speakers overall, \"Latino\" refers specifically to people of Latin American descent. \"Hispanic\" can also be used for the people and culture of Spain as well as Latin America. While originally the term \"Hispanic\" referred primarily to the Hispanos of New Mexico within the United States, today, organizations in the country use the term as a broad catchall to refer to persons with a historical and cultural relationship with Spain regardless of race and ethnicity. The United States Census Bureau uses \"Hispanic or Latino\" to refer to \"a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race\" and states that Hispanics or Latinos can be of any race and any ancestry.\nBecause of the technical distinctions involved in defining \"race\" vs. \"ethnicity\", there is confusion among the general population about the designation of Hispanic identity. Currently, the United States Census Bureau defines six race categories:\nA 1997 notice by the U.S. Office of Management and Budget defined \"Hispanic or Latino\" persons as being \"persons who trace their origin or descent to Mexico, Puerto Rico, Cuba, Central and South America, and other Spanish cultures.\" The United States Census uses the ethnonyms \"Hispanic or Latino\" to refer to \"a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Hispanic culture or origin regardless of race.\"\nThe 2010 census asked if the person was \"Spanish/Hispanic/Latino\". The United States census uses the \"Hispanic or Latino\" to refer to \"a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race.\" The Census Bureau also explains that \"[o]rigin can be viewed as the heritage, nationality group, lineage, or country of birth of the person or the person's ancestors before their arrival in the United States. People who identify their origin as Hispanic, Latino or Spanish may be of any race.\"\nThe U.S. Department of Transportation defines \"Hispanic\" as, \"persons of Mexican, Puerto Rican, Cuban, Central or South American, or other Spanish culture or origin, regardless of race.\" This definition has been adopted by the Small Business Administration as well as by many federal, state, and municipal agencies for the purposes of awarding government contracts to minority owned businesses.\nThe Congressional Hispanic Caucus and the Congressional Hispanic Conference include representatives of Spanish and Portuguese, Puerto Rican and Mexican descent. The Hispanic Society of America is dedicated to the study of the arts and cultures of the Hispanic and Lusitanic world. The Hispanic Association of Colleges and Universities, proclaimed champions of Hispanic success in higher education, is committed to Hispanic educational success in the United States, and the Hispanic and Lusitanic world.\nThe U.S. Equal Employment Opportunity Commission encourages any individual who believes that he or she is Hispanic to self-identify as Hispanic. The United States Department of Labor \u2013 Office of Federal Contract Compliance Programs encourages the same self-identification. As a result, individuals with origins to part of the Spanish Empire may self-identify as Hispanic, because an employer may not override an individual's self-identification.\nThe 1970 census was the first time that a \"Hispanic\" identifier was used and data collected with the question. The definition of \"Hispanic\" has been modified in each successive census.\nIn a recent study, most Spanish speakers of Spanish or Hispanic American descent do not prefer the term \"Hispanic\" or \"Latino\" when it comes to describing their identity. Instead, they prefer to be identified by their country of origin. When asked if they have a preference for either being identified as \"Hispanic\" or \"Latino\", the Pew study finds that \"half (51%) say they have no preference for either term.\" Among those who do express a preference, \"'Hispanic' is preferred over 'Latino' by more than a two-to-one margin\u201433% versus 14%.\" 21% prefer to be referred to simply as \"Americans\". A majority (51%) say they most often identify themselves by their family's country of origin, while 24% say they prefer a pan-ethnic label such as Hispanic or Latino.\nCulture.\nThe Miguel de Cervantes Prize is awarded to Hispanic writers, whereas the Latin Grammy Award recognizes Hispanic musicians, and the Platino Awards as given to outstanding Hispanic films.\nMusic.\nFolk and popular dance and music also varies greatly among Hispanics. For instance, the music from Spain is a lot different from the Hispanic American, although there is a high grade of exchange between both continents. In addition, due to the high national development of the diverse nationalities and regions of Spain, there is a lot of music in the different languages of the Peninsula (Catalan, Galician and Basque, mainly). See, for instance, Music of Catalonia or Rock catal\u00e0, Music of Galicia, Cantabria and Asturias, and Basque music. Flamenco is also a very popular music style in Spain, especially in Andalusia. Spanish ballads \"romances\" can be traced in Argentina as \"milongas\", same structure but different scenarios.\nOn the other side of the ocean, Hispanic America is also home to a wide variety of music, even though \"Latin\" music is often erroneously thought of, as a single genre. Hispanic Caribbean music tends to favor complex polyrhythms of African origin. Mexican music shows combined influences of mostly European and Native American origin, while traditional Northern Mexican music\u2014norte\u00f1o and banda\u2014 polka, has influence from polka music brought by Central European settlers to Mexico which later influenced western music. The music of Hispanic Americans\u2014such as tejano music\u2014has influences in rock, jazz, R&amp;B, pop, and country music as well as traditional Mexican music such as Mariachi. Meanwhile, native Andean sounds and melodies are the backbone of Peruvian and Bolivian music, but also play a significant role in the popular music of most South American countries and are heavily incorporated into the folk music of Ecuador and the tunes of Colombia, and in Chile where they play a fundamental role in the form of the greatly followed nueva canci\u00f3n. In U.S. communities of immigrants from these countries it is common to hear these styles. Rock en espa\u00f1ol, Latin hip-hop, Salsa, Merengue, Bachata, Cumbia and Reggaeton styles tend to appeal to the broader Hispanic population, and varieties of Cuban music are popular with many Hispanics of all backgrounds.\nLiterature.\nSpanish-language literature and folklore is very rich and is influenced by a variety of countries. There are thousands of writers from many places, and dating from the Middle Ages to the present. Some of the most recognized writers are:\nSports.\nIn the majority of the Hispanic countries, association football is the most popular sport. The men's national teams of Argentina, Uruguay and Spain have won the FIFA World Cup a total six times. The Spanish La Liga is one of the most popular in the world, known for FC Barcelona and Real Madrid. Meanwhile, the Argentine Primera Divisi\u00f3n is one of the strongest leagues in the Americas.\nHowever, baseball is the most popular sport in some Central American and Caribbean countries (especially Cuba, Dominican Republic, Puerto Rico,Nicaragua and Venezuela), as well as in the diaspora in the United States. Notable Hispanic teams in early baseball are the All Cubans, Cuban Stars and New York Cubans. The Hispanic Heritage Baseball Museum recognizes Hispanic baseball personalities. Nearly 30 percent (22 percent foreign-born Hispanics) of MLB players today have Hispanic heritage.\nSeveral Hispanic sportspeople have been successful worldwide, such as Diego Maradona, Alfredo di Stefano, Lionel Messi, Diego Forl\u00e1n, Fernando Torres, Xavi, Andr\u00e9s Iniesta, Iker Casillas, Xabi Alonso (association football), Juan Manuel Fangio, Juan Pablo Montoya, Eliseo Salazar, Fernando Alonso, Marc Gen\u00e9, Carlos Sainz Sr. and Carlos Sainz Jr. (auto racing), \u00c1ngel Nieto, Dani Pedrosa, Jorge Lorenzo, Marc M\u00e1rquez, Marc Coma, Nani Roma (motorcycle racing), Emanuel Gin\u00f3bili, Pau Gasol, Marc Gasol (basketball), Julio C\u00e9sar Ch\u00e1vez, Sa\u00fal \u00c1lvarez, Carlos Monz\u00f3n (boxing), Miguel Indurain, Alberto Contador, Santiago Botero, Rigoberto Ur\u00e1n, Nairo Quintana (cycling), Roberto de Vicenzo, \u00c1ngel Cabrera, Sergio Garc\u00eda, Severiano Ballesteros, Jos\u00e9 Mar\u00eda Olaz\u00e1bal (golf), Luciana Aymar (field hockey), Yair Rodr\u00edguez, Brandon Moreno, Ilia Topuria (mixed martial arts), Rafael Nadal, Marcelo R\u00edos, Guillermo Vilas, Gabriela Sabatini, Juan Mart\u00edn del Potro (tennis).\nNotable Hispanic sports television networks are ESPN Deportes, Fox Deportes, Televisa Deportes, Azteca Deportes and TyC Sports.\nReligion.\nThe Spanish and the Portuguese took the Catholic faith to their colonies in the Americas, Africa, and Asia; Catholicism remains the predominant religion amongst most Hispanics. A small but growing number of Hispanics belong to a Protestant denomination. Hispanic Christians form the largest ethno-linguistic group among Christians in the world, about 18% of the world's Christian population are Hispanic (around 430 million).\nIn the United States, some 65% of Hispanics and Latinos report themselves Catholic and 21% Protestant, with 13% having no affiliation. A minority among the Catholics, about one in five, are charismatics. Among the Protestant, 85% are \"Born-again Christians\" and belong to Evangelical or Pentecostal churches. Among the smallest groups, less than 4%, are Jewish.\nChristianity.\nAmong the Spanish-speaking Catholics, most communities celebrate their homeland's patron saint, dedicating a day for this purpose with festivals and religious services. Some Spanish-speakers in Latin America syncretize Roman Catholicism and African or Native American rituals and beliefs. Such is the case of Santer\u00eda, popular with Afro-Cubans, which combines old African beliefs in the form of Roman Catholic saints and rituals. Other syncretistic beliefs include Spiritism and Curanderismo. In Catholic tradition, \"Our Lady of the Pillar\" is considered the Patroness of the Hispanic people and the Hispanic world.\nIslam.\nWhile a tiny minority, there are some Muslims in Latin America, in the United States, and in the Philippines. Those in the Philippines live predominantly in Bangsamoro.\nJudaism.\nThere are also Spanish-speaking Jews, most of whom are the descendants of Ashkenazi Jews who migrated from Europe (German Jews, Russian Jews, Polish Jews, etc.) to Hispanic America, particularly Argentina, Uruguay, Peru, and Cuba (Argentina is host to the third-largest Jewish population in the Western Hemisphere, after the United States and Canada) in the 19th century and following World War II. Many Spanish-speaking Jews also originate from the small communities of reconverted descendants of anusim\u2014those whose Spanish Sephardi Jewish ancestors long ago hid their Jewish ancestry and beliefs in fear of persecution by the Spanish Inquisition in the Iberian Peninsula and Ibero-America. The Spanish Inquisition led to many forced conversions of Spanish Jews.\nGenetic studies on the (male) Y-chromosome conducted by the University of Leeds in 2008 appear to support the idea that the number of forced conversions have been previously underestimated significantly. They found that twenty percent of Spanish males have Y-chromosomes associated with Sephardic Jewish ancestry. This may imply that there were more forced conversions than was previously thought.\nThere are also thought to be many Catholic-professing descendants of marranos and Spanish-speaking crypto-Jews in the Southwestern United States and scattered through Hispanic America. Additionally, there are Sephardic Jews who are descendants of those Jews who fled Spain to Turkey, Syria, and North Africa, some of whom have now migrated to Hispanic America, holding on to some Spanish/Sephardic customs, such as the Ladino language, which mixes Spanish, Hebrew, Arabic and others, though written with Hebrew and Latin characters. Ladinos were also African slaves captive in Spain held prior to the colonial period in the Americas. (See also History of the Jews in Hispanic America and List of Hispanic American Jews.)\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56121", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=56121", "title": "Market liquidity", "text": "Finance property of an asset\nIn business, economics or investment, market liquidity is a market's feature whereby an individual or firm can quickly purchase or sell an asset without causing a drastic change in the asset's price. Liquidity involves the trade-off between the price at which an asset can be sold, and how quickly it can be sold. In a liquid market, the trade-off is mild: one can sell quickly without having to accept a significantly lower price. In a relatively illiquid market, an asset must be discounted in order to sell quickly. A liquid asset is an asset which can be converted into cash within a relatively short period of time, or cash itself, which can be considered the most liquid asset because it can be exchanged for goods and services instantly at face value.\nOverview.\nA liquid asset has some or all of the following features: it can be sold rapidly, with minimal loss of value, anytime within market hours. The essential characteristic of a liquid market is that there are always ready and willing buyers and sellers. It is similar to, but distinct from, market depth, which relates to the trade-off between quantity being sold and the price it can be sold for, rather than the liquidity trade-off between speed of sale and the price it can be sold for. A market may be considered both deep and liquid if there are ready and willing buyers and sellers in large quantities.\nAn illiquid asset is an asset which is not readily salable (without a drastic price reduction, and sometimes not at any price) due to uncertainty about its value or the lack of a market in which it is regularly traded. The mortgage-related assets which resulted in the subprime mortgage crisis are examples of illiquid assets, as their value was not readily determinable despite being secured by real property. Before the crisis, they had moderate liquidity because it was believed that their value was generally known.\nSpeculators and market makers are key contributors to the liquidity of a market or asset. Speculators are individuals or institutions that seek to profit from anticipated increases or decreases in a particular market price. Market makers seek to profit by charging for the immediacy of execution: either implicitly by earning a bid/ask spread or explicitly by charging execution commissions. By doing this, they provide the capital needed to facilitate the liquidity. The risk of illiquidity does not apply only to individual investments: whole portfolios are subject to market risk. Financial institutions and asset managers that oversee portfolios are subject to what is called \"structural\" and \"contingent\" liquidity risk. Structural liquidity risk, sometimes called funding liquidity risk, is the risk associated with funding asset portfolios in the normal course of business. Contingent liquidity risk is the risk associated with finding additional funds or replacing maturing liabilities under potential, future-stressed market conditions. When a central bank tries to influence the liquidity (supply) of money, this process is known as open market operations.\nEffect on asset values.\nThe market liquidity of assets affects their prices and expected returns. Theory and empirical evidence suggest that investors require higher return on assets with lower market liquidity to compensate them for the higher cost of trading these assets. That is, for an asset with given cash flow, the higher its market liquidity, the higher its price and the lower is its expected return. In addition, risk-averse investors require higher expected return if the asset's market-liquidity risk is greater. This risk involves the exposure of the asset return to shocks in overall market liquidity, the exposure of the asset's own liquidity to shocks in market liquidity and the effect of market return on the asset's own liquidity. Here too, the higher the liquidity risk, the higher the expected return on the asset or the lower is its price.\nOne example of this is a comparison of assets with and without a liquid secondary market. The liquidity discount is the reduced promised yield or expected return for such assets, like the difference between newly issued U.S. Treasury bonds compared to off the run treasuries with the same term to maturity. Initial buyers know that other investors are less willing to buy off-the-run treasuries, so the newly issued bonds have a higher price (and hence lower yield).\nFutures.\nIn the futures markets, there is no assurance that a liquid market may exist for offsetting a commodity contract at all times. Some future contracts and specific delivery months tend to have increasingly more trading activity and have higher liquidity than others. The most useful indicators of liquidity for these contracts are the trading volume and open interest.\nThere is also dark liquidity, referring to transactions that occur off-exchange and are therefore not visible to investors until after the transaction is complete. It does not contribute to public price discovery.\nBanking.\nIn banking, liquidity is the ability to meet obligations when they come due without incurring unacceptable losses. Managing liquidity is a daily process requiring bankers to monitor and project cash flows to ensure adequate liquidity is maintained. Maintaining a balance between short-term assets and short-term liabilities is critical. For an individual bank, clients' deposits are its primary liabilities (in the sense that the bank is meant to give back all client deposits on demand), whereas reserves and loans are its primary assets (in the sense that these loans are owed to the bank, not by the bank). The investment portfolio represents a smaller portion of assets, and serves as the primary source of liquidity. Investment securities can be liquidated to satisfy deposit withdrawals and increased loan demand. Banks have several additional options for generating liquidity, such as selling loans, borrowing from other banks, borrowing from a central bank, such as the US Federal Reserve bank, and raising additional capital. In a worst-case scenario, depositors may demand their funds when the bank is unable to generate adequate cash without incurring substantial financial losses. In severe cases, this may result in a bank run.\nBanks can generally maintain as much liquidity as desired because bank deposits are insured by governments in most developed countries. A lack of liquidity can be remedied by raising deposit rates and effectively marketing deposit products. However, an important measure of a bank's value and success is the cost of liquidity. A bank can attract significant liquid funds. Lower costs generate stronger profits, more stability, and more confidence among depositors, investors, and regulators.\nStock market.\nThe market liquidity of stock depends on whether it is listed on an exchange and the level of buyer interest. The bid/ask spread is one indicator of a stock's liquidity. For liquid stocks, such as Microsoft or General Electric, the spread is often just a few pennies \u2013 much less than 1% of the price. For illiquid stocks, the spread can be much larger, amounting to a few percent of the trading price.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56122", "revid": "14423536", "url": "https://en.wikipedia.org/wiki?curid=56122", "title": "Sorghum (genus)", "text": "Genus of flowering plants\nSorghum () or broomcorn is a genus of about 25 species of flowering plants in the grass family (Poaceae). \"Sorghum bicolor\" is grown as a cereal for human consumption and as animal fodder.\nEvolution.\nPhylogeny.\nThe \"Sorghum\" genus is closely related to maize within the PACMAD clade of grasses, and more distantly to the cereals of the BOP clade such as wheat and barley.\nTaxonomy.\nThe \"Sorghum\" genus is in the grass family, Poaceae, in the subfamily Panicoideae, in the tribe Andropogoneae \u2013 the same as maize (\"Zea mays\"), big bluestem (\"Andropogon gerardi\"), and sugarcane (\"Saccharum\" spp.). Accepted species recorded include:\nSeventeen of the 25 species are native to Australia, with the range of some extending to Africa, Asia, Mesoamerica, and certain islands in the Indian and Pacific Oceans.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "56124", "revid": "43618068", "url": "https://en.wikipedia.org/wiki?curid=56124", "title": "Lakota people", "text": "Indigenous people of the Great Plains\nThe Lakota (; or ) are a Native American people. Also known as the Teton Sioux (from ), they are one of the three prominent subcultures of the Sioux people, with the Eastern Dakota (Santee) and Western Dakota (). Their current lands are in North and South Dakota. They speak \u2009\u2014\u2009the Lakota language, the westernmost of three closely related languages that belong to the Siouan language family.\nThe seven bands or \"sub-tribes\" of the Lakota are:\nNotable Lakota persons include (Sitting Bull) from the , (Touch the Clouds) from the Miniconjou; (Black Elk), (Red Cloud), and (Billy Mills), all ; (Crazy Horse) from the and Miniconjou, and (Spotted Tail) from the Brul\u00e9. Activists from the late 20th century to present include Russell Means (Ogl\u00e1la).\nHistory.\nEarly history.\nEarly Lakota history is recorded in their winter counts (Lakota: \"wan\u00edyetu w\u00f3wapi\"), pictorial calendars painted on hides, or later recorded on paper. The Battiste Good winter count records Lakota history to 900\u00a0CE when White Buffalo Calf Woman gave the Lakota people the White Buffalo Calf Pipe.\nSiouan language speakers may have originated in the lower Mississippi River region and then migrated to or originated in the Ohio Valley. They were agriculturalists and may have been part of the Mound Builder civilization during the 9th\u201312th centuries CE. Lakota legend and other sources state they originally lived near the Great Lakes: \n\"The tribes of the Dakota before European contact in the 1600s lived in the region around Lake Superior. In this forest environment, they lived by hunting, fishing, and gathering wild rice. They also grew some corn, but their locale was near the limit of where corn could be grown.\"\nIn the late 16th and early 17th centuries, Dakota-Lakota speakers lived in the upper Mississippi Region in territory now organized as the states of Minnesota, Wisconsin, Iowa, and the Dakotas. Conflicts with Anishnaabe and Cree peoples pushed the Lakota west onto the Great Plains in the mid- to late-17th century. Around 1730 Cheyenne people introduced the Lakota to horses, which they called \"\u0161u\u014bkawaka\u014b\" (\"dog [of] power/mystery/wonder\"). After they adopted horse culture, Lakota society centered on the buffalo hunt on horseback.\nIn 1660, French explorers estimated the total population of the Sioux (Lakota, Santee, Yankton, and Yanktonai) at 28,000. In 1805, the Lakota population was estimated at 8,500. In 1881, it reached 16,110. They were one of the few Native American tribes to increase in population in the 19th century, a time of widespread disease and warfare. In 2010, the Lakota population was more than 170,000, of whom about 2,000 still spoke the Lakota language (\"Lak\u021f\u00f3tiyapi\").\nAfter 1720, the Lakota branch of the Seven Council Fires split into two major sects, the Sa\u00f4ne, who moved to the Lake Traverse area on the South Dakota\u2013North Dakota\u2013Minnesota border, and the Ogl\u00e1la-Si\u010dh\u00e1\u014b\u01e7u, who occupied the James River valley. By about 1750 the Sa\u00f4ne had moved to the east bank of the Missouri River, followed 10 years later by the Ogl\u00e1la and Brul\u00e9 (Si\u010dh\u00e1\u014b\u01e7u). The large and powerful Arikara, Mandan, and Hidatsa villages had long prevented the Lakota from crossing the Missouri River.\nThe great smallpox epidemic of 1772\u20131780 destroyed three-quarters of the members of these tribes. The Lakota crossed the river into the drier, short-grass prairies of the High Plains. These newcomers were the Sa\u00f4ne, well-mounted and increasingly confident, who spread out quickly. In 1765, a Sa\u00f4ne exploring and raiding party led by Chief Standing Bear discovered the Black Hills (the \"Paha Sapa\"), then the territory of the Cheyenne. Ten years later, the Ogl\u00e1la and Brul\u00e9 also crossed the Missouri. Under pressure from the Lakota, the Cheyenne moved west to the Powder River country. The Lakota made the Black Hills their home.\nTreaties and conflicts with the United States.\nInitial United States contact with the Lakota during the Lewis and Clark Expedition of 1804\u20131806 was marked by a standoff. Lakota bands refused to allow the explorers to continue upstream, and the expedition prepared for battle, which never came.\nSome bands of Lakota became the first indigenous people to help the United States Army in an inter-tribal war west of the Missouri, during the Arikara War in 1823. In 1843, the southern Lakota attacked the village of Pawnee Chief Blue Coat near the Loup in Nebraska, killing many and burning half of the earth lodges. The next time the Lakota inflicted a blow so severe to the Pawnee would be in 1873, during the Massacre Canyon battle near Republican River.\nNearly half a century later, after the United States had built Fort Laramie without permission on Lakota and Arapaho land, it negotiated the Fort Laramie Treaty of 1851 to protect European-American travelers on the Oregon Trail. The Cheyenne and Lakota had previously attacked emigrant parties in a competition for resources, and also because some settlers had encroached on their lands. The Fort Laramie Treaty acknowledged Lakota sovereignty over the Great Plains in exchange for free passage for European Americans on the Oregon Trail for \"as long as the river flows and the eagle flies\". \nThe U.S. government did not enforce the treaty restriction against unauthorized settlement, and Lakota and other bands attacked settlers and even emigrant trains as part of their resistance to this encroachment. Public pressure increased for the U.S. Army to punish them. On September 3, 1855, 700 soldiers under U.S. Brevet Major General William S. Harney avenged the Grattan massacre, by attacking a Lakota village in Nebraska, killing about 100 men, women, and children. A series of short \"wars\" followed, and in 1862\u20131864, as Native American refugees from the \"Dakota War of 1862\" in Minnesota fled west to their allies in Montana and Dakota Territory. After the American Civil War increasing illegal settlement by whites on the Plains resulted in war again with the Lakota.\nThe Black Hills were considered sacred by the Lakota, and they objected to mining. Between 1866 and 1868 the U.S. Army fought the Lakota and their allies along the Bozeman Trail over U.S. forts built to protect miners traveling along the trail. Oglala Chief Red Cloud led his people to victory in Red Cloud's War. In 1868, the United States signed the Fort Laramie Treaty of 1868, exempting the Black Hills from all white settlement forever. But four years later gold was discovered there, and prospectors descended on the area. The Lakota attacks on settlers and miners were met by military force conducted by such army commanders as Lieutenant Colonel George Armstrong Custer. General Philip Sheridan encouraged his troops to hunt and kill the buffalo as a means of \"destroying the Indians' commissary.\"\nThe allied Lakota and Arapaho bands and the unified Northern Cheyenne were involved in much of the warfare after 1860. They fought a successful delaying action against General George Crook's army at the Battle of the Rosebud, preventing Crook from locating and attacking their camp. A week later they defeated the U.S. 7th Cavalry in 1876 at the Battle of the Little Bighorn at the Crow Indian Reservation (1868 boundaries). Custer attacked an encampment of several tribes, which was much larger than he realized. Their combined forces, led by Chief Crazy Horse, killed 258 soldiers, wiping out the entire Custer battalion and inflicting more than 50% casualties on the regiment. \nAlthough the Lakota beat Custer's army, the Lakota and their allies did not get to enjoy their victory over the U.S. Army for long. The U.S. Congress authorized funds to expand the army by 2,500 men. The reinforced U.S. Army defeated the Lakota bands in a series of battles, ending the Great Sioux War in 1877. The Lakota were eventually confined to reservations, prevented from hunting buffalo beyond those territories, and forced to accept government food distribution. They were largely dispersed throughout North and South Dakota, as well as other places around the United States.\nIn 1877, some of the Lakota bands signed a treaty that ceded the Black Hills to the United States. The nature of this treaty and its passage were controversial. The number of Lakota leaders who backed the treaty is highly disputed. Low-intensity conflicts continued in the Black Hills. Fourteen years later, Sitting Bull was killed at Standing Rock reservation on December 15, 1890. The U.S. Army attacked Spotted Elk (aka Bigfoot)'s Minicoujou band of Lakota on December 29, 1890, at Pine Ridge, killing 153 Lakota (tribal estimates are higher), including numerous women and children, in the Wounded Knee Massacre.\nReservation era.\nToday, the Lakota are found mostly in the five reservations of western South Dakota:\nLakota also live on the Fort Peck Indian Reservation in northeastern Montana, the Fort Berthold Indian Reservation of northwestern North Dakota, and several small reserves in Saskatchewan and Manitoba. During the Minnesota and Black Hills wars, their ancestors fled for refuge to \"Grandmother's [i.e. Queen Victoria's] Land\" (Canada).\nLarge numbers of Lakota live in Rapid City and other towns in the Black Hills, and in metro Denver. Lakota elders joined the Unrepresented Nations and Peoples Organization (UNPO) to seek protection and recognition for their cultural and land rights.\nGovernment.\nUnited States.\nLegally and by treaty classified as a \"domestic dependent nation\" within the United States, the federally recognized Lakota tribes are represented locally by officials elected to councils for the several reservations and communities in the Dakotas, Minnesota, and Nebraska. These tribes have government-to-government relationships with the United States federal government, primarily through the Bureau of Indian Affairs in the Department of Interior.\nAs semi-autonomous political entities, tribal governments have certain rights that are independent of state laws. For instance, they may operate Indian gaming on their reservation based on the Indian Gaming Regulatory Act of 1988. They operate with the federal government. These relationship are negotiated and contested.\nMost Lakota tribal members are also citizens of the United States. They can vote in local, state/provincial and federal elections. They are represented at the state and national level by officials elected from the political districts of their respective states and Congressional Districts.\nTribal members living both on and off the individual reservations are eligible to vote in periodic elections for that tribe. Each tribe has its own requirements for citizenship, as well its own constitution, bylaws, and elections. or articles of incorporation. Most follow a multi-member tribal council model, with a chairman or president elected at-large, directly by the voters.\nCanada.\nNine bands of Dakota and Lakota reside in Manitoba and southern Saskatchewan, with a total of 6,000 registered members. They are recognized as First Nations but are not considered \"treaty Indians\". As First Nations they receive rights and entitlements through the Crown\u2013Indigenous Relations and Northern Affairs Canada department. But because they are not recognized as treaty Indians, they did not participate in the land settlement and natural resource revenues. The Dakota rejected a $60-million land-rights settlement in 2008. \nIndependence movement.\nThe Lakota are among tribal nations that have taken actions, participated in occupations, and proposed independence movements, particularly since the era of rising activism since the mid to late 20th century. They filed land claims against the federal government for what they defined as illegal taking of the Black Hills in the nineteenth century.\nIn 1980, the Supreme Court ruled in their favor and decided in \"United States v. Sioux Nation of Indians\" to award US$122 million to eight bands of Sioux Indians as compensation for their Black Hills land claims. The Sioux have refused the money, because accepting the settlement would legally terminate their demands for return of the Black Hills. The money remains in a Bureau of Indian Affairs account, accruing compound interest. As of 2011, the account has grown to over $1 billion.\nIn September 2007, the United Nations passed a non-binding Declaration on the Rights of Indigenous Peoples. Canada, the United States, Australia, and New Zealand refused to sign.\nIn December 2007, a small group of people led by American Indian Movement activist Russell Means, under the name Lakota Freedom Delegation, traveled to Washington D.C. to announce a withdrawal of the Lakota Sioux from all treaties with the United States government. These activists had no standing under any elected tribal government.\nOfficial Lakota tribal leaders issued public responses to the effect that, in the words of Rosebud Lakota tribal chairman Rodney Bordeaux, \"We do not support what Means and his group are doing and they don't have any support from any tribal government I know of. They don't speak for us.\"\nMeans declared \"The Republic of Lakotah\", defining it as a sovereign nation with property rights over thousands of square miles in South Dakota, North Dakota, Nebraska, Wyoming and Montana. The group stated that they do not act for or represent the tribal governments \"set up by the BIA or those Lakota who support the BIA system of government\".\n\"The Lakota Freedom Delegation\" did not include any elected leaders from any of the tribes. Means had previously run for president of the Oglala Sioux tribe and twice been defeated. Several tribal governments \u2013 elected by tribal members \u2013 issued statements distancing themselves from the independence declaration. Some said that they were watching the independent movement closely. No elected tribal governments endorsed the declaration.\nCurrent activism.\nThe Lakota People made national news when NPR's \"Lost Children, Shattered Families\" investigative story aired regarding issues related to foster care for Native American children. It exposed what many critics consider to be the \"kidnapping\" of Lakota children from their homes by the state of South Dakota's Department of Social Services (D.S.S.). It was noted by NPR that over half of the children in foster care in South Dakota were of Native descent. \nLakota activists such as Madonna Thunder Hawk and Chase Iron Eyes, along with the \"Lakota People's Law Project\", have alleged that Lakota grandmothers are illegally denied the right to foster their own grandchildren. They are working to redirect federal funding away from the state of South Dakota's D.S.S. to new tribal foster care programs. This would be a historic shift away from the state's traditional control over Lakota foster children.\nA short film, \"Lakota in America\", was produced by Square. The film features Genevieve Iron Lightning, a young Lakota dancer on the Cheyenne River Reservation, one of the poorest communities in the United States. Unemployment, addiction, alcoholism, and suicide are all challenges for Lakota on the reservation.\nEthnonyms.\nThe name \"Lakota\" comes from the Lakota autonym, \"Lakota\" \"feeling affection, friendly, united, allied\". The early French historic documents did not distinguish a separate Teton division, instead grouping them with other \"Sioux of the West\", Santee and Yankton bands.\nThe names \"Teton\" and \"Tetuwan\" come from the Lakota name \"th\u00edt\u021fu\u014bwa\u014b\", the meaning of which is obscure. This term was used to refer to the Lakota by non-Lakota Sioux groups. Other derivations and spelling variations include: ti tanka, Tintonyanyan, Titon, Tintonha, Thintohas, Tinthenha, Tinton, Thuntotas, Tintones, Tintoner, Tintinhos, Ten-ton-ha, Thinthonha, Tinthonha, Tentouha, Tintonwans, Tindaw, Tinthow, Atintons, Anthontans, Atentons, Atintans, Atrutons, Titoba, Tetongues, Teton Sioux, Teeton, Ti toan, Teetwawn, Teetwans, Ti-t'-wawn, Ti-twans, Tit'wan, Tetans, Tieton, and Teetonwan.\nEarly French sources call the Lakota \"Sioux\" with an additional modifier, such as Sioux of the West, West Schious, Sioux des prairies, Sioux occidentaux, Sioux of the Meadows, Nadooessis of the Plains, Prairie Indians, Sioux of the Plain, Maskoutens-Nadouessians, Mascouteins Nadouessi, and Sioux nomades.\nToday many of the tribes continue to officially call themselves \"Sioux\". In the 19th and 20th centuries, this was the name which the US government applied to all Dakota/Lakota people. Some tribes have formally or informally adopted traditional names: the Rosebud Sioux Tribe is also known as the \"Si\u010dh\u00e1\u014b\u01e7u Oy\u00e1te\" (Brul\u00e9 Nation), and the Oglala often use the name \"Ogl\u00e1la Lak\u021f\u00f3ta Oy\u00e1te\", rather than the English \"Oglala Sioux Tribe\" or OST. The alternate English spelling of Ogallala is deprecated, even though it is closer to the correct pronunciation. \nThe Lakota have names for their own subdivisions. The Lakota also are the most western of the three Sioux groups, occupying lands in both North and South Dakota.\nReservations.\nToday, one half of all enrolled Sioux live off reservations.\nLakota reservations recognized by the U.S. government include:\nSome Lakota also live on other Sioux reservations in eastern South Dakota, Minnesota, and Nebraska:\nSeveral Lakota live on the Wood Mountain First Nation reserve, near Wood Mountain Regional Park in Saskatchewan, Canada.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
