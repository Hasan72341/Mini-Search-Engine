{"id": "47420", "revid": "10753531", "url": "https://en.wikipedia.org/wiki?curid=47420", "title": "7-Eleven", "text": "American multinational convenience store chain\n7-Eleven, Inc. is an American convenience store chain, headquartered in Irving, Texas. It is a wholly owned subsidiary of Seven-Eleven Japan, which in turn is owned by the retail holdings company Seven &amp; I Holdings, a Japanese retail holding company.\nThe chain was founded in 1927 as the Southland Ice Company, operating an ice house storefront in Dallas. Then owned by Southland Corporation, the number of convenience stores expanded and were named Tote'm Stores between 1928 and 1946. Southland Corporation changed the stores' name to 7-Eleven in 1946, reflecting expanded hours of operation (7 am to 11 pm).\nSouthland Corporation started franchising its stores in 1961; in 1973 Ito-Yokado, a Japanese supermarket chain, signed a franchisee agreement with Southland Corporation to develop 7-Eleven convenience stores in Japan. Operating the Japanese stores under Seven-Eleven Japan, Ito-Yokado acquired a 70% stake in Southland Corporation in 1991; as majority owner, it changed Southland Corporation's name to 7-Eleven, Inc. that same year, then expanded to 100% ownership in November 2005, making 7-Eleven, Inc. a wholly owned subsidiary of Seven-Eleven Japan. Ito-Yokado reorganized its collective businesses as a holding company in 2005, Seven &amp; I Holdings, with 7-Eleven, Inc. wholly held by Seven-Eleven Japan.\n7-Eleven operates, franchises and licenses roughly 85,000 stores in 20 countries and territories as of August 2024. Its stores operate under its namesake brand globally, including the United States, where it also operates as Speedway nationally but mostly in the Midwest and East Coast, and as Stripes Convenience Stores within the West South Central United States. Both Speedway and Stripes operate alongside 7-Eleven's namesake stores in several American markets. 7-Eleven also operates A-Plus locations with the name licensed from owner and fellow Dallas\u2013Fort Worth metroplex-based Energy Transfer Partners, though most of these stores have since been rebranded as standard 7-Eleven stores.\nEtymologies.\nThe company's first outlets were in Dallas, named \"Tote'm Stores\" because customers \"toted\" away their purchases. Some stores featured \"native\" totem poles in front of the store. In 1946, the chain's name was changed from \"Tote'm\" to \"7-11\" to reflect the company's new, extended hours, 7:00\u00a0a.m. to 11:00\u00a0p.m., seven days per week. In July 1999, the corporate name of the US company was changed from \"The Southland Corporation\" to \"7-Eleven Inc.\"\nSince 1968, 7-Eleven's logos have included a lowercase \"n\". The first wife of John P. Thompson Sr., the company's president during the 1960s, thought the all-capitals version seemed a little aggressive. She suggested the change \"to make the logo look more graceful\".\nHistory.\nIn 1927, a Southland Ice Company employee named John Jefferson Green began selling ice. He subsequently began to sell eggs, milk, and bread from 16 ice house storefronts in Dallas, with permission from one of Southland's founding directors, Joe\u00a0C. Thompson Sr. Although small grocery stores and general merchandisers were available, Thompson theorized that selling products such as bread and milk in convenience stores would reduce the need for customers to travel long distances for basic items. Thompson eventually bought the Southland Ice Company and turned it into the Southland Corporation, which oversaw several locations in the Dallas area.\nIn 1928, a manager named Jenna Lira brought a totem pole from Alaska and placed it in front of her store. The pole served as a marketing tool for the company, as it attracted a great deal of attention. Soon, executives added totem poles in front of every store and eventually adopted an Alaska Native-inspired theme for their stores. Later on, the stores began operating under the name \"Tote'm Stores\". In the same year, the company began constructing filling stations in some of its Dallas locations as an experiment. Joe Thompson also provided a distinct characteristic to the company's stores, training the staff so that people would receive the same quality and service in every store. Southland also started to have a uniform for its ice station service boys.\nIn 1931, the Great Depression affected the company, sending it toward bankruptcy. Nevertheless, the company continued its operations through re-organization and receivership. A Dallas banker, W.\u00a0W.\u00a0Overton Jr., also helped to revive the company's finances by selling the company's bonds for seven cents on the dollar. This brought the company's ownership under the control of a board of directors.\nIn 1946, in an effort to continue the company's post-war recovery, the name of the franchise was changed to 7-Eleven to reflect the stores' new hours of operation (7\u00a0am to 11\u00a0pm), which were unprecedented at the time. In 1963, 7-Eleven experimented with a 24-hour schedule in Austin, Texas, after an Austin store stayed open all night to satisfy customer demand. Later on, 24-hour stores were established in Fort Worth and Dallas, Texas, as well as Las Vegas, Nevada. In 1971, Southland acquired convenience stores of the former Pak-A-Sak chain owned by Graham Allen Penniman Sr., of Shreveport, Louisiana.\nWith the purchase in 1963 of 126 Speedee Mart franchised convenience stores in California (all already open 7\u201311), the company entered the franchise business. The company signed its first area licensing agreement in 1968 with Garb-Ko, Inc. of Saginaw, Michigan, which became the first U.S. domestic area 7-Eleven licensee.\nIn the late 1980s, Southland Corporation was threatened by a rumored corporate takeover, prompting the Thompson family to take steps to convert the company into a private model by buying out public shareholders in a tender offer. In December 1987, John Philp Thompson Sr., the chairman and CEO of 7-Eleven, completed a $5.2\u00a0billion management buyout of the company. The buyout suffered from the effects of the 1987 stock market crash and after failing initially to raise high yield debt financing, the company was required to offer a portion of stock as an inducement to invest in the company's bonds.\nVarious assets, such as the Chief Auto Parts chain, the ice division, and hundreds of store locations, were sold between 1987 and 1990 to relieve debt incurred during the buyout. This downsizing also resulted in numerous metropolitan areas losing 7-Eleven stores to rival convenience store operators. In October 1990, the heavily indebted Southland Corp. filed a pre-packaged Chapter 11 bankruptcy in order to transfer control of 70% of the company to Japanese affiliate Ito-Yokado.\nSouthland exited bankruptcy in March 1991, after a cash infusion of $430\u00a0million from Ito-Yokado and Seven-Eleven Japan. These two Japanese entities now controlled 70% of the company, with the founding Thompson family retaining 5 percent. In 1999, Southland Corp. changed its name to 7-Eleven, Inc., citing the divestment of operations other than 7-Eleven. In 2005, Seven-Eleven Japan made a tender offer and 7-Eleven, Inc. became its wholly owned subsidiary. In 2007, Seven &amp; i Holdings announced that it would be expanding its U.S. operations, with an additional 1,000 7-Eleven stores in the U.S.\nFor the 2010 rankings, 7-Eleven climbed to the No.\u00a03 spot in \"Entrepreneur\" magazine's 31st Annual Franchise\u00a0500, \"the first and most comprehensive ranking in the world\". This was the 17th year 7-Eleven was named in the top\u00a010.\nIn February 2010, 7-Eleven opened a concept store in DeLand, Florida across from Stetson University, designed to meet LEED environmental standards.\nIn 2020, 7-Eleven announced it would purchase Speedway for $21\u00a0billion.\nIn 2021, 7-Eleven rolled out a $70 million ad campaign, their largest investment in advertising in years, doubling their market spending from the previous year. The commercials, directed by Harmony Korine, were to reflect the \"evolution\" of the chain's store format, drawing attention to, in part, the fact that \"this isn't just gas station food, there's real restaurant quality food at 7-Eleven\", according to CMO Marissa Jarrantt.\nOn August 19, 2024, it was reported that Alimentation Couche-Tard\u2014owner of competitor Circle K\u2014had made a buyout offer for Seven &amp; i Holdings. Later, on September 6, 2024, Seven &amp; i Holdings rejected this offer as too low and rife with regulatory risk, although a \"sweetened offer\" might be considered.\nOn March 6, 2025, 7-Eleven's parent company Seven &amp; I Holdings announced that it would spin off the US store operations into its own publicly traded entity by the end of 2026, following the announcement of the appointment of its first non-Japanese CEO in Stephen Hayes Dacus.\nProducts and services.\n7-Eleven in the United States sells Slurpee drinks, a partially frozen soft drink introduced in 1965 (Oklahoma's stores sold these as \"Icy Drink\" until 2020), and Big Gulp beverages, introduced in 1976. Other products include: 7-Select private-brand products, coffee, fresh-made daily sandwiches, fresh fruit, salads, bakery items, hot and prepared foods, gasoline, dairy products, carbonated beverages and energy drinks, juices, donuts, financial services, and product delivery services.\n7-Eleven is known for its relatively large drink sizes and 24-hour accessibility. 7-Eleven offers beverages in sizes as large as 128 ounces (3785 mL) (Team Gulp). These beverage sizes were all among the largest sold soft drinks when they were introduced. 7-Eleven has often been associated with these large sodas in popular culture. For example, Mayor Michael Bloomberg's proposed ban on large sodas in New York City was frequently referred to as the 'Big Gulp ban' (even though the ban would not apply to 7-Eleven as convenience and grocery stores in New York are regulated by the state).\nIn 2012, 7-Eleven changed the size of the Double Gulp from 64 ounces to 50 ounces (1478 mL). The older style cups were too wide at the base to fit into vehicle beverage holders. This size change was not a reaction to the aforementioned large soda ban proposal, according to a spokesperson.\nIn February 2020, they opened a cashier-less location at the 7-Eleven headquarters in Irving, Texas.\nGlobal operations.\nAsia.\nCambodia.\nOn August 30, 2021, 7-Eleven and Thailand's Charoen Pokphand opened the first 7-Eleven store in Phnom Penh's Chroy Changvar district. The company hinted at plans to open at least six more stores in Phnom Penh in 2021. According to plans, products from local small- and medium-sized enterprises (SME) sold in 7-Eleven in Cambodia will comprise at least 50 percent of the stock.\nChina.\n7-Eleven opened its first store in China in Shenzhen, Guangdong in 1992 and later expanded to Beijing in 2004, Tianjin and Shanghai in 2009, Chengdu in 2011, Qingdao in 2012, Chongqing in 2013, Hangzhou and Ningbo in 2017, Nanjing in 2018, and Wuhan, Xi'an, and Fuzhou in 2019. In China's 7-Eleven stores where Slurpees are offered, the Chinese name (s\u012bl\u00e8b\u012bng) is used. They also offer a wide array of warm food, including traditional items like steamed buns, and stores in Chengdu offer a full variety of onigiri (\u996d\u56e2). Beverages, alcohol, candy, periodicals, and other convenience items are available as well. The majority of these stores are open for 24 hours a day. As of September 2021, 7-Eleven has 2,582 stores in mainland China.\nHong Kong.\n7-Eleven first opened in Hong Kong in 1981, when it was a British colony. As of July 2019[ [update]], it operates as a subsidiary of the DFI Retail Group (formerly Dairy Farm International). It is popularly called \"Tsat\u00b9-zai\u00b2\" (, meaning \"little seven\") or \"Tsat\u00b9-sap\u2076-jat\u00b9\" (, meaning \"seven eleven\"). As of 2012, 7-Eleven had 964 stores in Hong Kong, of which 563 were operated by franchisees. Hong Kong reportedly has the second-highest density of 7-Eleven stores, after Macao. All 7-Eleven stores in Hong Kong accept the ubiquitous Octopus card as a method of payment. They also accept payments for utility bills and public housing rent.\nIn November 1980, Southland Corporation and Hong Kong conglomerate Jardine Matheson signed a franchise agreement to bring 7-Eleven to the territory. The first 7-Eleven shop opened in Happy Valley on April 3, 1981. The chain expanded aggressively across Hong Kong throughout the 1980s. The 50th store opened in Kwai Chung on October 6, 1983, while the 200th was inaugurated by Simon Keswick at Tai Po Centre on May 7, 1987. The stores were sold to Dairy Farm, part of Jardine Matheson, in 1989.\nOctopus card readers were introduced in all 7-Eleven stores in July 1999, although at first these could only be used to add value to the card. In September 2004, the number of locations in Hong Kong was substantially boosted when Dairy Farm acquired Daily Stop, a rival convenience store chain, from SCMP Retailing (HK). The chain's 84 shops, located mainly in MTR and Kowloon\u2013Canton Railway stations (as well as shopping centers and housing estates), were converted to 7-Eleven stores.\nIn 2009, a 7-Eleven location in Quarry Bay opened with a hot food counter, called \"7 Caf\u00e9\", selling traditional Hong Kong street food and milk tea. This feature was subsequently extended to select other 7-Eleven locations across Hong Kong under the \"Daily Caf\u00e9\" and \"Hot Shot\" brands.\nIndia.\nOn 7 October 2021, Reliance Retail announced its partnership with 7-Eleven to open its stores in India. The announcement came a day after Future Group, another retail conglomerate, announced the end of its partnership with 7-Eleven, citing the inability to meet the target of opening stores and payment of franchisee fees. The first 7-Eleven in India opened in Mumbai on 9 October 2021 at Blue Fortuna, Military Road, Marol, Andheri East. Initially opened as a 24 hours outlet it was soon curtailed to shut its doors at 12:00am. 7-Eleven now operates stores in Mumbai, Pune, Thane, Kalyan-Dombivali, Mira-Bhayander, and Vasai-Virar.\nIndonesia.\nIn 2008, 7-Eleven announced plans to expand its business in Indonesia through a master franchise agreement with Modern Sevel Indonesia. Modern Sevel Indonesia's initial plans were to focus on opening stores in Jakarta, targeting densely populated commercial and business areas. There were 190 7-Eleven stores in Indonesia as of 2014[ [update]] which then reduced to only 166 stores in September 2016.\n7-Eleven then closed its doors in Indonesia in 2017, citing low sales.\nIsrael.\nIn October 2021, it was announced across Israeli media that 7-Eleven had signed a contract with Electra Consumer Products to open hundreds of stores in Israel. The first Israeli 7-Eleven location opened in January 2023 at Tel Aviv's Dizengoff Center. Under the agreement with 7-Eleven, Electra was to open a further approximately 400 branded stores in Israel, 300 of them through franchisees. Ultimately the effort failed, and the stores were sold in May 2024.\nJapan.\nJapan has the highest number of 7-Eleven locations in the world, as of the company's 85,000+ stores around the globe, 21,668 stores (nearly 25% of global stores) are in Japan, with 2,824 stores in Tokyo alone. Japanese 7-Eleven stores often bear the name of its holding company Seven &amp; I Holdings\u2014in fact, Seven &amp; I's subsidiary Seven-Eleven Japan, the master franchisee for Japan, is the direct parent company of 7-Eleven, Inc. On September 1, 2005, Seven &amp; i Holdings Co., Ltd., a new holding company, became the parent company of 7-Eleven, Ito-Yokado, and Denny's Japan.\nAs of July 2019[ [update]], 7-Eleven has stores in all 47 prefectures of Japan with the opening of 14 new locations in Okinawa Prefecture.\nThe aesthetics of the store are somewhat different from that of 7-Eleven stores in other countries as the stores offer a wider selection of products and services. 7-Eleven stores in Japan are also popular among tourists from other countries, as the Seven Bank automated teller machines at branches will accept foreign debit and credit cards for withdrawing cash in Japanese yen.\nFollowing the example of other convenience stores in Japan, 7-Eleven has solar panels and LEDs installed in about 1,400 of its stores.\nIn July 2019, 7-Eleven launched then almost immediately suspended a mobile payment service, 7pay. The service was hacked upon launch, and attackers were able to spend money from affected customers' accounts.\nLaos.\nOn August 31, 2020, 7-Eleven and Thailand's Charoen Pokphand announced a 30-year master franchise agreement. The first Laotian 7-Eleven was expected to open in the country's capital, Vientiane, in 2022. It officially opened on September 7, 2023, at Souphanouvong Road, Nongpanai Village, Sikhottabong district in Vientiane.\nMacau.\n7-Eleven entered the Macau market in 2005 under the ownership of Dairy Farm, a Hong Kong-based conglomerate operating 7-Eleven stores in Hong Kong. With a land area of about in 2024, Macau has 45 stores.\nMalaysia.\nMalaysian 7-Eleven stores are owned by 7-Eleven Malaysia Sdn. Bhd., which operates 3,225 stores nationwide. 7-Eleven in Malaysia was incorporated on June 4, 1984, as a joint veture of Jardine Matheson, Innovest and Antah Holdings group. The first 7-Eleven store was opened in October 1984, in Jalan Bukit Bintang, Kuala Lumpur.\nIts 2,000th outlet at Jalan Klang Lama opened in July 2016.\nPhilippines.\nIn the Philippines, 7-Eleven was run by the Philippine Seven Corporation (PSC). Its first store, located at the corner of EDSA and Kamias Road in Quezon City, opened on February 29, 1984.\nOn July 28, 1988, PSC transferred the Philippine area license to operate 7-Eleven stores to its affiliate, Phil-Seven Properties Corporation (\u201cPSPC\u201d), together with some of its store properties. In exchange thereof, PSC received 47% of PSPC stock as payment.\nOn May 2, 1996, the stockholders of both PSC and PSPC approved the merger of the two companies to advance PSC group's expansion. On October 30, 1996, Securities and Exchange Commission approved the merger and PSPC was then absorbed by PSC as the surviving entity. In 2000, President Chain Store Corporation (PCSC) of Taiwan, also a licensee of 7-Eleven, purchased the majority shares of PSC and thus formed a strategic alliance for the convenience store industry within the area.\nIn February 2009, 7-Eleven has signed a non-exclusive contract with Chevron Philippines to open its stores in selected Caltex gas stations nationwide.\nIn 2012, they opened their first store outside of Luzon in Cebu City, which soon expanded to the other parts of Cebu as well as its neighboring provinces. It was followed with the branch openings in Bacolod City in 2013, Iloilo City in 2014, Davao City and Cagayan de Oro in 2015. The number of stores eventually spread from these major cities to smaller towns and provinces near them.\nIn February 2020, 7-Eleven and GCash, the mobile wallet of Alipay and Globe, have teamed up for the introduction of a new payment option for physical purchases: scan-to-pay (STP) via a barcode feature in the GCash app. This enables the customers to generate their unique barcodes through the GCash app and allow the cashier to scan their barcodes to complete the transaction.\nIn 2020, due to the effect of COVID-19 pandemic in the Philippines, the Philippine Seven Corporation (PSC) slashed the store openings to 200 from the original 400 stores planned to be open due to financial difficulties from the growing pandemic situation. On July 11, 2021, coinciding with the 94th founding anniversary of the convenience store chain, 7-Eleven Philippines opened its 3,000th store in Meycauayan, Bulacan.\nOn October 15, 2024, as part of 7-Eleven Philippines' 40th year of operations, they opened their milestone 4,000th store in Newport, Makati City.\nSingapore.\nIn Singapore, 7-Eleven forms the largest chain of convenience stores island-wide. There are 393 7-Eleven stores in the country as of February 2018. Stores in Singapore are operated by DFI Retail Group (formerly Dairy Farm International Holdings), franchised under a licensing agreement with 7-Eleven Incorporated. The first 7-Eleven store in Singapore was opened along Upper Changi Road in June 1983, and in 1986 the first franchised 7-Eleven store (under the Jardines) was opened. The license was then acquired by Cold Storage Singapore, a subsidiary of the Dairy Farm Group, in 1989.\nIn 2006, Shell Singapore and 7-Eleven agreed to rebrand all 68 of its Shell Select convenience stores into 7-Eleven. The partnership was terminated in October 2017, and the remaining 52 7-Eleven stores in Shell petrol stations were gradually rebranded back into Shell Select.\nSouth Korea.\n7-Eleven has a major presence in the Republic of Korea convenience store market, where it competes with CU, GS25 (formerly LG25), and independent competitors. There are 11,067 7-Eleven stores in the Republic of Korea; with only Japan and Thailand hosting more stores. The first 7-Eleven store in the Republic of Korea opened in May 1989 in Songpa-gu in Seoul with a franchise license under the Lotte Group. In January 2010, Lotte Group acquired the Buy the Way convenience store chain and rebranded its 1,000 stores under the 7-Eleven brand.\nIn 2021, 7-Eleven announced that it would be working with a South Korean nonprofit to create jobs and franchising opportunities for North Korean defectors in South Korea.\nIn January 2022, Lotte acquired the entire stake of Ministop Korea Co. for 313.37 billion won ($263 million). After acquisition, all the Ministop store were gradually converted to 7-Eleven.\nTaiwan.\n7-Eleven is the largest convenience store chain in Taiwan, and is owned by President Chain Store Corporation (PCSC). The first fourteen stores opened in 1979, and struggled to make a profit. Southland Corporation partnered with Uni-President to modernise the stores. However, business was still slow, and Uni-President opted to stock Asian foods. In 1986, 7-Eleven made its first profit in Taiwan. The 5,000th store was opened in July 2014. In January 2018, an experimental and unstaffed shop branded the X-Store was opened. 7-Eleven announced plans to operate a combination store in partnership with Domino's Pizza in February 2019. The 6,000th store was opened on February 20, 2021. The 7,000th store was opened on July 5, 2024.\nIn the early 2000s, 7-Eleven and Dentsu introduced a corporate mascot named Open-Chan (Open \u5c0f\u5c07), an extraterrestrial dog who wears a rainbow-shaped crown from a fictional planet known as Planet Open to be a \"cartoon spokesperson\" for the store chain in Taiwan. Open-Chan quickly grew in popularity among Taiwanese children soon after its initial debut. After Open-Chan's subsequent rise to prominence in Taiwan, the character was even introduced in Japan. The unique convenience store culture formed by President Chain Store (7-Eleven in Taiwan) has become a part of Taiwanese culture.\n7-Eleven Taiwan also operates an MVNO called ibon mobile, which offers prepaid and postpaid SIM cards using the FarEasTone network.\nThailand.\nThe first Thai 7-Eleven opened on 1 June 1989 on Patpong Road in Bangkok. The chain consists of both company-owned (45%) and franchised shops (55%). CP All, a listed subsidiary of Charoen Pokphand, is the 7-Eleven owner and franchisor in Thailand; Charoen Pokphand received the franchise rights for Thailand in 1988. As of 2022, CP All has a total of 13,838 stores in Thailand, an increase from 12,432 in 2020. In 2018, 7-Eleven generated 335,532 million baht in income for CP. 7-Eleven holds a 70% market share in the convenience store category, opposed by some 7,000 other convenience stores (e.g., FamilyMart) and 400,000 \"mom and pop\" shops. Thailand has the second largest number of 7-Eleven stores after Japan.\nIn an effort to reduce plastic pollution the parent company of 7-Eleven stores in Thailand, CP All, announced their intent in November 2018 to reduce and eventually end the use of single-use plastic bags. As of January 2020[ [update]], 7-Eleven\u2014along with 42 other Thai retailers\u2014will stop giving single-use plastic bags to customers. However, the use of plastic bags is still prevalent in many shops throughout the country, as are plastic straws.\nUnited Arab Emirates.\nSeven &amp; I Holdings announced in June 2014 that they had agreed a contract with Seven Emirates Investment LLC to open the first Middle Eastern 7-Eleven in Dubai, United Arab Emirates during the summer of 2015. The company also said that they had plans to open about 100 stores in the country by the end of 2017. The first store was opened in October 2015. The country has 13 stores as of January 2018, but as of the 2020s, 7 Eleven has shut down and is now absent in Dubai until further notice.\nVietnam.\nThe first 7-Eleven store in Vietnam opened on June 15, 2017, making Vietnam the 17th country to host the world's largest convenience store chain. Seven System Vietnam (SSV) is the Master Franchisee of the 7-Eleven convenience store system in Vietnam, based in Ho Chi Minh City.\nAustralia.\nThe first 7-Eleven in Australia opened on 24 August 1977, in the Melbourne suburb of Oakleigh. The majority of stores are located in metropolitan areas, particularly in central business district areas. Stores in suburban areas often operate as petrol stations and most are owned and operated as franchises, with a central administration. 7-Eleven bought Mobil's remaining Australian petrol stations in 2010, converting them to 7-Eleven convenience store\u00a0/ petrol stations. In South Australia all Mobil petrol stations were later sold to Peregrine Corporation and rebranded to OTR convenience store\u00a0/ petrol stations.\nIn April 2014, 7-Eleven announced plans to start operating stores in Western Australia, with 11 stores planned to operate within the first year and a total of 75 stores established within five years. The first store was opened on October 30, 2014, in the city of Fremantle. The country has 675 stores as of January 2018.\nIn April 2022, 7-Eleven Australia settled a class-action lawsuit from its franchisees for A$98 million, amid claims that it had misled franchisees about the profitability of its business model.\nIn December 2023, Seven &amp; i Holdings of Japan agreed to purchase the Australian 7-Eleven franchise from its original franchise owners for A$1.71 billion. The purchase was finalised in April 2024.\nClass actions.\nIn August 2015, Fairfax Media and the ABC's \"Four Corners\" programme reported on the employment practices of certain 7-Eleven franchisees in Australia. The investigation found that many 7-Eleven employees were being underpaid at rates of around A$10 to A$14 per hour before tax, well under the legally required minimum award rate of A$24.69 per hour. The \"Four Corners\" investigation into 7-Eleven won a Walkley Award in 2015.\nFranchisees underpaying their staff would typically maintain rosters and pay records that appeared to show the employee being paid the legally required rate; however, these records only included half of the hours the employee actually worked in a week. Employees were then paid on the basis of these records, resulting in them effectively being paid half the legally required rate. It was also reported that workers were often not paid loadings and penalty rates that they are legally entitled to, for working overtime hours, nights, weekends, and public holidays.\nAfter these reports came to light and received widespread attention, some employees had alleged to Fairfax Media that they had begun to be paid correctly through the 7-Eleven payroll system; however, they were then asked by the franchisee to pay back half their wages in cash. 7-Eleven subsequently announced they would fund an inquiry to investigate instances of wage fraud. The inquiry was conducted by an independent panel chaired by former Australian Competition &amp; Consumer Commission chairman Allan Fels, and with the support of professional services firm Deloitte.\nIn September 2015, chairman Russ Withers and chief executive Warren Wilmot announced they were resigning from the company. Deputy chairman Michael Smith replaced Withers, while Bob Baily was appointed as interim chief executive.\nIn December 2015, Stewart Levitt of law firm Levitt Robinson Solicitors, who featured prominently in the \"Four Corners\" program, announced a potential class action lawsuit against 7-Eleven head office on behalf of franchisees who had allegedly been lured into signing on with 7-Eleven by false representations. This announcement was made on the same day as a Court finding describing Levvit Robinson's \"hellish bullying\" of Dr Brendan French, miring the action in controversy. Also on that day, 7-Eleven offered to pay \"the first $25 million of back-pay claims brought by current and former workers. Franchisees would then pay the next $5 million and any payments after that would be split 50-50 between head office and franchisees.\" Fels \"described the $25 million offer from head office as a 'significant step forward'\" but added that his panel's investigation would not be effected. 7-Eleven ultimately paid more than $173 million for \"systematic wage theft\" to workers employed between 2015 and 2020.\nLevvit Robinson was forced to retract misleading statements made in advertising to 7-Eleven franchisees in June 2018 by the Federal Court of Australia. This occurred only months after Levvit Robinson launched a new class action against 7-Eleven that included the ANZ Bank despite banks having stopped loans to 7-Eleven franchisees in 2015. In a settlement approved by the Federal Court in 2022, 7-Eleven agreed to pay $98 million to franchisees alleging that they were misled regarding store profitability. Though the settlement was reached without any admission of fault, the case included allegations that 7-Eleven had misrepresented employee-related costs as about seven percent of total costs, when a more accurate figure was around thirteen percent. This difference made many franchisees \"unable to make a profit unless they underpaid staff,\" as was shown in the wage theft class action.\nEurope.\nDenmark.\nThe first 7-Eleven store in Denmark was opened at \u00d8sterbro in Copenhagen on September 14, 1993. There are 185 stores, mostly in Copenhagen, Aarhus, Aalborg, and Odense, including two stores at Copenhagen Central Station. In Denmark, 7-Eleven had an agreement with Shell, with a nationwide network of Shell/7-Eleven service stations. This ended in 2024 when the national franchise operator Reitan Danmark did not renew the agreement and instead used their own brand of service station Uno-X. 7-Eleven also have an agreement with the Danish railway company DSB to have 7-Eleven stores at most S-train stations and other train stations.\nIn 2022, 7-Eleven in Denmark suffered a widespread ransomware attack that caused all stores to temporarily close. 7-Eleven did not comply with the attacker's demands. No customer data was compromised in the attack.\nThe three biggest 7-Eleven stores in terms of revenue in the world, are all located in Denmark. The top two stores are both located at Copenhagen Central Station and the third are located at Copenhagen Airport beyond security.\nNorway.\n7-Eleven has been established in Norway since 13 September 1986, when the first store opened in Oslo. In 2004, Reitan Convenience, a branch of the Norwegian Reitan Group bought the rights to use the 7-Eleven brand in Norway, Sweden and Denmark and since then has massively grown the number of operating shops in Scandinavia.\nSweden.\n7-Eleven entered Sweden in March 1984 with their first branch in Stockholm. Reitan acquired the brands right after 1997, and now has almost 200 stores throughout Sweden.\nIn May 2024, Reitan announced its intention to stop selling cigarettes in its Swedish stores, including all 7-Eleven Swedish stores, by 2026.\nTurkey.\n7-Eleven entered the Turkish market in 1989. Major stakeholder of the master franchise, \u00d6zer \u00c7iller sold his shares in 1993, after his wife Tansu \u00c7iller became the Prime Minister. In the 2010s, 7-Eleven left the Turkish market, transferring most of its stores to franchise owners.\nUnited Kingdom.\nDuring the 1980s, 7-Eleven convenience stores were based in London and the South East of England. The first shop opened in Hendon, north London in 1985. In October 1997 all 57 UK 7-Eleven stores were sold to Budgens. The company announced in 2014 they had planned to return to the UK market, but this did not progress beyond its announcement. In 2019, the company announced again it had planned to return, but as of October 2022 no stores had been opened.\nNorth America.\nCanada.\nThe first 7-Eleven store to open in Canada was in Calgary, Alberta, on June 29, 1969. There are 562 7-Eleven stores in Canada as of \u00a02022[ [update]]. Winnipeg, Manitoba, has the world's largest number of Slurpee consumers, with an estimated 1,500,000 Slurpees sold since the first 7-Eleven opened on March 21, 1970. All 7-Eleven locations in Canada are corporate operated. Like its U.S. counterparts every July 11 the stores offer free Slurpees on \"7-Eleven Day\".\nA limited number of 7-Eleven locations feature gas stations from Shell Canada, Petro-Canada, or Esso. In November 2005, 7-Eleven started offering the Speak Out Wireless cellphone service in Canada. 7-Eleven locations also featured CIBC ATMs\u2014in June 2012, these machines were replaced with ATMs operated by Scotiabank. 7-Eleven abandoned the Ottawa, Ontario, market in December 2009 after selling its six outlets to Quickie Convenience Stores, a regional chain. Following concerns over the fate of 7-Eleven Speak Out Wireless customers, Quickie offered the option for SpeakOut customers to port into the Good2Go mobile provider. SpeakOut subsequently offered online sales as an option, and continues to offer Ottawa-based phone numbers to new and existing customers. 7-Eleven is similarly absent from the Quebec market due to its saturation by chains like Alimentation Couche-Tard and by independent d\u00e9panneurs.\nIn March 2016, 7-Eleven acquired 148 Imperial Oil-owned Esso gas stations in Alberta and British Columbia for C$2.8\u00a0billion. Most of their convenience stores were converted to 7-Eleven stores, and they remain supplied by Esso. Some locations were not converted to 7-Eleven; these locations operate under the transitional banner \"smartstop 24/7\" with their existing store formats, typically inherited from On the Run.\nMexico.\nIn Mexico, the first 7-Eleven store opened in 1976 in Monterrey in association with Grupo Chapa (now Iconn) and 7-Eleven, Inc. under the name Super 7. In 1995, Super 7 was renamed to 7-Eleven, which now has 1,835 stores in several areas of the country, making it the second-largest convenience store chain in the country, between Oxxo and Circle K. When stores are located within classically designed buildings (such as in Centro Hist\u00f3rico buildings) or important landmarks, the storefront logo is displayed in monochrome with gold or silver lettering.\nUnited States.\nSupermarket News ranked 7-Eleven's North American operations No. 11 in the 2007 \"Top 75 North American Food Retailers\", based on the 2006 fiscal year estimated sales of US$15.0\u00a0billion. Based on the 2005 revenue, 7-Eleven is the 24th largest retailer in the United States. As of 2013[ [update]], 8,144 7-Eleven franchised units exist across the United States. Franchise fees range between US$10,000 \u2013 $1,000,000 and the ongoing royalty rate varies. 7-Eleven America has its headquarters in the Cypress Waters development in Irving, Texas. Small-size Slurpees are free on \"7-Eleven Day\", on July 11. This holiday first became widely celebrated on July 11, 2008, when first discovered by J. Brabank and C. Johnson. One exception is 2020, when the COVID-19 pandemic caused that year's cancellation. 7 Rewards members got a free medium Slurpee in their app instead.\n7-Eleven Stores of Oklahoma operated independently beginning in 1953 under an agreement with the Brown family. As part of this franchise agreement, 7-Elevens in Oklahoma bore slight differences to stores elsewhere: for instance, products such as Big Bite hot dogs were not sold there, the Slurpee was branded as the \"Icy Drink\", and Oklahoma stores operated their own loyalty program called \"Thx!\", which did not intersect with the national 7Rewards system. On March 2, 2020, 7-Eleven, Inc. announced it had officially closed on the acquisition of over 100 of these independently operated 7-Eleven stores in Oklahoma. All of these 100 stores were in the greater Oklahoma City metropolitan area. This acquisition increased the total number of 7-Eleven stores in the US and Canada to nearly 9800. Following the purchase, the Oklahoma 7-Elevens were fully integrated into national branding, marketing, and loyalty campaigns.\nIn April 2021, 7-Eleven launched the \"Take it to Eleven\" ad campaign. The slogan was partially inspired by the chain's name, but also the term \"up to eleven\" made popular in the film \"This is Spinal Tap\". The slogan was only for the main 7-Eleven brand and not A-Plus or Stripes.\nBy summer 2021, the company had installed just a few electric vehicle charging stations, but announced plans to expand considerably, with a target of 250 DC fast-charging locations in the U.S. and Canada by the end of 2022, starting with four states (California, Colorado, Florida and Texas). Less than two years later, in mid-March 2023, 7-Eleven announced plans for 7Charge, \"its new, proprietary EV charging network and app\", promoting the Android and iOS mobile apps, which allow users to find 7-Eleven - and future Speedway and Stripes - charger locations and pay for charging. 7Charge locations offer CCS and CHAdeMO charging; Tesla drivers, and other vehicles using the (once-proprietary) NACS connector, can also charge, but require a user-supplied CCS adapter.\nIn August 2022, 7-Eleven acquired Skipcart, a same-day and on-demand delivery platform.\nIn early 2024, parent company Seven &amp; i Holdings' CEO Ryuichi Isaka announced changes to the business model of US stores, placing the company's focus on fresh foods instead of a \"[reliance] on gasoline and cigarettes\". The company is working with food supplier Warabeya Nichiyo, which already supplies 7-Eleven's commissary food offerings in Japan, to create a US supply chain that will bring higher-quality Western and Japanese food offerings to 7-Eleven stores. Isaka also noted 7-Eleven's desire to grow its footprint and consolidate a larger portion of the US convenience store market. The company also announced plans to add hot and cold food options and a larger baked goods selection at up to 1,600 Speedway and Stripes stores.\nFuel.\nIn the U.S., many 7-Eleven locations used to have filling stations with gasoline distributed by Citgo, which in 1983 was purchased by Southland Corporation. 50% of Citgo was sold in 1986 to Petr\u00f3leos de Venezuela, S.A., and the remaining 50% was acquired in 1990. Although Citgo was the predominant partner of 7-Eleven, other oil companies are also co-branded with 7-Eleven, including Fina, Exxon, Mobil, Gulf, Marathon, BP, Amoco, Phillips 66, Conoco, 76, Shell, Chevron (some former TETCO convenience stores were co-branded with Chevron, and Texaco prior to the 7-Eleven purchase in late 2012), Sunoco, and Sinclair. Conoco is the largest 7-Eleven licensee in North America. The Pittsburgh market alone\u2014where 7-Eleven is the market leader by store count but third behind Sheetz and GetGo in revenue\u20147-Eleven currently offers fuel from Exxon, Gulf, Marathon (both legacy 7-Eleven locations and \nSpeedway), BP, and Sunoco (the latter two being from 7-Eleven's acquisitions of their company-owned-and-operated locations in the area) and also having previously offered Citgo and Pennzoil at some locations. In more recent years, some 7-Eleven locations sell 7-Eleven branded fuel without a Big Oil brand, much like 7-Eleven's primary rival Circle K has done in recent years.\n7-Eleven signed an agreement with Exxon-Mobil in December 2010 for the acquisition of 183 sites in Florida. This was followed by the acquisition of 51 ExxonMobil sites in North Texas in August 2011.\nRegardless of fuel brand, 7-Eleven has its own fleet network, 7 Fleet, for business customers and truck drivers at locations large enough to feature dedicated fueling lanes for semi trucks, though 7 Fleet can also be used at standard 7-Eleven locations as well. It is mostly designed to compete with Pilot Flying J's One9 Network designed for owner-operator drivers, as well as drivers that go to standard Pilot Flying J, Love's Travel Stops &amp; Country Stores, and TravelCenters of America locations.\nOn August 2, 2020, Seven &amp; i Holdings announced to buy Speedway LLC for $21\u00a0billion. The deal closed on May 14, 2021. 7-Eleven was ordered by U.S. antitrust regulators to divest 293 stores across 20 states. 124 stores were sold to Anabi Oil, 106 stores were sold to Cross-America Partners LP and 63 stores were sold to Jacksons Food Stores. 7-Eleven also dropped Speedway's participation in Pilot Flying J's One9 Network in favor of 7 Fleet.\nSouth America.\nBrazil.\nIn Brazil, during the 1990s, 7-Eleven had 17 stores in the city of S\u00e3o Paulo in a joint venture between Esteve S.A. Exportadora and Southland Corporation, but all closed due to high competition. In 2018, the company entered into partnership talks to operate convenience stores at gas stations operated by Petrobras Distribuidora in the country, but these did not progress further.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47422", "revid": "31125104", "url": "https://en.wikipedia.org/wiki?curid=47422", "title": "Monster group", "text": "Sporadic simple group\nIn the area of abstract algebra known as group theory, the monster group M (also known as the Fischer\u2013Griess monster, the friendly giant, or simply the Monster) is the largest sporadic simple group; it has order\n = 246\u00a0\u00b7 320\u00a0\u00b7 59\u00a0\u00b7 76\u00a0\u00b7 112\u00a0\u00b7 133\u00a0\u00b7 17\u00a0\u00b7 19\u00a0\u00b7 23\u00a0\u00b7 29\u00a0\u00b7 31\u00a0\u00b7 41\u00a0\u00b7 47\u00a0\u00b7 59\u00a0\u00b7 71\n = 32!\u00a0\u00b7 10!\u00a0\u00b7 4!2\u00a0\u00b7 2\u00a0\u00b7 7\u00a0\u00b7 13\u00a0\u00b7 41\u00a0\u00b7 47\u00a0\u00b7 59\u00a0\u00b7 71\n \u2248 8.08 x 1053.\nThe finite simple groups have been completely classified. Every such group belongs to one of 18\u00a0countably infinite families or is one of 26\u00a0sporadic groups that do not follow such a systematic pattern. The monster group contains 20\u00a0sporadic groups (including itself) as subquotients. Robert Griess, who proved the existence of the monster in 1982, has called those 20\u00a0groups the \"happy family\", and the remaining six exceptions \"pariahs\".\nIt is difficult to give a good constructive definition of the monster because of its complexity. Martin Gardner wrote a popular account of the monster group in his June 1980 Mathematical Games column in \"Scientific American\".\nHistory.\nThe monster was predicted by Bernd Fischer (unpublished, about 1973) and Robert Griess as a simple group containing a double cover of Fischer's baby monster group as a centralizer of an involution. Within a few months, the order of M was found by Griess using the Thompson order formula, and Fischer, Conway, Norton and Thompson discovered other groups as subquotients, including many of the known sporadic groups, and two new ones: the Thompson group and the Harada\u2013Norton group. The character table of the monster, a was calculated in 1979 by Fischer and Donald Livingstone using computer programs written by Michael Thorne. It was not clear in the 1970s whether the monster actually existed. Griess constructed M as the automorphism group of the Griess algebra, a dimensional commutative nonassociative algebra over the real numbers; he first announced his construction in Ann Arbor on 14\u00a0January 1980. In his 1982 paper, he referred to the monster as the \"Friendly Giant\", but this name has not been generally adopted. John Conway and Jacques Tits subsequently simplified this construction.\nGriess's construction showed that the monster exists. Thompson showed that its uniqueness (as a simple group satisfying certain conditions coming from the classification of finite simple groups) would follow from the existence of a dimensional faithful representation. A proof of the existence of such a representation was announced by Norton, though he never published the details. Griess, Meierfrankenfeld, and Segev gave the first complete published proof of the uniqueness of the monster (more precisely, they showed that a group with the same centralizers of involutions as the monster is isomorphic to the monster).\nThe monster was a culmination of the development of sporadic simple groups and can be built from any two of three subquotients: The Fischer group Fi24, the baby monster, and the Conway group Co1.\nThe Schur multiplier and the outer automorphism group of the monster are both trivial.\nRepresentations.\nThe minimal degree of a faithful complex representation is which is the product of the three largest prime divisors of the order of M.\nThe smallest faithful linear representation over any field has dimension over the field with two elements, only one less than the dimension of the smallest faithful complex representation.\nThe smallest faithful permutation representation of the monster is on\n = 24\u00a0\u00b7 37\u00a0\u00b7 53\u00a0\u00b7 74\u00a0\u00b7 11\u00a0\u00b7 132\u00a0\u00b7 29\u00a0\u00b7 41\u00a0\u00b7 59\u00a0\u00b7 71 \u2248 1020\npoints.\nThe monster can be realized as a Galois group over the rational numbers, and as a Hurwitz group.\nThe monster is unusual among simple groups in that there is no known easy way to represent its elements. This is not due so much to its size as to the absence of \"small\" representations. For example, the simple groups A100 and SL20(2) are far larger but easy to calculate with as they have \"small\" permutation or linear representations. Alternating groups, such as A100, have permutation representations that are \"small\" compared to the size of the group, and all finite simple groups of Lie type, such as SL20(2), have linear representations that are \"small\" compared to the size of the group. All sporadic groups other than the monster also have linear representations small enough that they are easy to work with on a computer (the next hardest case after the monster is the baby monster, with a representation of dimension ).\nComputer construction.\nMartin Seysen (2022) implemented a fast Python package named https://, which claims to be the first implementation of the monster group where arbitrary operations can effectively be performed. The documentation states that multiplication of group elements takes less than 40 milliseconds on a typical modern PC, which is five orders of magnitude faster than estimated by Robert A. Wilson in 2013. The mmgroup software package has been used to find two new maximal subgroups of the monster group.\nPreviously, Robert A. Wilson had found explicitly (with the aid of a computer) two invertible 196,882 by 196,882 matrices (with elements in the field of order 2) which together generate the monster group by matrix multiplication; this is one dimension lower than the dimensional representation in characteristic 0. Performing calculations with these matrices was possible but is too expensive in terms of time and storage space to be useful, as each such matrix occupies over four and a half gigabytes.\nWilson asserts that the best description of the monster is to say, \"It is the automorphism group of the monster vertex algebra\". This is not much help however, because nobody has found a \"really simple and natural construction of the \"monster vertex algebra\"\".\nWilson with collaborators found a method of performing calculations with the monster that was considerably faster, although now superseded by Seysen's abovementioned work. Let V be a 196,882 dimensional vector space over the field with 2\u00a0elements. A large subgroup H (preferably a maximal subgroup) of the Monster is selected in which it is easy to perform calculations. The subgroup H chosen is 31+12.2.Suz.2, where Suz is the Suzuki group. Elements of the monster are stored as words in the elements of H and an extra generator T. It is reasonably quick to calculate the action of one of these words on a vector in V. Using this action, it is possible to perform calculations (such as the order of an element of the monster). Wilson has exhibited vectors u and v whose joint stabilizer is the trivial group. Thus (for example) one can calculate the order of an element g of the monster by finding the smallest such that and This and similar constructions (in different characteristics) were used to find some of the non-local maximal subgroups of the monster group.\nSubquotients.\nThe monster contains 20 of the 26\u00a0sporadic groups as subquotients. This diagram, based on one in the book \"Symmetry and the Monster\" by Mark Ronan, shows how they fit together. The lines signify inclusion, as a subquotient, of the lower group by the upper one. The circled symbols denote groups not involved in larger sporadic groups. For the sake of clarity redundant inclusions are not shown.\nMaximal subgroups.\nThe monster has 46\u00a0conjugacy classes of maximal subgroups. Non-abelian simple groups of some 60\u00a0isomorphism types are found as subgroups or as quotients of subgroups. The largest alternating group represented is A12.\nThe 46 classes of maximal subgroups of the monster are given by the following table. Previous unpublished work of Wilson et. al had purported to rule out any almost simple subgroups with non-abelian simple socles of the form U3(4), L2(8), and L2(16). However, the latter was contradicted by Dietrich et al., who found a new maximal subgroup of the form U3(4). The same authors had previously found a new maximal subgroup of the form L2(13) and confirmed that there are no maximal subgroups with socle L2(8) or L2(16), thus completing the classification in the literature.\nNote that tables of maximal subgroups have often been found to contain subtle errors, and in particular at least two of the subgroups in this table were incorrectly omitted from some previous lists.\nMcKay's E8 observation.\nThere are also connections between the monster and the extended Dynkin diagrams formula_1 specifically between the nodes of the diagram and certain conjugacy classes in the monster, known as \"McKay's E8 observation\". This is then extended to a relation between the extended diagrams formula_2 and the groups 3.Fi24\u2032, 2.B, and M, where these are (3/2/1-fold central extensions) of the Fischer group, baby monster group, and monster. These are the sporadic groups associated with centralizers of elements of type 1A, 2A, and 3A in the monster, and the order of the extension corresponds to the symmetries of the diagram. See ADE classification: trinities for further connections (of McKay correspondence type), including (for the monster) with the rather small simple group PSL(2,11) and with the 120 tritangent planes of a canonic sextic curve of genus 4 known as Bring's curve.\nMoonshine.\nThe monster group is one of two principal constituents in the monstrous moonshine conjecture by Conway and Norton, which relates discrete and non-discrete mathematics and was finally proved by Richard Borcherds in 1992.\nIn this setting, the monster group is visible as the automorphism group of the monster module, a vertex operator algebra, an infinite dimensional algebra containing the Griess algebra, and acts on the monster Lie algebra, a generalized Kac\u2013Moody algebra.\nMany mathematicians, including Conway, have seen the monster as a beautiful and still mysterious object. Conway said of the monster group: \"There's never been any kind of explanation of why it's there, and it's obviously not there just by coincidence. It's got too many intriguing properties for it all to be just an accident.\" Simon P. Norton, an expert on the properties of the monster group, is quoted as saying, \"I can explain what Monstrous Moonshine is in one sentence, it is the voice of God.\"\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "47425", "revid": "1320139886", "url": "https://en.wikipedia.org/wiki?curid=47425", "title": "Bungee jumping", "text": "Jumping while connected to an elastic cord\nBungee jumping (), also spelled bungy jumping, is an activity that involves a person jumping from a great height while connected to a large elastic cord. The launching pad is usually erected on a tall structure such as a building or crane, a bridge across a deep ravine, or on a natural geographic feature such as a cliff. It is also possible to jump from a type of aircraft that has the ability to hover above the ground, such as a hot-air-balloon or helicopter. The thrill comes from the free-falling and the rebound. When the person jumps, the cord stretches and the jumper flies upwards again as the cord recoils, and continues to oscillate up and down until all the kinetic energy is dissipated.\nEtymology.\nThe word \"bungee\" originates from West Country dialect of the English language, meaning \"anything thick and squat\", as defined by James Jennings in his book \"Observations of Some of the Dialects in The West of England\" published 1825. In 1928, the word started to be used for a rubber eraser. \nThe Oxford English Dictionary records early use of the phrase in 1938 relating to launching of gliders using an elasticated cord, and also as \"a long nylon-cased rubber band used for securing luggage\".\nHistory.\nEarly tethered jumping.\nThe land diving (Sa: \"\") of Pentecost Island in Vanuatu is an ancient ritual in which young men jump from tall wooden platforms with vines tied to their ankles as a test of their courage and passage into manhood. Unlike in modern bungee-jumping, land-divers intentionally hit the ground, but the vines absorb sufficient force to make the impact non-lethal. The land-diving ritual on Pentecost has been claimed as an inspiration by A. J. Hackett, prompting calls from the islanders' representatives for compensation for what they view as the unauthorised appropriation of their cultural property.\nA tower high with a system to drop a \"car\" suspended by a cable of \"best rubber\" was proposed for the Chicago World Fair, 1892\u20131893. The car, seating about two hundred people, would have been shoved from a platform on the tower and then would have bounced to a stop. The designer engineer suggested that for safety the ground below \"be covered with eight feet of feather bedding\". The proposal was declined by the Fair's organizers.\nBraked rope sliding.\nIn 1963, Jim Tyson, a Sydney, Australia commando, slid down a catenary rope, from a cliff, across a Sydney-area river, braking with carpet, releasing mid-river, and swimming to an accessible river bank.&lt;ref name=\"LIFE/1963/Cliff\"&gt;&lt;/ref&gt;\nModern sport.\nThe first modern bungee jumps were made on 1 April 1979 from the Clifton Suspension Bridge in Bristol, England, by David Kirke&lt;ref name=\"thetimes/DK\"&gt;&lt;/ref&gt; and Simon Keeling, members of the Oxford University Dangerous Sports Club,&lt;ref name=\"ou/DSC\"&gt;&lt;/ref&gt; and Geoff Tabin, a professional climber who tied the ropes for the jump. The students had come up with the idea after discussing the \"vine jumping\" ritual of Vanuatu. The jumpers were arrested shortly after, but continued with jumps in the US from the Golden Gate Bridge and the Royal Gorge Bridge. The last jump was sponsored by and televised on the American programme \"That's Incredible\", spreading the concept worldwide. By 1982, Kirk and Keelling were jumping from mobile cranes and hot air balloons.\nColorado climbers Mike Munger and Charlie Fowler may have bungee-jumped earlier in Eldorado Springs, CO in 1977. Both were cutting-edge alpinists, preparing for a trip to Monte Fitzroy in Patagonia by simulating long falls onto a springy, nylon climbing rope. They scrambled up to a large tree at the top of the wall, above a severely overhanging climb appropriately named \"Diving Board\", and tied one end of the rope into the tree. With a piece of flat seat belt webbing around his waist and some homemade leg loops, Mike tied into the other end of the rope and, after no small amount of trepidation, he jumped. He then ascended the rope mechanically to the tree and untied, and then tied in and jumped. The total fall was about .\nOrganised commercial&lt;ref name=\"eoNZ/ajH\"&gt;&lt;/ref&gt; bungee jumping began with the New Zealander, A. J. Hackett, who made his first jump from Auckland's Greenhithe Bridge in 1986. During the following years, Hackett performed a number of jumps from bridges and other structures (including the Eiffel Tower), building public interest in the sport, and opening the world's first permanent commercial bungee site, the Kawarau Bridge Bungy at the Kawarau Gorge Suspension Bridge near Queenstown in the South Island of New Zealand. Hackett remains one of the largest commercial operators, with concerns in several countries.\nSeveral million successful jumps have taken place since 1980. This safety record is attributable to bungee operators rigorously conforming to standards and guidelines governing jumps, such as double checking calculations and fittings for every jump. As with any sport, injuries can still occur (see below), and there have been fatalities. A relatively common mistake in fatality cases is to use a cord that is too long. The cord should be substantially shorter than the height of the jumping platform to allow it room to stretch. When the cord becomes taut and then is stretched, the tension in the cord progressively increases, building up its potential energy. Initially the tension is less than the jumper's weight and the jumper continues to accelerate downwards. At some point, the tension equals the jumper's weight and the acceleration is temporarily zero. With further stretching, the jumper has an increasing upward acceleration and at some point has zero vertical velocity before recoiling upward.\nThe Bloukrans River Bridge was the first bridge to be used as a bungee jump launch spot in Africa when Face Adrenalin introduced bungee jumping to the African continent in 1990. Bloukrans Bridge Bungy has been operated commercially by Face Adrenalin since 1997, and is the highest commercial bridge bungee in the world.\nIn 2008, Carl Dionisio of Durban performed a 30 meter bungee jump attached to a cord made of 18,500 condoms.\nHe currently runs the only Ocean Touch bungee jump in the World at Calheta Beach in Madeira, Portugal, and claims to be the only person operating in the bungee industry single-handed. \nHe holds the world record for being the only person to bungee jump while driving a tower crane at the same time, something that he has done hundreds of times since 2017.\nEquipment.\nThe elastic rope first used in bungee jumping, and still used by many commercial operators, is factory-produced braided shock cord. This special bungee cord consists of many latex strands enclosed in a tough outer cover. The outer cover may be applied when the latex is pre-stressed, so that the cord's resistance to extension is already significant at the cord's natural length. This gives a harder, sharper bounce. The braided cover also provides significant durability benefits. Other operators, including A. J. Hackett and most southern-hemisphere operators, use unbraided cords with exposed latex strands. These give a softer, longer bounce and can be home-produced.\nAccidents where participants became detached led many commercial operators to use a body harness, if only as a backup for an ankle attachment. Body harnesses generally derive from climbing equipment rather than parachute equipment.\nThe highest jump.\nMilad tower bungee jumping with a height of 280 meters is the highest jumping platform in the world\nIn August 2005, AJ Hackett added a SkyJump to the Macau Tower, making it the world's highest jump at . The SkyJump did not qualify as the world's highest \"bungee\" as it is not strictly speaking a bungee jump, but instead what is referred to as a 'Decelerator-Descent' jump, using a steel cable and decelerator system, rather than an elastic rope. On 17 December 2006, the Macau Tower started operating a proper bungee jump, which became the \"Highest Commercial Bungee Jump in the World\" according to the Guinness Book of Records. The Macau Tower Bungy has a \"Guide cable\" system that limits swing (the jump is very close to the structure of the tower itself) but does not have any effect on the speed of descent, so this still qualifies the jump for the World Record.\nKushma Bungee Jump is the world's second-highest bungee jump with a height of . It is located in the gorge of Kaligandaki River and world-first natural canyon bungee jump. Another commercial bungee jump currently in operation is just smaller, at . This jump, made without guide ropes, is from the top of the Verzasca Dam near Locarno, Switzerland. It appears in the opening scene of the James Bond film \"GoldenEye\". The Bloukrans Bridge Bungy in South Africa and the Verzasca Dam jumps are pure freefall swinging bungee from a single cord.\nGuinness only records jumps from fixed objects to guarantee the accuracy of the measurement. John Kockleman however recorded a bungee jump from a hot air balloon in California in 1989. In 1991 Andrew Salisbury jumped from from a helicopter over Cancun for a television program and with Reebok sponsorship. The full stretch was recorded at . He landed safely under parachute.\nOne commercial jump higher than all others is at the Royal Gorge Bridge in Colorado. The height of the platform is . However, this jump is rarely available, as part of the Royal Gorge Go Fast Games\u2014first in 2005, then again in 2007. Previous to this the record was held in West Virginia, USA, by New Zealander Chris Allum, who bungee jumped from the New River Gorge Bridge on \"Bridge Day\" 1992 to set a world's record for the longest bungee jump from a fixed structure.\nVariations.\nCatapult.\nIn \"Catapult\" (Reverse Bungee or Bungee Rocket), the jumper starts on the ground. The jumper is secured and the cord is stretched, then released and shooting the jumper up into the air. This is often achieved using either a crane or a hoist attached to a (semi-)perma structure. This simplifies the action of stretching the cord and later lowering the participant to the ground.\nTrampoline.\n\"Bungee Trampoline\" uses elements from bungee and trampolining. The participant begins on a trampoline and is fitted into a body harness, which is attached via bungee cords to two high poles on either side of the trampoline. As they begin to jump, the bungee cords are tightened, allowing a higher jump than could normally be made from a trampoline alone.\nRunning.\n\"Bungee Running\" involves no jumping as such. It merely consists of, as the name suggests, running along a track (often inflatable) with a bungee cord attached. One often has a velcro-backed marker that marks how far the runner got before the bungee cord pulled back. This activity can often be found at fairs and carnivals and is often most popular with children.\nRamp.\nBungee jumping off a ramp. Two rubber cords \u2013 the \"bungees\" \u2013 are tied around the participant's waist to a harness. Those bungee cords are linked to steel cables along which they can slide due to stainless pulleys. The participants bicycle, sled or ski before jumping.\nSCAD diving.\nSCAD diving (Suspended Catch Air Device) is similar to bungee jumping in that the participant is dropped from a height, but in this variation there is no cord; instead the participant free-falls into a net. Untrained SCAD divers use a special free fall harness to ensure the correct falling position. Free-style SCAD divers do not use harnesses. The landing into the huge airtube framed net is extremely soft and forgiving. The SCAD was invented by MONTIC Hamburg, Germany in 1997.\nRisk of injury.\nBungee jumping injuries may be divided into those that occur after jumping secondary to equipment mishap or tragic accident, and those that occur regardless of safety measures.\nIn the first instance, injury can happen if the safety harness fails, the cord length is miscalculated, or the cord is not properly connected to the jump platform. In 1986, a man died during rehearsals for a bungee jumping stunt on a BBC television programme, because the cord sprang loose from a carabiner clip.\nInjuries that occur despite safety measures generally relate to the abrupt rise in upper body intravascular pressure during bungee cord recoil. Eyesight damage is the most frequently reported complication. Impaired eyesight secondary to retinal haemorrhage may be transient or take several weeks to resolve. In one case, a 26-year-old woman's eyesight was still impaired after 7 months. Whiplash injuries may occur as the jumper is jolted on the bungee cord and in at least one case, this has led to quadriplegia secondary to a broken neck. Very serious injury can also occur if the jumper's neck or body gets entangled in the cord. More recently, carotid artery dissection leading to a type of stroke after bungee jumping has also been described.\nIn popular culture.\nIn the film \"GoldenEye\" and its associated videogame, James Bond makes a jump over the edge of a dam in Russia (in reality the dam is in Switzerland: Verzasca Dam, and the jump was genuine, not an animated special effect). The jump in the dam later makes an appearance as a Roadblock task in the 14th season of the reality competition series \"The Amazing Race\".\nA fictional proto-bungee jump is a plot point in the Michael Chabon novel \"The Amazing Adventures of Kavalier and Clay\".\nIn Valiant (comics) #171 (January 8, 1966), the two boys from Worrag island in \"The Wild Wonders\" in a circus story, jump from high up and seem ready to crash to their deaths, but are stopped by elasticated ropes tied to an ankle of each one.\nIn the video game \"Aero the Acro-Bat\", Aero will perform bungee jumping to obtain items like keys to open gates in a level.\nIn Taskmaster New Zealand, series 2 episode 7 Matt Heath uses bungee jumping to solve one of the tasks.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47426", "revid": "771402", "url": "https://en.wikipedia.org/wiki?curid=47426", "title": "Water polo", "text": "Competitive team sport played in water\nWater polo is a competitive team sport played in water between two teams of seven players each. The game consists of four quarters in which the teams attempt to score goals by throwing the ball into the opposing team's goal. The team with more goals at the end of the game wins the match. Each team is made up of six field players and one goalkeeper. Excluding the goalkeeper, players participate in both offensive and defensive roles. It is typically played in an all-deep pool where players cannot touch the bottom.\nA game consists mainly of the players swimming to move about the pool, treading water (mainly using the eggbeater kick), passing the ball, and shooting at the goal. Teamwork, tactical thinking and awareness are also highly important aspects. Water polo is a highly physical and demanding sport and has frequently been cited as one of the most difficult to play.\nSpecial equipment for water polo includes a water polo ball, a ball of varying colors which floats on the water; numbered and coloured caps; and two goals, which either float in the water or are attached to the sides of the pool.\nThe game is thought to have originated in Scotland in the mid-19th century; specifically, William Wilson is thought to have developed it in the 1870s as a sort of \"water rugby\". The game further developed with the formation of the London Water Polo League and has since expanded, becoming popular in parts of Europe, the United States, Brazil, China, Canada, and Australia.\nHistory.\nThe history of water polo as a team sport began as a demonstration of strength and swimming skill in mid-19th century England and Scotland, where water sports and racing exhibitions were a feature of county fairs and festivals. Men's water polo was among the first team sports introduced at the modern Olympic games in 1900. The present-day game involves teams of seven players (plus up to six substitutes), with a water polo ball similar in size to a soccer ball but constructed of air-tight nylon.\nOne of the earliest recorded viewings of water polo was conducted at the 4th Open Air Fete of the London Swimming Club, held at the Crystal Palace, London on 15 September 1873. Another antecedent of the modern game of water polo was a game of water 'handball' played at Bournemouth on 13 July 1876. This was a game between 12 members of the Premier Rowing Club, with goals being marked by four flags placed in the water near to the midpoint of Bournemouth Pier. The game started at 6:00 pm and lasted for 15 minutes (when the ball burst) watched by a large crowd; with plans being made for play on a larger scale the following week.\nThe rules of water polo were originally developed in the late nineteenth century in Great Britain by William Wilson. Wilson is believed to have been the First Baths Master of the Arlington Baths Club in Glasgow. The first games of 'aquatic football' were played at the Arlington in the late 19th century (the club was founded in 1870), with a ball constructed of India rubber. This \"water rugby\" came to be called \"water polo\" based on the English pronunciation of the Balti word for ball, \"pulu\". Early play allowed brute strength, wrestling and holding opposing players underwater to recover the ball. Players held underwater for lengthy periods usually surrendered possession. The goalie stood outside the playing area and defended the goal by jumping in on any opponent attempting to score by placing the ball on the deck.\nGeography.\nWater polo is now popular in many countries around the world, notably Europe (particularly in Spain, France, Netherlands, Germany, Italy, Croatia, Hungary, Serbia, Montenegro, Greece and Romania), Australia, Brazil, Canada and the United States.\nSome countries have two principal competitions: a more prestigious league which is typically a double round-robin tournament restricted to the elite clubs, and a cup which is a single-elimination tournament open to both the elite and lesser clubs.\nRules.\nThe rules of water polo cover the play, procedures, equipment and officiating of water polo. These rules are similar throughout the world, although slight variations to the rules occur regionally and depending on the governing body. Governing bodies of water polo include World Aquatics, the international governing organization; European Aquatics, which governs international European matches; the NCAA, which governs collegiate matches in the United States; the NFHS, which governs high schools in the US, and the IOC, which governs Olympic events.\nPositions.\nThere are seven players in the water from each team at one time. There are six players that play out and one goalkeeper. Unlike most common team sports, there is little positional play; field players will often fill several positions throughout the game as situations demand. These positions usually consist of a center forward, a center back, the two wing players and the two drivers. Players who are skilled in all positions of offense or defense are called utility players. Utility players tend to come off of the bench, though this is not absolute. Certain body types are more suited for particular positions, and left-handed players are especially coveted on the right-hand side of the field, allowing teams to launch two-sided attacks.\nOffense.\nThe offensive positions include: one center forward (also called a \"set\", \"hole-set\", \"center\", \"setter\", \"hole\", or \"2-meter man\", located on or near the 2-meter, roughly in the center of the goal), two wings (located on or near the 2-meter, just outside of the goal posts, respectively), two drivers (also called \"flats\", located on or near the 5-meter, roughly at the goal posts, respectively), and one \"point\" (usually just behind the 5 meter, roughly in the center of the goal, respectively), positioned farthest from the goal. The wings, drivers and point are often called the perimeter players; while the hole-set directs play. There is a typical numbering system for these positions in U.S. NCAA men's division one polo. Beginning with the offensive wing to the opposing goalie's right side is called one. The flat in a counter clockwise from one is called two. Moving along in the same direction the point player is three, the next flat is four, the final wing is five, and the hole set is called six. Additionally, the position in which a player is can give advantages based on a player's handedness, to improve a shooting or passing angle (for example, the right wing is often left handed).\nThe center sets up in front of the opposing team's goalie and scores the most individually (especially during lower level play where flats do not have the required strength to effectively shoot from outside or to penetrate and then pass to teammates like the point guard in basketball, or center midfield player in soccer). The center's position nearest to the goal allows explosive shots from close-range.\nDefense.\nDefensive positions are often the same, but just switched from offence to defence. For example, the centre forward or hole set, who directs the attack on offence, on defence is known as \"hole D\" (also known as set guard, hole guard, hole check, pit defence or two-metre defence), and guards the opposing team's centre forward (also called the hole). Defence can be played man-to-man or in zones, such as a 2\u20134 (four defenders along the goal line). It can also be played as a combination of the two in what is known as an \"M drop\" defence, in which the point defender moves away (\"sloughs off\") his man into a zone in order to better defend the centre position. In this defence, the two wing defenders split the area furthest from the goal, allowing them a clearer lane for the counter-attack if their team recovers the ball.\nGoalkeeper.\nThe goalkeeper has the main role in blocking shots against the goal as well as guiding and informing their defense of imposing threats and gaps in the defense. The goalkeeper usually begins the offensive play by passing the ball across the pool to an attacker. It is not unusual for a goalkeeper to make an assisting pass to a goal on a break away.\nThe goalkeeper is given several privileges above those of the other players, but only within the five-meter area in front of their own goal:\nIn general, a foul that would cause an ejection of a field player might bring on a five-metre shot on the goalkeeper. Also, if a goalkeeper pushes the ball under water, the action will not be punished with a turnover like with field players, but with a penalty shot.\nCommon techniques and practices.\nOffense strategy.\nPlayer positioning.\nThe most basic positional set up is known as a \"3\u20133\", so called because there are two lines in front of the opponent's goal. Another set up, used more by professional teams, is known as an \"arc\", \"umbrella\", or \"mushroom\"; perimeter players form the shape of an arc around the goal, with the hole set as the handle or stalk. Yet another option for offensive set is called a 4\u20132 or double hole; there are two center forward offensive players in front of the goal. Double hole is most often used in \"man up\" situations, or when the defense has only one skilled \"hole D\", or to draw in a defender and then pass out to a perimeter player for a shot (\"kick out\").\nAnother, albeit less common offense, is the \"motion c\", sometimes nicknamed \"washing machine offence\", in which two \"weak-side\" (to the right of the goal for right-handed players) perimeter players set up as a wing and a flat. The remaining four players swim in square pattern in which a player swims from the point to the hole and then out to the strong side wing. The wing moves to the flat and the flat to the point. The weak side wing and flat then control the tempo of play and try to make passes into the player driving towards the centre forward who can then either shoot or pass. This form of offence is used when no dominate hole set is available, or the hole defence is too strong. It is also seen much more often in women's water polo where teams may lack a player of sufficient size or strength to set up in the centre forward. The best advantage to this system is it makes man-coverage much more difficult for the defender and allows the offence to control the game tempo better once the players are \"set up\". The main drawback is this constant motion can be very tiring as well as somewhat predictable as to where the next pass is going to go.\nAdvancing the ball.\nWhen the offence takes possession of the ball, the strategy is to advance the ball down the field of play and to score a goal. Players can move the ball by throwing it to a teammate or swimming with the ball in front of them (dribbling). If an attacker uses their arm to push away a defending player and free up space for a pass or shot, the referee will rule a turnover and the defence will take possession of the ball. If an attacker advances inside the 2-metre line without the ball or before the ball is inside the 2-metre area, they are ruled offside and the ball is turned over to the defence. This is often overlooked if the attacker is well to the side of the pool or when the ball is at the other side of the pool.\nSetting the ball.\nThe key to the offence is to accurately pass (or \"set\") the ball into the centre forward or hole set, positioned directly in front of the goal (\"the hole\"). Any field player may throw the hole set a \"wet pass\". A wet pass is one that hits the water just outside the hole set's reach. A dry pass may also be used. This is where the hole set receives the ball directly in his hand and then attempts a shot at the cage. This pass is much more difficult because if the pass is not properly caught, the officials will be likely to call an offensive foul resulting in a change of ball possession. The hole set attempts to take possession of the ball [after a wet pass], to shoot at the goal, or to draw a foul from his defender. A minor foul is called if his defender (called the \"hole D\") attempts to impede movement before the hole set has possession. The referee indicates the foul with one short whistle blow and points one hand to the spot of the foul and the other hand in the direction of the attack of the team to whom the free throw has been awarded. The hole set then has a \"reasonable amount of time\" (typically about three seconds; there is no FINA rule on this issue) to re-commence play by making a free pass to one of the other players. The defensive team cannot hinder the hole set until the free throw has been taken, but the hole set cannot shoot a goal once the foul has been awarded until the ball has been played by at least one other player. If the hole set attempts a goal without the free throw, the goal is not counted and the defence takes possession of the ball, unless the shot is made outside the 5-metre line. As soon as the hole set has a free pass, the other attacking players attempt to swim (or \"drive\") away from their defenders towards the goal. The players at the flat position will attempt to set a screen (also known as a pick) for the driver. If a driver gets free from a defender, the player calls for the pass from the hole set and attempts a shot at the goal.\nMan-Up (5 on 6).\nIf a defender interferes with a free throw, holds or sinks an attacker who is not in possession or splashes water into the face of an opponent, the defensive player is excluded from the game for twenty seconds, known as a 'kick out' or an ejection. The attacking team typically positions 4 players on the 2 metre line, and 2 players on 5 metre line (4\u20132), passing the ball around until an open player attempts a shot. Other formations include a 3\u20133 (two lines of three attackers each) or arc (attackers make an arc in front of the goal and one offensive player sits in the 'hole' or 'pit' in front of the goal). The five defending players try to pressure the attackers, block shots and prevent a goal being scored for the 20 seconds while they are a player down. The other defenders can only block the ball with one hand to help the goalkeeper. The defensive player is allowed to return immediately if the offence scores, or if the defence recovers the ball before the twenty seconds expires.\nDefense strategy.\nOn defence, the players work to regain possession of the ball and to prevent a goal in their own net. The defence attempts to knock away or steal the ball from the offense or to commit a foul in order to stop an offensive player from taking a goal shot. The defender attempts to stay between the attacker and the goal, a position known as \"inside water\".\nGoalkeeper.\nEven with good backup from the rest of the defenders, stopping attacks can prove very difficult if the goalkeeper remains in the middle of the goal. The most defensible position is along a semicircular line connecting the goalposts and extending out in the centre. Depending on the ball carrier's location, the goalkeeper is positioned along that semicircle roughly a metre out of the goal to reduce the attacker's shooting angle. The goalkeeper stops using their hands to tread water once the opponent enters at about the 7-metre mark and starts to lift their upper body using the eggbeater technique to prepare to block the shot. Finally, the goalkeeper tries to block the ball down, which is often hard for the longer reaches, but prevents an offensive rebound and second shot. As is the case with other defensive players, a goalkeeper who aggressively fouls an attacker in position to score can be charged with a penalty shot for the other team. The goalkeeper can also be ejected for twenty seconds if a major foul is committed. Also, inside the five metre mark, the goalie can swing at the ball with a closed fist without being penalised.\nAdvantage rule.\nIf an offensive player, such as the centre forward, has possession of the ball in front of the goal, the defensive player tries to steal the ball or to keep the centre from shooting or passing. If the defender cannot achieve these aims, he may commit a foul intentionally. The hole set then is given a free throw but must pass off the ball to another offensive player, rather than making a direct shot at the goal. Defensive perimeter players may also intentionally cause a minor foul and then move toward the goal, away from their attacker, who must take a free throw. This technique, called sloughing, allows the defense an opportunity to double-team the hole set and possibly steal the inbound pass. The referee may refrain from declaring a foul, if in his judgment this would give the advantage to the offender's team. This is known as the \"Advantage Rule\".\nInjuries.\nWater polo is a contact sport, with little protective gear besides swimsuits and caps with ear protectors, and thus injuries are common. Among the most frequent serious injuries are those affecting the head and shoulders. Those induced to the head are usually caused by elbows or the ball itself, while shoulder injuries are a result of grabbing and pushing while throwing the ball or simply of repetitive overexertion of joints and muscles when taking hard shots. The hands and fingers are vulnerable areas, due to contact when opponents attempt to steal the ball, or when players block shots. Other injuries take place underwater, such as leg and groin injuries, as many actions cannot be seen from above the surface and not much padding is used to protect the players.\nSunburn is a common minor injury in outdoor matches. Players often don't apply sunscreen as it makes their skin, and hence the ball, slippery; FINA and most state governing bodies forbid the use of copious sunscreen to make the body harder for the opposing team to grip.\nVariations.\nInner tube water polo is a style of water polo in which players, excluding the goalkeeper, are required to float in inner tubes. By floating in an inner tube players expend less energy than traditional water polo players, not having to tread water. This allows casual players to enjoy water polo without undertaking the intense conditioning required for conventional water polo.\nSurf polo, another variation of water polo, is played on surfboards. First played on the beaches of Waikiki in Hawaii in the 1930s and 1940s, it is credited to Louis Kahanamoku, Duke Kahanamoku's brother.\nCanoe polo or kayak polo is one of the eight disciplines of canoeing pursued in the UK, known simply as \"polo\" by its aficionados. Polo combines paddling and ball handling skills with a contact team game, where tactics and positional play are as important as the speed and fitness of the individual athletes.\nFlippa ball is a precursor variant intended for younger and beginner players to learn the basics of polo. In Singapore it has been adopted as a fitness activity for senior citizens. It is played in shallow water and permits touching the bottom of the pool so treading water is not required. Players rotate positions after each score.\nWater polo equipment.\nLittle player equipment is needed to play water polo. Items required in water polo include:\nMajor competitions.\nSummer Olympics.\nMen's water polo at the Olympics was the first team sport introduced at the 1900 games, along with cricket, rugby, football, polo (with horses), rowing and tug of war. Women's water polo became an Olympic sport at the 2000 Sydney Olympic Games after political protests from the Australian women's team.\nOne of the most historically known matches often referred to as the \"Blood in the Water match\", was a 1956 Summer Olympics semi-final match between Hungary and the Soviet Union, played in Melbourne. As the athletes left for the games, the Hungarian revolution began, and the Soviet army crushed the uprising. The Hungarians defeated the Soviets 4\u20130 before the game was called off in the final minute to prevent angry Hungarians in the crowd reacting to Valentin Prokopov punching Ervin Z\u00e1dor.\nOther tournaments.\nEvery 2 to 4 years since 1973, a men's Water Polo World Championship is organized within the FINA World Aquatics Championships. Women's water polo was added in 1986. A second tournament series, the FINA Water Polo World Cup, has been held every other year since 1979. In 2002, FINA organised the sport's first international league, the FINA Water Polo World League.\nThere is also a European Water Polo Championship that is held every other year.\nProfessional water polo is played in many Southern and Eastern European countries like Croatia, Greece, Hungary, Italy, Montenegro, Russia, Serbia, Spain, etc. with the LEN Euroleague tournament played amongst the best teams.\nThere is also a World Club Water Polo Challenge.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47427", "revid": "57939", "url": "https://en.wikipedia.org/wiki?curid=47427", "title": "Atlas (moon)", "text": "Moon of Saturn\nAtlas is an inner satellite of Saturn which was discovered by Richard Terrile in 1980 from Voyager photos and was designated S/1980 S 28. In 1983 it was officially named after Atlas of Greek mythology, because it \"holds the rings on its shoulders\" like the Titan Atlas held the sky up above the Earth. It is also designated Saturn XV.\nAtlas is the closest satellite to the sharp outer edge of the A ring, and was long thought to be a shepherd satellite for this ring. However, now it is known that the outer edge of the ring is instead maintained by a 7:6 orbital resonance with the larger but more distant moons Janus and Epimetheus. In 2004 a faint, thin ring, temporarily designated R/2004 S 1, was discovered in the Atlantean orbit.\nHigh-resolution images taken in June 2005 by \"Cassini\" revealed Atlas to have a roughly spherical centre surrounded by a large, smooth equatorial ridge. The most likely explanation for this unusual and prominent structure is that ring material swept up by the moon accumulates on the moon, with a strong preference for the equator due to the ring's thinness. The size of the equatorial ridge is comparable with the expected Roche lobe of the moon, which means that for any additional particles impacting the equator, the centrifugal force will nearly overcome Atlas's tiny gravity, and they will probably be lost.\nAtlas is significantly perturbed by Prometheus and to a lesser degree by Pandora, leading to excursions in longitude of up to 600\u00a0km (~0.25\u00b0) away from the precessing Keplerian orbit with a rough period of about 3 years. Because the orbits of Prometheus and Pandora are chaotic, it is suspected that Atlas's may be as well.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47428", "revid": "199747", "url": "https://en.wikipedia.org/wiki?curid=47428", "title": "Atlas (mythology)", "text": "Deity in Greek mythology\nIn Greek mythology, Atlas (; , \"\u00c1tl\u0101s\") is a Titan condemned to hold up the heavens or sky for eternity after the Titanomachy. Atlas also plays a role in the myths of two of the greatest Greek heroes: Heracles (Hercules in Roman mythology) and Perseus. According to the ancient Greek poet Hesiod, Atlas stood at the ends of the earth in the extreme west. Later, he became commonly identified with the Atlas Mountains in northwest Africa and was said to be the first King of Mauretania (modern-day Morocco and west Algeria, not to be confused with the modern-day country of Mauritania). Atlas was said to have been skilled in philosophy, mathematics, and astronomy. In antiquity, he was credited with inventing the first celestial sphere. In some texts, he is even credited with the invention of astronomy itself.\nAtlas was the son of the Titan Iapetus and the Oceanid Asia or Clymene. He was a brother of Epimetheus and Prometheus. He had many children, mostly daughters, the Hesperides, the Hyades, the Pleiades, and the nymph Calypso who lived on the island Ogygia.\nFlemish cartographer Gerardus Mercator characterized Atlas as the founder of geography, leading to the modern sense of the term \"atlas\" for a collection of maps after Mercator published his own work in honor of the Titan.\nThe \"Atlantic Ocean\" is derived from \"Sea of Atlas\". The name of Atlantis mentioned in Plato's Timaeus' dialogue derives from \"Atlantis nesos\" (), literally meaning \"Atlantean Island\".\nEtymology.\nThe etymology of the name \"Atlas\" is uncertain. Virgil translated etymologies of Greek names by combining them with adjectives that explained them: for Atlas his adjective is \"durus\", \"hard, enduring\", which suggested to George Doig that Virgil was aware of the Greek \u03c4\u03bb\u1fc6\u03bd\u03b1\u03b9 \"to endure\"; Doig offers the further possibility that Virgil was aware of Strabo's remark that the native North African name for this mountain was \"Douris\". Since the Atlas Mountains rise in the region inhabited by Berbers, it has been suggested that the name might be taken from one of the Berber languages, specifically from the word \"\u00e1dr\u0101r\" \"mountain\".\nTraditionally historical linguists etymologize the Ancient Greek word (genitive: ) as comprised from copulative and the Proto-Indo-European root 'to uphold, support' (whence also ), and which was later reshaped to an nt-stem. However, Robert S. P. Beekes argues that it cannot be expected that this ancient Titan carries an Indo-European name, and he suggests instead that the word is of Pre-Greek origin, as such words often end in \"-ant\".\nMythology.\nWar and punishment.\nAtlas and his brother Menoetius sided with the Titans in their war against the Olympians, the Titanomachy. When the Titans were defeated, many of them (including Menoetius) were confined to Tartarus, but Zeus condemned Atlas to stand at the western edge of the earth and hold up the sky on his shoulders. Thus, he was \"Atlas Telamon\", \"enduring Atlas\", and became a doublet of Coeus, the embodiment of the celestial axis around which the heavens revolve.\nA common misconception today is that Atlas was forced to hold the Earth on his shoulders, but Classical art shows Atlas holding the celestial spheres, not the terrestrial globe; the solidity of the marble globe borne by the Farnese Atlas may have aided the conflation, reinforced in the 16th century by the developing usage of \"atlas\" to describe a corpus of terrestrial maps.\nEncounter with Perseus.\nThe Greek poet Polyidus c.\u2009398 BC tells a tale of Atlas, then a shepherd, encountering Perseus who turned him to stone. Ovid later gives a more detailed account of the incident, combining it with the myth of Heracles. In this account Atlas is not a shepherd, but a king of Mauretania. According to Ovid, Perseus arrives in Atlas's Kingdom and asks for shelter, declaring he is a son of Zeus. Atlas, fearful of a prophecy that warned of a son of Zeus stealing his golden apples from his orchard, refuses Perseus hospitality. In this account, Atlas is turned not just into stone by Perseus, but an entire mountain range: Atlas's head the peak, his shoulders ridges and his hair woods. The prophecy did not relate to Perseus stealing the golden apples but to Heracles, another son of Zeus, and Perseus's great-grandson.\nEncounter with Heracles.\nOne of the Twelve Labours of the hero Heracles was to fetch some of the golden apples that grow in Hera's garden, tended by Atlas's reputed daughters, the Hesperides (which were also called the Atlantides), and guarded by the dragon Ladon. Heracles went to Atlas and offered to hold up the heavens while Atlas got the apples from his daughters.\nUpon his return with the apples, however, Atlas attempted to trick Heracles into carrying the sky permanently by offering to deliver the apples himself, as anyone who purposely took the burden must carry it forever, or until someone else took it away. Heracles, suspecting Atlas did not intend to return, pretended to agree to Atlas's offer, asking only that Atlas take the sky again for a few minutes so Heracles could rearrange his cloak as padding on his shoulders. When Atlas set down the apples and took the heavens upon his shoulders again, Heracles took the apples and ran away.\nIn some versions, Heracles instead built the two great Pillars of Hercules to hold the sky away from the earth, liberating Atlas much as he liberated Prometheus.\nOther mythological characters named Atlas.\nBesides the Titan, there are other mythological characters who were also called Atlas:\nKing of Atlantis.\nAccording to Plato, the first king of Atlantis was also named Atlas, but that Atlas was a son of Poseidon and the mortal woman Cleito. The works of Eusebius and Diodorus also give an Atlantean account of Atlas. In these accounts, Atlas' father was Uranus and his mother was Gaia. His grandfather was Elium \"King of Phoenicia\" who lived in Byblos with his wife Beruth. Atlas was raised by his sister, Basilia.\nKing of Mauretania.\nAtlas was also a legendary king of Mauretania, the land of the Mauri in antiquity roughly corresponding with modern Morocco. In the 16th century, Gerardus Mercator put together the first collection of maps to be called an \"Atlas\" and devoted his book to the \"King of Mauretania\".\nAtlas became associated with Northwest Africa over time. He had been connected with the Hesperides, or \"Nymphs\", which guarded the golden apples, and Gorgons both of which were said to live beyond Ocean in the extreme west of the world since Hesiod's Theogony. Diodorus and Palaephatus mention that the Gorgons lived in the Gorgades, islands in the Aethiopian Sea. The main island was called Cerna, and modern-day arguments have been advanced that these islands may correspond to Cape Verde due to Phoenician exploration.\nThe Northwest Africa region emerged as the canonical home of the King via separate sources. In particular, according to Ovid, after Perseus turns Atlas into a mountain range, he flies over Aethiopia, the blood of Medusa's head giving rise to Libyan snakes. By the time of the Roman Empire, the habit of associating Atlas's home to a chain of mountains, the Atlas Mountains, which were near Mauretania and Numidia, was firmly entrenched.\nOther.\nThe identifying name \"Aril\" is inscribed on two 5th-century BC Etruscan bronze items: a mirror from Vulci and a ring from an unknown site. Both objects depict the encounter with Atlas of Hercle\u2014the Etruscan Heracles\u2014identified by the inscription; they represent rare instances where a figure from Greek mythology was imported into Etruscan mythology, but the name was not. The Etruscan name \"Aril\" is etymologically independent.\nGenealogy.\nSources describe Atlas as the father, by different goddesses, of numerous children, mostly daughters. Some of these are assigned conflicting or overlapping identities or parentage in different sources.\nHyginus, in his \"Fabulae\", adds an older Atlas who is the son of Aether and Gaia.\nCultural influence.\nAtlas' best-known cultural association is in cartography. The first publisher to associate the Titan Atlas with a group of maps was the print-seller Antonio Lafreri, who included a depiction of the Titan on the engraved titlepage he applied to his \"ad hoc\" assemblages of maps, \"Tavole Moderne di Geografia de la Maggior parte del Mondo di Diversi Autori\" (1572). However, Lafreri did not use the word \"Atlas\" in the title of his work; this was an innovation of Gerardus Mercator, who named his work \"Atlas Sive Cosmographicae Meditationes de Fabrica Mundi et Fabricati\" (1585\u20131595), using the word \"Atlas\" as a dedication specifically to honor the Titan Atlas, in his capacity as King of Mauretania, a learned philosopher, mathematician, and astronomer.\nIn psychology, Atlas is used metaphorically to describe the personality of someone whose childhood was characterized by excessive responsibilities.\nAyn Rand's novel \"Atlas Shrugged\" (1957) references the popular misconception of Atlas holding up the entire world on his back by comparing the capitalist and intellectual class as being \"modern Atlases\" who hold the modern world up at great expense to themselves.\nMichael J. Anderson explains that the earliest Greek vase paintings and sculptures depict Atlas with a rigid stance, representing his bearing the burden of Zeus's everlasting punishment. The depiction of Atlas as a muscular figure under the weight of a celestial globe or vault visually express the Greek concept of suffering, resulting from arrogance and rebellion. These artistic patterns explore larger Greek art themes that portray Titans as a symbol of divine punishment and cosmic order.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "47429", "revid": "237572", "url": "https://en.wikipedia.org/wiki?curid=47429", "title": "Atlas", "text": "Collection of maps\nAn atlas is a collection of maps; it is typically a bundle of maps of Earth or of a continent or region of Earth. Advances in astronomy have also resulted in atlases of the celestial sphere or of other planets.\nAtlases have traditionally been bound into book form, but today, many atlases are in multimedia formats. In addition to presenting geographical features and political boundaries, many atlases often feature geopolitical, social, religious, and economic statistics. They also have information about the map and places in it.\nEtymology.\nThe use of the word \"atlas\" in a geographical context dates from 1595 when the Flemish geographer Gerardus Mercator published (\"Atlas or cosmographical meditations upon the creation of the universe and the universe as created\"). This title provides Mercator's definition of the word as a description of the creation and form of the whole universe, not simply as a collection of maps. The volume that was published posthumously one year after his death is a wide-ranging text but, as the editions evolved, it became simply a collection of maps and it is in that sense that the word was used from the middle of the 17th century. The neologism coined by Mercator was a mark of his respect for the Titan Atlas, the \"King of Mauretania\", whom he considered to be the first great geographer.\nHistory.\nThe first work that contained systematically arranged maps of uniform size representing the first modern atlas was prepared by Italian cartographer Pietro Coppo in the early 16th century; however, it was not published at that time, so it is conventionally not considered the first atlas. Rather, that title is awarded to the collection of maps by the Brabantian cartographer Abraham Ortelius printed in 1570.\nAtlases published nowadays are quite different from those published in the 16th\u201319th centuries. Unlike today, most atlases were not bound and ready for the customer to buy, but their possible components were shelved separately. The client could select the contents to their liking, and have the maps coloured/gilded or not. The atlas was then bound. Thus, early printed atlases with the same title page can be different in contents.\nStates began producing national atlases in the 19th century.\nTypes.\nA \"travel atlas\" is made for easy use during travel, and often has spiral bindings, so it may be folded flat. National atlases in Europe are typically printed at a scale of 1:250,000 to 1:500,000; city atlases are 1:20,000 to 1:25,000, doubling for the central area (for example, Geographers' A-Z Map Company's A\u2013Z atlas of London is 1:22,000 for Greater London and 1:11,000 for Central London). A travel atlas may also be referred to as a \"road map\".\nA \"desk atlas\" is made similar to a reference book. It may be in hardback or paperback form.\nStar atlases depict the celestial sphere in cartographic format, focusing on the major named asterisms. There are atlases of the other planets (and their satellites) in the Solar System.\nAtlases of anatomy exist, mapping out organs of the human body or other organisms.\nSelected atlases.\nSome cartographically or commercially important atlases are:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nSources\nOnline atlases\nHistory of atlases\nHistorical atlases online\nOther links"}
{"id": "47430", "revid": "48144537", "url": "https://en.wikipedia.org/wiki?curid=47430", "title": "Atlas (anatomy)", "text": "First spine bone, supports skull\nIn anatomy, the atlas (C1) is the most superior (first) cervical vertebra of the spine and is located in the neck. \nThe bone is named for Atlas of Greek mythology, just as Atlas bore the weight of the heavens, the first cervical vertebra supports the head. However, the term atlas was first used by the ancient Romans for the seventh cervical vertebra (C7) due to its suitability for supporting burdens. In Greek mythology, Atlas was condemned to bear the weight of the heavens as punishment for rebelling against Zeus. Ancient depictions of Atlas show the globe of the heavens resting at the base of his neck, on C7. Sometime around 1522, anatomists decided to call the first cervical vertebra the atlas. Scholars believe that by switching the designation atlas from the seventh to the first cervical vertebra Renaissance anatomists were commenting that the point of man's burden had shifted from his shoulders to his head\u2014that man's true burden was not a physical load, but rather, his mind.\nThe atlas is the topmost vertebra and the axis (the vertebra below it) forms the joint connecting the skull and spine. The atlas and axis are specialized to allow a greater range of motion than normal vertebrae. They are responsible for the nodding and rotation movements of the head.\nThe atlanto-occipital joint allows the head to nod up and down on the vertebral column. The dens acts as a pivot that allows the atlas and attached head to rotate on the axis, side to side.\nThe atlas's chief peculiarity is that it has no body, which has fused with the next vertebra. It is ring-like and consists of an anterior and a posterior arch and two lateral masses.\nThe atlas and axis are important neurologically because the brainstem extends down to the axis.\nStructure.\nAnterior arch.\nThe anterior arch forms about one-fifth of the ring: its anterior surface is convex, and presents at its center the anterior tubercle for the attachment of the \"longus colli\" muscles and the anterior longitudinal ligament; posteriorly it is concave, and marked by a smooth, oval or circular facet (\"fovea dentis\"), for articulation with the odontoid process (dens) of the axis.\nThe upper and lower borders respectively give attachment to the anterior atlantooccipital membrane and the anterior atlantoaxial ligament; the former connects it with the occipital bone above, and the latter with the axis below.\nPosterior arch.\nThe posterior arch forms about two-fifths of the circumference of the ring: it ends behind in the posterior tubercle, which is the rudiment of a spinous process and gives origin to the \"recti capitis posteriores minores\" and the ligamentum nuchae. The diminutive size of this process prevents any interference with the movements between the atlas and the skull.\nThe posterior part of the arch presents above and behind a rounded edge for the attachment of the posterior atlantooccipital membrane, while immediately behind each superior articular process is the superior vertebral notch (\"sulcus arteriae vertebralis\"). This is a groove that is sometimes converted into a foramen by ossification of the posterior atlantooccipital membrane to create a delicate bony spiculum which arches backward from the posterior end of the superior articular process. This anatomical variant is known as an arcuate foramen.\nThis groove transmits the vertebral artery, which, after ascending through the foramen in the transverse process, winds around the lateral mass in a direction backward and medially to enter the vertebrobasilar circulation through the foramen magnum; it also transmits the suboccipital nerve (first spinal nerve).\nOn the under surface of the posterior arch, behind the inferior articular facets, are two shallow grooves, the inferior vertebral notches. The lower border gives attachment to the posterior atlantoaxial ligament, which connects it with the axis.\nLateral masses.\nThe lateral masses are the most bulky and solid parts of the atlas, in order to support the weight of the head.\nEach carries two articular facets, a superior and an inferior.\nVertebral foramen.\nJust below the medial margin of each superior facet is a small tubercle, for the attachment of the transverse atlantal ligament which stretches across the ring of the atlas and divides the vertebral foramen into two unequal parts:\nThis part of the vertebral canal is of considerable size, much greater than is required for the accommodation of the spinal cord.\nTransverse processes.\nThe transverse processes are large; they project laterally and downward from the lateral masses, and serve for the attachment of muscles which assist in rotating the head. They are long, and their anterior and posterior tubercles are fused into one mass; the foramen transversarium is directed from below, upward and backward.\nDevelopment.\nThe atlas is usually ossified from three centers.\nOf these, one appears in each lateral mass about the seventh week of fetal life, and extends backward; at birth, these portions of bone are separated from one another behind by a narrow interval filled with cartilage.\nBetween the third and fourth years they unite either directly or through the medium of a separate center developed in the cartilage.\nAt birth, the anterior arch consists of cartilage; in this a separate center appears about the end of the first year after birth, and joins the lateral masses from the sixth to the eighth year.\nThe lines of union extend across the anterior portions of the superior articular facets.\nOccasionally there is no separate center, the anterior arch being formed by the forward extension and ultimate junction of the two lateral masses; sometimes this arch is ossified from two centers, one on either side of the middle line.\nVariations.\nAccessory transverse foramen of the atlas is present in 1.4\u201312.5% across the population.\nForamen arcuale or a bony bridge above the vertebral artery on the posterior arch of the atlas may be present. This foramen has an overall prevalence of 9.1%. Arch defects refer to the condition where a gap or cleft exists at the anterior arch or posterior arch of the atlas. The prevalence of the posterior arch defect and anterior arch defect was 0.95% and 0.087%, respectively. The anterior arch defect may be presented along with posterior arch defect, a condition known as combined arch defect or bipartite atlas.\nFunction.\nMuscular attachments.\nTransverse processes.\nUpper surface:\nInterior and dorsal part:\nLower surface:\nPosterior tubercle.\nUpper surface:\nLower surface:\nClinical significance.\nThere are 5 types of C1 fractures referred to as the Levine Classification of Atlas Fractures\nType 1: Isolated bony apophysis (transverse process fracture)\nType 2: Isolated posterior arch fractures\nType 3: Isolated anterior arch fracture\nType 4: Comminuted fracture of the lateral mass\nType 5: Bilateral burst fracture (AKA Jefferson Fracture)\nA break in the first vertebra is referred to as a Jefferson fracture.\nCraniocervical junction misalignment is also suspected as a factor in neurodegenerative diseases where altered CSF flow plays a part in the pathological process.\nHyperextension (Whiplash) Injury\nA rear-end traffic collision or a poorly performed rugby tackle can both result in the head being whipped back on the shoulders, causing whiplash. In minor cases, the anterior longitudinal ligament of the spine is damaged which is acutely painful for the patient.\nIn more severe cases, fractures can occur to any of the cervical vertebrae as they are suddenly compressed by rapid deceleration. Again, since the vertebral foramen is large there is less chance of spinal cord involvement.\nThe worst-case scenario for these injuries is that dislocation or subluxation of the cervical vertebrae occurs. This often happens at the C2 level, where the body of C2 moves anteriorly with respect to C3. Such an injury may well lead to spinal cord involvement, and as a consequence quadriplegia or death may occur. More commonly, subluxation occurs at the C6/C7 level (50% of cases).\nReferences.\n \"This article incorporates text in the public domain from the 20th edition of\" Gray's Anatomy \"(1918)\"\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47431", "revid": "28928298", "url": "https://en.wikipedia.org/wiki?curid=47431", "title": "Atlas Mountains", "text": "North African mountain range\nThe Atlas Mountains are a mountain range in the Maghreb in North Africa. They separate the Sahara Desert from the Mediterranean Sea and the Atlantic Ocean; the mountain range stretches around through Morocco, Algeria and Tunisia. The mountains are associated with the Titan Atlas. The range's highest peak is Toubkal, in central Morocco, with an elevation of . The Atlas Mountains are primarily inhabited by Berber populations.\nThe terms for 'mountain' are \"Adrar\" and \"adras\" in some Berber languages, and these terms are believed to be cognates of the toponym \"Atlas\". The mountains are home to a number of animals and plants which are mostly found within Africa but some of which can be found in Europe. Many of these species are endangered and a few are already extinct. The weather is generally cool but summers are sunny, and the average temperature there is 25\u00a0\u00b0C.\nGeology.\nThe basement rock of most of Africa was formed during the Precambrian supereon and is much older than the Atlas Mountains lying on the continent. The Atlas was formed during three subsequent phases of Earth's geology.\nThe first tectonic deformation phase involves only the Anti-Atlas, which was formed in the Paleozoic Era (~300 million years ago) as the result of continental collisions. North America, Europe and Africa were connected millions of years ago.\nThe Anti-Atlas Mountains are believed to have originally been formed as part of the Alleghenian orogeny. These mountains were formed when Africa and America collided and were once a chain rivaling today's Himalayas. Today, the remains of this chain can be seen in the Fall Line region in the Eastern United States. Some remnants can also be found in the later formed Appalachians in North America.\nA second phase took place during the Mesozoic Era (before ~66 My). It consisted of a widespread extension of the Earth's crust that rifted and separated the continents mentioned above. This extension was responsible for the formation of many thick intracontinental sedimentary basins including the present Atlas. Most of the rocks forming the surface of the present High Atlas were deposited under the ocean at that time.\nIn the Paleogene and Neogene Periods (~66 million to ~1.8 million years ago), the mountain chains that today constitute the Atlas were uplifted, as the land masses of Europe and Africa collided at the southern end of the Iberian Peninsula. Such convergent tectonic boundaries occur where two plates slide towards each other forming a subduction zone (if one plate moves underneath the other), and/or a continental collision (when the two plates contain continental crust). In the case of the Africa-Europe collision, it is clear that tectonic convergence is partially responsible for the formation of the High Atlas, as well as for the closure of the Strait of Gibraltar and the formation of the Alps and the Pyrenees.\nHowever, there is a lack of evidence for the nature of the subduction in the Atlas region, or for the thickening of the Earth's crust generally associated with continental collisions. One of the most striking features of the Atlas to geologists is the relatively small amount of crustal thickening and tectonic shortening despite the important altitude of the mountain range. Recent studies suggest that deep processes rooted in the Earth's mantle may have contributed to the uplift of the High and Middle Atlas.\nNatural resources.\nThe Atlas are rich in natural resources. There are deposits of iron ore, lead ore, copper, silver, mercury, rock salt, phosphate, marble, anthracite coal and natural gas among other resources.\nSubranges.\nThe range can be divided into four general regions:\nAnti-Atlas.\nThe Anti-Atlas extends from the Atlantic Ocean in the southwest of Morocco toward the northeast to the heights of Ouarzazate and further east to the city of Tafilalt (altogether a distance of approximately ). In the south it borders the Sahara. The easternmost point of the anti-Atlas is the Jbel Saghro range and its northern boundary is flanked by sections of the High Atlas range. It includes the Djebel Siroua, a massif of volcanic origin with the highest summit of the range at 3,304 m. The Jebel Bani is a much lower range running along the southern side of the Anti Atlas.\nHigh Atlas.\nThe High Atlas in central Morocco rises in the west at the Atlantic coast and stretches in an eastern direction to the Moroccan-Algerian border. It has several peaks over , including the highest summit in North Africa, Toubkal (), and further east Ighil m'Goun (), the second major summit of the range. At the Atlantic and to the southwest, the range drops abruptly and makes a transition to the coast and the Anti-Atlas range. To the north, in the direction of Marrakesh, the range descends less abruptly. On the heights of Ouarzazate the massif is cut through by the Draa Valley which opens southward. It is mainly inhabited by Berber people, who live in small villages and cultivate the high plains of the Ourika Valley. Near Barrage Cavagnac there is a hydroelectric dam that has created the artificial lake Lalla Takerkoust. The lake serves also as a source for fish for the local fishermen.\nThe largest villages and towns of the area are Ouarzazate, Tahannaout, Amizmiz, Imlil, Tin Mal and Ijoukak.\nMiddle Atlas.\nThe Middle Atlas is completely in Morocco and is the northernmost of its three main Atlas ranges. The range lies north of the High Atlas, separated by the Moulouya and Oum Er-Rbia rivers, and south of the Rif mountains, separated by the Sebou River. To the west are the main coastal plains of Morocco with many of the major cities and, to the east, the high barren plateau that lies between the Saharan and Tell Atlas. The high point of the range is the jbel Bou Naceur (3340 m). The Middle Atlas experiences more rain than the ranges to the south, making it an important water catchment for the coastal plains and important for biodiversity. It is home to the majority of the world's population of Barbary macaque.\nSaharan Atlas.\nThe Saharan Atlas of Algeria runs east of the High Atlas, crossing Algeria from the Moroccan border and into Tunisia. The Aures Mountains are often presented as being the easternmost part of the Saharan Atlas. Though not as high as the High Atlas, they reach similar altitudes as the Tell Atlas range that runs to the north of them and closer to the coast. The highest peak in the range, outside of the Aures Mountains, is the high Djebel Aissa. They mark the northern edge of the Sahara Desert. The mountains see some rainfall and are better suited to agriculture than the plateau region to the north. Today, most of the population of the region are Berbers (Imazighen).\nTell Atlas.\nThe Tell Atlas is a mountain chain over in length, belonging to the Atlas mountain ranges and stretching from Eastern Morocco to Tunisia, and through Algeria. It parallels the Mediterranean coast and joins with the Saharan Atlas in Eastern Algeria and Tunisia. The highest summit of the Tell Atlas is the Lalla Khadidja in the Djurdjura range of Kabylia. The western end of the Tell Atlas merges with the Middle Atlas range in Morocco.\nThe area immediately to the south of the Tell Atlas is the high plateau of the Hautes Plaines, with lakes in the wet season and salt flats in the dry. The eastern half of the Tell Atlas has the most humid climate of North Africa, with annual precipitation reaching well above , and sometimes over like in the Collo Peninsula or near Ain Draham. An important amount of snow falls on the summits in winter.\nAur\u00e8s.\nThe Aur\u00e8s Mountains are the easternmost portion of the Atlas mountain range. It covers parts of Algeria and Tunisia. The Aur\u00e8s natural region is named after the range.\nFlora and fauna.\nFlora in the mountains include the Atlas cedar, evergreen oak and many semi-evergreen oaks such as the Algerian oak. In areas that receive more rainfall, like the Kabylie range, cork oaks, arbutus (cane apple), heather shrub, rockroses and lavender can be found. \nAnimals that live in the area include the Barbary macaque (misnamed as the Barbary ape), Barbary leopard, Barbary stag, Barbary sheep, Atlas Mountain badger, Cuvier's gazelle, North African boar, striped hyena, red fox, northern bald ibis, Algerian nuthatch, dipper, and Atlas mountain viper.\nMany animals used to inhabit the Atlas mountains such as the Atlas bear, North African elephant, North African aurochs, bubal hartebeest and Atlas wild ass, but these subspecies are all extinct. Barbary lions are currently extinct in the wild, but descendants exist in captivity.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47433", "revid": "219723", "url": "https://en.wikipedia.org/wiki?curid=47433", "title": "Atlas (architecture)", "text": "Architectural support sculpted in the form of a man\nIn European architectural sculpture, an atlas (also known as an atlant, or atlante or atlantid; plural atlantes) is a support sculpted in the form of a man, which may take the place of a column, a pier or a pilaster. Another Greek term for such a sculptural support is telamon (plural telamones or telamons).\nThe term \"atlantes\" is the Greek plural of the name Atlas\u2014the Titan who was forced to hold the sky on his shoulders for eternity. The alternative term, \"telamones\", also is derived from a later mythological hero, Telamon, one of the Argonauts, who was the father of Ajax. \nThe caryatid is the female precursor of this architectural form in Greece, a woman standing in the place of each column or pillar. Caryatids are found at the treasuries at Delphi and the Erechtheion on the Acropolis at Athens for Athene. They usually are in an Ionic context and represented a ritual association with the goddesses worshiped within. The Atlante is typically life-size or larger; smaller similar figures in the decorative arts are called terms. The body of many Atlantes turns into a rectangular pillar or other architectural feature around the waist level, a feature borrowed from the term. The pose and expression of Atlantes very often show their effort to bear the heavy load of the building, which is rarely the case with terms and caryatids. The herma or herm is a classical boundary marker or wayside monument to a god which is usually a square pillar with only a carved head on top, about life-size, and male genitals at the appropriate mid-point. Figures that are rightly called Atlantes may sometimes be described as herms.\nAtlantes express extreme effort in their function, heads bent forward to support the weight of the structure above them across their shoulders, forearms often lifted to provide additional support, providing an architectural motif. Atlantes and caryatids were noted by the Roman late Republican architect Vitruvius, whose description of the structures, rather than surviving examples, transmitted the idea of atlantes to the Renaissance architectural vocabulary.\nOrigin.\nNot only did the Caryatids precede them, but similar architectural figures already had been made in ancient Egypt out of monoliths. Atlantes originated in Greek Sicily and in Magna Graecia, Southern Italy. The are fallen ones from the Early Classical Greek temple of Zeus, the \"Olympeion\", in Agrigento, Sicily. Atlantes also played a significant role in Mannerist and Baroque architecture. \nDuring the eighteenth and nineteenth centuries, the designs of many buildings featured glorious atlantes that looked much like Greek originals. Their inclusion in the final design\nfor the portico of the Hermitage Museum in St. Petersburg that was built for Tsar Nicholas I of Russia in the 1840\u2019s made the use of atlantes especially fashionable. The Hermitage portico incorporates ten enormous atlantes, approximately three times life-size, carved from Serdobol granite, which were designed by Johann Halbig and executed by the sculptor Alexander Terebenev.\nMesoamerica.\nSimilar carved stone columns or pillars in the shape of fierce men at some sites of Pre-Columbian Mesoamerica are typically called Atlantean figures. These figures are considered to be \"massive statues of Toltec warriors\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47434", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=47434", "title": "TAIEX", "text": "Taiwanese stock market index\nThe Taiwan Stock Exchange Capitalization Weighted Stock Index (), TWSE Capitalization Weighted Stock Index, or TAIEX () is a stock market index for companies traded on the Taiwan Stock Exchange (TWSE). The TAIEX covers all of the listed stocks excluding preferred stocks, full-delivery stocks and newly listed stocks, which are listed for less than one calendar month. It was first published in 1967 by TWSE with 1966 being the base year with a value of 100.\nThe Taiwanese stock market experienced a \"bubble economy\" from 1986 to 1990. When in 1986 the index was around 1000 points, it reached a peak of over 12,000 points in February 1990 then plummeted to around 2500 points by October 1990.\nAnnual Returns.\nThe following table shows the annual development of the TAIEX since 1966.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47436", "revid": "12336988", "url": "https://en.wikipedia.org/wiki?curid=47436", "title": "Atlas (topology)", "text": "Set of charts that describes a manifold\nIn mathematics, particularly topology, an atlas is a concept used to describe a manifold. An atlas consists of individual \"charts\" that, roughly speaking, describe individual regions of the manifold. In general, the notion of atlas underlies the formal definition of a manifold and related structures such as vector bundles and other fiber bundles.\nCharts.\nThe definition of an atlas depends on the notion of a \"chart\". A chart for a topological space \"M\" is a homeomorphism formula_1 from an open subset \"U\" of \"M\" to an open subset of a Euclidean space. The chart is traditionally recorded as the ordered pair formula_2.\nWhen a coordinate system is chosen in the Euclidean space, this defines coordinates on formula_3: the coordinates of a point formula_4 of formula_3 are defined as the coordinates of formula_6 The pair formed by a chart and such a coordinate system is called a local coordinate system, coordinate chart, coordinate patch, coordinate map, or local frame.\nFormal definition of atlas.\nAn atlas for a topological space formula_7 is an indexed family formula_8 of charts on formula_7 which covers formula_7 (that is, formula_11). If for some fixed \"n\", the image of each chart is an open subset of \"n\"-dimensional Euclidean space, then formula_7 is said to be an \"n\"-dimensional manifold. \nThe plural of atlas is \"atlases\", although some authors use \"atlantes\".\nAn atlas formula_13 on an formula_14-dimensional manifold formula_7 is called an adequate atlas if the following conditions hold:\nEvery second-countable manifold admits an adequate atlas. Moreover, if formula_23 is an open covering of the second-countable manifold formula_7, then there is an adequate atlas formula_13 on formula_7, such that formula_27 is a refinement of formula_28.\nTransition maps.\nformula_7\nformula_30\nformula_31\nformula_32\nformula_33\nformula_34\nformula_35\nformula_36\nformula_36\nTwo charts on a manifold, and their respective transition map\nA transition map provides a way of comparing two charts of an atlas. To make this comparison, we consider the composition of one chart with the inverse of the other. This composition is not well-defined unless we restrict both charts to the intersection of their domains of definition. (For example, if we have a chart of Europe and a chart of Russia, then we can compare these two charts on their overlap, namely the European part of Russia.)\nTo be more precise, suppose that formula_38 and formula_39 are two charts for a manifold \"M\" such that formula_40 is non-empty.\nThe transition map formula_41 is the map defined by\nformula_42\nNote that since formula_43 and formula_44 are both homeomorphisms, the transition map formula_45 is also a homeomorphism.\nMore structure.\nOne often desires more structure on a manifold than simply the topological structure. For example, if one would like an unambiguous notion of differentiation of functions on a manifold, then it is necessary to construct an atlas whose transition functions are differentiable. Such a manifold is called differentiable. Given a differentiable manifold, one can unambiguously define the notion of tangent vectors and then directional derivatives.\nIf each transition function is a smooth map, then the atlas is called a smooth atlas, and the manifold itself is called smooth. Alternatively, one could require that the transition maps have only \"k\" continuous derivatives in which case the atlas is said to be formula_46.\nVery generally, if each transition function belongs to a pseudogroup formula_47 of homeomorphisms of Euclidean space, then the atlas is called a formula_48-atlas. If the transition maps between charts of an atlas preserve a local trivialization, then the atlas defines the structure of a fibre bundle.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "47437", "revid": "50996310", "url": "https://en.wikipedia.org/wiki?curid=47437", "title": "Pope Paul III", "text": "Head of the Catholic Church from 1534 to 1549\nPope Paul III (; ; born Alessandro Farnese; 29 February 1468 \u2013 10 November 1549) was head of the Catholic Church and ruler of the Papal States from 13 October 1534 to his death, in November 1549.\nPaul III came to the papal throne in the time following the sack of Rome in 1527, which was rife with uncertainties in the Catholic Church as the Protestant Reformation progressed. His pontificate initiated the Catholic Reformation with the Council of Trent in 1545, and witnessed wars of religion in which Emperor Charles V launched military campaigns against the Protestants in Germany. He recognized new Catholic religious orders and societies such as the Jesuits, the Barnabites, and the Congregation of the Oratory. His efforts were distracted by nepotism to advance the power and fortunes of his family, including his illegitimate son Pier Luigi Farnese.\nPaul III was a significant patron of artists, including Michelangelo, and Nicolaus Copernicus dedicated his heliocentric treatise to him.\nBiography.\nEarly career and family.\nBorn in 1468 at Canino, Latium (then part of the Papal States), Alessandro Farnese was the second son of Pier Luigi I Farnese, Signore di Montalto (1435\u20131487) and Giovanna Caetani, a member of the Caetani family which had produced Pope Gelasius II and Pope Boniface VIII. The Farnese family had prospered over the centuries, but it was Alessandro's ascendency to the papacy and his dedication to family interests which brought about the most significant increase in the family's wealth and power.\nAlessandro was given a humanist education at the University of Pisa and the court of Lorenzo de' Medici. Initially trained as an apostolic notary, he joined the Roman Curia in 1491 and in 1493 Pope Alexander VI appointed him Cardinal-Deacon of \"Santi Cosma e Damiano\". Alessandro's sister, Giulia, was reputedly a mistress of Alexander VI, and might have been instrumental in securing this appointment for her brother. For this reason, he was sometimes mockingly referred to as the \"Borgia brother-in-law\", just as Giulia was mocked as \"the Bride of Christ\". Much later (in 1535), the Venetian nobleman Soriano recorded that Alessandro was called \"cardinale Fregnese\" (Cardinal Pussy, or Cardinal Cunt) on account of the relationship between his sister and Alexander VI.\nAs a young cleric, Alessandro lived a notably dissolute life, taking a mistress, Silvia Ruffini. Between about 1500 and 1510, she gave birth to at least four children: Costanza, Pier Luigi (who was later created Duke of Parma), Paolo, and Ranuccio. In July 1505, Pope Julius II legitimated the two eldest sons so that they could inherit the Farnese family estates. On 23 June 1513, Pope Leo X published a second legitimation of Pier Luigi, and also legitimized Ranuccio (the second son Paolo had already died).\nOn 28 March 1509, Alessandro was named Bishop of Parma, but he was not ordained a priest until 26 June 1519 and not consecrated a bishop until 2 July 1519. As Bishop of Parma, he came under the influence of his vicar-general, Bartolomeo Guidiccioni. This led to Alessandro breaking off the relationship with his mistress and committing himself to reform in his diocese. Under Pope Clement VII (1523\u201334) he was named Cardinal Bishop of Ostia and Dean of the College of Cardinals.\nPontificate.\nPapal election.\nOn the death of Clement VII in 1534, he was elected as Pope Paul III on 13 October 1534. Farnese, who did not fall within any of the factions, was considered a very good choice by the cardinals since his state of health denoted a short papacy which would give those cardinals time to select a proper candidate for a future conclave. On 3 November, Paul III was formally crowned by the protodeacon Innocenzo Cybo.\nThe elevation to the cardinalate of his grandsons, Alessandro Farnese, aged 14, and Guido Ascanio Sforza, aged 16, displeased the reform party and drew a protest from Emperor Charles V, but this was forgiven when, shortly after, he introduced into the Sacred College Reginald Pole, Gasparo Contarini, Jacopo Sadoleto, and Giovanni Pietro Caraffa, who would become Pope Paul IV.\nPolitics and religion.\nThe fourth pope during the period of the Protestant Reformation, Paul III became the first to take active reform measures in response to Protestantism. Soon after his elevation, 2 June 1536, Paul III summoned a general council to meet at Mantua in the following May, but the opposition of the Protestant princes and the refusal of Federico II Gonzaga, Duke of Mantua to assume the responsibility of maintaining order frustrated the project. Paul III first deferred for a year and then discarded the whole project.\nIn 1536, Paul III invited a committee of nine eminent prelates, distinguished by learning and piety alike, to report on the reformation and rebuilding of the Church. In 1537 they produced the celebrated \"Consilium de emendenda ecclesia\", exposing gross abuses in the Roman Curia, the church administration, and public worship; and proffering bold proposals aimed at abolishing such abuses. The report was widely printed, and the pope was in earnest when he took up the problem of reform. He clearly perceived that Emperor Charles V would not rest until the problems were grappled with in earnest.\nHowever, to the Protestants, the report seemed far from thorough; Martin Luther had his edition (1538) prefaced with a vignette showing the cardinals cleaning the Augean stable of the Roman Church with foxtails instead of brooms. In the end, no results followed from the committee's recommendations.\nAs a consequence of the extensive campaign against \"idolatry\" in England, culminating with the dismantling of the shrine of St. Thomas Becket at Canterbury, Paul III excommunicated Henry VIII on 17 December 1538 and issued an interdict on England.\nIn 1534, a decision by Paul III favoured the activity of merchants of all nationalities and religions from the Levant and allowed them to settle with their families in Ancona, which had become part of the Papal States under his predecessor Clement VII. This decision helped make Ancona a prosperous trading city for centuries to come. A Venetian travelling through Ancona in 1535 recorded that the city was \"full of merchants from every nation and mostly Greeks and Turks.\" In the second half of the 16th century, the presence of Greek and other merchants from the Ottoman Empire declined after a series of restrictive measures taken by the Italian authorities and the pope.\nAround this time, family complications arose. In order to vest his grandson Ottavio Farnese with the Duchy of Camerino, Paul forcibly wrested the same from the duke of Urbino (1540). He also incurred virtual war with his own subjects and vassals by the imposition of burdensome taxes. Perugia, renouncing its obedience, was besieged by Paul's son, Pier Luigi, and forfeited its freedom entirely on its surrender. The burghers of Colonna were duly vanquished, and Ascanio was banished (1541). After this, the time seemed ripe for annihilating heresy.\nIn 1540, the Church officially recognized the society forming about Ignatius of Loyola, which became the Society of Jesus. In 1542, a second stage in the process of Counter-Reformation was marked by the institution, or reorganization, of the Congregation of the Holy Office of the Inquisition.\nOn another side, the emperor was insisting that Rome should forward his designs toward a peaceable recovery of the German Protestants. Accordingly, Paul III despatched Giovanni Morone (not as yet a cardinal) as nuncio to Hagenau and Worms in 1540; and in 1541 Cardinal Gasparo Contarini took part in the adjustment proceedings at the Conference of Regensburg. It was Contarini who proposed the famous formula \"by faith alone are we justified,\" which did not, however, supersede the Roman Catholic doctrine of good works. At Rome, this definition was rejected in the consistory of 27 May, and Luther declared that he could accept it only provided the opposers would admit that this formula constituted a change of doctrine.\nHowever, after the Regensburg Conference had proved fruitless, the emperor insisted on a still larger council, with the final result being the Council of Trent, which finally was convoked on 15 March 1545, under the bull \"Laetare Hierusalem\".\nMeanwhile, after the peace of Crespy (September 1544), Emperor Charles V (1519 \u2013 1556) began to put down Protestantism by force. Pending the Diet of Worms in 1545, the emperor concluded a covenant of joint action with the papal legate Cardinal Alessandro Farnese, with Paul III agreeing to aid in the projected war against the German Protestant princes and estates. This prompt acquiescence was probably grounded on personal motives: Because the emperor was preoccupied in Germany, the moment now seemed opportune for the pope to acquire for his son Pier Luigi the duchies of Parma and Piacenza. Although these belonged to the Papal States, Paul III planned to overcome the reluctance of the cardinals by exchanging these papal duchies for the less valuable domains of Camerino and Nepi. The emperor agreed, welcoming the prospect of 12,000 infantry, 500 cavalry, and considerable funds from the pope.\nIn Germany the campaign began in the west, where Archbishop of Cologne Hermann of Wied had converted to Protestantism in 1542. Emperor Charles began open warfare against the Protestant princes, estates, and cities allied in the Schmalkaldic League (see Philip of Hesse). Hermann was excommunicated on 16 April 1546 and compelled by the emperor to abdicate in February 1547. By the close of 1546, Charles V had subjugated South Germany. The victory at the Battle of M\u00fchlberg on 24 April 1547 established his imperial sovereignty everywhere in Germany, and the two leaders of the League were captured. The emperor declared the Augsburg Interim as a magnanimous compromise with the defeated schismatics.\nAlthough the emperor had subdued the German Protestant armies, he had failed to support the pope's territorial ambitions for his son Pier Luigi, and relations between them cooled. The situation came to a total rupture when Ferrante Gonzaga, the imperial vice-regent, forcibly expelled Pier Luigi.\nIn 1547, the pope's son was assassinated at Piacenza, and Paul III placed some of the blame on the emperor. In the same year, and after the death of Francis I of France (1515\u201347) deprived the pope of a potential ally, the stress of circumstances compelled him to accept the ecclesiastical measures in the emperor's Interim.\nWith reference to the assassinated prince's inheritance, the restitution of which Paul III demanded ostensibly in the name of the church, the pope's design was thwarted by the emperor, who refused to surrender Piacenza, and by Pier Luigi's heir in Parma, Ottavio Farnese.\nIn consequence of a violent altercation on this account with Cardinal Farnese, Paul III, at the age of 81, became so overwrought that an attack of sickness ensued from which he died on 10 November 1549.\nPaul III proved unable to suppress the Protestant Reformation, but it was during his pontificate that the foundation was laid for the Counter-Reformation. He decreed the second and final excommunication of Henry VIII of England in December 1538. His efforts in Parma led to the War of Parma two years after his death.\nSlavery and \"Sublimis Deus\".\nIn May\u2013June 1537, Paul issued the bull \"Sublimis Deus\" (also known as \"Unigenitus\" and \"Veritas ipsa\"), described by Prein (2008) as the \"Magna Carta\" for the human rights of the indigenous peoples of the Americas in its declaration that \"the Indians were human beings and they were not to be robbed of their freedom or possessions\". The subsequent implementing document \"Pastorale officium\" declared automatic excommunication for anyone who failed to abide by the new ruling.\nHowever, it met with strong opposition from the Council of the West Indies and the Crown, which declared that it violated their patronato rights, and the pope annulled the orders the following year with the document \"Non Indecens Videtur\". Stogre (1992) notes that \"Sublimis Deus\" is not present in Denzinger, the authoritative compendium of official Catholic teachings, and Davis (1988) asserts it was annulled due to a dispute with the Spanish crown. However, the original bull continued to circulate and be quoted by las Casas and others who supported Indian rights.\nAccording to Falkowski (2002) \"Sublimis Deus\" had the effect of revoking the bull of Alexander VI, \"Inter caetera\", but still leaving the colonizers the duty of converting the native people. Father Gustavo Gutierrez describes it as \"the most important papal document relating to the condition of native Indians and that it was addressed to all Christians\". Maxwell (1975) notes that the bull did not change the traditional teaching that the enslavement of Indians was permissible if they were considered \"enemies of Christendom\", as this would be considered by the Church as a \"just war\". He further argues that the Indian nations had every right to self-defence. Stark - 2003 - describes the bull as \"magnificent\" and believes that it was long forgotten due to the neglect of Protestant historians. Falola noted that the bull related to the native populations of the New World and did not condemn the transatlantic slave trade stimulated by the Spanish monarchy and the Holy Roman Emperor.\nIn 1545, Paul repealed an ancient law that allowed slaves to claim their freedom under the emperor's statue on Rome's Capitoline Hill, in view of the number of homeless people and tramps in the city. The decree included those who had become Christians after their enslavement and those born to Christian slaves. The right of inhabitants of Rome to publicly buy and sell slaves of both sexes was affirmed. Stogre (1992) asserts that the lifting of restrictions was due to a shortage of slaves in Rome. In 1548, Paul authorized the purchase and possession of Muslim slaves in the Papal states.\nAlso in 1537, Paul issued \"Altitudo divini consilii\". This bull discusses evangelization and conversion, including the real way to apply the sacraments, in particular baptism. This was especially important in the early days of colonial rule, when hundreds and sometimes thousands of indigenous people were baptized every day. One interesting aspect of this bull is its discussion of how to deal with local practices, for example, polygamy. After their conversion, polygamous men had to marry their first wife, but if they could not remember which wife was the first, they then \"could choose among the wives the one they preferred.\"\nPatron of the arts.\nArguably the most significant artistic work produced during Paul's reign was the \"Last Judgement\" by Michelangelo in the Sistine Chapel of the Vatican Palace. Although the work was commissioned by Paul III's predecessor, Pope Clement VII, following the latter's death in 1534 Paul renewed the commission and oversaw its completion in 1541.\nAs a cardinal, Alessandro had begun construction of the Palazzo Farnese in central Rome, and its planned size and magnificence increased upon his election to the papacy. The palace was initially designed by the architect Antonio da Sangallo the Younger, received further architectural refinement from Michelangelo, and was completed by Giacomo della Porta. Like other Farnese family buildings, the imposing palace proclaims the family's power and wealth, similarly to Alessandro's Villa Farnese at Caprarola. In 1546, after the death of Sangallo, Paul appointed the elderly Michelangelo to take supervision of the building of St. Peter's Basilica. Paul also commissioned Michelangelo to paint the 'Crucifixion of St. Peter' and the 'Conversion of St. Paul' (1542\u201350), his last frescoes, in the Pauline Chapel of the Vatican.\nPaul III's artistic and architectural commissions were numerous and varied. The Venetian artist Titian painted a portrait of the pope in 1543, and in 1546, the well-known portrait of Paul III with his grandsons Cardinal Alessandro Farnese and Ottavio Farnese, Duke of Parma. Both are now in the Capodimonte Museum in Naples. The military fortifications in Rome and the Papal States were strengthened during his reign. He had Michelangelo move the ancient bronze of the Emperor Marcus Aurelius to the Capitoline Hill, where it became the centerpiece to the Piazza del Campidoglio.\nOther activities.\nSociety of Jesus and religious orders.\nOn 27 September 1540, Paul III formally approved the establishment of the Society of Jesus in the papal bull, \"Regimini militantis Ecclesiae\". Originally, Paul III restricted the fledgling order to 60 members in the bull \"Iniunctum nobis\", but he lifted that restriction upon seeing just how effective they were in their missionary actions. In 1548, he permitted Saint Ignatius of Loyola to print his \"Spiritual Exercises\".\nSimilarly, in 1540, Paul III approved the Rule of the Somaschi Fathers, and on 9 June 1544, he approved the Rule for the Ursulines in the bull \"Regimini Universalis\".\nConsistories.\nThroughout his papacy, Paul III elevated 71 cardinals in 12 consistories. Six of those whom he named, and later revealed publicly, were nominated \"in pectore\". Among those he named were his three immediate successors: Giovanni Maria Ciocchi del Monte (the future Pope Julius III), Marcello Cervini (the future Pope Marcellus II), and Gian Pietro Carafa (the future Pope Paul IV). Among those he named were Reginald Pole, Rodrigo Luis de Borja y de Castre-Pin\u00f3s (the great-great-grandson of Pope Alexander VI), Ippolito II d'Este (the grandson of Pope Alexander VI), and Enrique de Borja y Arag\u00f3n (the great-grandson of Pope Alexander VI). Paul III also named John Fisher as a cardinal, but King Henry VIII had him executed after warning the pope not to nominate him.\nIn 1535, Paul III intended to nominate Desiderius Erasmus to the cardinalate, but he declined on the grounds of ill health and his age. In preparations for the 1542 consistory, Paul III intended to nominate Giovanni Guidiccioni, but the latter died before the consistory took place. In that 1542 consistory, according to Conradus Eubel, the pope is said to have reserved an undefined number of other cardinals \"in pectore\".\nCanonizations.\nDuring his papacy, Paul III canonized Gin\u00e9s de la Jara (1541).\nDeath.\nOn 3 November 1549, Paul III celebrated the anniversary of his papal coronation. However, the pope was severely depressed by the deceit of his own family and the fall of Parma to Emperor Charles V, and it is known that he had a very heated argument with his cardinal nephew, Alessandro Farnese, to the point that he grabbed his red beretta, tore it into shreds, and threw it down to the ground in his anger. He had worked himself up so much to the point that he may have suffered a heart attack. On 6 November, the pope suddenly contracted a fever, retreating to the Quirinal Hill where he had hoped that the fresher air would help ease his malady. On 7 November, the agent of King Ferdinand I of Bohemia and Hungary, Diego Lasso, wrote that the pope's temperature had increased that morning, while the French ambassador in Rome reported to King Henry II of France that Paul III suffered from a catarrh at 7:00 pm, opining that the pope had very little time to live. \nPaul III died on 10 November 1549 from a catarrh. It is said that he repented of his nepotism on his deathbed.\nPaul III's bronze tomb, executed by Guglielmo della Porta, is located in Saint Peter's Basilica.\nFictional portrayal.\nStendhal's novel \"La Chartreuse de Parme\" was inspired by an inauthentic Italian account of the dissolute youth of Alessandro Farnese.\nThe character of Pope Paul III, played by Peter O'Toole, in the series \"The Tudors\" is loosely inspired by him. The young Alessandro Farnese is played by Diarmuid Noyes in the serial \"Borgia\" and by Cyron Melville in \"The Borgias\". His image is portrayed in a parody of the \"Sgt. Pepper's Lonely Hearts Club Band\" album cover, placed inside of the album \"We're Only in It for the Money\" by the Mothers of Invention.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47438", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=47438", "title": "Hephaistos", "text": ""}
{"id": "47442", "revid": "42988205", "url": "https://en.wikipedia.org/wiki?curid=47442", "title": "Roc", "text": "Roc, ROC, or R.O.C. may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47443", "revid": "48181213", "url": "https://en.wikipedia.org/wiki?curid=47443", "title": "Vera Wang", "text": "American fashion designer (born 1949)\nVera Ellen Wang (; born June 27, 1949) is an American fashion designer. Wang initially pursued a career in figure skating before transitioning to fashion. She got her start working for Walmart then worked for \"Vogue\" and Ralph Lauren before launching her own bridal gown boutique in 1990.\nWang gained international recognition for her wedding dress designs. Her gowns have been worn by numerous celebrities. She expanded her brand to include ready-to-wear fashion, accessories, fragrances, and home goods. In December of 2024, Wang sold her namesake brand after 35 years in business to WHP Global.\nEarly life.\nVera Ellen Wang was born June 27, 1949, in New York City to Chinese parents who immigrated to the United States in the mid-1940s. Her mother, Florence Wu (Wu Chifang), worked as a translator for the United Nations, while her father, Cheng Ching Wang (Wang Chengqing), a graduate of Yanjing University and MIT, owned a medicine company, and held the following positions: Director, Singapore Petroleum Company Pte. Ltd., Chairman &amp; President, Oceanic Petroleum Corporation, Chairman &amp; President, Oceanic Petroleum (Asia) Corporation, Chairman &amp; President of Summit Group of Companies (now U.S. Summit Company), Chairman of the Vera Wang Group 1990-1998. Wang has one brother, Kenneth, who is a life member of MIT Corporation, a board of trustees that governs the Massachusetts Institute of Technology.\nWang began figure skating at the age of eight, training under Peter Dunfield and Sonya Klopfer in Denver during the summers and with the Skating Club of New York during the rest of the year. While in high school, she trained with pairs partner James Stuart, and competed at the 1968 U.S. Figure Skating Championships. She was featured in \"Sports Illustrated\"'s \"Faces in the Crowd\" in the January 9, 1968 issue. When she failed to make the U.S. Olympics team, she said, \"I was devastated when I did not qualify for the Olympic team.\" Then she entered the fashion industry. Wang continues to enjoy skating, saying, \"Skating is multidimensional\".\nWang attended Friends Seminary, graduated from Chapin School in 1967, attended the University of Paris, and earned a degree in art history from Sarah Lawrence College.\nIn 1968, Wang was presented as a debutante to high society at the International Debutante Ball at the Waldorf Astoria New York.\nCareer.\nWang was hired to be an editor at \"Vogue\" immediately upon graduation from Sarah Lawrence College, making her the youngest editor at that magazine. She stayed at \"Vogue\" for 17 years, leaving in 1987 to join Ralph Lauren, for whom she worked for two years. At 40, she resigned and became an independent bridal wear designer.\nWang has made wedding gowns for public figures such as Hayley Williams, Vanessa Hudgens, Chelsea Clinton, Karenna Gore, Ivanka Trump, Campbell Brown, Alicia Keys, Mariah Carey, Victoria Beckham, Sarah Michelle Gellar, Avril Lavigne, Hilary Duff, Khloe Kardashian, and Kim Kardashian. Wang started off as being best known for her elegant wedding dresses. Wang's evening wear has also been worn by Michelle Obama.\nFigure skaters who have worn costumes designed by Wang at the Winter Olympic Games include Nancy Kerrigan (1992 and 1994), Michelle Kwan (1998 and 2002), Evan Lysacek (2010), and Nathan Chen (2018 and 2022). Wang was inducted into the U.S. Figure Skating Hall of Fame in 2009 for her contribution to the sport as a costume designer. She designed the uniforms worn by the Philadelphia Eagles Cheerleaders.\nOn October 23, 2001, her book \"Vera Wang on Weddings\" was released. In June 2005, she won the Council of Fashion Designers of America (CFDA) Womenswear Designer of the Year. On May 27, 2006, Wang was awarded the Andr\u00e9 Leon Talley Lifetime Achievement Award from the Savannah College of Art and Design.\nWang's evening wear has been worn by stars at many red carpet events, including Viola Davis at the 2012 Academy Awards, and Sofia Vergara at the 65th Emmy Awards.\nShe was awarded the Council of Fashion Designers of America Lifetime Achievement Award in 2013.\nIn 2006, Wang reached a deal with Kohl's, a chain of department stores, to produce a less expensive line of ready-to-wear clothing exclusively for them called Simply Vera.\nForbes placed her the 34th in the list America's Richest Self-Made Women 2018, her revenues rising to $630\u00a0million in that year.\nOn September 10, 2019, after a sabbatical of two years during which she had presented her collections only via films, Vera Wang returned to the New York Fashion Week runway for her Spring/Summer 2020 fashion show, which celebrated the 30th anniversary of her brand. The show received very positive reviews, with Godfrey Deeny describing it as a \"notable collection by one of the few New York designers with a truly distinctive fashion DNA\", while Bridget Foley presented Wang's creations as \"Beautiful, seriously designed clothes, presented with gutsy panache\". However, the show was marred by several major footwear malfunctions, especially during the finale when four models fell down, including Fei Fei Sun, who fell twice in a row, leading chief fashion critic Vanessa Friedman to state that \"In 2019, no woman should be tortured by what she wears\".\nIn December 2024, WHP Global, a brand management firm based in New York, announced an agreement to acquire the IP of the Vera Wang fashion brand. As part of the transaction, Wang herself will continue the role as Founder &amp; Chief Creative Officer and will join WHP Global as a shareholder.\nRetail.\nIn 1990, Wang opened her first design salon in the Carlyle Hotel in New York City that features her trademark bridal gowns. She has since opened bridal boutiques in New York, London, Tokyo, and Sydney and has also expanded her brand through her fragrance, jewelry, eyewear, shoes, and homeware collections.\n\"White by Vera Wang\" launched on February 11, 2011, at David's Bridal. Prices of the bridal gowns range from $600 to $1,400. In 2002, Wang began to enter the home fashion industry and launched The Vera Wang China and Crystal Collection, followed by the 2007 release of her diffusion line called Simply Vera, which are sold exclusively by Kohl's.\nIn spring 2012, Wang teamed up with Men's Wearhouse to offer two tuxedo styles available in both the retail and rental areas of their inventory.\nIn June 2012, she expanded in Australia with the opening of \"Vera Wang Bride Sydney\" and her first Asian flagship store \"Vera Wang Bridal Korea\", helmed by President Jung Mi-ri, in upmarket neighborhood Cheongdam-dong in Gangnam-gu, Seoul.\nIn a 2013 interview with CBS, Wang described her transition from journalist to businesswoman as \"painful, and not only that, I have no choice. So I think when you start there's a certain innocence because of that freedom, and as you evolve you begin to see the parameters of what you can and can't do. So I make decisions that are very tiny that will affect an hour of work, I make decisions that will impact the lives of the people that work for me. It's in fashion as well, micro work, a centimeter of proportion and then it's macro to see what a vision is on the red carpet.\"\nPersonal life.\nIn June 1989, Wang married investor Arthur P. Becker in an interfaith Baptist and Jewish ceremony. They have two daughters, both of whom were adopted. In July 2012, the couple announced their separation.\nIn popular culture.\nSeveral movies and television shows have featured Wang's works, including \"The West Wing\", \"The Newsroom\", \"Gossip Girl\", \"Sex and the City\", \"Revenge\", \"The Simpsons\", and the film \"Sex and the City\".\nWang appeared in a cameo in season 5 episode 11 of the TV series \"Gossip Girl\" and as herself in the second season of \"Ugly Betty\" in episode 2.07.\nRecognition.\nShe was recognized as one of the BBC's 100 women of 2021.\nIn 2023, she received the Board of Directors' Tribute at the 2023 CFDA Fashion Awards for her Bridal impact.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47444", "revid": "44721228", "url": "https://en.wikipedia.org/wiki?curid=47444", "title": "Copycat suicide", "text": "Emulation of another suicide\nA copycat suicide is defined as an emulation of another suicide that the person attempting suicide knows about either from local knowledge or due to accounts or depictions of the original suicide on television and in other media. The publicized suicide serves as a trigger, in the absence of protective factors, for the next suicide by a susceptible or suggestible person. This is referred to as suicide contagion.\nA spike in emulation suicides after a widely publicized suicide is known as the Werther effect, after rumours of such a spike following the publication of Goethe's novel \"The Sorrows of Young Werther\".\nSuicides occasionally spread through a school system, through a community, or in terms of a celebrity suicide wave, nationally. This is called a suicide cluster. \"Point clusters\" are clusters of suicides in both time and space, and have been linked to direct social learning from nearby individuals. \"Mass clusters\" are clusters of suicides in time but not space, and have been linked to the broadcasting of information concerning celebrity suicides via the mass media.\nHistory.\nOne of the earliest known associations between the media and suicide arose from Goethe's novel \"Die Leiden des jungen Werthers\" (\"The Sorrows of Young Werther\"). Soon after its publication in 1774, young men began to mimic the main character by dressing in yellow pants and blue jackets. In the novel, Werther shoots himself with a pistol after he is rejected by the woman he loves, and shortly after its publication there were reports of young men using the same method to kill themselves in acts of hopelessness.\nThis resulted in the book being banned in several places. Hence the term \"Werther effect\" is used in the technical literature to designate copycat suicides. The term was coined by researcher David Phillips in 1974.\nReports in 1985 and 1989 by Phillips and his colleagues found that suicides and other accidents seem to rise after a well-publicized suicide.\nDemographic factors.\nPeople who are young or old \u2013 but not middle-aged \u2013 seem to be most susceptible to this effect. At least five percent of youth suicides may be influenced by contagion.\nDue to the effects of differential identification, the people who attempt to copy a suicidal act tend to have the same age and gender as the triggering suicide.\nAccording to a study from four South Korean researchers, the rate of suicide significantly increased following celebrity suicides between 2005 and 2008. Additionally, substantial upsurges were discovered in the subgroups that corresponded to each celebrity, particularly in the group when all variables (sex, age, and technique) were comparable.\nTiming.\nThese suicidal actions tend to happen in the days and sometimes weeks after a suicide is announced. In exceptional cases, such as a widely discussed suicide by a celebrity, an increased level of thinking about suicide may persist for up to one year.\nFactors in suicide reporting.\nCopycat suicide is mostly blamed on the media. A study conducted in 2002 found evidence that \"the influence of media on suicidal behaviour has been shown for newspaper and television reports of actual suicides, film and television portrayals of suicides, and suicide in literature, especially suicide manuals.\" \"Hearing about a suicide seems to make those who are vulnerable feel they have permission to do it,\" Phillips said. He cited studies that showed that people were more likely to engage in dangerous deviant behavior, such as drug taking, if someone else had set the example first.\nThe Werther effect not only predicts an increase in suicide, but the majority of the suicides will take place in the same or a similar way as the one publicized. The more similar the person in the publicized suicide is to the people exposed to the information about it, the more likely the age group or demographic is to die by suicide. The increase generally happens only in areas where the suicide story was highly publicized. Upon learning of someone else's suicide, some people decide that action may be appropriate for them as well, especially if the publicized suicide was of someone in a situation similar to their own.\nPublishing the means of suicides, romanticized and sensationalized reporting\u2014particularly about celebrities, suggestions that there is an epidemic, glorifying the deceased and simplifying the reasons all lead to increases in the suicide rate. People may see suicide as a glamorous ending, with the deceased getting attention, sympathy, and concern that they never got in life. A second possible factor is that vulnerable youth may feel, \"If they couldn't cut it, neither can I\". An increased rate of suicides has been shown to occur up to ten days after a television report. Studies in Japan and Germany have replicated findings of an imitative effect. Etzersdorfer et al. in an Austrian study showed a strong correlation between the number of papers distributed in various areas and the number of subsequent firearm suicides in each area after a related media report. Higher rates of copycat suicides have been found in those with similarities in race, age, and gender to the deceased in the original report. However, a study from South Korea found that preventative measures restricting how the news reports on suicide do not have much effect, possibly suggesting some other causal factor is at play for changing suicide rates.\nStack analyzed the results from 42 studies and found that those measuring the effect of a celebrity suicide story were 14.3 times more likely to find a copycat effect than studies that did not. Studies based on a real as opposed to a fictional story were 4.03 times more likely to uncover a copycat effect and research based on televised stories was 82% less likely to report a copycat effect than research based on newspapers. Other scholars have been less certain about whether copycat suicides truly happen or are selectively hyped. For instance, fears of a suicide wave following the suicide of Kurt Cobain never materialized in an actual increase in suicides. Coverage of Cobain's suicide in the local Seattle area focused largely on treatment for mental health issues, suicide prevention and the suffering Cobain's death caused to his family. Perhaps as a result, the local suicide rate actually declined in the following months.\nFurthermore, there is evidence for an indirect Werther effect, i.e. the perception that suicidal media content influences others which, in turn, can concurrently or additionally influence one person's own future thoughts and behaviors. Similarly, the researcher Gerard Sullivan has critiqued research on copycat suicides, suggesting that data analyses have been selective and misleading and that the evidence for copycat suicides is much less consistent than suggested by some researchers. Additionally, a review by Cheng and colleagues finds that most research on the relationship of media coverage to suicidality lacks solid theoretical grounding, making interpreting the research very difficult. Sonia Livingstone argues that media effects research as a whole has many flaws, and needs serious evaluation as a field before additional interpretations can be drawn from such research.\nStudies show a high incidence of psychiatric disorders in suicide cases at the time of their death with the total figure ranging from 87.3% to 98%, with mood disorders and substance abuse being the two most common.\nSocial proof model.\nAn alternate model to explain copycat suicide, called \"social proof\" by Robert Cialdini, goes beyond the theories of glorification and simplification of reasons to look at why copycat suicides are so similar, demographically and in actual methods, to the originally publicized suicide. In the social proof model, people imitate those who seem similar, despite or even because of societal disapproval. This model is important because it has nearly opposite ramifications for what the media ought to do about the copycat suicide effect than the standard model does. To deal with this problem, Alex Mesoudi of Queen Mary University of London, developed a computer model of a community of 1000 people, to examine how copycat suicides occur. These were divided into 100 groups of 10, in a model designed to represent different levels of social organization, such as schools or hospitals within a town or state. Mesoudi then circulated the simulation through 100 generations. He found the simulated people acted just as sociologists' theory predicted. They were more likely to die by suicide in clusters, either because they had learned this trait from their friends, or because suicidal people are more likely to be like one another.\nJournalism codes.\nVarious countries have national journalism codes which range from one extreme of, \"Suicide and attempted suicide should in general never be given any mention\" (Norway) to a more moderate, \"In cases of suicide, publishing or broadcasting information in an exaggerated way that goes beyond normal dimensions of reporting with the purpose of influencing readers or spectators should not occur.\" University of London psychologist Alex Mesoudi recommends that reporters follow the sort of guidelines the World Health Organization and others endorse for coverage of any suicide: use extreme restraint in covering these deaths\u2014keep the word \"suicide\" out of the headline, don't romanticize the death, and limit the number of stories. \"Photography, pictures, visual images or film depicting such cases should not be made public\" (Turkey). While many countries do not have national codes, media outlets still often have in-house guidelines along similar lines. In the United States, there are no industry-wide standards. A survey of in-house guides of 16 US daily newspapers showed that only three mentioned the word \"suicide,\" and none gave guidelines about publishing the method of suicide. Craig Branson, online director of the American Society of News Editors (ASNE), has been quoted as saying, \"Industry codes are very generic and totally voluntary. Most ethical decisions are left to individual editors at individual papers. The industry would fight any attempt to create more specific rules or standards, and editors would no doubt ignore them.\" Guidelines on the reporting of suicides in Ireland were introduced with attempt to remove any positive connotations the act might have (e.g. using the term \"completed\" rather than \"successful\" when describing a suicide attempt which resulted in a death).\nCanada's public broadcaster, the Canadian Broadcasting Corporation, abides by standards that \"avoid describing the act in detail or illustrating the method\" of suicides.\nJournalist training.\nAustralia is one of the few countries where there is a concerted effort to teach journalism students about this subject. In the 2000s, the Mindframe national media initiative followed an ambivalent response by the Australian Press Council to an earlier media resource kit issued by Suicide Prevention Australia and the Australian Institute for Suicide Research and Prevention. The UK-based media ethics charity MediaWise provides training for journalists on reporting suicide and related issues.\nHeadline is Ireland's media monitoring programme for suicide and mental health issues, set up by Shine (national mental health organisation) and the Health Service Executives National Office for Suicide Prevention as part of the program Reach Out: National Strategy for action on Suicide Prevention. Headline works with media professionals and students to find ways to collaborate to ensure that suicide, mental health and mental illness are responsibly covered in the media and provides information on reporting on mental health and suicidal behavior, literature and daily analysis of news stories. Headline also serves as a vehicle for the public to become involved in helping to monitor the Irish media on issues relating to mental health and suicide.\nStudies suggest that the risk of suicide fell significantly when media outlets began following recommendations for suicide reporting in the late 20th century.\nPrevention.\nThe Papageno effect is the effect that mass media can have by presenting non-suicide alternatives to crises. It is named after a lovelorn character, Papageno, from the 18th-century opera \"The Magic Flute\"; he was contemplating suicide until other characters showed him a different way to resolve his problems.\nIf a novel or news can induce self-harm, then it must be assumed that those narratives might have a positive effect on prevention. There is more research into the damage done by \"irresponsible media reports\" than into the protective effects of positive stories, but when newspapers refuse to publicize suicide events or change the way that they provide information about suicide events, the risk of copycat suicides declines.\nLate twentieth/early twenty-first century research.\nAn example occurred in Vienna, Austria where the media reported a dramatic increase of copycat suicides. Reduction began when a working group of the Austrian Association for Suicide Prevention developed media guidelines and initiated discussions with the media which culminated with an agreement to abstain from reporting on cases of suicide. Examples of celebrities whose suicides have triggered suicide clusters include Ruan Lingyu, the Japanese musicians Yukiko Okada and hide, the South Korean actress Choi Jin-Sil, whose suicide caused suicide rates to rise by 162.3% and Marilyn Monroe, whose death was followed by an increase of 200 more suicides than average for that August month.\nAnother famous case is the self-immolation of Mohamed Bouazizi, a Tunisian street vendor who set himself on fire on 17 December 2010, an act that was a catalyst for the Tunisian Revolution and sparked the Arab Spring, including several men who emulated Bouazizi's act.\nA 2017 study published in \"JAMA Internal Medicine\" found the online series \"13 Reasons Why\" which chronicled a fictional teen's suicide was associated with an increase in suicide related Internet searches, including a 26% increase in searches for \"how to commit suicide\", an 18% increase for \"commit suicide\" and 9% increase for \"how to kill yourself\". On 29 May 2019, research published in \"JAMA Psychiatry\" outlined an association of increased suicides in 10- to 19-year-olds in the United States in the three months following the release of \"13 Reasons Why\", consistent with a media contagion of suicide in the show. However, some media scholar studies implied that viewing \"13 Reasons Why\" was not associated with suicidal ideation but actually with reduced depressive symptoms.\nThe cause-and-effect relationship between media and suicide is not simple to prove. Sonia Livingstone emphasized the claim of causality in media-effect cannot be considered conclusive because of different methodological approaches and disciplinary perspective. Even if it is accepted that media can have an effect on suicidal ideation, it is not a sufficient condition to drive people to commit suicide; the effects that media can have on suicidal behaviour are certainly less important than individual psychological and social risk factors.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47447", "revid": "16924", "url": "https://en.wikipedia.org/wiki?curid=47447", "title": "Graeco-Roman mythology", "text": ""}
{"id": "47449", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=47449", "title": "Chessboard", "text": "Any board used in the game chess\nA chessboard is a game board used to play chess. It consists of 64 squares, 8 rows by 8 columns, on which the chess pieces are placed. It is square in shape and uses two colors of squares, one light and one dark, in a checkered pattern. During play, the board is oriented such that each player's near-right corner square is a light square.\nThe columns of a chessboard are known as \"&lt;dfn id=\"\"&gt;files&lt;/dfn&gt;\", the rows are known as \"&lt;dfn id=\"\"&gt;ranks&lt;/dfn&gt;\", and the lines of adjoining same-colored squares (each running from one edge of the board to an adjacent edge) are known as \"&lt;dfn id=\"\"&gt;diagonals&lt;/dfn&gt;\". Each square of the board is named using algebraic, descriptive, or numeric chess notation; algebraic notation is the FIDE standard. In algebraic notation, using White's perspective, files are labeled \"a\" through \"h\" from left to right, and ranks are labeled \"1\" through \"8\" from bottom to top; each square is identified by the file and rank that it occupies. The a- through d-files constitute the &lt;dfn id=\"\"&gt;queenside&lt;/dfn&gt;, and the e- through h-files constitute the &lt;dfn id=\"\"&gt;kingside&lt;/dfn&gt;; the 1st through 4th ranks constitute White's side, and the 5th through 8th ranks constitute Black's side.\nHistory and evolution.\nAsht\u0101pada board on which chaturanga was played\nThe earliest known ancestor of the chessboard is the Asht\u0101pada board. Among other games, it was used to play chaturanga, a historical precursor to chess, beginning around the 6th century in India. The board uses a single color for all squares and is divided into eight columns by eight rows, with marked squares called \"castles\" in the corners of each quadrant. Unlike in Asht\u0101pada, castles serve no function in chaturanga.\nThe chessboard acquired its modern checkered pattern in the 10th century with the arrival of chess in Europe. This pattern was based on that of the then-5\u00d75 draughts board. As a result of this change, each diagonal was now highlighted by a continuous sequence of same-colored squares, which later facilitated the introduction of the modern bishop and queen movements in the 15th century.\nThe \"Libro de los juegos\" (1283) contains a description of the chessboard, describing eight rows and columns as the ideal number, deeming the practice of chess on the 10\u00d710 board too tiresome and on the 6\u00d76 board too quick. In the 13th century, some players began using the convention that the first square of the far right column should be light-colored; this convention was endorsed by Pedro Damiano at the end of the 15th century.\nIn contemporary chess, a \"digital board\" is a chess board connected to a computer that is capable of transmitting the moves to the computer itself: the information about the moves can be used to play a game against a chess engine, or simply to record the moves of a game automatically. A digital board uses sensors to detect the position of the pieces, and each piece move can be recorded.\nIn 1998, the 33rd Chess Olympiad was held in Elista. The games were digitally broadcast over the internet thanks to the introduction of digital chess boards developed by Digital Game Technology: 328 boards were used for the event.\nIn 2003, ex-world champion Garry Kasparov faced chess engine X3D Fritz in a series of four matches in a virtual environment, where the computer-generated board hovered in the air in front of Kasparov, who used special glasses. This was the first man\u2013machine game of chess performed in a completely simulated environment.\nManufacture.\nChessboards have been made from numerous materials over the years, such as ebony, ivory, marble, metal, glass, and plastic. They can also be found as decorative elements in plazas, gardens, and living rooms.\nHigh-level games generally use wooden boards, while vinyl, plastic, and cardboard are common for less important tournaments and matches, as well as for home use. Additionally, some very large chessboards are built into or drawn on the ground. Rarely, decorative glass and marble boards are permitted for games conducted by national or international chess federations.\nWooden boards are traditionally made of unstained woods that are light brown and dark brown in color. To reduce cost, some boards are made with veneers of more expensive woods glued to an inner piece of plywood or chipboard. A variety of color combinations are used for plastic, vinyl, and silicone boards. Common dark-light combinations are black and white, as well as brown, green or blue with buff or cream.\nFor international or continental championships, FIDE's regulations state that wooden boards should be used. For other FIDE tournaments, wood, plastic, or cardboard boards may be used, and the board should be rigid in all instances. The board may also be made of marble, as long as there is an appropriate contrast between the light and dark squares. The finishing should be neutral or frosted but never shiny. The squares should be from 5 to 6 cm in length, at least twice the diameter of a pawn's base. If the table and the board are two separate pieces, the latter must be fixed so it stays in place.\nBoard notation.\nThere are various systems for recording moves and referring to the squares of the chessboard; the standard contemporary system is algebraic notation. In algebraic notation, the files are identified by the letters \"a\" to \"h\", from left to right from the White player's point of view, and the ranks by the numbers \"1\" to \"8\", with 1 being closest to the White player. Each square on the board is identified by a unique coordinate pairing, from a1 to h8.\nIn the older descriptive notation, the files are labelled by the piece originally occupying its first rank (e.g. queen, &lt;dfn id=\"\"&gt;king's rook&lt;/dfn&gt;, &lt;dfn id=\"\"&gt;queen's bishop&lt;/dfn&gt;), and ranks by the numbers \"1\" to \"8\" from each player's point of view. This method is no longer commonly used. FIDE stopped using descriptive notation in 1981.\nICCF numeric notation assigns numbers to both files and ranks, with rank 1 being the one closest to the player with the white pieces. The file leftmost to the White player (\"a\" in algebraic notation and \"QR\" in descriptive notation) is file one and the rightmost to them (\"h\" in algebraic notation and \"KR\" in descriptive notation) is file eight.\nVariant boards.\nVariant chessboard shapes and sizes go back to the Persian origins of the game in the 10th century, when the book \"Muraj adh-dhahab\" (Board of the Gods) described six different variants of chess, including circular and cylinder chess. Due to the widespread creation of new variants, a wide variety of sizes can be found. Gli\u0144ski's hexagonal chess utilizes a board with 91 hexagonal spaces of three different colors. One innovation of the 13th century was the cylindrical board for use in cylinder chess.\nThe board used for the Persian Tamerlane chess is one of the first recorded variant chessboards, with eleven columns by ten rows along with two citadels. Each player has a citadel to the right of their second rank, which may be occupied by the opponent's king, in which case that opponent may declare a draw. In 1617, Pietro Carrera proposed a variant that received his name, Carrera's Chess, with a 10\u00d78 board, later used in other variants such as Capablanca chess and Gothic Chess. Other sizes, with ten rows by ten columns, are used in Omega Chess and Grand Chess; Omega Chess has four additional squares, one in each corner of the board. Los Alamos chess uses a smaller 6\u00d76 board.\nJapanese shogi uses a board with nine columns by nine rows. The board of Chinese xianqi consists of nine columns by ten rows; here, the pieces are placed on the intersections of the lines that divide the squares, rather than within the squares themselves. Each player has a 3\u00d73 palace in the central three columns and the closest three rows, within which the player's general and advisors must stay. Between the central two rows is a river that the elephant cannot cross and past which the soldier increases in strength. A similar board without a river is used in Korean janggi.\nSome chess variants use more than a single board per match. Bughouse chess, for example, involves four players playing two simultaneous matches on separate boards. Alice Chess is a popular variant which is usually played on two boards to facilitate the movement of pieces between the boards. Three-dimensional boards are often represented by multiple two-dimensional boards. Variants may use anywhere from two to eight boards. For example, Raumschach utilizes five boards of twenty-five squares each, totaling 125 squares. Another noteworthy variant, \"Star Trek\" Chess, utilizes a board of sixty-four squares with movable parts divided into seven levels. In the initial position, each player occupies two of the movable four-square attack boards. The white pieces start in the lower level, using attack boards connected to this level and the first two rows of the board, while the black pieces start at the top, using the attack boards and first two rows of the third level.\nOther representations.\nThe game of chess has been represented in the arts since its creation. Chess sets usually had considerable artistic value; they were made of noble materials, such as ebony and ivory, and in large sizes. Many of the pieces in these sets were offered to churches as relics. The book \"Liber miraculorum sancte Fidis\" tells a story in which a nobleman, after miraculously escaping from prison, is forced to carry a chessboard until a sanctuary as gesture of gratitude. More frequently, however, there are stories in which the chessboard is used as a weapon. The French tale of Ogier the Dane reports how the son of Charlemagne brutally kills one of Ogier's sons with a chessboard after losing a match, although there is no evidence confirming the veracity of the story.\nIn 1250, a sermon called \"Quaedam moralitas de scaccario per Innocentium papum\" (The Innocent Morality) showed the world as being represented by a chessboard. The white and black squares represented the two conditions of life and death, or praise and censure; over these, the pieces, representing humanity, would confront each other in the adversities of the game, which symbolized life.\nDue to its simple geometry, the chessboard is often used in mathematical puzzles or problems unrelated to chess, such as the wheat and chessboard problem and the mutilated chessboard problem. The term \"infinite chessboard\" is sometimes used to refer to a grid.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography"}
{"id": "47454", "revid": "18426370", "url": "https://en.wikipedia.org/wiki?curid=47454", "title": "Stratosphere", "text": "Layer of the atmosphere above the troposphere\nThe stratosphere is the second-lowest layer of the atmosphere of Earth, located above the troposphere and below the mesosphere. Pronounced , the name originates from from grc \" \"\" ()\"\u00a0'layer, stratum' and \" -sphere\". The stratosphere is composed of stratified temperature zones, with the warmer layers of air located higher (closer to outer space) and the cooler layers lower (closer to the planetary surface of the Earth). The increase of temperature with altitude is a result of the absorption of the Sun's ultraviolet (UV) radiation by the ozone layer, where ozone is exothermically photolyzed into oxygen in a cyclical fashion. This temperature inversion is in contrast to the troposphere, where temperature decreases with altitude, and between the troposphere and stratosphere is the tropopause border that demarcates the beginning of the temperature inversion.\nNear the equator, the lower edge of the stratosphere is as high as , at mid-latitudes around , and at the poles about . Temperatures range from an average of near the tropopause to an average of near the mesosphere. Stratospheric temperatures also vary within the stratosphere as the seasons change, reaching particularly low temperatures in the polar night (winter). Winds in the stratosphere can far exceed those in the troposphere, reaching near in the Southern polar vortex.\nDiscovery.\nIn 1902, L\u00e9on Teisserenc de Bort from France and Richard Assmann from Germany, in separate but coordinated publications and following years of observations, published the discovery of an isothermal layer at around 11\u201314\u00a0 km (6.8-8.7\u00a0 mi), which is the base of the lower stratosphere. This was based on temperature profiles from mostly unmanned and a few manned instrumented balloons.\nOzone layer.\nThe mechanism describing the formation of the ozone layer was described by British mathematician and geophysicist Sydney Chapman in 1930, and is known as the Chapman cycle or ozone\u2013oxygen cycle. Molecular oxygen absorbs high energy sunlight in the UV-C region, at wavelengths shorter than about 240\u00a0nm. Radicals produced from the homolytically split oxygen molecules combine with molecular oxygen to form ozone. Ozone in turn is photolyzed much more rapidly than molecular oxygen as it has a stronger absorption that occurs at longer wavelengths, where the solar emission is more intense. Ozone (O3) photolysis produces O and O2. The oxygen atom product combines with atmospheric molecular oxygen to reform O3, releasing heat. The rapid photolysis and reformation of ozone heat the stratosphere, resulting in a temperature inversion. This increase of temperature with altitude is characteristic of the stratosphere; its resistance to vertical mixing means that it is stratified. Within the stratosphere temperatures increase with altitude \"(see temperature inversion)\"; the top of the stratosphere has a temperature of about 270 K (\u22123\u00b0C or 26.6\u00b0F).\nThis vertical stratification, with warmer layers above and cooler layers below, makes the stratosphere dynamically stable: there is no regular convection and associated turbulence in this part of the atmosphere. However, exceptionally energetic convection processes, such as volcanic eruption columns and overshooting tops in severe supercell thunderstorms, may carry convection into the stratosphere on a very local and temporary basis. Overall, the attenuation of solar UV at wavelengths that damage DNA by the ozone layer allows life to exist on the planet's surface outside of the ocean. All air entering the stratosphere must pass through the tropopause, the temperature minimum that divides the troposphere and stratosphere. The rising air is literally freeze-dried; the stratosphere is a very dry place. The top of the stratosphere is called the stratopause, above which the temperature decreases with height.\nFormation and destruction.\nSydney Chapman gave a correct description of the source of stratospheric ozone and its ability to generate heat within the stratosphere; he also wrote that ozone may be destroyed by reacting with atomic oxygen, making two molecules of molecular oxygen. We now know that there are additional ozone loss mechanisms and that these mechanisms are catalytic, meaning that a small amount of the catalyst can destroy a great number of ozone molecules. The first is due to the reaction of hydroxyl radicals (\u2022OH) with ozone. \u2022OH is formed by the reaction of electrically excited oxygen atoms produced by ozone photolysis, with water vapor. While the stratosphere is dry, additional water vapour is produced in situ by the photochemical oxidation of methane (CH4). The HO2 radical produced by the reaction of OH with O3 is recycled to OH by reaction with oxygen atoms or ozone. In addition, solar proton events can significantly affect ozone levels via radiolysis with the subsequent formation of OH. Nitrous oxide (N2O) is produced by biological activity at the surface and is oxidized to NO in the stratosphere; the so-called NOx radical cycles also deplete stratospheric ozone. Finally, chlorofluorocarbon molecules are photolyzed in the stratosphere releasing chlorine atoms that react with ozone giving ClO and O2. The chlorine atoms are recycled when ClO reacts with O in the upper stratosphere, or when ClO reacts with itself in the chemistry of the Antarctic ozone hole.\nPaul J. Crutzen, Mario J. Molina and F. Sherwood Rowland were awarded the Nobel Prize in Chemistry in 1995 for their work describing the formation and decomposition of stratospheric ozone.\nAircraft flight.\nCommercial airliners typically cruise at altitudes of which is in the lower reaches of the stratosphere in temperate latitudes. This optimizes fuel efficiency, mostly due to the low temperatures encountered near the tropopause and low air density, reducing parasitic drag on the airframe. Stated another way, it allows the airliner to fly faster while maintaining lift equal to the weight of the plane. (The fuel consumption depends on the drag, which is related to the lift by the lift-to-drag ratio.) It also allows the airplane to stay above the turbulent weather of the troposphere.\nThe Concorde aircraft cruised at Mach 2 at about , and the SR-71 cruised at Mach 3 at , all within the stratosphere.\nBecause the temperature in the tropopause and lower stratosphere is largely constant with increasing altitude, very little convection and its resultant turbulence occurs there. Most turbulence at this altitude is caused by variations in the jet stream and other local wind shears, although areas of significant convective activity (thunderstorms) in the troposphere below may produce turbulence as a result of convective overshoot.\nOn October 24, 2014, Alan Eustace became the record holder for reaching the altitude record for a manned balloon at . Eustace also broke the world records for vertical speed skydiving, reached with a peak velocity of 1,321\u00a0 km/h (822\u00a0 mph) and total freefall distance of \u2013 lasting four minutes and 27 seconds.\nCirculation and mixing.\nThe stratosphere is a region of intense interactions among radiative, dynamical, and chemical processes, in which the horizontal mixing of gaseous components proceeds much more rapidly than does vertical mixing. The overall circulation of the stratosphere is termed as Brewer-Dobson circulation, which is a single-celled circulation, spanning from the tropics up to the poles, consisting of the tropical upwelling of air from the tropical troposphere and the extra-tropical downwelling of air. Stratospheric circulation is a predominantly wave-driven circulation in that the tropical upwelling is induced by the wave force by the westward propagating Rossby waves, in a phenomenon called Rossby-wave pumping.\nAn interesting feature of stratospheric circulation is the quasi-biennial oscillation (QBO) in the tropical latitudes, which is driven by gravity waves that are convectively generated in the troposphere. The QBO induces a secondary circulation that is important for the global stratospheric transport of tracers, such as ozone or water vapor.\nAnother large-scale feature that significantly influences stratospheric circulation is the breaking planetary waves resulting in intense quasi-horizontal mixing in the midlatitudes. This breaking is much more pronounced in the winter hemisphere where this region is called the surf zone. This breaking is caused due to a highly non-linear interaction between the vertically propagating planetary waves and the isolated high potential vorticity region known as the polar vortex. The resultant breaking causes large-scale mixing of air and other trace gases throughout the midlatitude surf zone. The timescale of this rapid mixing is much smaller than the much slower timescales of upwelling in the tropics and downwelling in the extratropics.\nDuring northern hemispheric winters, sudden stratospheric warmings, caused by the absorption of Rossby waves in the stratosphere, can be observed in approximately half of the winters when easterly winds develop in the stratosphere. These events often precede unusual winter weather and may even be responsible for the cold European winters of the 1960s.\nStratospheric warming of the polar vortex results in its weakening. When the vortex is strong, it keeps the cold, high-pressure air masses \"contained\" in the Arctic; when the vortex weakens, air masses move equatorward, and results in rapid changes of weather in the mid latitudes.\nUpper-atmospheric lightning.\nUpper-atmospheric lightning is a family of short-lived electrical breakdown phenomena that occur well above the altitudes of normal lightning and storm clouds. Upper-atmospheric lightning is believed to be electrically induced forms of luminous plasma. Lightning extending above the troposphere into the stratosphere is referred to as blue jet, and that reaching into the mesosphere as red sprite.\nLife.\nBacterial life survives in the stratosphere, making it a part of the biosphere. In 2001, dust was collected at a height of 41 kilometres in a high-altitude balloon experiment and was found to contain bacterial material when examined later in the laboratory.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47456", "revid": "7098284", "url": "https://en.wikipedia.org/wiki?curid=47456", "title": "Pater Noster (disambiguation)", "text": "Pater Noster, or the Lord's Prayer, is a prayer in Christianity.\nPater Noster or Paternoster may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47457", "revid": "3337", "url": "https://en.wikipedia.org/wiki?curid=47457", "title": "Ultraviolet energy", "text": ""}
{"id": "47458", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=47458", "title": "Troposhere", "text": ""}
{"id": "47459", "revid": "8356162", "url": "https://en.wikipedia.org/wiki?curid=47459", "title": "James T. Kirk", "text": "Character in the Star Trek media franchise\nJames Tiberius Kirk, often known as Captain Kirk, is a fictional character in the \"Star Trek\" media franchise. Originally played by Canadian actor William Shatner, Kirk is best known as the captain of the starship USS \"Enterprise\" in the (1966\u20131969). Kirk leads his crew as they explore new worlds and \"boldly go where no man has gone before\". Often, the characters of Spock and Leonard \"Bones\" McCoy act as his logical and emotional sounding boards, respectively. \nKirk first appears in the \"Star Trek\" episode \"The Man Trap\", broadcast on September 8, 1966, although the first episode recorded featuring Shatner was \"Where No Man Has Gone Before\". Shatner continued in the role for the show's three seasons, and he later provided the voice of the animated version of Kirk in ' (1973\u20131974). Shatner returned to the role for ' (1979) and six subsequent films. Kirk has also been portrayed in numerous films, books, comics, webisodes, and video games. The character has also been the subject of multiple spoofs and satires.\nAmerican actor Chris Pine portrays a young version of the character in the 2009 \"Star Trek\" film and its two sequels. Paul Wesley portrays Kirk on the Paramount+ series \"\" (2022\u2013present), set prior to Kirk's captaincy of the Enterprise.\nDepiction.\nJames Tiberius Kirk was born in Riverside, Iowa, on March 22, 2233, where he was raised by his parents, George and Winona Kirk. Although born on Earth, Kirk lived for a time on , where he was one of nine surviving witnesses to the massacre of 4,000 colonists by . James Kirk's brother, George Samuel Kirk, is first mentioned in \"What Are Little Girls Made Of?\" and introduced and killed in \"Operation \u2013 Annihilate!\", leaving behind three children.\nKirk became the first and only student at Starfleet Academy to defeat the \"Kobayashi Maru\" test, garnering a commendation for original thinking after he reprogrammed the computer to make the \"no-win scenario\" winnable. Kirk was granted a field commission as an ensign and posted to advanced training aboard the USS \"Republic\". He was then promoted to lieutenant junior grade and returned to Starfleet Academy as a student instructor. According to a friend, students could either \"think or sink\" in his class, and Kirk himself was \"a stack of books with legs\". Upon graduating in the top five percent, Kirk was promoted to lieutenant and served aboard the USS \"Farragut\". While assigned to the \"Farragut\", Kirk commanded his first planetary survey and survived a deadly attack by a bizarre cloud-like creature that killed a large portion of the \"Farragut\"'s crew, including his commanding officer, Captain Garrovick. Kirk blamed himself for years for hesitating to fire his assigned weapons upon seeing the threat until a later encounter with the creature showed that firing immediately with conventional weapons would have been useless.\nKirk became Starfleet's youngest starship captain after receiving command of the for a five-year mission, three years of which are depicted in the original \"Star Trek\" series (1966\u20131969). Kirk's most significant relationships in the television series are with first officer Spock and chief medical officer Dr. Leonard \"Bones\" McCoy. McCoy is someone to whom Kirk unburdens himself and is a foil to Spock. Robert Jewett and John Shelton Lawrence's \"The Myth of the American Superhero\" describes Kirk as \"a hard-driving leader who pushes himself and his crew beyond human limits\". Terry J. Erdman and Paula M. Block, in their \"Star Trek 101\" primer, note that while \"cunning, courageous and confident\", Kirk also has a \"tendency to ignore Starfleet regulations when he feels the end justifies the means\"; he is \"the quintessential officer, a man among men and a hero for the ages\". Although Kirk throughout the series becomes romantically involved with various women, when confronted with a choice between a woman and the \"Enterprise\", \"his ship always won\".\nRoddenberry wrote in a production memo that Kirk is not afraid of being fallible, but rather is afraid of the consequences to his ship and crew should he make an error in judgment. Roddenberry wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[Kirk] has any normal man's insecurities and doubts, but he knows he cannot ever show them\u2014except occasionally in private with ship's surgeon McCoy or in subsequent moments with Mr. Spock whose opinions Kirk has learned to value so highly.\nIn ' (1979), Admiral Kirk is Chief of Starfleet Operations, and he takes command of the \"Enterprise\" from Captain Willard Decker. \"Star Trek\" creator Gene Roddenberry's novelization of \"The Motion Picture\" depicts Kirk married to a Starfleet officer killed during a transporter accident. At the beginning of ' (1982), Kirk takes command of the \"Enterprise\" from Captain Spock to pursue his enemy from \"Space Seed\", Khan Noonien Singh. The movie introduces Kirk's former lover Carol and his son, David Marcus. Spock, who notes that \"commanding a starship is [Kirk's] first, best destiny\", dies at the end of \"Star Trek II\". In ' (1984), Admiral Kirk leads his surviving officers in a successful mission to rescue Spock from a planet on which he is reborn. Although Kirk is demoted to Captain in ' (1986) for disobeying Starfleet orders, he also receives command of the new starship named \"Enterprise\".\nIn \"Star Trek Generations\" (1994), Kirk is lost and presumed killed when a new USS \"Enterprise\" is damaged by an energy ribbon. Instead, the ribbon is an entry to the timeless Nexus, where Captain Jean-Luc Picard finds Kirk alive. Picard persuades Kirk to return to Picard's present to help stop the villain Soran from destroying Veridian III's sun. Although Kirk initially refuses, he agrees after realizing the Nexus is not a place where he can make a difference. The two leave the Nexus and stop Soran. However, Kirk is mortally wounded; as he dies, Picard assures him that he helped to \"make a difference\". Picard buries Kirk on the planet. In (2023), Kirk\u2019s body is revealed to be stored in stasis at the Daystrom Institute by Section 31.\nKirk appears in several episode of \"\" (2022-present) as an officer assigned to the \"Farragut.\" Throughout his appearances, he is introduced to officers who will serve with him when he becomes captain of the \"Enterprise\": Nyota Uhura, Montgomery Scott, and Spock. The series also depicts his relationship with his brother Sam and other members of the \"Enterprise\" crew under the command of Christopher Pike.\nKelvin Timeline.\nThe alternate \"Kelvin Timeline\" reveal different origins for Kirk, the formation of his association with Spock, and how they came to serve together on the \"Enterprise\". In this timeline, Kirk is born on a shuttle escaping the starship USS \"Kelvin\" as it is attacked by a Romulan ship from the future. His father is killed in the attack. George and Winona Kirk name their son \"James Tiberius\" after his maternal and paternal grandfathers, respectively.\nAlthough the film treats specific details from Star Trek as mutable, characterizations are meant to \"remain the same.\" Kirk is initially portrayed as \"a reckless, bar-fighting rebel\" who eventually matures. According to Pine, the character is \"a 25-year-old [who acts like a] 15-year-old\" and who is \"angry at the world\" until he enrolls in Starfleet Academy after being challenged to by Captain Christopher Pike. Kirk and Spock clash at Starfleet Academy, but, over the course of the first film, Kirk focuses his \"passion and obstinance and the spectrum of emotions\" and becomes captain of the \"Enterprise\". He is also aided by a meeting with the time-displaced Spock of the original timeline, who inspires Kirk to live up to his full potential after learning about the parallel version of himself and his accomplishments as Captain in the elder Spock's timeline.\nDevelopment.\nConception and television.\nJeffrey Hunter played the commanding officer of the USS \"Enterprise\", Captain Christopher Pike, in the rejected \"Star Trek\" television pilot \"\". In developing a new pilot episode, called \"Where No Man Has Gone Before\", series creator Gene Roddenberry changed the captain's name to \"James Kirk\" after rejecting other options like Hannibal, Timber, Flagg and Raintree. The episode title may have been inspired by Captain James Cook, whose journal entry \"ambition leads me ... farther than any other man has been before me\" inspired the episode title, and became the series catch-phrase in the opening voice-over. The character is in part based on C. S. Forester's Horatio Hornblower hero, and NBC wanted the show to emphasize the captain's \"rugged individualism\". Roddenberry had previously used the middle name of Tiberius for the leading character in his earlier television series, \"The Lieutenant\", which was to feature several actors who would later go on to be part of the production of \"Star Trek\".\nJack Lord was Desilu Productions' original choice to play Kirk, but his demand for fifty-percent ownership of the show led to him not being hired. The second pilot episode was successful, and \"Where No Man Has Gone Before\" was broadcast as the third episode of \"Star Trek\" on September 22, 1966.\nWilliam Shatner tried to imbue the character with qualities of \"awe and wonder\" absent from \"The Cage\". He also drew upon his experiences as a Shakespearean actor to invigorate the character, whose dialogue at times is laden with jargon. Not only did Shatner take inspiration from Roddenberry's suggestion of Hornblower, but also from Alexander the Great\u00a0\u2013 \"the athlete and the intellectual of his time\"\u00a0\u2013 whom Shatner had played for an unsold television pilot two years earlier. In addition, the actor based Kirk partly on himself because \"the fatigue factor [after weeks of daily filming] is such that you try to be as honest about yourself as possible\". A comedy veteran, Shatner suggested making the show's characters as comfortable working in space as they would be at sea, thus having Kirk be a humorous \"good-pal-the-captain, who in time of need would snap to and become the warrior\". Changing the character to be \"a man with very human emotions\" also allowed for the development of the Spock character. Shatner wrote: \"Kirk was a man who marveled and greatly appreciated the endless surprises presented to him by the universe ... He didn't take things for granted and, more than anything else, respected life in every one of its weird weekly adventure forms.\"\nFilms.\nShatner did not expect \"Star Trek\" to be successful, so when it was cancelled in 1969, he assumed it would be the end of his association with the franchise. Following \"Star Trek's\" popularity after its cancelation, Shatner went on to voice Kirk in the animated \"Star Trek\" series, star in the first seven \"Star Trek\" films, and provide voice acting for several games. \"\" director and writer Nicholas Meyer, who had never seen an episode of \"Star Trek\" before he was assigned to direct, conceived a \"\"Hornblower\" in outer space\" atmosphere, unaware that those books had been an influence on the show. Meyer also emphasized parallels to Sherlock Holmes, in that both characters waste away in the absence of stimuli: new cases for Holmes; starship adventures for Kirk.\nMeyer's \"The Wrath of Khan\" script focuses on Kirk's age, with McCoy giving him a pair of glasses as a birthday present. The script states that Kirk is 49, but Shatner was unsure about being specific about Kirk's age because he was hesitant to portray a middle-aged version of himself. Shatner changed his mind when producer Harve Bennett convinced Shatner that he could age gracefully like Spencer Tracy. Spock's sacrifice at the end of the film allows for Kirk's spiritual rebirth; after commenting earlier that he feels old and worn out, Kirk states in the final scene that he feels \"young.\" Additionally, Spock's self-sacrificing solution to the no-win \"Kobayashi Maru\" scenario, which Kirk had cheated his way through, forces Kirk to confront death and to grow as a character.\nBoth Shatner and test audiences were dissatisfied that Kirk was fatally shot in the back in the original ending of the film \"Star Trek Generations\". An addendum inserted while Shatner's \"Star Trek Movie Memories\" memoir was being printed expresses his enthusiasm at being called back to film a rewritten ending. Despite the rewrite, \"Generations\" co-writer Ronald D. Moore said that Kirk's death, which was intended to \"resonate throughout the Star Trek franchise\", failed to \"pay off the themes [of death and mortality] in the way we wanted\". Malcolm McDowell, whose character kills Kirk, was dissatisfied with both versions of Kirk's death: he believed Kirk should have been killed \"in a big way\". McDowell claims to have received death threats after \"Generations\" was released.\nFranchise \"reboot\".\nIn \"Star Trek\" (2009), screenwriters Alex Kurtzman and Roberto Orci focused their story on Kirk and Spock in the movie's alternative timeline while attempting to preserve key character traits from the previous depictions. Kurtzman said casting someone whose portrayal of Kirk would show that the character \"is being honored and protected\" was \"tricky\", but that the \"spirit of Kirk is very much alive and well\" in Pine's depiction. Due to his belief that he could not take himself seriously as a leader, Pine recalled having difficulty with his audition, which required him \"to bark '\"Trek\" jargon'\", but his charisma impressed director J. J. Abrams. Pine's chemistry with Zachary Quinto, playing Spock, led Abrams to offer Pine the role. Jimmy Bennett played Kirk in scenes depicting the character's childhood. The writers turned to material from the novel \"Best Destiny\" for inspiration as to Kirk's childhood.\nIn preparing to play Kirk, Pine decided to embrace the character's key traits\u00a0\u2013 \"charming, funny, leader of men\"\u00a0\u2013 rather than try to fit the \"predigested image\" of Shatner's portrayal. Pine specifically did not try to mirror Shatner's cadence, believing that doing so would become \"an impersonation\". Pine said he wanted his portrayal of Kirk to most resemble Harrison Ford's Indiana Jones or Han Solo characters, highlighting their humor and \"accidental hero\" traits.\nA misunderstanding arose during the film's production about the possibility of Shatner making a cameo appearance. According to Abrams, the production team considered ways to resurrect Shatner's deceased Kirk character, but could not devise a way that was not \"lame\". However, Abrams believed Shatner misinterpreted language about trying to get \"him\" into the movie as a reference to Shatner, and not his character. Shatner released a YouTube video expressing disappointment at not being approached for a cameo. Although Shatner questioned the wisdom of not including him in the film, he predicted the movie would be \"wonderful\" and that he was \"kidding\" about Abrams not offering him a cameo.\n\"Star Trek: Strange New Worlds\".\nKirk also appears in \"\", which is set six years before the events of . He first appears in the final episode of Season 1, portrayed by Paul Wesley, and appeared as a recurring guest in Season 2. In this series, the \"Enterprise\" has another captain, Kirk's predecessor Christopher Pike, who first appeared in \"\".\nReception.\nAccording to Shatner, early \"Star Trek\" reviewers described his performance as \"wooden\", with most of the show's acting praise and media interest going to Nimoy. However, Shatner's mannerisms when portraying Kirk have become \"instantly recognizable\" and Shatner won a Saturn Award for Best Actor in 1982 for \"The Wrath of Khan\". \"Star Trek II\" director Nicholas Meyer said Shatner \"gives the best performance of his life\" in \"The Wrath of Khan\". \"The Guardian\" called Pine's performance of Kirk an \"unqualified success\", and \"The Boston Globe\" said Pine is \"a fine, brash boy Kirk\". \"Slate\", which called Pine \"a jewel\", described his performance as \"channel[ing]\" Shatner without being an impersonation.\n\"Slate\" described Shatner's depiction of Kirk as an \"expansive, randy, faintly ridiculous, and yet supremely capable leader of men, Falstaffian in his love of life and largeness of spirit\". \"The Myth of the American Superhero\" refers to Kirk as a \"superhuman redeemer\" who \"like a true superhero ... regularly escapes after risking battle with monsters or enemy spaceships\". Although some episodes question Kirk's position as a hero, \"Star Trek\" \"never left the viewer in doubt for long\". Others have commented that Kirk's exaggerated \"strength, intelligence, charm, and adventurousness\" make him unrealistic. Kirk is described as able to find ways \"through unanticipated problems to reach [his] goals\" and his leadership style is most \"appropriate in a tight, geographically identical team with a culture of strong leadership.\" Although Roddenberry conceived the character as being \"in a very real sense ... 'married'\u00a0\" to the \"Enterprise\", Kirk has been noted for \"his sexual exploits with gorgeous females of every size, shape and type\"; he has been called \"promiscuous\" and labeled a \"womanizer\". \"The Last Lecture\" author Randy Pausch believed he became a better teacher, colleague, and husband because he watched Kirk run the \"Enterprise\"; Pausch wrote that \"for ambitious boys with a scientific bent, there could be no greater role model than James T. Kirk\". David A. Goodman commented that Kirk \"has as much reality as possible for a fictional character.\"\nIn 2012, \"IGN\" ranked the character Captain Kirk, as depicted in the original series, films, and the new Kirk in 2009 film \"Star Trek\", as the number one top character of the \"Star Trek\" universe. In 2016, Kirk was ranked as the #1 most important character of Starfleet within the \"Star Trek\" science fiction universe by \"Wired\" magazine, out of 100 characters of the franchise.\nIn 2018, \"CBR\" ranked Kirk the best Starfleet character of \"Star Trek,\" including later television series.\nIn July 2019, \"Screen Rant\" ranked Kirk the 8th smartest character of \"Star Trek\".\nCultural impact.\nIn 1985, Riverside, Iowa petitioned Roddenberry and Paramount Pictures for permission to \"adopt\" Kirk as their town's \"Future Son\". Shatner and Roddenberry approved the proposal. Paramount wanted $40,000 for a license to reproduce a bust of Kirk, but the city instead set a plaque and built a replica of the \"Enterprise\" (named the \"USS \"Riverside\"\"), and the Riverside Area Community Club holds an annual \"Trek Fest\" in anticipation of Kirk's birthday.\nKirk has been the subject of a wide range of television spoofs that aired in many countries, including \"The Carol Burnett Show\" and KI.KA's \"Bernd das Brot\". John Belushi's impression of Kirk for \"Saturday Night Live\", which he described as his favorite role, was \"dead-on\". Jim Carrey has been praised for his satire of the character in a 1992 episode of \"In Living Color\". Comedian Kevin Pollak is well known for his impressions of Shatner as Kirk.\nKirk's memorable scream of \"Khan!\" in the 1982 movie \"\" has become a pop culture icon in its own right, spawning internet memes and is widely parodied and paid tribute to.\nKirk has been referenced in the lyrics of many pop songs. Early examples include the 1979 song \"Where's Captain Kirk?\" by Spizzenergi, the 1982 rap song \"Tough\" by Kurtis Blow, and 1983's \"99 Luftballons\" by Nena (both German and English versions). More recently, in the 2003 remix of 1998\u2019s \"That Don't Impress Me Much\", Shania Twain puts forth Captain Kirk as one of the unattainable ideals to whom her unappealingly haughty suitor apparently thinks himself equal.\nKirk has been merchandised in a variety of ways, including collectible busts, action figures, mugs, t-shirts, and Christmas tree ornaments. A Kirk Halloween mask was altered and used as the mask worn by the character Michael Myers in the \"Halloween\" film franchise. In 2002, Kirk's captain's chair from the original \"Star Trek\" was auctioned for $304,000.\nIn a 2010 Space Foundation survey, Kirk tied with cosmonaut Yuri Gagarin as the No. 6 most popular space hero.\nCaptain Kirk has also been portrayed in feline form. First, anthropomorphically, in two episodes of the 1975 Filmation Saturday morning animated children\u2019s series \"The Secret Lives of Waldo Kitty\". The cartoon is based around the title character\u2019s fantasies about being various heroic felines based on popular culture icons. Later came scientific illustrator Jenny Parks' 2017 book \"Star Trek Cats\", in which Kirk is depicted as an orange tabby cat.\nThe Kirk crater on Pluto's moon, Charon, is named after the character.\nFan productions.\nIn addition to television, feature films, books, and parodies, Kirk has also been portrayed in non-canon fan fiction.\n\"Star Trek: New Voyages\".\nThe \"\" fan production, known from 2008 until 2015 as \"Star Trek: Phase II\", portrays the further voyages of the original \"Enterprise\" crew. The series' creators feel that \"Kirk, Spock, McCoy and the rest should be treated as 'classic' characters like Willy Loman from \"Death of a Salesman\", Gandalf from \"The Lord of the Rings\" or even Hamlet, Othello or Romeo. Many actors have and can play the roles, each offering a different interpretation of said character.\"\nJames Cawley played Kirk in most of the ten episode \"Phase II\" series from its beginning in 2004 before replacing himself with actor Brian Gross. \"Wired\" observes that while Cawley's depiction \"lacks Shatner's vulnerability\", the actor has enough swagger \"to be passable in the role\". Cawley's portrayal was well-known enough at Paramount that a group of \"\" writers called for Cawley's attention at a science fiction convention by shouting \"Hey, Kirk!\" at him while Shatner sat nearby.\n\"Star Trek Continues\".\nFirst produced in 2013, the 11 episode series \"Star Trek Continues\" also looked to chronicle the \"lost seasons\" of \"\". The series developer and producer is anime voice actor Vic Mignogna, who also plays the role of Kirk. Rounding out the core cast is fellow voice actor Todd Haberkorn as Spock, Chris Doohan (son of the original Scotty actor James Doohan) as Scotty, and as McCoy first author-producer Larry Nemecek, followed by voice actor Chuck Huber. It also co-starred Grant Imahara (\"MythBusters\") as Sulu.\nThe first episode, \"Pilgrim of Eternity\" (with Michael Forest reprising his role as Apollo from the original series episode \"Who Mourns for Adonais?\") was released in 2013. The second episode, \"Lolani\" (featuring guest star Lou Ferrigno), was released in February 2014, and a third episode, \"Fairest of Them All\" was released in June 2014 and won a Burbank International Film Festival award for \"Best New Media \u2013 Drama\". \"Star Trek Continues\" also won a Geekie Award for \"Best Web Series\". On June 19, 2015, Episode 4 of the series was posted and titled \"White Iris\". All eleven full episodes have been released as of December, 2017.\nLegacy.\nIn October 2021, Kirk's actor from \"\" William Shatner flew to space aboard a Blue Origin sub-orbital capsule. At age 90, he became the oldest person to fly to space and one of the first 600 to do so.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47460", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=47460", "title": "Mesosphere", "text": "Layer of the atmosphere directly above the stratosphere and below the thermosphere\nThe mesosphere (; from grc \" \"\" ()\"\u00a0'middle' and \" -sphere\") is the third layer of the atmosphere, directly above the stratosphere and directly below the thermosphere. In the mesosphere, temperature decreases as altitude increases. This characteristic is used to define limits: it begins at the top of the stratosphere (sometimes called the stratopause), and ends at the mesopause, which is the coldest part of Earth's atmosphere, with temperatures below . The exact upper and lower boundaries of the mesosphere vary with latitude and with season (higher in winter and at the tropics, lower in summer and at the poles), but the lower boundary is usually located at altitudes from above sea level, and the upper boundary (the mesopause) is usually from .\nThe stratosphere and mesosphere are sometimes collectively referred to as the \"middle atmosphere\", which spans altitudes approximately between above Earth's surface. The mesopause, at an altitude of , separates the mesosphere from the thermosphere\u2014the second-outermost layer of Earth's atmosphere. On Earth, the mesopause nearly co-incides with the turbopause, below which different chemical species are well-mixed due to turbulent eddies. Above this level the atmosphere becomes non-uniform because the scale heights of different chemical species differ according to their molecular masses.\nThe term \"near space\" is also sometimes used to refer to altitudes within the mesosphere. This term does not have a technical definition, but typically refers to the region roughly between the Armstrong limit (about 62,000 ft or 19 km, above which humans require a pressure suit in order to survive) and the K\u00e1rm\u00e1n line (where astrodynamics must take over from aerodynamics in order to achieve flight); or, by another definition, to the space between the highest altitude commercial airliners fly at (about 40,000 ft (12.2 km)) and the lowest perigee of satellites being able to orbit the Earth (about 45 mi (73 km)). Some sources distinguish between the terms \"near space\" and \"upper atmosphere\", so that only the layers closest to the K\u00e1rm\u00e1n line are described as \"near space\".\nTemperature.\nWithin the mesosphere, temperature decreases with increasing height. This is a result of decreasing absorption of solar radiation by the rarefied atmosphere having a diminishing relative ozone concentration as altitude increases (ozone being the main absorber in the UV wavelengths that survived absorption by the thermosphere). Additionally, this is also a result of increasing cooling by CO2 radiative emission. The top of the mesosphere, called the mesopause, is the coldest part of Earth's atmosphere. Temperatures in the upper mesosphere fall as low as about , varying according to latitude and season.\nDynamic features.\nThe main most important features in this region are strong zonal (East-West) winds, atmospheric tides, internal atmospheric gravity waves (commonly called \"gravity waves\"), and planetary waves. Most of these tides and waves start in the troposphere and lower stratosphere, and propagate to the mesosphere. In the mesosphere, gravity-wave amplitudes can become so large that the waves become unstable and dissipate. This dissipation deposits momentum into the mesosphere and largely drives global circulation.\nNoctilucent clouds are located in the mesosphere. The upper mesosphere is also the region of the ionosphere known as the \"D layer\", which is only present during the day when some ionization occurs with nitric oxide being ionized by Lyman series-alpha hydrogen radiation. The ionization is so weak that when night falls, and the source of ionization is removed, the free electron and ion form back into a neutral molecule.\nA deep sodium layer is located between . Made of unbound, non-ionized atoms of sodium, the sodium layer radiates weakly to contribute to the airglow. The sodium has an average concentration of 400,000 atoms per cubic centimetre. This band is regularly replenished by sodium sublimating from incoming meteors. Astronomers have begun utilizing this sodium band to create \"guide stars\" as part of the adaptive optical correction process used to produce ultra-sharp ground-based observations. Other metal layers, e.g. iron and potassium, exist in the upper mesosphere/lower thermosphere region as well.\nBeginning in October 2018, a distinct type of aurora has been identified, originating in the mesosphere. Often referred to as 'dunes' due to their resemblance to sandy ripples on a beach, the green undulating lights extend toward the equator. They have been identified as originating about above the surface. Since auroras are caused by ultra-high-speed solar particles interacting with atmospheric molecules, the green color of these dunes has tentatively been explained by the interaction of those solar particles with oxygen molecules. The dunes therefore occur where mesospheric oxygen is more concentrated.\nMillions of meteors enter the Earth's atmosphere, averaging 40,000 tons per year. The ablated material, called meteoric smoke, is thought to serve as condensation nuclei for noctilucent clouds.\nExploration.\nThe mesosphere lies above altitude records for aircraft, while only the lowest few kilometers are accessible to balloons, for which the altitude record is . Meanwhile, the mesosphere is below the minimum altitude for orbital spacecraft due to high atmospheric drag. It has only been accessed through the use of sounding rockets, which are only capable of taking mesospheric measurements for a few minutes per mission. As a result, it is the least-understood part of the atmosphere, resulting in the humorous moniker \"ignorosphere\". The presence of red sprites and blue jets (electrical discharges or lightning within the lower mesosphere), noctilucent clouds, and density shears within this poorly understood layer are of current scientific interest.\nOn February 1, 2003, Space Shuttle \"Columbia\" broke up on reentry at about altitude, in the lower mesosphere, killing all seven crew members.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47463", "revid": "43520964", "url": "https://en.wikipedia.org/wiki?curid=47463", "title": "Thermosphere", "text": "Layer of the Earth's atmosphere above the mesosphere and below the exosphere\nThe thermosphere is the layer in the Earth's atmosphere directly above the mesosphere and below the exosphere. Within this layer of the atmosphere, ultraviolet radiation causes photoionization/photodissociation of molecules, creating ions; the bulk of the ionosphere thus exists within the thermosphere. Taking its name from the Greek \u03b8\u03b5\u03c1\u03bc\u03cc\u03c2 (pronounced \"thermos\") meaning heat, the thermosphere begins at about 80\u00a0km (50\u00a0mi) above sea level. At these high altitudes, the residual atmospheric gases sort into strata according to molecular mass (see turbosphere). Thermospheric temperatures increase with altitude due to absorption of highly energetic solar radiation. Temperatures are highly dependent on solar activity, and can rise to or more. Radiation causes the atmospheric particles in this layer to become electrically charged, enabling radio waves to be refracted and thus be received beyond the horizon. In the exosphere, beginning at about 600\u00a0km (375\u00a0mi) above sea level, the atmosphere turns into outer space, although, by the judging criteria set for the definition of the K\u00e1rm\u00e1n line (100\u00a0km), most of the thermosphere is part of outer space. The border between the thermosphere and exosphere is known as the thermopause.\nThe highly attenuated gas in this layer can reach . Despite the high temperature, an observer or object will experience low temperatures in the thermosphere, because the extremely low density of the gas (practically a hard vacuum) is insufficient for the molecules to conduct heat. A normal thermometer will read significantly below , at least at night, because the energy lost by thermal radiation would exceed the energy acquired from the atmospheric gas by direct contact. In the anacoustic zone above , the density is so low that molecular interactions are too infrequent to permit the transmission of sound. \nThe dynamics of the thermosphere are dominated by atmospheric tides, which are driven predominantly by diurnal heating. Atmospheric waves dissipate above this level because of collisions between the neutral gas and the ionospheric plasma.\nThe thermosphere is uninhabited with the exception of the International Space Station, which orbits the Earth within the middle of the thermosphere between and the Tiangong space station, which orbits between .\nNeutral gas constituents.\nIt is convenient to separate the atmospheric regions according to the two temperature minima at an altitude of about (the tropopause) and at about (the mesopause) (Figure 1). The thermosphere (or the upper atmosphere) is the height region above , while the region between the tropopause and the mesopause is the middle atmosphere (stratosphere and mesosphere) where absorption of solar UV radiation generates the temperature maximum near an altitude of and causes the ozone layer.\nThe density of the Earth's atmosphere decreases nearly exponentially with altitude. The total mass of the atmosphere is M = \u03c1A H\u00a0 \u2243 1\u00a0kg/cm2 within a column of one square centimeter above the ground (with \u03c1A = 1.29\u00a0kg/m3 the atmospheric density on the ground at z = 0 m altitude, and H \u2243 8\u00a0km the average atmospheric scale height). Eighty percent of that mass is concentrated within the troposphere. The mass of the thermosphere above about is only 0.002% of the total mass. Therefore, no significant energetic feedback from the thermosphere to the lower atmospheric regions can be expected.\nTurbulence causes the air within the lower atmospheric regions below the turbopause at about to be a mixture of gases that does not change its composition. Its mean molecular weight is 29\u00a0g/mol with molecular oxygen (O2) and nitrogen (N2) as the two dominant constituents. Above the turbopause, however, diffusive separation of the various constituents is significant, so that each constituent follows its barometric height structure with a scale height inversely proportional to its molecular weight. The lighter constituents atomic oxygen (O), helium (He), and hydrogen (H) successively dominate above an altitude of about and vary with geographic location, time, and solar activity. The ratio\nN2/O which is a measure of the electron density at the ionospheric F region is highly affected by these variations. These changes follow from the diffusion of the minor constituents through the major gas component during dynamic processes.\nThe thermosphere contains an appreciable concentration of elemental sodium located in a thick band that occurs at the edge of the mesosphere, above Earth's surface. The sodium has an average concentration of 400,000 atoms per cubic centimeter. This band is regularly replenished by sodium sublimating from incoming meteors. Astronomers have begun using this sodium band to create \"guide stars\" as part of the optical correction process in producing ultra-sharp ground-based observations.\nEnergy input.\nEnergy budget.\nThe thermospheric temperature can be determined from density observations as well as from direct satellite measurements. The temperature vs. altitude z in Fig. 1 can be simulated by the so-called Bates profile:\n(1) \u00a0 formula_1\nwith T\u221e the exospheric temperature above about 400\u00a0km altitude, \nTo = 355\u00a0K, and zo = 120\u00a0km reference temperature and height, and s an empirical parameter depending on T\u221e and decreasing with T\u221e. That formula is derived from a simple equation of heat conduction. One estimates a total heat input of qo\u2243 0.8 to 1.6\u00a0mW/m2 above zo = 120\u00a0km altitude. In order to obtain equilibrium conditions, that heat input qo above zo is lost to the lower atmospheric regions by heat conduction.\nThe exospheric temperature T\u221e is a fair measurement of the solar XUV radiation. Since solar radio emission F at 10.7\u00a0 cm wavelength is a good indicator of solar activity, one can apply the empirical formula for quiet magnetospheric conditions.\n(2) \u00a0 formula_2\nwith T\u221e in K, Fo in 10\u22122 W m\u22122 Hz\u22121 (the Covington index) a value of F averaged over several solar cycles. The Covington index varies typically between 70 and 250 during a solar cycle, and never drops below about 50. Thus, T\u221e varies between about 740 and 1350\u00a0K. During very quiet magnetospheric conditions, the still continuously flowing magnetospheric energy input contributes by about 250\u00a0 K to the residual temperature of 500\u00a0 K in eq.(2). The rest of 250\u00a0 K in eq.(2) can be attributed to atmospheric waves generated within the troposphere and dissipated within the lower thermosphere.\nSolar XUV radiation.\nThe solar X-ray and extreme ultraviolet radiation (XUV) at wavelengths &lt; 170\u00a0 nm is almost completely absorbed within the thermosphere. This radiation causes the various ionospheric layers as well as a temperature increase at these heights (Figure 1).\nWhile the solar visible light (380 to 780\u00a0 nm) is nearly constant with the variability of not more than about 0.1% of the solar constant, the solar XUV radiation is highly variable in time and space. For instance, X-ray bursts associated with solar flares can dramatically increase their intensity over preflare levels by many orders of magnitude over some time of tens of minutes. In the extreme ultraviolet, the Lyman \u03b1 line at 121.6\u00a0nm represents an important source of ionization and dissociation at ionospheric D layer heights. During quiet periods of solar activity, it alone contains more energy than the rest of the XUV spectrum. Quasi-periodic changes of the order of 100% or greater, with periods of 27 days and 11 years, belong to the prominent variations of solar XUV radiation. However, irregular fluctuations over all time scales are present all the time. During the low solar activity, about half of the total energy input into the thermosphere is thought to be solar XUV radiation. That solar XUV energy input occurs only during daytime conditions, maximizing at the equator during equinox.\nSolar wind.\nThe second source of energy input into the thermosphere is solar wind energy which is transferred to the magnetosphere by mechanisms that are not well understood. One possible way to transfer energy is via a hydrodynamic dynamo process. Solar wind particles penetrate the polar regions of the magnetosphere where the geomagnetic field lines are essentially vertically directed. An electric field is generated, directed from dawn to dusk. Along the last closed geomagnetic field lines with their footpoints within the auroral zones, field-aligned electric currents can flow into the ionospheric dynamo region where they are closed by electric Pedersen and Hall currents. Ohmic losses of the Pedersen currents heat the lower thermosphere (see e.g., Magnetospheric electric convection field). Also, penetration of high energetic particles from the magnetosphere into the auroral regions enhance drastically the electric conductivity, further increasing the electric currents and thus Joule heating. During the quiet magnetospheric activity, the magnetosphere contributes perhaps by a quarter to the thermosphere's energy budget. This is about 250\u00a0 K of the exospheric temperature in eq.(2). During the very large activity, however, this heat input can increase substantially, by a factor of four or more. That solar wind input occurs mainly in the auroral regions during both day and night.\nAtmospheric waves.\nTwo kinds of large-scale atmospheric waves within the lower atmosphere exist: internal waves with finite vertical wavelengths which can transport wave energy upward, and external waves with infinitely large wavelengths that cannot transport wave energy. Atmospheric gravity waves and most of the atmospheric tides generated within the troposphere belong to the internal waves. Their density amplitudes increase exponentially with height so that at the mesopause these waves become turbulent and their energy is dissipated (similar to breaking of ocean waves at the coast), thus contributing to the heating of the thermosphere by about 250\u00a0 K in eq.(2). On the other hand, the fundamental diurnal tide labeled (1, \u22122) which is most efficiently excited by solar irradiance is an external wave and plays only a marginal role within the lower and middle atmosphere. However, at thermospheric altitudes, it becomes the predominant wave. It drives the electric Sq-current within the ionospheric dynamo region between about 100 and 200\u00a0 km height.\nHeating, predominately by tidal waves, occurs mainly at lower and middle latitudes. The variability of this heating depends on the meteorological conditions within the troposphere and middle atmosphere, and may not exceed about 50%.\nDynamics.\nWithin the thermosphere above an altitude of about , all atmospheric waves successively become external waves, and no significant vertical wave structure is visible. The atmospheric wave modes degenerate to the spherical functions Pnm with m a meridional wave number and n the zonal wave number (m = 0: zonal mean flow; m = 1: diurnal tides; m = 2: semidiurnal tides; etc.). The thermosphere becomes a damped oscillator system with low-pass filter characteristics. This means that smaller-scale waves (greater numbers of (n,m)) and higher frequencies are suppressed in favor of large-scale waves and lower frequencies. If one considers very quiet magnetospheric disturbances and a constant mean exospheric temperature (averaged over the sphere), the observed temporal and spatial distribution of the exospheric temperature distribution can be described by a sum of spheric functions:\n(3) \u00a0 formula_3\nHere, it is \u03c6 latitude, \u03bb longitude, and t time, \u03c9a the angular frequency of one year, \u03c9d the angular frequency of one solar day, and \u03c4 = \u03c9dt + \u03bb the local time. ta = June 21 is the date of northern summer solstice, and \u03c4d = 15:00 is the local time of maximum diurnal temperature.\nThe first term in (3) on the right is the global mean of the exospheric temperature (of the order of 1000\u00a0 K). The second term [with P20 = 0.5(3 sin2(\u03c6)\u22121)] represents the heat surplus at lower latitudes and a corresponding heat deficit at higher latitudes (Fig. 2a). A thermal wind system develops with the wind toward the poles in the upper level and winds away from the poles in the lower level. The coefficient \u0394T20 \u2248 0.004 is small because Joule heating in the aurora regions compensates that heat surplus even during quiet magnetospheric conditions. During disturbed conditions, however, that term becomes dominant, changing sign so that now heat surplus is transported from the poles to the equator. The third term (with P10 = sin \u03c6) represents heat surplus on the summer hemisphere and is responsible for the transport of excess heat from the summer into the winter hemisphere (Fig. 2b). Its relative amplitude is of the order \u0394T10 \u2243 0.13. The fourth term (with P11(\u03c6) = cos \u03c6) is the dominant diurnal wave (the tidal mode (1,\u22122)). It is responsible for the transport of excess heat from the daytime hemisphere into the nighttime hemisphere (Fig. 2d). Its relative amplitude is \u0394T11\u2243 0.15, thus on the order of 150\u00a0K. Additional terms (e.g., semiannual, semidiurnal terms, and higher-order terms) must be added to eq.(3). However, they are of minor importance. Corresponding sums can be developed for density, pressure, and the various gas constituents.\nThermospheric storms.\nIn contrast to solar XUV radiation, magnetospheric disturbances, indicated on the ground by geomagnetic variations, show an unpredictable impulsive character, from short periodic disturbances of the order of hours to long-standing giant storms of several days' duration. The reaction of the thermosphere to a large magnetospheric storm is called a thermospheric storm. Since the heat input into the thermosphere occurs at high latitudes (mainly into the auroral regions), the heat transport is represented by the term P20 in eq.(3) is reversed. Also, due to the impulsive form of the disturbance, higher-order terms are generated which, however, possess short decay times and thus quickly disappear. The sum of these modes determines the \"travel time\" of the disturbance to the lower latitudes, and thus the response time of the thermosphere with respect to the magnetospheric disturbance. Important for the development of an ionospheric storm is the increase of the ratio N2/O during a thermospheric storm at middle and higher latitude. An increase of N2 increases the loss process of the ionospheric plasma and causes therefore a decrease of the electron density within the ionospheric F-layer (negative ionospheric storm).\nClimate change.\nA contraction of the thermosphere has been observed as a possible result in part due to increased carbon dioxide concentrations, the strongest cooling and contraction occurring in that layer during solar minimum. The most recent contraction in 2008\u20132009 was the largest such since at least 1967.\nPhenomena in the thermosphere.\nELVES are a type of upper-atmospheric lightning that occur at the lower boundary of the thermosphere. They often appear at above the ground over thunderstorms as a expanding and flat dimly red glow around in diameter that lasts for typically one millisecond. ELVES were first recorded on a Space Shuttle mission off French Guiana on October 7, 1990.\nELVES is a whimsical acronym for \"emissions of light and very low frequency perturbations due to electromagnetic pulse sources.\" This refers to the process by which the light is generated; the excitation of nitrogen molecules due to electron collisions (the electrons possibly having been energized by the electromagnetic pulse caused by a discharge from an underlying thunderstorm).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47465", "revid": "34577847", "url": "https://en.wikipedia.org/wiki?curid=47465", "title": "Aten", "text": "Ancient Egyptian god\nAten, also Aton, Atonu, or Itn (, reconstructed ) was the focus of Atenism, the religious system formally established in ancient Egypt by the late Eighteenth Dynasty pharaoh Akhenaten. Exact dating for the Eighteenth Dynasty is contested, though a general date range places the dynasty in the years 1550 to 1292\u00a0BCE. The worship of Aten and the coinciding rule of Akhenaten are major identifying characteristics of a period within the Eighteenth Dynasty referred to as the Amarna Period (c.\u20091353\u00a0\u2013 1336\u00a0BCE).\nAtenism and the worship of the Aten as the sole god of ancient Egypt state worship did not persist beyond Akhenaten's death. Not long after his death, one of Akhenaten's Eighteenth Dynasty successors, Tutankhamun, reopened the state temples to other Egyptian gods and re-positioned Amun as the pre-eminent solar deity. Aten is depicted as a solar disc emitting rays terminating in human hands.\nEtymology.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nThe word \"Aten\" appears in the Old Kingdom as a noun meaning \"disc\" which referred to anything flat and circular; the sun was called the \"disc of the day\" where Ra was thought to reside. By analogy, the term \"silver aten\" was sometimes used to refer to the moon. High relief and low relief illustrations of the Aten show it with a curved surface. Therefore, the late scholar Hugh Nibley insisted that a more correct translation would be globe, orb or sphere, rather than disk.\nOrigins.\nThe Aten was the disc of the sun and originally an aspect of Ra, the sun god in traditional ancient Egyptian religion. Aten does not have a creation myth or family but is mentioned in the \"Book of the Dead\". The first known reference to Aten the sun-disk as a deity is in \"The Story of Sinuhe\" from the 12th Dynasty, in which the deceased king is described as rising as a god to the heavens and \"uniting with the sun-disk, the divine body merging with its maker\".\nWhile the Aten was worshipped under the reign of Amenhotep III, it was made the sole deity to receive state and official cult worship under his successor Akhenaten, though archaeological evidence suggests the closing of the state temples of other Egyptian gods likely did not stop household worship of the traditional pantheon. Inscriptions, such as the \"Great Hymn to the Aten\", found in temples and tombs during Akhenaten's reign showcase the Aten as the creator, giver of life, and nurturing spirit of the world.\nReligion.\nAten was extensively worshipped as a solar deity during the reign of Amenhotep III where it was depicted as a falcon-headed god like Ra. While Aten was the preeminent creator deity of a pantheon of ancient Egyptian gods under Amenhotep III, it was not until his successor that Aten would be the only god acknowledged via state worship. During the reign of Amenhotep III's successor, Amenhotep IV, the Aten became the sole god of the Egyptian state religion, and Amenhotep IV changed his name to Akhenaten to reflect his close link with the supreme deity. The sole worship of Aten can be referred to as Atenism. Many of the core principles of Atenism were recorded in the capital city Akhenaten founded and moved his dynastic government to, Akhetaten, referred to as either Amarna, El-Amarna, or Tell el-Amarna by modern scholars.\nIn Atenism, night is a time to fear. Work is done best when the sun, and thus Aten, is present. The Aten created all countries and people, and cares for every creature. According to the inscriptions, the Aten created a Nile river in the sky (rain) for the Syrians. The rays of the sun disk only holds out life to the royal family, and because of this non-royals receives life from Akhenaten and Nefertiti, later Neferneferuaten, in exchange for loyalty to the Aten. In inscriptions, like the Hymn to the Aten and the King, the Aten is depicted as caring for the people through Akhenaten, placing the royal family as intermediaries for the worship of the Aten. There is only one known instance of the Aten talking.\nIn the Hymn to Aten, a love for humanity and the Earth is depicted in Aten's mannerisms:\n\"Aten bends low, near the earth, to watch over his creation; he takes his place in the sky for the same purpose; he wearies himself in the service of the creatures; he shines for them all; he gives them sun and sends them rain. The unborn child and the baby chick are cared for; and Akhenaten asks his divine father to 'lift up' the creatures for his sake so that they might aspire to the condition of perfection of his father, Aten.\"\nAkhenaten represented himself as the son of Aten, mirroring many of his predecessors' claims of divine birth and their positions as the embodiment of Horus. Akhenaten positioned himself as the only intermediary who could speak to Aten, emphasizing the dominance of Aten as the preeminent deity. This has led to discussion of whether Atenism should be considered a monotheistic religion, and thus making it one of the first examples of monotheism.\nAten is both a unique deity and a continuation of the traditional idea of a sun-god in ancient Egyptian religion, deriving a lot of the concepts of power and representation from the earlier solar deities like Ra, but building on top of the power Ra and many of his contemporaries represents. Aten carried absolute power in the universe, representing the life-giving force of light to the world as well as merging with the concept and goddess Ma'at to develop further responsibilities for Aten beyond the power of light itself.\nWorship.\nThe cult-center of the Aten was at the capital city Akhenaten founded, Akhetaten, though other cult sites have been found in Thebes and Heliopolis. The use of Amarna as a capital city and religious center was relatively short lived compared to the 18th Dynasty or New Kingdom as a whole as it was shortly abandoned after the death of Akhenaten. Inscriptions found on boundary stela accredited to Akhenaten discuss his desire to make the city a place of worship to Aten, dedicating the city to the god and emphasizing the royal residences' efforts in worship. Major principles of the Aten's cult worship were recorded via inscriptions on temples and tombs from the period. Straying significantly from the tradition of ancient Egyptian temples being hidden and more enclosed the further one went into the site, temples of Aten were open and did not have roofs in order to allow the rays of the sun inside. No statues of Aten were allowed as they were seen as idolatry. However, these were typically replaced by functionally equivalent representations of Akhenaten and his family venerating the Aten and receiving the ankh, the breath of life, from him. Compared to periods before and after the Amarna Period, Priests had less to do since offerings, such as fruits, flowers, and cakes were limited, and oracles were not needed.\nIn the worship of the Aten, the daily service of purification, anointment, and clothing of the divine image that is traditionally found in ancient Egyptian worship was not performed. Instead, incense and food-stuff offerings such as meats, wines, and fruits were placed onto open-air altars. A common scene in carved depictions of Akhenaten giving offering to Aten has him consecrating the sacrificed goods with a royal scepter. Instead of barque-processions, the royal family rode in a chariot on festival days. Elite women were known to worship the Aten in sun-shade temples in Akhetaten.\nIconography.\nAten was considered to have been everywhere and intangible as Aten was the sunlight and energy in the world. Therefore, he did not have physical representations that other traditional ancient Egyptian gods had, instead represented via the sun disc and reaching rays of light tipped with human-like hands. The explanation as to why the Aten could not be fully represented was that the Aten was beyond creation. Thus the inscriptions of scenes of gods carved in stone previously depicted animals and human forms instead showed the Aten as an orb above with life-giving rays stretching toward the royal figure. This power transcended human or animal form.\nLater, iconoclasm was enforced, and even sun disc depictions of Aten were prohibited in an edict issued by Akhenaten. In the edict, he stipulated that Aten's name was to be spelt phonetically.\nArchitecture.\nTwo temples were central to the city of Akhetaten. The larger of the two had an \"open, unroofed structure covering an area of about 800 by 300 metres (2,600 ft \u00d7 1,000 ft) at the northern end of the city\". Doorways had broken lintels and raised thresholds. Temples to the Aten were open-air structures with little-to-no roofing to maximize the amount of sunlight on the interior making them unique compared to other Egyptian temples of the time. Balustrades depict Akhenaten and the royal family embracing the rays of the Aten flanked stairwells, ramps, and altars. These fragments were initially identified as stele but were later reclassified as balustrades based on the presence of scenes on both sides.\nRoyal titulary.\nInscriptions in tombs and temples during the Amarna Period often gave Aten a royal titulary enclosed in a double cartouche. Some have interpreted this to mean that Akhenaten was the embodiment of Aten, and the worship of Aten is directly worship of Akhenaten; but others have taken this as an indicator of Aten as the supreme ruler even over the current reigning royalty.\nThere were two forms of the title; the first had the names of other gods, and the second later one was more 'singular' and referred only to the Aten himself. The early form was \"Re-Horakhti who rejoices in the Horizon, in his name Shu, which is the Aten.\" The later form was \"Re, ruler of the two horizons, who rejoices in the Horizon, in his name of light, which is the Aten.\"\nQuestion of monotheism.\nRa-Horus, more usually referred to as Ra-Horakhty (\"Ra who is Horus of the two horizons\"), is a synthesis of two other gods, both of which are attested from very early on in ancient Egyptian religious practice. During the Amarna Period, this synthesis was seen as the invisible source of energy of the sun god, of which the visible manifestation was the Aten, the solar disk. Thus Ra-Horus-Aten was a development of old ideas which came gradually. The real change, as some see it, was the apparent abandonment of all other gods on the state level, especially Amun-Ra, prohibition of idolatry, and the debatable introduction of quasi-monotheism by Akhenaten. The syncretism is readily apparent in the Great Hymn to the Aten in which Re-Herakhty, Shu, and Aten are merged into the creator god. Others see Akhenaten as a practitioner of an Aten monolatry, as he did not actively deny the existence of other gods; he simply refrained from worshipping any but the Aten. Other scholars call the religion henotheistic.\nEnd of Atenism.\nAs pharaoh, Akhenaten was considered the 'high priest' or even a prophet of the Aten, and during his reign was one of the main propagators of Atenism in Egypt. After the death of Akhenaten, Tutankhamun reinstated the cult of Amun, and the ban on the state worship of non-Atenism deities was lifted in favor of a return to the traditional ancient Egyptian pantheon. The point of this transition can be seen in the name-change of Tutankh\"aten\" into Tutankh\"amun\" indicating the loss of favor in the worship of the Aten. While there was no purge of the cult after Akhenaten's death, the Aten persisted in Egypt for another ten years or so until it seemed to fade. When Tutankhamun came into power, his religious reign was one of tolerance, with the major difference being that the Aten was no longer the only god worshipped within official, state capacity. Tutankhamun made efforts to rebuild the state temples that were destroyed during Akhenaten's reign and reinstate the traditional pantheon of gods. This seemed to be \"a move based publicly on the doctrine that Egypt's woes stemmed directly from its ignoring the gods, and in turn the gods' abandonment of Egypt\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47466", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=47466", "title": "Mesopause", "text": "Boundary between the mesosphere and thermosphere\nThe mesopause is the point of minimum temperature at the boundary between the mesosphere and the thermosphere atmospheric regions. Due to the lack of solar heating and very strong radiative cooling from carbon dioxide, the mesosphere is the coldest region on Earth with temperatures as low as -100\u00a0\u00b0C (-148\u00a0\u00b0F or 173 K). The altitude of the mesopause for many years was assumed to be at around 85\u00a0km (53\u00a0mi), but observations to higher altitudes and modeling studies in the last 10 years have shown that in fact there are two mesopauses - one at about 85\u00a0km and a stronger one at about 100\u00a0km (62\u00a0mi), with a layer of slightly warmer air between them.\nAnother feature is that the summer mesopause is cooler than the winter (sometimes referred to as the \"mesopause anomaly\"). It is due to a summer-to-winter circulation giving rise to upwelling at the summer pole and downwelling at the winter pole. Air rising will expand and cool resulting in a cold summer mesopause and conversely downwelling air results in compression and associated increase in temperature at the winter mesopause. In the mesosphere the summer-to-winter circulation is due to gravity wave dissipation, which deposits momentum against the mean east\u2013west flow, resulting in a small north\u2013south circulation.\nIn recent years the mesopause has also been the focus of studies on global climate change associated with increases in CO2. Unlike the troposphere, where greenhouse gases result in the atmosphere heating up, increased CO2 in the mesosphere acts to cool the atmosphere due to increased radiative emission. This results in a measurable effect - the mesopause should become cooler with increased CO2. Observations do show a decrease of temperature of the mesopause, though the magnitude of this decrease varies and is subject to further study. Modeling studies of this phenomenon have also been carried out.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47468", "revid": "62", "url": "https://en.wikipedia.org/wiki?curid=47468", "title": "Apollo (god)", "text": ""}
{"id": "47469", "revid": "50854919", "url": "https://en.wikipedia.org/wiki?curid=47469", "title": "Amor", "text": "Amor (\"love\" in Latin, Spanish and Portuguese) may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47470", "revid": "49863756", "url": "https://en.wikipedia.org/wiki?curid=47470", "title": "Apollo asteroid", "text": "Group of near-Earth asteroids\nThe Apollo asteroids are a group of near-Earth asteroids named after 1862 Apollo, discovered by German astronomer Karl Reinmuth in the 1930s. They are Earth-crossing asteroids that have an orbital semi-major axis greater than that of the Earth (a &gt; 1 AU) but perihelion distances less than the Earth's aphelion distance (q &lt; 1.017 AU).\nAs of \u00a02025[ [update]], the number of known Apollo asteroids is 21,083, making the class the largest group of near-Earth objects (\"cf\". the Aten, Amor and Atira asteroids), of which 1,742 are numbered (asteroids are not numbered until they have been observed at two or more oppositions), 81 are named, and 2,130 are identified as potentially hazardous asteroids.\nThe closer their semi-major axis is to Earth's, the less eccentricity is needed for the orbits to cross. The Chelyabinsk meteor, that exploded over the city of Chelyabinsk in the southern Urals region of Russia on February 15, 2013, injuring an estimated 1,500 people with flying glass from broken windows, was an Apollo-class asteroid.\nApollo asteroids are generally named after Greek deities.\nList.\nThe largest known Apollo asteroid is 1866 Sisyphus, with a diameter of about 8.5\u00a0km. Examples of known Apollo asteroids include:"}
{"id": "47473", "revid": "47120006", "url": "https://en.wikipedia.org/wiki?curid=47473", "title": "Algal bloom", "text": "Spread of planktonic algae in water\nAn algal bloom or algae bloom is a rapid increase or accumulation in the population of algae in fresh water or marine water systems. It may be a benign or harmful algal bloom.\nAlgal bloom is often recognized by the discoloration in the water from the algae's pigments. The term \"algae\" encompasses many types of aquatic photosynthetic organisms, both macroscopic multicellular organisms like seaweed and microscopic unicellular organisms like cyanobacteria. \"Algal bloom\" commonly refers to the rapid growth of microscopic unicellular algae, not macroscopic algae. An example of a macroscopic algal bloom is a kelp forest.\nAlgal blooms are the result of a nutrient, like nitrogen or phosphorus from various sources (for example fertilizer runoff or other forms of nutrient pollution), entering the aquatic system and causing excessive growth of algae. An algal bloom affects the whole ecosystem.\nConsequences range from benign effects, such as feeding of higher trophic levels, to more harmful effects like blocking sunlight from reaching other organisms, causing a depletion of oxygen levels in the water, and, depending on the organism, releasing toxins into the water. Yet, algae also play a crucial role by producing about 70 % of Earth's oxygen, which supports terrestrial life. Blooms that can injure animals or the ecology, especially those blooms where toxins are secreted by the algae, are usually called \"harmful algal blooms\" (HAB), and can lead to fish die-offs, cities cutting off water to residents, or states having to close fisheries. The process of the oversupply of nutrients leading to algae growth and oxygen depletion is called eutrophication.\nAlgal and bacterial blooms have persistently contributed to mass extinctions driven by global warming in the geologic past, such as during the end-Permian extinction driven by Siberian Traps volcanism and during the biotic recovery following the mass extinction (by delaying the recovery).\nDescription.\nThe term \"algal bloom\" is defined inconsistently depending on the scientific field, and can range from a \"minibloom\" of harmless algae to a large, harmful bloom event. Since \"algae\" is a broad term including organisms of widely varying sizes, growth rates, and nutrient requirements, there is no officially recognized threshold level as to what is defined as a bloom. Because there is no scientific consensus, blooms can be described and quantified in several ways: measurements of new algal biomass, the concentration of photosynthetic pigment, quantification of the bloom's negative effect, or relative concentration of the algae compared to the rest of the microbial community. For example, definitions of blooms have included:\nBlooms are the result of a nutrient needed by the particular algae being introduced to the local aquatic system. This growth-limiting nutrient is typically nitrogen or phosphorus, but can also be iron, vitamins, or amino acids. There are several mechanisms for the addition of these nutrients to the water. In the open ocean and along coastlines, upwelling from both winds and topographical ocean floor features can draw nutrients to the photic, or sunlit zone of the ocean. Along coastal regions and in freshwater systems, agricultural, city, and sewage runoff can cause algal blooms.\nAlgal blooms, especially large algal bloom events, can reduce the transparency of the water and can discolor it. The photosynthetic pigments in the algal cells, like chlorophyll and photoprotective pigments, determine the color of the algal bloom. Depending on the organism, its pigments, and the depth in the water column, algal blooms can be green, red, brown, golden, or purple. Bright green blooms in freshwater systems are frequently a result of cyanobacteria (colloquially known as \"blue-green algae\") such as \"Microcystis\". Blooms may also consist of macroalgal (non-phytoplanktonic) species. These blooms are recognizable by large blades of algae that may wash up onto the shoreline.\nOnce the nutrient is present in the water, the algae begin to grow at a much faster rate than usual. In a mini bloom, this fast growth benefits the whole ecosystem by providing food and nutrients for other organisms.\nOf particular note are the harmful algal blooms (HABs), which are algal bloom events involving toxic or otherwise harmful phytoplankton. Many species can cause harmful algal blooms. For example,\nFreshwater algal blooms.\nFreshwater algal blooms are the result of an excess of nutrients, particularly some phosphates. Excess nutrients may originate from fertilizers that are applied to land for agricultural or recreational purposes and may also originate from household cleaning products containing phosphorus.\nThe reduction of phosphorus inputs is required to mitigate blooms that contain cyanobacteria. In lakes that are stratified in the summer, autumn turnover can release substantial quantities of bio-available phosphorus potentially triggering algal blooms as soon as sufficient photosynthetic light is available. Excess nutrients can enter watersheds through water runoff. Excess carbon and nitrogen have also been suspected as causes. Presence of residual sodium carbonate acts as catalyst for the algae to bloom by providing dissolved carbon dioxide for enhanced photosynthesis in the presence of nutrients.\nWhen phosphates are introduced into water systems, higher concentrations cause increased growth of algae and plants. Algae tend to grow very quickly under high nutrient availability, but each alga is short-lived, and the result is a high concentration of dead organic matter which starts to decompose. Natural decomposers present in the water begin decomposing the dead algae, consuming dissolved oxygen present in the water during the process. This can result in a sharp decrease in available dissolved oxygen for other aquatic life. Without sufficient dissolved oxygen in the water, animals and plants may die off in large numbers. This may also be known as a dead zone.\nBlooms may be observed in freshwater aquariums when fish are overfed and excess nutrients are not absorbed by plants. These are generally harmful for fish, and the situation can be corrected by changing the water in the tank and then reducing the amount of food given.\nNatural causes of algal blooms.\nAlgal blooms in freshwater systems are not always caused by human contamination and have been observed to occur naturally in both eutrophic and oligotrophic lakes. Eutrophic lakes contain an abundance of nutrients such as nitrogen and phosphates which increase the likelihood for blooms. Oligotrophic lakes don't contain much of these nutrients. Oligotrophic lakes are defined by various degrees of scarcity. The trophic state index (TSI) measures nutrients in freshwater systems and a TSI under 30 defines oligotrophic waters. Algal blooms in oligotrophic bodies of water have also been observed. This is a result of cyanobacteria which cause blooms in eutrophic lakes and oligotrophic lakes despite the latter containing a lack of natural and man-made nutrients.\nNutrient uptake and cyanobacteria.\nA cause for algal blooms in nutrient-lacking environments come in the form of nutrient uptake. Cyanobacteria have evolved to have better nutrient uptake in oligotrophic waters. Cyanobacteria utilize nitrogen and phosphates in their biological processes. Because of this, cyanobacteria are known to be important in the nitrogen and phosphate fixing cycle in oligotrophic waters. Cyanobacteria can fix nitrogen by accessing atmospheric nitrogen () that has been dissolved into water and transforming it into nitrogen accessible to other organisms. This higher amount of nitrogen is then able to sustain large algae blooms in oligotrophic waters.\nCyanobacteria are able to retain high phosphorus uptake in the absence of nutrients which help their success in oligotrophic environments. Cyanobacteria species such as \"D. lemmermannii\" are able to move between the hypolimnion which is rich in nutrients such as phosphates and the nutrient-poor metalimnion which lacks phosphates. This causes phosphates to be brought up to the metalimnion and give organisms an abundance of phosphates, exacerbating the likelihood for algal blooms.\nUpwelling of nutrients.\nUpwelling events happen when nutrients such as phosphates and nitrogen are moved from the nutrient dense hypolimnion to the nutrient poor metalimnion. This happens as result of geological processes such as seasonal overturn when lake surfaces freeze or melt, prompting mixing due to changing water densities mixing up the composition of limnion layers and mixing nutrients around the system. This overabundance in nutrients leads to blooms.\nMarine algal blooms.\nTurbulent storms churn the ocean in summer, adding nutrients to sunlit waters near the surface. This sparks a feeding frenzy each spring that gives rise to massive blooms of phytoplankton. Tiny molecules found inside these microscopic plants harvest vital energy from sunlight through photosynthesis. The natural pigments, called chlorophyll, allow phytoplankton to thrive in Earth's oceans and enable scientists to monitor blooms from space.\nSatellites reveal the location and abundance of phytoplankton by detecting the amount of chlorophyll present in coastal and open waters\u2014the higher the concentration, the larger the bloom. Observations show blooms typically last until late spring or early summer, when nutrient stocks are in decline and predatory zooplankton start to graze. The visualization on the left immediately below uses NASA SeaWiFS data to map bloom populations.\nThe NAAMES study conducted between 2015 and 2019 investigated aspects of phytoplankton dynamics in ocean ecosystems, and how such dynamics influence atmospheric aerosols, clouds, and climate.\nIn France, citizens are requested to report coloured waters through the project PHENOMER. This helps to understand the occurrence of marine blooms.\nWildfires can cause phytoplankton blooms via oceanic deposition of wildfire aerosols.\nHarmful algal blooms.\nA harmful algal bloom (HAB) is an algal bloom that causes negative impacts to other organisms via production of natural toxins, mechanical damage to other organisms, or by other means. The diversity of these HABs make them even harder to manage, and present many issues, especially to threatened coastal areas. HABs are often associated with large-scale marine mortality events and have been associated with various types of shellfish poisonings. Due to their negative economic and health impacts, HABs are often carefully monitored.\nHAB has been proved to be harmful to humans. Humans may be exposed to toxic algae by direct consuming seafood containing toxins, swimming or other activities in water, and breathing tiny droplets in the air that contain toxins. Because human exposure can take place by consuming seafood products that contain the toxins expelled by HAB algae, food-borne diseases are present and can affect the nervous, digestive, respiratory, hepatic, dermatological, and cardiac systems in the body.\nBeach users have often experienced upper respiratory diseases, eye and nose irritation, fever, and have often needed medical care in order to be treated. Ciguatera fish poisoning (CFP) is very common from the exposure of algal blooms. Water-borne diseases are also present as our drinking waters can be contaminated by cyanotoxins.\nIf the HAB event results in a high enough concentration of algae the water may become discoloured or murky, varying in colour from purple to almost pink, normally being red or green. Not all algal blooms are dense enough to cause water discolouration.\nBioluminescence.\n\"Dinoflagellate\"s are microbial eukaryotes that link bioluminesce and toxin production in algal blooms. They use a luciferin-luciferase reaction to create a blue light emission glow. There are seventeen major types of \"dinoflagellate\" toxins, in which the strains, Saxitoxin and Yessotoxin, are both bioluminescent and toxic. These two strains are found to have similar niches in coastal areas. A surplus of \"Dinoflagellates\" in the night time creates a blue-green glow, however, in the day, it presents as a red brown color which names algal blooms, Red Tides. \"Dinoflagellates\" have been reported to be the cause of seafood poisoning from the neurotoxins.\nManagement.\nThere are three major categories for management of algal blooms consisting of mitigation, prevention, and control. \nWithin mitigation, routine monitoring programs are implemented for toxins in shellfish for early warnings and an overall surveillance of the area to monitor and quantify harmful algal blooms. The HAB levels of the shellfish will be determined and can manage restrictions to keep contaminated shellfish off the food market. Moving fish pens away from algal blooms is also another form of mitigation.\nWithin prevention, we can reduce surface runoff carrying excess nutrients by increasing the amount of permeable surfaces and vegetation. Permeable surfaces help absorb the runoff before it can make its way into the waterway. We can put into place permeable streets and parking lots which help allow for the pollution from vehicles and other runoff nutrients to be soaked up and/or slowed.\nVegetation filters, absorbs, and slows the runoff which also helps to reduce the amount of excess nutrients making their way into the waterway. Examples of planted vegetation to help reduce runoff include rain gardens, replacing grass with native plants, planting trees in yards and along waterways, and even rooftop gardens. Farmers can reduce their impact on our waterways by planting cover crops, planting forested buffers, reducing their fertilizer use, and putting up fences to keep livestock out of streams.\nWithin control, there are mechanical, biological, chemical, genetic and environmental controls. Mechanical control involves dispersing clay into the water to aggregate with the HAB leading to less of these HAB to go through the process of sedimentation. Biological control varies largely and can be used through pheromones or releasing sterile males to reduce reproduction. Chemical control uses toxic chemical release. However, it may cause problems of mortality of other non targeted organisms. Genetic control involves genetically engineering species in their environmental tolerances and reproduction processes. However, there are problems of harming indigenous organisms. For environmental control, it can use water circulation and aeration.\nEnvironmental impacts.\nHarmful algae blooms have many negative environmental impacts and is a worsening issue that is spreading in area. A tiny brown tide organism that was formerly restricted to the northeastern US and South Africa, is now causing massive blooms along the coast of China which is similar to that of other brown tides. HABs can lead to anaerobic (lack of oxygen) environments which can kill any organisms living within the water, fish poisoning, respiratory problems and illness among beach goers.\nHABs have a large effect on the Great Lakes St. Lawrence River Basin. Invasive zebra and quagga mussels are positively correlated with their impact on the environment. These mussels increase the cycling of phosphorus which therefore increases harmful algae blooms in areas they are present. Harmful algae blooms continue to infect water supplies at the Binational Great Lakes Basin and due to the world's recovery from the COVID-19 pandemic, solving the issue has become a low priority. This economical problem has become part of politics in the United States, whereas in allied countries such as Canada there is low concern.\nThe impact of harmful algae blooms on the environment have a substantial effect on marine life. For example, in August 2024 the growth of the toxic algae, \"Pseudo-nitzschia\", along California coasts were making sea lions sick and aggressive to beach goers. Scientists claim this is a seasonal occurrence. The growth of \"Pseudo-nitzschia\" leads to the production of a dominic acid which accumulates in fishes such as sardines, anchovies, and squids. This directly affects the food web and the primary food source of sea lions. Once the toxins are transferred via consumption, they can cause seizures, brain damage, and death to the animal. During this surge, people reported bites and unpredictable, aggressive behavior from the infected sea lions. In this sickened state, the sea lions are scared and act out of fear in order to protect themselves. Pregnant sea lions are most vulnerable to toxic algae poisoning and are more likely to die from the effects.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47474", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=47474", "title": "Aperture", "text": "Hole or opening through which light travels\nIn optics, the aperture of an optical system (including a system consisting of a single lens) is the hole or opening that primarily limits light propagated through the system. The aperture defines a bundle of rays from each point on an object that will come to a focus in the image plane.\nAn optical system typically has many structures that limit ray bundles (ray bundles are also known as \"pencils\" of light). These structures may be the edge of a lens or mirror, or a ring or other fixture that holds an optical element in place or may be a special element such as a diaphragm placed in the optical path to limit the light admitted by the system. These structures are called \"stops\", and the aperture stop is the stop that primarily determines the cone of rays that an optical system accepts (see entrance pupil). As a result, it also determines the ray cone angle and brightness at the image point (see exit pupil). Optical systems are typically designed for a particular stop to be the aperture stop, but it is possible for different stops to serve as the aperture stop for objects at different distances. Some rays from object points away from the optical axis may clip on surfaces other than the aperture stop. This is called vignetting. The aperture stop is not necessarily the smallest stop in the system. Magnification and demagnification by lenses and other elements can cause a relatively large stop to be the aperture stop for the system. \nIn some contexts, especially in photography and astronomy, \"aperture\" refers to the opening diameter of the aperture stop through which light can pass. For example, in a telescope, the aperture stop is typically the edges of the objective lens or mirror (or of the mount that holds it). One then speaks of a telescope as having, for example, a aperture. In astrophotography, the aperture may be given as a linear measure (for example, in inches or millimetres) or as the dimensionless ratio between that measure and the focal length. In other photography, it is usually given as a ratio.\nA usual expectation is that the term \"aperture\" refers to the opening of the aperture stop, but in reality, the term aperture and the aperture stop are mixed in use. Sometimes even stops that are not the aperture stop of an optical system are also called apertures. Contexts need to clarify these terms.\nThe word aperture is also used in other contexts to indicate a system which blocks off light outside a certain region. In astronomy, for example, a photometric aperture around a star usually corresponds to a circular window around the image of a star within which the light intensity is assumed.\nApplication.\nThe aperture stop is an important element in most optical designs. Its most obvious feature is that it limits the amount of light that can reach the image/film plane. This can be either unavoidable due to the practical limit of the aperture stop size, or deliberate to prevent saturation of a detector or overexposure of film. In both cases, the size of the aperture stop determines the amount of light admitted by an optical system. The aperture stop also affects other optical system properties:\nIn addition to an aperture stop, a photographic lens may have one or more \"field stops\", which limit the system's field of view. When the field of view is limited by a field stop in the lens (rather than at the film or sensor) vignetting results; this is only a problem if the resulting field of view is less than was desired.\nIn astronomy, the opening diameter of the aperture stop (called the \"aperture\") is a critical parameter in the design of a telescope. Generally, one would want the \"aperture\" to be as large as possible, to collect the maximum amount of light from the distant objects being imaged. The size of the aperture is limited, however, in practice by considerations of its manufacturing cost and time and its weight, as well as prevention of aberrations (as mentioned above).\nApertures are also used in laser energy control, close aperture z-scan technique, diffractions/patterns, and beam cleaning. Laser applications include spatial filters, Q-switching, high intensity x-ray control.\nIn light microscopy, the word aperture may be used with reference to either the condenser (that changes the angle of light onto the specimen field), field iris (that changes the area of illumination on specimens) or possibly objective lens (forms primary images). \"See\" Optical microscope.\nIn photography.\nThe aperture stop of a photographic lens can be adjusted to control the amount of light reaching the film or image sensor. In combination with variation of shutter speed, the aperture size will regulate the film's or image sensor's degree of exposure to light. Typically, a fast shutter will require a larger aperture to ensure sufficient light exposure, and a slow shutter will require a smaller aperture to avoid excessive exposure.\nA device called a diaphragm usually serves as the aperture stop and controls the aperture (the opening of the aperture stop). The diaphragm functions much like the iris of the eye\u00a0\u2013 it controls the effective diameter of the lens opening (called pupil in the eyes). Reducing the aperture size (increasing the f-number) provides less light to sensor and also increases the depth of field (by limiting the angle of cone of image light reaching the sensor), which describes the extent to which subject matter lying closer than or farther from the actual plane of focus appears to be in focus. In general, the smaller the aperture (the larger the f-number), the greater the distance from the plane of focus the subject matter may be while still appearing in focus.\nThe lens aperture is usually specified as an f-number, the ratio of focal length to effective aperture diameter (the diameter of the entrance pupil). A lens typically has a set of marked \"f-stops\" that the f-number can be set to. A lower f-number denotes a greater aperture which allows more light to reach the film or image sensor. The photography term \"one f-stop\" refers to a factor of \u221a2 (approx. 1.41) change in f-number which corresponds to a \u221a2 change in aperture diameter, which in turn corresponds to a factor of 2 change in light intensity (by a factor 2 change in the aperture area).\nAperture priority is a semi-automatic shooting mode used in cameras. It permits the photographer to select an aperture setting and let the camera decide the shutter speed and sometimes also ISO sensitivity for the correct exposure. This is also referred to as Aperture Priority Auto Exposure, A mode, AV mode (aperture-value mode), or semi-auto mode.\nTypical ranges of apertures used in photography are about &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/22 or &lt;templatestyles src=\"F//styles.css\" /&gt;f/2 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/16, covering six stops, which may be divided into wide, middle, and narrow of two stops each, roughly (using round numbers) &lt;templatestyles src=\"F//styles.css\" /&gt;f/2 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/4, &lt;templatestyles src=\"F//styles.css\" /&gt;f/4 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/8, and &lt;templatestyles src=\"F//styles.css\" /&gt;f/8 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/16 or (for a slower lens) &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/5.6, &lt;templatestyles src=\"F//styles.css\" /&gt;f/5.6 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/11, and &lt;templatestyles src=\"F//styles.css\" /&gt;f/11 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/22. These are not sharp divisions, and ranges for specific lenses vary.\nMaximum and minimum apertures.\nThe specifications for a given lens typically include the maximum and minimum aperture (opening) sizes, for example, &lt;templatestyles src=\"F//styles.css\" /&gt;f/0.95 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/22. In this case, &lt;templatestyles src=\"F//styles.css\" /&gt;f/0.95 is currently the maximum aperture (the widest opening on a full-frame format for practical use), and &lt;templatestyles src=\"F//styles.css\" /&gt;f/22 is the minimum aperture (the smallest opening). The maximum aperture tends to be of most interest and is always included when describing a lens. This value is also known as the lens \"speed\", as it affects the exposure time. As the aperture area is proportional to the light admitted by a lens or an optical system, the aperture diameter is proportional to the square root of the light admitted, and thus inversely proportional to the square root of required exposure time, such that an aperture of &lt;templatestyles src=\"F//styles.css\" /&gt;f/2 allows for exposure times one quarter that of &lt;templatestyles src=\"F//styles.css\" /&gt;f/4. (&lt;templatestyles src=\"F//styles.css\" /&gt;f/2 is 4 times larger than &lt;templatestyles src=\"F//styles.css\" /&gt;f/4 in the aperture area.)\nLenses with apertures opening &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 or wider are referred to as \"fast\" lenses, although the specific point has changed over time (for example, in the early 20th century aperture openings wider than &lt;templatestyles src=\"F//styles.css\" /&gt;f/6 were considered fast. The fastest lenses for the common 35 mm film format in general production have apertures of &lt;templatestyles src=\"F//styles.css\" /&gt;f/1.2 or &lt;templatestyles src=\"F//styles.css\" /&gt;f/1.4, with more at &lt;templatestyles src=\"F//styles.css\" /&gt;f/1.8 and &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.0, and many at &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 or slower; &lt;templatestyles src=\"F//styles.css\" /&gt;f/1.0 is unusual, though sees some use. When comparing \"fast\" lenses, the image format used must be considered. Lenses designed for a small format such as half frame or APS-C need to project a much smaller image circle than a lens used for large format photography. Thus the optical elements built into the lens can be far smaller and cheaper.\nIn exceptional circumstances lenses can have even wider apertures with f-numbers smaller than 1.0; see lens speed: fast lenses for a detailed list. For instance, both the current Leica Noctilux-M 50mm ASPH and a 1960s-era Canon 50mm rangefinder lens have a maximum aperture of &lt;templatestyles src=\"F//styles.css\" /&gt;f/0.95. Cheaper alternatives began appearing in the early 2010s, such as the Cosina Voigtl\u00e4nder &lt;templatestyles src=\"F//styles.css\" /&gt;f/0.95 Nokton (several in the range) and &lt;templatestyles src=\"F//styles.css\" /&gt;f/0.8 () Super Nokton manual focus lenses in the Micro Four-Thirds System, and the Venus Optics (Laowa) Argus &lt;templatestyles src=\"F//styles.css\" /&gt;f/0.95.\nProfessional lenses for some movie cameras have f-numbers as small as &lt;templatestyles src=\"F//styles.css\" /&gt;f/0.75. Stanley Kubrick's film \"Barry Lyndon\" has scenes shot by candlelight with a NASA/Zeiss 50mm f/0.7, the fastest lens in film history. Beyond the expense, these lenses have limited application due to the correspondingly shallower depth of field (DOF)\u00a0\u2013 the scene must either be shallow, shot from a distance, or will be significantly defocused, though this may be the desired effect.\nZoom lenses typically have a maximum relative aperture (minimum f-number) of &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 to &lt;templatestyles src=\"F//styles.css\" /&gt;f/6.3 through their range. High-end lenses will have a constant aperture, such as &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 or &lt;templatestyles src=\"F//styles.css\" /&gt;f/4, which means that the relative aperture will stay the same throughout the zoom range. A more typical consumer zoom will have a variable maximum relative aperture since it is harder and more expensive to keep the maximum relative aperture proportional to the focal length at long focal lengths; &lt;templatestyles src=\"F//styles.css\" /&gt;f/3.5 to &lt;templatestyles src=\"F//styles.css\" /&gt;f/5.6 is an example of a common variable aperture range in a consumer zoom lens.\nBy contrast, the minimum aperture does not depend on the focal length\u00a0\u2013 it is limited by how narrowly the aperture closes, not the lens design\u00a0\u2013 and is instead generally chosen based on practicality: very small apertures have lower sharpness due to diffraction at aperture edges, while the added depth of field is not generally useful, and thus there is generally little benefit in using such apertures. Accordingly, DSLR lens typically have minimum aperture of &lt;templatestyles src=\"F//styles.css\" /&gt;f/16, &lt;templatestyles src=\"F//styles.css\" /&gt;f/22, or &lt;templatestyles src=\"F//styles.css\" /&gt;f/32, while large format may go down to &lt;templatestyles src=\"F//styles.css\" /&gt;f/64, as reflected in the name of Group f/64. Depth of field is a significant concern in macro photography, however, and there one sees smaller apertures. For example, the Canon MP-E 65mm can have effective aperture (due to magnification) as small as &lt;templatestyles src=\"F//styles.css\" /&gt;f/96. The pinhole optic for Lensbaby creative lenses has an aperture of just &lt;templatestyles src=\"F//styles.css\" /&gt;f/177.\nAperture area.\nThe amount of light captured by an optical system is proportional to the area of the entrance pupil that is the object space-side image of the aperture of the system, equal to:\nformula_1\nWhere the two equivalent forms are related via the f-number \"N = f\" / \"D\", with focal length \"f\" and entrance pupil diameter \"D\".\nThe focal length value is not required when comparing two lenses of the same focal length; a value of 1 can be used instead, and the other factors can be dropped as well, leaving area proportion to the reciprocal square of the f-number \"N\".\nIf two cameras of different format sizes and focal lengths have the same angle of view, and the same aperture area, they gather the same amount of light from the scene. In that case, the relative focal-plane illuminance, however, would depend only on the f-number \"N\", so it is less in the camera with the larger format, longer focal length, and higher f-number. This assumes both lenses have identical transmissivity.\nAperture control.\nThough as early as 1933 Torkel Korling had invented and patented for the Graflex large format reflex camera an automatic aperture control, not all early 35mm single lens reflex cameras had the feature. With a small aperture, this darkened the viewfinder, making viewing, focusing, and composition difficult. Korling's design enabled full-aperture viewing for accurate focus, closing to the pre-selected aperture opening when the shutter was fired and simultaneously synchronising the firing of a flash unit. From 1956 SLR camera manufacturers separately developed \"automatic aperture control\" (the Miranda T 'Pressure Automatic Diaphragm', and other solutions on the Exakta Varex IIa and Praktica FX2) allowing viewing at the lens's maximum aperture, stopping the lens down to the working aperture at the moment of exposure, and returning the lens to maximum aperture afterward. The first SLR cameras with internal (\"through-the-lens\" or \"TTL\") meters (e.g., the Pentax Spotmatic) required that the lens be stopped down to the working aperture when taking a meter reading. Subsequent models soon incorporated mechanical coupling between the lens and the camera body, indicating the working aperture to the camera for exposure while allowing the lens to be at its maximum aperture for composition and focusing; this feature became known as open-aperture metering.\nFor some lenses, including a few long telephotos, lenses mounted on bellows, and perspective-control and tilt/shift lenses, the mechanical linkage was impractical, and automatic aperture control was not provided. Many such lenses incorporated a feature known as a \"preset\" aperture, which allows the lens to be set to working aperture and then quickly switched between working aperture and full aperture without looking at the aperture control. A typical operation might be to establish rough composition, set the working aperture for metering, return to full aperture for a final check of focus and composition, and focusing, and finally, return to working aperture just before exposure. Although slightly easier than stopped-down metering, operation is less convenient than automatic operation. Preset aperture controls have taken several forms; the most common has been the use of essentially two lens aperture rings, with one ring setting the aperture and the other serving as a limit stop when switching to working aperture. Examples of lenses with this type of preset aperture control are the Nikon PC Nikkor 28\u00a0mm &lt;templatestyles src=\"F//styles.css\" /&gt;f/3.5 and the SMC Pentax Shift 6\u00d77 75\u00a0mm &lt;templatestyles src=\"F//styles.css\" /&gt;f/4.5. The Nikon PC Micro-Nikkor 85\u00a0mm &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8D lens incorporates a mechanical pushbutton that sets working aperture when pressed and restores full aperture when pressed a second time.\nCanon EF lenses, introduced in 1987, have electromagnetic diaphragms, eliminating the need for a mechanical linkage between the camera and the lens, and allowing automatic aperture control with the Canon TS-E tilt/shift lenses. Nikon PC-E perspective-control lenses, introduced in 2008, also have electromagnetic diaphragms, a feature extended to their E-type range in 2013.\nOptimal aperture.\nOptimal aperture depends both on optics (the depth of the scene versus diffraction), and on the performance of the lens.\nOptically, as a lens is stopped down, the defocus blur at the Depth of Field (DOF) limits decreases but diffraction blur increases. The presence of these two opposing factors implies a point at which the combined blur spot is minimized (Gibson 1975, 64); at that point, the f-number is optimal for image sharpness, for this given depth of field\u00a0\u2013 a wider aperture (lower \"f\"-number) causes more defocus, while a narrower aperture (higher \"f\"-number) causes more diffraction.\nAs a matter of performance, lenses often do not perform optimally when fully opened, and thus generally have better sharpness when stopped down some\u00a0\u2013 this is sharpness in the plane of critical focus, setting aside issues of depth of field. Beyond a certain point, there is no further sharpness benefit to stopping down, and the diffraction occurred at the edges of the aperture begins to become significant for imaging quality. There is accordingly a sweet spot, generally in the &lt;templatestyles src=\"F//styles.css\" /&gt;f/4 \u2013 &lt;templatestyles src=\"F//styles.css\" /&gt;f/8 range, depending on lens, where sharpness is optimal, though some lenses are designed to perform optimally when wide open. How significant this varies between lenses, and opinions differ on how much practical impact this has.\nWhile optimal aperture can be determined mechanically, how much sharpness is \"required\" depends on how the image will be used\u00a0\u2013 if the final image is viewed under normal conditions (e.g., an 8\u2033\u00d710\u2033 image viewed at 10\u2033), it may suffice to determine the f-number using criteria for minimum required sharpness, and there may be no practical benefit from further reducing the size of the blur spot. But this may not be true if the final image is viewed under more demanding conditions, e.g., a very large final image viewed at normal distance, or a portion of an image enlarged to normal size (Hansma 1996). Hansma also suggests that the final-image size may not be known when a photograph is taken, and obtaining the maximum practicable sharpness allows the decision to make a large final image to be made at a later time; see also critical sharpness.\nIn biology.\nIn many living optical systems, the eye consists of an iris which adjusts the size of the pupil, through which light enters. The iris is analogous to the diaphragm, and the pupil (which is the adjustable opening in the iris) the aperture. Refraction in the cornea causes the effective aperture (the entrance pupil in optics parlance) to differ slightly from the physical pupil diameter. The entrance pupil is typically about 4\u00a0mm in diameter, although it can range from as narrow as 2\u00a0mm (&lt;templatestyles src=\"F//styles.css\" /&gt;f/8.3) in diameter in a brightly lit place to 8\u00a0mm (&lt;templatestyles src=\"F//styles.css\" /&gt;f/2.1) in the dark as part of adaptation. In rare cases, some individuals are able to dilate their pupils even beyond 8\u00a0mm (in scotopic lighting, close to the physical limit of the iris. In humans, the average iris diameter is about 11.5\u00a0mm, which naturally influences the maximal size of the pupil as well, where larger iris diameters would typically have pupils which are able to dilate to a wider extreme than those with smaller irises. Maximum dilated pupil size also decreases with age.\nThe iris controls the size of the pupil via two complementary sets muscles, the sphincter and dilator muscles, which are innervated by the parasympathetic and sympathetic nervous systems respectively, and act to induce pupillary constriction and dilation respectively. The state of the pupil is closely influenced by various factors, primarily light (or absence of light), but also by emotional state, interest in the subject of attention, arousal, sexual stimulation, physical activity, accommodation state, and cognitive load. The field of view is not affected by the size of the pupil.\nSome individuals are also able to directly exert manual and conscious control over their iris muscles and hence are able to voluntarily constrict and dilate their pupils on command. However, this ability is rare and potential use or advantages are unclear.\nEquivalent aperture range.\nIn digital photography, the 35mm-equivalent aperture range is sometimes considered to be more important than the actual f-number. Equivalent aperture is the f-number adjusted to correspond to the f-number of the same size absolute aperture diameter on a lens with a 35mm equivalent focal length. Smaller equivalent f-numbers are expected to lead to higher image quality based on more total light from the subject, as well as lead to reduced depth of field. For example, a Sony Cyber-shot DSC-RX10 uses a 1\" sensor, 24 \u2013 200\u00a0mm with maximum aperture constant along the zoom range; &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 has equivalent aperture range &lt;templatestyles src=\"F//styles.css\" /&gt;f/7.6, which is a lower equivalent f-number than some other &lt;templatestyles src=\"F//styles.css\" /&gt;f/2.8 cameras with smaller sensors.\nHowever, modern optical research concludes that sensor size does not actually play a part in the depth of field in an image. An aperture's f-number is not modified by the camera's sensor size because it is a ratio that only pertains to the attributes of the lens. Instead, the higher crop factor that comes as a result of a smaller sensor size means that, in order to get an equal framing of the subject, the photo must be taken from further away, which results in a less blurry background, changing the perceived depth of field. Similarly, a smaller sensor size with an equivalent aperture will result in a darker image because of the pixel density of smaller sensors with equivalent megapixels. Every photosite on a camera's sensor requires a certain amount of surface area that is not sensitive to light, a factor that results in differences in pixel pitch and changes in the signal-noise ratio. However, neither the changed depth of field, nor the perceived change in light sensitivity are a result of the aperture. Instead, equivalent aperture can be seen as a rule of thumb to judge how changes in sensor size might affect an image, even if qualities like pixel density and distance from the subject are the actual causes of changes in the image.\nIn scanning or sampling.\nThe terms \"scanning aperture\" and \"sampling aperture\" are often used to refer to the opening through which an image is sampled, or scanned, for example in a Drum scanner, an image sensor, or a television pickup apparatus. The sampling aperture can be a literal optical aperture, that is, a small opening in space, or it can be a time-domain aperture for sampling a signal waveform.\nFor example, film grain is quantified as \"graininess\" via a measurement of film density fluctuations as seen through a 0.048\u00a0mm sampling aperture.\nIn popular culture.\nAperture Science, a fictional company in the Portal fictional universe, is named after the optical system. The company's logo heavily features an aperture in its logo, and has come to symbolize the series, fictional company, and the Aperture Science Laboratories Computer-Aided Enrichment Center that the game series takes place in.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47476", "revid": "51070", "url": "https://en.wikipedia.org/wiki?curid=47476", "title": "Altimeter", "text": "Instrument used to determine the height of an object above a certain point\nAn altimeter or an altitude meter is an instrument used to measure the altitude of an object above a fixed level. The measurement of altitude is called altimetry, which is related to the term bathymetry, the measurement of depth under water. \nTypes.\nSonic altimeter.\nIn 1931, the US Army Air corps and General Electric together tested a sonic altimeter for aircraft, which was considered more reliable and accurate than one that relied on air pressure when heavy fog or rain was present. The new altimeter used a series of high-pitched sounds like those made by a bat to measure the distance from the aircraft to the surface, which on return to the aircraft was converted to feet shown on a gauge inside the aircraft cockpit.\nRadar altimeter.\nA radar altimeter measures altitude more directly, using the time taken for a radio signal to reflect from the surface back to the aircraft. Alternatively, Frequency Modulated Continuous-wave radar can be used. The greater the frequency shift the further the distance travelled. This method can achieve much better accuracy than the pulsed radar for the same outlay and radar altimeters that use frequency modulation are industry standard. The radar altimeter is used to measure height above ground level during landing in commercial and military aircraft. Radar altimeters are also a component of terrain avoidance warning systems, warning the pilot if the aircraft is flying too low, or if there is rising terrain ahead. Radar altimeter technology is also used in terrain-following radar allowing combat aircraft to fly at very low height above the terrain.\nAfter extensive research and experimentation, it has been shown that \"phase radio-altimeters\" are most suitable for ground effect vehicles, as compared to laser, isotropic or ultrasonic altimeters.\nLaser altimeter.\nLidar technology is used to help navigate the helicopter Ingenuity on its record-setting flights over the terrain of Mars by means of a downward-facing Lidar altimeter.\nSatellite navigation.\nSatellite navigation receivers like those used with the Global Positioning System (GPS) can also determine altitude by trilateration with four or more satellites. In aircraft, altitude determined using autonomous GPS is not reliable enough to supersede the pressure altimeter without using some method of augmentation. In hiking and climbing, it is common to find that the altitude measured by GPS is off by as much as depending on satellite orientation.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47477", "revid": "13266769", "url": "https://en.wikipedia.org/wiki?curid=47477", "title": "Ames Research Center", "text": "Research center operated by NASA\nThe Ames Research Center (ARC), also known as NASA Ames, is a major NASA research center at Moffett Federal Airfield in California's Silicon Valley. It was founded in 1939 as the second National Advisory Committee for Aeronautics (NACA) laboratory. That agency was dissolved and its assets and personnel transferred to the newly created National Aeronautics and Space Administration (NASA) on October 1, 1958. NASA Ames is named in honor of Joseph Sweetman Ames, a physicist and one of the founding members of NACA. At last estimate NASA Ames had over US$3 billion in capital equipment, 2,300 research personnel and a US$750 million annual budget.\nAmes was founded to conduct wind-tunnel research on the aerodynamics of propeller-driven aircraft; however, its role has expanded to encompass spaceflight and information technology. Ames plays a role in many NASA missions. It provides leadership in astrobiology; small satellites; robotic lunar exploration; the search for habitable planets; supercomputing; intelligent/adaptive systems; advanced thermal protection; planetary science; and airborne astronomy. Ames also develops tools for a safer, more efficient national airspace. The center's current director is Eugene Tu.\nThe site was mission center for several key missions (\"Kepler\", the Stratospheric Observatory for Infrared Astronomy (SOFIA), Interface Region Imaging Spectrograph) and a major contributor to the \"new exploration focus\" as a participant in the Orion crew exploration vehicle.\nMissions.\nAlthough Ames is a NASA Research Center, and not a flight center, it has nevertheless been closely involved in a number of astronomy and space missions.\nThe Pioneer program's eight successful space missions from 1965 to 1978 were managed by Charles Hall at Ames, initially aimed at the inner Solar System. By 1972, it supported the bold flyby missions to Jupiter and Saturn with \"Pioneer 10\" and \"Pioneer 11\". Those two missions were trail blazers (radiation environment, new moons, gravity-assist flybys) for the planners of the more complex \"Voyager 1\" and \"Voyager 2\" missions, launched five years later. In 1978, the end of the program brought about a return to the inner solar system, with the Pioneer Venus Orbiter and Multiprobe, this time using orbital insertion rather than flyby missions.\nLunar Prospector was the third mission selected by NASA for full development and construction as part of the Discovery Program. At a cost of $62.8 million, the 19-month mission was put into a low polar orbit of the Moon, accomplishing mapping of surface composition and possible polar ice deposits, measurements of magnetic and gravity fields, and study of lunar outgassing events. Based on Lunar Prospector Neutron Spectrometer (NS) data, mission scientists have determined that there is indeed water ice in the polar craters of the Moon. The mission ended July 31, 1999, when the orbiter was guided to an impact into a crater near the lunar south pole in an (unsuccessful) attempt to analyze lunar polar water by vaporizing it to allow spectroscopic characterization from Earth telescopes.\nThe 11-pound (5\u00a0kg) GeneSat-1, carrying bacteria inside a miniature laboratory, was launched on December 16, 2006. The very small NASA satellite has proven that scientists can quickly design and launch a new class of inexpensive spacecraft\u2014and conduct significant science.\nThe Lunar Crater Observation and Sensing Satellite (LCROSS) mission to look for water on the Moon was a 'secondary payload spacecraft.' LCROSS began its trip to the Moon on the same rocket as the Lunar Reconnaissance Orbiter (LRO), which continues to conduct a different lunar task. It launched in April 2009 on an Atlas V rocket from Kennedy Space Center, Florida.\nThe \"Kepler\" mission was NASA's first mission capable of finding Earth-size and smaller planets. The \"Kepler\" mission monitored the brightness of stars to find planets that pass in front of them during the planets' orbits. During such passes or 'transits,' the planets will slightly decrease the star's brightness.\nStratospheric Observatory for Infrared Astronomy (SOFIA) was a joint venture of the U.S. and German aerospace agencies, NASA and the German Aerospace Center (DLR) to make an infrared telescope platform that can fly at altitudes high enough to be in the infrared-transparent regime above the water vapor in the Earth's atmosphere. The aircraft was supplied by the U.S., and the infrared telescope by Germany. Modifications of the Boeing 747SP airframe to accommodate the telescope, mission-unique equipment and large external door were made by L-3 Communications Integrated Systems of Waco, Texas.\nThe Interface Region Imaging Spectrograph mission is a partnership with the Lockheed Martin Solar and Astrophysics Laboratory to understand the processes at the boundary between the Sun's chromosphere and corona. This mission is sponsored by the NASA Small Explorer program.\nThe Lunar Atmosphere Dust Environment Explorer (LADEE) mission has been developed by NASA Ames. This successfully launched to the Moon on September 6, 2013.\nIn addition, Ames has played a support role in a number of missions, most notably the \"Mars Pathfinder\" and Mars Exploration Rover missions, where the Ames Intelligent Robotics Laboratory played a key role. NASA Ames was a partner on the Mars \"Phoenix\", a Mars Scout Program mission to send a high-latitude lander to Mars, deployed a robotic arm to dig trenches up to 1.6 feet (one half meter) into the layers of water ice and analyzing the soil composition. Ames is also a partner on the Mars Science Laboratory and its \"Curiosity\" rover, a next generation Mars rover to explore for signs of organics and complex molecules.\nAir traffic control automation research.\nThe Aviation Systems Division conducts research and development in two primary areas: air traffic management, and high-fidelity flight simulation. For air traffic management, researchers are creating and testing concepts to allow for up to three times today's level of aircraft in the national airspace. Automation and its attendant safety consequences are key foundations of the concept development. Historically, the division has developed products that have been implemented for the flying public, such as the Traffic Management Adviser, which is being deployed nationwide. For high-fidelity flight simulation, the division operates the world's largest flight simulator (the Vertical Motion Simulator), a Level-D 747-400 simulator, and a panoramic air traffic control tower simulator. These simulators have been used for a variety of purposes including continued training for Space Shuttle pilots, development of future spacecraft handling qualities, helicopter control system testing, Joint Strike Fighter evaluations, and accident investigations. Personnel in the division have a variety of technical backgrounds, including guidance and control, flight mechanics, flight simulation, and computer science. Customers outside NASA have included the FAA, DOD, DHS, DOT, NTSB, Lockheed Martin, and Boeing.\nThe center's flight simulation and guidance laboratory was listed on the National Register of Historic Places in 2017.\nInformation technology.\nAmes is the home of NASA's large research and development divisions in advanced supercomputing, human factors, and artificial intelligence (Intelligent Systems). These Research &amp; Development organizations support NASA's Exploration efforts, as well as the continued operations of the International Space Station, and the space science and Aeronautics work across NASA. The center also runs and maintains the E Root nameserver of the DNS.\nThe Intelligent Systems Division (Code TI) is NASA's leading R&amp;D Division developing advanced intelligent software and systems for all of NASA Mission Directorates. It provides software expertise for earth science applications, aeronautics, space science missions, International Space Station, and the Crewed Exploration Vehicle (CEV).\nThe first AI in space (Deep Space 1) was developed from Code TI, as is the MAPGEN software that daily plans the activities for the Mars Exploration Rovers, the same core reasoner is used for Ensemble to operate \"Phoenix\" Lander, and the planning system for the International Space Station's solar arrays. Integrated System Health Management for the International Space Station's control moment gyroscopes, collaborative systems with semantic search tools, and robust software engineering round out the scope of Code TI's work.\nThe Human Systems Integration Division \"advances human-centered design and operations of complex aerospace systems through analysis, experimentation, and modeling of human performance and human-automation interaction to make dramatic improvements in safety, efficiency and mission success\". For decades, the Human Systems Integration Division has been on the leading edge of human-centered aerospace research. The Division is home to over 100 researchers, contractors and administrative staff.\nThe NASA Advanced Supercomputing Division at Ames operates several of the agency's most powerful supercomputers, including the petaflop-scale Pleiades, Aitken, and Electra systems. Originally called the Numerical Aerodynamic Simulation Division, the facility has housed more than 40 production and test supercomputers since its construction in 1987, and has served as a leader in high-performance computing, developing technology used across the industry, including the NAS Parallel Benchmarks and the Portable Batch System (PBS) job scheduling software.\nIn September 2009, Ames launched NEBULA as a fast and powerful Cloud Computing Platform to handle NASA's massive data sets that complied with security requirements. This innovative pilot uses open-source components, complies with FISMA and can scale to Government-sized demands while being extremely energy efficient. In July 2010, NASA CTO Chris C. Kemp open sourced Nova, the technology behind the NEBULA Project in collaboration with Rackspace, launching OpenStack. OpenStack has subsequently become one of the largest and fastest growing open source projects in the history of computing, and as of 2014[ [update]] has been included in most major distributions of Linux including Red Hat, Oracle, HP, SUSE, and Canonical.\nImage processing.\nNASA Ames was one of the first locations to conduct research on image processing of satellite-platform aerial photography. Some of the pioneering techniques of contrast enhancement using Fourier analysis were developed at Ames in conjunction with researchers at ESL Inc.\nWind tunnels.\nThe NASA Ames Research Center wind tunnels are known not only for their immense size, but also for their diverse characteristics that enable various kinds of scientific and engineering research.\nARC Unitary Plan Wind Tunnel.\nThe Unitary Plan Wind Tunnel (UPWT) was completed in 1956 at a cost of $27 million under the Unitary Plan Act of 1949. Since its completion, the UPWT facility has been the most heavily used NASA wind tunnel within the NASA Wind Tunnel Fleet. Every major commercial transport and almost every military jet built in the United States over the last 40 years has been tested in this facility. Mercury, Gemini, and Apollo spacecraft, as well as Space Shuttle, were also tested in this tunnel complex.\nNational Full-Scale Aerodynamics Complex (NFAC).\nAmes Research Center also houses the world's largest wind tunnel, part of the National Full-Scale Aerodynamic Complex (NFAC): it is large enough to test full-sized planes, rather than scale models. The complex of wind tunnels was listed on the National Register in 2017.\nThe 40 by 80 foot wind tunnel circuit was originally constructed in the 1940s and is now capable of providing test velocities up to . It is used to support an active research program in aerodynamics, dynamics, model noise, and full-scale aircraft and their components. The aerodynamic characteristics of new configurations are investigated with an emphasis on estimating the accuracy of computational methods. The tunnel is also used to investigate the aeromechanical stability boundaries of advanced rotorcraft and rotor-fuselage interactions. Stability and control derivatives are also determined, including the static and dynamic characteristics of new aircraft configurations. The acoustic characteristics of most of the full-scale vehicles are also determined, as well as acoustic research aimed at discovering and reducing aerodynamic sources of noise. In addition to the normal data gathering methods (e.g., balance system, pressure measuring transducers, and temperature sensing thermocouples), state-of-the-art, non-intrusive instrumentation (e.g., laser velocimeters and shadowgraphs) are available to help determine flow direction and velocity in and around the lifting surfaces of aircraft. The 40 by 80 Foot Wind Tunnel is primarily used for determining the low- and medium-speed aerodynamic characteristics of high-performance aircraft, rotorcraft, and fixed wing, powered-lift V/STOL aircraft.\nThe 80 by 120 Foot Wind Tunnel is the world's largest wind tunnel test section. This open circuit leg was added and a new fan drive system was installed in the 1980s. It is currently capable of air speeds up to . This section is used in similar ways to the 40 by 80 foot section, but it is capable of testing larger aircraft, albeit at slower speeds. Some of the test programs that have come through the 80 by 120 Foot include: F-18 High Angle of Attack Vehicle, DARPA/Lockheed Common Affordable Lightweight Fighter, XV-15 Tilt Rotor, and Advance Recovery System Parafoil. The 80 by 120 foot test section is capable of testing a full size Boeing 737.\nAlthough decommissioned by NASA in 2003, the NFAC is now being operated by the United States Air Force as a satellite facility of the Arnold Engineering Development Complex (AEDC).\nArc Jet Complex.\nThe Ames Arc Jet Complex is an advanced thermophysics facility where sustained hypersonic- and hyperthermal testing of vehicular thermoprotective systems takes place under a variety of simulated flight- and re-entry conditions. Of its seven available test bays, four currently contain Arc Jet units of differing configurations. These are the Aerodynamic Heating Facility (AHF), the Turbulent Flow Duct (TFD), the Panel Test Facility (PTF), and the Interaction Heating Facility (IHF). The support equipment includes two D.C. power supplies, a steam ejector-driven vacuum system, a water-cooling system, high-pressure gas systems, data acquisition system, and other auxiliary systems.\nThe largest power supply is capable of delivering 75 megawatts (MW) over 30 minutes or 150 MW over 15 seconds, which, coupled with a high-volume 5-stage steam ejector vacuum-pumping system, allows Ames to match high-altitude atmospheric conditions with large samples. The Thermo-Physics Facilities Branch operates four arc jet facilities. The Interaction Heating Facility (IHF), with an available power of over 60-MW, is one of the highest-power arc jets available. It is a very flexible facility, capable of long run times of up to one hour, and able to test large samples in both a stagnation and flat plate configuration. The Panel Test Facility (PTF) uses a unique semielliptic nozzle for testing panel sections. Powered by a 20-MW arc heater, the PTF can perform tests on samples for up to 20 minutes. The Turbulent Flow Duct provides supersonic, turbulent high temperature air flows over flat surfaces. The TFD is powered by a 20-MW H\u00fcls arc heater and can test samples in size. The Aerodynamic Heating Facility (AHF) has similar characteristics to the IHF arc heater, offering a wide range of operating conditions, sample sizes and extended test times. A cold-air-mixing plenum allows for simulations of ascent or high-speed flight conditions. Catalycity studies using air or nitrogen can be performed in this flexible rig. A 5-arm model support system allows the user to maximize testing efficiency. The AHF can be configured with either a H\u00fcls or segmented arc heater, up to 20-MW. 1 MW is enough power to supply 750 homes.\nThe Arc Jet Complex was listed on the National Register in 2017.\nRange complex.\nAmes Vertical Gun Range.\nThe Ames Vertical Gun Range (AVGR) was designed to conduct scientific studies of lunar impact processes in support of the Apollo missions. In 1979, it was established as a National Facility, funded through the Planetary Geology and Geophysics Program. In 1995, increased scientific needs across various disciplines resulted in joint core funding by three different science programs at NASA Headquarters (Planetary Geology and Geophysics, Exobiology, and Solar System Origins). In addition, the AVGR provides programmatic support for various proposed and ongoing planetary missions (e.g. Stardust, Deep Impact).\nUsing its 0.30 cal light-gas gun and powder gun, the AVGR can launch projectiles to velocities ranging from . By varying the gun's angle of elevation with respect to the target vacuum chamber, impact angles from 0\u00b0 to 90\u00b0 relative to the gravitational vector are possible. This unique feature is extremely important in the study of crater formation processes.\nThe target chamber is approximately in diameter and height and can accommodate a wide variety of targets and mounting fixtures. It can maintain vacuum levels below , or can be back filled with various gases to simulate different planetary atmospheres. Impact events are typically recorded with high-speed video/film, or Particle Image Velocimetry (PIV).\nHypervelocity Free-Flight Range.\nThe Hypervelocity Free-Flight (HFF) Range currently comprises two active facilities: the Aerodynamic Facility (HFFAF) and the Gun Development Facility (HFFGDF). The HFFAF is a combined Ballistic Range and Shock-tube Driven Wind Tunnel. Its primary purpose is to examine the aerodynamic characteristics and flow-field structural details of free-flying aeroballistic models.\nThe HFFAF has a test section equipped with 16 shadowgraph-imaging stations. Each station can be used to capture an orthogonal pair of images of a hypervelocity model in flight. These images, combined with the recorded flight time history, can be used to obtain critical aerodynamic parameters such as lift, drag, static and dynamic stability, flow characteristics, and pitching moment coefficients. For very high Mach number (M &gt; 25) simulations, models can be launched into a counter-flowing gas stream generated by the shock tube. The facility can also be configured for hypervelocity impact testing and has an aerothermodynamic capability as well. The HFFAF is currently configured to operate the light-gas gun in support of continuing thermal imaging and transition research for NASA's hypersonics program.\nThe HFFGDF is used for gun performance enhancement studies, and occasional impact testing. The Facility uses the same arsenal of light-gas and powder guns as the HFFAF to accelerate particles that range in size from diameter to velocities ranging from . Most of the research effort to date has centered on Earth atmosphere entry configurations (Mercury, Gemini, Apollo, and Shuttle), planetary entry designs (Viking, Pioneer Venus, \"Galileo\" and MSL), and aerobraking (AFE) configurations. The facility has also been used for scramjet propulsion studies (National Aerospace Plane (NASP)) and meteoroid/orbital debris impact studies (Space Station and RLV). In 2004, the facility was utilized for foam-debris dynamics testing in support of the Return To Flight effort. As of March 2007, the GDF has been reconfigured to operate a cold gas gun for subsonic CEV capsule aerodynamics.\nElectric Arc Shock Tube.\nThe Electric Arc Shock Tube (EAST) Facility is used to investigate the effects of radiation and ionization that occur during very high velocity atmospheric entries. In addition, the EAST can also provide air-blast simulations requiring the strongest possible shock generation in air at an initial pressure loading of or greater. The facility has three separate driver configurations, to meet a range of test requirements: the driver can be connected to a diaphragm station of either a or a shock tube, and the high-pressure shock tube can also drive a shock tunnel. Energy for the drivers is supplied by a 1.25-MJ-capacitor storage system.\nList of center directors.\nThe following persons had served as the Ames Research Center director:\nUnited States Geological Survey (USGS).\nIn September 2016, the United States Geological Survey (USGS) announced plans to relocate its West Coast science center from nearby Menlo Park to the Ames Research Center at Moffett Field. The relocation is expected to take five years and will begin in 2017 with 175 of the USGS employees moving to Moffett. The relocation is designed to save money on the $7.5 million annual rent the USGS pays for its Menlo Park campus. The land in Menlo Park is owned by the General Services Administration, which is required by federal law to charge market-rate rent.\nEducation.\nNASA Ames Visitor Center.\nThe NASA Experience exhibit at the Chabot Space and Science Center serves as the visitor center for NASA's Ames Research Center. The NASA Experience provides a dynamic and interactive space for the public to learn about local contributions to space exploration across the years. From models of spacecraft and genuine spacesuits from as early as the Mercury and Gemini missions to artifacts related to NASA's upcoming Artemis missions, the NASA Ames Visitor Center gives visitors access to over 80 years of Ames history and a look into current and future projects. Ames' expertise in wind tunnel testing, rover design and testing, space robotics, supercomputing, and more is on display. The exhibit was opened on November 12, 2021.\nNASA Ames Exploration Center.\nThe NASA Ames Exploration Center is a science museum and education center for NASA. There are displays and interactive exhibits about NASA technology, missions and space exploration. A Moon rock, meteorite, and other geologic samples are on display. The theater shows movies with footage from NASA's explorations of Mars and the planets, and about the contributions of the scientists at Ames. This facility is currently closed.\nRobotics Alliance Project.\nIn 1999, Mark Le\u00f3n developed NASA's Robotics Education Project\u00a0\u2014 now called the Robotics Alliance Project\u00a0\u2014 under his mentor Dave Lavery, which has reached over 100,000 students nationwide using FIRST robotics and BOTBALL robotics competitions. The Project's FIRST branch originally comprised FRC http://, an all-male team from Bellarmine High School in San Jose, California. In 2006, http://, an all-female team, was founded in collaboration with the Girl Scouts. In 2012, http:// of Mountain View High School joined the Project, though the team continues to operate at their school. All three teams are highly decorated. All three have won Regional competitions, two have won the FIRST Championship, two have won the Regional Chairman's Award, and one is a Hall of Fame team. The three teams are collectively referred to as \"House teams\".\nThe mission of the project is \"To create a human, technical, and programmatic resource of robotics capabilities to enable the implementation of future robotic space exploration missions.\"\nPublic-private partnerships.\nThe federal government has re-tasked portions of the facility and human resources to http:// private sector industry, research, and education.\nHP became the first corporate affiliate of a new Bio-Info-Nano Research and Development Institute (BIN-RDI); a collaborative venture established by the University of California Santa Cruz and NASA, based at Ames. The Bio|Info|Nano R&amp;D Institute is dedicated to creating scientific breakthroughs by the convergence of biotechnology, information technology, and nanotechnology.\nSingularity University hosts its leadership and educational program at the facility. The Organ Preservation Alliance http:// is also headquartered there; the Alliance is a nonprofit organization that works in partnership with the Methuselah Foundation's New Organ Prize \"to catalyze breakthroughs on the remaining obstacles towards the long-term storage of organs\" to overcome the drastic unmet medical need for viable organs for transplantation. Kleenspeed Technologies is headquartered there.\nGoogle.\nOn September 28, 2005, Google and Ames Research Center disclosed details to a long-term research partnership. In addition to pooling engineering talent, Google planned to build a facility on the ARC campus. One of the projects between Ames, Google, and Carnegie Mellon University is the Gigapan Project \u2013 a robotic platform for creating, sharing, and annotating terrestrial gigapixel images. The Planetary Content Project seeks to integrate and improve the data that Google uses for its Google Moon and Google Mars projects. On 4\u00a0June 2008, Google announced it had leased from NASA, at Moffett Field, for use as office space and employee housing.\nConstruction of the new Google project which is near Google's Googleplex headquarters began in 2013 and has a target opening date in 2015. It is called \"Bay View\" as it overlooks San Francisco Bay.\nIn May\u00a02013, Google announced that it was launching the Quantum Artificial Intelligence Lab, to be hosted by ARC. The lab will house a 512\u00a0qubit quantum computer from D-Wave Systems, and the Universities Space Research Association (USRA) will invite researchers from around the world to share time on it. The goal being to study how quantum computing might advance machine learning.\nAnnounced on November 10, 2014, Planetary Ventures LLC (a Google subsidiary) will lease the Moffett Federal Airfield from NASA Ames, a site of about 1,000\u00a0acres formerly costing the agency $6.3\u00a0million annually in maintenance and operation costs. The lease includes the restoration of the site's historic landmark Hangar One, as well as hangars Two and Three. The lease went into effect in March\u00a02015, and spans 60\u00a0years.\nLiving and working at Ames.\nAn official NASA ID is required to enter Ames.\nIn support of families working at NASA Ames Research Center, the Ames Child Care Center(ACCC) was opened in 1986. The center's goal is to serve the children of NASA employees, civil servants, contractors, and military employees working at Ames Research Center and Moffett Federal Air Field. The ACCC moved to an new on-site location in 2002 as a result of additional funding from NASA and private donors. In 2005, the ACCC opened to the general public, though at increased tuition rates compared to ACCC affiliates.\nThere are myriad activities both inside the research center and around the base for full-time workers and interns alike. Portions of a fitness trail remain inside the base (also called a \"Parcourse trail\"), Sections of it are now inaccessible due to changes in base layout since it was installed.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "47481", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=47481", "title": "Aquifer", "text": "Underground water-bearing rock\nAn aquifer is an underground layer of water-bearing material consisting of permeable or fractured rock, or of unconsolidated materials (gravel, sand, or silt). Aquifers vary greatly in their characteristics. The study of water flow in aquifers and the characterization of aquifers is called \"hydrogeology\". Related concepts include aquitard, a bed of low permeability along an aquifer, and aquiclude (or \"aquifuge\"), a solid and impermeable region underlying or overlying an aquifer, the pressure of which could lead to the formation of a confined aquifer. Aquifers can be classified as saturated versus unsaturated; aquifers versus aquitards; confined versus unconfined; isotropic versus anisotropic; porous, karst, or fractured; and transboundary aquifer.\nGroundwater from aquifers is sustainably harvested by humans through the use of qanats leading to a well. This groundwater is mainly used for agricultral purposes but is used for other reasons such as home use, industrial use and is sometimes used as a source of renewable energy. Groundwater is a major source of fresh water for many regions, although it can present various challenges for the environment, such as overdrafting (extracting groundwater beyond the equilibrium yield of the aquifer), groundwater-related subsidence of land, and the salinization or pollution of the groundwater.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nProperties.\nDepth.\nAquifers occur from near-surface to deeper than . Those closer to the surface are not only more likely to be used for water supply and irrigation, but are also more likely to be replenished by local rainfall.Although aquifers are sometimes characterized as \"underground rivers or lakes,\" they are actually porous rock saturated with water.\nMany desert areas have limestone hills or mountains within them or close to them that can be exploited as groundwater resources. Part of the Atlas Mountains in North Africa, the Lebanon and Anti-Lebanon ranges between Syria and Lebanon, the Jebel Akhdar in Oman, parts of the Sierra Nevada and neighboring ranges in the United States' Southwest, have shallow aquifers that are exploited for their water. Overexploitation can lead to the exceeding of the practical sustained yield; i.e., more water is taken out than can be replenished.\nAlong the coastlines of certain countries, such as Libya and Israel, increased water usage associated with population growth has caused a lowering of the water table and the subsequent contamination of the groundwater with saltwater from the sea.\nIn 2013 large freshwater aquifers were discovered under continental shelves of Australia, China, North America and South Africa. They contain an estimated half a million cubic kilometers of \"low salinity\" water that could be economically processed into potable water. The reserves formed when ocean levels were lower and rainwater made its way into the ground in land areas that were not submerged until the ice age ended 20,000 years ago. The volume is estimated to be 100 times the amount of water extracted from other aquifers since 1900.\nClassification.\nAn \"aquitard\" is a zone within the Earth that restricts the flow of groundwater from one aquifer to another. An aquitard can sometimes, if completely impermeable, be called an \"aquiclude\" or \"aquifuge\". Aquitards are composed of layers of either clay or non-porous rock with low hydraulic conductivity.\nSaturated versus unsaturated.\nGroundwater can be found at nearly every point in the Earth's shallow subsurface to some degree, although aquifers do not necessarily contain fresh water. The Earth's crust can be divided into two regions: the \"saturated zone\" or \"phreatic zone\" (e.g., aquifers, aquitards, etc.), where all available spaces are filled with water, and the \"unsaturated zone\" (also called the vadose zone), where there are still pockets of air that contain some water, but can be filled with more water.\n\"Saturated\" means the pressure head of the water is greater than atmospheric pressure (it has a gauge pressure &gt; 0). The definition of the water table is the surface where the pressure head is equal to atmospheric pressure (where gauge pressure = 0).\n\"Unsaturated\" conditions occur above the water table where the pressure head is negative (absolute pressure can never be negative, but gauge pressure can) and the water that incompletely fills the pores of the aquifer material is under suction. The water content in the unsaturated zone is held in place by surface adhesive forces and it rises above the water table (the zero-gauge-pressure isobar) by capillary action to saturate a small zone above the phreatic surface (the capillary fringe) at less than atmospheric pressure. This is termed tension saturation and is not the same as saturation on a water-content basis. Water content in a capillary fringe decreases with increasing distance from the phreatic surface. The capillary head depends on soil pore size. In sandy soils with larger pores, the head will be less than in clay soils with very small pores. The normal capillary rise in a clayey soil is less than but can range between .\nThe capillary rise of water in a small-diameter tube involves the same physical process. The water table is the level to which water will rise in a large-diameter pipe (e.g., a well) that goes down into the aquifer and is open to the atmosphere.\nAquifers versus aquitards.\nAquifers are typically saturated regions of the subsurface that produce an economically feasible quantity of water to a well or spring (e.g., sand and gravel or fractured bedrock often make good aquifer materials).\nAn aquitard is a zone within the Earth that restricts the flow of groundwater from one aquifer to another. A completely impermeable aquitard is called an \"aquiclude\" or \"aquifuge\". Aquitards contain layers of either clay or non-porous rock with low hydraulic conductivity.\nIn mountainous areas (or near rivers in mountainous areas), the main aquifers are typically unconsolidated alluvium, composed of mostly horizontal layers of materials deposited by water processes (rivers and streams), which in cross-section (looking at a two-dimensional slice of the aquifer) appear to be layers of alternating coarse and fine materials. Coarse materials, because of the high energy needed to move them, tend to be found nearer the source (mountain fronts or rivers), whereas the fine-grained material will make it farther from the source (to the flatter parts of the basin or overbank areas\u2014sometimes called the pressure area). Since there are less fine-grained deposits near the source, this is a place where aquifers are often unconfined (sometimes called the forebay area), or in hydraulic communication with the land surface.\nConfined versus unconfined.\nAn unconfined aquifer has no impermeable barrier immediately above it, such that the water level can rise in response to recharge. A confined aquifer has an overlying impermeable barrier that prevents the water level in the aquifer from rising any higher. An aquifer in the same geologic unit may be confined in one area and unconfined in another. \"Unconfined\" aquifers are sometimes also called \"water table\" or \"phreatic\" aquifers, because their upper boundary is the water table or phreatic surface (see Biscayne Aquifer). Typically (but not always) the shallowest aquifer at a given location is unconfined, meaning it does not have a confining layer (an aquitard or aquiclude) between it and the surface. The term \"perched\" refers to ground water accumulating above a low-permeability unit or strata, such as a clay layer. This term is generally used to refer to a small local area of ground water that occurs at an elevation higher than a regionally extensive aquifer. The difference between perched and unconfined aquifers is their size (perched is smaller). Confined aquifers are aquifers that are overlain by a confining layer, often made up of clay. The confining layer might offer some protection from surface contamination.\nIf the distinction between confined and unconfined is not clear geologically (i.e., if it is not known if a clear confining layer exists, or if the geology is more complex, e.g., a fractured bedrock aquifer), the value of storativity returned from an aquifer test can be used to determine it (although aquifer tests in unconfined aquifers should be interpreted differently than confined ones). Confined aquifers have very low storativity values (much less than 0.01, and as little as 10-5), which means that the aquifer is storing water using the mechanisms of aquifer matrix expansion and the compressibility of water, which typically are both quite small quantities. Unconfined aquifers have storativities (typically called specific yield) greater than 0.01 (1% of bulk volume); they release water from storage by the mechanism of actually draining the pores of the aquifer, releasing relatively large amounts of water (up to the drainable porosity of the aquifer material, or the minimum volumetric water content).\nIsotropic versus anisotropic.\nIn isotropic aquifers or aquifer layers the hydraulic conductivity (K) is equal for flow in all directions, while in anisotropic conditions it differs, notably in horizontal (Kh) and vertical (Kv) sense.\nSemi-confined aquifers with one or more aquitards work as an anisotropic system, even when the separate layers are isotropic, because the compound Kh and Kv values are different (see hydraulic transmissivity and hydraulic resistance).\nWhen calculating flow to drains or flow to wells in an aquifer, the anisotropy is to be taken into account lest the resulting design of the drainage system may be faulty.\nPorous, karst, or fractured.\nTo properly manage an aquifer its properties must be understood. Many properties must be known to predict how an aquifer will respond to rainfall, drought, pumping, and contamination. Considerations include where and how much water enters the groundwater from rainfall and snowmelt, how fast and in what direction the groundwater travels, and how much water leaves the ground as springs. Computer models can be used to test how accurately the understanding of the aquifer properties matches the actual aquifer performance. Environmental regulations require sites with potential sources of contamination to demonstrate that the hydrology has been characterized.\nPorous.\nPorous aquifers typically occur in sand and sandstone. Their properties depend on the depositional sedimentary environment and later natural cementation of the sand grains. The environment where a sand body was deposited controls the orientation of the sand grains, the horizontal and vertical variations, and the distribution of shale layers. Even thin shale layers are important barriers to groundwater flow. All these factors affect the porosity and permeability of sandy aquifers.\nSandy deposits formed in shallow marine environments and in windblown sand dune environments have moderate to high permeability while sandy deposits formed in river environments have low to moderate permeability. Rainfall and snowmelt enter the groundwater where the aquifer is near the surface. Groundwater flow directions can be determined from potentiometric surface maps of water levels in wells and springs. Aquifer tests and well tests can be used with Darcy's law flow equations to determine the ability of a porous aquifer to convey water.\nAnalyzing this type of information over an area gives an indication how much water can be pumped without overdrafting and how contamination will travel. In porous aquifers groundwater flows as slow seepage in pores between sand grains. A groundwater flow rate of 1 foot per day (0.3 m/d) is considered to be a high rate for porous aquifers, as illustrated by the water slowly seeping from sandstone in the accompanying image to the left.\nPorosity is important, but, \"alone\", it does not determine a rock's ability to act as an aquifer. Areas of the Deccan Traps, a basaltic lava formation in west-central India, are examples of rock formations with high porosity but low permeability, making them poor aquifers. Similarly, the micro-porous (Upper Cretaceous) Chalk Group of south east England, although having a reasonably high porosity, has a low grain-to-grain permeability, with its good water-yielding characteristics mostly due to micro-fracturing and fissuring.\nKarst.\nKarst aquifers typically develop in limestone. Surface water containing natural carbonic acid moves down into small fissures in limestone. This carbonic acid gradually dissolves limestone thereby enlarging the fissures. The enlarged fissures allow a larger quantity of water to enter which leads to a progressive enlargement of openings. Abundant small openings store a large quantity of water. The larger openings form a conduit system that drains the aquifer to springs.\nCharacterization of karst aquifers requires field exploration to locate sinkholes, swallets, sinking streams, and springs in addition to studying geological maps. Conventional hydrogeologic methods such as aquifer tests and potentiometric mapping are insufficient to characterize the complexity of karst aquifers. These conventional investigation methods need to be supplemented with dye traces, measurement of spring discharges, and analysis of water chemistry. U.S. Geological Survey dye tracing has determined that conventional groundwater models that assume a uniform distribution of porosity are not applicable for karst aquifers.\nLinear alignment of surface features such as straight stream segments and sinkholes develop along fracture traces. Locating a well in a fracture trace or intersection of fracture traces increases the likelihood to encounter good water production. Voids in karst aquifers can be large enough to cause destructive collapse or subsidence of the ground surface that can initiate a catastrophic release of contaminants. Groundwater flow rate in karst aquifers is much more rapid than in porous aquifers as shown in the accompanying image to the left. For example, in the Barton Springs Edwards aquifer, dye traces measured the karst groundwater flow rates from 0.5 to 7 miles per day (0.8 to 11.3\u00a0km/d). The rapid groundwater flow rates make karst aquifers much more sensitive to groundwater contamination than porous aquifers.\nIn the extreme case, groundwater may exist in \"underground rivers\" (e.g., caves underlying karst topography).\nFractured.\nIf a rock unit of low porosity is highly fractured, it can also make a good aquifer (via fissure flow), provided the rock has a hydraulic conductivity sufficient to facilitate movement of water.\nUndersea.\n2015 remote sensing and 2025 direct sampling discovered an undersea reservoir of fresh water off the coast of Nantucket, which may strech from Maine to New Jersey and represent one of the largest aquifers in the United States. The aquifer may represent glacial melt that moved from dry land to undersea due to long-term sea level rise caused by the Holocene glacial retreat, or it may be connected to land and get recharged by rainwater.\nHuman use of groundwater.\nChallenges for using groundwater include: overdrafting (extracting groundwater beyond the equilibrium yield of the aquifer), groundwater-related subsidence of land, groundwater becoming saline, and groundwater pollution.\nBy country or continent.\nAfrica.\nAquifer depletion is a problem in some areas, especially in northern Africa, where one example is the Great Manmade River project of Libya. However, new methods of groundwater management such as artificial recharge and injection of surface waters during seasonal wet periods has extended the life of many freshwater aquifers, especially in the United States.\nAustralia.\nThe Great Artesian Basin situated in Australia is arguably the largest groundwater aquifer in the world (over ). It plays a large part in water supplies for Queensland, some remote parts of South Australia and most of the Northern Territorry.\nCanada.\nDiscontinuous sand bodies at the base of the McMurray Formation in the Athabasca Oil Sands region of northeastern Alberta, Canada, are commonly referred to as the Basal Water Sand (BWS) aquifers. Saturated with water, they are confined beneath impermeable bitumen-saturated sands that are exploited to recover bitumen for synthetic crude oil production. Where they are deep-lying and recharge occurs from underlying Devonian formations they are saline, and where they are shallow and recharged by surface water they are non-saline. The BWS typically pose problems for the recovery of bitumen, whether by open-pit mining or by \"in situ\" methods such as steam-assisted gravity drainage (SAGD), and in some areas they are targets for waste-water injection.\nSouth America.\nThe Guarani Aquifer, located beneath the surface of Argentina, Brazil, Paraguay, and Uruguay, is one of the world's largest aquifer systems and is an important source of fresh water. Named after the Guarani people, it covers , with a volume of about , a thickness of between and a maximum depth of about .\nUnited States.\nThe Ogallala Aquifer of the central United States is one of the world's great aquifers, but in places it is being rapidly depleted by growing municipal use, and continuing agricultural use. This huge aquifer, which underlies portions of eight states, contains primarily fossil water from the time of the last glaciation. Annual recharge, in the more arid parts of the aquifer, is estimated to total only about 10 percent of annual withdrawals. According to a 2013 report by the United States Geological Survey (USGS), the depletion between 2001 and 2008, inclusive, is about 32 percent of the cumulative depletion during the entire 20th century.\nIn the United States, the biggest users of water from aquifers include agricultural irrigation and oil and coal extraction. \"Cumulative total groundwater depletion in the United States accelerated in the late 1940s and continued at an almost steady linear rate through the end of the century. In addition to widely recognized environmental consequences, groundwater depletion also adversely impacts the long-term sustainability of groundwater supplies to help meet the Nation\u2019s water needs.\"\nAn example of a significant and sustainable carbonate aquifer is the Edwards Aquifer in central Texas. This carbonate aquifer has historically been providing high quality water for nearly 2 million people, and even today, is full because of tremendous recharge from a number of area streams, rivers and lakes. The primary risk to this resource is human development over the recharge areas.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47483", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=47483", "title": "Volcanoe", "text": ""}
{"id": "47484", "revid": "635492", "url": "https://en.wikipedia.org/wiki?curid=47484", "title": "Atmospheric pressure", "text": "Static pressure exerted by the weight of the Earth's atmosphere\nAtmospheric pressure, also known as air pressure or barometric pressure (after the barometer), is the pressure within the atmosphere of Earth. The standard atmosphere (symbol: atm) is a unit of pressure defined as , which is equivalent to 1,013.25 millibars, 760mm Hg, 29.9212inchesHg, or 14.696psi. The atm unit is roughly equivalent to the mean sea-level atmospheric pressure on Earth; that is, the Earth's atmospheric pressure at sea level is approximately 1 atm. \nIn most circumstances, atmospheric pressure is closely approximated by the hydrostatic pressure caused by the weight of air above the measurement point. As elevation increases, there is less overlying atmospheric mass, so atmospheric pressure decreases with increasing elevation. Because the atmosphere is thin relative to the Earth's radius\u2014especially the dense atmospheric layer at low altitudes\u2014the Earth's gravitational acceleration as a function of altitude can be approximated as constant and contributes little to this fall-off. Pressure measures force per unit area, with SI units of pascals (1 pascal = 1 newton per square metre, 1N/m2). On average, a column of air with a cross-sectional area of 1 square centimetre (cm2), measured from the mean (average) sea level to the top of Earth's atmosphere, has a mass of about 1.03 kilogram and exerts a force or \"weight\" of about 10.1 newtons, resulting in a pressure of 10.1 N/cm2 or 101kN/m2 (101 kilopascals, kPa). A column of air with a cross-sectional area of 1in2 would have a weight of about 14.7lbf, resulting in a pressure of 14.7lbf/in2.\nMechanism.\nAtmospheric pressure is caused by the gravitational attraction of the planet on the atmospheric gases above the surface and is a function of the mass of the planet, the radius of the surface, and the amount and composition of the gases and their vertical distribution in the atmosphere. It is modified by the planetary rotation and local effects such as wind velocity, density variations due to temperature and variations in composition.\nMean sea-level pressure.\nThe \"mean sea-level pressure\" (MSLP) is the atmospheric pressure at mean sea level. This is the atmospheric pressure normally given in weather reports via meteorologists on radio, television, and newspapers or on the Internet.\nThe \"altimeter setting\" in aviation is an atmospheric pressure adjustment.\nAverage \"sea-level pressure\" is . In aviation weather reports (METAR), QNH is transmitted around the world in hectopascals or millibars (1 hectopascal = 1 millibar). In the United States, Canada, and Japan altimeter setting is reported in inches of mercury (to two decimal places). The United States and Canada also report \"sea-level pressure\" SLP, which is adjusted to sea level by a different method, in the remarks section, not in the internationally transmitted part of the code, in hectopascals or millibars. However, in Canada's public weather reports, sea level pressure is instead reported in kilopascals. In the US weather code remarks, three digits are all that are transmitted; decimal points and the one or two most significant digits are omitted: is transmitted as 132; is transmitted as 000; 998.7hPa is transmitted as 987; etc. A system transmitting the last three digits transmits the same code (800) for 1080.0 hPa as for 980.0 hPa.\nThe highest \"sea-level pressure\" on Earth occurs in Siberia, where the Siberian High often attains a \"sea-level pressure\" above , with record highs close to . The lowest measurable \"sea-level pressure\" is found at the centres of tropical cyclones and tornadoes, with a record low of . \nSurface pressure.\n\"Surface pressure\" is the atmospheric pressure at a location on Earth's surface (terrain and oceans). It is directly proportional to the mass of air over that location.\nFor numerical reasons, atmospheric models such as general circulation models (GCMs) usually predict the nondimensional \"logarithm of surface pressure\".\nThe average value of surface pressure on Earth is 985 hPa. This is in contrast to mean sea-level pressure, which involves the extrapolation of pressure to sea level for locations above or below sea level. The average pressure at mean sea level (MSL) in the International Standard Atmosphere (ISA) is 1,013.25 hPa, or 1 atmosphere (atm), or 29.92 inches of mercury.\nPressure (P), mass (m), and acceleration due to gravity (g) are related by P = F/A = (m*g)/A, where A is the surface area. Atmospheric pressure is thus proportional to the weight per unit area of the atmospheric mass above that location.\nAltitude variation.\nPressure on Earth varies with the altitude of the surface, so air pressure on mountains is usually lower than air pressure at sea level. Pressure varies smoothly from the Earth's surface to the top of the mesosphere. Although the pressure changes with the weather, NASA has averaged the conditions for all parts of the earth year-round. As altitude increases, atmospheric pressure decreases. One can calculate the atmospheric pressure at a given altitude. Temperature and humidity also affect the atmospheric pressure. Pressure is proportional to temperature and inversely related to humidity, and both of these are necessary to compute an accurate figure. The graph &lt;templatestyles src=\"If mobile/styles.css\" /&gt;on the rightabove was developed for a temperature of 15\u00a0\u00b0C and a relative humidity of 0%.\nAt low altitudes above sea level, the pressure decreases by about for every 100 metres. For higher altitudes within the troposphere, the following equation (the barometric formula) relates atmospheric pressure \"p\" to altitude \"h\":\nformula_1\nThe values in these equations are:\nLocal variation.\nAtmospheric pressure varies widely on Earth, and these changes are important in studying weather and climate. Atmospheric pressure shows a diurnal or semidiurnal (twice-daily) cycle caused by global atmospheric tides. This effect is strongest in tropical zones, with an amplitude of a few hectopascals, and almost zero in polar areas. These variations have two superimposed cycles, a circadian (24\u00a0h) cycle, and a semi-circadian (12\u00a0h) cycle.\nRecords.\nThe highest adjusted-to-sea level barometric pressure ever recorded on Earth (above 750 meters) was measured in Tosontsengel, Mongolia on 19 December 2001. The highest adjusted-to-sea level barometric pressure ever recorded (below 750 meters) was at Agata in Evenk Autonomous Okrug, Russia (66\u00b053'N, 93\u00b028'E, elevation: ) on 31 December 1968 of . The discrimination is due to the problematic assumptions (assuming a standard lapse rate) associated with reduction of sea level from high elevations.\nThe Dead Sea, the lowest place on Earth at below sea level, has a correspondingly high typical atmospheric pressure of 1,065hPa. A below-sea-level surface pressure record of was set on 21 February 1961.\nThe lowest non-tornadic atmospheric pressure ever measured was 870\u00a0hPa (0.858\u00a0atm; 25.69\u00a0inHg), set on 12 October 1979, during Typhoon Tip in the western Pacific Ocean. The measurement was based on an instrumental observation made from a reconnaissance aircraft.\nMeasurement based on the depth of water.\nOne atmosphere () is also the pressure caused by the weight of a column of freshwater of approximately . Thus, a diver 10.3\u00a0m under water experiences a pressure of about 2 atmospheres (1\u00a0atm of air plus 1\u00a0atm of water). Conversely, 10.3\u00a0m is the maximum height to which water can be raised using suction under standard atmospheric conditions.\nLow pressures, such as natural gas lines, are sometimes specified in inches of water, typically written as \"w.c.\" (water column) gauge or \"w.g.\" (inches water) gauge. A typical gas-using residential appliance in the US is rated for a maximum of , which is approximately 14\u00a0w.g. Similar metric units with a wide variety of names and notation based on millimetres, centimetres or metres are now less commonly used.\nBoiling point of liquids.\nPure water boils at at Earth's standard atmospheric pressure. The boiling point is the temperature at which the vapour pressure is equal to the atmospheric pressure around the liquid. Because of this, the boiling point of liquids is lower at lower pressure and higher at higher pressure. Cooking at high elevations, therefore, requires adjustments to recipes or pressure cooking. A rough approximation of elevation can be obtained by measuring the temperature at which water boils; in the mid-19th century, this method was used by explorers. Conversely, if one wishes to evaporate a liquid at a lower temperature, for example in distillation, the atmospheric pressure may be lowered by using a vacuum pump, as in a rotary evaporator.\nMeasurement and maps.\nAn important application of the knowledge that atmospheric pressure varies directly with altitude was in determining the height of hills and mountains, thanks to reliable pressure measurement devices. In 1774, Nevil Maskelyne was confirming Newton's theory of gravitation at and on Schiehallion mountain in Scotland, and he needed to measure elevations on the mountain's sides accurately. This event is known as the Schiehallion experiment. William Roy, using barometric pressure, was able to confirm Maskelyne's height determinations; the agreement was within one meter (3.28 feet). This method became and continues to be useful for survey work and map making.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47485", "revid": "50348316", "url": "https://en.wikipedia.org/wiki?curid=47485", "title": "King (chess)", "text": "Chess piece\nThe king (\u2654, \u265a) is one of the six types of pieces in the game of chess and represents the most important unit on the board. It may move to any adjacent square that is not controlled by an enemy piece; it may also perform, in tandem with the rook, a special move called \"castling\". If a player's king is threatened with capture, it is said to be \"in check\", and the player must remove or evade the threat of &lt;dfn id=\"\"&gt;capture&lt;/dfn&gt; immediately, such as by moving it away from the attacked square. If this cannot be done, the king is said to be in checkmate, resulting in a loss for that player. A player cannot make any move that places their own king in check. Despite this, the king can become a strong offensive piece in the endgame or, rarely, the middlegame.\nIn algebraic notation, the king is abbreviated by the letter \"K\" among English speakers. The white king starts the game on e1; the black king starts on e8. Unlike all other pieces, each player can have only one king, and the kings are never removed from the board during the game.\n&lt;templatestyles src=\"Template:TOC_left/styles.css\" /&gt;\nPlacement and movement.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nThe white king starts on e1, on the first &lt;dfn id=\"\"&gt;file&lt;/dfn&gt; to the right of the queen from White's perspective. The black king starts on e8, directly across from the white king. Each king starts on a square opposite its own color.\nA king can move one square horizontally, vertically, and diagonally unless the square is already occupied by a friendly piece or the move would place the king in check. If the square is occupied by an undefended enemy piece, the king may capture it, removing it from play. Opposing kings may never occupy adjacent squares (see opposition) to give check, as that would put the moving king in check as well. The king can give discovered check, however, by unblocking a bishop, rook, or queen.\nCastling.\nThe king can make a special move, in conjunction with a rook of the same color, called \"castling\". When castling, the king moves two squares horizontally toward one of its rooks, and that rook is placed on the square over which the king crossed.\nCastling is permissible under the following conditions:\nCastling with the h-file rook is known as \"castling kingside\" or \"short castling\" (denoted 0-0 in algebraic notation), while castling with the a-file rook is known as \"castling queenside\" or \"long castling\" (denoted 0-0-0).\nStatus in games.\nCheck and checkmate.\nA king that is in a square controlled by an enemy piece is said to be \"in check\", and the player in check must immediately respond the situation. There are three possible ways to remove the king from check:\nIf none of the three options are available, the player's king has been \"checkmated\", and the player loses the game.\nIf the king is under attack by two different pieces simultaneously (referred to as double check), it is not possible to capture or block both of them simultaneously so the king must move.\nIn casual games, when placing the opponent's king in check, it is common to announce this by saying \"check\", but this is not required by the rules of chess. In tournament games, it is unusual to announce check; competent players are expected to know when they are in check.\nStalemate.\nA stalemate occurs when a player, on their turn, has no legal moves, and the player's king is not in check.\nIf this happens, the king is said to have been stalemated, and the game ends in a draw. A player who has very little or no chance of winning will often, in order to avoid a loss, try to entice the opponent to inadvertently place the player's king in stalemate (see swindle).\nRole in gameplay.\nIn the opening and middlegame, the king will rarely play an active role in the development of an offensive or defensive position, with the notable exception of a king walk. Instead, it will normally castle and seek safety on the edge of the board behind friendly pawns. In the endgame, however, the king emerges to play an active role as an offensive piece, and can assist in the promotion of the player's remaining pawns.\nIt is not meaningful to assign a value to the king relative to the other pieces, as it cannot be captured or exchanged and must be protected at all costs. In this sense, its value could be considered infinite. As an assessment of the king's capability as an offensive piece in the endgame, it is often considered to be slightly stronger than a bishop or knight. Emanuel Lasker gave it the value of a knight plus a pawn (i.e. four points on the scale of chess piece relative value), though some other theorists evaluate it closer to three points. It is better at defending friendly pawns than the knight is, and it is better at attacking enemy pawns than the bishop is.\nHistory.\nThe king's predecessor is the piece of the same name in shatranj. Like the modern king, it is the most important piece in the game and can move to any neighboring square. However, in shatranj, baring the king is a win unless the opponent can do the same immediately afterward; stalemating the king is a win; and castling does not exist.\nStrategy.\nAlthough the king is the most valuable piece on the board due to its role in determining the outcome of the game, its practical activity varies greatly between the opening, middlegame, and endgame. In the opening and middlegame, the king is generally kept safe through castling and by remaining behind pawn cover. However, in the endgame the king becomes an active and often decisive piece, capable of supporting passed pawns, restricting the opponent\u2019s king, and participating directly in the attack.\nShouldering.\nAnother common endgame technique is \"shouldering\". In this strategy, a king deliberately occupies a path that prevents the opposing king from approaching critical areas of the board, often pushing the rival king further away from the action. Shouldering is frequently decisive in races to support passed pawns.\nCentralisation.\nIn the endgame, the king\u2019s value as a fighting piece increases dramatically. Because there are fewer threats of checkmate, the king can safely advance toward the center of the board. A centralized king controls a wide range of squares, assists in pawn promotion, and exerts influence over both flanks.\nOpposition.\nOne of the most fundamental strategic ideas in king endgames is the \"opposition\". Opposition occurs when two kings stand on the same file, rank, or diagonal with only one square separating them. The player not having the move is said to \u201chave the opposition.\u201d This forces the opposing king to yield ground, and is often the critical factor in achieving promotion of a pawn or preventing the opponent\u2019s advance. \nOutflanking.\nClosely related to opposition is the concept of \"outflanking\". When direct opposition cannot be maintained, a king can maneuver around the opponent\u2019s king to gain access to key squares. Outflanking allows a player to break through an opponent\u2019s defensive setup, especially in pawn endgames, and is often combined with zugzwang to force progress.\nDimensions &amp; Design.\nThe king is traditionally the tallest and most prominent piece in a chess set, serving as a clear visual indicator of its importance. In the Staunton pattern, the international standard for tournament play since 1849, the king typically measures about 3.75 inches (95\u00a0mm) in height in a standard set used on boards with 2.25-inch (57\u00a0mm) squares. Its base diameter is usually about 1.5\u20132 inches (38\u201350\u00a0mm), roughly 40\u201350% of its height, providing stability while maintaining proportional harmony with the board. The maximum width of the king, measured at the widest part of the body or crown, is typically slightly larger than the base, often around 2\u20132.25 inches (50\u201357\u00a0mm). Chess sets vary widely in scale, from miniature travel sets under 1 inch (25\u00a0mm) tall to large exhibition sets exceeding 6 inches (150\u00a0mm), but the king consistently remains the tallest and most easily recognizable piece.\nThe king\u2019s design is generally the most ornate among the chess pieces. In the Staunton design, it is crowned with a stylized cross, distinguishing it from the queen, which bears a coronet. This cross serves both decorative and practical purposes, allowing the piece to be quickly identified during play. Historical designs have varied considerably: in Islamic chess sets, kings were often abstract shapes or inscribed markers, while medieval European sets sometimes depicted kings with elaborate crowns, thrones, or scepters. Modern decorative sets may portray kings with detailed crowns, helmets, or other regal attributes, but the Staunton form remains standard in official play for its clarity and simplicity.\nUnicode.\nUnicode defines three codepoints for a king:\n\u2654 U+2654 White Chess King\n\u265a U+265A Black Chess King\n\ud83e\ude00 U+1FA00 Neutral Chess King\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47486", "revid": "50674919", "url": "https://en.wikipedia.org/wiki?curid=47486", "title": "Atoll", "text": "Ring-shaped coral reef\nAn atoll () is a ring-shaped island, including a coral rim that encircles a lagoon. There may be coral islands or cays on the rim. Atolls are located in warm tropical or subtropical parts of the oceans and seas where corals can develop. Most of the approximately 440 atolls in the world are in the Pacific Ocean.\nTwo different, well-cited models, the subsidence model and the antecedent karst model, have been used to explain the development of atolls. According to Charles Darwin's subsidence model, the formation of an atoll is explained by the sinking of a volcanic island around which a coral fringing reef has formed. Over geologic time, the volcanic island becomes extinct and eroded as it subsides completely beneath the surface of the ocean. As the volcanic island subsides, the coral fringing reef becomes a barrier reef that is detached from the island. Eventually, the reef and the small coral islets on top of it are all that is left of the original island, and a lagoon has taken the place of the former volcano. The lagoon is not the former volcanic crater. For the atoll to persist, the coral reef must be maintained at the sea surface, with coral growth matching any relative change in sea level (sinking of the island or rising oceans).\nAn alternative model for the origin of atolls is called the antecedent karst model. In the antecedent karst model, the first step in the formation of an atoll is the development of a flat top, mound-like coral reef during the subsidence of an oceanic island of either volcanic or nonvolcanic origin below sea level. Then, when relative sea level drops below the level of the flat surface of coral reef, it is exposed to the atmosphere as a flat topped island which is dissolved by rainfall to form limestone karst. Because of hydrologic properties of this karst, the rate of dissolution of the exposed coral is lowest along its rim and the rate of dissolution increases inward to its maximum at the center of the island. As a result, a saucer shaped island with a raised rim forms. When relative sea level submerges the island again, the rim provides a rocky core on which coral grow again to form the islands of an atoll and the flooded bottom of the saucer forms the lagoon within them.\nUsage.\nThe word \"atoll\" comes from the Dhivehi word (, ). Dhivehi is an Indo-Aryan language spoken in the Maldives. The word's first recorded English use was in 1625 as \"atollon\". Charles Darwin coined the term in his monograph, \"The Structure and Distribution of Coral Reefs\". He recognized the word's indigenous origin and defined it as a \"circular group of coral islets\", synonymously with \"lagoon-island\".2\nMore modern definitions of \"atoll\" describe them as \"annular reefs enclosing a lagoon in which there are no promontories other than reefs and islets composed of reef detritus\" or \"in an exclusively morphological sense, [as] a ring-shaped ribbon reef enclosing a lagoon\".\nDistribution and size.\nThere are approximately 440 atolls in the world. Most of the world's atolls are in the Pacific Ocean (with concentrations in the Caroline Islands, the Coral Sea Islands, the Marshall Islands, the Tuamotu Islands, Kiribati, Tokelau, and Tuvalu) and the Indian Ocean (the Chagos Archipelago, Lakshadweep, the atolls of the Maldives, and the Outer Islands of Seychelles). In addition, Indonesia also has several atolls spread across the archipelago, such as in the Thousand Islands, Taka Bonerate Islands, and atolls in the Raja Ampat Islands. The Atlantic Ocean has no large groups of atolls, other than eight atolls east of Nicaragua that belong to the Colombian department of San Andres and Providencia in the Caribbean.\nReef-building corals will thrive only in warm tropical and subtropical waters of oceans and seas, and therefore atolls are found only in the tropics and subtropics. The northernmost atoll in the world is Kure Atoll at 28\u00b025\u2032 N, along with other atolls of the Northwestern Hawaiian Islands. The southernmost atolls in the world are Elizabeth Reef at 29\u00b057\u2032 S, and nearby Middleton Reef at 29\u00b027\u2032 S, in the Tasman Sea, both of which are part of the Coral Sea Islands Territory. The next southerly atoll is Ducie Island in the Pitcairn Islands Group, at 24\u00b041\u2032 S. \nThe atoll closest to the Equator is Aranuka of Kiribati. Its southern tip is just north of the Equator.\nBermuda is sometimes claimed as the \"northernmost atoll\" at a latitude of 32\u00b018\u2032 N. At this latitude, coral reefs would not develop without the warming waters of the Gulf Stream. However, Bermuda is termed a \"pseudo-atoll\" because its general form, while resembling that of an atoll, has a very different origin of formation.\nIn most cases, the land area of an atoll is very small in comparison to the total area. Atoll islands are low lying, with their elevations less than . Measured by total area, Lifou () is the largest raised coral atoll of the world, followed by Rennell Island (). More sources, however, list Kiritimati as the largest atoll in the world in terms of land area. It is also a raised coral atoll ( land area; according to other sources even ), main lagoon, other lagoons (according to other sources total lagoon size).\nThe geological formation known as a reef knoll refers to the elevated remains of an ancient atoll within a limestone region, appearing as a hill. The second largest atoll by dry land area is Aldabra, with . Huvadhu Atoll, situated in the southern region of the Maldives, holds the distinction of being the largest atoll based on the sheer number of islands it comprises, with a total of 255 individual islands.\nFormation.\nIn 1842, Charles Darwin explained the creation of coral atolls in the southern Pacific Ocean based upon observations made during a five-year voyage aboard HMS Beagle from 1831 to 1836. Darwin's explanation suggests that several tropical island types: from high volcanic island, through barrier reef island, to atoll, represented a sequence of gradual subsidence of what started as an oceanic volcano. He reasoned that a fringing coral reef surrounding a volcanic island in the tropical sea will grow upward as the island subsides (sinks), becoming an \"almost atoll\", or barrier reef island, as typified by an island such as Aitutaki in the Cook Islands, and Bora Bora and others in the Society Islands. The fringing reef becomes a barrier reef for the reason that the outer part of the reef maintains itself near sea level through biotic growth, while the inner part of the reef falls behind, becoming a lagoon because conditions are less favorable for the coral and calcareous algae responsible for most reef growth. In time, subsidence carries the old volcano below the ocean surface and the barrier reef remains. At this point, the island has become an atoll.\nAs formulated by J. E. Hoffmeister, F. S. McNeil, E. G. Prudy, and others, the antecedent karst model argues that atolls are Pleistocene features that are the direct result of the interaction between subsidence and preferential karst dissolution that occurred in the interior of flat topped coral reefs during exposure during glacial lowstands of sea level. The elevated rims along an island created by this preferential karst dissolution become the sites of coral growth and islands of atolls when flooded during interglacial highstands.\nThe research of A. W. Droxler, St\u00e9phan J Jorry and others supports the antecedent karst model as they found that the morphology of modern atolls are independent of any influence of an underlying submerged and buried island and are not rooted to an initial fringing reef/barrier reef attached to a slowly subsiding volcanic edifice. In fact, the Neogene reefs underlying the studied modern atolls overlie and completely bury the subsided island are all non-atoll, flat-topped reefs. In fact, they found that atolls did not form doing the subsidence of an island until MIS-11, Mid-Brunhes, long after the many the former islands had been completely submerged and buried by flat topped reefs during the Neogene.\nAtolls are the product of the growth of tropical marine organisms, and so these islands are found only in warm tropical waters. Volcanic islands located beyond the warm water temperature requirements of hermatypic (reef-building) organisms become seamounts as they subside, and are eroded away at the surface. An island that is located where the ocean water temperatures are just sufficiently warm for upward reef growth to keep pace with the rate of subsidence is said to be at the Darwin Point. Islands in colder, more polar regions evolve toward seamounts or guyots; warmer, more equatorial islands evolve toward atolls, for example Kure Atoll. However, ancient atolls during the Mesozoic appear to exhibit different growth and evolution patterns.\nCoral atolls are important as sites where dolomitization of calcite occurs. Several models have been proposed for the dolomitization of calcite and aragonite within them. They are the evaporative, seepage-reflux, mixing-zone, burial, and seawater models. Although the origin of replacement dolomites remains problematic and controversial, it is generally accepted that seawater was the source of magnesium for dolomitization and the fluid in which calcite was dolomitized to form the dolomites found within atolls. Various processes have been invoked to drive large amounts of seawater through an atoll in order for dolomitization to occur.\nInvestigation by the Royal Society of London.\nIn 1896, 1897 and 1898, the Royal Society of London carried out drilling on Funafuti atoll in Tuvalu for the purpose of investigating the formation of coral reefs. They wanted to determine whether traces of shallow water organisms could be found at depth in the coral of Pacific atolls. This investigation followed the work on the structure and distribution of coral reefs conducted by Charles Darwin in the Pacific.\nThe first expedition in 1896 was led by Professor William Johnson Sollas of the University of Oxford. Geologists included Walter George Woolnough and Edgeworth David of the University of Sydney. Professor Edgeworth David led the expedition in 1897. The third expedition in 1898 was led by Alfred Edmund Finckh.\nReferences.\nInline citations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47487", "revid": "40993790", "url": "https://en.wikipedia.org/wiki?curid=47487", "title": "Azimuth", "text": "Horizontal angle from north or other reference cardinal direction\nAn azimuth (; from ) is the horizontal angle from a cardinal direction, most commonly north, in a local or observer-centric spherical coordinate system. \nMathematically, the relative position vector from an observer (origin) to a point of interest is projected perpendicularly onto a reference plane (the horizontal plane); the angle between the projected vector and a reference vector on the reference plane is called the azimuth.\nWhen used as a celestial coordinate, the azimuth is the horizontal direction of a star or other astronomical object in the sky. The star is the point of interest, the reference plane is the local area (e.g. a circular area with a 5\u00a0km radius at sea level) around an observer on Earth's surface, and the reference vector points to true north. The azimuth is the angle between the north vector and the star's vector on the horizontal plane.\nAzimuth is usually measured in degrees (\u00b0), in the positive range 0\u00b0 to 360\u00b0 or in the signed range -180\u00b0 to +180\u00b0. The concept is used in navigation, astronomy, engineering, mapping, mining, and ballistics.\nEtymology.\nThe word \"azimuth\" is used in all European languages today. It originates from medieval Arabic (\"al-sum\u016bt\", pronounced \"as-sum\u016bt\"), meaning \"the directions\" (plural of Arabic \"al-samt\" = \"the direction\"). The Arabic word entered late medieval Latin in an astronomy context and in particular in the use of the Arabic version of the astrolabe astronomy instrument. Its first recorded use in English is in the 1390s in Geoffrey Chaucer's \"A Treatise on the Astrolabe\". The first known record in any Western language is in Spanish in the 1270s in an astronomy book that was largely derived from Arabic sources, the \"Libros del saber de astronom\u00eda\" commissioned by King Alfonso X of Castile.\nIn astronomy.\nIn the horizontal coordinate system, used in celestial navigation, azimuth is one of the two coordinates. The other is \"altitude\", sometimes called elevation above the horizon. \nIt is also used for satellite dish installation (see also: sat finder).\nIn modern astronomy azimuth is nearly always measured from the north.\nIn navigation.\n In land navigation, azimuth is usually denoted alpha, \"\u03b1\", and defined as a horizontal angle measured clockwise from a north base line or \"meridian\". \"Azimuth\" has also been more generally defined as a horizontal angle measured clockwise from any fixed reference plane or easily established base direction line.\nToday, the reference plane for an azimuth is typically true north, measured as a 0\u00b0 azimuth, though other angular units (grad, mil) can be used. Moving clockwise on a 360 degree circle, east has azimuth 90\u00b0, south 180\u00b0, and west 270\u00b0. There are exceptions: some navigation systems use south as the reference vector. Any direction can be the reference vector, as long as it is clearly defined.\nQuite commonly, azimuths or compass bearings are stated in a system in which either north or south can be the zero, and the angle may be measured clockwise or anticlockwise from the zero. For example, a bearing might be described as \"(from) south, (turn) thirty degrees (toward the) east\" (the words in brackets are usually omitted), abbreviated \"S30\u00b0E\", which is the bearing 30 degrees in the eastward direction from south, i.e. the bearing 150 degrees clockwise from north. The reference direction, stated first, is always north or south, and the turning direction, stated last, is east or west. The directions are chosen so that the angle, stated between them, is positive, between zero and 90 degrees. If the bearing happens to be exactly in the direction of one of the cardinal points, a different notation, e.g. \"due east\", is used instead.\nIn geodesy.\nWe are standing at latitude formula_1, longitude zero; we want to find the azimuth from our viewpoint to Point 2 at latitude formula_2, longitude \"L\" (positive eastward). We can get a fair approximation by assuming the Earth is a sphere, in which case the azimuth \"\u03b1\" is given by\nformula_3\nA better approximation assumes the Earth is a slightly-squashed sphere (an \"oblate spheroid\"); \"azimuth\" then has at least two very slightly different meanings. Normal-section azimuth is the angle measured at our viewpoint by a theodolite whose axis is perpendicular to the surface of the spheroid; geodetic azimuth (or geodesic azimuth) is the angle between north and the ellipsoidal geodesic (the shortest path on the surface of the spheroid from our viewpoint to Point 2). The difference is usually negligible: less than 0.03 arc second for distances less than 100\u00a0km.\nNormal-section azimuth can be calculated as follows:\n formula_4\nwhere \"f\" is the flattening and \"e\" the eccentricity for the chosen spheroid (e.g., &lt;templatestyles src=\"Fraction/styles.css\" /&gt;\u2044 for WGS84).\nIf \"\u03c6\"1 = 0 then\n formula_5\nTo calculate the azimuth of the Sun or a star given its declination and hour angle at a specific location, modify the formula for a spherical Earth. Replace \"\u03c6\"2 with declination and longitude difference with hour angle, and change the sign (since the hour angle is positive westward instead of east).\nIn cartography.\nThe cartographical azimuth or grid azimuth (in decimal degrees) can be calculated when the coordinates of 2 points are known in a flat plane (cartographical coordinates):\nformula_6\nRemark that the reference axes are swapped relative to the (counterclockwise) mathematical polar coordinate system and that the azimuth is clockwise relative to the north.\nThis is the reason why the X and Y axis in the above formula are swapped.\nIf the azimuth becomes negative, one can always add 360\u00b0.\nThe formula in radians would be slightly easier:\nformula_7\nNote the swapped formula_8 in contrast to the normal formula_9 atan2 input order.\nThe opposite problem occurs when the coordinates (\"X\"1, \"Y\"1) of one point, the distance \"D\", and the azimuth \"\u03b1\" to another point (\"X\"2, \"Y\"2) are known, one can calculate its coordinates:\nformula_10\nThis is typically used in triangulation and azimuth identification (AzID), especially in radar applications.\nMap projections.\nThere is a wide variety of azimuthal map projections. They all have the property that directions (the azimuths) from a central point are preserved. Some navigation systems use south as the reference plane. However, any direction can serve as the plane of reference, as long as it is clearly defined for everyone using that system.\nRelated coordinates.\nRight ascension.\nIf, instead of measuring from and along the horizon, the angles are measured from and along the celestial equator, the angles are called right ascension if referenced to the Vernal Equinox, or hour angle if referenced to the celestial meridian.\nPolar coordinate.\nIn mathematics, the azimuth angle of a point in cylindrical coordinates or spherical coordinates is the anticlockwise angle between the positive \"x\"-axis and the projection of the vector onto the \"xy\"-plane. A special case of an azimuth angle is the angle in polar coordinates of the component of the vector in the \"xy\"-plane, although this angle is normally measured in radians rather than degrees and denoted by \"\u03b8\" rather than \"\u03c6\".\nOther uses.\nFor magnetic tape drives, \"azimuth\" refers to the angle between the tape head(s) and tape.\nIn sound localization experiments and literature, the \"azimuth\" refers to the angle the sound source makes compared to the imaginary straight line that is drawn from within the head through the area between the eyes.\nAn azimuth thruster in shipbuilding is a propeller that can be rotated horizontally.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47488", "revid": "765510", "url": "https://en.wikipedia.org/wiki?curid=47488", "title": "Barometer", "text": "Scientific instrument used to measure atmospheric pressure\nA barometer is a scientific instrument that is used to measure air pressure in a certain environment. Pressure tendency can forecast short term changes in the weather. Many measurements of air pressure are used within surface weather analysis to help find surface troughs, pressure systems and frontal boundaries.\nBarometers and pressure altimeters (the most basic and common type of altimeter) are essentially the same instrument, but used for different purposes. An altimeter is intended to be used at different altitudes matching the corresponding atmospheric pressure to the altitude, while a barometer is kept at a constant altitude and measures subtle pressure changes caused by weather and elements of weather. The average atmospheric pressure on the Earth's surface varies between 940 and 1040 hPa (mbar). The average atmospheric pressure at sea level is 1013 hPa (mbar).\nEtymology.\nThe word \"barometer\" is derived from the Ancient Greek (), meaning \"weight\", and (), meaning \"measure\".\nHistory.\nEvangelista Torricelli is usually credited with inventing the barometer in 1643,&lt;ref name=\"http://www.islandnet.com/~see/weather/history/barometerhistory1.htm\"&gt;&lt;/ref&gt;&lt;ref name=\"http://www.barometerfair.com/history_of_the_barometer.htm\"&gt;&lt;/ref&gt; although the historian W. E. Knowles Middleton suggests the more likely date is 1644 (when Torricelli first reported his experiments; the 1643 date was only suggested after his death).Gasparo Berti, an Italian mathematician and astronomer, also built a rudimentary water barometer sometime between 1640 and 1644, but it was not a true barometer as it was not intended to move and record variable air pressure. French scientist and philosopher Ren\u00e9 Descartes described the design of an experiment to determine atmospheric pressure as early as 1631, but there is no evidence that he built a working barometer at that time. In 1668, Robert Hooke's marine barometer, made by Henry Hunt, was noticed, and efforts were made to make it sea-worthy.\nBaliani's siphon experiment.\nOn 27 July 1630, Giovanni Battista Baliani wrote a letter to Galileo Galilei explaining an experiment he had made in which a siphon, led over a hill about 21 m high, failed to work. When the end of the siphon was opened in a reservoir, the water level in that limb would sink to about 10 m above the reservoir. Galileo responded with an explanation of the phenomenon: he proposed that it was the power of a vacuum that held the water up, and at a certain height the amount of water simply became too much and the force could not hold any more, like a cord that can support only so much weight. This was a restatement of the theory of \"horror vacui\" (\"nature abhors a vacuum\"), which dates to Aristotle, and which Galileo restated as \"resistenza del vacuo\".\nBerti's vacuum experiment.\nGalileo's ideas, presented in his \"Discorsi\" (\"Two New Sciences\"), reached Rome in December 1638. Physicists Gasparo Berti and father Raffaello Magiotti were excited by these ideas, and decided to seek a better way to attempt to produce a vacuum other than with a siphon. Magiotti devised such an experiment. Four accounts of the experiment exist, all written some years later. No exact date was given, but since \"Two New Sciences\" reached Rome in December 1638, and Berti died before January 2, 1644, science historian W. E. Knowles Middleton places the event to sometime between 1639 and 1643. Present were Berti, Magiotti, Jesuit polymath Athanasius Kircher, and Jesuit physicist Niccol\u00f2 Zucchi.\nIn brief, Berti's experiment consisted of filling with water a long tube that had both ends plugged, then standing the tube in a basin of water. The bottom end of the tube was opened, and water that had been inside of it poured out into the basin. However, only part of the water in the tube flowed out, and the level of the water inside the tube stayed at an exact level, which happened to be , the same height limit Baliani had observed in the siphon. What was most important about this experiment was that the lowering water had left a space above it in the tube which had no intermediate contact with air to fill it up. This seemed to suggest the possibility of a vacuum existing in the space above the water.\nEvangelista Torricelli.\nEvangelista Torricelli, who was Galileo's amanuensis for the last three months of his life, interpreted the results of the experiments in a novel way. He proposed that the weight of the atmosphere, not an attracting force of the vacuum, held the water in the tube. In a letter to Michelangelo Ricci in 1644 concerning the experiments, he wrote:\nMany have said that a vacuum does not exist, others that it does exist in spite of the repugnance of nature and with difficulty; I know of no one who has said that it exists without difficulty and without a resistance from nature. I argued thus: If there can be found a manifest cause from which the resistance can be derived which is felt if we try to make a vacuum, it seems to me foolish to try to attribute to vacuum those operations which follow evidently from some other cause; and so by making some very easy calculations, I found that the cause assigned by me (that is, the weight of the atmosphere) ought by itself alone to offer a greater resistance than it does when we try to produce a vacuum.\nIt was traditionally thought, especially by the Aristotelians, that the air did not have weight; that is, that the kilometers of air above the surface of the Earth did not exert any weight on the bodies below it. Even Galileo had accepted the weightlessness of air as a simple truth. Torricelli proposed that rather than an attractive force of the vacuum sucking up water, air did indeed have weight, which pushed on the water, holding up a column of it. He argued that the level that the water stayed at\u2014c. 10.3\u00a0m above the water surface below\u2014was reflective of the force of the air's weight pushing on the water in the basin, setting a limit for how far down the water level could sink in a tall, closed, water-filled tube. He viewed the barometer as a balance\u2014an instrument for measurement\u2014as opposed to merely an instrument for creating a vacuum, and since he was the first to view it this way, he is traditionally considered the inventor of the barometer, in the sense in which we now use the term.\nTorricelli's mercury barometer.\nBecause of rumors circulating in Torricelli's gossipy Italian neighborhood, which included that he was engaged in some form of sorcery or witchcraft, Torricelli realized he had to keep his experiment secret to avoid the risk of being arrested. He needed to use a liquid that was heavier than water, and from his previous association and suggestions by Galileo, he deduced that by using mercury, a shorter tube could be used. With mercury, which is about 14 times denser than water, a tube only 80\u00a0cm was now needed, not 10.5\u00a0m. Furthermore, Torricelli demonstrated that atmospheric pressure could support a column of mercury approximately 30 inches high.\nBlaise Pascal.\nIn 1646, Blaise Pascal along with Pierre Petit, had repeated and perfected Torricelli's experiment after hearing about it from Marin Mersenne, who himself had been shown the experiment by Torricelli toward the end of 1644. Pascal further devised an experiment to test the Aristotelian proposition that it was vapours from the liquid that filled the space in a barometer. His experiment compared water with wine, and since the latter was considered more \"spiritous\", the Aristotelians expected the wine to stand lower (since more vapours would mean more pushing down on the liquid column). Pascal performed the experiment publicly, inviting the Aristotelians to predict the outcome beforehand. The Aristotelians predicted the wine would stand lower. It did not.\nFirst atmospheric pressure vs. altitude experiment.\nHowever, Pascal went even further to test the mechanical theory. If, as suspected by mechanical philosophers like Torricelli and Pascal, air had weight, the pressure would be less at higher altitudes. Therefore, Pascal wrote to his brother-in-law, Florin Perier, who lived near a mountain called the Puy de D\u00f4me, asking him to perform a crucial experiment. Perier was to take a barometer up the Puy de D\u00f4me and make measurements along the way of the height of the column of mercury. He was then to compare it to measurements taken at the foot of the mountain to see if those measurements taken higher up were in fact smaller. In September 1648, Perier carefully and meticulously carried out the experiment, and found that Pascal's predictions had been correct. The column of mercury stood lower as the barometer was carried to a higher altitude.\nTypes.\nWater barometers.\nThe concept that decreasing atmospheric pressure predicts stormy weather, postulated by Lucien Vidi, provides the theoretical basis for a weather prediction device called a \"weather glass\" or a \"Goethe barometer\" (named for Johann Wolfgang von Goethe, the renowned German writer and polymath who developed a simple but effective weather ball barometer using the principles developed by Torricelli). The French name, \"le barom\u00e8tre Li\u00e8geois\", is used by some English speakers. This name reflects the origins of many early weather glasses \u2013 the glass blowers of Li\u00e8ge, Belgium.\nThe weather ball barometer consists of a glass container with a sealed body, half filled with water. A narrow spout connects to the body below the water level and rises above the water level. The narrow spout is open to the atmosphere. When the air pressure is lower than it was at the time the body was sealed, the water level in the spout will rise above the water level in the body; when the air pressure is higher, the water level in the spout will drop below the water level in the body. A variation of this type of barometer can be easily made at home. \nMercury barometers.\nA mercury barometer is an instrument used to measure atmospheric pressure in a certain location and has a vertical glass tube closed at the top sitting in an open mercury-filled basin at the bottom. Mercury in the tube adjusts until the weight of it balances the atmospheric force exerted on the reservoir. High atmospheric pressure places more force on the reservoir, forcing mercury higher in the column. Low pressure allows the mercury to drop to a lower level in the column by lowering the force placed on the reservoir. Since higher temperature levels around the instrument will reduce the density of the mercury, the scale for reading the height of the mercury is adjusted to compensate for this effect. The tube has to be at least as long as the amount dipping in the mercury + head space + the maximum length of the column.\nTorricelli documented that the height of the mercury in a barometer changed slightly each day and concluded that this was due to the changing pressure in the atmosphere. He wrote: \"We live submerged at the bottom of an ocean of elementary air, which is known by incontestable experiments to have weight\". Inspired by Torricelli, Otto von Guericke on 5 December 1660 found that air pressure was unusually low and predicted a storm, which occurred the next day.\nThe mercury barometer's design gives rise to the expression of atmospheric pressure in inches or millimeters of mercury (mmHg). A torr was originally defined as 1 mmHg. The pressure is quoted as the level of the mercury's height in the vertical column. Typically, atmospheric pressure is measured between and of Hg. One atmosphere (1 atm) is equivalent to of mercury.\nDesign changes to make the instrument more sensitive, simpler to read, and easier to transport resulted in variations such as the basin, siphon, wheel, cistern, Fortin, multiple folded, stereometric, and balance barometers.\nIn 2007, a European Union directive was enacted to restrict the use of mercury in new measuring instruments intended for the general public, effectively ending the production of new mercury barometers in Europe. The repair and trade of antiques (produced before late 1957) remained unrestricted.\nFitzroy barometer.\n\"Fitzroy\" barometers combine the standard mercury barometer with a thermometer, as well as a guide of how to interpret pressure changes.\nFortin barometer.\nFortin barometers use a variable displacement mercury cistern, usually constructed with a thumbscrew pressing on a leather diaphragm bottom (V in the diagram). This compensates for displacement of mercury in the column with varying pressure. To use a Fortin barometer, the level of mercury is set to zero by using the thumbscrew to make an ivory pointer (O in the diagram) just touch the surface of the mercury. The pressure is then read on the column by adjusting the vernier scale so that the mercury just touches the sightline at Z. Some models also employ a valve for closing the cistern, enabling the mercury column to be forced to the top of the column for transport. This prevents water-hammer damage to the column in transit.\nSympiesometer.\nA sympiesometer is a compact and lightweight barometer that was widely used on ships in the early 19th century. The sensitivity of this barometer was also used to measure altitude.\nSympiesometers have two parts. One is a traditional mercury thermometer that is needed to calculate the expansion or contraction of the fluid in the barometer. The other is the barometer, consisting of a J-shaped tube open at the lower end and closed at the top, with small reservoirs at both ends of the tube.\nIn 1778, Blondeau developed an iron tube barometer using narrow-bore musket barrels. This design resulted in a durable and polished instrument that resisted mercury corrosion and minimized breakage from the ship's movement.\nMarine barometer.\nThe need for a practical marine barometer arose from the urgent necessity of weather prediction at sea, where sailors faced frequent, and often dangerous, changes in wind, calm, and storm conditions. Traditional mercury barometers, though useful on land, proved unreliable on ships due to their susceptibility to the ship\u2019s motion. Oscillations caused the mercury to strike the top of the glass tube, leading to frequent breakage and making air pressure readings nearly impossible to interpret accurately during voyages.\nRoger North observed that many, including Robert Hooke, attempted to resolve these issues but often abandoned the endeavor due to technical limitations. Nonetheless, Hooke remained persistent, proposing several adaptations including narrowing the open end of the siphon tube and exploring spiral tube designs. His most notable contribution was the creation of a double thermometer marine barometer, also referred to as a manometer, which was presented to the Royal Society in 1668 and constructed by Henry Hunt.\nHooke\u2019s marine barometer marked a turning point in the development of nautical meteorological tools. It featured a compact, affordable design tailored for maritime use, becoming the first instrument specifically constructed for sailors. The device combined a sealed spirit thermometer with an open air-based thermometer, calibrated to reflect barometric pressure changes through liquid displacement. Hooke\u2019s use of hydrogen-filled containers and colorful almond oil further enhanced visibility and responsiveness. Notably, Edmund Halley tested this prototype on his South Atlantic voyage from 1698 to 1700 and praised its reliability in forecasting weather changes. His endorsement led to greater interest and validation by the Royal Society. Figure 8 below is from this report, depicting the Hooke Barometer, with detailed description in the writing.\nBuilding on Hooke\u2019s foundation, John Patrick sought to improve the design by replacing the water with mercury, advertising his version as a \u201cnew marine barometer.\u201d Though some criticized it for the difficulty of reading the mercury column due to shipboard vibrations, navigator Christopher Middleton employed it during expeditions to Hudson's Bay. He consistently found it effective in forecasting storms, wind changes, and even the proximity of ice.\nA significant advancement occurred during Captain James Cook\u2019s renowned voyages in the late 18th century. As part of preparations for Cook\u2019s second Pacific expedition (1772\u20131775), the Board of Longitude and the Royal Society commissioned the production of marine barometers. Renowned instrument maker Edward Nairne was chosen to supply the equipment. Contrary to expectations for spiral tubes, Nairne opted for straight, constricted tubes mounted on boards, coupled with a gimbaled suspension system to ensure vertical orientation and stability at sea.\nNairne\u2019s design represented a leap in functionality. The narrow bore significantly reduced mercury motion, enabling more accurate readings even in turbulent conditions. These instruments proved so reliable that they were adopted not only by the Royal Navy but also by international expeditions. The East India Company, Russian explorers, and French and Spanish navigators, including Jean-Fran\u00e7ois de Galaup, comte de Lap\u00e9rouse (voyage in 1785) and Alessandro Malaspina (voyage in 1789), incorporated variants of Nairne\u2019s barometer into their voyages.\nDespite the widespread use of Nairne\u2019s marine barometer, it was not without limitations. Lap\u00e9rouse lauded the device\u2019s predictive capabilities but also noted inconsistencies in mercury behavior, highlighting the complexity of translating instrument readings into reliable forecasts. In response to the fragility of glass tubes, other scientists, such as Le Roy, proposed alternate models like the folded Huygens barometer, designed for enhanced durability and reduced oscillation aboard ships.\nThe marine barometer\u2019s practical value was reaffirmed in 1801 when the Royal Society sent Captain Matthew Flinders on a three-year voyage from New Holland to New South Wales, equipped with one of Nairne\u2019s barometers. In his official correspondence, Flinders confirmed the instrument\u2019s success and expressed appreciation for its stability and precision in recording atmospheric conditions.\nThroughout its evolution, the marine barometer transitioned from a theoretical invention to a critical navigational and meteorological tool. Its development not only reflected ingenuity in overcoming the challenges of shipboard instrumentation but also underscored its importance in the broader context of global exploration. These devices empowered mariners to make informed decisions, contributing to safer and more efficient voyages across the world's oceans. \nWheel barometers.\nA wheel barometer uses a \"J\" tube sealed at the top of the longer limb. The shorter limb is open to the atmosphere, and floating on top of the mercury there is a small glass float. A fine silken thread is attached to the float which passes up over a wheel and then back down to a counterweight (usually protected in another tube). The wheel turns the point on the front of the barometer. As atmospheric pressure increases, mercury moves from the short to the long limb, the float falls, and the pointer moves. When pressure falls, the mercury moves back, lifting the float and turning the dial the other way.\nAround 1810 the wheel barometer, which could be read from a great distance, became the first practical and commercial instrument favoured by farmers and the educated classes in the UK. The face of the barometer was circular with a simple dial pointing to an easily readable scale: \"Rain - Change - Dry\" with the \"Change\" at the top centre of the dial. Later models added a barometric scale with finer graduations: \"Stormy (28 inches of mercury), Much Rain (28.5), Rain (29), Change (29.5), Fair (30), Set fair (30.5), very dry (31)\".\nNatalo Aiano is recognised as one of the finest makers of wheel barometers, an early pioneer in a wave of artisanal Italian instrument and barometer makers that were encouraged to emigrate to the UK. He listed as working in Holborn, London c.\u20091785\u20131805. From 1770 onwards, a large number of Italians came to England because they were accomplished glass blowers or instrument makers. By 1840 it was fair to say that the Italians dominated the industry in England.\nVacuum pump oil barometer.\nUsing vacuum pump oil as the working fluid in a barometer has led to the creation of the new \"World's Tallest Barometer\" in February 2013. The barometer at Portland State University (PSU) uses doubly distilled vacuum pump oil and has a nominal height of about 12.4\u00a0m for the oil column height; expected excursions are in the range of \u00b10.4\u00a0m over the course of a year. Vacuum pump oil has very low vapour pressure and is available in a range of densities; the lowest density vacuum oil was chosen for the PSU barometer to maximize the oil column height.\nAneroid barometers.\nAn aneroid barometer is an instrument used for measuring air pressure via a method that does not involve liquid. Although Gottfried Wilhelm Leibniz first proposed the concept of an aneroid barometer around 1700, it was not until 1844 that French scientist Lucien Vidi successfully invented it. The aneroid barometer uses a small, flexible metal box called an aneroid cell (capsule), which is made from an alloy of beryllium and copper. The evacuated capsule (or usually several capsules, stacked to add up their movements) is prevented from collapsing by a strong spring. Small changes in external air pressure cause the cell to expand or contract. This expansion and contraction drives mechanical levers such that the tiny movements of the capsule are amplified and displayed on the face of the aneroid barometer. Many models include a manually set needle which is used to mark the current measurement so that a relative change can be seen. This type of barometer is common in homes and in recreational boats. It is also used in meteorology, mostly in barographs, and as a pressure instrument in radiosondes.\nBarographs.\nA barograph is a recording aneroid barometer where the changes in atmospheric pressure are recorded on a paper chart.\nThe principle of the barograph is same as that of the aneroid barometer. Whereas the barometer displays the pressure on a dial, the barograph uses the small movements of the box to transmit by a system of levers to a recording arm that has at its extreme end either a scribe or a pen. A scribe records on smoked foil while a pen records on paper using ink, held in a nib. The recording material is mounted on a cylindrical drum which is rotated slowly by a clock. Commonly, the drum makes one revolution per day, per week, or per month, and the rotation rate can often be selected by the user.\nMEMS barometers.\nMicroelectromechanical systems (or MEMS) barometers are extremely small devices between 1 and 100 micrometres in size (0.001 to 0.1\u00a0mm). They are created via photolithography or photochemical machining. Typical applications include miniaturized weather stations, electronic barometers and altimeters.\nA barometer can also be found in smartphones such as the Samsung Galaxy Nexus, Samsung Galaxy S3-S6, Motorola Xoom, Apple iPhone 6 and newer iPhones, and Timex Expedition WS4 smartwatch, based on MEMS and piezoresistive pressure-sensing technologies. Inclusion of barometers on smartphones was originally intended to provide a faster GPS lock. However, third party researchers were unable to confirm additional GPS accuracy or lock speed due to barometric readings. The researchers suggest that the inclusion of barometers in smartphones may provide a solution for determining a user's elevation, but also suggest that several pitfalls must first be overcome.\nMore unusual barometers.\nThere are many other more unusual types of barometer. From variations on the storm barometer, such as the Collins Patent Table Barometer, to more traditional-looking designs such as Hooke's Otheometer and the Ross Sympiesometer. Some, such as the Shark Oil barometer, work only in a certain temperature range, achieved in warmer climates.\nApplications.\nBarometric pressure and the pressure tendency (the change of pressure over time) have been used in weather forecasting since the late 19th century. When used in combination with wind observations, reasonably accurate short-term forecasts can be made. Simultaneous barometric readings from across a network of weather stations allow maps of air pressure to be produced, which were the first form of the modern weather map when created in the 19th century. Isobars, lines of equal pressure, when drawn on such a map, give a contour map showing areas of high and low pressure. Localized high atmospheric pressure acts as a barrier to approaching weather systems, diverting their course. Atmospheric lift caused by low-level wind convergence into the surface brings clouds and sometimes precipitation. The larger the change in pressure, especially if more than 3.5\u00a0hPa (0.1 inHg), the greater the change in weather that can be expected. If the pressure drop is rapid, a low pressure system is approaching, and there is a greater chance of rain. Rapid pressure rises, such as in the wake of a cold front, are associated with improving weather conditions, such as clearing skies.\nWith falling air pressure, gases trapped within the coal in deep mines can escape more freely. Thus low pressure increases the risk of firedamp accumulating. Collieries therefore keep track of the pressure. In the case of the Trimdon Grange colliery disaster of 1882 the mines inspector drew attention to the records and in the report stated \"the conditions of atmosphere and temperature may be taken to have reached a dangerous point\".\nAneroid barometers are used in scuba diving. A submersible pressure gauge is used to keep track of the contents of the diver's air tank. Another gauge is used to measure the hydrostatic pressure, usually expressed as a depth of sea water. Either or both gauges may be replaced with electronic variants or a dive computer.\nCompensations.\nTemperature.\nThe density of mercury depends on temperature, so readings must be adjusted for the temperature of the instrument. For this purpose mercury thermometers may be mounted on barometers. Temperature compensation of an aneroid barometer can be accomplished by including a bi-metal element in the mechanical linkages. Inexpensive aneroid barometers sold for domestic use typically are manufactured to be accurate at room temperature, and have no provision for further adjustment for temperature.\nAltitude.\nAs the air pressure decreases at altitudes above sea level (and increases below sea level) the uncorrected reading of the barometer will depend on its location. The reading is then adjusted to an equivalent sea-level pressure for purposes of reporting. For example, if a barometer located at sea level and under fair weather conditions is moved to an altitude of 1,000 feet (305\u00a0m), about 1 inch of mercury (~35 hPa) must be added on to the reading. The barometer readings at the two locations should be the same if there are negligible changes in time, horizontal distance, and temperature. If this were not done, there would be a false indication of an approaching storm at the higher elevation.\nAneroid barometers have a mechanical adjustment that allows the equivalent sea level pressure to be read directly and without further adjustment if the instrument is not moved to a different altitude. Setting an aneroid barometer is similar to resetting an analog clock that is not at the correct time. Its dial is rotated so that the current atmospheric pressure from a known accurate and nearby barometer (such as the local weather station) is displayed. No calculation is needed, as the source barometer reading has already been converted to equivalent sea-level pressure, and this is transferred to the barometer being set\u2014regardless of its altitude. Though somewhat rare, a few aneroid barometers intended for monitoring the weather are calibrated to manually adjust for altitude. In this case, knowing \"either\" the altitude or the current atmospheric pressure would be sufficient for future accurate readings.\nThe table below shows examples for three locations in the city of San Francisco, California. Note the corrected barometer readings are identical, and based on equivalent sea-level pressure. (Assume a temperature of 15\u00a0\u00b0C.)\nIn 1787, during a scientific expedition on Mont Blanc, De Saussure undertook research and executed physical experiments on the boiling point of water at different heights. He calculated the height at each of his experiments by measuring how long it took an alcohol burner to boil an amount of water, and by these means he determined the height of the mountain to be 4775 metres. (This later turned out to be 32 metres less than the actual height of 4807 metres). For these experiments De Saussure brought specific scientific equipment, such as a barometer and thermometer. His calculated boiling temperature of water at the top of the mountain was fairly accurate, only off by 0.1 kelvin.\nBased on his findings, the pressure altimeter was developed as a specific application of the barometer. In the mid-19th century, this method was used by explorers.\nEquation.\nWhen atmospheric pressure is measured by a barometer, the pressure is also referred to as the \"barometric pressure\". Assume a barometer with a cross-sectional area \"A\", a height \"h\", filled with mercury from the bottom at Point B to the top at Point C. The pressure at the bottom of the barometer, Point B, is equal to the atmospheric pressure. The pressure at the very top, Point C, can be taken as zero because there is only mercury vapour above this point and its pressure is very low relative to the atmospheric pressure. Therefore, one can find the atmospheric pressure using the barometer and this equation:\nPatm = \u03c1gh\nwhere \u03c1 is the density of mercury, g is the gravitational acceleration, and h is the height of the mercury column above the free surface area. The physical dimensions (length of tube and cross-sectional area of the tube) of the barometer itself have no effect on the height of the fluid column in the tube.\nIn thermodynamic calculations, a commonly used pressure unit is the \"standard atmosphere\". This is the pressure resulting from a column of mercury of 760\u00a0mm in height at 0\u00a0\u00b0C. For the density of mercury, use \u03c1Hg = 13,595\u00a0kg/m3 and for gravitational acceleration use g = 9.807\u00a0m/s2.\nIf water were used (instead of mercury) to meet the standard atmospheric pressure, a water column of roughly 10.3\u00a0m (33.8\u00a0ft) would be needed.\nStandard atmospheric pressure as a function of elevation:\nNote: 1 torr = 133.3 Pa = 0.03937 inHg\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47490", "revid": "2051880", "url": "https://en.wikipedia.org/wiki?curid=47490", "title": "Biodegradation", "text": "Decomposition by living organisms\nBiodegradation is the breakdown of organic matter by microorganisms, such as bacteria and fungi. It is generally assumed to be a natural process, which differentiates it from composting. Composting is a human-driven process in which biodegradation occurs under a specific set of circumstances.\nThe process of biodegradation is threefold: first an object undergoes biodeterioration, which is the mechanical weakening of its structure; then follows biofragmentation, which is the breakdown of materials by microorganisms; and finally assimilation, which is the incorporation of the old material into new cells.\nIn practice, almost all chemical compounds and materials are subject to biodegradation, the key element being time. Things like vegetables may degrade within days, while glass and some plastics take many millennia to decompose. A standard for biodegradability used by the European Union is that greater than 90% of the original material must be converted into CO2, water and minerals by biological processes within 6 months.\nMechanisms.\nThe process of biodegradation can be divided into three stages: biodeterioration, biofragmentation, and assimilation. Biodeterioration is sometimes described as a surface-level degradation that modifies the mechanical, physical and chemical properties of the material. This stage occurs when the material is exposed to abiotic factors in the outdoor environment and allows for further degradation by weakening the material's structure. Some abiotic factors that influence these initial changes are compression (mechanical), light, temperature and chemicals in the environment.\u00a0While biodeterioration typically occurs as the first stage of biodegradation, it can in some cases be parallel to biofragmentation. Hueck, however, defined Biodeterioration as the undesirable action of living organisms on Man's materials, involving such things as breakdown of stone facades of buildings, corrosion of metals by microorganisms or merely the esthetic changes induced on man-made structures by the growth of living organisms.\nBiofragmentation of a polymer is the lytic process in which bonds within a polymer are cleaved, generating oligomers and monomers in its place. The steps taken to fragment these materials also differ based on the presence of oxygen in the system. The breakdown of materials by microorganisms when oxygen is present is aerobic digestion, and the breakdown of materials when oxygen is not present is anaerobic digestion. The main difference between these processes is that anaerobic reactions produce methane, while aerobic reactions do not (however, both reactions produce carbon dioxide, water, some type of residue, and a new biomass). In addition, aerobic digestion typically occurs more rapidly than anaerobic digestion, while anaerobic digestion does a better job reducing the volume and mass of the material. Due to anaerobic digestion's ability to reduce the volume and mass of waste materials and produce a natural gas, anaerobic digestion technology is widely used for waste management systems and as a source of local, renewable energy.\nIn the assimilation stage, the resulting products from biofragmentation are then integrated into microbial cells. Some of the products from fragmentation are easily transported within the cell by membrane carriers. However, others still have to undergo biotransformation reactions to yield products that can then be transported inside the cell. Once inside the cell, the products enter catabolic pathways that either lead to the production of adenosine triphosphate (ATP) or elements of the cells structure.\nCpolymer + O2 \u2192 Cresidue + Cbiomass + CO2 + H2O\nCpolymer \u2192 Cresidue + Cbiomass + CO2 + CH4 + H2O\nFactors affecting biodegradation rate.\nIn practice, almost all chemical compounds and materials are subject to biodegradation processes. The significance, however, is in the relative rates of such processes, such as days, weeks, years or centuries. A number of factors determine the rate at which this degradation of organic compounds occurs. Factors include light, water, oxygen and temperature. The degradation rate of many organic compounds is limited by their bioavailability, which is the rate at which a substance is absorbed into a system or made available at the site of physiological activity, as compounds must be released into solution before organisms can degrade them. The rate of biodegradation can be measured in a number of ways. Respirometry tests can be used for aerobic microbes. First one places a solid waste sample in a container with microorganisms and soil, and then aerates the mixture. Over the course of several days, microorganisms digest the sample bit by bit and produce carbon dioxide \u2013 the resulting amount of CO2 serves as an indicator of degradation. Biodegradability can also be measured by anaerobic microbes and the amount of methane or alloy that they are able to produce.\nIt's important to note factors that affect biodegradation rates during product testing to ensure that the results produced are accurate and reliable. Several materials will test as being biodegradable under optimal conditions in a lab for approval but these results may not reflect real world outcomes where factors are more variable. For example, a material may have tested as biodegrading at a high rate in the lab may not degrade at a high rate in a landfill because landfills often lack light, water, and microbial activity that are necessary for degradation to occur. Thus, it is very important that there are standards for plastic biodegradable products, which have a large impact on the environment. The development and use of accurate standard test methods can help ensure that all plastics that are being produced and commercialized will actually biodegrade in natural environments. One test that has been developed for this purpose is DINV 54900.\nRecent advances have enabled real-time monitoring of polymer biodegradation using biosensors combined with machine learning, improving the accuracy of degradation assessments under varying environmental conditions.\nPlastics.\nThe term Biodegradable Plastics refers to materials that maintain their mechanical strength during practical use but break down into low-weight compounds and non-toxic byproducts after their use. This breakdown is made possible through an attack of microorganisms on the material, which is typically a non-water-soluble polymer. Such materials can be obtained through chemical synthesis, fermentation by microorganisms, and from chemically modified natural products.\nPlastics biodegrade at highly variable rates. PVC-based plumbing is selected for handling sewage because PVC resists biodegradation. Some packaging materials on the other hand are being developed that would degrade readily upon exposure to the environment. Examples of synthetic polymers that biodegrade quickly include polycaprolactone, other polyesters and aromatic-aliphatic esters, due to their ester bonds being susceptible to attack by water. A prominent example is poly-3-hydroxybutyrate, the renewably derived polylactic acid. Others are the cellulose-based cellulose acetate and celluloid (cellulose nitrate).\nUnder low oxygen conditions plastics break down more slowly. The breakdown process can be accelerated in specially designed compost heap. Starch-based plastics will degrade within two to four months in a home compost bin, while polylactic acid is largely undecomposed, requiring higher temperatures. Polycaprolactone and polycaprolactone-starch composites decompose slower, but the starch content accelerates decomposition by leaving behind a porous, high surface area polycaprolactone. Nevertheless, it takes many months.\nIn 2016, a bacterium named \"Ideonella sakaiensis\" was found to biodegrade PET. In 2020, the PET degrading enzyme of the bacterium, PETase, has been genetically modified and combined with MHETase to break down PET faster, and also degrade PEF. In 2021, researchers reported that a mix of microorganisms from cow stomachs could break down three types of plastics.\nMany plastic producers have gone so far even to say that their plastics are compostable, typically listing corn starch as an ingredient. However, these claims are questionable because the plastics industry operates under its own definition of compostable:\n\"that which is capable of undergoing biological decomposition in a compost site such that the material is not visually distinguishable and breaks down into carbon dioxide, water, inorganic compounds and biomass at a rate consistent with known compostable materials.\" (Ref: ASTM D 6002)\nThe term \"composting\" is often used informally to describe the biodegradation of packaging materials. Legal definitions exist for compostability, the process that leads to compost. Four criteria are offered by the European Union:\nBiodegradable technology.\nBiodegradable technology is established technology with some applications in product packaging, production, and medicine. The chief barrier to widespread implementation is the trade-off between biodegradability and performance. For example, lactide-based plastics are inferior packaging properties in comparison to traditional materials.\nOxo-biodegradation is defined by CEN (the European Standards Organisation) as \"degradation resulting from oxidative and cell-mediated phenomena, either simultaneously or successively.\" While sometimes described as \"oxo-fragmentable,\" and \"oxo-degradable\" these terms describe only the first or oxidative phase and should not be used for material which degrades by the process of oxo-biodegradation defined by CEN: the correct description is \"oxo-biodegradable.\" Oxo-biodegradable formulations accelerate the biodegradation process but it takes considerable skill and experience to balance the ingredients within the formulations so as to provide the product with a useful life for a set period, followed by degradation and biodegradation.\nBiodegradable technology is especially utilized by the bio-medical community. Biodegradable polymers are classified into three groups:\nmedical, ecological, and dual application, while in terms of origin they are divided into two groups: natural and synthetic. The Clean Technology Group is exploiting the use of supercritical carbon dioxide, which under high pressure at room temperature is a solvent that can use biodegradable plastics to make polymer drug coatings. The polymer (meaning a material composed of molecules with repeating structural units that form a long chain) is used to encapsulate a drug prior to injection in the body and is based on lactic acid, a compound normally produced in the body, and is thus able to be excreted naturally. The coating is designed for controlled release over a period of time, reducing the number of injections required and maximizing the therapeutic benefit. Professor Steve Howdle states that biodegradable polymers are particularly attractive for use in drug delivery, as once introduced into the body they require no retrieval or further manipulation and are degraded into soluble, non-toxic by-products. Different polymers degrade at different rates within the body and therefore polymer selection can be tailored to achieve desired release rates.\nOther biomedical applications include the use of biodegradable, elastic shape-memory polymers. Biodegradable implant materials can now be used for minimally invasive surgical procedures through degradable thermoplastic polymers. These polymers are now able to change their shape with increase of temperature, causing shape memory capabilities as well as easily degradable sutures. As a result, implants can now fit through small incisions, doctors can easily perform complex deformations, and sutures and other material aides can naturally biodegrade after a completed surgery.\nBiodegradation vs. composting.\nThere is no universal definition for biodegradation and there are various definitions of composting, which has led to much confusion between the terms. They are often lumped together; however, they do not have the same meaning. Biodegradation is the naturally occurring breakdown of materials by microorganisms such as bacteria and fungi or other biological activity. Composting is a human-driven process in which biodegradation occurs under a specific set of circumstances. The predominant difference between the two is that one process is naturally occurring and one is human-driven.\nBiodegradable material is capable of decomposing without an oxygen source (anaerobically) into carbon dioxide, water, and biomass, but the timeline is not very specifically defined. Similarly, compostable material breaks down into carbon dioxide, water, and biomass; however, compostable material also breaks down into inorganic compounds. The process for composting is more specifically defined, as it is controlled by humans. Essentially, composting is an accelerated biodegradation process due to optimized circumstances. Additionally, the end product of composting not only returns to its previous state, but also generates and adds beneficial microorganisms to the soil called humus. This organic matter can be used in gardens and on farms to help grow healthier plants in the future. Composting more consistently occurs within a shorter time frame since it is a more defined process and is expedited by human intervention. Biodegradation can occur in different time frames under different circumstances, but is meant to occur naturally without human intervention.\nEven within composting, there are different circumstances under which this can occur. The two main types of composting are at-home versus commercial. Both produce healthy soil to be reused \u2013 the main difference lies in what materials are able to go into the process. At-home composting is mostly used for food scraps and excess garden materials, such as weeds. Commercial composting is capable of breaking down more complex plant-based products, such as corn-based plastics and larger pieces of material, like tree branches. Commercial composting begins with a manual breakdown of the materials using a grinder or other machine to initiate the process. Because at-home composting usually occurs on a smaller scale and does not involve large machinery, these materials would not fully decompose in at-home composting. Furthermore, one study has compared and contrasted home and industrial composting, concluding that there are advantages and disadvantages to both.\nThe following studies provide examples in which composting has been defined as a subset of biodegradation in a scientific context. The first study, \"Assessment of Biodegradability of Plastics Under Simulated Composting Conditions in a Laboratory Test Setting,\" clearly examines composting as a set of circumstances that falls under the category of degradation. Additionally, this next study looked at the biodegradation and composting effects of chemically and physically crosslinked polylactic acid. Notably discussing composting and biodegrading as two distinct terms. The third and final study reviews European standardization of biodegradable and compostable material in the packaging industry, again using the terms separately.\nThe distinction between these terms is crucial because waste management confusion leads to improper disposal of materials by people on a daily basis. Biodegradation technology has led to massive improvements in how we dispose of waste; there now exist trash, recycling, and compost bins in order to optimize the disposal process. However, if these waste streams are commonly and frequently confused, then the disposal process is not at all optimized. Biodegradable and compostable materials have been developed to ensure more of human waste is able to breakdown and return to its previous state, or in the case of composting even add nutrients to the ground. When a compostable product is thrown out as opposed to composted and sent to a landfill, these inventions and efforts are wasted. Therefore, it is important for citizens to understand the difference between these terms so that materials can be disposed of properly and efficiently.\nEnvironmental and social effects.\nPlastic pollution from illegal dumping poses health risks to wildlife. Animals often mistake plastics for food, resulting in intestinal entanglement. Slow-degrading chemicals, like polychlorinated biphenyls (PCBs), nonylphenol (NP), and pesticides also found in plastics, can release into environments and subsequently also be ingested by wildlife.\nThese chemicals also play a role in human health, as consumption of tainted food (in processes called biomagnification and bioaccumulation) has been linked to issues such as cancers, neurological dysfunction, and hormonal changes. A well-known example of biomagnification impacting health in recent times is the increased exposure to dangerously high levels of mercury in fish, which can affect sex hormones in humans.\nIn efforts to remediate the damages done by slow-degrading plastics, detergents, metals, and other pollutants created by humans, economic costs have become a concern. Marine litter in particular is notably difficult to quantify and review. Researchers at the World Trade Institute estimate that cleanup initiatives' cost (specifically in ocean ecosystems) has hit close to thirteen billion dollars a year. The main concern stems from marine environments, with the biggest cleanup efforts centering around garbage patches in the ocean. The Great Pacific Garbage Patch, a garbage patch the size of Mexico, is located in the Pacific Ocean. It is estimated to be upwards of a million square miles in size. While the patch contains more obvious examples of litter (plastic bottles, cans, and bags), tiny microplastics are nearly impossible to clean up. \"National Geographic\" reports that even more non-biodegradable materials are finding their way into vulnerable environments \u2013 nearly thirty-eight million pieces a year.\nMaterials that have not degraded can also serve as shelter for invasive species, such as tube worms and barnacles. When the ecosystem changes in response to the invasive species, resident species and the natural balance of resources, genetic diversity, and species richness is altered. These factors may support local economies in way of hunting and aquaculture, which suffer in response to the change. Similarly, coastal communities which rely heavily on ecotourism lose revenue thanks to a buildup of pollution, as their beaches or shores are no longer desirable to travelers. The World Trade Institute also notes that the communities who often feel most of the effects of poor biodegradation are poorer countries without the means to pay for their cleanup. In a positive feedback loop effect, they in turn have trouble controlling their own pollution sources.\nEtymology of \"biodegradable\".\nThe first known use of \"biodegradable\" in a biological context was in 1959 when it was employed to describe the breakdown of material into innocuous components by microorganisms. Now \"biodegradable\" is commonly associated with environmentally friendly products that are part of the earth's innate cycles like the carbon cycle and capable of decomposing back into natural elements.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47492", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=47492", "title": "Biomass (ecology)", "text": "Total mass of living organisms in a given area (all species or selected species)\nBiomass is the total mass of living biological organisms in a given area or ecosystem at a specific time. Biomass may refer to the \"species biomass\", which is the mass of one or more species, or to \"community biomass\", which is the mass of all species in the community. It encompasses microorganisms, plants, and animals, and is typically expressed as total mass or average mass per unit area.\nThe method used to measure biomass depends on the context. In some cases, biomass refers to the wet weight of organisms as they exist in nature. For example, in a salmon fishery, the salmon biomass might be regarded as the total wet weight the salmon would have if they were taken out of the water. In other contexts, biomass can be measured in terms of the dried organic mass, so perhaps only 30% of the actual weight might count, the rest being water. In other contexts, it may refer to dry weight (excluding water content), or to the mass of organic carbon, excluding inorganic components such as bones, shells, or teeth.\nIn 2018, Bar-On et al. estimated Earth's total live biomass at approximately 550 billion tonnes of carbon, the majority of which is found in plants. A 1998 study by Field et al. estimated global annual net primary production at just over 100 billion tonnes of carbon per year. While bacteria were once believed to account for a biomass comparable to that of plants, more recent research indicates they represent a much smaller proportion. The total number of DNA base pairs on Earth \u2013 sometimes used as a possible approximation of global biodiversity \u2013 has been estimated at , with a mass of around 50 billion tonnes. By the year 2020, the mass of human-made materials or anthropogenic mass, defined as \"the mass embedded in inanimate solid objects made by humans (that have not yet been demolished or taken out of service)\", was projected to surpass that of all living biomass on Earth.\nEcological pyramids.\nAn ecological pyramid is a graphical representation that shows, for a given ecosystem, the relationship between biomass or biological productivity and trophic levels.\nAn ecological pyramid provides a snapshot in time of an ecological community.\nThe bottom of the pyramid represents the primary producers (autotrophs). The primary producers take energy from the environment in the form of sunlight or inorganic chemicals and use it to create energy-rich molecules such as carbohydrates. This mechanism is called primary production. The pyramid then proceeds through the various trophic levels to the apex predators at the top.\nWhen energy is transferred from one trophic level to the next, typically only ten percent is used to build new biomass. The remaining ninety percent goes to metabolic processes or is dissipated as heat. This energy loss means that productivity pyramids are never inverted, and generally limits food chains to about six levels. However, in oceans, biomass pyramids can be wholly or partially inverted, with more biomass at higher levels.\nTerrestrial biomass.\nTerrestrial biomass generally decreases markedly at each higher trophic level (plants, herbivores, carnivores). Examples of terrestrial producers are grasses, trees and shrubs. These have a much higher biomass than the animals that consume them, such as deer, zebras and insects. The level with the least biomass are the highest predators in the food chain, such as foxes and eagles.\nIn a temperate grassland, grasses and other plants are the primary producers at the bottom of the pyramid. Then come the primary consumers, such as grasshoppers, voles and bison, followed by the secondary consumers, shrews, hawks and small cats. Finally the tertiary consumers, large cats and wolves. The biomass pyramid decreases markedly at each higher level.\nChanges in plant species in the terrestrial ecosystem can result in changes in the biomass of soil decomposer communities. Biomass in C3 and C4 plant species can change in response to altered concentrations of CO2. C3 plant species have been observed to increase in biomass in response to increasing concentrations of CO2 of up to 900 ppm.\nOcean biomass.\nOcean or marine biomass, in a reversal of terrestrial biomass, can increase at higher trophic levels. In the ocean, the food chain typically starts with phytoplankton, and follows the course:\nPhytoplankton \u2192 zooplankton \u2192 predatory zooplankton \u2192 filter feeders \u2192 predatory fish\nPhytoplankton are the main primary producers at the bottom of the marine food chain. Phytoplankton use photosynthesis to convert inorganic carbon into protoplasm. They are then consumed by zooplankton that range in size from a few micrometers in diameter in the case of protistan microzooplankton to macroscopic gelatinous and crustacean zooplankton.\nZooplankton comprise the second level in the food chain, and includes small crustaceans, such as copepods and krill, and the larva of fish, squid, lobsters and crabs.\nIn turn, small zooplankton are consumed by both larger predatory zooplankters, such as krill, and by forage fish, which are small, schooling, filter-feeding fish. This makes up the third level in the food chain.\nA fourth trophic level can consist of predatory fish, marine mammals and seabirds that consume forage fish. Examples are swordfish, seals and gannets.\nApex predators, such as orcas, which can consume seals, and shortfin mako sharks, which can consume swordfish, make up a fifth trophic level. Baleen whales can consume zooplankton and krill directly, leading to a food chain with only three or four trophic levels.\nMarine environments can have inverted biomass pyramids. In particular, the biomass of consumers (copepods, krill, shrimp, forage fish) is larger than the biomass of primary producers. This happens because the ocean's primary producers are tiny phytoplankton which are r-strategists that grow and reproduce rapidly, so a small mass can have a fast rate of primary production. In contrast, terrestrial primary producers, such as forests, are K-strategists that grow and reproduce slowly, so a much larger mass is needed to achieve the same rate of primary production.\nAmong the phytoplankton at the base of the marine food web are members from a phylum of bacteria called cyanobacteria. Marine cyanobacteria include the smallest known photosynthetic organisms. The smallest of all, \"Prochlorococcus\", is approximately 0.5 to 0.8 micrometres across. In terms of individual numbers, \"Prochlorococcus\" is possibly the most plentiful species on Earth: a single millilitre of surface seawater can contain 100,000 cells or more. Worldwide, there are estimated to be several octillion (1027) individuals. \"Prochlorococcus\" is ubiquitous between 40\u00b0N and 40\u00b0S and dominates in the oligotrophic (nutrient poor) regions of the oceans. The bacterium accounts for an estimated 20% of the oxygen in the Earth's atmosphere, and forms part of the base of the ocean food chain.\nBacterial biomass.\nBacteria and archaea are both classified as prokaryotes, and their biomass is commonly estimated together. The global biomass of prokaryotes is estimated at 30 billion tonnes C, dominated by bacteria.\nThe estimates for the global biomass of prokaryotes had changed significantly over recent decades, as more data became available. A much-cited study from 1998 collected data on abundances (number of cells) of bacteria and archaea in different natural environments, and estimated their total biomass at 350 to 550 billion tonnes C. This vast amount is similar to the biomass of carbon in all plants. The vast majority of bacteria and archaea were estimated to be in sediments deep below the seafloor or in the deep terrestrial biosphere (in deep continental aquifers). However, updated measurements reported in a 2012 study reduced the calculated prokaryotic biomass in deep subseafloor sediments from the original \u2248300 billion tonnes C to \u22484 billion tonnes C (range 1.5\u201322 billion tonnes). This update originates from much lower estimates of both the prokaryotic abundance and their average weight.\nA census published in PNAS in May 2018 estimated global bacterial biomass at \u224870 billion tonnes C, of which \u224860 billion tonnes are in the terrestrial deep subsurface. It also estimated the global biomass of archaea at \u22487 billion tonnes C. A later study by the Deep Carbon Observatory published in 2018 reported a much larger dataset of measurements, and updated the total biomass estimate in the deep terrestrial biosphere. It used this new knowledge and previous estimates to update the global biomass of bacteria and archaea to 23\u201331 billion tonnes C. Roughly 70% of the global biomass was estimated to be found in the deep subsurface. The estimated number of prokaryotic cells globally was estimated to be 11\u201315 \u00d7 1029. With this information, the authors of the May 2018 PNAS article revised their estimate for the global biomass of prokaryotes to \u224830 billion tonnes C, similar to the Deep Carbon Observatory estimate.\nThese estimates convert global abundance of prokaryotes into global biomass using average cellular biomass figures that are based on limited data. Recent estimates used an average cellular biomass of about 20\u201330 femtogram carbon (fgC) per cell in the subsurface and terrestrial habitats.\nGlobal biomass.\nThe total global biomass has been estimated at 550 billion tonnes C. A breakdown of the global biomass is given by kingdom in the table below, based on a 2018 study by Bar-On et. al.\n Animals represent less than 0.5% of the total biomass on Earth, with about 2 billion tonnes C in total. Most animal biomass is found in the oceans, where arthropods, such as copepods, account for about 1 billion tonnes C and fish for another 0.7 billion tonnes C. Roughly half of the biomass of fish in the world are mesopelagic, such as lanternfish, spending most of the day in the deep, dark waters. Marine mammals such as whales and dolphins account for about 0.006 billion tonnes C.\nLand animals account for about 500 million tonnes C, or about 20% of the biomass of animals on Earth. Terrestrial arthropods account for about 150 million tonnes C, most of which is found in the topsoil. Land mammals account for about 180 million tonnes C, most of which are humans (about 80 million tonnes C) and domesticated mammals (about 90 million tonnes C). Wild terrestrial mammals account for only about 3 million tonnes C, less than 2% of the total mammalian biomass on land.\nMost of the global biomass is found on land, with only 5 to 10 billion tonnes C found in the oceans. On land, there is about 1,000 times more plant biomass (\"phytomass\") than animal biomass (\"zoomass\"). About 18% of this plant biomass is eaten by the land animals. However, marine animals eat most of the marine autotrophs, and the biomass of marine animals is greater than that of marine autotrophs.\nAccording to a 2020 study published in \"Nature\", human-made materials, or technomass, outweigh all living biomass on earth, with plastic alone exceeding the mass of all land and marine animals combined.\nGlobal rate of production.\nNet primary production is the rate at which new biomass is generated, mainly due to photosynthesis. Global primary production can be estimated from satellite observations. Satellites scan the normalised difference vegetation index (NDVI) over terrestrial habitats and scan sea-surface chlorophyll levels over oceans. This results in 56.4 billion tonnes C/yr (53.8%) for terrestrial primary production and 48.5 billion tonnes C/yr for oceanic primary production. Thus, the total photoautotrophic primary production for the Earth is about 104.9 billion tonnes C/yr. This translates to about 426 gC/m2/yr for land production (excluding areas with permanent ice cover) and 140 gC/m2/yr for the oceans.\nHowever, there is a much more significant difference in standing stocks\u2014while accounting for almost half of the total annual production, oceanic autotrophs account for only about 0.2% of the total biomass.\nTerrestrial freshwater ecosystems generate about 1.5% of the global net primary production.\nSome global producers of biomass, in order of productivity rates, are\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "47493", "revid": "44062", "url": "https://en.wikipedia.org/wiki?curid=47493", "title": "Biological (disambiguation)", "text": "Biological is the adjectival form of \"biology\", the study of life. Biological may also refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47496", "revid": "2666701", "url": "https://en.wikipedia.org/wiki?curid=47496", "title": "Biota", "text": "A Biota is the assemblage of living organisms in a specific place and time:\nIt may also mean:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47497", "revid": "45364314", "url": "https://en.wikipedia.org/wiki?curid=47497", "title": "Doctor Doom", "text": "Supervillain appearing in Marvel Comics\nDoctor Doom is a supervillain appearing in American comic books published by Marvel Comics. Created by Stan Lee and Jack Kirby, the character first appeared in \"The Fantastic Four\" #5 (1962), and has since endured as the archenemy of the superhero team the Fantastic Four.\nVictor Werner von Doom is the monarch of the fictional European country of Latveria who uses his mastery of both science and sorcery in pursuit of his goals to bring order to humanity through world domination, and prove his intellectual superiority over Mister Fantastic\u2014his old college rival and the leader of the Fantastic Four. Doom blames Mister Fantastic for his disfigurement, and wears a magically forged suit of armor with a metal mask and green hooded cloak to conceal his facial scars. Regarded as one of the smartest characters and greatest threats in the Marvel Universe, Doom has stolen the abilities of cosmic beings such as the Silver Surfer and the Beyonder in his lust for power, although his pride and arrogance frequently lead to the failures of his schemes of conquest. While his primary obsession is the Fantastic Four, Doom has also fought other heroes, including Spider-Man, Iron Man, Doctor Strange, Black Panther, the X-Men, and the Avengers.\nThe character has been listed among the greatest comic book villains ever created and adapted in various media incarnations, including films, television series, and video games. Joseph Culp, Julian McMahon, Toby Kebbell, and Robert Downey Jr. have portrayed Doom in live-action, while Simon Templeman, Paul Dobson, Clive Revill, and Lex Lang, among others, have provided his voice in animation and video games.\nPublication history.\nCreation and development.\nLike many of Marvel's Silver Age characters, Doom was conceived by Stan Lee and Jack Kirby. With the \"Fantastic Four\" title performing well, Lee and Kirby were trying to dream up a \"soul-stirring...super sensational new villain\" for the series. Looking for a name, Lee latched onto \"Doctor Doom\" as \"eloquent in its simplicity \u2014 magnificent in its implied menace.\"\nDue to the rush to publish, the character was not given a full origin story until \"Fantastic Four Annual\" #2, two years after his debut.\nKirby stated in a 1987 interview: \"Dr. Doom was the classic conception of Death, of approaching Death. I saw Dr. Doom as The Man in the Iron Mask, who symbolized approaching Death. It was the reason for the armor and the hood. Death is connected with armor and inhuman-like steel. Death is something without mercy and human flesh contains that element of mercy. Therefore, I had to erase it, and I did it with a mask.\"\nKirby further described Doom as being \"paranoid\", wrecked by his twisted face and wanting the whole world to be like him. Kirby went on to say that \"Doom is an evil person, but he's not always been evil. He was [respected]...but through a flaw in his own character, he was a perfectionist.\" At one point in the 1970s, Kirby drew his interpretation of what Doom would look like under the mask, giving Doom only \"a tiny scar on his cheek\". Due to this slight imperfection, Doom hides his face not from the world, but from himself. To Kirby, this is the motivation for Doom's vengeance against the world; because others are superior due to this slight scar, Doom wants to elevate himself above them. Stan Lee's writing typically showed Doom's arrogance as his constant downfall, and how his pride leads to von Doom's disfigurement at the hands of his own machine, and to the failures of many of his schemes.\nWhile the Fantastic Four had fought various villains such as the Mole Man, Skrulls, the Miracle Man, and Namor the Sub-Mariner, Doom managed to overshadow them all and became the Fantastic Four's archnemesis. During the 1970s, Doom branched out to more Marvel titles such as \"Astonishing Tales\", \"The Incredible Hulk\", and \"Super-Villain Team-Up\" (1975). Beginning with issue #42, he also had appearances in \"Marvel Team-Up\" (February 1976). Doom's origin was also a feature in \"Astonishing Tales\" when his ties to the villain Mephisto were revealed.\nIn the book \"Superhero: The Secret Origin of a Genre\", Peter Coogan writes that Doom's original appearance was representative of a change in the portrayal of \"mad scientists\" to full-fledged villains, often with upgraded powers. These supervillains are genre-crossing villains who exist in adventures \"in a world in which the ordinary laws of nature are slightly suspended\"; characters such as Professor Moriarty, Count Dracula, Auric Goldfinger, Hannibal Lecter, Joker, Lex Luthor, and Darth Vader, also fit this description. Sanderson also found traces of William Shakespeare's characters Richard III and Iago in Doom; all of them \"are descended from the 'vice' figure of medieval drama\", who address the audience in monologs detailing their thoughts and ambitions.\n1980s\u20131990s.\nIn 1976, Marvel and DC Comics collaborated on \"Superman vs. the Amazing Spider-Man\", and seeking to replicate that success the two companies again teamed the characters in \"Superman and Spider-Man\" in 1981. Marvel editor-in-chief Jim Shooter co-wrote the story with Marv Wolfman, and recalled choosing Victor von Doom based on his iconic status: \"I figured I needed the heaviest-duty bad guy we had to offer \u2014 Doctor Doom. Their greatest hero against our greatest villain.\"\nIn 1981 John Byrne began his six-year run writing and illustrating \"Fantastic Four\", sparking a \"second golden age\" for the title but also attempting to \"turn the clock back [...] get back and see fresh what it was that made the book great at its inception.\" Doctor Doom made his first appearance under Byrne's tenure with issue #236. Whereas Kirby had intimated that Doom's disfigurement was more a figment of Victor's vain personality, Byrne decided that Doom's face was truly ravaged: only Doom's own robot slaves are allowed to see the monarch without his helmet. Byrne emphasized other aspects of Doom's personality; despite his ruthless nature, Victor von Doom is a man of honor. Returning to Latveria after being temporarily deposed, Doctor Doom abandons a scheme to wrest mystical secrets from Doctor Strange to oversee his land's reconstruction. Despite a tempestuous temper, Doom occasionally shows warmth and empathy to others; he tries to free his mother from Mephisto and treats Kristoff Vernard like his own son. Byrne gave further detail regarding Doom's scarring: Byrne introduced the idea that the accident at Empire State University only left Victor with a small scar that was exaggerated into a more disfiguring accident by Doom's own arrogance\u2014by donning his newly forged face mask before it had fully cooled, he caused massive irreparable damage.\nAfter his debut, Doctor Doom remained a key villain in \"Fantastic Four\" throughout the 1980s, appearing in titles as \"Punisher\", \"The Spectacular Spider-Man\", and \"Excalibur\". During Steven Englehart's run on \"Fantastic Four\", Doom was exiled by his heir, Kristoff, but this storyline was left unresolved when Englehart departed. Walt Simonson's \"Fantastic Four\" #350 controversially revealed that the Doom seen during Englehart's arc was a robotic imposter, with the real Doom returning in new armor to reclaim Latveria. Simonson's retcon suggested the last true appearance of Doom was in the \"Battle of the Baxter Building,\" but later writers often disregarded his interpretations, leading to further revisions of Doom's character and history.\n2000s\u20132010s.\nMark Waid began to redefine Doctor Doom in the 2003 \"Unthinkable\" storyline (\"Fantastic Four\" vol. 3, #66-70 and #500), where Doom forsakes technology for mysticism. He kills his first love, Valeria, to gain powers from demons and imprisons Franklin Richards in Hell. Doom challenges Reed Richards to escape a magical prison, but with Doctor Strange's help, Richards succeeds, causing Doom to be dragged to Hell. Doom remained there until the 2004 \"Ragnarok\" storyline in \"Thor\", where Thor's hammer, Mj\u00f6lnir, provided his escape. In 2005\u20132006, Doctor Doom starred in the limited series \"Books of Doom\", written by Ed Brubaker. This retelling of his origin explored the early, less-seen parts of Doom's life and questioned whether his path to dictatorship was fated or due to personal faults\u2014a nature versus nurture debate. Brubaker's portrayal was influenced by the original Lee/Kirby version, and he chose not to show Doom's face, following Kirby's example.\nIn \"Spider-Man/Fantastic Four\" #4, the Mighty Avengers invaded Latveria due to Victor's involvement in a chemical bomb plot involving the Venom symbiote, which was actually orchestrated by Kristoff Vernard. In the \"Siege\" storyline, Doctor Doom initially supports Norman Osborn's attack on Asgard but later withdraws. He also stars in \"Doomwar\" written by Jonathan Maberry, where he allies with the isolationist Desturi to seize control of Wakanda.\nIn \"Fantastic Four: Three\", Doctor Doom seeks to be \"reborn\" and plans to abdicate his throne to Kristoff. Valeria von Doom visits him, notices his brain damage, and offers to restore his mental capacity in exchange for his help with Reed and the Fantastic Four. A humbled Doom later attends Johnny Storm's funeral, and is recommended for the Future Foundation. Leading up to \"Secret Wars\", Doom usurps the power of the Beyonders, creating a new Battleworld where he assumes the role of God. However, Reed Richards and a group of heroes challenge Doom, and with the Molecule Man's help, they restore the multiverse. Reed ultimately uses the Beyonder's power to heal Doom's face and purify his soul.\nDoom returns to his kingdom, saving Tony Stark from Latverian rebels and claiming to be a new man. He relinquishes his dictatorship, entrusting Tony with a Wand of Watoomb to defeat Madame Masque. When more rebels appear, he teleports Stark to the Bronx Zoo, and they later confront Madame Masque in Chicago. After discovering she's possessed, Doom helps Tony trap her in his armor and exorcises the demon. He then vanishes before Tony wakes up. Doom later interrupts Tony's breakfast with Amara, trying to prove he has changed, but Tony remains distrustful. Following Stark's coma caused by Captain Marvel, Doom takes up the Iron Man mantle, faces Mephisto disguised as the Maker, joins the Avengers, and eventually conceives a child with Dr. Amara Perera.\nWhen Ben Grimm and Johnny Storm seek their teammates to restore the Fantastic Four's powers, Doom follows them into parallel universes to assist an alternate Reed Richards against a version of Doom who has merged with Galactus. Meanwhile, Gwenpool, unaware of Doom's reformation, tries to attack him using an AI Doombot named Vincent. Doom easily captures her but is amused by her taunts. Gwen escapes, believing Doom is still a threat, and attacks again, releasing an earlier version of Doom. Doom defeats his doppelganger to save Gwen, who realizes her mistake and hopes for reform. Later, she enlists Doom, Vincent, Doctor Strange, and Terrible Eye to help her friend Cecil regain human form.\nDoctor Doom was featured in his first solo series in 2019. Doom is framed for a Moon space station explosion he had warned about, while also experiencing random encounters with rival supervillain Kang the Conqueror due to a theorized quantum entanglement.\nFictional character biography.\nVictor von Doom was born in Latveria to a tribe of Romani people under the rule of an unnamed nobleman, the Baron. His mother, the witch Cynthia Von Doom, died at the hands of Mephisto when Victor was young. His father, Werner von Doom, a renowned medicine man, kept her sorcery a secret to protect Victor. After Cynthia's death, the Baron's wife fell ill, and when Werner failed to save her, he was labeled a murderer and forced to flee with young Victor. Werner ultimately died of exposure on a mountainside, leaving Victor to discover his mother's occult instruments and swear revenge on the Baron. As Victor grew, he became a brilliant inventor, merging sorcery and technology to defend the Roma people. His exploits attracted the attention of the dean of Empire State University, who offered him a chance to study in the U.S., prompting Victor to leave his homeland and his love, Valeria, behind.\nUpon arriving in the United States, Victor von Doom met Reed Richards, his future rival. He built a machine to communicate with the dead, specifically his mother, but ignored Richards\u2019 warnings about its flaws, leading to a catastrophic explosion that severely damaged his face. Expelled after the incident, Victor traveled until he collapsed on a Tibetan mountainside, where he was rescued by monks. Mastering their disciplines, he forged an iron mask that permanently bonded to his skin, adopting the identity of Doctor Doom. As Doom, he sought revenge on those he held responsible for his accident, particularly Reed Richards, and successfully led a revolution to take over Latveria.\n1960s.\nIn his first appearance, Doctor Doom captures the Invisible Girl, using her as a hostage to force the Fantastic Four to travel back in time to steal Blackbeard's enchanted treasure to help him conquer the world. However, Reed Richards tricks Doom by swapping the treasure for worthless chains. Doom then allies with the Sub-Mariner, who installs a magnetic device in the Baxter Building to pull them into space, aiming to eliminate the Fantastic Four. The Sub-Mariner returns the Baxter Building to New York, leaving Doom stranded on an asteroid. After learning the secrets of the advanced Ovoids, Doom swaps consciousnesses with Mister Fantastic but accidentally switches back, ending up trapped in Sub-Atomica when hit by a shrinking ray he intended for the Fantastic Four. Doom takes over this micro-world but is ousted by the Fantastic Four and thrown into space while trying to send them there. Saved by Rama-Tut, he returns to Earth and uses a special berry juice to turn the Fantastic Four against each other. However, Richards outsmarts Doom with the hallucinogenic juice, leading Doom to believe he has killed him and depart. During the 1960s, Doom attempted to recruit Spider-Man into joining forces with him, and he came into conflict with the Avengers when Quicksilver and Scarlet Witch illegally entered Latveria to find a long-lost relative of theirs. He stole the Silver Surfer's powers in 1967, but lost them after breaching a barrier Galactus had set for the Surfer on Earth.\n1970s and 1980s.\nDuring the 1970s, Doctor Doom expanded into more Marvel titles, featuring a battle for the Latverian throne against Prince Rudolfo in \"Astonishing Tales\". In August 1981, he appeared in \"Iron Man\", where Stark thwarted Doom's time-travelling plan to enlist Morgan le Fay to defeat King Arthur's forces with an army of revived warriors. Stranded in the past due to this interference, Doom vowed revenge, but he had to postpone it to return to the present day.\nDoctor Doom later allies with the Puppet Master to trap the Fantastic Four in the miniature city of \"Liddleville,\" using cybernetic copies of their bodies. However, he sabotages the plan to disrupt Reed's focus, but the Puppet Master ultimately aids the FF in escaping, trapping Doom in the android body he used to monitor them.\nDuring John Byrne's 1980s run, Doctor Doom attempted to steal Terrax the Tamer's cosmic powers, leading to a fight that destroyed his body. He survived by transferring his consciousness to another human and was later restored to his original body by the Beyonder. On Battleworld, Doom briefly succeeded in stealing the Beyonder's power, but it was too vast for him to control, allowing the Beyonder to reclaim it.\n1990s.\nWhen Franklin Richards was kidnapped by Onslaught, Doctor Doom joined forces with the Fantastic Four, Avengers and the X-Men to battle him in Central Park. During the fight, Doom was forced to sacrifice himself alongside others to contain Onslaught, which allowed the X-Men to destroy him. Though believed dead, Doom and the heroes were saved by Franklin, who created a pocket dimension called Counter-Earth, where Doom later uncovered a secret power linked to Franklin and persuaded the boy to relinquish control of the world.\n2000s.\nWhen Susan Richards faced complications with her second pregnancy, Johnny Storm contacted Doctor Doom for help, knowing he couldn't resist the chance to outdo Reed. Doom saved Susan's daughter and cured Johnny's inability to \"flame off\" by channeling Johnny's excess energy into her. Afterward, Doom named the baby \"Valeria\" and plotted to make her his familiar. His lust for power led to him sacrifice his long-lost love Valeria to demons for magical powers equivalent to years of sorcery study. With this power, he trapped Franklin in Hell, immobilized Doctor Strange, and neutralized the Fantastic Four. However, Reed freed Doctor Strange's astral self, allowing them to outsmart Doom and provoke his demonic benefactors to take him to Hell.\nTo eliminate Doom as a threat, Reed took control of Latveria to dismantle his equipment, and planned to trap them both in a pocket dimension. This backfired when the team intervened, leading Doom to transfer his spirit into Sue, Johnny, and Ben. Reed was forced to kill Ben to stop Doom. Doom returned to Hell, and Reed later used a machine Doom had once created to travel to Heaven and restore Ben to life. Doom remained in Hell until he escaped during a dimensional tear caused by Mj\u00f6lnir's fall to Earth, though he focused on rebuilding his power base instead of lifting the hammer. These events were later removed from Marvel continuity in the 2015 \"Secret Wars\".\nLater, a Doombot was defeated by Reed Richards, Hank Pym, Iron Man, and She-Hulk in New York City, raising questions about Doom's involvement. In the midst of the superhero Civil War, he sends a message to Storm and the Black Panther, inviting an alliance between Latveria and Wakanda. In Latveria, Panther spurns the invitation, detonating an EMP that blacked out a local portion of Latveria before Doctor Doom's robots could destroy his ship. It is later revealed that Doom is working with the Red Skull on a weapon, believing it would lead him to become the Baron of Iron, despite his disagreements with the Skull's principles.\nAt the end of the first chapter of the X-Men event \"\", Doom is contacted by Beast to help reverse the effects of Decimation but rejects the offer, admitting he lacks talent in genetics. In \"Spider-Man: One More Day\", Doom is approached by Spider-Man for help in saving Aunt May. Additionally, he transforms Latveria into a refugee camp for Atlanteans after the destruction of their kingdom, and allies with Loki to manipulate his brother into unwittingly releasing his Asgardian allies.\nDoctor Doom later defends Latveria against the Mighty Avengers after it is revealed that one of his satellites carried the 'Venom Virus' released in New York City, a result of hacking by one of Doom's enemies. During a battle with Iron Man and the Sentry, the time travel mechanism in his armor overloads, trapping them all in the past; Doom continues his relationship with Morgan le Fay using his time machine. Although he and Iron Man eventually return to the present, Doom leaves Iron Man in his exploding castle and is falsely incarcerated at the Raft. He later escapes the Raft in the \"Secret Invasion\" storyline, thanks to a virus was uploaded into the prison's systems by the Skrulls.\nAfter the Secret Invasion and the onset of \"Dark Reign,\" Doctor Doom joined the Cabal with Norman Osborn, Emma Frost, Namor, Loki's female form, and the Hood, seeking revenge for his tarnished reputation. Quasimodo researched Doctor Doom for Norman Osborn. He advises Norman to not trust Doctor Doom despite being a man of honesty and to treat his alliance with him with utmost care.\nSoon after, he allies with the isolationist Desturi to seize control of Wakanda. Doom severely injures T'Challa, the Black Panther, aiming to take Wakanda's vibranium for his own enhancement. However, T'Challa destroys the vibranium stockpile, believing his people can survive without it. In \"Fantastic Four\" #566-569, written by Mark Millar, Doctor Doom receives a power upgrade after being thrown back in time by the Marquis of Death. He fights through time to seek revenge, claiming to have rebuilt himself to destroy the Marquis. However, later issues ignore this arc, suggesting it was merely a dream of Valeria von Doom. Doom then joins the supervillain group Intelligencia but is betrayed and captured during their plan. He escapes with Bruce Banner's help and returns to Latveria, seemingly damaged by the experience.\n2010s.\nAt the start of the \"Siege\" storyline, Doom, working with the Cabal, demands that Osborn reverse his actions against Namor, but Osborn refuses. After a violent attack by the Void, it's revealed that the \"Doctor Doom\" present was actually a Doombot, which releases nanites that destroy Avengers Tower and force evacuations. The real Doom warns Osborn not to strike him again, threatening further consequences.\nIt's revealed that the Scarlet Witch at Wundagore Mountain is a Doombot, indicating that the real Wanda was captured by Doom after the House of M event. Wanda's enhanced powers resulted from her and Doom's attempt to channel the Life Force to resurrect her children, which ultimately overwhelmed her. With Wiccan's help, they sought to use the entity possessing Wanda to restore mutant powers, but the Young Avengers intervened, concerned about the consequences. Doom aimed to transfer this entity into himself, gaining god-like powers, but accidentally killed Cassie before Wanda and Wiccan could reclaim those powers from him.\nIn \"Fantastic Four: Three,\" a guilt-ridden Doctor Doom, planning to abdicate his throne to Kristoff, is approached by Valeria, who asks for his help with her father. Noticing Doom's brain damage and memory loss from a previous battle, she offers to restore his mental faculties in exchange for assisting with the Fantastic Four, which he agrees to. Later, Doom attends Johnny Storm's funeral. Due to this agreement, Doom is recommended by Nathaniel and Valeria von Doom to join the Future Foundation. Despite an angry attack from the Thing, Mister Fantastic and the Invisible Woman welcome him. Valeria learns that Kristoff Vernard is Doom's backup for restoring his memories, so they all head to Latveria, where a brain transfer machine successfully restores Doom's knowledge. Although Kristoff offers to return the throne to Doom, he declines, citing a promise to Valeria to help her defeat Mister Fantastic when needed. Doom then plans a symposium to defeat the Council of Reeds\u2014alternate versions of Reed Richards trapped in their universe. Mister Fantastic, Victor, Valeria, and Nathaniel Richards meet with the supervillain geniuses and Uatu the Watcher about what to do with the Council of Reeds.\nAround this time, von Doom performed brain surgery on the Hulk to separate him from Bruce Banner, extracting the uniquely Banner elements from the Hulk's brain and cloning a new body for Banner, in return for a favor from the Hulk. This clone is killed soon afterward. Later, Doom is apparently killed by the Mad Celestials, but survives and creates the Parliament of Doom.\nHe later returns rule Latveria for a millennium. An ill-fated excursion into the alternate universe of the one of Infinity Gauntlets resulted in Reed and Nathaniel Richards rescuing Doom from his own council.\nDuring the confrontation between the Avengers and the X-Men, Doom allies with Magneto and others against Red Skull's Red Onslaught form. In an attempt to atone for past misdeeds, Doom absorbs the Scarlet Witch reality-altering powers and resurrects the dead Cassie Lang, whom he had accidentally killed. He subsequently makes a Faustian deal with an unspecified demon to resurrect Brother Voodoo. After returning to normal, Doom is taken into captivity for his initial killing of Lang.\nAs the final Incursion approaches in the \"Secret Wars\" storyline, Doom usurps the power of the Beyonders with the aid of Doctor Strange and the Molecule Man. He then creates a new Battleworld from the destroyed multiverse, claiming the role of God, and rewriting history to resurrect those he killed, while taking Sue as his wife and assigning roles to Franklin and Valeria. Ultimately, Reed and a group of survivors challenge Doom, and with Molecule Man's help, they restore the multiverse. Reed chooses to heal Doom's face using the Beyonder's power.\nIn the \"All-New, All-Different Marvel\", Doom returns to Latveria and saves Tony Stark by using a sonic attack to incapacitate a group of rebels. He tells Tony he's a new man and gives him one of the Wands of Watoomb for protection against Madame Masque. When more rebels arrive, Doom teleports Iron Man to the Bronx Zoo, then to the Jackpot Club in Chicago to confront a Masque. Realizing she is demonically possessed, Doom has Tony trap her in the Iron Man armor while he exorcises the demon. He disappears before Tony regains consciousness, then later interrupts Tony's breakfast date with Amara to prove he has changed, but Tony remains distrustful and Doom leaves again, not without a little smooch though.\nAfter Tony Stark's defeat by Captain Marvel in \"Civil War II\", Doom discovers his calling to heal the world, reflecting on his dissatisfaction as a god. Inspired by Stark, he establishes Stark's legacy, fights for his brand of justice as the third Iron Man, and later conflicts with Mephisto disguised as Maker. Doom joins the Avengers and conceives a child with Dr. Amara Perera, prompting a group of villains led by the Hood to target him. The final battle occurs when the Hood tries to take over Stark Industries, leading to a confrontation between Doom and the Hood, during which Doom's face is severely burned by a demon. After the villains' defeat, Victor retreats to the ruins of Castle Doom.\nA young woman named Zora Vokuvic breaks into Castle Doom, demanding to see Doctor Doom and insisting that Latveria needs its leader back amid turmoil. Initially resistant, Doom is persuaded when Zora hands him his iconic mask, prompting him to venture out and quell the civil war, vowing to restore the nation with his own strength.\n2020s.\nDoctor Doom is framed for the destruction of the Antlion space station by Symkarian rebels and is killed while on the run, only to be sent back to Earth by Death as her \"greatest servant.\" After fending off assassins including Taskmaster and MODOK, he sends Reed Richards his solution to the black hole threatening Earth and sets off to regain his power. During the \"King in Black\" storyline, Doctor Doom confronts Iron Man during Knull's invasion. Iron Man is bonded with an Extremis-powered Symbiote, and they are attacked by a Symbiote-possessed Santa Claus, revealed to be Mike Dunworthy. Doom seeks to learn from Iron Man's new armor, but is turned down, leaving him to ponder whether Santa Claus could be a Sorcerer Supreme.\nDuring the \"Blood Hunt\" storyline, Doctor Doom puts Latveria on high alert amidst a vampire invasion, ordering border guards to maintain defenses while noting he will have new subjects to attend to. He later approaches Strange Academy students in Madripoor after the disappearance of Agatha Harkness and the Living Darkhold. At the Latverian embassy in Alberia, Doom saves Tatiana Keska from vampires, despite knowing she has grievances against him. Doctor Doom informs Doctor Strange and Clea that Blade is possessed by Varnae and declares they need mages to bring back the Sun, requesting the title of Sorcerer Supreme. After being temporarily granted the title, he casts a spell that removes the Darkforce surrounding Earth, which inadvertently allows vampires to walk in sunlight. However, he then betrays his promise and refuses to return the title, making Doctor Strange disappear.\nIn the \"Venom War\" storyline, Doctor Doom encountered Flexo and provided him with tactics for dealing with Eddie Brock in his King in Black form. This proves useful when Flexo severely injured Eddie Brock at the Grand Garden Arena.\nPowers and abilities.\nConsidered to be one of the smartest men on the Earth, Doom's high level intellect rivals his sworn nemesis Reed Richards. Doom has notably restored the Thing's human form\u2014though Reed Richards also achieved this, he struggled to maintain it. However, Richards managed to process complex calculations to save Kitty Pryde from disintegration, a feat Doom admitted he could not replicate. Doom has leveraged his scientific prowess to steal or replicate the powers of cosmic beings such as the Silver Surfer, the Beyonder, and even Galactus's world-ship. Doom often uses \"Doombots,\" his robot doubles, to retroactively explain his actions or erase events from his history. This device was also used to depict Kristoff Vernard believing himself to be the real Doom for a time.\nIn addition to being a genius level scientist and inventor, Doom is also a very powerful sorcerer with a mastery of dark magic, giving him a unique advantage over his rival, Mister Fantastic. Initially trained by Tibetan monks, Doom's magical powers are later enhanced by his lover Morgan le Fay. The magical spells Doom casts grant him additional powers abilities including energy absorption and projection, technopathy, dimensional travel, healing, and is able to summon hordes of demonic creatures. Doom placed second in a magic tournament held by the ancient sorcerer the Aged Genghis, and after Doctor Strange relinquished the title of Sorcerer Supreme, he acknowledged Doom's potential to assume that role.\nThe alien Ovoids inadvertently taught Doom how to psionically transfer his consciousness into another being through eye contact. However, he rarely employs this ability, as it can revert if his concentration breaks, and he is reluctant to do so due to his ego about his appearance.\nDoom's armor enhances his strength and durability to superhuman levels, allowing him to contend with more physically powerful foes like Spider-Man and the Hulk, although he prefers long-range tactics against stronger opponents. The armor is nearly indestructible, shielding him from various forms of manipulation and housing advanced weaponry, including lasers, a force field generator, and lethal electric shocks. Additionally, his armor supports him with air, food, water, and energy systems for extended periods in extreme environments such as outer space. Even without the armor, Doom is a skilled hand-to-hand combatant, capable of defeating strong opponents due to his knowledge of pressure points and skill with melee weapons.\nMonarch of Latveria.\nAs the absolute monarch of Latveria, Doctor Doom rules the country with an iron fist and has frequently used his political power for his own personal benefit. Doom has reshaped the country in his own image, renaming both the capital city Hassenstadt and Castle Sabbat to Doomstadt and Castle Doom respectively. Doom frequently monitors the citizens of Latveria from Castle Doom and uses his Doombots to maintain order within his nation. Despite his infamous reputation as a supervillain, Doom has diplomatic immunity \u2013 allowing him to escape legal prosecution for most of his crimes he commits outside of Latveria. Doom also has total control of the nation's natural and technological resources, along with its manpower, economy, and military. Though from the outside it seems tyrannical, it seems the Latverian people really do adore Doom, as shown with two of his apprentices, Zora Vukovic, (aka, Victorious) and Kristoff Vernard. He is also known to harbor fugitive supervillains within Latveria as means of protecting them from prosecution, although he only does this for villains who play a part in his schemes. After renouncing his rulership, it is likely he lost this status.\nPsychology.\nDoom's primary weakness is his arrogance, which often blinds him to his own role in his failures. Layla Miller noted that he refuses to accept responsibility for the accident that scarred his face, instead blaming Reed Richards. While Doom typically views himself as superior, he occasionally listens to heroes like Mister Fantastic when it benefits him. Even when allied with others, he often seeks personal gain, as seen when he attempted to steal Thanos\u2019 Infinity Gauntlet during a confrontation with the Titan. Doom adheres to a strict code of honor, keeping his word but often interpreting promises in a self-serving way. For instance, while he may not harm someone directly, he won't stop others from doing so. His sense of honor has led him to save Captain America and spare Spider-Man's life, but he refuses to attack weakened opponents, preferring that any victory over the Fantastic Four come solely from him. Despite his flaws, Doom is devoted to his subjects. When judged by the Panther God of Wakanda, it is revealed that Doom truly wished for a utopian future where humanity thrived, albeit one where he was in power.\nInventions.\nDoctor Victor von Doom's genius in science and technology has allowed him to build numerous devices to handle enemies or acquire greater power. The most notable among them include:\nDoombots.\nMost Doombots are decoys of the real Doctor Doom. They are sometimes depicted without hoods to prevent readers from confusing them with the real Doom. They are used for many missions, typically those where he fears defeat, thus functioning as his version of a Life Model Decoy. The Doombots are programmed to believe themselves to be the real Doctor Doom unless they are in his presence.\nServo-Guards.\nThe Servo-Guards are robots that are programmed to attack the enemies of Doom.\nTime Platform.\nThe Time Platform is Doctor Doom's time machine which features a 10-by-10-foot platform and a control console, allowing transport to any point in Earth's timestream. He can return on his own using his armor's time circuitry, and one such machine was captured by the Fantastic Four, who used it to send Godzilla back in time.\nEmpowerment Device.\nThis unnamed device that was made on Battleworld was used by Doctor Doom to imbue people with superpowers like he did to Mary MacPherran and Marsha Rosenberg when they were turned into Titania and Volcana.\nCultural influence and legacy.\nCritical reception.\n\"UGO Networks\" described Doctor Doom as an iconic figure in Marvel Comics, known for his \"iron mask and emerald cowl\", along with his formidable armor and army of Doombots. George Marston from \"Newsarama\" described Doom as one of the \"best Marvel supervillains,\" highlighting his intelligence, mastery of both science and magic, and his recent return to villainy in the Fantastic Four relaunch. David Harth of \"CBR.com\" referred to Doctor Doom as one of the \"coolest Avengers villains,\" praising his charisma, distinctive speech, impressive armor, and rich backstory that adds depth to his character beyond typical villainy.\nOther versions.\n1602.\nIn Neil Gaiman's alternative-universe tale, \"Marvel 1602\", Dr. Doom is \"Count Otto von Doom\", also known as \"Otto the Handsome\". A mastermind genius of physics and even genetics, von Doom keeps the Four of the Fantastick imprisoned in his castle, continually tapping Richard Reed for knowledge. The Four eventually escape during an attack on Doom's castle by the other heroes of the time, which also leads to the scarring of his face.\nOtto von Doom returns in \"1602: The Fantastick Four\", in which he plans to visit a city beyond the edge of the world, believing they have knowledge that could restore his face. He kidnaps William Shakespeare to record these events.\n2099.\nDoom (Victor von Doom) is a Marvel Comics anti-hero featured in the Marvel 2099 comic book \"Doom 2099\". The character is based on Doctor Doom, created by Stan Lee and Jack Kirby. The comic was written by John Francis Moore for its first two years and by Warren Ellis for its third.\nAge of Apocalypse.\nIn the Age of Apocalypse, Victor von Doom is an agent of the Human High Council and the Head of Security. His facial scar is the result of a mutant uprising in Latveria. Like his 616 counterpart, von Doom remains a ruthless, ambitious and honorable man, though he does not express his counterpart's goal to rule the world.\nAmalgam Comics.\nIn the Amalgam Comics universe, Dr. Doom was a Cadmus scientist with Reed Richards and sabotaged their space project out of jealousy. He also experimented on himself using a gene sample of DC's Doomsday, transforming him into Doctor Doomsday.\nCombat Colin.\nDoctor Doom made an appearance in Marvel UK's \"Combat Colin\" strip. A superheroes convention is attacked by the robotic Steamroller Man. After Combat Colin and his sidekick Semi-Automatic Steve defeats the robot, its controller is revealed to be Doctor Doom, who explains that after years of being defeated by American superheroes he thought (wrongly) he could stand a chance against some incompetent Brits. The final panel shows Doom back in his lair, surrounded by newspaper cuttings detailing his past defeats and wondering how he would do in a fight with Thomas the Tank Engine.\nDoom Supreme.\nIn an alternate reality, Doctor Doom, known as Doom Supreme, became a master of dark arts after losing his beloved Valeria, using her remains to forge his armor through sacrifice. He also sacrificed alternate versions of himself to power his armor and intimidated others into allegiance, claiming to be \"the Doom who breaks other Dooms.\"\nDoom Supreme, after witnessing a battle between the Prehistoric Avengers and a younger Thanos, is advised by Mephisto to form a new Masters of Evil from the worst villains in the Multiverse to conquer it, saving Earth-616 for last. He assembles a powerful group, including Black Skull, Dark Phoenix, and her Berserkers, Ghost Goblin, Kid Thanos, and King Killmonger. They attack various Earths, eventually arriving on Earth-616, where he defeats Orb and sends his team to fight the Avengers. After a climactic battle at Avengers Mountain, where it explodes, Doom Supreme enhances his power by placing a fragment of the Watcher's eye into his own.\nDoom Supreme interacts with a version of Doctor Doom called Doom-Thing. When Doom-Thing threatens him, Doom Supreme subdues him and explains that he collects alternate versions of Doctor Doom, leading Doom-Thing to swear allegiance to him while tasked with cleaning up his mess.\nDoom Supreme arrives on Earth-616 after Agamotto breaks up the Avengers' fight with the Prehistoric Avengers, removes Agamotto's eyes, and kills the Prehistoric Star Brand as the Multiversal Masters of Evil arrive.\nAs most of the Multiversal Masters of Evil are defeated, Dark Phoenix confronts Doom Supreme, only to discover he's just a hologram on Doom the Living Planet, surrounded by his loyal Doctor Doom variants. Doom Supreme arrives at the God Quarry, instructing his variants to hold off the Multiversal Avengers while he casts a spell to claim rights to all existence. During the battle, Avenger Prime, revealed to be a variation of Loki, joins the fray.\nAs the Multiversal Avengers battle the Doctor Doom variants, Doom Supreme begins turning the air into acid, he is confronted by Avenger Prime and Namor. Old Man Phoenix and Echo use their powers to make Doom the Living Planet bleed molten blood, injuring the Doctor Doom variants.\nAmidst the chaos, Avenger Prime breaks off from fighting Doom Supreme due to news of a Celestial-sized Mephisto attacking. Ka-Zar and an alternate Galactus join the fight, while Gorilla-Man and Ursa Major decimate the Doctor Doom variants. As Doom Supreme confronts Dark Phoenix, claiming to have awaited her, they share a tense moment before he turns against Mephisto. Ultimately, the Multiversal Avengers regroup, and apprehend the remaining Doctor Doom variants.\nEarth-111.\nIn this reality, visited by Ben Grimm while attempting to recover the coordinates of the Ultimate Nullifer, divided between the subconscious minds of four alternative Johnny Storms, Doom was the leader of the 'Challengers of Doom' consisting of himself, Reed Richard, Sue Storm, and the Hulk-, with Latveria having been destroyed in an unspecified past disaster and Doom relocating to New York to become a hero. When Galactus came to Earth and landed in Russia, Doom dismissed it as a hoax, prompting Grimm to note that this Doom was more arrogant than the version he knew as the Doom of Earth-616 was at least willing to listen to even his enemies if the situation was serious enough rather than dismiss their views as automatically irrelevant.\nEarth-1191.\nUnlike most Dooms this version is portrayed as an old, senile man with delusions of retaining his old authority and physique- seen by Layla Miller as a natural 'evolution' of Doom's inability to accept that \"he\" might be the reason for his failures-, although with periods of lucidity in which, while physically decrepit, he is still as smart and devious (and in turn, dangerous) as ever. Currently he's been taken in by Layla Miller and Madrox who hope that he can aid them in traveling back to their present. After finding and reactivating a time machine provided by the future X-Men, Doom, while in another lucid moment, betrays them, ordering the time traveling \"Cortex\" to use his abilities to \"Destroy all the mutants\". Doom also is able to take control of the heavily cyberized Cyclops though his cybernetic implants, forcing Cyclops' daughter Ruby to take him down. After this battle, Doom vanishes.\nEarth-691.\nIn the alternate 31st century of the original Guardians of the Galaxy, designated as Earth-691, Doom is revealed to have somehow managed to place his brain inside the adamantium skeleton of Wolverine at some unknown point in the past, hiding this fact with a perfect replica of his original armor, cloak, hood, and simulated flesh intended to give the appearance that he has managed to prolong his life to unnatural lengths. He eventually comes face to face with Rancor, a descendant of Wolverine, and offers to become her ally, though he secretly intended to use her as a pawn for his own gain, while she intends to slay him on her quest to determine what happened to Wolverine. During their battle, Doom reveals himself to be in control of Wolverine's skeleton, heavily modified with cybernetics and missing half of one of the claws, which had eventually come into the possession of Rancor. Rancor manages to strike at one of Doom's robotic eyes, forcing him to retreat. Doom is not seen again in this reality.\nEarth-X.\nIn the dystopian future of Earth-X, Doctor Doom has killed the Invisible Woman and Human Torch, but died in the process. Reed Richards took his place as ruler of Latveria and also wears his armor.\nHeroes Reborn (2021).\nIn the 2021 \"Heroes Reborn\" reality, a variation of Doctor Doom appears as a member of the Masters of Doom and can turn into Doctor Juggernaut using the Gem of Cyttorak. After escaping from the Negative Zone, Doctor Juggernaut attacked the front lawn of the White House and fought Hyperion.\nHouse of M.\nIn the \"House of M\" continuity, Doom rules Latveria, with his mother alive, married to Valeria, and having adopted Kristoff. After Reed Richards's test flight results in the deaths of Reed, Sue, and John Jameson from cosmic radiation, Ben Grimm survives but loses his intelligence. This prompts Doom to form the Fearsome Four with himself, the It (Grimm), the Invincible Woman (Valeria), and the Inhuman Torch (Kristoff). However, due to Doom's arrogance and brutality, the It betrays the team, leading to the deaths of Valeria and Kristoff, and his mother's kidnapping. Left broken, Doom genetically mutates himself, transforming his skin into liquid metal and gaining the ability to form weapons from his hands as he prepares for revenge.\nIron Man 2093.\nIn an alternative future set in 2093, where a reborn King Arthur rules a renewed Camelot, Doom and Iron Man are drawn to stop a plan to destroy most of Earth's population. While Iron Man battles his descendant wielding Excalibur, Doom confronts his future self, who has allied with the future Iron Man to complete a sinister plan. Rejecting his future counterpart, Doom asserts that no amount of power justifies such an affront to his dignity, then kills the older Doom, suggesting a stable time loop where the younger Doom will always defeat his future self.\nMarvel Mangaverse.\nIn \"Marvel Mangaverse\", Doom makes T'Channa, T'Challa's sister, his disciple, she banishes him to another dimension and assumes the identity of Doctor Doom.\nMarvel Two-In-One (2017).\nWhen the Thing and the Human Torch search the multiverse for the missing Richards' family, their first new universe visited is a world where the Thing died in the Fantastic Four's first battle with Galactus. This universe's Doctor Doom saved Earth by transferring his mind into Galactus, with the result that Earth has been spared but Doom's hunger for power has led him to devour every other planet in the universe. With the help of alternate counterparts of Reed Richards and Emma Frost from an undesignated universe, they are able to defeat him by transferring his mind into Emma Frost's body, while she becomes the Life Bringer, inverting Galactus's nature as she uses Galactus's body to restore the planets Doom devoured.\nMarvel Universe vs the Avengers.\nWeeks after a pathogen has turned most of Earth's population into homicidal cannibals, Dr. Doom aids the Avengers in repelling the infected and offers them Doom Stones to prevent further transformations, but demands absolute loyalty in return. Most Avengers reluctantly accept, while Thor returns to Asgard; however, they later discover that the stones do not halt the infection but enhance cognitive function in the cannibals and accelerate the transformation process. Hawkeye uncovers Doom's infection and, after Doom activates the stones' harmful effects, he is ultimately killed by Hawkeye with an arrow tipped with Wolverine's severed adamantium claws.\nMarvel Zombies vs. The Army Of Darkness.\nIn more of an anti-hero role, the Earth-2149 version of Doom is still ruler of Latveria. Doom has fortified his castle to defend against the infected superheroes and refuses help from Nick Fury, and takes in refugees for the purpose of repopulating the planet once the situation is resolved. To the disgust of his allies, it is revealed Doom has only chosen the hardiest breeding stock of the Latverian survivors; there are no elderly people or children. He creates a makeshift portal that will allow escape into other dimensions, should the zombie plague doom the planet. Doom is forced to vaporize the mutant Dazzler and the zombified Enchantress after the latter infects the former, but is then attacked by the zombified Marvel superheroes and infected by Reed Richards, who had infected himself and his teammates on purpose.\nShortly before he turns, a \"dying\" Doom heroically uses his portal to allow the refugees escape to another reality. Despite his defenses, the zombies breaks through once more. With only himself and Ash Williams remaining, he reveals he has been infected by the virus, and cannot go through the portal himself. Though tempted to eat Ash, he resists, as he considers Ash to have allowed him revenge against Reed Richards, allowing the man to escape, even giving him the ability to choose one of many realities. As Ash escapes through it, Doom ultimately destroys the device, trapping himself with the zombies. Having witnessed Doom saving the refugees by using his portal for their escape, the zombified Thing suddenly attacked Doom. He is later seen in New York, as a zombie who attempts to devour Galactus before engaging in battle with the zombified superheroes who succeeded in absorbing Galactus' cosmic powers before him, presumably being killed by them off-screen.\nMC2.\nIn the Fantastic Five series set in the MC2, Doom is revealed to be held captive by the Sub-Mariner for ten years, after the destruction of Atlantis. Doom manages to escape, and uses the same device he once employed to imbue Terrax with the Power Cosmic on his Doombots- unable to use it on himself as his human body would be destroyed from the strain-, and attempts to take over the world. Taking advantage of Doom's desire to prove himself superior, Reed Richards challenges Doom to a psychic duel, using a device that will send the loser's mind to the Crossroads of Infinity. The two are so evenly matched that both are sent to the Crossroads - although Namor notes that it is possible that Reed sacrificed himself to try to save Doom - leaving their bodies as empty shells, although Reed's teammates note that there is always a possibility that the two shall return so long as their bodies remain alive.\nMini-Marvels.\nDoctor Doom makes shortly cameos in Mini-Marvels. In the \"Classic Mini Marvels\" section, there's a short story about him in which he tries to read Marvel's comics. He can be seen in \"Civil Guards\" as one of the doctors that are experimenting with Spider-Man's body. The Avengers also have a photo of Doom in their house, as shown in \"World War Hulk\".\nMutant X.\nIn the Mutant X universe, Doom leads the United Nations in battle against the Goblyn Queen and later the Beyonder.\nAn alternate of Doom with the powers of Charles Xavier is a superhero and leader of his own X-Men.\nOld Man Logan.\nIn the alternative Wolverine-centric future shown on Earth-807128, the supervillains of the Marvel universe finally won and divided America (later renamed Amerika) up amongst themselves. Doctor Doom has his own area of land called New Latveria (also called Doom's Lair). He is seen for only a few panels dressed in all gray standing atop a cliff watching a now old Logan and Hawkeye driving the Spider Buggy built by the Human Torch. It is revealed that Clyde Wyncham has taken on the role of Doom.\nOn Earth-21923 that was similar to Earth-807128, Doctor Doom's history is still the same. After Old Man Logan killed Red Skull and Hulk, a power vacuum was caused in Amerika which led to Doctor Doom taking over the Presidential Quarter. At one point during his takeover of the Presidential Quarter, he came across a village that was established by Baron Mordo. When Doctor Doom cut off Baron Mordo's access to Agatha Harkness, he killed Baron Mordo, took the Darkhold that was in his possession, and freed Sofia Strange and those enslaved to Mordo. In addition, he allowed Agatha to leave with her life while stating that she owes him. When asked by Sofia on what he plans to do with the Darkhold, Doctor Doom states that he is going to use it to rule.\nOld Woman Laura.\nIn an alternate timeline, Doom gathered most of the supervillains on Earth into an army, attempting to conquer the world. When his \"soldiers\" were defeated and imprisoned, Doom retreated to Latveria, creating an impenetrable forcefield over the whole country. Decades later, Wolverine receives word that Doom is holding her clone sister Bellona prisoner, and leads a covert attack to take Doom out once and for all. Doom manages to capture Laura, revealing that he deliberately leaked the intel that drove her to attack. He attempts to transfer his mind into Laura's body in order to escape his own decrepit form, only to discover that Laura is herself dying. Doom returns to his own body, but is killed by Laura immediately after.\nSpider-Man: Life Story.\nIn \"Spider-Man: Life Story\" a reality in which the Marvel superheroes aged in real time and started their careers the same year as their publication, Doctor Doom has taken over Earth as a result of the Civil War between Captain America and Iron Man and a resistance being formed to combat him composed of those that didn't die or disappear entirely.\n\"The End\".\nIn Alan Davis' mini-series, , Dr. Doom appears as a four-armed cyborg with little of his humanity left. Doom breaks during the \"mutant wars\" and is now a killing machine, focused on the deaths of the Fantastic Four. He engages in a final battle with the Four and is seemingly killed in an explosion when his powers react with those of Franklin and Valeria Richards. Doom is later revealed to be alive and conquers the Negative Zone after killing Annihilus and obtaining his power rod and immortality.\n\"The Last Fantastic Four Story\".\nDr. Doom is seen trying to destroy the Adjudicator, however he and his robotic planes are shown to disintegrate and he is last seen cheering the Fantastic Four (even though he will find a way to destroy them). How he survived remains unknown.\nUltimate Marvel.\nIn the Ultimate Marvel universe, Victor Van Damme, a descendant of Vlad Tepes Dracula, is part of the Baxter Building think tank alongside Reed Richards and Susan Storm. He secretly reprograms a teleporter's coordinates, causing the accident that gives the Ultimate Fantastic Four their powers and transforms him into a metallic-skinned figure with clawed hands and goat-hooved legs. When he learns of their transformation, he derides Reed as a \"freak,\" believing himself to be the only one worthy of such power.\nFor a time, Doom leads a micronation in Copenhagen known as \"Free State,\" where citizens live rent-free in exchange for loyalty and receive dragon tattoos that serve as mind control devices. The Fantastic Four eventually liberate the citizens during their first battle, but they cannot imprison Doom due to his diplomatic immunity. During the Namor fiasco, it was revealed that the mother of Sue and Johnny Storm possesses a similar dragon tattoo. After the collapse of the Keep, Doom returned to Latveria and transformed it from a Third World nation into the ninth-richest country in the world within six months. Although the citizens, now bearing Doom's dragon tattoos, revere him as \"the good doctor,\" their loyalty comes at a cost. Meanwhile, Doom switches bodies with Reed Richards to claim a life he believes is rightfully his, leaving Reed in control of Latveria while he seeks recognition by saving Johnny Storm from an alien parasite. Despite Reed's efforts to combat the zombie Fantastic Four, Doom ultimately reverts their bodies and defeats the threat himself.\nDoom seems to reappear in issue six of \"Ultimate Power\", seeking control over the Supremeverse, but is revealed in issue eight to be a Doombot. Later, in \"Ultimates 3,\" he is shown controlling Ultron, though he lacks his distinctive goat legs and resembles his 616 counterpart, with the reason for this change yet to be explained. The Thing supposedly killed Victor in his Latverian home as retribution for Doom's causing the events of \"Ultimatum\"; however' Doom later reveals that while in the zombie universe he had Sue and Johnny Storm's mother, Mary Storm, in disguise ruling in his absence. This was who The Thing unknowingly killed. The Ultimate version of Reed Richards appears to have taken over the role of Doctor Doom.\nMore recently, the Parliament of Doom (an organization constituted by numerous Doctor Dooms of alternative universes led by the Victor von Doom of Earth-616) fought the Fantastic Four in an alternative past of Earth-616, being one of the Doctor Dooms present very similar to Ultimate Doctor Doom, with features such as goat-hooved legs. It is yet unknown if that Doctor Doom is in fact from the Ultimate Universe or an alternative version of that reality.\nFollowing the events of \"Cataclysm\", Phil Coulson and Danny Ketch reveal that the real Victor (who once again has his goat-hooved legs) was being held captive by S.H.I.E.L.D. before it disbanded. He is released and forced to join the Future Foundation alongside Invisible Woman, Falcon, and Tony Stark.\nWhen the time-displaced young X-Men are transferred into the Ultimate Marvel universe by accident, Doom captures and brainwashes the younger Beast to serve him, requiring the displaced X-Men to join forces with the new Ultimate Spider-Man and the Ultimate Marvel X-Men to rescue him.\nLater, it is revealed that Doom teamed up with Hydra to \"save\" the world from corrupt governments, capturing Miles Morales and Jessica Drew to exploit their DNA for a super soldier army. However, Miles unexpectedly taps into a hidden power, defeating Doom and Hydra, just as another Earth's image appears in the sky.\nUltimate Doom is a scientific genius who views science as an art, having transformed his body into solid metal and gained powers similar to Colossus, including the ability to expel his internal organs as poisonous gas and regenerate, though he cannot heal wounds inflicted by his own spikes. He exhibits super strength and can launch metal spikes from his forearms, making him formidable enough to break the arms of a zombie version of Ben Grimm. The Ultimate Universe's Doom is also an accomplished sorcerer.\nVenomverse.\nIn Venomverse, Doom was consumed by the Poisons and became their second in command after Poison Thanos. He with the other Poison Heroes started battling the Army of Venoms in which he was the only survivor. Then the Poisons tried to invade Earth-616, he and Thanos along with the other Poisons got killed, after the Poison Queen was destroyed.\nWarlock.\nOn the original Counter Earth, Victor von Doom is a genius, but not a villain. He appears in the original \"Warlock\" series. Doom's armor is now represented only by the metallic mask. He is referred to as \"the most famous egghead in the country\", the \"like spirit-brother\" of Reed Richards, and \"a man as dedicated to counter-Earth's survival as Earth's von Doom is to its enslavement\".\nHe and Reed Richards are mentioned in passing in \"Marvel Premiere\" #2 and \"Warlock\" #2 as scientists without any sort of super-powers.\nIn \"Warlock\" #6, it is told how von Doom and Richards had been roommates, friends and lab partners in college. After von Doom's disfiguring accident, Richards' emotional support prevented him from becoming a villain. When Richards and his three companions hijacked a spaceship in order to be the first humans in space, von Doom unsuccessfully attempted to talk him out of it. Though all of the counter \"fantastic four\" were exposed to cosmic radiation, it was only Richards that was affected due to the interference of Man-Beast though the effects only became apparent ten years later when he became the Brute due to \"latent cosmic radiation - and a mysterious guiding hand\".\nIn \"Warlock\" #5-7, Victor von Doom was employed at the Livermore Valley Lab in California where he worked on \"Earth-Corer-1\", a vehicle designed to tunnel into the Earth. He warned the president of a massive earthquake that would be caused by a nuclear bomb test. The resulting earthquake accidentally activated \"Antipersonnel missiles, nicknamed deathbirds\" which began killing people indiscriminately. Doom was able to destroy them with Adam Warlock's help, using one of his inventions called the \"deactivator\". He died while helping Adam Warlock stop the Brute from absorbing all of Counter-Earth's geothermal energy, turning him back into Reed Richards.\nWhat If?\nThere are some \"What If\" stories revolving around Doctor Doom:\nKing Loki.\nIn the future depicted in \"Loki: Agent of Asgard\", Doctor Doom discovers Latveria completely destroyed after King Loki destroyed the Earth. Doom attempts to prevent this future by imprisoning the Loki of the present.\nUltimate Universe.\nIn Earth-6160, a world marked by alternate history due to the interference of the Maker, Latveria is an independent territory close to the European Coalition and used as his seat of political power, as he reshapes the global status quo, eroding national identities and independence in favor of a new world order, creating a Council composed of leaders of regional power blocs and creating a permanent war economy between their territories, while they operate covertly as a group, keeping their local populations controlled as a worldwide form of shadow government.\nIt is shown through Howard Stark's history files that the Maker arrived in 1963, establishing the futuristic enclave known as \"The City\" at Latveria, showcasing himself as a force of progress to the world and performing feats such as taking care of external threats like Galactus and performing a moon landing. He is regarded as an Imperator, or the de facto ruler of the Earth, due to being regarded as the only one capable of keeping the public safe from global threats and disseminating technological advances on a regular base, using the organization H.A.N.D as a secret police. As Stark rebels against the Maker, he imprisons both of them and Kang inside the city, leaving two years before it opens.\nKing Thor.\nIn King Thor's timeline, Doom acquired the powers of Iron Fist, Ghost Rider, Star Brand and Doctor Strange. He wanted to destroy the new mortals that appeared in a desolate Earth, but he got attacked by King Thor and Old Man Phoenix. During the battle, he killed Logan, which allowed the Phoenix Force to go to Thor right before Doom was about to use his Penance Stare to Thor. Now with the power of the Phoenix Force, Thor was able to defeat Doom.\nIn other media.\nAs the archenemy of the Fantastic Four, Doctor Doom has appeared in various forms of Marvel-related media and been featured in almost every adaptation of the \"Fantastic Four\" franchise, including films, television series, and video games.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47498", "revid": "35019008", "url": "https://en.wikipedia.org/wiki?curid=47498", "title": "All your base are belong to us", "text": "Internet meme from a video game\n\"All your base are belong to us\" is an Internet meme based on a poorly translated phrase from the opening cutscene of the Japanese video game \"Zero Wing\". The phrase first appeared on the European release of the 1991 Sega Mega Drive conversion of the 1989 Japanese arcade game. \nBy the early 2000s, a GIF animation depicting the opening text became widespread on web forums. A music video accompanied by a techno remix of the clip, originally posted on the website Newgrounds, gained popularity and became a derivative Internet meme in its own right. The original meme has been referenced many times in media outside of the forums.\nThe original phrase in Japanese, uttered by the enigmatic antagonist \"CATS\", is \"\u541b\u9054\u306e\u57fa\u5730\u306f\u3001\u5168\u3066CATS\u304c\u3044\u305f\u3060\u3044\u305f\" (Hepburn Romanization: \"Kimitachi no kichi wa subete CATS ga itadaita\"), which can be translated more accurately as \"All of your bases have been taken over by CATS\" (see the transcript below).\n\"Zero Wing\" transcript.\nBelow are some other examples of text as it appeared in the poorly translated English release, alongside a more accurate translation from the original Japanese. \nHistory.\n\"Zero Wing\" was released in Japanese arcades by developer Toaplan on 1 July 1989, and in North America in April 1990. The European release of the game on the Sega Mega Drive, also known as the Sega Genesis, which contained the script of the meme's origin, occurred in July 1991.\n\"Zero Wing\" is one of the most widely known examples of poor linguistic translation in video games. Toaplan staff Tatsuya Uemura (programmer and music composer of \"Zero Wing\" and Toaplan founding member) and Masahiro Yuge (fellow Toaplan composer and founder) addressed the meme in interviews during the 2010s. According to Uemura, the English translation in the Mega Drive version was handled by a member of the Toaplan design team in charge of export and overseas business, and not by a professional translator.250 Uemura said that the English skills of the team member who prepared the translations were \"really terrible\".250\nThe first references of the meme could be seen in 1999 and the early 2000s when an animated GIF of the scene appeared on forums and sites like Zany Video Game Quotes, OverClocked, and TribalWar forums. In November 2000, Kansas City computer programmer and part-time disc jockey Jeffrey Ray Roberts (1977\u20132011), of the gabber band The Laziest Men on Mars, made a techno dance track, \"Invasion of the Gabber Robots,\" which remixed some of the \"Zero Wing\" video game music with a voice-over of the phrase, \"All your base are belong to us\". (The original music for \"Zero Wing\" was written by Tatsuya Uemura and arranged by Noriyuki Iwadare.) On 16 February 2001, user Bad_CRC posted an animated music video accompanying the song onto the Flash game and animation sharing site Newgrounds. The video was shared rapidly, soon becoming an Internet meme and receiving widespread media attention. The meme's popularity was seen throughout the 2000s when it was broadcast unauthorized onto the ticker of a Raleigh, North Carolina, TV channel, used as a placeholder message by YouTube while under maintenance, and reproduced onto T-shirts.\nThe 15th and 20th anniversaries of the posting of the remix on Newgrounds were recognized by numerous culture sites. The meme has been highlighted for its uniqueness in that, unlike other memes of the time, it lacked sexual innuendos or vulgarity.\nMentions in media.\nThe phrase or some variation of lines from the game has appeared numerous times in films, commercials, news broadcasts, other games, and social media posts.\nOn 1 April 2003, in Sturgis, Michigan, seven people placed signs through the town that read: \"All your base are belong to us. You have no chance to survive make your time.\" They claimed to be playing an April Fools' joke, but most people who saw the signs were unfamiliar with the phrase. Many residents were upset that the signs appeared while the US was at war with Iraq, and police chief Eugene Alli said the signs could be \"a borderline terrorist threat, depending on what someone interprets it to mean\".\nIn February 2004, North Carolina State University students and members of TheWolfWeb in Raleigh, North Carolina, exploited a web-based service used by local schools and businesses to report weather-related closures to display the phrase within a news ticker on a live news broadcast on News 14 Carolina.\nOn 1 June 2006, the phrase \"All Your Video Are Belong to Us\" appeared in all-caps below the YouTube logo as a placeholder while YouTube was under maintenance. Some users believed the site had been hacked, leading YouTube to add the message \"No, we haven't be [\"sic\"] hacked. Get a sense of humor.\"\nIn the E3 2016 demo for the 2017 video game \"\", \"all your base are\" could be seen written in the game's symbol-based fictional language.\nOn 19 January 2019, Democratic Party Congresswoman Alexandria Ocasio-Cortez of New York tweeted \"All your base (are) belong to us\" in response to a poll which found that 45% of Republicans approved of her suggested implementation of a 70% marginal tax rate for individual income over $10 million per year.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47499", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=47499", "title": "Boreal", "text": "Boreal, northern, of the north. Derived from the name of the god of the north wind from Ancient Greek civilisation, Boreas. It may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47500", "revid": "2902776", "url": "https://en.wikipedia.org/wiki?curid=47500", "title": "Northern", "text": "Northern may refer to the following:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47501", "revid": "47684007", "url": "https://en.wikipedia.org/wiki?curid=47501", "title": "Brightness temperature", "text": "Measure of electromagnetic energy\nBrightness temperature or radiance temperature is a measure of the intensity of electromagnetic energy coming from a source. In particular, it is the temperature at which a black body would have to be in order to duplicate the observed intensity of a grey body object at a frequency formula_1.\nThis concept is used in radio astronomy, planetary science, materials science and climatology.\nThe brightness temperature provides \"a more physically recognizable way to describe intensity\".\nWhen the electromagnetic radiation observed is thermal radiation emitted by an object simply by virtue of its temperature, then the actual temperature of the object will always be equal to or higher than the brightness temperature. Since the emissivity is limited by 1, the brightness temperature is a lower bound of the object\u2019s actual temperature.\nFor radiation emitted by a non-thermal source such as a pulsar, synchrotron, maser, or a laser, the brightness temperature may be far higher than the actual temperature of the source. In this case, the brightness temperature is simply a measure of the intensity of the radiation as it would be measured at the origin of that radiation. \nIn some applications, the brightness temperature of a surface is determined by an optical measurement, for example using a pyrometer, with the intention of determining the real temperature. As detailed below, the real temperature of a surface can in some cases be calculated by dividing the brightness temperature by the emissivity of the surface. Since the emissivity is a value between 0 and 1, the real temperature will be greater than or equal to the brightness temperature. At high frequencies (short wavelengths) and low temperatures, the conversion must proceed through Planck's law.\nThe brightness temperature is not a temperature as ordinarily understood. It characterizes radiation, and depending on the mechanism of radiation can differ considerably from the physical temperature of a radiating body (though it is theoretically possible to construct a device which will heat up by a source of radiation with some brightness temperature to the actual temperature equal to brightness temperature). \nNonthermal sources can have very high brightness temperatures. In pulsars the brightness temperature can reach 1030\u00a0K. For the radiation of a helium\u2013neon laser with a power of 1\u00a0mW, a frequency spread \u0394f = 1 GHz, an output aperture of 1\u00a0mm2, and a beam dispersion half-angle of 0.56\u00a0mrad, the brightness temperature would be .\nFor a black body, Planck's law gives:\nformula_2\nwhere formula_3 (the Intensity or Brightness) is the amount of energy emitted per unit surface area per unit time per unit solid angle and in the frequency range between formula_1 and formula_5; formula_6 is the temperature of the black body; formula_7 is the Planck constant; formula_1 is frequency; formula_9 is the speed of light; and formula_10 is the Boltzmann constant.\nFor a grey body the spectral radiance is a portion of the black body radiance, determined by the emissivity formula_11.\nThat makes the reciprocal of the brightness temperature:\nformula_12\nAt low frequency and high temperatures, when formula_13, we can use the Rayleigh\u2013Jeans law:\nformula_14\nso that the brightness temperature can be simply written as:\nformula_15\nIn general, the brightness temperature is a function of formula_1, and only in the case of blackbody radiation it is the same at all frequencies. The brightness temperature can be used to calculate the spectral index of a body, in the case of non-thermal radiation.\nCalculating by frequency.\nThe brightness temperature of a source with known spectral radiance can be expressed as:\nformula_17\nWhen formula_13 we can use the Rayleigh\u2013Jeans law:\nformula_19\nFor narrowband radiation with very low relative spectral linewidth formula_20 and known radiance formula_21 we can calculate the brightness temperature as:\nformula_22\nCalculating by wavelength.\nSpectral radiance of black-body radiation is expressed by wavelength as:\nformula_23\nSo, the brightness temperature can be calculated as:\nformula_24\nFor long-wave radiation formula_25 the brightness temperature is:\nformula_26\nFor almost monochromatic radiation, the brightness temperature can be expressed by the radiance formula_21 and the coherence length formula_28:\nformula_29\nIn oceanography.\nIn oceanography, the microwave brightness temperature, as measured by satellites looking at the ocean surface, depends on salinity as well as on the temperature and roughness (e.g. from wind-driven waves) of the water.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47502", "revid": "1275474609", "url": "https://en.wikipedia.org/wiki?curid=47502", "title": "Calibration", "text": "Check on the accuracy of measurement devices\nIn measurement technology and metrology, calibration is the comparison of measurement values delivered by a device under test with those of a calibration standard of known accuracy. Such a standard could be another measurement device of known accuracy, a device generating the quantity to be measured such as a voltage, a sound tone, or a physical artifact, such as a meter ruler.\nThe outcome of the comparison can result in one of the following:\nStrictly speaking, the term \"calibration\" means just the act of comparison and does not include any subsequent adjustment.\nThe calibration standard is normally traceable to a national or international standard held by a metrology body.\nBIPM Definition.\nThe formal definition of calibration by the International Bureau of Weights and Measures (BIPM) is the following: \"Operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties (of the calibrated instrument or secondary standard) and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication.\"\nThis definition states that the calibration process is purely a comparison, but introduces the concept of measurement uncertainty in relating the accuracies of the device under test and the standard.\nModern calibration processes.\nThe increasing need for known accuracy and uncertainty and the need to have consistent and comparable standards internationally has led to the establishment of national laboratories. In many countries a National Metrology Institute (NMI) will exist which will maintain primary standards of measurement (the main SI units plus a number of derived units) which will be used to provide traceability to customer's instruments by calibration.\nThe NMI supports the metrological infrastructure in that country (and often others) by establishing an unbroken chain, from the top level of standards to an instrument used for measurement. Examples of National Metrology Institutes are NPL in the UK, NIST in the United States, PTB in Germany and many others. Since the Mutual Recognition Agreement was signed it is now straightforward to take traceability from any participating NMI and it is no longer necessary for a company to obtain traceability for measurements from the NMI of the country in which it is situated, such as the National Physical Laboratory in the UK.\nQuality of calibration.\nTo improve the quality of the calibration and have the results accepted by outside organizations it is desirable for the calibration and subsequent measurements to be \"traceable\" to the internationally defined measurement units. Establishing traceability is accomplished by a formal comparison to a standard which is directly or indirectly related to national standards (such as NIST in the USA), international standards, or certified reference materials. This may be done by national standards laboratories operated by the government or by private firms offering metrology services.\nQuality management systems call for an effective metrology system which includes formal, periodic, and documented calibration of all measuring instruments. ISO 9000 and ISO 17025 standards require that these traceable actions are to a high level and set out how they can be quantified.\nTo communicate the quality of a calibration the calibration value is often accompanied by a traceable uncertainty statement to a stated confidence level. This is evaluated through careful uncertainty analysis.\nSome times a DFS (Departure From Spec) is required to operate machinery in a degraded state. Whenever this does happen, it must be in writing and authorized by a manager with the technical assistance of a calibration technician.\nMeasuring devices and instruments are categorized according to the physical quantities they are designed to measure. These vary internationally, e.g., NIST 150-2G in the U.S. and NABL-141 in India. Together, these standards cover instruments that measure various physical quantities such as electromagnetic radiation (RF probes), sound (sound level meter or noise dosimeter), time and frequency (intervalometer), ionizing radiation (Geiger counter), light (light meter), mechanical quantities (limit switch, pressure gauge, pressure switch), and, thermodynamic or thermal properties (thermometer, temperature controller). The standard instrument for each test device varies accordingly, e.g., a dead weight tester for pressure gauge calibration and a dry block temperature tester for temperature gauge calibration.\nInstrument calibration prompts.\nCalibration may be required for the following reasons:\nIn general use, calibration is often regarded as including the process of adjusting the output or indication on a measurement instrument to agree with value of the applied standard, within a specified accuracy. For example, a thermometer could be calibrated so the error of indication or the correction is determined, and adjusted (e.g. via calibration constants) so that it shows the true temperature in Celsius at specific points on the scale. This is the perception of the instrument's end-user. However, very few instruments can be adjusted to exactly match the standards they are compared to. For the vast majority of calibrations, the calibration process is actually the comparison of an unknown to a known and recording the results.\nBasic calibration process.\nPurpose and scope.\nThe calibration process begins with the design of the measuring instrument that needs to be calibrated. The design has to be able to \"hold a calibration\" through its calibration interval. In other words, the design has to be capable of measurements that are \"within engineering tolerance\" when used within the stated environmental conditions over some reasonable period of time. Having a design with these characteristics increases the likelihood of the actual measuring instruments performing as expected.\nBasically, the purpose of calibration is for maintaining the quality of measurement as well as to ensure the proper working of particular instrument.\nIntervals.\nThe exact mechanism for assigning tolerance values varies by country and as per the industry type. The measuring of equipment is manufacturer generally assigns the measurement tolerance, suggests a calibration interval (CI) and specifies the environmental range of use and storage. The using organization generally assigns the actual calibration interval, which is dependent on this specific measuring equipment's likely usage level. The assignment of calibration intervals can be a formal process based on the results of previous calibrations. The standards themselves are not clear on recommended CI values:\n\"ISO 17025\"\n\"A calibration certificate (or calibration label) shall not contain any recommendation on the calibration interval except where this has been agreed with the customer. This requirement may be superseded by legal regulations.\u201d\n\"ANSI/NCSL Z540\"\n\"...shall be calibrated or verified at periodic intervals established and maintained to assure acceptable reliability...\"\n\"ISO-9001\"\n\"Where necessary to ensure valid results, measuring equipment shall...be calibrated or verified at specified intervals, or prior to use...\u201d\n\"MIL-STD-45662A\"\n\"... shall be calibrated at periodic intervals established and maintained to assure acceptable accuracy and reliability...Intervals shall be shortened or may be lengthened, by the contractor, when the results of previous calibrations indicate that such action is appropriate to maintain acceptable reliability.\"\nStandards required and accuracy.\nThe next step is defining the calibration process. The selection of a standard or standards is the most visible part of the calibration process. Ideally, the standard has less than 1/4 of the measurement uncertainty of the device being calibrated. When this goal is met, the accumulated measurement uncertainty of all of the standards involved is considered to be insignificant when the final measurement is also made with the 4:1 ratio. This ratio was probably first formalized in Handbook 52 that accompanied MIL-STD-45662A, an early US Department of Defense metrology program specification. It was 10:1 from its inception in the 1950s until the 1970s, when advancing technology made 10:1 impossible for most electronic measurements.\nMaintaining a 4:1 accuracy ratio with modern equipment is difficult. The test equipment being calibrated can be just as accurate as the working standard. If the accuracy ratio is less than 4:1, then the calibration tolerance can be reduced to compensate. When 1:1 is reached, only an exact match between the standard and the device being calibrated is a completely correct calibration. Another common method for dealing with this capability mismatch is to reduce the accuracy of the device being calibrated.\nFor example, a gauge with 3% manufacturer-stated accuracy can be changed to 4% so that a 1% accuracy standard can be used at 4:1. If the gauge is used in an application requiring 16% accuracy, having the gauge accuracy reduced to 4% will not affect the accuracy of the final measurements. This is called a limited calibration. But if the final measurement requires 10% accuracy, then the 3% gauge never can be better than 3.3:1. Then perhaps adjusting the calibration tolerance for the gauge would be a better solution. If the calibration is performed at 100 units, the 1% standard would actually be anywhere between 99 and 101 units. The acceptable values of calibrations where the test equipment is at the 4:1 ratio would be 96 to 104 units, inclusive. Changing the acceptable range to 97 to 103 units would remove the potential contribution of all of the standards and preserve a 3.3:1 ratio. Continuing, a further change to the acceptable range to 98 to 102 restores more than a 4:1 final ratio.\nThis is a simplified example. The mathematics of the example can be challenged. It is important that whatever thinking guided this process in an actual calibration be recorded and accessible. Informality contributes to tolerance stacks and other difficult to diagnose post calibration problems.\nAlso in the example above, ideally the calibration value of 100 units would be the best point in the gauge's range to perform a single-point calibration. It may be the manufacturer's recommendation or it may be the way similar devices are already being calibrated. Multiple point calibrations are also used. Depending on the device, a zero unit state, the absence of the phenomenon being measured, may also be a calibration point. Or zero may be resettable by the user-there are several variations possible. Again, the points to use during calibration should be recorded.\nThere may be specific connection techniques between the standard and the device being calibrated that may influence the calibration. For example, in electronic calibrations involving analog phenomena, the impedance of the cable connections can directly influence the result.\nManual and automatic calibrations.\nCalibration methods for modern devices can be manual or automatic.\nAs an example, a manual process may be used for calibration of a pressure gauge. The procedure requires multiple steps, to connect the gauge under test to a reference master gauge and an adjustable pressure source, to apply fluid pressure to both reference and test gauges at definite points over the span of the gauge, and to compare the readings of the two. The gauge under test may be adjusted to ensure its zero point and response to pressure comply as closely as possible to the intended accuracy. Each step of the process requires manual record keeping. \nAn automatic pressure calibrator is a device that combines an electronic control unit, a pressure intensifier used to compress a gas such as Nitrogen, a pressure transducer used to detect desired levels in a hydraulic accumulator, and accessories such as liquid traps and gauge fittings. An automatic system may also include data collection facilities to automate the gathering of data for record keeping.\nProcess description and documentation.\nAll of the information above is collected in a calibration procedure, which is a specific test method. These procedures capture all of the steps needed to perform a successful calibration. The manufacturer may provide one or the organization may prepare one that also captures all of the organization's other requirements. There are clearinghouses for calibration procedures such as the Government-Industry Data Exchange Program (GIDEP) in the United States.\nThis exact process is repeated for each of the standards used until transfer standards, certified reference materials and/or natural physical constants, the measurement standards with the least uncertainty in the laboratory, are reached. This establishes the traceability of the calibration.\nSee Metrology for other factors that are considered during calibration process development.\nAfter all of this, individual instruments of the specific type discussed above can finally be calibrated. The process generally begins with a basic damage check. Some organizations such as nuclear power plants collect \"as-found\" calibration data before any routine maintenance is performed. After routine maintenance and deficiencies detected during calibration are addressed, an \"as-left\" calibration is performed.\nMore commonly, a calibration technician is entrusted with the entire process and signs the calibration certificate, which documents the completion of a successful calibration.\nThe basic process outlined above is a difficult and expensive challenge. The cost for ordinary equipment support is generally about 10% of the original purchase price on a yearly basis, as a commonly accepted rule-of-thumb. Exotic devices such as scanning electron microscopes, gas chromatograph systems and laser interferometer devices can be even more costly to maintain.\nThe 'single measurement' device used in the basic calibration process description above does exist. But, depending on the organization, the majority of the devices that need calibration can have several ranges and many functionalities in a single instrument. A good example is a common modern oscilloscope. There easily could be 200,000 combinations of settings to completely calibrate and limitations on how much of an all-inclusive calibration can be automated.\nTo prevent unauthorized access to an instrument tamper-proof seals are usually applied after calibration. The picture of the oscilloscope rack shows these, and prove that the instrument has not been removed since it was last calibrated as they will possible unauthorized to the adjusting elements of the instrument. There also are labels showing the date of the last calibration and when the calibration interval dictates when the next one is needed. Some organizations also assign unique identification to each instrument to standardize the record keeping and keep track of accessories that are integral to a specific calibration condition.\nWhen the instruments being calibrated are integrated with computers, the integrated computer programs and any calibration corrections are also under control.\nHistorical development.\nOrigins.\nThe words \"calibrate\" and \"calibration\" entered the English language as recently as the American Civil War, in descriptions of artillery, thought to be derived from a measurement of the calibre of a gun.\nSome of the earliest known systems of measurement and calibration seem to have been created between the ancient civilizations of Egypt, Mesopotamia and the Indus Valley, with excavations revealing the use of angular gradations for construction. The term \"calibration\" was likely first associated with the precise division of linear distance and angles using a dividing engine and the measurement of gravitational mass using a weighing scale. These two forms of measurement alone and their direct derivatives supported nearly all commerce and technology development from the earliest civilizations until about AD 1800.\nCalibration of weights and distances (c.\u20091100 CE).\nEarly measurement devices were \"direct\", i.e. they had the same units as the quantity being measured. Examples include length using a yardstick and mass using a weighing scale. At the beginning of the twelfth century, during the reign of Henry I (1100-1135), it was decreed that a yard be \"the distance from the tip of the King's nose to the end of his outstretched thumb.\" However, it wasn't until the reign of Richard I (1197) that we find documented evidence.\n\"Assize of Measures\"\n\"Throughout the realm there shall be the same yard of the same size and it should be of iron.\"\nOther standardization attempts followed, such as the Magna Carta (1225) for liquid measures, until the M\u00e8tre des Archives from France and the establishment of the Metric system.\nThe early calibration of pressure instruments.\nOne of the earliest pressure measurement devices was the Mercury barometer, credited to Torricelli (1643), which read atmospheric pressure using Mercury. Soon after, water-filled manometers were designed. All these would have linear calibrations using gravimetric principles, where the difference in levels was proportional to pressure. The normal units of measure would be the convenient inches of mercury or water.\nIn the direct reading hydrostatic manometer design on the right, applied pressure Pa pushes the liquid down the right side of the manometer U-tube, while a length scale next to the tube measures the difference of levels. The resulting height difference \"H\" is a direct measurement of the pressure or vacuum with respect to atmospheric pressure. In the absence of differential pressure both levels would be equal, and this would be used as the zero point. \nThe Industrial Revolution saw the adoption of \"indirect\" pressure measuring devices, which were more practical than the manometer.\nAn example is in high pressure (up to 50 psi) steam engines, where mercury was used to reduce the scale length to about 60 inches, but such a manometer was expensive and prone to damage. This stimulated the development of indirect reading instruments, of which the Bourdon tube invented by Eug\u00e8ne Bourdon is a notable example. \nIn the front and back views of a Bourdon gauge on the right, applied pressure at the bottom fitting reduces the curl on the flattened pipe proportionally to pressure. This moves the free end of the tube which is linked to the pointer. The instrument would be calibrated against a manometer, which would be the calibration standard. For measurement of indirect quantities of pressure per unit area, the calibration uncertainty would be dependent on the density of the manometer fluid, and the means of measuring the height difference. From this other units such as pounds per square inch could be inferred and marked on the scale.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47503", "revid": "2051880", "url": "https://en.wikipedia.org/wiki?curid=47503", "title": "Carbon cycle", "text": "Natural processes of carbon exchange\nThe carbon cycle is a part of the biogeochemical cycle where carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of Earth. Other major biogeochemical cycles include the nitrogen cycle and the water cycle. Carbon is the main component of biological compounds as well as a major component of many rocks such as limestone. The carbon cycle comprises a sequence of events that are key to making Earth capable of sustaining life. It describes the movement of carbon as it is recycled and reused throughout the biosphere, as well as long-term processes of carbon sequestration (storage) to and release from carbon sinks. At 422.7 parts per million (ppm), the global average carbon dioxide has set a new record high in 2024. \nTo describe the dynamics of the carbon cycle, a distinction can be made between the \"fast\" and \"slow\" carbon cycle. The fast cycle is also referred to as the \"biological carbon cycle\". Fast cycles can complete within years, moving substances from atmosphere to biosphere, then back to the atmosphere. Slow or geological cycles (also called deep carbon cycle) can take millions of years to complete, moving substances through the Earth's crust between rocks, soil, ocean and atmosphere. \nHumans have disturbed the carbon cycle for many centuries. They have done so by modifying land use and by mining and burning carbon from ancient organic remains (coal, petroleum and gas). Carbon dioxide in the atmosphere has increased nearly 52% over pre-industrial levels by 2020, resulting in global warming. The increased carbon dioxide has also caused a reduction in the ocean's pH value and is fundamentally altering marine chemistry. Carbon dioxide is critical for photosynthesis.\nMain compartments.\nThe carbon cycle was first described by Antoine Lavoisier and Joseph Priestley, and popularised by Humphry Davy. The global carbon cycle is now usually divided into the following major \"reservoirs of carbon\" (also called carbon pools) interconnected by pathways of exchange:\nThe carbon exchanges between reservoirs occur as the result of various chemical, physical, geological, and biological processes. The ocean contains the largest active pool of carbon near the surface of the Earth.\nThe natural flows of carbon between the atmosphere, ocean, terrestrial ecosystems, and sediments are fairly balanced; so carbon levels would be roughly stable without human influence.\nAtmosphere.\nCarbon in the Earth's atmosphere exists in two main forms: carbon dioxide and methane. Both of these gases absorb and retain heat in the atmosphere and are partially responsible for the greenhouse effect. Methane produces a larger greenhouse effect per volume as compared to carbon dioxide, but it exists in much lower concentrations and is more short-lived than carbon dioxide. Thus, carbon dioxide contributes more to the global greenhouse effect than methane.\nCarbon dioxide is removed from the atmosphere primarily through photosynthesis and enters the terrestrial and oceanic biospheres. Carbon dioxide also dissolves directly from the atmosphere into bodies of water (ocean, lakes, etc.), as well as dissolving in precipitation as raindrops fall through the atmosphere. When dissolved in water, carbon dioxide reacts with water molecules and forms carbonic acid, which contributes to ocean acidity. It can then be absorbed by rocks through weathering. It also can acidify other surfaces it touches or be washed into the ocean.\nHuman activities over the past two centuries have increased the amount of carbon in the atmosphere by nearly 50% as of year 2020, mainly in the form of carbon dioxide, both by modifying ecosystems' ability to extract carbon dioxide from the atmosphere and by emitting it directly, e.g., by burning fossil fuels and manufacturing concrete.\nIn the far future (2 to 3 billion years), the rate at which carbon dioxide is absorbed into the soil via the carbonate\u2013silicate cycle will likely increase due to expected changes in the sun as it ages. The expected increased luminosity of the Sun will likely speed up the rate of surface weathering. This will eventually cause most of the carbon dioxide in the atmosphere to be squelched into the Earth's crust as carbonate. Once the concentration of carbon dioxide in the atmosphere falls below approximately 50 parts per million (tolerances vary among species), C3 photosynthesis will no longer be possible. This has been predicted to occur 600 million years from the present, though models vary.\nOnce the oceans on the Earth evaporate in about 1.1 billion years from now, plate tectonics will very likely stop due to the lack of water to lubricate them. The lack of volcanoes pumping out carbon dioxide will cause the carbon cycle to end between 1 billion and 2 billion years into the future.\nTerrestrial biosphere.\nThe terrestrial biosphere includes the organic carbon in all land-living organisms, both alive and dead, as well as carbon stored in soils. About 500 gigatons of carbon are stored above ground in plants and other living organisms, while soil holds approximately 1,500 gigatons of carbon. Most carbon in the terrestrial biosphere is organic carbon, while about a third of soil carbon is stored in inorganic forms, such as calcium carbonate. Organic carbon is a major component of all organisms living on Earth. Autotrophs extract it from the air in the form of carbon dioxide, converting it to organic carbon, while heterotrophs receive carbon by consuming other organisms.\nBecause carbon uptake in the terrestrial biosphere is dependent on biotic factors, it follows a diurnal and seasonal cycle. In CO2 measurements, this feature is apparent in the Keeling curve. It is strongest in the northern hemisphere because this hemisphere has more land mass than the southern hemisphere and thus more room for ecosystems to absorb and emit carbon.\nCarbon leaves the terrestrial biosphere in several ways and on different time scales. The combustion or respiration of organic carbon releases it rapidly into the atmosphere. It can also be exported into the ocean through rivers or remain sequestered in soils in the form of inert carbon. Carbon stored in soil can remain there for up to thousands of years before being washed into rivers by erosion or released into the atmosphere through soil respiration. Between 1989 and 2008 soil respiration increased by about 0.1% per year. In 2008, the global total of CO2 released by soil respiration was roughly 98 billion tonnes, about 3 times more carbon than humans are now putting into the atmosphere each year by burning fossil fuel (this does not represent a net transfer of carbon from soil to atmosphere, as the respiration is largely offset by inputs to soil carbon). There are a few plausible explanations for this trend, but the most likely explanation is that increasing temperatures have increased rates of decomposition of soil organic matter, which has increased the flow of CO2. The length of carbon sequestering in soil is dependent on local climatic conditions and thus changes in the course of climate change. \nOcean.\nThe ocean can be conceptually divided into a surface layer within which water makes frequent (daily to annual) contact with the atmosphere, and a deep layer below the typical mixed layer depth of a few hundred meters or less, within which the time between consecutive contacts may be centuries. The dissolved inorganic carbon (DIC) in the surface layer is exchanged rapidly with the atmosphere, maintaining equilibrium. Partly because its concentration of DIC is about 15% higher but mainly due to its larger volume, the deep ocean contains far more carbon\u2014it is the largest pool of actively cycled carbon in the world, containing 50 times more than the atmosphere\u2014but the timescale to reach equilibrium with the atmosphere is hundreds of years: the exchange of carbon between the two layers, driven by thermohaline circulation, is slow.\nCarbon enters the ocean mainly through the dissolution of atmospheric carbon dioxide, a small fraction of which is converted into carbonate. It can also enter the ocean through rivers as dissolved organic carbon. It is converted by organisms into organic carbon through photosynthesis and can either be exchanged throughout the food chain or precipitated into the oceans' deeper, more carbon-rich layers as dead soft tissue or in shells as calcium carbonate. It circulates in this layer for long periods of time before either being deposited as sediment or, eventually, returned to the surface waters through thermohaline circulation. \nOceans are basic (with a current pH value of 8.1 to 8.2). The increase in atmospheric CO2 shifts the pH of the ocean towards neutral in a process called ocean acidification. Oceanic absorption of CO2 is one of the most important forms of carbon sequestering. The projected rate of pH reduction could slow the biological precipitation of calcium carbonates, thus decreasing the ocean's capacity to absorb CO2.\nGeosphere.\nThe geologic component of the carbon cycle operates slowly in comparison to the other parts of the global carbon cycle. It is one of the most important determinants of the amount of carbon in the atmosphere, and thus of global temperatures.\nMost of the Earth's carbon is stored inertly in the Earth's lithosphere. Much of the carbon stored in the Earth's mantle was stored there when the Earth formed. Some of it was deposited in the form of organic carbon from the biosphere. Of the carbon stored in the geosphere, about 80% is limestone and its derivatives, which form from the sedimentation of calcium carbonate stored in the shells of marine organisms. The remaining 20% is stored as kerogens formed through the sedimentation and burial of terrestrial organisms under high heat and pressure. Organic carbon stored in the geosphere can remain there for millions of years.\nCarbon can leave the geosphere in several ways. Carbon dioxide is released during the metamorphism of carbonate rocks when they are subducted into the Earth's mantle. This carbon dioxide can be released into the atmosphere and ocean through volcanoes and hotspots. It can also be removed by humans through the direct extraction of kerogens in the form of fossil fuels. After extraction, fossil fuels are burned to release energy and emit the carbon they store into the atmosphere.\nTypes of dynamic.\nThere is a fast and a slow carbon cycle. The fast cycle operates in the biosphere and the slow cycle operates in rocks. The fast or biological cycle can complete within years, moving carbon from atmosphere to biosphere, then back to the atmosphere. The slow or geological cycle may extend deep into the mantle and can take millions of years to complete, moving carbon through the Earth's crust between rocks, soil, ocean and atmosphere.\nThe fast carbon cycle involves relatively short-term biogeochemical processes between the environment and living organisms in the biosphere (see diagram at start of article). It includes movements of carbon between the atmosphere and terrestrial and marine ecosystems, as well as soils and seafloor sediments. The fast cycle includes annual cycles involving photosynthesis and decadal cycles involving vegetative growth and decomposition. The reactions of the fast carbon cycle to human activities will determine many of the more immediate impacts of climate change.\nThe slow (or deep) carbon cycle involves medium to long-term geochemical processes belonging to the rock cycle (see diagram on the right). The exchange between the ocean and atmosphere can take centuries, and the weathering of rocks can take millions of years. Carbon in the ocean precipitates to the ocean floor where it can form sedimentary rock and be subducted into the Earth's mantle. Mountain building processes result in the return of this geologic carbon to the Earth's surface. There the rocks are weathered and carbon is returned to the atmosphere by degassing and to the ocean by rivers. Other geologic carbon returns to the ocean through the hydrothermal emission of calcium ions. In a given year between 10 and 100 million tonnes of carbon moves around this slow cycle. This includes volcanoes returning geologic carbon directly to the atmosphere in the form of carbon dioxide. However, this is less than one percent of the carbon dioxide put into the atmosphere by burning fossil fuels.\nProcesses within fast carbon cycle.\nTerrestrial carbon in the water cycle.\nThe movement of terrestrial carbon in the water cycle is shown in the diagram on the right and explained below:\u200a\nTerrestrial runoff to the ocean.\nTerrestrial and marine ecosystems are chiefly connected through riverine transport, which acts as the main channel through which erosive terrestrially derived substances enter into oceanic systems. Material and energy exchanges between the terrestrial biosphere and the lithosphere as well as organic carbon fixation and oxidation processes together regulate ecosystem carbon and dioxygen (O2) pools.\nRiverine transport, being the main connective channel of these pools, will act to transport net primary productivity (primarily in the form of dissolved organic carbon (DOC) and particulate organic carbon (POC)) from terrestrial to oceanic systems. During transport, part of DOC will rapidly return to the atmosphere through redox reactions, causing \"carbon degassing\" to occur between land-atmosphere storage layers. The remaining DOC and dissolved inorganic carbon (DIC) are also exported to the ocean. In 2015, inorganic and organic carbon export fluxes from global rivers were assessed as 0.50\u20130.70 Pg C y\u22121 and 0.15\u20130.35 Pg C y\u22121 respectively. On the other hand, POC can remain buried in sediment over an extensive period, and the annual global terrestrial to oceanic POC flux has been estimated at 0.20 Gg C y\u22121.\nBiological pump in the ocean.\nThe ocean biological pump is the ocean's biologically driven sequestration of carbon from the atmosphere and land runoff to the deep ocean interior and seafloor sediments. The biological pump is not so much the result of a single process, but rather the sum of a number of processes each of which can influence biological pumping. The pump transfers about 11 billion tonnes of carbon every year into the ocean's interior. An ocean without the biological pump would result in atmospheric CO2 levels about 400 ppm higher than the present day.\nMost carbon incorporated in organic and inorganic biological matter is formed at the sea surface where it can then start sinking to the ocean floor. The deep ocean gets most of its nutrients from the higher water column when they sink down in the form of marine snow. This is made up of dead or dying animals and microbes, fecal matter, sand and other inorganic material.\nThe biological pump is responsible for transforming dissolved inorganic carbon (DIC) into organic biomass and pumping it in particulate or dissolved form into the deep ocean. Inorganic nutrients and carbon dioxide are fixed during photosynthesis by phytoplankton, which both release dissolved organic matter (DOM) and are consumed by herbivorous zooplankton. Larger zooplankton - such as copepods, egest fecal pellets - which can be reingested, and sink or collect with other organic detritus into larger, more-rapidly-sinking aggregates. DOM is partially consumed by bacteria and respired; the remaining refractory DOM is advected and mixed into the deep sea. DOM and aggregates exported into the deep water are consumed and respired, thus returning organic carbon into the enormous deep ocean reservoir of DIC.\nA single phytoplankton cell has a sinking rate around one metre per day. Given that the average depth of the ocean is about four kilometres, it can take over ten years for these cells to reach the ocean floor. However, through processes such as coagulation and expulsion in predator fecal pellets, these cells form aggregates. These aggregates have sinking rates orders of magnitude greater than individual cells and complete their journey to the deep in a matter of days.\nAbout 1% of the particles leaving the surface ocean reach the seabed and are consumed, respired, or buried in the sediments. The net effect of these processes is to remove carbon in organic form from the surface and return it to DIC at greater depths, maintaining a surface-to-deep ocean gradient of DIC. Thermohaline circulation returns deep-ocean DIC to the atmosphere on millennial timescales. The carbon buried in the sediments can be subducted into the earth's mantle and stored for millions of years as part of the slow carbon cycle (see next section).\nViruses as regulators.\nViruses act as \"regulators\" of the fast carbon cycle because they impact the material cycles and energy flows of food webs and the microbial loop. The average contribution of viruses to the Earth ecosystem carbon cycle is 8.6%, of which its contribution to marine ecosystems (1.4%) is less than its contribution to terrestrial (6.7%) and freshwater (17.8%) ecosystems. Over the past 2,000 years, anthropogenic activities and climate change have gradually altered the regulatory role of viruses in ecosystem carbon cycling processes. This has been particularly conspicuous over the past 200 years due to rapid industrialization and the attendant population growth.\nProcesses within slow carbon cycle.\nSlow or deep carbon cycling is an important process, though it is not as well-understood as the relatively fast carbon movement through the atmosphere, terrestrial biosphere, ocean, and geosphere. The deep carbon cycle is intimately connected to the movement of carbon in the Earth's surface and atmosphere. If the process did not exist, carbon would remain in the atmosphere, where it would accumulate to extremely high levels over long periods of time. Therefore, by allowing carbon to return to the Earth, the deep carbon cycle plays a critical role in maintaining the terrestrial conditions necessary for life to exist.\nFurthermore, the process is also significant simply due to the massive quantities of carbon it transports through the planet. In fact, studying the composition of basaltic magma and measuring carbon dioxide flux out of volcanoes reveals that the amount of carbon in the mantle is actually greater than that on the Earth's surface by a factor of one thousand. Drilling down and physically observing deep-Earth carbon processes is evidently extremely difficult, as the lower mantle and core extend from 660 to 2,891\u00a0km and 2,891 to 6,371 \u00a0km deep into the Earth respectively. Accordingly, not much is conclusively known regarding the role of carbon in the deep Earth. Nonetheless, several pieces of evidence\u2014many of which come from laboratory simulations of deep Earth conditions\u2014have indicated mechanisms for the element's movement down into the lower mantle, as well as the forms that carbon takes at the extreme temperatures and pressures of said layer. Furthermore, techniques like seismology have led to a greater understanding of the potential presence of carbon in the Earth's core.\nCarbon in the lower mantle.\nCarbon principally enters the mantle in the form of carbonate-rich sediments on tectonic plates of ocean crust, which pull the carbon into the mantle upon undergoing subduction. Not much is known about carbon circulation in the mantle, especially in the deep Earth, but many studies have attempted to augment our understanding of the element's movement and forms within the region. For instance, a 2011 study demonstrated that carbon cycling extends all the way to the lower mantle. The study analyzed rare, super-deep diamonds at a site in Juina, Brazil, determining that the bulk composition of some of the diamonds' inclusions matched the expected result of basalt melting and crystallisation under lower mantle temperatures and pressures. Thus, the investigation's findings indicate that pieces of basaltic oceanic lithosphere act as the principle transport mechanism for carbon to Earth's deep interior. These subducted carbonates can interact with lower mantle silicates, eventually forming super-deep diamonds like the one found.\nHowever, carbonates descending to the lower mantle encounter other fates in addition to forming diamonds. In 2011, carbonates were subjected to an environment similar to that of 1800\u00a0km deep into the Earth, well within the lower mantle. Doing so resulted in the formations of magnesite, siderite, and numerous varieties of graphite. Other experiments\u2014as well as petrologic observations\u2014support this claim, indicating that magnesite is actually the most stable carbonate phase in most part of the mantle. This is largely a result of its higher melting temperature. Consequently, scientists have concluded that carbonates undergo reduction as they descend into the mantle before being stabilised at depth by low oxygen fugacity environments. Magnesium, iron, and other metallic compounds act as buffers throughout the process. The presence of reduced, elemental forms of carbon like graphite would indicate that carbon compounds are reduced as they descend into the mantle.\nPolymorphism alters carbonate compounds' stability at different depths within the Earth. To illustrate, laboratory simulations and density functional theory calculations suggest that tetrahedrally coordinated carbonates are most stable at depths approaching the core\u2013mantle boundary. A 2015 study indicates that the lower mantle's high pressure causes carbon bonds to transition from sp2 to sp3 hybridised orbitals, resulting in carbon tetrahedrally bonding to oxygen. CO3 trigonal groups cannot form polymerisable networks, while tetrahedral CO4 can, signifying an increase in carbon's coordination number, and therefore drastic changes in carbonate compounds' properties in the lower mantle. As an example, preliminary theoretical studies suggest that high pressure causes carbonate melt viscosity to increase; the melts' lower mobility as a result of its increased viscosity causes large deposits of carbon deep into the mantle.\nAccordingly, carbon can remain in the lower mantle for long periods of time, but large concentrations of carbon frequently find their way back to the lithosphere. This process, called carbon outgassing, is the result of carbonated mantle undergoing decompression melting, as well as mantle plumes carrying carbon compounds up towards the crust. Carbon is oxidised upon its ascent towards volcanic hotspots, where it is then released as CO2. This occurs so that the carbon atom matches the oxidation state of the basalts erupting in such areas.\nCarbon in the core.\nAlthough the presence of carbon in the Earth's core is well-constrained, recent studies suggest large inventories of carbon could be stored in this region. Shear (S) waves moving through the inner core travel at about fifty percent of the velocity expected for most iron-rich alloys. Because the core's composition is believed to be an alloy of crystalline iron and a small amount of nickel, this seismic anomaly indicates the presence of light elements, including carbon, in the core. In fact, studies using diamond anvil cells to replicate the conditions in the Earth's core indicate that iron carbide (Fe7C3) matches the inner core's wave speed and density. Therefore, the iron carbide model could serve as an evidence that the core holds as much as 67% of the Earth's carbon. Furthermore, another study found that in the pressure and temperature condition of the Earth's inner core, carbon dissolved in iron and formed a stable phase with the same Fe7C3 composition\u2014albeit with a different structure from the one previously mentioned. In summary, although the amount of carbon potentially stored in the Earth's core is not known, recent studies indicate that the presence of iron carbides can explain some of the geophysical observations.\nHuman influence on fast carbon cycle.\nSince the Industrial Revolution, and especially since the end of WWII, human activity has substantially disturbed the global carbon cycle by redistributing massive amounts of carbon from the geosphere. Humans have also continued to shift the natural component functions of the terrestrial biosphere with changes to vegetation and other land use. Man-made (synthetic) carbon compounds have been designed and mass-manufactured that will persist for decades to millennia in air, water, and sediments as pollutants. Climate change is amplifying and forcing further indirect human changes to the carbon cycle as a consequence of various positive and negative feedbacks.\nClimate change.\nCurrent trends in climate change lead to higher ocean temperatures and acidity, thus modifying marine ecosystems. Also, acid rain and polluted runoff from agriculture and industry change the ocean's chemical composition. Such changes can have dramatic effects on highly sensitive ecosystems such as coral reefs, thus limiting the ocean's ability to absorb carbon from the atmosphere on a regional scale and reducing oceanic biodiversity globally.\nThe exchanges of carbon between the atmosphere and other components of the Earth system, collectively known as the carbon cycle, currently constitute important negative (dampening) feedbacks on the effect of anthropogenic carbon emissions on climate change. Carbon sinks in the land and the ocean each currently take up about one-quarter of anthropogenic carbon emissions each year.\nThese feedbacks are expected to weaken in the future, amplifying the effect of anthropogenic carbon emissions on climate change. The degree to which they will weaken, however, is highly uncertain, with Earth system models predicting a wide range of land and ocean carbon uptakes even under identical atmospheric concentration or emission scenarios. Arctic methane emissions indirectly caused by anthropogenic global warming also affect the carbon cycle and contribute to further warming.\nFossil carbon extraction and burning.\nThe largest and one of the fastest growing human impacts on the carbon cycle and biosphere is the extraction and burning of fossil fuels, which directly transfer carbon from the geosphere into the atmosphere. Carbon dioxide is also produced and released during the calcination of limestone for clinker production. Clinker is an industrial precursor of cement.\nAs of 2020[ [update]], about 450 gigatons of fossil carbon have been extracted in total; an amount approaching the carbon contained in all of Earth's living terrestrial biomass. Recent rates of global emissions directly into the atmosphere have exceeded the uptake by vegetation and the oceans. These sinks have been expected and observed to remove about half of the added atmospheric carbon within about a century. Nevertheless, sinks like the ocean have evolving saturation properties, and a substantial fraction (20\u201335%, based on coupled models) of the added carbon is projected to remain in the atmosphere for centuries to millennia.\nHalocarbons.\nHalocarbons are less prolific compounds developed for diverse uses throughout industry; for example as solvents and refrigerants. Nevertheless, the buildup of relatively small concentrations (parts per trillion) of chlorofluorocarbon, hydrofluorocarbon, and perfluorocarbon gases in the atmosphere is responsible for about 10% of the total direct radiative forcing from all long-lived greenhouse gases (year 2019); which includes forcing from the much larger concentrations of carbon dioxide and methane. Chlorofluorocarbons also cause stratospheric ozone depletion. International efforts are ongoing under the Montreal Protocol and Kyoto Protocol to control rapid growth in the industrial manufacturing and use of these environmentally potent gases. For some applications more benign alternatives such as hydrofluoroolefins have been developed and are being gradually introduced.\nLand use changes.\nSince the invention of agriculture, humans have directly and gradually influenced the carbon cycle over century-long timescales by modifying the mixture of vegetation in the terrestrial biosphere. Over the past several centuries, direct and indirect human-caused land use and land cover change (LUCC) has led to the loss of biodiversity, which lowers ecosystems' resilience to environmental stresses and decreases their ability to remove carbon from the atmosphere. More directly, it often leads to the release of carbon from terrestrial ecosystems into the atmosphere.\nDeforestation for agricultural purposes removes forests, which hold large amounts of carbon, and replaces them, generally with agricultural or urban areas. Both of these replacement land cover types store comparatively small amounts of carbon so that the net result of the transition is that more carbon stays in the atmosphere. However, the effects on the atmosphere and overall carbon cycle can be intentionally and/or naturally reversed with reforestation.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47505", "revid": "50561845", "url": "https://en.wikipedia.org/wiki?curid=47505", "title": "Queen (chess)", "text": "Chess piece\nThe queen (\u2655, \u265b) is the most powerful piece in the game of chess. It can move any number of squares vertically, horizontally or &lt;dfn id=\"\"&gt;diagonally&lt;/dfn&gt;, combining the powers of the rook and bishop. Each player starts the game with one queen, placed in the middle of the first &lt;dfn id=\"\"&gt;rank&lt;/dfn&gt; next to the king. Because the queen is the strongest piece, a pawn is promoted to a queen in the vast majority of cases; if a pawn is promoted to a piece other than a queen, it is an underpromotion.\nThe predecessor to the queen is the \"ferz\", a weak piece only able to move or capture one step diagonally, originating from the Persian game of shatranj. The queen acquired its modern move in Spain in the 15th\u00a0century.\n&lt;templatestyles src=\"Template:TOC_left/styles.css\" /&gt;\nPlacement and movement.\nThe white queen starts on d1, while the black queen starts on d8. With the chessboard oriented correctly, the white queen starts on a white square and the black queen starts on a black square\u2014thus the mnemonics \"queen gets her color\", \"queen on [her] [own] color\", or \"the dress [queen piece] matches the shoes [square]\" (Latin: \"servat r\u0113g\u012bna col\u014drem\").\nThe queen can be moved any number of unoccupied squares in a straight line vertically, horizontally, or diagonally, thus combining the moves of the rook and bishop. The queen captures by moving to the square on which an enemy piece stands.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nAlthough both players start with one queen each, a pawn can be promoted to any of several types of pieces, including a queen, when the pawn is moved to the player's furthest rank (the opponent's &lt;dfn id=\"\"&gt;first rank&lt;/dfn&gt;). Such a queen created by promotion can be an additional or replacement queen. The queen is by far the most common piece type that a pawn is promoted to due to the relative power of a queen. Promotion to a queen is colloquially called \"queening\".\nDimensions &amp; Design.\nThe queen is traditionally the second tallest piece in a chess set after the king. It is distinguished by a stylized crown or coronet with multiple points or bumps, often topped with an orb in the center. In the Staunton design, the queen has a wide, stable base and a body that tapers upwards, often with a vase-like shape.\nPiece value.\nThe queen is typically worth about nine pawns, which is slightly stronger than a rook and a bishop together, but slightly weaker than two rooks, though there are exceptions. It is almost always disadvantageous to exchange the queen for a single piece other than the enemy's queen.\nThe reason that the queen is stronger than a combination of a rook and a bishop, even though they control the same number of squares, is twofold. First, the queen is more mobile than the rook and the bishop, as the entire power of the queen can be transferred to another location in one move, while transferring the entire firepower of a rook and bishop requires two moves, the bishop always being restricted to squares of one color. Second, unlike the bishop, the queen is not hampered by an inability to control squares of the opposite color to the square on which it stands. A factor in favor of the rook and bishop together is that they can attack (or defend) a square twice, whereas a queen can do so only once. However, experience has shown that this factor is usually less significant than the points favoring the queen.\nThe queen is strongest when the board is &lt;dfn id=\"\"&gt;open&lt;/dfn&gt;, the enemy king is poorly defended, or there are \"loose\" (i.e., undefended) pieces in the enemy camp. Because of its long range and ability to move in multiple directions, the queen is well-equipped to execute forks. Compared to other long-range pieces (i.e., rooks and bishops), the queen is less restricted and stronger in &lt;dfn id=\"\"&gt;closed&lt;/dfn&gt; positions.\nStrategy.\nA player should generally delay developing the queen, as developing it too quickly can expose it to attacks by enemy pieces, causing the player to lose time removing the queen from danger. Despite this, beginners often &lt;dfn id=\"\"&gt;develop&lt;/dfn&gt; the queen early in the game, hoping to plunder the enemy position and deliver an early checkmate, such as the scholar's mate.\nEarly queen attacks are rare in high-level chess, but there are some openings with early queen development that are used by high-level players. For example, the Scandinavian Defense (1.e4 d5), which often features queen moves by Black on the second and third moves, is considered sound and has been played at the world championship level. Some less common examples have also been observed in high-level games. The Danvers Opening (1.e4 e5 2.Qh5), which is widely characterized as a beginner's opening, has occasionally been played by the American grandmaster Hikaru Nakamura.\nA queen exchange often marks the beginning of the endgame, but there are queen endgames, and sometimes queens are exchanged in the opening, long before the endgame. A common goal in the endgame is to promote a pawn to a queen. As the queen has the largest range and mobility, queen and king vs. lone king is an easy win when compared to some other basic mates. Queen and king vs. rook and king is also a win for the player with the queen, but it is not easy.\nQueen sacrifice.\nA \"queen sacrifice\" is the deliberate sacrifice of a queen in order to gain a more favorable tactical position. One of the most widely known examples of this was in the game Anderssen\u2013Kieseritzky, 1851, where Anderssen sacrificed a queen (along with three other pieces) to reach checkmate.\nHistory.\nThe queen was originally the counsellor or prime minister or vizier (Sanskrit \"mantri\", Persian \"farz\u012bn\", Arabic \"firz\u0101n\", \"firz\" or \"waz\u012br\"). Initially, its only move was one square diagonally. Around 1300, its abilities were enhanced to allow it to jump two squares diagonally (onto a same-colored square) for its first move.\nThe \"fers\" changed into the queen over time. The first surviving mention of this piece as a queen or similar is the Latin in the \"Einsiedeln Poem\", a 98-line Medieval Latin poem written around 997 and preserved in a monastery at Einsiedeln in Switzerland. Some surviving early medieval pieces depict the piece as a queen. The word \"fers\" became grammatically feminized in several languages, such as \"alferza\" in Spanish and \"fierce\" or \"fierge\" in French. The \"Carmina Burana\" also refer to the queen as \"femina\" (woman) and \"coniunx\" (spouse), and the name \"amazon\" has sometimes been seen.\nDuring the great chess reform at the end of the 15th century, Catholic nations kept using an equivalent of Latin \"domina\" (\"lady\"), such as \"dama\" in Spanish, \"donna\" in Italy, and \"dame\" in France, all of which evoke \"Our Lady\". Protestant nations such as Germany and England, however, refused any derivatives of \"domina\" as it might have suggested some cult of the Virgin Mary and instead opted for secular terms such as \"K\u00f6nigin\" in German and \"queen\" in English.\nIn Russian, the piece keeps its Persian name of \"ferz\"; \"koroleva\" (queen) is colloquial and is never used by professional chess players. The names \"korolevna\" (king's daughter), \"tsaritsa\" (tsar's wife), and \"baba\" (old woman), however, are attested as early as 1694. In Arabic countries, the queen remains termed and, in some cases, depicted as a vizier.\nHistorian Marilyn Yalom proposes several factors that might have been partly responsible for influencing the piece towards its identity as a queen and its power in modern chess: the prominence of medieval queens such as Eleanor of Aquitaine, Blanche of Castile, and more particularly Isabella I of Castile; the cult of the Virgin Mary; the power ascribed to women in the troubadour tradition of courtly love; and the medieval popularity of chess as a game particularly suitable for women to play on equal terms with men. She points to medieval poetry depicting the Virgin as the chess-queen of God or \"Fierce Dieu\".\nSignificantly, the earliest surviving treatise to describe the modern movement of the queen (as well as the bishop and pawn), \"Repetici\u00f3n de amores e arte de axedres con CL iuegos de partido\" (\"Discourses on Love and the Art of Chess with 150 Problems\") by Luis Ram\u00edrez de Lucena, was published during the reign of Isabella I of Castile. Even before that, the Valencian poem \"Scachs d'amor\" (\"Chess of Love\") depicted a chess game between Francesc de Castellv\u00ed and Narc\u00eds de Vinyoles and commented on by Bernat Fenollar, which clearly had the modern moves of the queen and the bishop. Well before the queen's powers expanded, it was already being romantically described as essential to the king's survival, so that when the queen was lost, there was nothing more of value on the board.\nMarilyn Yalom wrote that:\nDuring the 15th century, the queen's move took its modern form as a combination of the move of the rook and the current move of the bishop. Starting from Spain, this new version\u00a0\u2013 called \"queen's chess\" (in Italian, \"scacchi della donna\") or, pejoratively, \"madwoman's chess\" (\"scacchi alla rabiosa\")\u00a0\u2013 spread throughout Europe rapidly, partly due to the advent of the printing press and the popularity of new books on chess. The new rules faced a backlash in some quarters, ranging from anxiety over a powerful female warrior figure to frank abuse against women in general.\nAt various times, the ability of pawns to be queened was restricted while the original queen was still on the board, so as not to cause scandal by providing the king with more than one queen. An early 12th-century Latin poem refers to a queened pawn as a \"ferzia\", as opposed to the original queen or \"regina\", to account for this.\nWhen the queen was attacked, it was customary to warn the opponent by announcing \"\"gardez la reine\" or simply \"gardez\"\", similar to the announcement of \"check\". Some published rules even required this announcement before the queen could be legally captured. This custom was largely abandoned in the 19th\u00a0century.\nIn Russia, for a long time, the queen could also move like a knight; some players disapproved of this ability to \"gallop like the horse\" (knight). The book \"A History of Chess\" by H.J.R. Murray, says that William Coxe, who was in Russia in 1772, saw chess played with the queen also moving like a knight. Such an augmented queen piece is now known as the fairy chess piece \"amazon\".\nAround 1230, the queen was also independently invented as a piece in Japan, where it formed part of the game of dai shogi. The piece was retained in the smaller and more popular chu shogi, but does not form a part of modern shogi.\nNomenclature.\nIn most languages the piece is known as \"queen\" or \"lady\" (e.g. Italian \"regina\" or Spanish \"dama\"). Asian and Eastern European languages tend to refer to it as \"vizier\", minister or advisor (e.g. Arabic/Persian \u0648\u0632\u06cc\u0631 \"wazir\" (vazir), Russian/Persian \u0444\u0435\u0440\u0437\u044c/\u0641\u0631\u0632 \"ferz\"). In Polish it is known as the \"hetman\", the name of a major historical military-political office, while in Estonian it is called \"lipp\" (\"flag\", \"standard\").\nUnicode.\nUnicode defines three codepoints for a queen:\n\u2655 U+2655 White Chess Queen\n\u265b U+265B Black Chess Queen\n\ud83e\ude01 U+1FA01 Neutral Chess Queen\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47506", "revid": "45595768", "url": "https://en.wikipedia.org/wiki?curid=47506", "title": "520s BC", "text": "Decade\nThis article concerns the period 529 BC \u2013 520 BC.\nDecade\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47507", "revid": "38001712", "url": "https://en.wikipedia.org/wiki?curid=47507", "title": "510s BC", "text": "Decade\nThis article concerns the period 519 BC \u2013 510 BC.\nDecade\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47510", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=47510", "title": "Cirrus cloud", "text": "Genus of atmospheric cloud\nCirrus (cloud classification symbol: Ci) is a genus of high cloud made of ice crystals. Cirrus clouds typically appear delicate and wispy with white strands. In the Earth's atmosphere, cirrus are usually formed when warm, dry air rises, causing water vapor deposition onto mineral dust and metallic particles at high altitudes. Globally, they form anywhere between above sea level, with the higher elevations usually in the tropics and the lower elevations in more polar regions.\nCirrus clouds can form from the tops of thunderstorms and tropical cyclones and sometimes predict the arrival of rain or storms. Although they are a sign that rain and maybe storms are on the way, cirrus themselves drop no more than falling streaks of ice crystals. These crystals dissipate, melt, and evaporate as they fall through warmer and drier air and never reach the ground. The word \"cirrus\" comes from the Latin prefix \"cirro-\", meaning \"tendril\" or \"curl\". Cirrus clouds warm the earth, potentially contributing to climate change. A warming earth will likely produce more cirrus clouds, potentially resulting in a self-reinforcing loop.\nOptical phenomena, such as sun dogs and halos, can be produced by light interacting with ice crystals in cirrus clouds. There are two other high-level cirrus-like clouds called cirrostratus and cirrocumulus. Cirrostratus looks like a sheet of cloud, whereas cirrocumulus looks like a pattern of small cloud tufts. Unlike cirrus and cirrostratus, cirrocumulus clouds contain droplets of supercooled (below freezing point) water.\nCirrus clouds form in the atmospheres of Mars, Jupiter, Saturn, Uranus, and Neptune; and on Titan, one of Saturn's larger moons. Some of these extraterrestrial cirrus clouds are made of ammonia or methane, much like water ice in cirrus on Earth. Some interstellar clouds, made of grains of dust smaller than a thousandth of a millimeter, are also called \"cirrus\".\nDescription.\nCirrus are wispy clouds made of long strands of ice crystals that are described as feathery, hair-like, or layered in appearance. First defined scientifically by Luke Howard in an 1803 paper, their name is derived from the Latin word \"cirrus\", meaning 'curl' or 'fringe'. They are transparent, meaning that the sun can be seen through them. Ice crystals in the clouds cause them to usually appear white, but the rising or setting sun can color them various shades of yellow or red. At dusk, they can appear gray.\nCirrus comes in five visually-distinct species: castellanus, fibratus, floccus, spissatus, and uncinus:\nEach species is divided into up to four varieties: intortus, vertebratus, radiatus, and duplicatus:\nCirrus clouds often produce hair-like filaments called fall streaks, made of heavier ice crystals that fall from the cloud. These are similar to the virga produced in liquid\u2013water clouds. The sizes and shapes of fall streaks are determined by the wind shear.\nCirrus cloud cover varies diurnally. During the day, cirrus cloud cover drops, and during the night, it increases. Based on CALIPSO satellite data, cirrus covers an average of 31% to 32% of the Earth's surface. Cirrus cloud cover varies wildly by location, with some parts of the tropics reaching up to 70% cirrus cloud cover. Polar regions, on the other hand, have significantly less cirrus cloud cover, with some areas having a yearly average of only around 10% coverage. These percentages treat clear days and nights, as well as days and nights with other cloud types, as lack of cirrus cloud cover.\nFormation.\nCirrus clouds are usually formed as warm, dry air rises, causing water vapor to undergo deposition onto particles, including mostly mineral dust and metallic particles at high altitudes. Particles gathered by research aircraft from cirrus clouds over several locations above North America and Central America included mineral dust (containing aluminum, potassium, calcium, iron, and silicon), metallic particles in elemental, sulfate and oxide forms (containing sodium, potassium, iron, nickel, copper, zinc, tin, silver, molybdenum and lead), possible biological particles (containing oxygen, carbon, nitrogen and phosphorus) and elemental carbon. The authors concluded that mineral dust contributed the largest number of ice nuclei to cirrus cloud formation.\nThe average cirrus cloud altitude increases as latitude decreases, but the altitude is always capped by the tropopause. These conditions commonly occur at the leading edge of a warm front. Because absolute humidity is low at such high altitudes, this genus tends to be fairly transparent. Cirrus clouds can also form inside fallstreak holes (also called \"cavum\").\nAt latitudes of 65\u00b0\u00a0N or S, close to polar regions, cirrus clouds form, on average, only above sea level. In temperate regions, at roughly 45\u00b0\u00a0N or S, their average altitude increases to above sea level. In tropical regions, at roughly 5\u00b0\u00a0N or S, cirrus clouds form above sea level on average. Across the globe, cirrus clouds can form anywhere from above sea level. Cirrus clouds form with a vast range of thicknesses. They can be as little as from top to bottom to as thick as . Cirrus cloud thickness is usually somewhere between those two extremes, with an average thickness of .\nThe jet stream, a high-level wind band, can stretch cirrus clouds long enough to cross continents. Jet streaks, bands of faster-moving air in the jet stream, can create arcs of cirrus cloud hundreds of kilometers long.\nCirrus cloud formation may be effected by organic aerosols (particles produced by plants) acting as additional nucleation points for ice crystal formation. However, research suggests that cirrus clouds more commonly form on mineral dust or metallic particles rather than on organic ones.\nTropical cyclones.\nSheets of cirrus clouds commonly fan out from the eye walls of tropical cyclones. (The eye wall is the ring of storm clouds surrounding the eye of a tropical cyclone.) A large shield of cirrus and cirrostratus typically accompanies the high altitude outflowing winds of tropical cyclones, and these can make the underlying bands of rain\u2014and sometimes even the eye\u2014difficult to detect in satellite photographs.\nThunderstorms.\nThunderstorms can form dense cirrus at their tops. As the cumulonimbus cloud in a thunderstorm grows vertically, the liquid water droplets freeze when the air temperature reaches the freezing point. The anvil cloud takes its shape because the temperature inversion at the tropopause prevents the warm, moist air forming the thunderstorm from rising any higher, thus creating the flat top. In the tropics, these thunderstorms occasionally produce copious amounts of cirrus from their anvils. High-altitude winds commonly push this dense mat out into an anvil shape that stretches downwind as much as several kilometers.\nIndividual cirrus cloud formations can be the remnants of anvil clouds formed by thunderstorms. In the dissipating stage of a cumulonimbus cloud, when the normal column rising up to the anvil has evaporated or dissipated, the mat of cirrus in the anvil is all that is left.\nContrails.\nContrails are an artificial type of cirrus cloud formed when water vapor from the exhaust of a jet engine condenses on particles, which come from either the surrounding air or the exhaust itself, and freezes, leaving behind a visible trail. The exhaust can trigger the formation of cirrus by providing ice nuclei when there is an insufficient naturally occurring supply in the atmosphere. One of the environmental impacts of aviation is that persistent contrails can form into large mats of cirrus, and increased air traffic has been implicated as one possible cause of the increasing frequency and amount of cirrus in Earth's atmosphere.\nUse in forecasting.\nRandom, isolated cirrus do not have any particular significance. A large number of cirrus clouds can be a sign of an approaching frontal system or upper air disturbance. The appearance of cirrus signals a change in weather\u2014usually more stormy\u2014in the near future. If the cloud is a \"cirrus castellanus\", there might be instability at the high altitude level. When the clouds deepen and spread, especially when they are of the \"cirrus radiatus\" variety or \"cirrus fibratus\" species, this usually indicates an approaching weather front. If it is a warm front, the cirrus clouds spread out into cirrostratus, which then thicken and lower into altocumulus and altostratus. The next set of clouds are the rain-bearing nimbostratus clouds. When cirrus clouds precede a cold front, squall line or multicellular thunderstorm, it is because they are blown off the anvil, and the next clouds to arrive are the cumulonimbus clouds. Kelvin-Helmholtz waves indicate extreme wind shear at high levels. When a jet streak creates a large arc of cirrus, weather conditions may be right for the development of winter storms.\nWithin the tropics, 36\u00a0hours prior to the center passage of a tropical cyclone, a veil of white cirrus clouds approaches from the direction of the cyclone. In the mid- to late-19th century, forecasters used these cirrus veils to predict the arrival of hurricanes. In the early 1870s the president of Bel\u00e9n College in Havana, Father Benito Vi\u00f1es, developed the first hurricane forecasting system; he mainly used the motion of these clouds in formulating his predictions. He would observe the clouds hourly from 4:00\u00a0am to 10:00\u00a0pm. After accumulating enough information, Vi\u00f1es began accurately predicting the paths of hurricanes; he summarized his observations in his book \"Apuntes Relativos a los Huracanes de las Antilles\", published in English as \"Practical Hints in Regard to West Indian Hurricanes\".\nEffects on climate.\nCirrus clouds cover up to 25% of the Earth (up to 70% in the tropics at night) and have a net heating effect. When they are thin and translucent, the clouds efficiently absorb outgoing infrared radiation while only marginally reflecting the incoming sunlight. When cirrus clouds are thick, they reflect only around 9% of the incoming sunlight, but they prevent almost 50% of the outgoing infrared radiation from escaping, thus raising the temperature of the atmosphere beneath the clouds by an average of 10\u00a0\u00b0C (18\u00a0\u00b0F)\u2014a process known as the greenhouse effect. Averaged worldwide, cloud formation results in a temperature loss of 5\u00a0\u00b0C (9\u00a0\u00b0F) at the earth's surface, mainly the result of stratocumulus clouds.\nCirrus clouds are likely becoming more common due to climate change. As their greenhouse effect is stronger than their reflection of sunlight, this would act as a self-reinforcing feedback. Metallic particles from human sources act as additional nucleation seeds, potentially increasing cirrus cloud cover and thus contributing further to climate change. Aircraft in the upper troposphere can create contrail cirrus clouds if local weather conditions are right. These contrails contribute to climate change.\nCirrus cloud thinning has been proposed as a possible geoengineering approach to reduce climate damage due to carbon dioxide. Cirrus cloud thinning would involve injecting particles into the upper troposphere to reduce the amount of cirrus clouds. The 2021 IPCC Assessment Report expressed low confidence in the cooling effect of cirrus cloud thinning, due to limited understanding.\nCloud properties.\nScientists have studied the properties of cirrus using several different methods. Lidar (laser-based radar) gives highly accurate information on the cloud's altitude, length, and width. Balloon-carried hygrometers measure the humidity of the cirrus cloud but are not accurate enough to measure the depth of the cloud. Radar units give information on the altitudes and thicknesses of cirrus clouds. Another data source is satellite measurements from the Stratospheric Aerosol and Gas Experiment program. These satellites measure where infrared radiation is absorbed in the atmosphere, and if it is absorbed at cirrus altitudes, then it is assumed that there are cirrus clouds in that location. NASA's Moderate-Resolution Imaging Spectroradiometer gives information on the cirrus cloud cover by measuring reflected infrared radiation of various specific frequencies during the day. During the night, it determines cirrus cover by detecting the Earth's infrared emissions. The cloud reflects this radiation back to the ground, thus enabling satellites to see the \"shadow\" it casts into space. Visual observations from aircraft or the ground provide additional information about cirrus clouds. Particle Analysis by Laser Mass Spectrometry (PALMS) is used to identify the type of nucleation seeds that spawned the ice crystals in a cirrus cloud.\nCirrus clouds have an average ice crystal concentration of 300,000 ice crystals per 10 cubic meters (270,000 ice crystals per 10 cubic yards). The concentration ranges from as low as 1 ice crystal per 10 cubic meters to as high as 100 million ice crystals per 10 cubic meters (just under 1 ice crystal per 10 cubic yards to 77 million ice crystals per 10 cubic yards), a difference of eight orders of magnitude. The size of each ice crystal is typically 0.25\u00a0millimeters, but they range from as short as 0.01\u00a0millimeters up to several millimeters. The ice crystals in contrails can be much smaller than those in naturally occurring cirrus cloud, being around 0.001\u00a0millimeters to 0.1\u00a0millimeters in length.\nIn addition to forming in different sizes, the ice crystals in cirrus clouds can crystallize in different shapes: solid columns, hollow columns, plates, rosettes, and conglomerations of the various other types. The shape of the ice crystals is determined by the air temperature, atmospheric pressure, and ice supersaturation (the amount by which the relative humidity exceeds 100%). Cirrus in temperate regions typically have the various ice crystal shapes separated by type. The columns and plates concentrate near the top of the cloud, whereas the rosettes and conglomerations concentrate near the base. In the northern Arctic region, cirrus clouds tend to be composed of only the columns, plates, and conglomerations, and these crystals tend to be at least four times larger than the minimum size. In Antarctica, cirrus are usually composed of only columns which are much longer than normal.\nCirrus clouds are usually colder than . At temperatures above , most cirrus clouds have relative humidities of roughly 100% (that is they are saturated). Cirrus can supersaturate, with relative humidities over ice that can exceed 200%. Below there are more of both undersaturated and supersaturated cirrus clouds. The more supersaturated clouds are probably young cirrus.\nOptical phenomena.\nCirrus clouds can produce several optical effects like halos around the Sun and Moon. Halos are caused by interaction of the light with hexagonal ice crystals present in the clouds which, depending on their shape and orientation, can result in a wide variety of white and colored rings, arcs and spots in the sky, including sun dogs, the 46\u00b0 halo, the 22\u00b0 halo, and circumhorizontal arcs. Circumhorizontal arcs are only visible when the Sun rises higher than 58\u00b0 above the horizon, preventing observers at higher latitudes from ever being able to see them.\nMore rarely, cirrus clouds are capable of producing glories, more commonly associated with liquid water-based clouds such as stratus. A glory is a set of concentric, faintly colored glowing rings that appear around the shadow of the observer, and are best observed from a high viewpoint or from a plane. Cirrus clouds only form glories when the constituent ice crystals are aspherical; researchers suggest that the ice crystals must be between 0.009\u00a0millimeters and 0.015\u00a0millimeters in length for a glory to appear.\nRelation to other clouds.\nCirrus clouds are one of three different genera of high-level clouds, all of which are given the prefix \"cirro-\". The other two genera are cirrocumulus and cirrostratus. High-level clouds usually form above . Cirrocumulus and cirrostratus are sometimes informally referred to as \"cirriform clouds\" because of their frequent association with cirrus.\nIn the intermediate range, from , are the mid-level clouds, which are given the prefix \"alto-\". They comprise two genera, altostratus and altocumulus. These clouds are formed from ice crystals, supercooled water droplets, or liquid water droplets.\nLow-level clouds usually form below and do not have a prefix. The two genera that are strictly low-level are stratus, and stratocumulus. These clouds are composed of water droplets, except during winter when they are formed of supercooled water droplets or ice crystals if the temperature at cloud level is below freezing. Three additional genera usually form in the low-altitude range, but may be based at higher levels under conditions of very low humidity. They are the genera cumulus, and cumulonimbus, and nimbostratus. These are sometimes classified separately as clouds of vertical development, especially when their tops are high enough to be composed of supercooled water droplets or ice crystals.\nCirrocumulus.\nCirrocumulus clouds form in sheets or patches and do not cast shadows. They commonly appear in regular, rippling patterns or in rows of clouds with clear areas between. Cirrocumulus are, like other members of the cumuliform category, formed via convective processes. Significant growth of these patches indicates high-altitude instability and can signal the approach of poorer weather. The ice crystals in the bottoms of cirrocumulus clouds tend to be in the form of hexagonal cylinders. They are not solid, but instead tend to have stepped funnels coming in from the ends. Towards the top of the cloud, these crystals have a tendency to clump together. These clouds do not last long, and they tend to change into cirrus because as the water vapor continues to deposit on the ice crystals, they eventually begin to fall, destroying the upward convection. The cloud then dissipates into cirrus. Cirrocumulus clouds come in four species: \"stratiformis\", \"lenticularis\", \"castellanus\", and \"floccus\". They are iridescent when the constituent supercooled water droplets are all about the same size.\nCirrostratus.\nCirrostratus clouds can appear as a milky sheen in the sky or as a striated sheet. They are sometimes similar to altostratus and are distinguishable from the latter because the Sun or Moon is always clearly visible through transparent cirrostratus, in contrast to altostratus which tends to be opaque or translucent. Cirrostratus come in two species, \"fibratus\" and \"nebulosus\". The ice crystals in these clouds vary depending upon the height in the cloud. Towards the bottom, at temperatures of around , the crystals tend to be long, solid, hexagonal columns. Towards the top of the cloud, at temperatures of around , the predominant crystal types are thick, hexagonal plates and short, solid, hexagonal columns. These clouds commonly produce halos, and sometimes the halo is the only indication that such clouds are present. They are formed by warm, moist air being lifted slowly to a very high altitude. When a warm front approaches, cirrostratus clouds become thicker and descend forming altostratus clouds, and rain usually begins 12 to 24 hours later.\nOther planets.\nCirrus clouds have been observed on several other planets. In 2008, the Martian Lander \"Phoenix\" took a time-lapse photograph of a group of cirrus clouds moving across the Martian sky using lidar. Near the end of its mission, the Phoenix Lander detected more thin clouds close to the north pole of Mars. Over the course of several days, they thickened, lowered, and eventually began snowing. The total precipitation was only a few thousandths of a millimeter. James Whiteway from York University concluded that \"precipitation is a component of the [Martian] hydrologic cycle\". These clouds formed during the Martian night in two layers, one around above ground and the other at surface level. They lasted through early morning before being burned away by the Sun. The crystals in these clouds were formed at a temperature of , and they were shaped roughly like ellipsoids 0.127\u00a0millimeters long and 0.042\u00a0millimeters wide.\nOn Jupiter, cirrus clouds are composed of ammonia. When Jupiter's South Equatorial Belt disappeared, one hypothesis put forward by Glenn Orten was that a large quantity of ammonia cirrus clouds had formed above it, hiding it from view. NASA's Cassini probe detected these clouds on Saturn and thin water-ice cirrus on Saturn's moon Titan. Cirrus clouds composed of methane ice exist on Uranus. On Neptune, thin wispy clouds which could possibly be cirrus have been detected over the Great Dark Spot. As on Uranus, these are probably methane crystals.\nInterstellar cirrus clouds are composed of tiny dust grains smaller than a micrometer and are therefore not true cirrus clouds, which are composed of frozen crystals. They range from a few light years to dozens of light years across. While they are not technically cirrus clouds, the dust clouds are referred to as \"cirrus\" because of their similarity to the clouds on Earth. They emit infrared radiation, similar to the way cirrus clouds on Earth reflect heat being radiated out into space.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nFootnotes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "47511", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=47511", "title": "Clarke Belt", "text": ""}
{"id": "47512", "revid": "8747689", "url": "https://en.wikipedia.org/wiki?curid=47512", "title": "Climate variability and change", "text": "Change in the statistical distribution of climate elements for an extended period\nClimate variability includes all the variations in the climate that last longer than individual weather events, whereas the term climate change only refers to those variations that persist for a longer period of time, typically decades or more. \"Climate change\" may refer to any time in Earth's history, but the term is now commonly used to describe contemporary climate change, often popularly referred to as global warming. Since the Industrial Revolution, the climate has increasingly been affected by human activities.\nThe climate system receives nearly all of its energy from the sun and radiates energy to outer space. The balance of incoming and outgoing energy and the passage of the energy through the climate system is Earth's energy budget. When the incoming energy is greater than the outgoing energy, Earth's energy budget is positive and the climate system is warming. If more energy goes out, the energy budget is negative and Earth experiences cooling.\nThe energy moving through Earth's climate system finds expression in weather, varying on geographic scales and time. Long-term averages and variability of weather in a region constitute the region's climate. Such changes can be the result of \"internal variability\", when natural processes inherent to the various parts of the climate system alter the distribution of energy. Examples include variability in ocean basins such as the Pacific decadal oscillation and Atlantic multidecadal oscillation. Climate variability can also result from \"external forcing\", when events outside of the climate system's components produce changes within the system. Examples include changes in solar output and volcanism.\nClimate variability has consequences for sea level changes, plant life, and mass extinctions; it also affects human societies.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nTerminology.\n\"Climate variability\" is the term to describe variations in the mean state and other characteristics of climate (such as chances or possibility of extreme weather, etc.) \"on all spatial and temporal scales beyond that of individual weather events.\" Some of the variability does not appear to be caused by known systems and occurs at seemingly random times. Such variability is called \"random variability\" or \"noise\". On the other hand, periodic variability occurs relatively regularly and in distinct modes of variability or climate patterns.\nThe term \"climate change\" is often used to refer specifically to anthropogenic climate change. Anthropogenic climate change is caused by human activity, as opposed to changes in climate that may have resulted as part of Earth's natural processes. \"Global warming\" became the dominant popular term in 1988, but within scientific journals global warming refers to surface temperature increases while climate change includes global warming and everything else that increasing greenhouse gas levels affect.\nA related term, \"climatic change\", was proposed by the World Meteorological Organization (WMO) in 1966 to encompass all forms of climatic variability on time-scales longer than 10 years, but regardless of cause. During the 1970s, the term climate change replaced climatic change to focus on anthropogenic causes, as it became clear that human activities had a potential to drastically alter the climate. Climate change was incorporated in the title of the Intergovernmental Panel on Climate Change (IPCC) and the UN Framework Convention on Climate Change (UNFCCC). Climate change is now used as both a technical description of the process, as well as a noun used to describe the problem.\nCauses.\nOn the broadest scale, the rate at which energy is received from the Sun and the rate at which it is lost to space determine the equilibrium temperature and climate of Earth. This energy is distributed around the globe by winds, ocean currents, and other mechanisms to affect the climates of different regions.\nFactors that can shape climate are called climate forcings or \"forcing mechanisms\". These include processes such as variations in solar radiation, variations in the Earth's orbit, variations in the albedo or reflectivity of the continents, atmosphere, and oceans, mountain-building and continental drift and changes in greenhouse gas concentrations. External forcing can be either anthropogenic (e.g. increased emissions of greenhouse gases and dust) or natural (e.g., changes in solar output, the Earth's orbit, volcano eruptions). There are a variety of climate change feedbacks that can either amplify or diminish the initial forcing. There are also key thresholds which when exceeded can produce rapid or irreversible change.\nSome parts of the climate system, such as the oceans and ice caps, respond more slowly in reaction to climate forcings, while others respond more quickly. An example of fast change is the atmospheric cooling after a volcanic eruption, when volcanic ash reflects sunlight. Thermal expansion of ocean water after atmospheric warming is slow, and can take thousands of years. A combination is also possible, e.g., sudden loss of albedo in the Arctic Ocean as sea ice melts, followed by more gradual thermal expansion of the water.\nClimate variability can also occur due to internal processes. Internal unforced processes often involve changes in the distribution of energy in the ocean and atmosphere, for instance, changes in the thermohaline circulation.\nInternal variability.\nClimatic changes due to internal variability sometimes occur in cycles or oscillations. For other types of natural climatic change, we cannot predict when it happens; the change is called \"random\" or \"stochastic\". From a climate perspective, the weather can be considered random. If there are little clouds in a particular year, there is an energy imbalance and extra heat can be absorbed by the oceans. Due to climate inertia, this signal can be 'stored' in the ocean and be expressed as variability on longer time scales than the original weather disturbances. If the weather disturbances are completely random, occurring as white noise, the inertia of glaciers or oceans can transform this into climate changes where longer-duration oscillations are also larger oscillations, a phenomenon called red noise. Many climate changes have a random aspect and a cyclical aspect. This behavior is dubbed \"stochastic resonance\". Half of the 2021 Nobel prize on physics was awarded for this work to Klaus Hasselmann jointly with Syukuro Manabe for related work on climate modelling. While Giorgio Parisi who with collaborators introduced the concept of stochastic resonance was awarded the other half but mainly for work on theoretical physics.\nOcean-atmosphere variability.\nThe ocean and atmosphere can work together to spontaneously generate internal climate variability that can persist for years to decades at a time. These variations can affect global average surface temperature by redistributing heat between the deep ocean and the atmosphere and/or by altering the cloud/water vapor/sea ice distribution which can affect the total energy budget of the Earth.\nOscillations and cycles.\nA \"climate oscillation\" or \"climate cycle\" is any recurring cyclical oscillation within global or regional climate. They are quasiperiodic (not perfectly periodic), so a Fourier analysis of the data does not have sharp peaks in the spectrum. Many oscillations on different time-scales have been found or hypothesized:\nOcean current changes.\nThe oceanic aspects of climate variability can generate variability on centennial timescales due to the ocean having hundreds of times more mass than in the atmosphere, and thus very high thermal inertia. For example, alterations to ocean processes such as thermohaline circulation play a key role in redistributing heat in the world's oceans.\nOcean currents transport a lot of energy from the warm tropical regions to the colder polar regions. Changes occurring around the last ice age (in technical terms, the last glacial period) show that the circulation in the North Atlantic can change suddenly and substantially, leading to global climate changes, even though the total amount of energy coming into the climate system did not change much. These large changes may have come from so called Heinrich events where internal instability of ice sheets caused huge ice bergs to be released into the ocean. When the ice sheet melts, the resulting water is very low in salt and cold, driving changes in circulation.\nLife.\nLife affects climate through its role in the carbon and water cycles and through such mechanisms as albedo, evapotranspiration, cloud formation, and weathering. Examples of how life may have affected past climate include:\nExternal climate forcing.\nGreenhouse gases.\nWhereas greenhouse gases released by the biosphere is often seen as a feedback or internal climate process, greenhouse gases emitted from volcanoes are typically classified as external by climatologists. Greenhouse gases, such as CO2, methane and nitrous oxide, heat the climate system by trapping infrared light. Volcanoes are also part of the extended carbon cycle. Over very long (geological) time periods, they release carbon dioxide from the Earth's crust and mantle, counteracting the uptake by sedimentary rocks and other geological carbon dioxide sinks.\nSince the Industrial Revolution, humanity has been adding to greenhouse gases by emitting CO2 from fossil fuel combustion, changing land use through deforestation, and has further altered the climate with aerosols (particulate matter in the atmosphere), release of trace gases (e.g. nitrogen oxides, carbon monoxide, or methane). Other factors, including land use, ozone depletion, animal husbandry (ruminant animals such as cattle produce methane), and deforestation, also play a role.\nThe US Geological Survey estimates are that volcanic emissions are at a much lower level than the effects of current human activities, which generate 100\u2013300 times the amount of carbon dioxide emitted by volcanoes. The annual amount put out by human activities may be greater than the amount released by supereruptions, the most recent of which was the Toba eruption in Indonesia 74,000 years ago.\nOrbital variations.\nSlight variations in Earth's motion lead to changes in the seasonal distribution of sunlight reaching the Earth's surface and how it is distributed across the globe. There is very little change to the area-averaged annually averaged sunshine; but there can be strong changes in the geographical and seasonal distribution. The three types of kinematic change are variations in Earth's eccentricity, changes in the tilt angle of Earth's axis of rotation, and precession of Earth's axis. Combined, these produce Milankovitch cycles which affect climate and are notable for their correlation to glacial and interglacial periods, their correlation with the advance and retreat of the Sahara, and for their appearance in the stratigraphic record.\nDuring the glacial cycles, there was a high correlation between CO2 concentrations and temperatures. Early studies indicated that CO2 concentrations lagged temperatures, but it has become clear that this is not always the case. When ocean temperatures increase, the solubility of CO2 decreases so that it is released from the ocean. The exchange of CO2 between the air and the ocean can also be impacted by further aspects of climatic change. These and other self-reinforcing processes allow small changes in Earth's motion to have a large effect on climate.\nSolar output.\nThe Sun is the predominant source of energy input to the Earth's climate system. Other sources include geothermal energy from the Earth's core, tidal energy from the Moon and heat from the decay of radioactive compounds. Both long term variations in solar intensity are known to affect global climate. Solar output varies on shorter time scales, including the 11-year solar cycle and longer-term modulations. Correlation between sunspots and climate and tenuous at best.\nThree to four billion years ago, the Sun emitted only 75% as much power as it does today. If the atmospheric composition had been the same as today, liquid water should not have existed on the Earth's surface. However, there is evidence for the presence of water on the early Earth, in the Hadean and Archean eons, leading to what is known as the faint young Sun paradox. Hypothesized solutions to this paradox include a vastly different atmosphere, with much higher concentrations of greenhouse gases than currently exist. Over the following approximately 4 billion years, the energy output of the Sun increased. Over the next five billion years, the Sun's ultimate death as it becomes a red giant and then a white dwarf will have large effects on climate, with the red giant phase possibly ending any life on Earth that survives until that time.\nVolcanism.\nThe volcanic eruptions considered to be large enough to affect the Earth's climate on a scale of more than 1 year are the ones that inject over 100,000 tons of SO2 into the stratosphere. This is due to the optical properties of SO2 and sulfate aerosols, which strongly absorb or scatter solar radiation, creating a global layer of sulfuric acid haze. On average, such eruptions occur several times per century, and cause cooling (by partially blocking the transmission of solar radiation to the Earth's surface) for a period of several years. Although volcanoes are technically part of the lithosphere, which itself is part of the climate system, the IPCC explicitly defines volcanism as an external forcing agent.\nNotable eruptions in the historical records are the 1991 eruption of Mount Pinatubo which lowered global temperatures by about 0.5\u00a0\u00b0C (0.9\u00a0\u00b0F) for up to three years, and the 1815 eruption of Mount Tambora causing the Year Without a Summer.\nAt a larger scale\u2014a few times every 50 million to 100 million years\u2014the eruption of large igneous provinces brings large quantities of igneous rock from the mantle and lithosphere to the Earth's surface. Carbon dioxide in the rock is then released into the atmosphere.\n Small eruptions, with injections of less than 0.1\u00a0Mt of sulfur dioxide into the stratosphere, affect the atmosphere only subtly, as temperature changes are comparable with natural variability. However, because smaller eruptions occur at a much higher frequency, they too significantly affect Earth's atmosphere.\nPlate tectonics.\nOver the course of millions of years, the motion of tectonic plates reconfigures global land and ocean areas and generates topography. This can affect both global and local patterns of climate and atmosphere-ocean circulation.\nThe position of the continents determines the geometry of the oceans and therefore influences patterns of ocean circulation. The locations of the seas are important in controlling the transfer of heat and moisture across the globe, and therefore, in determining global climate. A recent example of tectonic control on ocean circulation is the formation of the Isthmus of Panama about 5 million years ago, which shut off direct mixing between the Atlantic and Pacific Oceans. This strongly affected the ocean dynamics of what is now the Gulf Stream and may have led to Northern Hemisphere ice cover. During the Carboniferous period, about 300 to 360 million years ago, plate tectonics may have triggered large-scale storage of carbon and increased glaciation. Geologic evidence points to a \"megamonsoonal\" circulation pattern during the time of the supercontinent Pangaea, and climate modeling suggests that the existence of the supercontinent was conducive to the establishment of monsoons.\nThe size of continents is also important. Because of the stabilizing effect of the oceans on temperature, yearly temperature variations are generally lower in coastal areas than they are inland. A larger supercontinent will therefore have more area in which climate is strongly seasonal than will several smaller continents or islands.\nOther mechanisms.\nIt has been postulated that ionized particles known as cosmic rays could impact cloud cover and thereby the climate. As the sun shields the Earth from these particles, changes in solar activity were hypothesized to influence climate indirectly as well. To test the hypothesis, CERN designed the CLOUD experiment, which showed the effect of cosmic rays is too weak to influence climate noticeably.\nEvidence exists that the Chicxulub asteroid impact some 66 million years ago had severely affected the Earth's climate. Large quantities of sulfate aerosols were kicked up into the atmosphere, decreasing global temperatures by up to 26\u00a0\u00b0C and producing sub-freezing temperatures for a period of 3\u201316 years. The recovery time for this event took more than 30 years. The large-scale use of nuclear weapons has also been investigated for its impact on the climate. The hypothesis is that soot released by large-scale fires blocks a significant fraction of sunlight for as much as a year, leading to a sharp drop in temperatures for a few years. This possible event is described as nuclear winter.\nHumans' use of land impact how much sunlight the surface reflects and the concentration of dust. Cloud formation is not only influenced by how much water is in the air and the temperature, but also by the amount of aerosols in the air such as dust. Globally, more dust is available if there are many regions with dry soils, little vegetation and strong winds.\nEvidence and measurement of climate changes.\nPaleoclimatology is the study of changes in climate through the entire history of Earth. It uses a variety of proxy methods from the Earth and life sciences to obtain data preserved within things such as rocks, sediments, ice sheets, tree rings, corals, shells, and microfossils. It then uses the records to determine the past states of the Earth's various climate regions and its atmospheric system. Direct measurements give a more complete overview of climate variability.\nDirect measurements.\nClimate changes that occurred after the widespread deployment of measuring devices can be observed directly. Reasonably complete global records of surface temperature are available beginning from the mid-late 19th century. Further observations are derived indirectly from historical documents. Satellite cloud and precipitation data has been available since the 1970s.\nHistorical climatology is the study of historical changes in climate and their effect on human history and development. The primary sources include written records such as sagas, chronicles, maps and local history literature as well as pictorial representations such as paintings, drawings and even rock art. Climate variability in the recent past may be derived from changes in settlement and agricultural patterns. Archaeological evidence, oral history and historical documents can offer insights into past changes in the climate. Changes in climate have been linked to the rise and the collapse of various civilizations.\nProxy measurements.\nVarious archives of past climate are present in rocks, trees and fossils. From these archives, indirect measures of climate, so-called proxies, can be derived. Quantification of climatological variation of precipitation in prior centuries and epochs is less complete but approximated using proxies such as marine sediments, ice cores, cave stalagmites, and tree rings. Stress, too little precipitation or unsuitable temperatures, can alter the growth rate of trees, which allows scientists to infer climate trends by analyzing the growth rate of tree rings. This branch of science studying this called dendroclimatology. Glaciers leave behind moraines that contain a wealth of material\u2014including organic matter, quartz, and potassium that may be dated\u2014recording the periods in which a glacier advanced and retreated.\nAnalysis of ice in cores drilled from an ice sheet such as the Antarctic ice sheet, can be used to show a link between temperature and global sea level variations. The air trapped in bubbles in the ice can also reveal the CO2 variations of the atmosphere from the distant past, well before modern environmental influences. The study of these ice cores has been a significant indicator of the changes in CO2 over many millennia, and continues to provide valuable information about the differences between ancient and modern atmospheric conditions. The 18O/16O ratio in calcite and ice core samples used to deduce ocean temperature in the distant past is an example of a temperature proxy method.\nThe remnants of plants, and specifically pollen, are also used to study climatic change. Plant distributions vary under different climate conditions. Different groups of plants have pollen with distinctive shapes and surface textures, and since the outer surface of pollen is composed of a very resilient material, they resist decay. Changes in the type of pollen found in different layers of sediment indicate changes in plant communities. These changes are often a sign of a changing climate. As an example, pollen studies have been used to track changing vegetation patterns throughout the Quaternary glaciations and especially since the last glacial maximum. Remains of beetles are common in freshwater and land sediments. Different species of beetles tend to be found under different climatic conditions. Given the extensive lineage of beetles whose genetic makeup has not altered significantly over the millennia, knowledge of the present climatic range of the different species, and the age of the sediments in which remains are found, past climatic conditions may be inferred.\nAnalysis and uncertainties.\nOne difficulty in detecting climate cycles is that the Earth's climate has been changing in non-cyclic ways over most paleoclimatological timescales. Currently we are in a period of anthropogenic global warming. In a larger timeframe, the Earth is emerging from the latest ice age, cooling from the Holocene climatic optimum and warming from the \"Little Ice Age\", which means that climate has been constantly changing over the last 15,000 years or so. During warm periods, temperature fluctuations are often of a lesser amplitude. The Pleistocene period, dominated by repeated glaciations, developed out of more stable conditions in the Miocene and Pliocene climate. Holocene climate has been relatively stable. All of these changes complicate the task of looking for cyclical behavior in the climate.\nPositive feedback, negative feedback, and ecological inertia from the land-ocean-atmosphere system often attenuate or reverse smaller effects, whether from orbital forcings, solar variations or changes in concentrations of greenhouse gases. Certain feedbacks involving processes such as clouds are also uncertain; for contrails, natural cirrus clouds, oceanic dimethyl sulfide and a land-based equivalent, competing theories exist concerning effects on climatic temperatures, for example contrasting the Iris hypothesis and CLAW hypothesis.\nImpacts.\nLife.\nVegetation.\nA change in the type, distribution and coverage of vegetation may occur given a change in the climate. Some changes in climate may result in increased precipitation and warmth, resulting in improved plant growth and the subsequent sequestration of airborne CO2. Though an increase in CO2 may benefit plants, some factors can diminish this increase. If there is an environmental change such as drought, increased CO2 concentrations will not benefit the plant. So even though climate change does increase CO2 emissions, plants will often not use this increase as other environmental stresses put pressure on them. However, sequestration of CO2 is expected to affect the rate of many natural cycles like plant litter decomposition rates. A gradual increase in warmth in a region will lead to earlier flowering and fruiting times, driving a change in the timing of life cycles of dependent organisms. Conversely, cold will cause plant bio-cycles to lag.\nLarger, faster or more radical changes, however, may result in vegetation stress, rapid plant loss and desertification in certain circumstances. An example of this occurred during the Carboniferous Rainforest Collapse (CRC), an extinction event 300 million years ago. At this time vast rainforests covered the equatorial region of Europe and America. Climate change devastated these tropical rainforests, abruptly fragmenting the habitat into isolated 'islands' and causing the extinction of many plant and animal species.\nWildlife.\nOne of the most important ways animals can deal with climatic change is migration to warmer or colder regions. On a longer timescale, evolution makes ecosystems including animals better adapted to a new climate. Rapid or large climate change can cause mass extinctions when creatures are stretched too far to be able to adapt.\nHumanity.\nCollapses of past civilizations such as the Maya may be related to cycles of precipitation, especially drought, that in this example also correlates to the Western Hemisphere Warm Pool. Around 70 000 years ago the Toba supervolcano eruption created an especially cold period during the ice age, leading to a possible genetic bottleneck in human populations.\nChanges in the cryosphere.\nGlaciers and ice sheets.\nGlaciers are considered among the most sensitive indicators of a changing climate. Their size is determined by a mass balance between snow input and melt output. As temperatures increase, glaciers retreat unless snow precipitation increases to make up for the additional melt. Glaciers grow and shrink due both to natural variability and external forcings. Variability in temperature, precipitation and hydrology can strongly determine the evolution of a glacier in a particular season.\nThe most significant climate processes since the middle to late Pliocene (approximately 3 million years ago) are the glacial and interglacial cycles. The present interglacial period (the Holocene) has lasted about 11,700 years. Shaped by orbital variations, responses such as the rise and fall of continental ice sheets and significant sea-level changes helped create the climate. Other changes, including Heinrich events, Dansgaard\u2013Oeschger events and the Younger Dryas, however, illustrate how glacial variations may also influence climate without the orbital forcing.\nSea level change.\nDuring the Last Glacial Maximum, some 25,000 years ago, sea levels were roughly 130 m lower than today. The deglaciation afterwards was characterized by rapid sea level change. In the early Pliocene, global temperatures were 1\u20132\u02daC warmer than the present temperature, yet sea level was 15\u201325 meters higher than today.\nSea ice.\nSea ice plays an important role in Earth's climate as it affects the total amount of sunlight that is reflected away from the Earth. In the past, the Earth's oceans have been almost entirely covered by sea ice on a number of occasions, when the Earth was in a so-called Snowball Earth state, and completely ice-free in periods of warm climate. When there is a lot of sea ice present globally, especially in the tropics and subtropics, the climate is more sensitive to forcings as the ice\u2013albedo feedback is very strong.\nClimate history.\nVarious climate forcings are typically in flux throughout geologic time, and some processes of the Earth's temperature may be self-regulating. For example, during the Snowball Earth period, large glacial ice sheets spanned to Earth's equator, covering nearly its entire surface, and very high albedo created extremely low temperatures, while the accumulation of snow and ice likely removed carbon dioxide through atmospheric deposition. However, the absence of plant cover to absorb atmospheric CO2 emitted by volcanoes meant that the greenhouse gas could accumulate in the atmosphere. There was also an absence of exposed silicate rocks, which use CO2 when they undergo weathering. This created a warming that later melted the ice and brought Earth's temperature back up.\nPaleo-eocene thermal maximum.\nThe Paleocene\u2013Eocene Thermal Maximum (PETM) was a time period with more than 5\u20138\u00a0\u00b0C global average temperature rise across the event. This climate event occurred at the time boundary of the Paleocene and Eocene geological epochs. During the event large amounts of methane was released, a potent greenhouse gas. The PETM represents a \"case study\" for modern climate change as in the greenhouse gases were released in a geologically relatively short amount of time. During the PETM, a mass extinction of organisms in the deep ocean took place.\nThe Cenozoic.\nThroughout the Cenozoic, multiple climate forcings led to warming and cooling of the atmosphere, which led to the early formation of the Antarctic ice sheet, subsequent melting, and its later reglaciation. The temperature changes occurred somewhat suddenly, at carbon dioxide concentrations of about 600\u2013760 ppm and temperatures approximately 4\u00a0\u00b0C warmer than today. During the Pleistocene, cycles of glaciations and interglacials occurred on cycles of roughly 100,000\u00a0years, but may stay longer within an interglacial when orbital eccentricity approaches zero, as during the current interglacial. Previous interglacials such as the Eemian phase created temperatures higher than today, higher sea levels, and some partial melting of the West Antarctic ice sheet.\nClimatological temperatures substantially affect cloud cover and precipitation. At lower temperatures, air can hold less water vapour, which can lead to decreased precipitation. During the Last Glacial Maximum of 18,000 years ago, thermal-driven evaporation from the oceans onto continental landmasses was low, causing large areas of extreme desert, including polar deserts (cold but with low rates of cloud cover and precipitation). In contrast, the world's climate was cloudier and wetter than today near the start of the warm Atlantic Period of 8000 years ago.\nThe Holocene.\nThe Holocene is characterized by a long-term cooling starting after the Holocene Optimum, when temperatures were probably only just below current temperatures (second decade of the 21st century), and a strong African Monsoon created grassland conditions in the Sahara during the Neolithic Subpluvial. Since that time, several cooling events have occurred, including:\nIn contrast, several warm periods have also taken place, and they include but are not limited to:\nCertain effects have occurred during these cycles. For example, during the Medieval Warm Period, the American Midwest was in drought, including the Sand Hills of Nebraska which were active sand dunes. The black death plague of \"Yersinia pestis\" also occurred during Medieval temperature fluctuations, and may be related to changing climates.\nSolar activity may have contributed to part of the modern warming that peaked in the 1930s. However, solar cycles fail to account for warming observed since the 1980s to the present day. Events such as the opening of the Northwest Passage and recent record low ice minima of the modern Arctic shrinkage have not taken place for at least several centuries, as early explorers were all unable to make an Arctic crossing, even in summer. Shifts in biomes and habitat ranges are also unprecedented, occurring at rates that do not coincide with known climate oscillations .\nModern climate change and global warming.\nAs a consequence of humans emitting greenhouse gases, global surface temperatures have started rising. Global warming is an aspect of modern climate change, a term that also includes the observed changes in precipitation, storm tracks and cloudiness. As a consequence, glaciers worldwide have been found to be shrinking significantly. Land ice sheets in both Antarctica and Greenland have been losing mass since 2002 and have seen an acceleration of ice mass loss since 2009. Global sea levels have been rising as a consequence of thermal expansion and ice melt. The decline in Arctic sea ice, both in extent and thickness, over the last several decades is further evidence for rapid climate change.\nVariability between regions.\nIn addition to global climate variability and global climate change over time, numerous climatic variations occur contemporaneously across different physical regions.\nThe oceans' absorption of about 90% of excess heat has helped to cause land surface temperatures to grow more rapidly than sea surface temperatures. The Northern Hemisphere, having a larger landmass-to-ocean ratio than the Southern Hemisphere, shows greater average temperature increases. Variations across different latitude bands also reflect this divergence in average temperature increase, with the temperature increase of northern extratropics exceeding that of the tropics, which in turn exceeds that of the southern extratropics.\nUpper regions of the atmosphere have been cooling contemporaneously with a warming in the lower atmosphere, confirming the action of the greenhouse effect and ozone depletion.\nObserved regional climatic variations confirm predictions concerning ongoing changes, for example, by contrasting (smoother) year-to-year global variations with (more volatile) year-to-year variations in localized regions. Conversely, comparing different regions' warming patterns to their respective historical variabilities, allows the raw magnitudes of temperature changes to be placed in the perspective of what is normal variability for each region.\nRegional variability observations permit study of regionalized climate tipping points such as rainforest loss, ice sheet and sea ice melt, and permafrost thawing. Such distinctions underlie research into a possible global cascade of tipping points.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47513", "revid": "23596633", "url": "https://en.wikipedia.org/wiki?curid=47513", "title": "Climate model", "text": "Quantitative methods used to simulate climate\nNumerical climate models (or climate system models) are mathematical models that can simulate the interactions of important drivers of climate. These drivers are the atmosphere, oceans, land surface and ice. Scientists use climate models to study the dynamics of the climate system and to make projections of future climate and of climate change. Climate models can also be qualitative (i.e. not numerical) models and contain narratives, largely descriptive, of possible futures.\nClimate models take account of incoming energy from the Sun as well as outgoing energy from Earth. An imbalance results in a change in temperature. The incoming energy from the Sun is in the form of short wave electromagnetic radiation, chiefly visible and short-wave (near) infrared. The outgoing energy is in the form of long wave (far) infrared electromagnetic energy. These processes are part of the greenhouse effect.\nClimate models vary in complexity. For example, a simple radiant heat transfer model treats the Earth as a single point and averages outgoing energy. This can be expanded vertically (radiative-convective models) and horizontally. More complex models are the coupled atmosphere\u2013ocean\u2013sea ice global climate models. These types of models solve the full equations for mass transfer, energy transfer and radiant exchange. In addition, other types of models can be interlinked. For example Earth System Models include also land use as well as land use changes. This allows researchers to predict the interactions between climate and ecosystems.\nClimate models are systems of differential equations based on the basic laws of physics, fluid motion, and chemistry. Scientists divide the planet into a 3-dimensional grid and apply the basic equations to those grids. Atmospheric models calculate winds, heat transfer, radiation, relative humidity, and surface hydrology within each grid and evaluate interactions with neighboring points. These are coupled with oceanic models to simulate climate variability and change that occurs on different timescales due to shifting ocean currents and the much larger heat storage capacity of the global ocean. External drivers of change may also be applied. Including an ice-sheet model better accounts for long term effects such as sea level rise.\nUses.\nComplex climate models enable extreme event attribution, which is the science of identifying and quantifying the role that human-caused climate change plays in the frequency, intensity and impacts of extreme weather events. Attribution science aims to determine the degree to which such events can be explained by or linked to human-caused global warming, and are not simply due to random climate variability or natural weather patterns.\nThere are three major types of institution where climate models are developed, implemented and used:\nBig climate models are essential but they are not perfect.\u00a0Attention still needs to be given to the real world (what is happening and why). The global models are essential to assimilate all the observations, especially from space (satellites) and produce comprehensive analyses of what is happening, and then they can be used to make predictions/projections. Simple models have a role to play that is widely abused and fails to recognize the simplifications such as not including a water cycle.\nEnergy balance models (EBMs).\nSimulation of the climate system in full 3-D space and time was impractical prior to the establishment of large computational facilities starting in the 1960s. In order to begin to understand which factors may have changed Earth's paleoclimate states, the constituent and dimensional complexities of the system needed to be reduced. A simple quantitative model that balanced incoming/outgoing energy was first developed for the atmosphere in the late 19th century. Other EBMs similarly seek an economical description of surface temperatures by applying the conservation of energy constraint to individual columns of the Earth-atmosphere system.\nEssential features of EBMs include their relative conceptual simplicity and their ability to sometimes produce analytical solutions. Some models account for effects of ocean, land, or ice features on the surface budget. Others include interactions with parts of the water cycle or carbon cycle. A variety of these and other reduced system models can be useful for specialized tasks that supplement GCMs, particularly to bridge gaps between simulation and understanding.\nZero-dimensional models.\nZero-dimensional models consider Earth as a point in space, analogous to the pale blue dot viewed by Voyager 1 or an astronomer's view of very distant objects. This dimensionless view while highly limited is still useful in that the laws of physics are applicable in a bulk fashion to unknown objects, or in an appropriate lumped manner if some major properties of the object are known. For example, astronomers know that most planets in our own solar system feature some kind of solid/liquid surface surrounded by a gaseous atmosphere.\nModel with combined surface and atmosphere.\nA very simple model of the radiative equilibrium of the Earth is\nformula_1\nwhere\nThe constant parameters include\nThe constant formula_3 can be factored out, giving a nildimensional equation for the equilibrium\nformula_4\nwhere\nThe remaining variable parameters which are specific to the planet include\nThis very simple model is quite instructive. For example, it shows the temperature sensitivity to changes in the solar constant, Earth albedo, or effective Earth emissivity. The effective emissivity also gauges the strength of the atmospheric greenhouse effect, since it is the ratio of the thermal emissions escaping to space versus those emanating from the surface.\nThe calculated emissivity can be compared to available data. Terrestrial surface emissivities are all in the range of 0.96 to 0.99 (except for some small desert areas which may be as low as 0.7). Clouds, however, which cover about half of the planet's surface, have an average emissivity of about 0.5 (which must be reduced by the fourth power of the ratio of cloud absolute temperature to average surface absolute temperature) and an average cloud temperature of about . Taking all this properly into account results in an effective earth emissivity of about 0.64 (earth average temperature ).\nModels with separated surface and atmospheric layers.\nDimensionless models have also been constructed with functionally distinct atmospheric layers from the surface. The simplest of these is the zero-dimensional, one-layer model, which may be readily extended to an arbitrary number of atmospheric layers. The surface and atmospheric layer(s) are each characterized by a corresponding temperature and emissivity value, but no thickness. Applying radiative equilibrium (i.e conservation of energy) at the idealized interfaces between layers produces a set of coupled equations which are solvable.\nThese multi-layered EBMs are examples of multi-compartment models. They can estimate average temperatures closer to those observed for Earth's surface and troposphere. They likewise further illustrate the radiative heat transfer processes which underlie the greenhouse effect. Quantification of this phenomenon using a version of the one-layer model was first published by Svante Arrhenius in year 1896.\nRadiative-convective models.\nWater vapor is a main determinant of the emissivity of Earth's atmosphere. It both influences the flows of radiation and is influenced by convective flows of heat in a manner that is consistent with its equilibrium concentration and temperature as a function of elevation (i.e. relative humidity distribution). This has been shown by refining the zero dimension model in the vertical to a one-dimensional radiative-convective model which considers two processes of energy transport:\nRadiative-convective models typically use a distributed model of the atmosphere versus elevation. This has advantages over the lumped models and also lays a foundation for more complex models. They can estimate both surface temperature and the temperature variation with elevation in a more realistic manner. In particular, they properly simulate the observed decline in upper atmospheric temperature and the rise in surface temperature when trace amounts of other non-condensible greenhouse gases such as carbon dioxide are included.\nOther parameters are sometimes included to simulate localized effects in other dimensions and to address the factors that move energy about Earth. For example, the effect of ice-albedo feedback on global climate sensitivity has been investigated using a one-dimensional radiative-convective climate model.\nHigher-dimension models.\nThe zero-dimensional model may be expanded to consider the energy transported horizontally in the atmosphere. This kind of model may well be zonally averaged. This model has the advantage of allowing a rational dependence of \"local\" albedo and emissivity on temperature \u2013 the poles can be allowed to be icy and the equator warm \u2013 but the lack of true dynamics means that horizontal transports have to be specified.\nEarly examples include research of Mikhail Budyko and William D. Sellers who worked on the \"Budyko-Sellers model\". This work also showed the role of positive feedback in the climate system and has been considered foundational for the energy balance models since its publication in 1969.\nEarth systems models of intermediate complexity (EMICs).\nDepending on the nature of questions asked and the pertinent time scales, there are, on the one extreme, conceptual, more inductive models, and, on the other extreme, general circulation models operating at the highest spatial and temporal resolution currently feasible. Models of intermediate complexity bridge the gap. One example is the Climber-3 model. Its atmosphere is a 2.5-dimensional statistical-dynamical model with 7.5\u00b0 \u00d7 22.5\u00b0 resolution and time step of half a day; the ocean is MOM-3 (Modular Ocean Model) with a 3.75\u00b0 \u00d7 3.75\u00b0 grid and 24 vertical levels.\nBox models.\nBox models are simplified versions of complex systems, reducing them to boxes linked by fluxes. The boxes contain reservoirs (i.e. inventories) of species of matter and energy that are assumed to be mixed homogeneously. The concentration of any species is therefore uniform at any time within a box. However, the abundance of a species within a given box may vary as a function of time due to input flows or output flows; and may also vary due to the production, consumption or transformation of a species within the box.\nSimple box models, i.e. box model with a small number of boxes whose properties (e.g. their volume) do not change with time, are often useful to derive analytical formulas describing the dynamical and steady-state abundances of a species. The formulae are called governing equations and are derived from conservation laws (e.g. conservation of energy, conservation of mass, etc.). Larger sets of interacting species and equations are evaluated with numerical techniques to describe behavior of the system.\nBox models are used extensively to simulate environmental systems and ecosystems. In 1961 Henry Stommel was the first to use a simple 2-box model to study the stability of large-scale ocean circulation. A more complex model has examined interactions between ocean circulation and the carbon cycle.\nHistory.\nIncrease of forecasts confidence over time.\nThe Coupled Model Intercomparison Project (CMIP) has been a leading effort to foster improvements in GCMs and climate change understanding since 1995.\nThe IPCC stated in 2010 it has increased confidence in forecasts coming from climate models:\"There is considerable confidence that climate models provide credible quantitative estimates of future climate change, particularly at continental scales and above. This confidence comes from the foundation of the models in accepted physical principles and from their ability to reproduce observed features of current climate and past climate changes. Confidence in model estimates is higher for some climate variables (e.g., temperature) than for others (e.g., precipitation). Over several decades of development, models have consistently provided a robust and unambiguous picture of significant climate warming in response to increasing greenhouse gases.\"\nCoordination of research.\nThe World Climate Research Programme (WCRP), hosted by the World Meteorological Organization (WMO), coordinates research activities on climate modelling worldwide.\nA 2012 U.S. National Research Council report discussed how the large and diverse U.S. climate modeling enterprise could evolve to become more unified. Efficiencies could be gained by developing a common software infrastructure shared by all U.S. climate researchers, and holding an annual climate modeling forum, the report found.\nIssues.\nElectricity consumption.\nCloud-resolving climate models are nowadays run on high intensity super-computers which have a high power consumption and thus cause CO2 emissions.\u00a0They require exascale computing (billion billion \u2013 i.e., a quintillion \u2013 calculations per second). For example, the Frontier exascale supercomputer consumes 29 MW. It can simulate a year\u2019s worth of climate at cloud resolving scales in a day.\nTechniques that could lead to energy savings, include for example: \"reducing floating point precision computation; developing machine learning algorithms to avoid unnecessary computations; and creating a new generation of scalable numerical algorithms that would enable higher throughput in terms of simulated years per wall clock day.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nClimate models on the web:"}
{"id": "47514", "revid": "18952693", "url": "https://en.wikipedia.org/wiki?curid=47514", "title": "Cloud albedo", "text": "Fraction of incoming sunlight reflected by clouds\nCloud albedo is a measure of the albedo or reflectivity of a cloud. Clouds regulate the amount of solar radiation absorbed by a planet and its solar surface irradiance. Generally, increased cloud cover correlates to a higher albedo and a lower absorption of solar energy. Cloud albedo strongly influences the Earth's energy budget, accounting for approximately half of Earth's albedo. Cloud albedo is influenced by the conditions of cloud formation and variations in cloud albedo depend on the total mass of water, the size and shape of the droplets or particles and their distribution in space. Thick clouds reflect a large amount of incoming solar radiation, translating to a high albedo. Thin clouds tend to transmit more solar radiation and, therefore, have a low albedo. Changes in cloud albedo caused by variations in cloud properties have a significant effect on global climate, having the ability to spiral into feedback loops.\nCloud condensation nuclei and cloud albedo.\nOn a microscopic scale, clouds are formed through the condensation of water on cloud condensation nuclei. These nuclei are aerosols such as dust or sea salt but also include certain forms of pollution. Nuclei come from a variety of natural or anthropogenic sources. For example dust can arise from windblown desserts or from human agricultural or construction activities, similarly even pollutants like VOCs or sulfates may be emitted by plant life or volcanic activity respectively. The size, concentration, structure, and chemical composition of these particles influence cloud albedo. For example, black carbon aerosol particles absorb more solar radiation and sulfate aerosols reflects more solar radiation. Smaller particles form smaller cloud droplets, which tend to decrease precipitation efficiency of a cloud and increasing cloud albedo. Additionally, more cloud condensation nuclei increases the size of a cloud and the amount of reflected solar radiation.\nCauses of cloud albedo variation.\nCloud albedo on a planet varies from less than 10% to more than 90% and depends on liquid water/ice content, thickness of the cloud, droplet sizes, solar zenith angle, etc.\nWater content.\nHigher liquid water and ice content in a cloud increases its albedo, which is the dominant factor in determining the cloud's reflectivity. The change in albedo is greater for clouds with less water content to start with and larger clouds begin to receive diminishing returns with increased content. Water content taking the form of ice is common in high altitude clouds such as cirrus. \nCloud thickness.\nThicker clouds have a higher albedo than thinner ones. In fact thick clouds and thin clouds will occasionally respond differently to differences in other factors such as droplet size. Clouds that tend to be thicker and have higher albedos include cumulus, stratocumulus, and cumulonimbus clouds.\nLiquid water path.\nWater content and cloud thickness together make a cloud's liquid water path. This value also notably varies with changing cloud droplet size. Liquid water path is typically measured in units of g/m2 and in excess of 20 g/m2 clouds typically will become opaque to long-wavelength light although this may not hold true with cirrus clouds.\nDroplet size.\nIn general smaller droplet size is associated with increased albedo. That said, depending on the cloud location, thin clouds may actually have the opposite hold true. In the general and more influential cases however, decreased particle size makes clouds possess higher albedos by having a larger surface areas relative to their volumes. This makes the droplets whiter or more reflective.\nThe Twomey Effect (Aerosol Indirect Effect).\nThe Twomey Effect is increased cloud albedo due to cloud nuclei from pollution. Increasing aerosol concentration and aerosol density leads to higher cloud droplet concentration, smaller cloud droplets, and higher cloud albedo. In macrophysically identical clouds, a cloud with few larger drops will have a lower albedo than a cloud with more smaller drops. The smaller cloud particles similarly increase cloud albedo by reducing precipitation and prolonging the lifetime of a cloud. This subsequently increases cloud albedo as solar radiation is reflected over a longer period of time. The Albrecht Effect is the related concept of increased cloud lifetime from cloud nuclei.\nZenith angle.\nCloud albedo increases with the total water content or depth of the cloud and the solar zenith angle. The variation of albedo with zenith angle is most rapid when the sun is near the horizon, and least when the sun is overhead. Absorption of solar radiation by plane-parallel clouds decreases with increasing zenith angle because radiation that is reflected to space at the higher zenith angles penetrates less deeply into the cloud and is therefore less likely to be absorbed.\nInfluence on global climate.\nCloud albedo indirectly affects global climate through solar radiation scattering and absorption in Earth's radiation budget. Variations in cloud albedo cause atmospheric instability that influences the hydrological cycle, weather patterns, and atmospheric circulation. These effects are parameterized by cloud radiative forcing, a measure of short-wave and long-wave radiation in relation to cloud cover. The Earth Radiation Budget Experiment demonstrated that small variations in cloud coverage, structure, altitude, droplet size, and phase have significant effects on the climate. A five percent increase in short-wave reflection from clouds would counteract the greenhouse effect of the past two-hundred years.\nCloud albedo-climate feedback loops.\nThere are a variety of positive and negative cloud albedo-climate feedback loops in cloud and climate models. An example of a negative cloud-climate feedback loop is that as a planet warms, cloudiness increases, which increases a planet's albedo. An increase in albedo reduces absorbed solar radiation and leads to cooling. A counteracting positive feedback loop considers the rising of the high cloud layer, reduction in the vertical distribution of cloudiness, and decreased albedo.\nAir pollution can result in variation in cloud condensation nuclei, creating a feedback loop that influences atmospheric temperature, relative humidity, and cloud formation depending on cloud and regional characteristics. For example, increased sulfate aerosols can reduce precipitation efficiency, resulting in a positive feedback loop in which decreased precipitation efficiency increases aerosol atmospheric longevity. On the other hand, a negative feedback loop can be established in mixed-phase clouds in which black carbon aerosol can increase ice phase precipitation formation and reduce aerosol concentrations."}
{"id": "47515", "revid": "7007500", "url": "https://en.wikipedia.org/wiki?curid=47515", "title": "Cloud", "text": "Visible mass of particles suspended in the atmosphere\nIn meteorology, a cloud is an aerosol consisting of a visible mass of miniature liquid droplets, ice crystals, or other particles, suspended in the atmosphere of a planetary body or similar space. Water or various other chemicals may comprise the droplets and crystals. On Earth, clouds are formed as a result of saturation of the air when it is cooled to its dew point, or when it gains sufficient moisture, usually in the form of water vapor, from an adjacent source to raise the dew point to the ambient temperature.\nClouds are seen in the Earth's homosphere, which includes the troposphere, stratosphere, and mesosphere.\nNephology is the science of clouds, which is undertaken in the cloud physics branch of meteorology. The World Meteorological Organization uses two methods of naming clouds in their respective layers of the homosphere, Latin and common name.\nGenus types in the troposphere, the atmospheric layer closest to Earth's surface, have Latin names because of the universal adoption of Luke Howard's nomenclature that was formally proposed in 1802. It became the basis of a modern international system that divides clouds into five physical \"forms\" which can be further divided or classified into altitude \"levels\" to derive ten basic \"genera\". The five main forms are stratiform sheets or veils, cumuliform heaps, stratocumuliform bands, rolls, or ripples, cumulonimbiform towers often with fibrous tops, and cirriform wisps or patches. \"Low-level\" clouds do not have any altitude-related prefixes. However \"mid-level\" stratiform and stratocumuliform types are given the prefix \"alto-\" while \"high-level\" variants of these same two forms carry the prefix \"cirro-\". In the case of stratocumuliform clouds, the prefix \"strato-\" is applied to the low-level genus type but is dropped from the mid- and high-level variants to avoid double-prefixing with alto- and cirro-. Genus types with sufficient vertical extent to occupy more than one level do not carry any altitude-related prefixes. They are classified formally as low- or mid-level depending on the altitude at which each initially forms, and are also more informally characterized as \"multi-level\" or \"vertical\". Most of the ten genera derived by this method of classification can be subdivided into \"species\" and further subdivided into \"varieties\". Very low stratiform clouds that extend down to the Earth's surface are given the common names \"fog\" and \"mist\" but have no Latin names.\nIn the stratosphere and mesosphere, clouds also have common names for their main types. They may have the appearance of veils or sheets, wisps, or bands or ripples, but not heaps or towers as in the troposphere. They are seen infrequently, mostly in the polar regions of Earth. Clouds have been observed in the atmospheres of other planets and moons in the Solar System and beyond. However, due to their different temperature characteristics, they are often composed of other substances such as methane, ammonia, and sulfuric acid, as well as water.\nTropospheric clouds can have a direct effect on climate change on Earth. They may reflect incoming rays from the Sun which can contribute to a cooling effect where and when these clouds occur, or trap longer wave radiation that reflects up from the Earth's surface which can cause a warming effect. The altitude, form, and thickness of the clouds are the main factors that affect the local heating or cooling of the Earth and the atmosphere. Clouds that form above the troposphere are too scarce and too thin to have any influence on climate change. Clouds are the main uncertainty in climate sensitivity.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nEtymology.\nThe origin of the term \"cloud\" can be found in the Old English words or ', meaning a hill or a mass of stone. Around the beginning of the 13th century, the word came to be used as a metaphor for rain clouds, because of the similarity in appearance between a mass of rock and cumulus heap cloud. Over time, the metaphoric usage of the word supplanted the Old English ', which had been the literal term for clouds in general.\nHomospheric nomenclatures and cross-classification.\nThe table that follows is very broad in scope like the cloud genera template upon which it is partly based. There are some variations in styles of nomenclature between the classification scheme used for the troposphere (strict Latin except for surface-based aerosols) and the method used for the higher levels of the homosphere (common terms, some informally derived from Latin). However, these two schemes, both of which are authorized and used by the World Meteorological Organization, share a cross-classification of physical forms and altitude levels to derive the 10 tropospheric genera, the fog and mist that forms at surface level, and several additional major types above the troposphere. The cumulus genus includes four species that indicate vertical size which can affect the altitude levels.\nHistory of cloud science.\nAncient cloud studies were not made in isolation, but were observed in combination with other weather elements and even other natural sciences. Around 340 BC, Greek philosopher Aristotle wrote \"Meteorologica\", a work which represented the sum of knowledge of the time about natural science, including weather and climate. For the first time, precipitation and the clouds from which precipitation fell were called meteors, which originate from the Greek word \"meteoros\", meaning 'high in the sky'. From that word came the modern term meteorology, the study of clouds and weather. \"Meteorologica\" was based on intuition and simple observation, but not on what is now considered the scientific method. Nevertheless, it was the first known work that attempted to treat a broad range of meteorological topics in a systematic way, especially the hydrological cycle.\nAfter centuries of speculative theories about the formation and behavior of clouds, the first truly scientific studies were undertaken at the beginning of the 19th century by Luke Howard in England and Jean-Baptiste Lamarck in France. Howard was a methodical observer with a strong grounding in the Latin language, and used his background to formally classify the various tropospheric cloud types during 1802. He believed that scientific observations of the changing cloud forms in the sky could unlock the key to weather forecasting.\nLamarck had worked independently on cloud classification the same year and had come up with a different naming scheme that failed to make an impression even in his home country of France because it used unusually descriptive and informal French names and phrases for cloud types. His system of nomenclature included 12 categories of clouds, with such names as (translated from French) hazy clouds, dappled clouds, and broom-like clouds. By contrast, Howard used universally accepted Latin, which caught on quickly after it was published in 1803. As a sign of the popularity of the naming scheme, German dramatist and poet Johann Wolfgang von Goethe composed four poems about clouds, dedicating them to Howard.\nAn elaboration of Howard's system was eventually formally adopted by the International Meteorological Conference in 1891. This system covered only the tropospheric cloud types. However, the discovery of clouds above the troposphere during the late 19th century eventually led to the creation of separate classification schemes that reverted to the use of descriptive common names and phrases that somewhat recalled Lamarck's methods of classification. These very high clouds, although classified by these different methods, are nevertheless broadly similar to some cloud forms identified in the troposphere with Latin names.\nFormation.\nTerrestrial clouds can be found throughout most of the homosphere, which includes the troposphere, stratosphere, and mesosphere. Within these layers of the atmosphere, air can become saturated as a result of being cooled to its dew point or by having moisture added from an adjacent source. In the latter case, saturation occurs when the dew point is raised to the ambient air temperature.\nAdiabatic cooling.\nAdiabatic cooling occurs when one or more of three possible lifting agents \u2013 convective, cyclonic/frontal, or orographic \u2013 cause a parcel of air containing invisible water vapor to rise and cool to its dew point, the temperature at which the air becomes saturated. The main mechanism behind this process is adiabatic cooling. As the air is cooled to its dew point and becomes saturated, water vapor normally condenses to form cloud drops. This condensation normally occurs on cloud condensation nuclei such as salt or dust particles that are small enough to be held aloft by normal circulation of the air.\nOne agent is the convective upward motion of air caused by daytime solar heating at surface level. Low level airmass instability allows for the formation of cumuliform clouds in the troposphere that can produce showers if the air is sufficiently moist. On moderately rare occasions, convective lift can be powerful enough to penetrate the tropopause and push the cloud top into the stratosphere.\nFrontal and cyclonic lift occur in the troposphere when stable air is forced aloft at weather fronts and around centers of low pressure by a process called convergence. Warm fronts associated with extratropical cyclones tend to generate mostly cirriform and stratiform clouds over a wide area unless the approaching warm airmass is unstable, in which case cumulus congestus or cumulonimbus clouds are usually embedded in the main precipitating cloud layer. Cold fronts are usually faster moving and generate a narrower line of clouds, which are mostly stratocumuliform, cumuliform, or cumulonimbiform depending on the stability of the warm airmass just ahead of the front.\nA third source of lift is wind circulation forcing air over a physical barrier such as a mountain (orographic lift). If the air is generally stable, nothing more than lenticular cap clouds form. However, if the air becomes sufficiently moist and unstable, orographic showers or thunderstorms may appear.\nClouds formed by any of these lifting agents are initially seen in the troposphere where these agents are most active. However, water vapor that has been lifted to the top of troposphere can be carried even higher by gravity waves where further condensation can result in the formation of clouds in the stratosphere and mesosphere.\nNon-adiabatic cooling.\nAlong with adiabatic cooling that requires a lifting agent, three major nonadiabatic mechanisms exist for lowering the temperature of the air to its dew point. Conductive, radiational, and evaporative cooling require no lifting mechanism and can cause condensation at surface level resulting in the formation of fog.\nAdding moisture to the air.\nSeveral main sources of water vapor can be added to the air as a way of achieving saturation without any cooling process: evaporation from surface water or moist ground, precipitation or virga, and transpiration from plants.\nTropospheric classification.\nClassification in the troposphere is based on a hierarchy of categories with physical forms and altitude levels at the top. These are cross-classified into a total of ten genus types, most of which can be divided into species and further subdivided into varieties which are at the bottom of the hierarchy.\nClouds in the troposphere assume five physical forms based on structure and process of formation. These forms are commonly used for the purpose of satellite analysis. They are given below in approximate ascending order of instability or convective activity.\nLevels and genera.\nTropospheric clouds form in any of three levels (formerly called \"\u00e9tages\") based on altitude range above the Earth's surface. The grouping of clouds into levels is commonly done for the purposes of cloud atlases, surface weather observations, and weather maps. The base-height range for each level varies depending on the latitudinal geographical zone. Each altitude level comprises two or three genus-types differentiated mainly by physical form.\nThe standard levels and genus-types are summarised below in approximate descending order of the altitude at which each is normally based. Multi-level clouds with significant vertical extent are separately listed and summarized in approximate ascending order of instability or convective activity.\nHigh-level.\nHigh clouds form at altitudes of in the polar regions, in the temperate regions, and in the tropics. All cirriform clouds are classified as high, thus constitute a single genus \"cirrus\" (Ci). Stratocumuliform and stratiform clouds in the high altitude range carry the prefix \"cirro-\", yielding the respective genus names \"cirrocumulus\" (Cc) and \"cirrostratus\" (Cs). If limited-resolution satellite images of high clouds are analyzed without supporting data from direct human observations, distinguishing between individual forms or genus types becomes impossible, and they are collectively identified as \"high-type\" (or informally as \"cirrus-type\", though not all high clouds are of the cirrus form or genus).\nMid-level.\nNonvertical clouds in the middle level are prefixed by \"alto-\", yielding the genus names \"altocumulus\" (Ac) for stratocumuliform types and \"altostratus\" (As) for stratiform types. These clouds can form as low as above surface at any latitude, but may be based as high as near the poles, at midlatitudes, and in the tropics. As with high clouds, the main genus types are easily identified by the human eye, but distinguishing between them using satellite photography alone is not possible. When the supporting data of human observations are not available, these clouds are usually collectively identified as \"middle-type\" on satellite images.\nLow-level.\nLow clouds are found from near the surface up to . Genus types in this level either have no prefix or carry one that refers to a characteristic other than altitude. Clouds that form in the low level of the troposphere are generally of larger structure than those that form in the middle and high levels, so they can usually be identified by their forms and genus types using satellite photography alone.\nMulti-level or moderate vertical.\nThese clouds have low- to mid-level bases that form anywhere from near the surface to about and tops that can extend into the mid-altitude range and sometimes higher in the case of nimbostratus.\nTowering vertical.\nThese very large cumuliform and cumulonimbiform types have cloud bases in the same low- to mid-level range as the multi-level and moderate vertical types, but the tops nearly always extend into the high levels. Unlike less vertically developed clouds, they are required to be identified by their standard names or abbreviations in all aviation observations (METARS) and forecasts (TAFS) to warn pilots of possible severe weather and turbulence.\nSpecies.\nGenus types are commonly divided into subtypes called \"species\" that indicate specific structural details which can vary according to the stability and windshear characteristics of the atmosphere at any given time and location. Despite this hierarchy, a particular species may be a subtype of more than one genus, especially if the genera are of the same physical form and are differentiated from each other mainly by altitude or level. There are a few species, each of which can be associated with genera of more than one physical form. The species types are grouped below according to the physical forms and genera with which each is normally associated. The forms, genera, and species are listed from left to right in approximate ascending order of instability or convective activity.\nStable or mostly stable.\nOf the non-convective stratiform group, high-level cirrostratus comprises two species. Cirrostratus \"nebulosus\" has a rather diffuse appearance lacking in structural detail. Cirrostratus \"fibratus\" is a species made of semi-merged filaments that are transitional to or from cirrus. Mid-level altostratus and multi-level nimbostratus always have a flat or diffuse appearance and are therefore not subdivided into species. Low stratus is of the species nebulosus except when broken up into ragged sheets of stratus fractus (see below).\nCirriform clouds have three non-convective species that can form in \"stable\" airmass conditions. Cirrus fibratus comprise filaments that may be straight, wavy, or occasionally twisted by wind shear. The species \"uncinus\" is similar but has upturned hooks at the ends. Cirrus \"spissatus\" appear as opaque patches that can show light gray shading.\nStratocumuliform genus-types (cirrocumulus, altocumulus, and stratocumulus) that appear in mostly stable air with limited convection have two species each. The \"stratiformis\" species normally occur in extensive sheets or in smaller patches where there is only minimal convective activity. Clouds of the \"lenticularis\" species tend to have lens-like shapes tapered at the ends. They are most commonly seen as orographic mountain-wave clouds, but can occur anywhere in the troposphere where there is strong wind shear combined with sufficient airmass stability to maintain a generally flat cloud structure. These two species can be found in the high, middle, or low levels of the troposphere depending on the stratocumuliform genus or genera present at any given time.\nRagged.\nThe species \"fractus\" shows \"variable\" instability because it can be a subdivision of genus-types of different physical forms that have different stability characteristics. This subtype can be in the form of ragged but mostly \"stable\" stratiform sheets (stratus fractus) or small ragged cumuliform heaps with somewhat greater instability (cumulus fractus). When clouds of this species are associated with precipitating cloud systems of considerable vertical and sometimes horizontal extent, they are also classified as \"accessory clouds\" under the name \"pannus\" (see section on supplementary features).\nPartly unstable.\nThese species are subdivisions of genus types that can occur in partly unstable air with limited convection. The species \"castellanus\" appears when a mostly stable stratocumuliform or cirriform layer becomes disturbed by localized areas of airmass instability, usually in the morning or afternoon. This results in the formation of embedded cumuliform buildups arising from a common stratiform base. Castellanus resembles the turrets of a castle when viewed from the side, and can be found with stratocumuliform genera at any tropospheric altitude level and with limited-convective patches of high-level cirrus. Tufted clouds of the more detached \"floccus\" species are subdivisions of genus-types which may be cirriform or stratocumuliform in overall structure. They are sometimes seen with cirrus, cirrocumulus, altocumulus, and stratocumulus.\nA newly recognized species of stratocumulus or altocumulus has been given the name \"volutus\", a roll cloud that can occur ahead of a cumulonimbus formation. There are some volutus clouds that form as a consequence of interactions with specific geographical features rather than with a parent cloud. Perhaps the strangest geographically specific cloud of this type is the Morning Glory, a rolling cylindrical cloud that appears unpredictably over the Gulf of Carpentaria in Northern Australia. Associated with a powerful \"ripple\" in the atmosphere, the cloud may be \"surfed\" in glider aircraft.\nUnstable or mostly unstable.\nMore general airmass instability in the troposphere tends to produce clouds of the more freely convective cumulus genus type, whose species are mainly indicators of degrees of atmospheric instability and resultant vertical development of the clouds. A cumulus cloud initially forms in the low level of the troposphere as a cloudlet of the species \"humilis\" that shows only slight vertical development. If the air becomes more unstable, the cloud tends to grow vertically into the species \"mediocris\", then strongly convective \"congestus\", the tallest cumulus species which is the same type that the International Civil Aviation Organization refers to as 'towering cumulus'.\nWith highly unstable atmospheric conditions, large cumulus may continue to grow into even more strongly convective cumulonimbus \"calvus\" (essentially a very tall congestus cloud that produces thunder), then ultimately into the species \"capillatus\" when supercooled water droplets at the top of the cloud turn into ice crystals giving it a cirriform appearance.\nVarieties.\nGenus and species types are further subdivided into \"varieties\" whose names can appear after the species name to provide a fuller description of a cloud. Some cloud varieties are not restricted to a specific altitude level or form, and can therefore be common to more than one genus or species.\nOpacity-based.\nAll cloud varieties fall into one of two main groups. One group identifies the opacities of particular low and mid-level cloud structures and comprises the varieties \"translucidus\" (thin translucent), \"perlucidus\" (thick opaque with translucent or very small clear breaks), and \"opacus\" (thick opaque). These varieties are always identifiable for cloud genera and species with variable opacity. All three are associated with the stratiformis species of altocumulus and stratocumulus. However, only two varieties are seen with altostratus and stratus nebulosus whose uniform structures prevent the formation of a perlucidus variety. Opacity-based varieties are not applied to high clouds because they are always translucent, or in the case of cirrus spissatus, always opaque.\nPattern-based.\nA second group describes the occasional arrangements of cloud structures into particular patterns that are discernible by a surface-based observer (cloud fields usually being visible only from a significant altitude above the formations). These varieties are not always present with the genera and species with which they are otherwise associated, but only appear when atmospheric conditions favor their formation. \"Intortus\" and \"vertebratus\" varieties occur on occasion with cirrus fibratus. They are respectively filaments twisted into irregular shapes, and those that are arranged in fishbone patterns, usually by uneven wind currents that favor the formation of these varieties. The variety \"radiatus\" is associated with cloud rows of a particular type that appear to converge at the horizon. It is sometimes seen with the fibratus and uncinus species of cirrus, the stratiformis species of altocumulus and stratocumulus, the mediocris and sometimes humilis species of cumulus, and with the genus altostratus.\nAnother variety, \"duplicatus\" (closely spaced layers of the same type, one above the other), is sometimes found with cirrus of both the fibratus and uncinus species, and with altocumulus and stratocumulus of the species stratiformis and lenticularis. The variety \"undulatus\" (having a wavy undulating base) can occur with any clouds of the species stratiformis or lenticularis, and with altostratus. It is only rarely observed with stratus nebulosus. The variety \"lacunosus\" is caused by localized downdrafts that create circular holes in the form of a honeycomb or net. It is occasionally seen with cirrocumulus and altocumulus of the species stratiformis, castellanus, and floccus, and with stratocumulus of the species stratiformis and castellanus.\nCombinations.\nIt is possible for some species to show combined varieties at one time, especially if one variety is opacity-based and the other is pattern-based. An example of this would be a layer of altocumulus stratiformis arranged in seemingly converging rows separated by small breaks. The full technical name of a cloud in this configuration would be \"altocumulus stratiformis radiatus perlucidus\", which would identify respectively its genus, species, and two combined varieties.\nOther types.\nSupplementary features and accessory clouds are not further subdivisions of cloud types below the species and variety level. Rather, they are either \"hydrometeors\" or special cloud types with their own Latin names that form in association with certain cloud genera, species, and varieties. Supplementary features, whether in the form of clouds or precipitation, are directly attached to the main genus-cloud. Accessory clouds, by contrast, are generally detached from the main cloud.\nPrecipitation-based supplementary features.\nOne group of supplementary features are not actual cloud formations, but precipitation that falls when water droplets or ice crystals that make up visible clouds have grown too heavy to remain aloft. \"Virga\" is a feature seen with clouds producing precipitation that evaporates before reaching the ground, these being of the genera cirrocumulus, altocumulus, altostratus, nimbostratus, stratocumulus, cumulus, and cumulonimbus.\nWhen the precipitation reaches the ground without completely evaporating, it is designated as the feature \"praecipitatio\". This normally occurs with altostratus opacus, which can produce widespread but usually light precipitation, and with thicker clouds that show significant vertical development. Of the latter, \"upward-growing\" cumulus mediocris produces only isolated light showers, while \"downward growing\" nimbostratus is capable of heavier, more extensive precipitation. Towering vertical clouds have the greatest ability to produce intense precipitation events, but these tend to be localized unless organized along fast-moving cold fronts. Showers of moderate to heavy intensity can fall from cumulus congestus clouds. Cumulonimbus, the largest of all cloud genera, has the capacity to produce very heavy showers. Low stratus clouds usually produce only light precipitation, but this always occurs as the feature praecipitatio because this cloud genus lies too close to the ground to allow the formation of virga.\nCloud-based supplementary features.\n\"Incus\" is the most type-specific supplementary feature, seen only with cumulonimbus of the species capillatus. A cumulonimbus incus cloud top is one that has spread out into a clear anvil shape as a result of rising air currents hitting the stability layer at the tropopause where the air no longer continues to get colder with increasing altitude.\nThe \"mamma\" feature forms on the bases of clouds as downward-facing bubble-like protuberances caused by localized downdrafts within the cloud. It is also sometimes called \"mammatus\", an earlier version of the term used before a standardization of Latin nomenclature brought about by the World Meteorological Organization during the 20th century. The best-known is cumulonimbus with mammatus, but the mamma feature is also seen occasionally with cirrus, cirrocumulus, altocumulus, altostratus, and stratocumulus.\nA \"tuba\" feature is a cloud column that may hang from the bottom of a cumulus or cumulonimbus. A newly formed or poorly organized column might be comparatively benign, but can quickly intensify into a funnel cloud or tornado.\nAn \"arcus\" feature is a roll cloud with ragged edges attached to the lower front part of cumulus congestus or cumulonimbus that forms along the leading edge of a squall line or thunderstorm outflow. A large arcus formation can have the appearance of a dark menacing arch.\nSeveral new supplementary features have been formally recognized by the World Meteorological Organization (WMO). The feature \"fluctus\" can form under conditions of strong atmospheric wind shear when a stratocumulus, altocumulus, or cirrus cloud breaks into regularly spaced crests. This variant is sometimes known informally as a Kelvin\u2013Helmholtz (wave) cloud. This phenomenon has also been observed in cloud formations over other planets and even in the Sun's atmosphere. Another highly disturbed but more chaotic wave-like cloud feature associated with stratocumulus or altocumulus cloud has been given the Latin name \"asperitas\". The supplementary feature \"cavum\" is a circular fall-streak hole that occasionally forms in a thin layer of supercooled altocumulus or cirrocumulus. Fall streaks consisting of virga or wisps of cirrus are usually seen beneath the hole as ice crystals fall out to a lower altitude. This type of hole is usually larger than typical lacunosus holes. A \"murus\" feature is a cumulonimbus wall cloud with a lowering, rotating cloud base that can lead to the development of tornadoes. A \"cauda\" feature is a tail cloud that extends horizontally away from the murus cloud and is the result of air feeding into the storm.\nAccessory clouds.\nSupplementary cloud formations detached from the main cloud are known as accessory clouds. The heavier precipitating clouds, nimbostratus, towering cumulus (cumulus congestus), and cumulonimbus typically see the formation in precipitation of the \"pannus\" feature, low ragged clouds of the genera and species cumulus fractus or stratus fractus.\nA group of accessory clouds comprise formations that are associated mainly with upward-growing cumuliform and cumulonimbiform clouds of free convection. \"Pileus\" is a cap cloud that can form over a cumulonimbus or large cumulus cloud, whereas a \"velum\" feature is a thin horizontal sheet that sometimes forms like an apron around the middle or in front of the parent cloud. An accessory cloud recently officially recognized by the World Meteorological Organization is the \"flumen\", also known more informally as the \"beaver's tail\". It is formed by the warm, humid inflow of a super-cell thunderstorm, and can be mistaken for a tornado. Although the flumen can indicate a tornado risk, it is similar in appearance to pannus or scud clouds and does not rotate.\nMother clouds.\nClouds initially form in clear air or become clouds when fog rises above surface level. The genus of a newly formed cloud is determined mainly by air mass characteristics such as stability and moisture content. If these characteristics change over time, the genus tends to change accordingly. When this happens, the original genus is called a \"mother cloud\". If the mother cloud retains much of its original form after the appearance of the new genus, it is termed a \"genitus\" cloud. One example of this is \"stratocumulus cumulogenitus\", a stratocumulus cloud formed by the partial spreading of a cumulus type when there is a loss of convective lift. If the mother cloud undergoes a complete change in genus, it is considered to be a \"mutatus\" cloud.\nOther genitus and mutatus clouds.\nThe genitus and mutatus categories have been expanded to include certain types that do not originate from pre-existing clouds. The term \"flammagenitus\" (Latin for 'fire-made') applies to cumulus congestus or cumulonimbus that are formed by large scale fires or volcanic eruptions, the latter having reached altitudes of , and nuclear mushroom clouds having an upward extend of up to . Smaller low-level \"pyrocumulus\" or \"fumulus\" clouds formed by contained industrial activity are now classified as cumulus \"homogenitus\" (Latin for 'man-made'). Contrails formed from the exhaust of aircraft flying in the upper level of the troposphere can persist and spread into formations resembling cirrus which are designated cirrus \"homogenitus\". If a cirrus homogenitus cloud changes fully to any of the high-level genera, they are termed cirrus, cirrostratus, or cirrocumulus \"homomutatus\". Stratus cataractagenitus (Latin for 'cataract-made') are generated by the spray from waterfalls. \"Silvagenitus\" (Latin for 'forest-made') is a stratus cloud that forms as water vapor is added to the air above a forest canopy.\nLarge scale patterns.\nSometimes, certain atmospheric processes cause clouds to become organized into patterns that can cover large areas. These patterns are usually difficult to identify from surface level and are best seen from an aircraft or spacecraft.\nStratocumulus fields.\nStratocumulus clouds can be organized into \"fields\" that take on certain specially classified shapes and characteristics. In general, these fields are more discernible from high altitudes than from ground level. They can often be found in the following forms:\nVortex streets.\nThese patterns are formed from a phenomenon known as a K\u00e1rm\u00e1n vortex which is named after the engineer and fluid dynamicist Theodore von K\u00e1rm\u00e1n. Wind driven clouds, usually mid level altocumulus or high level cirrus, can form into parallel rows that follow the wind direction. When the wind and clouds encounter high elevation land features such as vertically prominent islands, they can form eddies around the high land masses that give the clouds a twisted appearance.\nDistribution.\nConvergence along low-pressure zones.\nAlthough the local distribution of clouds can be significantly influenced by topography, the global prevalence of cloud cover in the troposphere tends to vary more by latitude. It is most prevalent in and along low pressure zones of surface tropospheric convergence which encircle the Earth close to the equator and near the 50th parallels of latitude in the northern and southern hemispheres. The adiabatic cooling processes that lead to the creation of clouds by way of lifting agents are all associated with convergence; a process that involves the horizontal inflow and accumulation of air at a given location, as well as the rate at which this happens.&lt;ref name=\"Convergence/divergence\"&gt;&lt;/ref&gt; Near the equator, increased cloudiness is due to the presence of the low-pressure Intertropical Convergence Zone (ITCZ) where very warm and unstable air promotes mostly cumuliform and cumulonimbiform clouds. Clouds of virtually any type can form along the mid-latitude convergence zones depending on the stability and moisture content of the air. These extratropical convergence zones are occupied by the polar fronts where air masses of polar origin meet and clash with those of tropical or subtropical origin. This leads to the formation of weather-making extratropical cyclones composed of cloud systems that may be stable or unstable to varying degrees according to the stability characteristics of the various airmasses that are in conflict.\nDivergence along high pressure zones.\nDivergence is the opposite of convergence. In the Earth's troposphere, it involves the horizontal outflow of air from the upper part of a rising column of air, or from the lower part of a subsiding column often associated with an area or ridge of high pressure. Cloudiness tends to be least prevalent near the poles and in the subtropics close to the 30th parallels, north and south. The latter are sometimes referred to as the horse latitudes. The presence of a large-scale high-pressure subtropical ridge on each side of the equator reduces cloudiness at these low latitudes. Similar patterns also occur at higher latitudes in both hemispheres.\nLuminance, reflectivity, and coloration.\nThe luminance or brightness of a cloud is determined by how light is reflected, scattered, and transmitted by the cloud's particles. Its brightness may also be affected by the presence of haze or photometeors such as halos and rainbows. In the troposphere, dense, deep clouds exhibit a high reflectance (70\u201395%) throughout the visible spectrum. Tiny particles of water are densely packed and sunlight cannot penetrate far into the cloud before it is reflected out, giving a cloud its characteristic white color, especially when viewed from the top. Cloud droplets tend to scatter light efficiently, so that the intensity of the solar radiation decreases with depth into the gases. As a result, the cloud base can vary from a very light to very-dark-gray depending on the cloud's thickness and how much light is being reflected or transmitted back to the observer. High thin tropospheric clouds reflect less light because of the comparatively low concentration of constituent ice crystals or supercooled water droplets which results in a slightly off-white appearance. However, a thick dense ice-crystal cloud appears brilliant white with pronounced gray shading because of its greater reflectivity.\nAs a tropospheric cloud matures, the dense water droplets may combine to produce larger droplets. If the droplets become too large and heavy to be kept aloft by the air circulation, they will fall from the cloud as rain. By this process of accumulation, the space between droplets becomes increasingly larger, permitting light to penetrate farther into the cloud. If the cloud is sufficiently large and the droplets within are spaced far enough apart, a percentage of the light that enters the cloud is not reflected back out but is absorbed, giving the cloud a darker look. A simple example of this is one's being able to see farther in heavy rain than in heavy fog. This process of reflection/absorption is what causes the range of cloud color from white to black.\nStriking cloud colorations can be seen at any altitude, with the color of a cloud usually being the same as the incident light. During daytime when the sun is relatively high in the sky, tropospheric clouds generally appear bright white on top with varying shades of gray underneath. Thin clouds may look white or appear to have acquired the color of their environment or background. Red, orange, and pink clouds occur almost entirely at sunrise/sunset and are the result of the scattering of sunlight by the atmosphere. When the Sun is just below the horizon, low-level clouds are gray, middle clouds appear rose-colored, and high clouds are white or off-white. Clouds at night are black or dark gray in a moonless sky, or whitish when illuminated by the Moon. They may also reflect the colors of large fires, city lights, or auroras that might be present.\nA cumulonimbus cloud that appears to have a greenish or bluish tint is a sign that it contains extremely high amounts of water; hail or rain which scatter light in a way that gives the cloud a blue color. A green colorization occurs mostly late in the day when the sun is comparatively low in the sky and the incident sunlight has a reddish tinge that appears green when illuminating a very tall bluish cloud. Supercell type storms are more likely to be characterized by this but any storm can appear this way. Coloration such as this does not directly indicate that it is a severe thunderstorm, it only confirms its potential. Since a green/blue tint signifies copious amounts of water, a strong updraft to support it, high winds from the storm raining out, and wet hail; all elements that improve the chance for it to become severe, can all be inferred from this. In addition, the stronger the updraft is, the more likely the storm is to undergo tornadogenesis and to produce large hail and high winds.\nYellowish clouds may be seen in the troposphere in the late spring through early fall months during forest fire season. The yellow color is due to the presence of pollutants in the smoke. Yellowish clouds are caused by the presence of nitrogen dioxide and are sometimes seen in urban areas with high air pollution levels.\nEffects.\nTropospheric clouds exert numerous influences on Earth's troposphere and climate. First and foremost, they are the source of precipitation, thereby greatly influencing the distribution and amount of precipitation. Because of their differential buoyancy relative to surrounding cloud-free air, clouds can be associated with vertical motions of the air that may be convective, frontal, or cyclonic. The motion is upward if the clouds are less dense because condensation of water vapor releases heat, warming the air and thereby decreasing its density. This can lead to downward motion because lifting of the air results in cooling that increases its density. All of these effects are subtly dependent on the vertical temperature and moisture structure of the atmosphere and result in major redistribution of heat that affect the Earth's climate.\nThe complexity and diversity of clouds in the troposphere is a major reason for difficulty in quantifying the effects of clouds on climate and climate change. On the one hand, white cloud tops promote cooling of Earth's surface by reflecting shortwave radiation (visible and near infrared) from the Sun, diminishing the amount of solar radiation that is absorbed at the surface, enhancing the Earth's albedo. Most of the sunlight that reaches the ground is absorbed, warming the surface, which emits radiation upward at longer, infrared wavelengths. At these wavelengths, however, water in the clouds acts as an efficient absorber. The water reacts by radiating, also in the infrared, both upward and downward, and the downward longwave radiation results in increased warming at the surface. This is analogous to the greenhouse effect of greenhouse gases and water vapor.\nHigh-level genus-types particularly show this duality with both short-wave albedo cooling and long-wave greenhouse warming effects. On the whole, \"ice-crystal\" clouds in the upper troposphere (cirrus) tend to favor net warming. However, the cooling effect is dominant with mid-level and low clouds, especially when they form in extensive sheets. Measurements by NASA indicate that, on the whole, the effects of low and mid-level clouds that tend to promote cooling outweigh the warming effects of high layers and the variable outcomes associated with vertically developed clouds.\nAs difficult as it is to evaluate the influences of current clouds on current climate, it is even more problematic to predict changes in cloud patterns and properties in a future, warmer climate, and the resultant cloud influences on future climate. In a warmer climate, more water would enter the atmosphere by evaporation at the surface; as clouds are formed from water vapor, cloudiness would be expected to increase. But in a warmer climate, higher temperatures would tend to evaporate clouds. Both of these statements are considered accurate, and both phenomena, known as cloud feedbacks, are found in climate model calculations. Broadly speaking, if clouds, especially low clouds, increase in a warmer climate, the resultant cooling effect leads to a negative feedback in climate response to increased greenhouse gases. But if low clouds decrease, or if high clouds increase, the feedback is positive. Differing amounts of these feedbacks are the principal reason for differences in climate sensitivities of current global climate models. As a consequence, much research has focused on the response of low and vertical clouds to a changing climate. Leading global models produce quite different results, however, with some showing increasing low clouds and others showing decreases. For these reasons the role of tropospheric clouds in regulating weather and climate remains a leading source of uncertainty in global warming projections.\nStratospheric classification and distribution.\nPolar stratospheric clouds (PSC's) are found in the lowest part of the stratosphere. Moisture is scarce above the troposphere, so nacreous and non-nacreous clouds at this altitude range are restricted to polar regions in the winter where and when the air is coldest.\nPSC's show some variation in structure according to their chemical makeup and atmospheric conditions, but are limited to a single very high range of altitude of about Accordingly, they are classified as a singular type with no differentiated altitude levels, genus types, species, or varieties. There is no Latin nomenclature in the manner of tropospheric clouds, but rather descriptive names of several general forms using common English.\nSupercooled nitric acid and water PSC's, sometimes known as type 1, typically have a stratiform appearance resembling cirrostratus or haze, but because they are not frozen into crystals, do not show the pastel colors of the nacreous types. This type of PSC has been identified as a cause of ozone depletion in the stratosphere. The frozen nacreous types are typically very thin with mother-of-pearl colorations and an undulating cirriform or lenticular (stratocumuliform) appearance. These are sometimes known as type 2.\nMesospheric classification and distribution.\nNoctilucent clouds are the highest in the atmosphere and are found near the top of the mesosphere at about or roughly ten times the altitude of tropospheric high clouds. They are given this Latin derived name because of their illumination well after sunset and before sunrise. They typically have a bluish or silvery white coloration that can resemble brightly illuminated cirrus. Noctilucent clouds may occasionally take on more of a red or orange hue. They are not common or widespread enough to have a significant effect on climate. However, an increasing frequency of occurrence of noctilucent clouds since the 19th century may be the result of climate change.\nOngoing research indicates that convective lift in the mesosphere is strong enough during the polar summer to cause adiabatic cooling of small amount of water vapor to the point of saturation. This tends to produce the coldest temperatures in the entire atmosphere just below the mesopause. There is evidence that smoke particles from burnt-up meteors provide much of the condensation nuclei required for the formation of noctilucent cloud.\nNoctilucent clouds have four major types based on physical structure and appearance. Type I veils are very tenuous and lack well-defined structure, somewhat like cirrostratus fibratus or poorly defined cirrus. Type II bands are long streaks that often occur in groups arranged roughly parallel to each other. They are usually more widely spaced than the bands or elements seen with cirrocumulus clouds. Type III billows are arrangements of closely spaced, roughly parallel short streaks that mostly resemble cirrus. Type IV whirls are partial or, more rarely, complete rings of cloud with dark centers.\nDistribution in the mesosphere is similar to the stratosphere except at much higher altitudes. Because of the need for maximum cooling of the water vapor to produce noctilucent clouds, their distribution tends to be restricted to polar regions of Earth. Sightings are rare more than 45 degrees south of the North Pole or north of the South Pole.\nExtraterrestrial.\nCloud cover has been seen on most other planets in the Solar System. Venus's thick clouds are composed of sulfur dioxide (due to volcanic activity) and appear to be almost entirely stratiform. They are arranged in three main layers at altitudes of 45 to 65\u00a0km that obscure the planet's surface and can produce virga. No embedded cumuliform types have been identified, but broken stratocumuliform wave formations are sometimes seen in the top layer that reveal more continuous layer clouds underneath. On Mars, noctilucent, cirrus, cirrocumulus and stratocumulus composed of water-ice have been detected mostly near the poles. Water-ice fogs have also been detected on Mars.\nBoth Jupiter and Saturn have an outer cirriform cloud deck composed of ammonia, an intermediate stratiform haze-cloud layer made of ammonium hydrosulfide, and an inner deck of cumulus water clouds. Embedded cumulonimbus are known to exist near the Great Red Spot on Jupiter. The same category-types can be found covering Uranus and Neptune, but are all composed of methane. Saturn's moon Titan has cirrus clouds believed to be composed largely of methane. The \"Cassini\u2013Huygens\" Saturn mission uncovered evidence of polar stratospheric clouds.\nSome planets outside the Solar System are known to have atmospheric clouds. In 2013, the detection of high altitude optically thick clouds in the atmosphere of exoplanet Kepler-7b was announced, and, in December 2013, in the atmospheres of GJ 436 b and GJ 1214 b.\nIn culture and religion.\nClouds play an important mythical or non-scientific role in various cultures and religious traditions. The ancient Akkadians believed that the clouds (in meteorology, probably the supplementary feature \"mamma\") were the breasts of the sky goddess Antu, and that rain was milk from her breasts. In the Book of Exodus, Yahweh is described as guiding the Israelites through the desert in the form of a \"pillar of cloud\" by day and a \"pillar of fire\" by night. In Mandaeism, uthras (celestial beings) are occasionally mentioned as being in \"anana\" (\"clouds\"), which can also be interpreted as female consorts.\nIn the ancient Greek comedy \"The Clouds\", written by Aristophanes and first performed at the City Dionysia in 423 BC, the philosopher Socrates declares that the Clouds are the only true deities and tells the main character Strepsiades not to worship any deities other than the Clouds, but to pay homage to them alone. In the play, the Clouds change shape to reveal the true nature of whoever is looking at them. They are hailed the source of inspiration to comic poets and philosophers; they are masters of rhetoric, regarding eloquence and sophistry alike as their \"friends\".\nIn Shakespeare's \"Hamlet\", the prince mocks the courtier Polonius' sycophancy by pointing out various incompatible shapes in the same cloud, to which Polonius readily agrees every time.\nIn China, clouds are symbols of luck and happiness. Overlapping clouds are thought to imply eternal happiness and clouds of different colors are said to indicate \"multiplied blessings\".\nInformal cloud watching or cloud gazing is a popular activity involving watching the clouds and looking for shapes in them, a form of pareidolia.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47517", "revid": "234689", "url": "https://en.wikipedia.org/wiki?curid=47517", "title": "Planetary body", "text": ""}
{"id": "47518", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=47518", "title": "Cloud feedback", "text": "Feedback between climate change and the effect of clouds on radiation\nA cloud feedback is a climate change feedback where some aspects of cloud characteristics (e.g. cloud cover, composition or height) are altered due to climate change, and these changes then further affect the Earth's energy balance. On their own, clouds are already an important part of the climate system, as they consist of liquid droplets and ice particles, which absorb infrared radiation and reflect visible solar radiation. Clouds at low altitudes have a stronger cooling effect, and those at high altitudes have a stronger warming effect. Altogether, clouds make the Earth cooler than it would have been without them.\nIf climate change causes low-level cloud cover to become more widespread, then these clouds will increase planetary albedo and contribute to cooling, making the overall cloud feedback \"negative\" (one that slows down the warming). Vice versa, if they change in such a way that their warming effect increases relative to their cooling effect then the net cloud feedback, then the net cloud feedback will be \"positive\" and accelerate the warming, as clouds will be less reflective and trap more heat in the atmosphere. \nThere are many mechanisms by which cloud feedbacks occur. Most substantially, evidence points to climate change causing high clouds to rise in altitude (a positive feedback), the coverage of tropical low clouds to reduce (a positive feedback) and polar low clouds to become more reflective (a negative feedback). Aside from cloud responses to human-induced warming through greenhouse gases, the interaction of clouds with aerosol particles is known to affect cloud reflectivity, and may modulate the strength of cloud feedbacks. Cloud feedback processes have been represented in every major climate model from the 1980s onwards. Observations and climate model results now provide \"high confidence\" that the overall cloud feedback on climate change is positive.\nCloud feedbacks are estimated using both observational data and climate models. Uncertainty in both these aspects - for example, incomplete observational data or uncertainty in the representation of processes in models mean that cloud feedback estimates differ substantially between models. Thus, models can simulate cloud feedback as very positive or only weakly positive, and these disagreements are the main reason why climate models can have substantial differences in transient climate response and climate sensitivity. In particular, a minority of the Coupled Model Intercomparison Project Phase 6 (CMIP6) models have made headlines before the publication of the IPCC Sixth Assessment Report (AR6) due to their high estimates of equilibrium climate sensitivity (ECS). This had occurred because they estimated cloud feedback as highly positive. Although those particular models were soon found to contradict both observations and paleoclimate evidence, it is suggested to be problematic if ruling out these 'hot' models solely based on ECS and care should be taken when weighting climate model ensembles by temperature alone.\nOne reason why constraining cloud feedbacks has been difficult is because humans affect clouds in another major way besides the warming from greenhouse gases. Small atmospheric sulfate particles, or aerosols, are generated due to the same sulfur-heavy air pollution which also causes acid rain, but they are also very reflective, to the point their concentrations in the atmosphere cause reductions in visible sunlight known as global dimming. These particles affect the clouds in multiple ways, mostly making them more reflective through aerosol-cloud interactions. This means that changes in clouds caused by aerosols can be confused for an evidence of negative cloud feedback, and separating the two effects has been difficult.\nHow clouds affect radiation and climate feedback.\nClouds have two major effects on the Earth's energy budget. Firstly, they reflect shortwave radiation from sunlight back to space due to their high albedo - a cooling effect for the Earth. Secondly, the condensed and frozen water contained inside them absorbs longwave radiation emitted by the Earth's surface. Clouds themselves also emit longwave radiation, both towards the surface and to space. Clouds are usually colder than the surface, so that they emit less energy upward. The net longwave effect is that the presence of clouds reduces the radiation emitted to space, i.e. a warming effect.\nIn meteorology, the difference in the radiation budget caused by clouds, relative to cloud-free conditions, is described as the cloud radiative effect (CRE). This is also sometimes referred to as cloud radiative forcing (CRF). However, since cloud changes are not normally considered an external forcing of climate, CRE is the most commonly used term.\nThis can be described by the equation\nformula_1\nWhere \"CRE\" is cloud radiative effect (W m-2), \"Rall-sky\" is the radiation flux (W m-2) under actual sky conditions, and \"Rclear-sky\" is a hypothetical radiation flux (W m-2) computed for the identical temperature and moisture conditions but omitting the optical effects of clouds.\nCloud feedback is one of a number of climate feedbacks. Cloud feedback sums up the influence of all aspects of the cloud field on radiation, weighted by the sensitivity of each aspect to global average temperature change. In equation form, \nformula_2\nwhere \"N\" is the Earth's net radiation (W m-2), formula_3 is the change in some aspect or characteristic of cloudiness (e.g. cloud cover, thickness, particle sizes, height), and \"T\" is the global mean near-surface air temperature (K).\nOn a hypothetical cloud-free Earth, water vapor would contribute 67% and CO2 24% of the greenhouse effect keeping the planet warmer than it would be without an atmosphere. In actual (all-sky) conditions, clouds contribute 25%, and their screening effect lowers the vapor and CO2 contributions to 50% and 19% respectively. According to 1990 estimates, the presence of clouds reduces the outgoing longwave radiation by about 31 W/m2. However, it also increases the global albedo from 15% to 30%, and this reduces the amount of solar radiation absorbed by the Earth by about 44 W/m2. Thus, the observed global population of clouds contributes a net \"cooling\" of about 13 W/m2. If all clouds were removed with all else remaining the same, the Earth would lose this much cooling and the global temperatures would increase.\nClimate change increases the amount of water vapor in the atmosphere due to the Clausius\u2013Clapeyron relation, in what is known as the water-vapor feedback. It also affects a range of cloud properties, such as their height, the typical distribution throughout the atmosphere, and cloud microphysics, such as the amount of water droplets held, all of which then affect clouds' radiative forcing. Differences in those properties change the role of clouds in the Earth's energy budget. The name \"cloud feedback\" refers to this relationship between climate change, cloud properties, and clouds' radiative forcing. Clouds also affect the magnitude of internally generated climate variability.\nCloud feedback mechanisms.\nLow clouds.\nLow clouds include the cumulus, stratocumulus and stratus cloud types. Scientifically they tend to be defined as those clouds with cloud top pressure higher than 680 hPa, according the to International Satellite Cloud Climatology Project. The feedback of low clouds primarily arises from effects on shortwave radiation.\nTropical marine low-cloud feedback.\nMultiple lines of evidence, including global climate models, observations and high resolution process modelling, agree that the tropical low cloud amount is likely to decrease, which corresponds to a positive feedback.\nPossible break-up of equatorial stratocumulus clouds.\nIn 2019, a study employed a large eddy simulation model to estimate that equatorial stratocumulus clouds could break up and scatter when CO2 levels rise above 1,200\u2009ppm (almost three times higher than the current levels, and over 4 times greater than the preindustrial levels). The study estimated that this would cause a surface warming of about globally and in the subtropics, which would be in addition to at least already caused by such CO2 concentrations. In addition, stratocumulus clouds would not reform until the CO2 concentrations drop to a much lower level. It was suggested that this finding could help explain past episodes of unusually rapid warming such as Paleocene-Eocene Thermal Maximum. In 2020, further work from the same authors revealed that in their large eddy simulation, this tipping point cannot be stopped with solar radiation modification: in a hypothetical scenario where very high CO2 emissions continue for a long time but are offset with extensive solar radiation modification, the break-up of stratocumulus clouds is simply delayed until CO2 concentrations hit 1,700 ppm, at which point it would still cause around of unavoidable warming.\nHowever, because large eddy simulation models are simpler and smaller-scale than the general circulation models used for climate projections, with limited representation of atmospheric processes like subsidence, this finding is currently considered speculative. Other scientists say that the model used in that study unrealistically extrapolates the behavior of small cloud areas onto all cloud decks, and that it is incapable of simulating anything other than a rapid transition, with some comparing it to \"a knob with two settings\". Additionally, CO2 concentrations would only reach 1,200 ppm if the world follows Representative Concentration Pathway 8.5, which represents the highest possible greenhouse gas emission scenario and involves a massive expansion of coal infrastructure. In that case, 1,200 ppm would be passed shortly after 2100.\nMid-latitude marine low-cloud feedback.\nThere is both observational and modelling evidence that a positive mid-latitude low-cloud feedback is feasible. In part, such a feedback could arise for similar reasons to the tropical marine low-cloud feedback. In addition, a poleward shift of mid-latitude Storm tracks would reduce the solar radiation interacting with low cloud and result in a positive feedback.\nHigh-latitude low-cloud optical depth feedback.\nThe optical depth (or opacity) of cloud can increase if the number of cloud particles increases for given water content, or the water content increases. Related to this, a shift from liquid cloud particles to ice cloud particles tends to correspond to a shift from more numerous smaller particles to fewer larger particles, and therefore can decrease optical depth. A number of studies have explored the potential for high-latitude cloud optical depth to contribute to climate feedback. However, there is not clear evidence that a non-zero feedback exists for this cloud type.\nLand clouds.\nLand clouds can include clouds types of differing heights.\nLarger warming of land compared to ocean under climate change is expected to lead to reduced cloud cover over land, especially reduced low cloud cover. An increase in atmospheric temperature means that higher water vapour amounts will be needed to reach saturation. Because transport of moisture from the oceans and evaporation from the soil is not expected to increase by as much as the saturation level, the relative humidity of the air is expected to reduce, and therefore reduce the cloud amount. If low clouds reduce more than other clouds then this will result in increased solar absorption at the surface and a positive feedback.\nHigh clouds.\nHigh clouds include the cirrus, cirrostratus and cumulonimbus cloud types. Scientifically they tend to be defined as those clouds with cloud top pressure lower than 440 hPa.. The focus scientifically also tends to be on tropical ocean high cloud. \nUnlike low clouds, whose effect on radiation is primarily in the shortwave, high clouds substantially effect both shortwave and longwave radiation. However, the resultant net radiative effect involves a substantial, though not necessarily complete, cancellation of the two effects in the longwave and shortwave.\nFor high clouds the feedback is currently positive in total, as the shortwave feedback is near zero and the longwave feedback is positive. It is together with the mid-level cloud feedback a larger contributor to the total cloud feedback than low clouds.\nHigh-cloud altitude feedback.\nHigh clouds are expected to grow to taller heights under climate change. This arises from physical understanding which relates the height of convective cloud to the vertical profile of water vapour in the tropics. Predictions based on theory are broadly confirmed by projections with climate models and high resolution process models. As such, the high-cloud altitude feedback is one of the most clearly established positive cloud feedbacks.\nThe altitude of the high clouds increases with rising temperatures. Higher temperatures on the surface force the moisture to rise, which is fundamentally described by the Clausius Clapeyron equation. The altitude at which the radiative cooling is still effective is closely tied to the humidity and rises equally. The altitude, at which the radiative cooling becomes inefficient due to a lack of moisture, then determines the detrainment height of deep convection due to the mass conservation. The cloud top height therefore strongly depends on the surface temperature.\nThere are three theories on how the altitude and thus temperature depends on surface warming. The FAT (Fixed Anvil Temperature) hypothesis argues, that the isotherms shift upwards with global warming and the temperature at the cloud top stays therefore constant. This results in a positive feedback, since no more radiation is emitted while the surface temperature is rising. According to the FAT hypothesis this leads to a feedback of 0,27 W mformula_4 Kformula_5. The second hypothesis called PHAT (Proportionally Higher Anvil Temperature) claims a smaller cloud feedback of 0.20 W mformula_4 Kformula_5, due to a slight warming of the cloud tops which agrees better with observations. The static stability increases with higher surface temperatures in the upper troposphere and lets the clouds shift slightly to warmer temperatures. The third hypothesis is FAP (Fixed Anvil Pressure) which assumes a constant cloud top pressure with a warming climate, as if the cloud top does not move upwards. This results in a negative longwave feedback, which does not agree with observations. It can be used to calculate the impact of the cloud height change on the longwave feedback. Most models agree with the PHAT hypothesis which also agrees the most with observations.\nTropical high cloud area feedback.\nIt is broadly expected that high cloud amount originisting from deep convection will reduce with warming. Two mechanisms can lead to a decrease in the area fraction a. The warming at the surface decreases the moist adiabat temperature reduction with height which leads to a decrease of the clear sky subsidence. Since the convective mass flux has to be equal to the clear sky subsidence it decreases as well and with it potentially the cloud area fraction. Another argument for a smaller area fraction is that the self-aggregation of clouds increases at higher temperatures. This would lead to smaller convective areas and larger dry areas which increase the radiative longwave cooling. Recent work has shown that high cloud not of convective origin may not be so clearly predicted. However, high cloud that follows the broadly understood physical relationships tends to have a negative cloud radiative effect, and therefore a reduction in its amount can produce a small positive feedback.\nSome past research has conflated feedback associated with high cloud (also referred to as \"anvil cloud\") area with feedback associated with high cloud optical depth. More recent studies using independent approaches have used analysis that accurately determines feedback resulting from high cloud amount changes. These studies based on observations, high resolution process models and physical theory conclude that the net tropical high cloud amount feedback is near zero or slightly positive.\nHigh cloud optical depth feedback.\nOptical depth (or opacity) or clouds changes is as a result of composition or thickness. It has not been well-studied or distinguished from other forms of high cloud feedback until recently. Observations show high cloud optical depth has having reduced in the last couple of decades. Physical theory has proposed that there is potential for a large feedback in the shortwave component of optical depth (manifesting in cloud albedo). However, process based models show a large uncertainty of the optical depth feedback. The topic remains an active area of research, with cloud microphysical simulation being a major constraint on the ability of existing climate models to provide useful understanding of the optical depth feedback.\nChallenges.\nIt is difficult to detect the reason for a change in the SW and LW radiation due to cloud feedback, because there are a lot of cloud responses which could be the cause for a specific radiation feedback. Furthermore is it difficult to not count in clear sky effects. There are techniques to decompose the cloud feedbacks in models and their triggers in detail by showing the cloud fraction as a function of cloud-top pressure and the optical depth of the cloud. In the GCM, which are mostly used, the main challenge is the parametrization of clouds, especially in coarse-resolution models. The characteristics of clouds need to be parametrized in such a way, that the different feedbacks and physical interactions are as correct as possible in order to decrease the uncertainty of the models.\nAnother challenge when dealing with (high) cloud feedbacks, is that the LW and SW part often cancel each other out, so that only a small total feedback is left. The positive and negative feedback parts are not neglectable, since they can change independent of one another with rising temperature.\nRepresentation in climate models.\nClimate models have represented clouds and cloud processes for a very long time. Cloud feedback was already a standard feature in climate models designed in the 1980s. However, the physics of clouds are very complex, so models often represent various types of clouds in different ways, and even small variations between models can lead to significant changes in temperature and precipitation response. Climate scientists devote a lot of effort to resolving this issue. This includes the Cloud Feedback Model Intercomparison Project (CFMIP), where models simulate cloud processes under different conditions and their output is compared with the observational data. (AR6 WG1, Ch1, 223) When the Intergovernmental Panel on Climate Change had published its Sixth Assessment Report (AR6) in 2021, the uncertainty range regarding cloud feedback strength became 50% smaller since the time of the AR5 in 2014. \nThis happened because of major improvements in the understanding of cloud behaviour over the subtropical oceans. As the result, there was \"high confidence\" that the overall cloud feedback is positive (contributes to warming). The AR6 value for cloud feedback is +0.42 [\u20130.10 to 0.94] W m\u20132 per every in warming. This estimate is derived from multiple lines of evidence, including both models and observations. The tropical high-cloud amount feedback is the main remaining area for improvement. The only way total cloud feedback may still be slightly negative is if either this feedback, or the optical depth feedback in the Southern Ocean clouds is suddenly found to be \"extremely large\"; the probability of that is considered to be below 10%. As of 2024, most recent observations from the CALIPSO satellite instead indicate that the tropical cloud feedback is very weak.\nIn spite of these improvements, clouds remain the least well-understood climate feedback, and they are the main reason why models estimate differing values for equilibrium climate sensitivity (ECS). ECS is an estimate of long-term (multi-century) warming in response to a \"doubling\" in CO2-equivalent greenhouse gas concentrations: if the future emissions are not low, it also becomes the most important factor for determining 21st century temperatures. In general, the current generation of gold-standard climate models, CMIP6, operates with larger climate sensitivity than the previous generation, and this is largely because cloud feedback is about 20% more positive than it was in CMIP5.\nHowever, the \"median\" cloud feedback is only slightly larger in CMIP6 than it was in CMIP5; the average is so much higher only because several \"hot\" models have much stronger cloud feedback and higher sensitivity than the rest. Those models have a sensitivity of and their presence had increased the median model sensitivity from in CMIP5 to in CMIP6. These model results had attracted considerable attention when they were first published in 2019, as they would have meant faster and more severe warming if they were accurate. It was soon found that the output of those \"hot\" models is inconsistent with both observations and paleoclimate evidence, so the consensus AR6 value for cloud feedback is smaller than the mean model output alone. The best estimate of climate sensitivity in AR6 is at , as this is in a better agreement with observations and paleoclimate findings.\nRole of aerosol and aerosol-cloud interaction.\nAtmospheric aerosols\u2014fine particles suspended in the air\u2014affect cloud formation and properties, which also alters their impact on climate. While some aerosols, such as black carbon particles, make the clouds darker and thus contribute to warming, by far the strongest effect is from sulfates, which increase the number of cloud droplets, making the clouds more reflective, and helping them cool the climate more. These influences of aerosols on clouds are aerosol \"indirect\" effects, of which the famous one are the Twomey effect and the Albrecht effect through aerosols acting as cloud condensation nuclei (CCN). Less well understood indirect effects of aerosols are on the formation of ice, through variation in concentrations and types of ice nucleating particles. Aerosols also have an indirect effect on liquid water path, and determining it involves computationally heavy continuous calculations of evaporation and condensation within clouds. Climate models generally assume that aerosols increase liquid water path, which makes the clouds even more reflective. However, satellite observations taken in 2010s suggested that aerosols decreased liquid water path instead, and in 2018, this was reproduced in a model which integrated more complex cloud microphysics. Yet, 2019 research found that earlier satellite observations were biased by failing to account for the thickest, most water-heavy clouds naturally raining more and shedding more particulates: very strong aerosol cooling was seen when comparing clouds of the same thickness.\nMoreover, large-scale observations can be confounded by changes in other atmospheric factors, like humidity: i.e. it was found that while post-1980 improvements in air quality would have reduced the number of clouds over the East Coast of the United States by around 20%, this was offset by the increase in relative humidity caused by atmospheric response to AMOC slowdown. Similarly, while the initial research looking at sulfates from the 2014\u20132015 eruption of B\u00e1r\u00f0arbunga found that they caused no change in liquid water path, it was later suggested that this finding was confounded by counteracting changes in humidity. \nTo avoid confounders, many observations of aerosol effects focus on ship tracks, but post-2020 research found that visible ship tracks are a poor proxy for other clouds, and estimates derived from them overestimate aerosol cooling by as much as 200%. At the same time, other research found that the majority of ship tracks are \"invisible\" to satellites, meaning that the earlier research had underestimated aerosol cooling by overlooking them. Finally, 2023 research indicates that all climate models have underestimated sulfur emissions from volcanoes which occur in the background, outside of major eruptions, and so had consequently overestimated the cooling provided by anthropogenic aerosols, especially in the Arctic climate.\nEstimates of how much aerosols affect cloud cooling are very important, because the amount of sulfate aerosols in the air had undergone dramatic changes in the recent decades. First, it had increased greatly from the 1950s to 1980s, largely due to the widespread burning of sulfur-heavy coal, which caused an observable reduction in visible sunlight that had been described as global dimming. Then, it started to decline substantially from the 1990s onwards and is expected to continue to decline in the future, due to the measures to combat acid rain and other impacts of air pollution. Consequently, the aerosols provided a considerable cooling effect which counteracted or \"masked\" some of the greenhouse effect from human emissions, and this effect had been declining as well, which contributed to acceleration of climate change.\nClimate models do account for the presence of aerosols and their recent and future decline in their projections, and typically estimate that the cooling they provide in 2020s is similar to the warming from human-added atmospheric methane, meaning that simultaneous reductions in both would effectively cancel each other out. However, the existing uncertainty about aerosol-cloud interactions likewise introduces uncertainty into models, particularly when concerning predictions of changes in weather events over the regions with a poorer historical record of atmospheric observations. See also\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47519", "revid": "22802661", "url": "https://en.wikipedia.org/wiki?curid=47519", "title": "Cloud forcing", "text": ""}
{"id": "47520", "revid": "14423536", "url": "https://en.wikipedia.org/wiki?curid=47520", "title": "Coccolithophore", "text": "Unicellular algae responsible for the formation of chalk\nCoccolithophores, or coccolithophorids, are single-celled organisms which are part of the phytoplankton, the autotrophic (self-feeding) component of the plankton community. They form a group of about 200 species, and belong either to the kingdom Protista, according to Robert Whittaker's five-kingdom system, or clade Hacrobia, according to a newer biological classification system. Within the Hacrobia, the coccolithophores are in the phylum or division Haptophyta, class Prymnesiophyceae (or Coccolithophyceae). Coccolithophores are almost exclusively marine, are photosynthetic and mixotrophic, and exist in large numbers throughout the sunlight zone of the ocean.\nCoccolithophores are the most productive calcifying organisms on the planet, covering themselves with a calcium carbonate shell called a \"coccosphere\". It is formed when a cell arrange a series of interlocking coccoliths to completely cover its surface. Coccoliths (minute calcium carbonate platelets) are generated within the cell, suggesting that the cell diameter cannot be exceeded by the maximum coccolith size. In addition, coccolith size and the orientation of calcite crystals forming coccoliths can vary among species. Although the interlocking structure of coccoliths offers strong mechanical protection for cell, the specific topology of the coccoliths (the way they interconnect within the coccosphere) and the mechanisms governing their arrangement remain unclear. One key function may be that the coccosphere offers protection against microzooplankton predation, which is one of the main causes of phytoplankton death in the ocean.\nCoccolithophores are ecologically important, and biogeochemically they play significant roles in the marine biological pump and the carbon cycle. Depending on habitat, they can produce up to 40 percent of the local marine primary production. They are of particular interest to those studying global climate change because, as ocean acidity increases, their coccoliths may become even more important as a carbon sink. Management strategies are being employed to prevent eutrophication-related coccolithophore blooms, as these blooms lead to a decrease in nutrient flow to lower levels of the ocean.\nThe most abundant species of coccolithophore, \"Emiliania huxleyi\", belongs to the order Isochrysidales and family Noelaerhabdaceae. It is found in temperate, subtropical, and tropical oceans. This makes \"E. huxleyi\" an important part of the planktonic base of a large proportion of marine food webs. It is also the fastest growing coccolithophore in laboratory cultures. It is studied for the extensive blooms it forms in nutrient depleted waters after the reformation of the summer thermocline. and for its production of molecules known as alkenones that are commonly used by earth scientists as a means to estimate past sea surface temperatures.\nOverview.\nCoccolithophores (or coccolithophorids, from the adjective) form a group of about 200 phytoplankton species. They belong either to the kingdom Protista, according to Robert Whittaker's Five kingdom classification, or clade Hacrobia, according to the newer biological classification system. Within the Hacrobia, the coccolithophores are in the phylum or division Haptophyta, class Prymnesiophyceae (or Coccolithophyceae). Coccolithophores are distinguished by special calcium carbonate plates (or scales) of uncertain function called \"coccoliths\", which are also important microfossils. However, there are Prymnesiophyceae species lacking coccoliths (e.g. in genus \"Prymnesium\"), so not every member of Prymnesiophyceae is a coccolithophore.\nCoccolithophores are single-celled phytoplankton that produce small calcium carbonate (CaCO3) scales (coccoliths) which cover the cell surface in the form of a spherical coating, called a coccosphere. Many species are also mixotrophs, and are able to photosynthesise as well as ingest prey.\nCoccolithophores have been an integral part of marine plankton communities since the Jurassic. Today, coccolithophores contribute ~1\u201310% to inorganic carbon fixation (calcification) to total carbon fixation (calcification plus photosynthesis) in the surface ocean and ~50% to pelagic CaCO3 sediments. Their calcareous shell increases the sinking velocity of photosynthetically fixed CO2 into the deep ocean by ballasting organic matter. At the same time, the biogenic precipitation of calcium carbonate during coccolith formation reduces the total alkalinity of seawater and releases CO2. Thus, coccolithophores play an important role in the marine carbon cycle by influencing the efficiency of the biological carbon pump and the oceanic uptake of atmospheric CO2.\nAs of 2021, it is not known why coccolithophores calcify and how their ability to produce coccoliths is associated with their ecological success. The most plausible benefit of having a coccosphere seems to be a protection against predators or viruses. Viral infection is an important cause of phytoplankton death in the oceans, and it has recently been shown that calcification can influence the interaction between a coccolithophore and its virus. The major predators of marine phytoplankton are microzooplankton like ciliates and dinoflagellates. These are estimated to consume about two-thirds of the primary production in the ocean and microzooplankton can exert a strong grazing pressure on coccolithophore populations. Although calcification does not prevent predation, it has been argued that the coccosphere reduces the grazing efficiency by making it more difficult for the predator to utilise the organic content of coccolithophores. Heterotrophic protists are able to selectively choose prey on the basis of its size or shape and through chemical signals and may thus favor other prey that is available and not protected by coccoliths.\nStructure.\nCoccolithophores are spherical cells about 5\u2013100 micrometres across, enclosed by calcareous plates called coccoliths, which are about 2\u201325 micrometres across. Each cell contains two brown chloroplasts which surround the nucleus.\nEnclosed in each coccosphere is a single cell with membrane bound organelles. Two large chloroplasts with brown pigment are located on either side of the cell and surround the nucleus, mitochondria, golgi apparatus, endoplasmic reticulum, and other organelles. Each cell also has two flagellar structures, which are involved not only in motility, but also in mitosis and formation of the cytoskeleton. In some species, a functional or vestigial haptonema is also present. This structure, which is unique to haptophytes, coils and uncoils in response to environmental stimuli. Although poorly understood, it has been proposed to be involved in prey capture. \nEcology.\nLife history strategy.\nThe complex life cycle of coccolithophores is known as a haplodiplontic life cycle, and is characterized by an alternation of both asexual and sexual phases. The asexual phase is known as the haploid phase, while the sexual phase is known as the diploid phase. During the haploid phase, coccolithophores produce haploid cells through mitosis. These haploid cells can then divide further through mitosis or undergo sexual reproduction with other haploid cells. The resulting diploid cell goes through meiosis to produce haploid cells again, starting the cycle over. With coccolithophores, asexual reproduction by mitosis is possible in both phases of the life cycle, which is a contrast with most other organisms that have alternating life cycles. Both abiotic and biotic factors may affect the frequency with which each phase occurs.\nCoccolithophores reproduce asexually through binary fission. In this process the coccoliths from the parent cell are divided between the two daughter cells. There have been suggestions stating the possible presence of a sexual reproduction process due to the diploid stages of the coccolithophores, but this process has never been observed.\nK or r- selected strategies of coccolithophores depend on their life cycle stage. When coccolithophores are diploid, they are r-selected. In this phase they tolerate a wider range of nutrient compositions. When they are haploid they are K- selected and are often more competitive in stable low nutrient environments. Most coccolithophores are K strategist and are usually found on nutrient-poor surface waters. They are poor competitors when compared to other phytoplankton and thrive in habitats where other phytoplankton would not survive. These two stages in the life cycle of coccolithophores occur seasonally, where more nutrition is available in warmer seasons and less is available in cooler seasons. This type of life cycle is known as a complex heteromorphic life cycle.\nGlobal distribution.\nCoccolithophores occur throughout the world's oceans. Their distribution varies vertically by stratified layers in the ocean and geographically by different temporal zones. While most modern coccolithophores can be located in their associated stratified oligotrophic conditions, the most abundant areas of coccolithophores where there is the highest species diversity are located in subtropical zones with a temperate climate. While water temperature and the amount of light intensity entering the water's surface are the more influential factors in determining where species are located, the ocean currents also can determine the location where certain species of coccolithophores are found.\nAlthough motility and colony formation vary according to the life cycle of different coccolithophore species, there is often alternation between a motile, haploid phase, and a non-motile diploid phase. In both phases, the organism's dispersal is largely due to ocean currents and circulation patterns.\nWithin the Pacific Ocean, approximately 90 species have been identified with six separate zones relating to different Pacific currents that contain unique groupings of different species of coccolithophores. The highest diversity of coccolithophores in the Pacific Ocean was in an area of the ocean considered the Central North Zone which is an area between 30 oN and 5 oN, composed of the North Equatorial Current and the Equatorial Countercurrent. These two currents move in opposite directions, east and west, allowing for a strong mixing of waters and allowing a large variety of species to populate the area.\nIn the Atlantic Ocean, the most abundant species are \"E. huxleyi\" and \"Florisphaera profunda\" with smaller concentrations of the species \"Umbellosphaera\" \"irregularis\", \"Umbellosphaera tenuis\" and different species of \"Gephyrocapsa\". Deep-dwelling coccolithophore species abundance is greatly affected by nutricline and thermocline depths. These coccolithophores increase in abundance when the nutricline and thermocline are deep and decrease when they are shallow.\nThe complete distribution of coccolithophores is currently not known and some regions, such as the Indian Ocean, are not as well studied as other locations in the Pacific and Atlantic Oceans. It is also very hard to explain distributions due to multiple constantly changing factors involving the ocean's properties, such as coastal and equatorial upwelling, frontal systems, benthic environments, unique oceanic topography, and pockets of isolated high or low water temperatures.\nThe upper photic zone is low in nutrient concentration, high in light intensity and penetration, and usually higher in temperature. The lower photic zone is high in nutrient concentration, low in light intensity and penetration and relatively cool. The middle photic zone is an area that contains the same values in between that of the lower and upper photic zones.\nGreat Calcite Belt.\nThe Great Calcite Belt of the Southern Ocean is a region of elevated summertime upper ocean calcite concentration derived from coccolithophores, despite the region being known for its diatom predominance. The overlap of two major phytoplankton groups, coccolithophores and diatoms, in the dynamic frontal systems characteristic of this region provides an ideal setting to study environmental\ninfluences on the distribution of different species within these taxonomic groups.\nThe Great Calcite Belt, defined as an elevated particulate inorganic carbon (PIC) feature occurring alongside seasonally elevated chlorophyll a in spring and summer in the Southern Ocean, plays an important role in climate fluctuations, accounting for over 60% of the Southern Ocean area (30\u201360\u00b0 S). The region between 30\u00b0 and 50\u00b0 S has the highest uptake of anthropogenic carbon dioxide (CO2) alongside the North Atlantic and North Pacific oceans.\nEffect of global climate change on distribution.\nRecent studies show that climate change has direct and indirect impacts on Coccolithophore distribution and productivity. They will inevitably be affected by the increasing temperatures and thermal stratification of the top layer of the ocean, since these are prime controls on their ecology, although it is not clear whether global warming would result in net increase or decrease of coccolithophores. As they are calcifying organisms, it has been suggested that ocean acidification due to increasing carbon dioxide could severely affect coccolithophores. Recent CO2 increases have seen a sharp increase in the population of coccolithophores.\nRole in the food web.\nCoccolithophores are one of the more abundant primary producers in the ocean. As such, they are a large contributor to the primary productivity of the tropical and subtropical oceans, however, exactly how much has yet to have been recorded.\nDependence on nutrients.\nThe ratio between the concentrations of nitrogen, phosphorus and silicate in particular areas of the ocean dictates competitive dominance within phytoplankton communities. Each ratio essentially tips the odds in favor of either diatoms or other groups of phytoplankton, such as coccolithophores. A low silicate to nitrogen and phosphorus ratio allows coccolithophores to outcompete other phytoplankton species; however, when silicate to phosphorus to nitrogen ratios are high coccolithophores are outcompeted by diatoms. The increase in agricultural processes lead to eutrophication of waters and thus, coccolithophore blooms in these high nitrogen and phosphorus, low silicate environments.\nImpact on water column productivity.\nThe calcite in calcium carbonate allows coccoliths to scatter more light than they absorb. This has two important consequences: 1) Surface waters become brighter, meaning they have a higher albedo, and 2) there is induced photoinhibition, meaning photosythetic production is diminished due to an excess of light. In case 1), a high concentration of coccoliths leads to a simultaneous increase in surface water temperature and decrease in the temperature of deeper waters. This results in more stratification in the water column and a decrease in the vertical mixing of nutrients. However, a 2012 study estimated that the overall effect of coccolithophores on the increase in radiative forcing of the ocean is less than that from anthropogenic factors. Therefore, the overall result of large blooms of coccolithophores is a decrease in water column productivity, rather than a contribution to global warming.\nPredator-prey interactions.\nTheir predators include the common predators of all phytoplankton including small fish, zooplankton, and shellfish larvae. Viruses specific to this species have been isolated from several locations worldwide and appear to play a major role in spring bloom dynamics.\nToxicity.\nNo environmental evidence of coccolithophore toxicity has been reported, but they belong to the class Prymnesiophyceae which contain orders with toxic species. Toxic species have been found in the genera \"Prymnesium\" Massart and \"Chrysochromulina\" Lackey. Members of the genus \"Prymnesium\" have been found to produce haemolytic compounds, the agent responsible for toxicity. Some of these toxic species are responsible for large fish kills and can be accumulated in organisms such as shellfish; transferring it through the food chain. In laboratory tests for toxicity members of the oceanic coccolithophore genera \"Emiliania, Gephyrocapsa, Calcidiscus\" and \"Coccolithus\" were shown to be non-toxic as were species of the coastal genus \"Hymenomonas\", however several species of \"Pleurochrysis\" and \"Jomonlithus\", both coastal genera were toxic to \"Artemia\".\nCommunity interactions.\nCoccolithophorids are predominantly found as single, free-floating haploid or diploid cells.\nCompetition.\nMost phytoplankton need sunlight and nutrients from the ocean to survive, so they thrive in areas with large inputs of nutrient rich water upwelling from the lower levels of the ocean. Most coccolithophores require sunlight only for energy production, and have a higher ratio of nitrate uptake over ammonium uptake (nitrogen is required for growth and can be used directly from nitrate but not ammonium). Because of this they thrive in still, nutrient-poor environments where other phytoplankton are starving. Trade-offs associated with these faster growth rates include a smaller cell radius and lower cell volume than other types of phytoplankton.\nViral infection and coevolution.\nGiant DNA-containing viruses are known to lytically infect coccolithophores, particularly \"E. huxleyi\". These viruses, known as E. huxleyi viruses (EhVs), appear to infect the coccosphere coated diploid phase of the life cycle almost exclusively. It has been proposed that as the haploid organism is not infected and therefore not affected by the virus, the co-evolutionary \"arms race\" between coccolithophores and these viruses does not follow the classic Red Queen evolutionary framework, but instead a \"Cheshire Cat\" ecological dynamic. More recent work has suggested that viral synthesis of sphingolipids and induction of programmed cell death provides a more direct link to study a Red Queen-like coevolutionary arms race at least between the coccolithoviruses and diploid organism.\nEvolution and diversity.\nCoccolithophores are members of the clade Haptophyta, which is a sister clade to Centrohelida, which are both in Haptista. The oldest known coccolithophores are known from the Late Triassic, around the Norian-Rhaetian boundary. Diversity steadily increased over the course of the Mesozoic, reaching its apex during the Late Cretaceous. However, there was a sharp drop during the Cretaceous-Paleogene extinction event, when more than 90% of coccolithophore species became extinct. Coccoliths reached another, lower apex of diversity during the Paleocene-Eocene thermal maximum, but have subsequently declined since the Oligocene due to decreasing global temperatures, with species that produced large and heavily calcified coccoliths most heavily affected.\nCoccolithophore shells.\nEach coccolithophore encloses itself in a protective shell of coccoliths, calcified scales which make up its exoskeleton or coccosphere. The coccoliths are created inside the coccolithophore cell and while some species maintain a single layer throughout life only producing new coccoliths as the cell grows, others continually produce and shed coccoliths.\nComposition.\nThe primary constituent of coccoliths is calcium carbonate, or chalk. Calcium carbonate is transparent, so the organisms' photosynthetic activity is not compromised by encapsulation in a coccosphere.\nFormation.\nCoccoliths are produced by a biomineralization process known as coccolithogenesis. Generally, calcification of coccoliths occurs in the presence of light, and these scales are produced much more during the exponential phase of growth than the stationary phase. Although not yet entirely understood, the biomineralization process is tightly regulated by calcium signaling. Calcite formation begins in the golgi complex where protein templates nucleate the formation of CaCO3 crystals and complex acidic polysaccharides control the shape and growth of these crystals. As each scale is produced, it is exported in a Golgi-derived vesicle and added to the inner surface of the coccosphere. This means that the most recently produced coccoliths may lie beneath older coccoliths.\nDepending upon the phytoplankton's stage in the life cycle, two different types of coccoliths may be formed. Holococcoliths are produced only in the haploid phase, lack radial symmetry, and are composed of anywhere from hundreds to thousands of similar minute (ca 0.1\u00a0\u03bcm) rhombic calcite crystals. These crystals are thought to form at least partially outside the cell. Heterococcoliths occur only in the diploid phase, have radial symmetry, and are composed of relatively few complex crystal units (fewer than 100). Although they are rare, combination coccospheres, which contain both holococcoliths and heterococcoliths, have been observed in the plankton recording coccolithophore life cycle transitions. Finally, the coccospheres of some species are highly modified with various appendages made of specialized coccoliths.\nFunction.\nWhile the exact function of the coccosphere is unclear, many potential functions have been proposed. Most obviously coccoliths may protect the phytoplankton from predators. It also appears that it helps them to create a more stable pH. During photosynthesis carbon dioxide is removed from the water, making it more basic. Also calcification removes carbon dioxide, but chemistry behind it leads to the opposite pH reaction; it makes the water more acidic. The combination of photosynthesis and calcification therefore even out each other regarding pH changes. In addition, these exoskeletons may confer an advantage in energy production, as coccolithogenesis seems highly coupled with photosynthesis. Organic precipitation of calcium carbonate from bicarbonate solution produces free carbon dioxide directly within the cellular body of the alga, this additional source of gas is then available to the Coccolithophore for photosynthesis. It has been suggested that they may provide a cell-wall like barrier to isolate intracellular chemistry from the marine environment. More specific, defensive properties of coccoliths may include protection from osmotic changes, chemical or mechanical shock, and short-wavelength light. It has also been proposed that the added weight of multiple layers of coccoliths allows the organism to sink to lower, more nutrient rich layers of the water and conversely, that coccoliths add buoyancy, stopping the cell from sinking to dangerous depths. Coccolith appendages have also been proposed to serve several functions, such as inhibiting grazing by zooplankton.\nUses.\nCoccoliths are the main component of the Chalk, a Late Cretaceous rock formation which outcrops widely in southern England and forms the White Cliffs of Dover, and of other similar rocks in many other parts of the world. At the present day sedimented coccoliths are a major component of the calcareous oozes that cover up to 35% of the ocean floor and is kilometres thick in places. Because of their abundance and wide geographic ranges, the coccoliths which make up the layers of this ooze and the chalky sediment formed as it is compacted serve as valuable microfossils.\nCalcification, the biological production of calcium carbonate (CaCO3), is a key process in the marine carbon cycle. Coccolithophores are the major planktonic group responsible for pelagic CaCO3 production. The diagram on the right shows the energetic costs of coccolithophore calcification:\n (A) Transport processes include the transport into the cell from the surrounding seawater of primary calcification substrates Ca2+ and HCO3\u2212 (black arrows) and the removal of the end product H+ from the cell (gray arrow). The transport of Ca2+ through the cytoplasm to the CV is the dominant cost associated with calcification.\n (B) Metabolic processes include the synthesis of CAPs (gray rectangles) by the Golgi complex (white rectangles) that regulate the nucleation and geometry of CaCO3 crystals. The completed coccolith (gray plate) is a complex structure of intricately arranged CAPs and CaCO3 crystals.\n (C) Mechanical and structural processes account for the secretion of the completed coccoliths that are transported from their original position adjacent to the nucleus to the cell periphery, where they are transferred to the surface of the cell. The costs associated with these processes are likely to be comparable to organic-scale exocytosis in noncalcifying haptophyte algae.\nThe diagram on the left shows the benefits of coccolithophore calcification. (A) Accelerated photosynthesis includes CCM (1) and enhanced light uptake via scattering of scarce photons for deep-dwelling species (2). (B) Protection from photodamage includes sunshade protection from ultraviolet (UV) light and photosynthetic active radiation (PAR) (1) and energy dissipation under high-light conditions (2). (C) Armor protection includes protection against viral/bacterial infections (1) and grazing by selective (2) and nonselective (3) grazers.\nThe degree by which calcification can adapt to ocean acidification is presently unknown. Cell physiological examinations found the essential H+ efflux (stemming from the use of HCO3\u2212 for intra-cellular calcification) to become more costly with ongoing ocean acidification as the electrochemical H+ inside-out gradient is reduced and passive proton outflow impeded. Adapted cells would have to activate proton channels more frequently, adjust their membrane potential, and/or lower their internal pH. Reduced intra-cellular pH would severely affect the entire cellular machinery and require other processes (e.g. photosynthesis) to co-adapt in order to keep H+ efflux alive. The obligatory H+ efflux associated with calcification may therefore pose a fundamental constraint on adaptation which may potentially explain why \"calcification crisis\" were possible during long-lasting (thousands of years) CO2 perturbation events even though evolutionary adaption to changing carbonate chemistry conditions is possible within one year. Unraveling these fundamental constraints and the limits of adaptation should be a focus in future coccolithophore studies because knowing them is the key information required to understand to what extent the calcification response to carbonate chemistry perturbations can be compensated by evolution.\nSilicate- or cellulose-armored functional groups such as diatoms and dinoflagellates do not need to sustain the calcification-related H+ efflux. Thus, they probably do not need to adapt in order to keep costs for the production of structural elements low. On the contrary, dinoflagellates (except for calcifying species; with generally inefficient CO2-fixing RuBisCO enzymes may even profit from chemical changes since photosynthetic carbon fixation as their source of structural elements in the form of cellulose should be facilitated by the ocean acidification-associated CO2 fertilization. Under the assumption that any form of shell/exoskeleton protects phytoplankton against predation non-calcareous armors may be the preferable solution to realize protection in a future ocean.\nThe diagram on the right is a representation of how the comparative energetic effort for armor construction in diatoms, dinoflagellates and coccolithophores appear to operate. The frustule (diatom shell) seems to be the most inexpensive armor under all circumstances because diatoms typically outcompete all other groups when silicate is available. The coccosphere is relatively inexpensive under sufficient [CO2], high [HCO3\u2212], and low [H+] because the substrate is saturating and protons are easily released into seawater. In contrast, the construction of thecal elements, which are organic (cellulose) plates that constitute the dinoflagellate shell, should rather be favored at high H+ concentrations because these usually coincide with high [CO2]. Under these conditions dinoflagellates could down-regulate the energy-consuming operation of carbon concentrating mechanisms to fuel the production of organic source material for their shell. Therefore, a shift in carbonate chemistry conditions toward high [CO2] may promote their competitiveness relative to coccolithophores. However, such a hypothetical gain in competitiveness due to altered carbonate chemistry conditions would not automatically lead to dinoflagellate dominance because a huge number of factors other than carbonate chemistry have an influence on species composition as well.\nDefence against predation.\nCurrently, the evidence supporting or refuting a protective function of the coccosphere against predation is limited. Some researchers found that overall microzooplankton predation rates were reduced during blooms of the coccolithophore \"Emiliania huxleyi\", while others found high microzooplankton grazing rates on natural coccolithophore communities. In 2020, researchers found that \"in situ\" ingestion rates of microzooplankton on \"E. huxleyi\" did not differ significantly from those on similar sized non-calcifying phytoplankton. In laboratory experiments the heterotrophic dinoflagellate \"Oxyrrhis marina\" preferred calcified over non-calcified cells of \"E. huxleyi\", which was hypothesised to be due to size selective feeding behaviour, since calcified cells are larger than non-calcified \"E. huxleyi\". In 2015, Harvey et al. investigated predation by the dinoflagellate \"O. marina\" on different genotypes of non-calcifying \"E. huxleyi\" as well as calcified strains that differed in the degree of calcification. They found that the ingestion rate of \"O. marina\" was dependent on the genotype of \"E. huxleyi\" that was offered, rather than on their degree of calcification. In the same study, however, the authors found that predators which preyed on non-calcifying genotypes grew faster than those fed with calcified cells. In 2018, Strom et al. compared predation rates of the dinoflagellate \"Amphidinium longum\" on calcified relative to naked \"E. huxleyi\" prey and found no evidence that the coccosphere prevents ingestion by the grazer. Instead, ingestion rates were dependent on the offered genotype of E. huxleyi. Altogether, these two studies suggest that the genotype has a strong influence on ingestion by the microzooplankton species, but if and how calcification protects coccolithophores from microzooplankton predation could not be fully clarified.\nImportance in global climate change.\nImpact on the carbon cycle.\nCoccolithophores have both long and short term effects on the carbon cycle. The production of coccoliths requires the uptake of dissolved inorganic carbon and calcium. Calcium carbonate and carbon dioxide are produced from calcium and bicarbonate by the following chemical reaction:\nBecause coccolithophores are photosynthetic organisms, they are able to use some of the CO2 released in the calcification reaction for photosynthesis.\nHowever, the production of calcium carbonate drives surface alkalinity down, and in conditions of low alkalinity the CO2 is instead released back into the atmosphere.\nAs a result of this, researchers have postulated that large blooms of coccolithophores may contribute to global warming in the short term. A more widely accepted idea, however, is that over the long term coccolithophores contribute to an overall decrease in atmospheric CO2 concentrations. During calcification two carbon atoms are taken up and one of them becomes trapped as calcium carbonate. This calcium carbonate sinks to the bottom of the ocean in the form of coccoliths and becomes part of sediment; thus, coccolithophores provide a sink for emitted carbon, mediating the effects of greenhouse gas emissions.\nEvolutionary responses to ocean acidification.\nResearch also suggests that ocean acidification due to increasing concentrations of CO2 in the atmosphere may affect the calcification machinery of coccolithophores. This may not only affect immediate events such as increases in population or coccolith production, but also may induce evolutionary adaptation of coccolithophore species over longer periods of time. For example, coccolithophores use H+ ion channels in to constantly pump H+ ions out of the cell during coccolith production. This allows them to avoid acidosis, as coccolith production would otherwise produce a toxic excess of H+ ions. When the function of these ion channels is disrupted, the coccolithophores stop the calcification process to avoid acidosis, thus forming a feedback loop. Low ocean alkalinity, impairs ion channel function and therefore places evolutionary selective pressure on coccolithophores and makes them (and other ocean calcifiers) vulnerable to ocean acidification. In 2008, field evidence indicating an increase in calcification of newly formed ocean sediments containing coccolithophores bolstered the first ever experimental data showing that an increase in ocean CO2 concentration results in an increase in calcification of these organisms.\nDecreasing coccolith mass is related to both the increasing concentrations of CO2 and decreasing concentrations of in the world's oceans. This lower calcification is assumed to put coccolithophores at ecological disadvantage. Some species like \"Calcidiscus\" \"leptoporus\", however, are not affected in this way, while the most abundant coccolithophore species, \"E. huxleyi\" might be (study results are mixed). Also, highly calcified coccolithophorids have been found in conditions of low CaCO3 saturation contrary to predictions. Understanding the effects of increasing ocean acidification on coccolithophore species is absolutely essential to predicting the future chemical composition of the ocean, particularly its carbonate chemistry. Viable conservation and management measures will come from future research in this area. Groups like the European-based CALMARO are monitoring the responses of coccolithophore populations to varying pH's and working to determine environmentally sound measures of control.\nImpact on microfossil record.\nCoccolith fossils are prominent and valuable calcareous microfossils. They are the largest global source of biogenic calcium carbonate, and significantly contribute to the global carbon cycle. They are the main constituent of chalk deposits such as the white cliffs of Dover.\nOf particular interest are fossils dating back to the Palaeocene-Eocene Thermal Maximum 55 million years ago. This period is thought to correspond most directly to the current levels of CO2 in the ocean. Finally, field evidence of coccolithophore fossils in rock were used to show that the deep-sea fossil record bears a rock record bias similar to the one that is widely accepted to affect the land-based fossil record.\nImpact on the oceans.\nThe coccolithophorids help in regulating the temperature of the oceans. They thrive in warm seas and release dimethyl sulfide (DMS) into the air whose nuclei help to produce thicker clouds to block the sun. When the oceans cool, the number of coccolithophorids decrease and the amount of clouds also decrease. When there are fewer clouds blocking the sun, the temperature also rises. This, therefore, maintains the balance and equilibrium of nature.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\nSources of detailed information\nIntroductions to coccolithophores"}
{"id": "47521", "revid": "37874818", "url": "https://en.wikipedia.org/wiki?curid=47521", "title": "Condensation", "text": "Change of state of matter from a gas phase into a liquid phase\nCondensation is the change of the state of matter from the gas phase into the liquid phase, and is the reverse of vaporization. The word most often refers to the water cycle. It can also be defined as the change in the state of water vapor to liquid water when in contact with a liquid or solid surface or cloud condensation nuclei within the atmosphere. When the transition happens from the gaseous phase into the solid phase directly, the change is called deposition. Condensation is usually associated with water.\nInitiation.\nCondensation is initiated by the formation of atomic/molecular clusters of that species within its gaseous volume\u2014like rain drop or snow flake formation within clouds\u2014or at the contact between such gaseous phase and a liquid or solid surface. In clouds, this can be catalyzed by water-nucleating proteins, produced by atmospheric microbes, which are capable of binding gaseous or liquid water molecules.\nReversibility scenarios.\nA few distinct reversibility scenarios emerge here with respect to the nature of the surface.\nMost common scenarios.\nCondensation commonly occurs when a vapor is cooled and/or compressed to its saturation limit when the molecular density in the gas phase reaches its maximal threshold. Vapor cooling and compressing equipment that collects condensed liquids is called a \"condenser\". \nMeasurement.\nPsychrometry measures the rates of condensation through evaporation into the air moisture at various atmospheric pressures and temperatures. Water is the product of its vapor condensation\u2014condensation is the process of such phase conversion. \nApplications of condensation.\nCondensation is a crucial component of distillation, an important laboratory and industrial chemistry application.\nBecause condensation is a naturally occurring phenomenon, it can often be used to generate water in large quantities for human use. Many structures are made solely for the purpose of collecting water from condensation, such as air wells and fog fences. Such systems can often be used to retain soil moisture in areas where active desertification is occurring\u2014so much so that some organizations educate people living in affected areas about water condensers to help them deal effectively with the situation.\nIt is also a crucial process in forming particle tracks in a cloud chamber. In this case, ions produced by an incident particle act as nucleation centers for the condensation of the vapor producing the visible \"cloud\" trails. \nCommercial applications of condensation, by consumers as well as industry, include power generation, water desalination, thermal management, refrigeration, and air conditioning.\nBiological adaptation.\nNumerous living beings use water made accessible by condensation. A few examples of these are the Australian thorny devil, the darkling beetles of the Namibian coast, and the coast redwoods of the West Coast of the United States. \nCondensation in building construction.\nCondensation in building construction is an unwanted phenomenon as it may cause dampness, mold health issues, wood rot, corrosion, weakening of mortar and masonry walls, and energy penalties due to increased heat transfer. To alleviate these issues, the indoor air humidity needs to be lowered, or air ventilation in the building needs to be improved. This can be done in a number of ways, for example opening windows, turning on extractor fans, using dehumidifiers, drying clothes outside and covering pots and pans whilst cooking. Air conditioning or ventilation systems can be installed that help remove moisture from the air, and move air throughout a building. The amount of water vapor that can be stored in the air can be increased simply by increasing the temperature. However, this can be a double edged sword as most condensation in the home occurs when warm, moisture heavy air comes into contact with a cool surface. As the air is cooled, it can no longer hold as much water vapor. This leads to deposition of water on the cool surface. This is very apparent when central heating is used in combination with single glazed windows in winter. \nInterstructure condensation may be caused by thermal bridges, insufficient or lacking insulation, damp proofing or insulated glazing.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47522", "revid": "22041646", "url": "https://en.wikipedia.org/wiki?curid=47522", "title": "Conduction", "text": ""}
{"id": "47523", "revid": "33136685", "url": "https://en.wikipedia.org/wiki?curid=47523", "title": "Pinophyta", "text": ""}
{"id": "47525", "revid": "46821651", "url": "https://en.wikipedia.org/wiki?curid=47525", "title": "Contrail", "text": "Long, thin artificial clouds that sometimes form behind aircraft\nContrails (; short for \"condensation trails\") or vapour trails are line-shaped clouds produced by aircraft engine exhaust or changes in air pressure, typically at aircraft cruising altitudes several kilometres/miles above the Earth's surface. They are composed primarily of water, in the form of ice crystals. The combination of water vapor in aircraft engine exhaust and the low ambient temperatures at high altitudes cause the trails' formation. \nImpurities in the engine exhaust from the fuel, including soot and sulfur compounds (0.05% by weight in jet fuel) provide some of the particles that serve as cloud condensation nuclei for water droplet growth in the exhaust. If water droplets form, they can freeze to form ice particles that compose a contrail. Their formation can also be triggered by changes in air pressure in wingtip vortices, or in the air over the entire wing surface. Contrails, and other clouds caused directly by human activity, are called homogenitus.\nThe vapor trails produced by rockets are referred to as \"missile contrails\" or \"rocket contrails.\" The water vapor and aerosol produced by rockets promote the \"formation of ice clouds in ice supersaturated layers of the atmosphere.\" Missile contrail clouds mainly comprise \"metal oxide particles, high-temperature water vapor condensation particles, and other byproducts of engine combustion.\"\nDepending on the temperature and humidity at the altitude where the contrails form, they may be visible for only a few seconds or minutes, or may persist for hours and spread to be several kilometres/miles wide, eventually resembling natural cirrus or altocumulus clouds. Persistent contrails are of particular interest to scientists because they increase the cloudiness of the atmosphere. The resulting cloud forms are formally described as homomutatus, and may resemble cirrus, cirrocumulus, or cirrostratus, and are sometimes called cirrus aviaticus. Some persistent spreading contrails contribute to climate change.\nCondensation trails as a result of engine exhaust.\nEngine exhaust is predominantly made up of water and carbon dioxide, the combustion products of hydrocarbon fuels. Many other chemical byproducts of incomplete hydrocarbon fuel combustion, including volatile organic compounds, inorganic gases, polycyclic aromatic hydrocarbons, oxygenated organics, alcohols, ozone and particles of soot have been observed at lower concentrations. The exact quality is a function of engine type and basic combustion engine function, with up to 30% of aircraft exhaust being unburned fuel. (Micron-sized metallic particles resulting from engine wear have also been detected.) At high altitudes as this water vapor emerges into a cold environment, the localized increase in water vapor can raise the relative humidity of the air past saturation point. The vapor then condenses into tiny water droplets which freeze if the temperature is low enough. These millions of tiny water droplets and/or ice crystals form the contrails. The time taken for the vapor to cool enough to condense accounts for the contrail forming some distance behind the aircraft. At high altitudes, supercooled water vapor requires a trigger to encourage deposition or condensation. The exhaust particles in the aircraft's exhaust act as this trigger, causing the trapped vapor to condense rapidly. Exhaust contrails usually form at high altitudes; usually above , where the air temperature is below . They can also form closer to the ground when the air is cold and moist.\nA 2013\u20132014 study jointly supported by NASA, the German aerospace center DLR, and Canada's National Research Council NRC, determined that biofuels could reduce contrail generation. This reduction was explained by demonstrating that biofuels produce fewer soot particles, which are the nuclei around which the ice crystals form. The tests were performed by flying a DC-8 at cruising altitude with a sample-gathering aircraft flying in trail. In these samples, the contrail-producing soot particle count was reduced by 50 to 70 percent, using a 50% blend of conventional Jet A1 fuel and HEFA (hydroprocessed esters and fatty acids) biofuel produced from camelina.\nCondensation from decreases in pressure.\nAs a wing generates lift, it causes a vortex to form at the wingtip, and at the tip of the flap when deployed (wingtips and flap boundaries represent discontinuities in airflow). These wingtip vortices persist in the atmosphere long after the aircraft has passed. The reduction in pressure and temperature across each vortex can cause water to condense and make the cores of the wingtip vortices visible; this effect is more common on humid days. Wingtip vortices can sometimes be seen behind the wing flaps of airliners during takeoff and landing, and during Space Shuttle landings.\nThe visible cores of wingtip vortices contrast with the other major type of contrails which are caused by the combustion of fuel. Contrails produced from jet engine exhaust are seen at high altitude, directly behind each engine. By contrast, the visible cores of wingtip vortices are usually seen only at low altitude where the aircraft is travelling slowly after takeoff or before landing, and where the ambient humidity is higher; they trail behind the wingtips and wing flaps rather than behind the engines.\nAt high-thrust settings the fan blades at the intake of a turbofan engine reach transonic speeds, causing a sudden drop in air pressure. This creates the condensation fog (inside the intake) which is often observed by air travelers during takeoff.\nThe tips of rotating surfaces (such as propellers and rotors) sometimes produce visible contrails.\nIn firearms, a vapor trail is sometimes observed when firing under rare conditions, due to condensation induced by changes in air pressure around the bullet. A vapor trail from a bullet is observable from any direction. Vapor trail should not be confused with bullet trace, a refractive effect due to changes in air pressure as the bullet travels, which is a much more common phenomenon (and is usually only observable directly from behind the shooter).\nImpacts on climate.\nIt is considered that the largest contribution of aviation to climate change comes from contrails. \nIn general, aircraft contrails trap outgoing longwave radiation emitted by the Earth and atmosphere more than they reflect incoming solar radiation, resulting in a net increase in radiative forcing. In 1992, this warming effect was estimated between 3.5\u00a0mW/m2 and 17\u00a0mW/m2.\nIn 2009, its 2005 value was estimated at 12\u00a0mW/m2, based on the reanalysis data, climate models, and radiative transfer codes; with an uncertainty range of 5 to 26\u00a0mW/m2, and with a low level of scientific understanding. \nContrail cirrus may be air traffic's largest radiative forcing component, larger than all CO2 accumulated from aviation, and could triple from a 2006 baseline to 160\u2013180\u00a0mW/m2 by 2050 without intervention. For comparison, the total radiative forcing from human activities amounted to 2.72 W/m2 (with a range between 1.96 and 3.48W/m2) in 2019, and the increase from 2011 to 2019 alone amounted to 0.34W/m2. Contrail effects differ a lot depending on when they are formed, as they decrease the daytime temperature and increase the nighttime temperature, reducing their difference. In 2006, it was estimated that night flights contribute 60 to 80% of contrail radiative forcing while accounting for 25% of daily air traffic, and winter flights contribute half of the annual mean radiative forcing while accounting for 22% of annual air traffic.\nStarting from the 1990s, it was suggested that contrails during daytime have a strong cooling effect, and when combined with the warming from night-time flights, this would lead to a substantial reduction in diurnal temperature variation (the difference in the day's highs and lows at a fixed station). When no commercial aircraft flew across the USA following the September 11 attacks, the diurnal temperature variation was widened by . Measured across 4,000 weather stations in the continental United States, this increase was the largest recorded in 30 years. Without contrails, the local diurnal temperature range was higher than immediately before. In the southern US, the difference was diminished by about , and by in the US midwest. However, follow-up studies found that a natural change in cloud cover can more than explain these findings. The authors of a 2008 study wrote, \"The variations in high cloud cover, including contrails and contrail-induced cirrus clouds, contribute weakly to the changes in the diurnal temperature range, which is governed primarily by lower altitude clouds, winds, and humidity.\" \nIn 2011, a study of British meteorological records taken during World War II identified one event where the temperature was higher than the day's average near airbases used by USAAF strategic bombers after they flew in a formation. However, its authors cautioned that this was a single event, making it difficult to draw firm conclusions from it. Then, the global response to the 2020 coronavirus pandemic led to a reduction in global air traffic of nearly 70% relative to 2019. Thus, it provided an extended opportunity to study the impact of contrails on regional and global temperature. Multiple studies found \"no significant response of diurnal surface air temperature range\" as the result of contrail changes, and either \"no net significant global ERF\" (effective radiative forcing) or a very small warming effect. \nAn EU project launched in 2020 aims to assess the feasibility of minimising contrail effects by the operational choices in making flight plans. Other similar projects include ContrailNet from Eurocontrol, Reviate, and the Ciconia project, as well as Google's 'project contrails'.\nHead-on contrails.\nA contrail from an airplane flying towards the observer can appear to be generated by an object moving vertically. On 8 November 2010 in the US state of California, a contrail of this type gained media attention as a \"mystery missile\" that could not be explained by U.S. military and aviation authorities, and its explanation as a contrail took more than 24 hours to become accepted by U.S. media and military institutions.\nDistrails.\nWhere an aircraft passes through a cloud, it can disperse the cloud in its path. This is known as a distrail (short for \"dissipation trail\"). The plane's warm engine exhaust and enhanced vertical mixing in the aircraft's wake can cause existing cloud droplets to evaporate. If the cloud is sufficiently thin, such processes can yield a cloud-free corridor in an otherwise solid cloud layer. An early satellite observation of distrails that most likely were elongated, aircraft-induced fallstreak holes appeared in Corfidi and Brandli (1986).\nClouds form when invisible water vapor condenses into microscopic water droplets or into microscopic ice crystals. This may happen when air with a high proportion of gaseous water cools. A distrail forms when the heat of engine exhaust evaporates the liquid water droplets in a cloud, turning them back into invisible, gaseous water vapor. Distrails also may arise as a result of enhanced mixing (entrainment) of drier air immediately above or below a thin cloud layer following passage of an aircraft through the cloud, as shown in the second image below:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47526", "revid": "13220696", "url": "https://en.wikipedia.org/wiki?curid=47526", "title": "Convection", "text": "Fluid flow that occurs due to heterogeneous fluid properties and body forces\nConvection is single or multiphase fluid flow that occurs spontaneously through the combined effects of material property heterogeneity and body forces on a fluid, most commonly density and gravity (see buoyancy). When the cause of the convection is unspecified, convection due to the effects of thermal expansion and buoyancy can be assumed. Convection may also take place in soft solids or mixtures where particles can flow.\nConvective flow may be transient (such as when a multiphase mixture of oil and water separates) or steady state (see convection cell). The convection may be due to gravitational, electromagnetic or fictitious body forces. Heat transfer by natural convection plays a role in the structure of Earth's atmosphere, its oceans, and its mantle. Discrete convective cells in the atmosphere can be identified by clouds, with stronger convection resulting in thunderstorms. Natural convection also plays a role in stellar physics. Convection is often categorised or described by the main effect causing the convective flow; for example, thermal convection.\nConvection cannot take place in most solids because neither bulk current flows nor significant diffusion of matter can take place. \nGranular convection is a similar phenomenon in granular material instead of fluids.\nAdvection is the transport of any substance or quantity (such as heat) through fluid motion.\nConvection is a process involving bulk movement of a fluid that usually leads to a net transfer of heat through advection. Convective heat transfer is the intentional use of convection as a method for heat transfer. \nHistory.\nIn the 1830s, in \"The Bridgewater Treatises\", the term \"convection\" is attested in a scientific sense. In treatise VIII by William Prout, in the book on chemistry, it says:\n[...] This motion of heat takes place in three ways, which a common fire-place very well illustrates. If, for instance, we place a thermometer directly before a fire, it soon begins to rise, indicating an increase of temperature. In this case the heat has made its way through the space between the fire and the thermometer, by the process termed \"radiation\". If we place a second thermometer in contact with any part of the grate, and away from the direct influence of the fire, we shall find that this thermometer also denotes an increase of temperature; but here the heat must have travelled through the metal of the grate, by what is termed \"conduction\". Lastly, a third thermometer placed in the chimney, away from the direct influence of the fire, will also indicate a considerable increase of temperature; in this case a portion of the air, passing through and near the fire, has become heated, and has \"carried\" up the chimney the temperature acquired from the fire. There is at present no single term in our language employed to denote this third mode of the propagation of heat; but we venture to propose for that purpose, the term \"convection\", [in footnote: [Latin] \"Convectio\", a carrying or conveying] which not only expresses the leading fact, but also accords very well with the two other terms.\nLater, in the same treatise VIII, in the book on meteorology, the concept of convection is also applied to \"the process by which heat is communicated through water\".\nTerminology.\nToday, the word \"convection\" has different but related usages in different scientific or engineering contexts or applications.\nIn fluid mechanics, \"convection\" has a broader sense: it refers to the motion of fluid driven by density (or other property) difference.\nIn thermodynamics, \"convection\" often refers to heat transfer by convection, where the prefixed variant Natural Convection is used to distinguish the fluid mechanics concept of Convection (covered in this article) from convective heat transfer.\nSome phenomena which result in an effect superficially similar to that of a convective cell may also be (inaccurately) referred to as a form of convection; for example, thermo-capillary convection and granular convection.\nMechanisms.\nConvection may happen in fluids at all scales larger than a few atoms. There are a variety of circumstances in which the forces required for convection arise, leading to different types of convection, described below. In broad terms, convection arises because of body forces acting within the fluid, such as gravity.\nNatural convection.\nNatural convection is a flow whose motion is caused by some parts of a fluid being heavier than other parts. In most cases this leads to natural circulation: the ability of a fluid in a system to circulate continuously under gravity, with transfer of heat energy.\nThe driving force for natural convection is gravity. In a column of fluid, pressure increases with depth from the weight of the overlying fluid. The pressure at the bottom of a submerged object then exceeds that at the top, resulting in a net upward buoyancy force equal to the weight of the displaced fluid. Objects of higher density than that of the displaced fluid then sink. For example, regions of warmer low-density air rise, while those of colder high-density air sink. This creates a circulating flow: convection.\nGravity drives natural convection. Without gravity, convection does not occur, so there is no convection in free-fall (inertial) environments, such as that of the orbiting International Space Station. Natural convection can occur when there are hot and cold regions of either air or water, because both water and air become less dense as they are heated. But, for example, in the world's oceans it also occurs due to salt water being heavier than fresh water, so a layer of salt water on top of a layer of fresher water will also cause convection.\nNatural convection has attracted a great deal of attention from researchers because of its presence both in nature and engineering applications. In nature, convection cells formed from air raising above sunlight-warmed land or water are a major feature of all weather systems. Convection is also seen in the rising plume of hot air from fire, plate tectonics, oceanic currents (thermohaline circulation) and sea-wind formation (where upward convection is also modified by Coriolis forces). In engineering applications, convection is commonly visualized in the formation of microstructures during the cooling of molten metals, and fluid flows around shrouded heat-dissipation fins, and solar ponds. A very common industrial application of natural convection is free air cooling without the aid of fans: this can happen on small scales (computer chips) to large scale process equipment.\nNatural convection will be more likely and more rapid with a greater variation in density between the two fluids, a larger acceleration due to gravity that drives the convection or a larger distance through the convecting medium. Natural convection will be less likely and less rapid with more rapid diffusion (thereby diffusing away the thermal gradient that is causing the convection) or a more viscous (sticky) fluid.\nThe onset of natural convection can be determined by the Rayleigh number (Ra).\nDifferences in buoyancy within a fluid can arise for reasons other than temperature variations, in which case the fluid motion is called gravitational convection (see below). However, all types of buoyant convection, including natural convection, do not occur in microgravity environments. All require the presence of an environment which experiences g-force (proper acceleration).\nThe difference of density in the fluid is the key driving mechanism. If the differences of density are caused by heat, this force is called as \"thermal head\" or \"thermal driving head.\" A fluid system designed for natural circulation will have a heat source and a heat sink. Each of these is in contact with some of the fluid in the system, but not all of it. The heat source is positioned lower than the heat sink.\nMost fluids expand when heated, becoming less dense, and contract when cooled, becoming denser. At the heat source of a system of natural circulation, the heated fluid becomes lighter than the fluid surrounding it, and thus rises. At the heat sink, the nearby fluid becomes denser as it cools, and is drawn downward by gravity. Together, these effects create a flow of fluid from the heat source to the heat sink and back again.\nGravitational or buoyant convection.\nGravitational convection is a type of natural convection induced by buoyancy variations resulting from material properties other than temperature. Typically this is caused by a variable composition of the fluid. If the varying property is a concentration gradient, it is known as solutal convection. For example, gravitational convection can be seen in the diffusion of a source of dry salt downward into wet soil due to the buoyancy of fresh water in saline.\nVariable salinity in water and variable water content in air masses are frequent causes of convection in the oceans and atmosphere which do not involve heat, or else involve additional compositional density factors other than the density changes from thermal expansion (see \"thermohaline circulation\"). Similarly, variable composition within the Earth's interior which has not yet achieved maximal stability and minimal energy (in other words, with densest parts deepest) continues to cause a fraction of the convection of fluid rock and molten metal within the Earth's interior (see below).\nGravitational convection, like natural thermal convection, also requires a g-force environment in order to occur.\nSolid-state convection in ice.\nIce convection on Pluto is believed to occur in a soft mixture of nitrogen ice and carbon monoxide ice. It has also been proposed for Europa, and other bodies in the outer Solar System.\nThermomagnetic convection.\nThermomagnetic convection can occur when an external magnetic field is imposed on a ferrofluid with varying magnetic susceptibility. In the presence of a temperature gradient this results in a nonuniform magnetic body force, which leads to fluid movement. A ferrofluid is a liquid which becomes strongly magnetized in the presence of a magnetic field.\nCombustion.\nIn a zero-gravity environment, there can be no buoyancy forces, and thus no convection possible, so flames in many circumstances without gravity smother in their own waste gases. Thermal expansion and chemical reactions resulting in expansion and contraction gases allows for ventilation of the flame, as waste gases are displaced by cool, fresh, oxygen-rich gas. moves in to take up the low pressure zones created when flame-exhaust water condenses.\nExamples and applications.\nSystems of natural circulation include tornadoes and other weather systems, ocean currents, and household ventilation. Some solar water heaters use natural circulation. The Gulf Stream circulates as a result of the evaporation of water. In this process, the water increases in salinity and density. In the North Atlantic Ocean, the water becomes so dense that it begins to sink down.\nConvection occurs on a large scale in atmospheres, oceans, planetary mantles, and it provides the mechanism of heat transfer for a large fraction of the outermost interiors of the Sun and all stars. Fluid movement during convection may be invisibly slow, or it may be obvious and rapid, as in a hurricane. On astronomical scales, convection of gas and dust is thought to occur in the accretion disks of black holes, at speeds which may closely approach that of light.\nDemonstration experiments.\nThermal convection in liquids can be demonstrated by placing a heat source (for example, a Bunsen burner) at the side of a container with a liquid. Adding a dye to the water (such as food colouring) will enable visualisation of the flow.\nAnother common experiment to demonstrate thermal convection in liquids involves submerging open containers of hot and cold liquid coloured with dye into a large container of the same liquid without dye at an intermediate temperature (for example, a jar of hot tap water coloured red, a jar of water chilled in a fridge coloured blue, lowered into a clear tank of water at room temperature).\nA third approach is to use two identical jars, one filled with hot water dyed one colour, and cold water of another colour. One jar is then temporarily sealed (for example, with a piece of card), inverted and placed on top of the other. When the card is removed, if the jar containing the warmer liquid is placed on top no convection will occur. If the jar containing colder liquid is placed on top, a convection current will form spontaneously.\nConvection in gases can be demonstrated using a candle in a sealed space with an inlet and exhaust port. The heat from the candle will cause a strong convection current which can be demonstrated with a flow indicator, such as smoke from another candle, being released near the inlet and exhaust areas respectively.\nConvection cells.\nA convection cell, also known as a B\u00e9nard cell, is a characteristic fluid flow pattern in many convection systems. A rising body of fluid typically loses heat because it encounters a colder surface. In liquid, this occurs because it exchanges heat with colder liquid through direct exchange. In the example of the Earth's atmosphere, this occurs because it radiates heat. Because of this heat loss the fluid becomes denser than the fluid underneath it, which is still rising. Since it cannot descend through the rising fluid, it moves to one side. At some distance, its downward force overcomes the rising force beneath it, and the fluid begins to descend. As it descends, it warms again and the cycle repeats itself. Additionally, convection cells can arise due to density variations resulting from differences in the composition of electrolytes.\nAtmospheric convection.\nAtmospheric circulation.\nAtmospheric circulation is the large-scale movement of air, and is a means by which thermal energy is distributed on the surface of the Earth, together with the much slower (lagged) ocean circulation system. The large-scale structure of the atmospheric circulation varies from year to year, but the basic climatological structure remains fairly constant.\nLatitudinal circulation occurs because incident solar radiation per unit area is highest at the heat equator, and decreases as the latitude increases, reaching minima at the poles. It consists of two primary convection cells, the Hadley cell and the polar vortex, with the Hadley cell experiencing stronger convection due to the release of latent heat energy by condensation of water vapor at higher altitudes during cloud formation.\nLongitudinal circulation, on the other hand, comes about because the ocean has a higher specific heat capacity than land (and also thermal conductivity, allowing the heat to penetrate further beneath the surface ) and thereby absorbs and releases more heat, but the temperature changes less than land. This brings the sea breeze, air cooled by the water, ashore in the day, and carries the land breeze, air cooled by contact with the ground, out to sea during the night. Longitudinal circulation consists of two cells, the Walker circulation and El Ni\u00f1o / Southern Oscillation.\nWeather.\nSome more localized phenomena than global atmospheric movement are also due to convection, including wind and some of the hydrologic cycle. For example, a foehn wind is a down-slope wind which occurs on the downwind side of a mountain range. It results from the adiabatic warming of air which has dropped most of its moisture on windward slopes. Because of the different adiabatic lapse rates of moist and dry air, the air on the leeward slopes becomes warmer than at the same height on the windward slopes.\nA thermal column (or thermal) is a vertical section of rising air in the lower altitudes of the Earth's atmosphere. Thermals are created by the uneven heating of the Earth's surface from solar radiation. The Sun warms the ground, which in turn warms the air directly above it. The warmer air expands, becoming less dense than the surrounding air mass, and creating a thermal low. The mass of lighter air rises, and as it does, it cools by expansion at lower air pressures. It stops rising when it has cooled to the same temperature as the surrounding air. Associated with a thermal is a downward flow surrounding the thermal column. The downward moving exterior is caused by colder air being displaced at the top of the thermal. Another convection-driven weather effect is the sea breeze.\nWarm air has a lower density than cool air, so warm air rises within cooler air, similar to hot air balloons. Clouds form as relatively warmer air carrying moisture rises within cooler air. As the moist air rises, it cools, causing some of the water vapor in the rising packet of air to condense. When the moisture condenses, it releases energy known as latent heat of condensation which allows the rising packet of air to cool less than its surrounding air, continuing the cloud's ascension. If enough instability is present in the atmosphere, this process will continue long enough for cumulonimbus clouds to form, which support lightning and thunder. Generally, thunderstorms require three conditions to form: moisture, an unstable airmass, and a lifting force (heat).\nAll thunderstorms, regardless of type, go through three stages: the developing stage, the mature stage, and the dissipation stage. The average thunderstorm has a diameter. Depending on the conditions present in the atmosphere, these three stages take an average of 30 minutes to go through.\nOceanic circulation.\nSolar radiation affects the oceans: warm water from the Equator tends to circulate toward the poles, while cold polar water heads towards the Equator. The surface currents are initially dictated by surface wind conditions. The trade winds blow westward in the tropics, and the westerlies blow eastward at mid-latitudes. This wind pattern applies a stress to the subtropical ocean surface with negative curl across the Northern Hemisphere, and the reverse across the Southern Hemisphere. The resulting Sverdrup transport is equatorward. Because of conservation of potential vorticity caused by the poleward-moving winds on the subtropical ridge's western periphery and the increased relative vorticity of poleward moving water, transport is balanced by a narrow, accelerating poleward current, which flows along the western boundary of the ocean basin, outweighing the effects of friction with the cold western boundary current which originates from high latitudes. The overall process, known as western intensification, causes currents on the western boundary of an ocean basin to be stronger than those on the eastern boundary.\nAs it travels poleward, warm water transported by strong warm water current undergoes evaporative cooling. The cooling is wind driven: wind moving over water cools the water and also causes evaporation, leaving a saltier brine. In this process, the water becomes saltier and denser and decreases in temperature. Once sea ice forms, salts are left out of the ice, a process known as brine exclusion. These two processes produce water that is denser and colder. The water across the northern Atlantic Ocean becomes so dense that it begins to sink down through less salty and less dense water. (This open ocean convection is not unlike that of a lava lamp.) This downdraft of heavy, cold and dense water becomes a part of the North Atlantic Deep Water, a south-going stream.\nMantle convection.\nMantle convection is the slow creeping motion of Earth's rocky mantle caused by convection currents carrying heat from the interior of the Earth to the surface. It is one of 3 driving forces that causes tectonic plates to move around the Earth's surface.\nThe Earth's surface is divided into a number of tectonic plates that are continuously being created and consumed at their opposite plate boundaries. Creation (accretion) occurs as mantle is added to the growing edges of a plate. This hot added material cools down by conduction and convection of heat. At the consumption edges of the plate, the material has thermally contracted to become dense, and it sinks under its own weight in the process of subduction at an ocean trench. This subducted material sinks to some depth in the Earth's interior where it is prohibited from sinking further. The subducted oceanic crust triggers volcanism.\nConvection within Earth's mantle is the driving force for plate tectonics. Mantle convection is the result of a thermal gradient: the lower mantle is hotter than the upper mantle, and is therefore less dense. This sets up two primary types of instabilities. In the first type, plumes rise from the lower mantle, and corresponding unstable regions of lithosphere drip back into the mantle. In the second type, subducting oceanic plates (which largely constitute the upper thermal boundary layer of the mantle) plunge back into the mantle and move downwards towards the core-mantle boundary. Mantle convection occurs at rates of centimeters per year, and it takes on the order of hundreds of millions of years to complete a cycle of convection.\nNeutrino flux measurements from the Earth's core (see kamLAND) show the source of about two-thirds of the heat in the inner core is the radioactive decay of 40K, uranium and thorium. This has allowed plate tectonics on Earth to continue far longer than it would have if it were simply driven by heat left over from Earth's formation; or with heat produced from gravitational potential energy, as a result of physical rearrangement of denser portions of the Earth's interior toward the center of the planet (that is, a type of prolonged falling and settling).\nStack effect.\nThe Stack effect or chimney effect is the movement of air into and out of buildings, chimneys, flue gas stacks, or other containers due to buoyancy. Buoyancy occurs due to a difference in indoor-to-outdoor air density resulting from temperature and moisture differences. The greater the thermal difference and the height of the structure, the greater the buoyancy force, and thus the stack effect. The stack effect helps drive natural ventilation and infiltration. Some cooling towers operate on this principle; similarly the solar updraft tower is a proposed device to generate electricity based on the stack effect.\nStellar physics.\nThe convection zone of a star is the range of radii in which energy is transported outward from the core region primarily by convection rather than radiation. This occurs at radii which are sufficiently opaque that convection is more efficient than radiation at transporting energy.\nGranules on the photosphere of the Sun are the visible tops of convection cells in the photosphere, caused by convection of plasma in the photosphere. The rising part of the granules is located in the center where the plasma is hotter. The outer edge of the granules is darker due to the cooler descending plasma. A typical granule has a diameter on the order of 1,000 kilometers and each lasts 8 to 20 minutes before dissipating. Below the photosphere is a layer of much larger \"supergranules\" up to 30,000 kilometers in diameter, with lifespans of up to 24 hours.\nWater convection at freezing temperatures.\nWater is a fluid that does not obey the Boussinesq approximation. This is because its density varies nonlinearly with temperature, which causes its thermal expansion coefficient to be inconsistent near freezing temperatures. The density of water reaches a maximum at 4\u00a0\u00b0C and decreases as the temperature deviates. This phenomenon is investigated by experiment and numerical methods. Water is initially stagnant at 10\u00a0\u00b0C within a square cavity. It is differentially heated between the two vertical walls, where the left and right walls are held at 10\u00a0\u00b0C and 0\u00a0\u00b0C, respectively. The density anomaly manifests in its flow pattern. As the water is cooled at the right wall, the density increases, which accelerates the flow downward. As the flow develops and the water cools further, the decrease in density causes a recirculation current at the bottom right corner of the cavity.\nAnother case of this phenomenon is the event of super-cooling, where the water is cooled to below freezing temperatures but does not immediately begin to freeze. Under the same conditions as before, the flow is developed. Afterward, the temperature of the right wall is decreased to \u221210\u00a0\u00b0C. This causes the water at that wall to become supercooled, create a counter-clockwise flow, and initially overpower the warm current. This plume is caused by a delay in the nucleation of the ice. Once ice begins to form, the flow returns to a similar pattern as before and the solidification propagates gradually until the flow is redeveloped.\nNuclear reactors.\nIn a nuclear reactor, natural circulation can be a design criterion. It is achieved by reducing turbulence and friction in the fluid flow (that is, minimizing head loss), and by providing a way to remove any inoperative pumps from the fluid path. Also, the reactor (as the heat source) must be physically lower than the steam generators or turbines (the heat sink). In this way, natural circulation will ensure that the fluid will continue to flow as long as the reactor is hotter than the heat sink, even when power cannot be supplied to the pumps. Notable examples are the S5G\nand S8G United States Naval reactors, which were designed to operate at a significant fraction of full power under natural circulation, quieting those propulsion plants. The S6G reactor cannot operate at power under natural circulation, but can use it to maintain emergency cooling while shut down.\nBy the nature of natural circulation, fluids do not typically move very fast, but this is not necessarily bad, as high flow rates are not essential to safe and effective reactor operation. In modern design nuclear reactors, flow reversal is almost impossible. All nuclear reactors, even ones designed to primarily use natural circulation as the main method of fluid circulation, have pumps that can circulate the fluid in the case that natural circulation is not sufficient.\nMathematical models of convection.\nA number of dimensionless terms have been derived to describe and predict convection, including the Archimedes number, Grashof number, Richardson number, and the Rayleigh number.\nIn cases of mixed convection (natural and forced occurring together) one would often like to know how much of the convection is due to external constraints, such as the fluid velocity in the pump, and how much is due to natural convection occurring in the system.\nThe relative magnitudes of the Grashof number and the square of the Reynolds number determine which form of convection dominates. If formula_1, forced convection may be neglected, whereas if formula_2, natural convection may be neglected. If the ratio, known as the Richardson number, is approximately one, then both forced and natural convection need to be taken into account.\nOnset.\nThe onset of natural convection is determined by the Rayleigh number (Ra). This dimensionless number is given by\nformula_3\nwhere\nNatural convection will be more likely and/or more rapid with a greater variation in density between the two fluids, a larger acceleration due to gravity that drives the convection, and/or a larger distance through the convecting medium. Convection will be less likely and/or less rapid with more rapid diffusion (thereby diffusing away the gradient that is causing the convection) and/or a more viscous (sticky) fluid.\nFor thermal convection due to heating from below, as described in the boiling pot above, the equation is modified for thermal expansion and thermal diffusivity. Density variations due to thermal expansion are given by:\nformula_9\nwhere\nThe general diffusivity, formula_7, is redefined as a thermal diffusivity, formula_14.\nformula_15\nInserting these substitutions produces a Rayleigh number that can be used to predict thermal convection.\nformula_16\nTurbulence.\nThe tendency of a particular naturally convective system towards turbulence relies on the Grashof number (Gr).\nformula_17\nIn very sticky, viscous fluids (large \"\u03bd\"), fluid motion is restricted, and natural convection will be non-turbulent.\nFollowing the treatment of the previous subsection, the typical fluid velocity is of the order of formula_18, up to a numerical factor depending on the geometry of the system. Therefore, Grashof number can be thought of as Reynolds number with the velocity of natural convection replacing the velocity in Reynolds number's formula. However In practice, when referring to the Reynolds number, it is understood that one is considering forced convection, and the velocity is taken as the velocity dictated by external constraints (see below).\nBehavior.\nThe Grashof number can be formulated for natural convection occurring due to a concentration gradient, sometimes termed thermo-solutal convection. In this case, a concentration of hot fluid diffuses into a cold fluid, in much the same way that ink poured into a container of water diffuses to dye the entire space. Then:\nformula_19\nNatural convection is highly dependent on the geometry of the hot surface, various correlations exist in order to determine the heat transfer coefficient. \nA general correlation that applies for a variety of geometries is\n formula_20\nThe value of f4(Pr) is calculated using the following formula\n formula_21\nNu is the Nusselt number and the values of Nu0 and the characteristic length used to calculate Re are listed below (see also Discussion):\nWarning: The values indicated for the Horizontal cylinder are wrong; see discussion.\nNatural convection from a vertical plate.\nOne example of natural convection is heat transfer from an isothermal vertical plate immersed in a fluid, causing the fluid to move parallel to the plate. This will occur in any system wherein the density of the moving fluid varies with position. These phenomena will only be of significance when the moving fluid is minimally affected by forced convection.\nWhen considering the flow of fluid is a result of heating, the following correlations can be used, assuming the fluid is an ideal diatomic, has adjacent to a vertical plate at constant temperature and the flow of the fluid is completely laminar.\nNum = 0.478(Gr0.25)\nMean Nusselt number = Num = hmL/k\nwhere\nGrashof number = Gr = formula_22 \nwhere\nWhen the flow is turbulent different correlations involving the Rayleigh Number (a function of both the Grashof number and the Prandtl number) must be used.\nNote that the above equation differs from the usual expression for Grashof number because the value formula_11 has been replaced by its approximation formula_24, which applies for ideal gases only (a reasonable approximation for air at ambient pressure).\nPattern formation.\nConvection, especially Rayleigh\u2013B\u00e9nard convection, where the convecting fluid is contained by two rigid horizontal plates, is a convenient example of a pattern-forming system.\nWhen heat is fed into the system from one direction (usually below), at small values it merely diffuses (\"conducts\") from below upward, without causing fluid flow. As the heat flow is increased, above a critical value of the Rayleigh number, the system undergoes a bifurcation from the stable \"conducting\" state to the \"convecting\" state, where bulk motion of the fluid due to heat begins. If fluid parameters other than density do not depend significantly on temperature, the flow profile is symmetric, with the same volume of fluid rising as falling. This is known as Boussinesq convection.\nAs the temperature difference between the top and bottom of the fluid becomes higher, significant differences in fluid parameters other than density may develop in the fluid due to temperature. An example of such a parameter is viscosity, which may begin to significantly vary horizontally across layers of fluid. This breaks the symmetry of the system, and generally changes the pattern of up- and down-moving fluid from stripes to hexagons, as seen in the illustration. Such hexagons are one example of a convection cell.\nAs the Rayleigh number is increased even further above the value where convection cells first appear, the system may undergo other bifurcations, and other more complex patterns, such as spirals, may begin to appear.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47527", "revid": "49492066", "url": "https://en.wikipedia.org/wiki?curid=47527", "title": "Cryosphere", "text": "Earth's surface where water is frozen\nThe cryosphere is an umbrella term for those portions of Earth's surface where water is in solid form. This includes sea ice, ice on lakes or rivers, snow, glaciers, ice caps, ice sheets, and frozen ground (which includes permafrost). Thus, there is an overlap with the hydrosphere. The cryosphere is an integral part of the global climate system. It also has important feedbacks on the climate system. These feedbacks come from the cryosphere's influence on surface energy and moisture fluxes, clouds, the water cycle, atmospheric and oceanic circulation.\nThrough these feedback processes, the cryosphere plays a significant role in the global climate and in climate model response to global changes. Approximately 10% of the Earth's surface is covered by ice, but this is rapidly decreasing. Current reductions in the cryosphere (caused by climate change) are measurable in ice sheet melt, glaciers decline, sea ice decline, permafrost thaw and snow cover decrease.\nDefinition and terminology.\nThe cryosphere describes those portions of Earth's surface where water is in solid form. Frozen water is found on the Earth's surface primarily as snow cover, freshwater ice in lakes and rivers, sea ice, glaciers, ice sheets, and frozen ground and permafrost (permanently frozen ground). \nThe cryosphere is one of five components of the climate system. The others are the atmosphere, the hydrosphere, the lithosphere and the biosphere.\nThe term \"cryosphere\" comes from the Greek word \"kryos\", meaning \"cold\", \"frost\" or \"ice\" and the Greek word \"sphaira\", meaning \"globe\" or \"ball\".\n\"Cryospheric sciences\" is an umbrella term for the study of the cryosphere. As an interdisciplinary Earth science, many disciplines contribute to it, most notably geology, hydrology, and meteorology and climatology; in this sense, it is comparable to glaciology.\nThe term \"deglaciation\" describes the retreat of cryospheric features.\nProperties and interactions.\nThere are several fundamental physical properties of snow and ice that modulate energy exchanges between the surface and the atmosphere. The most important properties are the surface reflectance (albedo), the ability to transfer heat (thermal diffusivity), and the ability to change state (latent heat). These physical properties, together with surface roughness, emissivity, and dielectric characteristics, have important implications for observing snow and ice from space. For example, surface roughness is often the dominant factor determining the strength of radar backscatter. Physical properties such as crystal structure, density, length, and liquid water content are important factors affecting the transfers of heat and water and the scattering of microwave energy.\nResidence time and extent.\nThe residence time of water in each of the cryospheric sub-systems varies widely. Snow cover and freshwater ice are essentially seasonal, and most sea ice, except for ice in the central Arctic, lasts only a few years if it is not seasonal. A given water particle in glaciers, ice sheets, or ground ice, however, may remain frozen for 10\u2013100,000 years or longer, and deep ice in parts of East Antarctica may have an age approaching 1 million years.\nMost of the world's ice volume is in Antarctica, principally in the East Antarctic Ice Sheet. In terms of areal extent, however, Northern Hemisphere winter snow and ice extent comprise the largest area, amounting to an average 23% of hemispheric surface area in January. The large areal extent and the important climatic roles of snow and ice is related to their unique physical properties. This also indicates that the ability to observe and model snow and ice-cover extent, thickness, and physical properties (radiative and thermal properties) is of particular significance for climate research.\nSurface reflectance.\nThe surface reflectance of incoming solar radiation is important for the surface energy balance (SEB). It is the ratio of reflected to incident solar radiation, commonly referred to as albedo. Climatologists are primarily interested in albedo integrated over the shortwave portion of the electromagnetic spectrum (~300 to 3500\u00a0nm), which coincides with the main solar energy input. Typically, albedo values for non-melting snow-covered surfaces are high (~80\u201390%) except in the case of forests. \nThe higher albedos for snow and ice cause rapid shifts in surface reflectivity in autumn and spring in high latitudes, but the overall climatic significance of this increase is spatially and temporally modulated by cloud cover. (Planetary albedo is determined principally by cloud cover, and by the small amount of total solar radiation received in high latitudes during winter months.) Summer and autumn are times of high-average cloudiness over the Arctic Ocean so the albedo feedback associated with the large seasonal changes in sea-ice extent is greatly reduced. It was found that snow cover exhibited the greatest influence on Earth's radiative balance in the spring (April to May) period when incoming solar radiation was greatest over snow-covered areas.\nThermal properties of cryospheric elements.\nThe thermal properties of cryospheric elements also have important climatic consequences. Snow and ice have much lower thermal diffusivities than air. Thermal diffusivity is a measure of the speed at which temperature waves can penetrate a substance. Snow and ice are many orders of magnitude less efficient at diffusing heat than air. Snow cover insulates the ground surface, and sea ice insulates the underlying ocean, decoupling the surface-atmosphere interface with respect to both heat and moisture fluxes. The flux of moisture from a water surface is eliminated by even a thin skin of ice, whereas the flux of heat through thin ice continues to be substantial until it attains a thickness in excess of 30 to 40\u00a0cm. However, even a small amount of snow on top of the ice will dramatically reduce the heat flux and slow down the rate of ice growth. The insulating effect of snow also has major implications for the hydrological cycle. In non-permafrost regions, the insulating effect of snow is such that only near-surface ground freezes and deep-water drainage is uninterrupted.\nWhile snow and ice act to insulate the surface from large energy losses in winter, they also act to retard warming in the spring and summer because of the large amount of energy required to melt ice (the latent heat of fusion, 3.34 x 105 J/kg at 0\u00a0\u00b0C). However, the strong static stability of the atmosphere over areas of extensive snow or ice tends to confine the immediate cooling effect to a relatively shallow layer, so that associated atmospheric anomalies are usually short-lived and local to regional in scale. In some areas of the world such as Eurasia, however, the cooling associated with a heavy snowpack and moist spring soils is known to play a role in modulating the summer monsoon circulation.\nClimate change feedback mechanisms.\nThere are numerous cryosphere-climate feedbacks in the global climate system. These operate over a wide range of spatial and temporal scales from local seasonal cooling of air temperatures to hemispheric-scale variations in ice sheets over time scales of thousands of years. The feedback mechanisms involved are often complex and incompletely understood. For example, Curry \"et al.\" (1995) showed that the so-called \"simple\" sea ice-albedo feedback involved complex interactions with lead fraction, melt ponds, ice thickness, snow cover, and sea-ice extent.\nThe role of snow cover in modulating the monsoon is just one example of a short-term cryosphere-climate feedback involving the land surface and the atmosphere.\nComponents.\nGlaciers and ice sheets.\nIce sheets and glaciers are flowing ice masses that rest on solid land. They are controlled by snow accumulation, surface and basal melt, calving into surrounding oceans or lakes and internal dynamics. The latter results from gravity-driven creep flow (\"glacial flow\") within the ice body and sliding on the underlying land, which leads to thinning and horizontal spreading. Any imbalance of this dynamic equilibrium between mass gain, loss and transport due to flow results in either growing or shrinking ice bodies.Relationships between global climate and changes in ice extent are complex. The mass balance of land-based glaciers and ice sheets is determined by the accumulation of snow, mostly in winter, and warm-season ablation due primarily to net radiation and turbulent heat fluxes to melting ice and snow from warm-air advection Where ice masses terminate in the ocean, iceberg calving is the major contributor to mass loss. In this situation, the ice margin may extend out into deep water as a floating ice shelf, such as that in the Ross Sea.\nSea ice.\nSea ice covers much of the polar oceans and forms by freezing of sea water. Satellite data since the early 1970s reveal considerable seasonal, regional, and interannual variability in the sea ice covers of both hemispheres. Seasonally, sea-ice extent in the Southern Hemisphere varies by a factor of 5, from a minimum of 3\u20134 million km2 in February to a maximum of 17\u201320 million km2 in September. The seasonal variation is much less in the Northern Hemisphere where the confined nature and high latitudes of the Arctic Ocean result in a much larger perennial ice cover, and the surrounding land limits the equatorward extent of wintertime ice. Thus, the seasonal variability in Northern Hemisphere ice extent varies by only a factor of 2, from a minimum of 7\u20139 million km2 in September to a maximum of 14\u201316 million km2 in March.\nThe ice cover exhibits much greater regional-scale interannual variability than it does hemispherical. For instance, in the region of the Sea of Okhotsk and Japan, maximum ice extent decreased from 1.3 million km2 in 1983 to 0.85 million km2 in 1984, a decrease of 35%, before rebounding the following year to 1.2 million km2. The regional fluctuations in both hemispheres are such that for any several-year period of the satellite record some regions exhibit decreasing ice coverage while others exhibit increasing ice cover.\nSnow cover.\nMost of the Earth's snow-covered area is located in the Northern Hemisphere, and varies seasonally from 46.5 million km2 in January to 3.8 million km2 in August.\nSnow cover is an extremely important storage component in the water balance, especially seasonal snowpacks in mountainous areas of the world. Though limited in extent, seasonal snowpacks in the Earth's mountain ranges account for the major source of the runoff for stream flow and groundwater recharge over wide areas of the midlatitudes. For example, over 85% of the annual runoff from the Colorado River basin originates as snowmelt. Snowmelt runoff from the Earth's mountains fills the rivers and recharges the aquifers that over a billion people depend on for their water resources.\nFurthermore, over 40% of the world's protected areas are in mountains, attesting to their value both as unique ecosystems needing protection and as recreation areas for humans.\nIce on lakes and rivers.\nIce forms on rivers and lakes in response to seasonal cooling. The sizes of the ice bodies involved are too small to exert anything other than localized climatic effects. However, the freeze-up/break-up processes respond to large-scale and local weather factors, such that considerable interannual variability exists in the dates of appearance and disappearance of the ice. Long series of lake-ice observations can serve as a proxy climate record, and the monitoring of freeze-up and break-up trends may provide a convenient integrated and seasonally-specific index of climatic perturbations. Information on river-ice conditions is less useful as a climatic proxy because ice formation is strongly dependent on river-flow regime, which is affected by precipitation, snow melt, and watershed runoff as well as being subject to human interference that directly modifies channel flow, or that indirectly affects the runoff via land-use practices.\nLake freeze-up depends on the heat storage in the lake and therefore on its depth, the rate and temperature of any inflow, and water-air energy fluxes. Information on lake depth is often unavailable, although some indication of the depth of shallow lakes in the Arctic can be obtained from airborne radar imagery during late winter (Sellman \"et al.\" 1975) and spaceborne optical imagery during summer (Duguay and Lafleur 1997). The timing of breakup is modified by snow depth on the ice as well as by ice thickness and freshwater inflow.\nChanges caused by climate change.\nSnow cover decrease.\nStudies in 2021 found that Northern Hemisphere snow cover has been decreasing since 1978, along with snow depth. Paleoclimate observations show that such changes are unprecedented over the last millennia in Western North America.\nNorth American winter snow cover increased during the 20th century, largely in response to an increase in precipitation.\nBecause of its close relationship with hemispheric air temperature, snow cover is an important indicator of climate change.\nGlobal warming is expected to result in major changes to the partitioning of snow and rainfall, and to the timing of snowmelt, which will have important implications for water use and management. These changes also involve potentially important decadal and longer time-scale feedbacks to the climate system through temporal and spatial changes in soil moisture and runoff to the oceans.(Walsh 1995). Freshwater fluxes from the snow cover into the marine environment may be important, as the total flux is probably of the same magnitude as desalinated ridging and rubble areas of sea ice. In addition, there is an associated pulse of precipitated pollutants which accumulate over the Arctic winter in snowfall and are released into the ocean upon ablation of the sea ice.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47528", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=47528", "title": "Language is a virus", "text": ""}
{"id": "47530", "revid": "48970003", "url": "https://en.wikipedia.org/wiki?curid=47530", "title": "Cumulonimbus cloud", "text": "Genus of dense, towering vertical clouds\nCumulonimbus (from la\u00a0'swell'\u00a0'cloud') is a dense, towering, vertical cloud, typically forming from water vapor condensing in the lower troposphere that builds upward carried by powerful buoyant air currents. Above the lower portions of the cumulonimbus the water vapor becomes ice crystals, such as snow and graupel, the interaction of which can lead to hail and to lightning formation, respectively.\nWhen causing thunderstorms, these clouds may be called thunderheads. Cumulonimbus can form alone, in clusters, or along squall lines. These clouds are capable of producing lightning and other dangerous severe weather, such as tornadoes, hazardous winds, and large hailstones. Cumulonimbus progress from overdeveloped cumulus congestus clouds and may further develop as part of a supercell. Cumulonimbus is abbreviated as Cb.\nDescription.\nTowering cumulonimbus clouds are typically accompanied by smaller cumulus clouds. The cumulonimbus base may extend several kilometres (miles) across, or be as small as several tens of metres (yards) across, and occupy low to upper altitudes within the troposphere - formed at altitude from approximately . Normal peaks usually reach to as much as , with unusually high ones typically topping out around and extreme instances claimed to be as high as or more. Well-developed cumulonimbus clouds are characterized by a flat, anvil shaped top (anvil dome), caused by wind shear or inversion at the equilibrium level near the tropopause. The shelf of the anvil may precede the main cloud's vertical component for many kilometres (miles), and be accompanied by lightning. Occasionally, rising air parcels surpass the equilibrium level (due to momentum) and form an overshooting top culminating at the maximum parcel level. When vertically developed, this largest of all clouds usually extends through all three cloud regions. Even the smallest cumulonimbus cloud dwarfs its neighbors in comparison.\nEffects.\nCumulonimbus storm cells can produce torrential rain of a convective nature (often in the form of a rain shaft) and flash flooding, as well as straight-line winds. Most storm cells die after about 20\u00a0minutes, when the precipitation causes more downdraft than updraft, causing the energy to dissipate. If there is sufficient instability and moisture in the atmosphere, however (on a hot summer day, for example), the outflowing moisture and gusts from one storm cell can lead to new cells forming just a few kilometres (miles) from the former one a few tens of minutes later or in some cases hundreds of kilometres (miles) away many hours later. This process cause thunderstorm formation (and decay) to last for several hours or even over multiple days. Cumulonimbus clouds can also occur as dangerous winter storms called \"thundersnow\" which are associated with particularly intense snowfall rates and with blizzard conditions when accompanied by strong winds that further reduce visibility. However, cumulonimbus clouds are most common in tropical regions and are also frequent in moist environments during the warm season in the middle latitudes. A dust storm caused by a cumulonimbus downburst is a haboob.\nAviation.\nCumulonimbus are a notable hazard to aviation mostly due to potent wind currents but also reduced visibility and lightning, as well as atmospheric icing and hail if flying inside the cloud. Within and in the vicinity of thunderstorms there is significant turbulence and clear-air turbulence (particularly downwind), respectively. Wind shear within and under a cumulonimbus is often intense with downbursts being responsible for many accidents in earlier decades before training and technological detection and nowcasting measures were implemented. A small form of downburst, the microburst, is the most often implicated in crashes because of their rapid onset and swift changes in wind and aerodynamic conditions over short distances. Most downbursts are associated with visible precipitation shafts, however, dry microbursts are generally invisible to the naked eye. At least one fatal commercial airline accident was associated with flying through a tornado.\nLife cycle or stages.\nIn general, cumulonimbus require moisture, an unstable air mass, and a lifting force in order to form. Cumulonimbus typically go through three stages: the developing stage, the mature stage (where the main cloud may reach supercell status in favorable conditions), and the dissipation stage. The average thunderstorm has a diameter and a height of approximately . Depending on the conditions present in the atmosphere, these three stages take an average of 30 minutes to go through.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47531", "revid": "970543440", "url": "https://en.wikipedia.org/wiki?curid=47531", "title": "Convective", "text": ""}
{"id": "47532", "revid": "1316001688", "url": "https://en.wikipedia.org/wiki?curid=47532", "title": "Cumulus cloud", "text": "Genus of clouds, low-level cloud\nCumulus clouds are clouds that have flat bases and are often described as puffy, cotton-like, or fluffy in appearance. Their name derives from the Latin , meaning \"heap\" or \"pile\". Cumulus clouds are low-level clouds, generally less than in altitude unless they are the more vertical cumulus congestus form. Cumulus clouds may appear by themselves, in lines, or in clusters.\nCumulus clouds are often precursors of other types of clouds, such as cumulonimbus, when influenced by weather factors such as instability, humidity, and temperature gradient. Normally, cumulus clouds produce little or no precipitation, but they can grow into the precipitation-bearing cumulus congestus or cumulonimbus clouds. Cumulus clouds can be formed from water vapour, supercooled water droplets, or ice crystals, depending upon the ambient temperature. They come in many distinct subforms and generally cool the earth by reflecting the incoming solar radiation.\nCumulus clouds are part of the larger category of free-convective cumuliform clouds, which include cumulonimbus clouds. The latter genus-type is sometimes categorized separately as cumulonimbiform due to its more complex structure that often includes a cirriform or anvil top. There are also cumuliform clouds of limited convection that comprise stratocumulus (low-\u00e9tage), altocumulus (middle-\u00e9tage) and cirrocumulus (high-\u00e9tage). These last three genus-types are sometimes classified separately as stratocumuliform.\nFormation.\nCumulus clouds form via atmospheric convection as air warmed by the surface begins to rise. As the air rises, the temperature drops (following the lapse rate), causing the relative humidity (RH) to rise. If convection reaches a certain level the RH reaches one hundred percent, and the \"wet-adiabatic\" phase begins. At this point a positive feedback ensues: since the RH is above 100%, water vapor condenses, releasing latent heat, warming the air and spurring further convection.\nIn this phase, water vapor condenses on various nuclei present in the air, forming the cumulus cloud. This creates the characteristic flat-bottomed puffy shape associated with cumulus clouds. The height of the cloud (from its bottom to its top) depends on the temperature profile of the atmosphere and of the presence of any inversions. During the convection, surrounding air is entrained (mixed) with the thermal and the total mass of the ascending air increases.\nRain forms in a cumulus cloud via a process involving two non-discrete stages. The first stage occurs after the droplets coalesce onto the various nuclei. Langmuir writes that surface tension in the water droplets provides a slightly higher pressure on the droplet, raising the vapor pressure by a small amount. The increased pressure results in those droplets evaporating and the resulting water vapor condensing on the larger droplets. Due to the extremely small size of the evaporating water droplets, this process becomes largely meaningless after the larger droplets have grown to around 20\u00a0to 30\u00a0micrometres, and the second stage takes over. In the accretion phase, the raindrop begins to fall, and other droplets collide and combine with it to increase the size of the raindrop. Langmuir was able to develop a formula which predicted that the droplet radius would grow unboundedly within a discrete time period.\nDescription.\nThe liquid water density within a cumulus cloud has been found to change with height above the cloud base rather than being approximately constant throughout the cloud. In one particular study, the concentration was found to be zero at cloud base. As altitude increased, the concentration rapidly increased to the maximum concentration near the middle of the cloud. The maximum concentration was found to be anything up to 1.25\u00a0grams of water per kilogram of air. The concentration slowly dropped off as altitude increased to the height of the top of the cloud, where it immediately dropped to zero again.\nCumulus clouds can form in lines stretching over long called cloud streets. These cloud streets cover vast areas and may be broken or continuous. They form when wind shear causes horizontal circulation in the atmosphere, producing the long, tubular cloud streets. They generally form during high-pressure systems, such as after a cold front.\nThe height at which the cloud forms depends on the amount of moisture in the thermal that forms the cloud. Humid air will generally result in a lower cloud base. In temperate areas, the base of the cumulus clouds is usually below above ground level, but it can range up to in altitude. In arid and mountainous areas, the cloud base can be in excess of .\nCumulus clouds can be composed of ice crystals, water droplets, supercooled water droplets, or a mixture of them.\nOne study found that in temperate regions, the cloud bases studied ranged from above ground level. These clouds were normally above , and the concentration of droplets ranged from . This data was taken from growing isolated cumulus clouds that were not precipitating. The droplets were very small, ranging down to around 5\u00a0micrometres in diameter. Although smaller droplets may have been present, the measurements were not sensitive enough to detect them. The smallest droplets were found in the lower portions of the clouds, with the percentage of large droplets (around 20\u00a0to 30\u00a0micrometres) rising dramatically in the upper regions of the cloud. The droplet size distribution was slightly bimodal in nature, with peaks at the small and large droplet sizes and a slight trough in the intermediate size range. The skew was roughly neutral. Furthermore, large droplet size is roughly inversely proportional to the droplet concentration per unit volume of air. \nIn places, cumulus clouds can have \"holes\" where there are no water droplets. These can occur when winds tear the cloud and incorporate the environmental air or when strong downdrafts evaporate the water.\nSubforms.\nCumulus clouds come in four distinct species, \"cumulus humilis\", \"mediocris\", \"congestus\", and \"fractus\". These species may be arranged into the variety, \"cumulus radiatus\", and may be accompanied by up to seven supplementary features, \"cumulus pileus\", \"velum\", \"virga\", \"praecipitatio\", \"arcus\", \"pannus\", and \"tuba\".\nThe species \"Cumulus fractus\" is ragged in appearance and can form in clear air as a precursor to cumulus humilis and larger cumulus species-types, or it can form in precipitation as the supplementary feature \"pannus\" (also called scud) which can also include stratus fractus of bad weather. \"Cumulus humilis\" clouds look like puffy, flattened shapes. \"Cumulus mediocris\" clouds look similar, except that they have some vertical development. \"Cumulus congestus\" clouds have a cauliflower-like structure and tower high into the atmosphere, hence their alternate name \"towering cumulus\". The variety \"Cumulus radiatus\" forms in radial bands called cloud streets and can comprise any of the four species of cumulus.\nCumulus supplementary features are most commonly seen with the species congestus. \"Cumulus virga\" clouds are cumulus clouds producing virga (precipitation that evaporates while aloft), and \"cumulus praecipitatio\" produce precipitation that reaches the Earth's surface. \"Cumulus pannus\" comprise shredded clouds that normally appear beneath the parent cumulus cloud during precipitation. \"Cumulus arcus\" clouds have a gust front, and \"cumulus tuba\" clouds have funnel clouds or tornadoes. \"Cumulus pileus\" clouds refer to cumulus clouds that have grown so rapidly as to force the formation of pileus over the top of the cloud. \"Cumulus velum\" clouds have an ice crystal veil over the growing top of the cloud.\nThere are also cumulus cataractagenitus, which are formed by waterfalls.\nForecast.\nCumulus humilis clouds usually indicate fair weather. Cumulus mediocris clouds are similar, except that they have some vertical development, which implies that they can grow into cumulus congestus or even cumulonimbus clouds, which can produce heavy rain, lightning, severe winds, hail, and even tornadoes. Cumulus congestus clouds, which appear as towers, will often grow into cumulonimbus storm clouds. They can produce precipitation. Glider pilots often pay close attention to cumulus clouds, as they can be indicators of rising air drafts or thermals underneath that can suck the plane high into the sky\u2014a phenomenon known as cloud suck.\nEffects on climate.\nDue to reflectivity, clouds cool the earth by around , an effect largely caused by stratocumulus clouds. However, at the same time, they heat the earth by around by reflecting emitted radiation, an effect largely caused by cirrus clouds. This averages out to a net loss of . Cumulus clouds, on the other hand, have a variable effect on heating the Earth's surface. The more vertical \"cumulus congestus\" species and cumulonimbus genus of clouds grow high into the atmosphere, carrying moisture with them, which can lead to the formation of cirrus clouds. The researchers speculated that this might even produce a positive feedback, where the increasing upper atmospheric moisture further warms the earth, resulting in an increasing number of \"cumulus congestus\" clouds carrying more moisture into the upper atmosphere.\nRelation to other clouds.\nCumulus clouds are a genus of free-convective low-level cloud along with the related limited-convective cloud stratocumulus. These clouds form from ground level to at all latitudes. Stratus clouds are also low-level. In the middle level are the alto- clouds, which consist of the limited-convective stratocumuliform cloud altocumulus and the stratiform cloud altostratus. Mid-level clouds form from to in polar areas, in temperate areas, and in tropical areas. The high-level cloud, cirrocumulus, is a stratocumuliform cloud of limited convection. The other clouds in this level are cirrus and cirrostratus. High clouds form in high latitudes, in temperate latitudes, and in low, tropical latitudes. Cumulonimbus clouds, like cumulus congestus, extend vertically rather than remaining confined to one level.\nCirrocumulus clouds.\nCirrocumulus clouds form in patches and cannot cast shadows. They commonly appear in regular, rippling patterns or in rows of clouds with clear areas between. Cirrocumulus are, like other members of the cumuliform and stratocumuliform categories, formed via convective processes. Significant growth of these patches indicates high-altitude instability and can signal the approach of poorer weather. The ice crystals in the bottoms of cirrocumulus clouds tend to be in the form of hexagonal cylinders. They are not solid, but instead tend to have stepped funnels coming in from the ends. Towards the top of the cloud, these crystals have a tendency to clump together. These clouds do not last long, and they tend to change into cirrus because as the water vapor continues to deposit on the ice crystals, they eventually begin to fall, destroying the upward convection. The cloud then dissipates into cirrus. Cirrocumulus clouds come in four species which are common to all three genus-types that have limited-convective or stratocumuliform characteristics: \"stratiformis\", \"lenticularis\", \"castellanus\", and \"floccus\". They are iridescent when the constituent supercooled water droplets are all about the same size.\nAltocumulus clouds.\nAltocumulus clouds are a mid-level cloud that forms from high to in polar areas, in temperate areas, and in tropical areas. They can have precipitation and are commonly composed of a mixture of ice crystals, supercooled water droplets, and water droplets in temperate latitudes. However, the liquid water concentration was almost always significantly greater than the concentration of ice crystals, and the maximum concentration of liquid water tended to be at the top of the cloud while the ice concentrated itself at the bottom. The ice crystals in the base of the altocumulus clouds and in the virga were found to be dendrites or conglomerations of dendrites while needles and plates resided more towards the top. Altocumulus clouds can form via convection or via the forced uplift caused by a warm front.\nStratocumulus clouds.\nA stratocumulus cloud is another type of stratocumuliform cloud. Like cumulus clouds, they form at low levels and via convection. However, unlike cumulus clouds, their growth is almost completely retarded by a strong inversion. As a result, they flatten out like stratus clouds, giving them a layered appearance. These clouds are extremely common, covering on average around twenty-three percent of the Earth's oceans and twelve percent of the Earth's continents. They are less common in tropical areas and commonly form after cold fronts. Additionally, stratocumulus clouds reflect a large amount of the incoming sunlight, producing a net cooling effect. Stratocumulus clouds can produce drizzle, which stabilizes the cloud by warming it and reducing turbulent mixing.\nCumulonimbus clouds.\nCumulonimbus clouds are the final form of growing cumulus clouds. They form when \"cumulus congestus\" clouds develop a strong updraft that propels their tops higher and higher into the atmosphere until they reach the tropopause at in altitude. Cumulonimbus clouds, commonly called thunderheads, can produce high winds, torrential rain, lightning, gust fronts, waterspouts, funnel clouds, and tornadoes. They commonly have anvil clouds.\nHorseshoe clouds.\nA short-lived horseshoe cloud may occur when a horseshoe vortex deforms a cumulus cloud.\nExtraterrestrial.\nSome cumuliform and stratocumuliform clouds have been discovered on most other planets in the Solar System. On Mars, the Viking Orbiter detected cirrocumulus and stratocumulus clouds forming via convection primarily near the polar icecaps. The \"Galileo\" space probe detected massive cumulonimbus clouds near the Great Red Spot on Jupiter. Cumuliform clouds have also been detected on Saturn. In 2008, the \"Cassini\" spacecraft determined that cumulus clouds near Saturn's south pole were part of a cyclone over in diameter. The Keck Observatory detected whitish cumulus clouds on Uranus. Like Uranus, Neptune has methane cumulus clouds. Venus, however, does not appear to have cumulus clouds.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "47533", "revid": "903612589", "url": "https://en.wikipedia.org/wiki?curid=47533", "title": "Cyclonic", "text": ""}
{"id": "47535", "revid": "41719476", "url": "https://en.wikipedia.org/wiki?curid=47535", "title": "Haptophyte", "text": "Type of algae\nThe haptophytes, classified either as the Haptophytina, Haptophyta or Prymnesiophyta (named for \"Prymnesium\"), are a clade of algae that can produce minerals.\nThe names Haptophyceae or Prymnesiophyceae are sometimes used instead. This ending implies classification at the class rank rather than as a division. Although the phylogenetics of this group has become much better understood in recent years, there remains some dispute over which rank is most appropriate.\nCharacteristics.\nThe chloroplasts are pigmented similarly to those of the heterokonts, but the structure of the rest of the cell is different, so it may be that they are a separate line whose chloroplasts are derived from similar red algal endosymbionts. Haptophyte chloroplasts contain chlorophylls a, c1, and c2 but lack chlorophyll b. For carotenoids, they have beta-, alpha-, and gamma- carotenes. Like diatoms and brown algae, they have also fucoxanthin, an oxidized isoprenoid derivative that is likely the most important driver of their brownish-yellow color.\nThe cells typically have two slightly unequal flagella, both of which are smooth, and a unique organelle called a haptonema, which is superficially similar to a flagellum but differs in the arrangement of microtubules and in its use. The name comes from the Greek \"hapsis\", touch, and \"nema\", round thread. The mitochondria have tubular cristae.\nMost haptophytes reportedly produce chrysolaminarin rather than starch as their major storage polysaccharide, but some Pavlovaceae produce paramylon. The chain length of the chrysolaminarin is reportedly short (polymers of 20\u201350 glycosides, unlike the 300+ of comparable amylose), and it is located in cytoplasmic membrane-bound vacuoles.\nSignificance.\nThe best-known haptophytes are coccolithophores, which make up 673 of the 762 described haptophyte species, and have an exoskeleton of calcareous plates called coccoliths. Coccolithophores are some of the most abundant marine phytoplankton, especially in the open ocean, and are extremely abundant as microfossils, forming chalk deposits. Other planktonic haptophytes of note include \"Chrysochromulina\" and \"Prymnesium\", which periodically form toxic marine algal blooms, and \"Phaeocystis\", blooms of which can produce unpleasant foam which often accumulates on beaches.\nHaptophytes are economically important, as species such as \"Pavlova lutheri\" and \"Isochrysis sp.\" are widely used in the aquaculture industry to feed oyster and shrimp larvae. They contain a large amount of polyunsaturated fatty acids such as docosahexaenoic acid (DHA), stearidonic acid and alpha-linolenic acid. \"Tisochrysis lutea\" contains betain lipids and phospholipids.\nClassification.\nThe haptophytes were first placed in the class Chrysophyceae (golden algae), but ultrastructural data have provided evidence to classify them separately. Both molecular and morphological evidence supports their division into five orders; coccolithophores make up the Isochrysidales and Coccolithales. Very small (2\u20133\u03bcm) uncultured pico-prymnesiophytes are ecologically important.\nHaptophytes was discussed to be closely related to cryptomonads.\nHaptophytes are closely related to the SAR clade.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47537", "revid": "46051904", "url": "https://en.wikipedia.org/wiki?curid=47537", "title": "Configuration", "text": "Configuration or configurations may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "47538", "revid": "17790", "url": "https://en.wikipedia.org/wiki?curid=47538", "title": "High-frequency", "text": ""}
{"id": "47540", "revid": "14383484", "url": "https://en.wikipedia.org/wiki?curid=47540", "title": "Daylight savings time", "text": ""}
{"id": "47541", "revid": "48998989", "url": "https://en.wikipedia.org/wiki?curid=47541", "title": "United Nations Security Council Resolution 242", "text": "1967 resolution on withdrawal of Israel and recognition of boundaries\nUnited Nations Security Council Resolution 242 (S/RES/242) was adopted unanimously by the UN Security Council on November 22, 1967, in the aftermath of the Six-Day War. It was adopted under Chapter VI of the UN Charter. The resolution was sponsored by British ambassador Lord Caradon and was one of five drafts under consideration.\nThe preamble refers to the \"inadmissibility of the acquisition of territory by war and the need to work for a just and lasting peace in the Middle East in which every State in the area can live in security\".\nOperative Paragraph One \"Affirms that the fulfillment of Charter principles requires the establishment of a just and lasting peace in the Middle East which should include the application of both the following principles:\n(i) Withdrawal of Israeli armed forces from territories occupied in the recent conflict;\n(ii) Termination of all claims or states of belligerency and respect for and acknowledgment of the sovereignty, territorial integrity and political independence of every State in the area and their right to live in peace within secure and recognized boundaries free from threats or acts of force.\"\nEgypt, Jordan, Israel and Lebanon entered into consultations with the UN Special representative over the implementation of 242. After denouncing it in 1967, Syria \"conditionally\" accepted the resolution in March 1972. Syria formally accepted UN Security Council Resolution 338, the cease-fire at the end of the Yom Kippur War (in 1973), which embraced Resolution 242. \nOn 1 May 1968, the Israeli ambassador to the UN expressed Israel's position to the Security Council: \"My government has indicated its acceptance of the Security Council resolution for the promotion of agreement on the establishment of a just and lasting peace. I am also authorized to reaffirm that we are willing to seek agreement with each Arab State on all matters included in that resolution.\"\nResolution 242 is one of the most widely affirmed resolutions on the Arab\u2013Israeli conflict and formed the basis for later negotiations between the parties. These led to peace treaties between Israel and Egypt (1979) and Jordan (1994), as well as the 1993 and 1995 agreements with the Palestinians.\nContext.\nThe resolution is the formula proposed by the Security Council for the resolution of the Arab\u2013Israeli conflict, in particular, ending the state of belligerency then existing between the 'States concerned', Israel and Egypt, Jordan, Syria and Lebanon. The resolution deals with five principles; withdrawal of Israeli forces, 'peace within secure and recognized boundaries', freedom of navigation, a just settlement of the refugee problem and security measures including demilitarized zones. It also provided for the appointment of a Special Representative to proceed to the Middle East in order to promote agreement on a peaceful and accepted settlement in accordance with the principles outlined in the resolution.\nUpon presenting the draft resolution to the Security Council, the U.K. representative Lord Caradon said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;All of us recognize that peace is the prize. None of us wishes a temporary truce or a superficial accommodation. We could never advocate a return to uneasy hostility. As I have said, my Government would never wish to be associated with any so-called settlement which was only a continuation of a false truce, and all of us without any hesitation at all can agree that we seek a settlement within the principles laid down in Article 2 of the Charter. So much for the preamble.\nAs to the first operative paragraph, and with due respect for fulfillment of Charter principles, we consider it essential that there should be applied the principles of both withdrawal and security, and we have no doubt that the words set out throughout that paragraph are perfectly clear.\nAs to the second operative paragraph, there is I believe no vestige of disagreement between us all that there must be a guarantee of freedom of navigation through international waterways. There must be a just settlement of the refugee problem. There must be a guarantee and adequate means to ensure the territorial inviolability and political independence of every State in the area.\nAs to the third operative paragraph, I have said before that I consider that the United Nations special representative should be free to decide himself the exact means and methods by which he pursues his endeavors in contact with the States concerned both to promote agreement and to assist efforts to achieve a peaceful and accepted and final settlement.\"\nUnited States secretary of state Dean Rusk commented on the most significant area of disagreement regarding the resolution:\nThere was much bickering over whether that resolution should say from \"the\" territories or from \"all\" territories. In the French version, which is equally authentic, it says withdrawal de territory, with de meaning \"the.\" We wanted that to be left a little vague and subject to future negotiation because we thought the Israeli border along the West Bank could be \"rationalized\"; certain anomalies could easily be straightened out with some exchanges of territory, making a more sensible border for all parties. We also wanted to leave open demilitarization measures in the Sinai and the Golan Heights and take a fresh look at the old city of Jerusalem. But we never contemplated any significant grant of territory to Israel as a result of the June 1967 war. On that point we and the Israelis to this day remain sharply divided. This situation could lead to real trouble in the future. Although every President since Harry Truman has committed the United States to the security and independence of Israel, I'm not aware of any commitment the United States has made to assist Israel in retaining territories seized in the Six-Day War.\nA memorandum from the President's Special Assistant, Walt Rostow, to President Johnson said: \"What's on the Arab Ambassadors' minds boils down to one big question: Will we make good on our pledge to support the territorial integrity of all states in the Middle East? Our best answer is that we stand by that pledge, but the only way to make good on it is to have a genuine peace. The tough question is whether we'd force Israel back to 4 June borders if the Arabs accepted terms that amounted to an honest peace settlement. Secretary Rusk told the Yugoslav Foreign Minister: 'The US had no problem with frontiers as they existed before the outbreak of hostilities. If we are talking about national frontiers\u2014in a state of peace\u2014then we will work toward restoring them.' But we all know that could lead to a tangle with the Israelis.\"\nRusk met with Foreign Minister Nikezic on August 30, 1967. However, according to telegram 30825 to Belgrade, September 1, which summarizes the conversation, Rusk said the key to a settlement was to end the state of war and belligerence and that if a way could be found to deal with this, other things would fall into place; the difference between pre-June 5 positions and secure national boundaries was an important difference.\nPresident Johnson responded to a complaint from President Tito that Israel could change the frontiers without Arab consent: \"You note that the Arabs feel the US interprets the draft resolution to imply a change of frontiers to their detriment. We have no preconceptions on frontiers as such. What we believe to be important is that the frontiers be secure. For this the single most vital condition is that they be acceptable to both sides. It is a source of regret to us that the Arabs appear to misunderstand our proposal and misread our motives.\"\nFurthermore, Secretary Rusk's Telegram dated March 2, 1968, to the U.S. Interests Section of the Spanish Embassy in Cairo summarizing Undersecretary of State for Political Affairs Eugene Rostow\u2019s conversation with Soviet Ambassador Anatoly Dobrynin states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Rostow said ... resolution required agreement on \"secure and recognized\" boundaries, which, as practical matter, and as matter of interpreting resolution, had to precede withdrawals. Two principles were basic to Article I of resolution. Paragraph from which Dobrynin quoted was linked to others, and he did not see how anyone could seriously argue, in light of history of resolution in Security Council, withdrawal to borders of June 4th was contemplated. These words had been pressed on Council by Indians and others, and had not been accepted. Rusk\nIn an address delivered on September 1, 1982, President Ronald Reagan said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the pre-1967 borders Israel was barely 10 miles wide at its narrowest point. The bulk of Israel's population lived within artillery range of hostile Arab armies. I am not about to ask Israel to live that way again...\nSo the United States will not support the establishment of an independent Palestinian state in the West Bank and Gaza, and we will not support annexation or permanent control by Israel.\nThere is, however, another way to peace. The final status of these lands must, of course, be reached through the give-and-take of negotiations; but it is the firm view of the United States that self-government by the Palestinians of the West Bank and Gaza in association with Jordan offers the best chance for a durable, just and lasting peace.\nIt is the United States' position that\u00a0\u2013 in return for peace\u00a0\u2013 the withdrawal provision of Resolution 242 applies to all fronts, including the West Bank and Gaza.\nWhen the border is negotiated between Jordan and Israel, our view on the extent to which Israel should be asked to give up territory will be heavily affected by the extent of true peace and normalization and the security arrangements offered in return.\nFinally, we remain convinced that Jerusalem must remain undivided, but its final status should be decided through negotiations.\nAccording to Michael Lynk, there are three schools of thought concerning the proper legal interpretation of the withdrawal phrase. Some of the parties involved have suggested that the indefinite language is a \u201cperceptible loophole\u201d, that authorizes \u201cterritorial revision\u201d for Israel's benefit. Some have stated that the indefinite language was used to permit insubstantial and mutually beneficial alterations to the 1949 armistices lines, but that unilateral annexation of the captured territory was never authorized. Other parties have said that no final settlement obtained through force or the threat of force could be considered valid. They insist that the Security Council cannot create loopholes in peremptory norms of international law or the UN Charter, and that any use of indefinite language has to be interpreted in line with the overriding legal principles regarding the \u201cinadmissibility of the acquisition of territory by war\u201d and the prohibitions on mass deportations or displacement in connection with the settlement of the refugee problem.\nAlexander Orakhelashvili says that the Security Council manifestly lacks the competence to validate agreements imposed through coercion, not least because the peremptory prohibition of the use of force is a limitation on the Council's powers and the voidness of coercively imposed treaties is the clear consequence of jus cogens and the conventional law as reflected in the Vienna Convention on the Law of Treaties. A recent South African study concluded that the ultimate status and boundaries will require negotiation between the parties, according to Security Council Resolutions 242 and 338. The same study also found that the provisions of the Fourth Geneva Convention which govern \u2018special agreements\u2019 that can adversely affect the rights of protected persons precludes any change in status of the territory obtained through an agreement concluded during a state of belligerent occupation.\nContent.\nPreamble.\nThe second preambular reference states: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Emphasizing the inadmissibility of the acquisition of territory by war and the need to work for a just and lasting peace in which every State in the area can live in security.\"\nSrijita Jha and Akshadha Mishra said that \"until 1945, annexation by conquest was a valid mode of acquisition of territory.\" Following World War I, Article 10 of the Covenant of the League of Nations limited (but did not eliminate) the concept of the right of conquest, that is, members of the League of Nations were not required to preserve \"the territorial integrity and existing political independence\" of a state engaging in a war of aggression. Since World War II, Article 2 of the Charter of the United Nations requires all members to \"refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the purposes of the United Nations.\"\nMichael Lynk says that article 2 of the Charter embodied a prevailing legal principle that there could be \"no title by conquest\". He says that principle had been expressed through numerous international conferences, doctrines, and treaties since the late 19th Century. Lynk cites the examples of the First International Conference of American States in 1890; the United States Stimson Doctrine of 1932; the 1932 League of Nations resolution on Japanese aggression in China; the Buenos Aires Declaration of 1936; and the Atlantic Charter of 1941. Surya Sharma says that under the UN Charter, a war in self-defense cannot result in the acquisition of title by conquest. He says that even if a war is lawful in origin it cannot exceed the limits of legitimate self-defense.\nLand for peace.\nThe resolution also calls for the implementation of the \"land for peace\" formula, calling for Israeli withdrawal from \"territories\" it had occupied in 1967 in exchange for peace with its neighbors. This was an important advance at the time, considering that there were no peace treaties between any Arab state and Israel until the Egypt\u2013Israel peace treaty of 1979. \"Land for peace\" served as the basis of the Egypt\u2013Israel Peace Treaty, in which Israel withdrew from the Sinai Peninsula (Egypt withdrew its claims to the Gaza Strip in favor of the Palestine Liberation Organization). Jordan renounced its claims regarding the West Bank in favor of the Palestine Liberation Organization, and has signed the Israel\u2013Jordan peace treaty in 1994, that established the Jordan River as the boundary of Jordan.\nThroughout the 1990s, there were Israeli-Syrian negotiations regarding a normalization of relations and an Israeli withdrawal from the Golan Heights. But a peace treaty was not made, mainly due to Syria's desire to recover and retain 25 square kilometers of territory in the Jordan River Valley which it seized in 1948 and occupied until 1967. As the United Nations recognizes only the 1948 borders, there is little support for the Syrian position outside the Arab bloc nor in resolving the Golan Heights issue.\nThe UN resolution does not specifically mention the Palestinians. The United Kingdom had recognized the union between the West Bank and Transjordan. Lord Caradon said that the parties assumed that withdrawal from occupied territories as provided in the resolution was applicable to East Jerusalem. \"Nevertheless so important is the future of Jerusalem that it might be argued that we should have specifically dealt with that issue in the 1967 Resolution. It is easy to say that now, but I am quite sure that if we had attempted to raise or settle the question of Jerusalem as a separate issue at that time our task in attempting to find a unanimous decision would have been far greater if not impossible.\"\nJudge Higgins of the International Court of Justice explained \"from Security Council resolution 242 (1967) through to Security Council Resolution 1515 (2003), the key underlying requirements have remained the same\u00a0\u2013 that Israel is entitled to exist, to be recognized, and to security, and that the Palestinian people are entitled to their territory, to exercise self-determination, and to have their own State. Security Council resolution 1515 (2003)\nenvisages that these long-standing obligations are to be secured (...) by negotiation\"\nThe United States Secretary of State Madeleine Albright told the U.N. Security Council in 1994 that \"We simply do not support the description of the territories occupied by Israel in 1967 as 'Occupied Palestinian Territory'. In the view of my Government, this language could be taken to indicate sovereignty, a matter which both Israel and the PLO have agreed must be decided in negotiations on the final status of the territories. Had this language appeared in the operative paragraphs of the resolution, let me be clear: we would have exercised our veto. In fact, we are today voting against a resolution in the Commission on the Status of Women precisely because it implies that Jerusalem is 'occupied Palestinian territory'.\"\nThe Palestinians were represented by the Palestine Liberation Organization in negotiations leading to the Oslo Accords. They envisioned a 'permanent settlement based on Security Council Resolution 242'. The main premise of the Oslo Accords was the eventual creation of Palestinian autonomy in some or all of the territories captured during the Six-Day War, in return for Palestinian recognition of Israel. However, the Foreign Minister of the Palestinian Authority, Nabil Shaath, said: \"Whether a state is announced now or after liberation, its borders must be those of 4 June 1967. We will not accept a state without borders or with borders based on UN Resolution 242, which we believe is no longer suitable. On the contrary, Resolution 242 has come to be used by Israel as a way to procrastinate.\"\nThe Security Council subsequently adopted resolution 1515 (2003), which recalled resolution 242 and endorsed the Middle East Quartet's Road Map towards a permanent, two-State solution to the Israeli\u2013Palestinian conflict. The Quartet Plan calls for direct, bilateral negotiations as part of a comprehensive resolution of the Arab\u2013Israeli conflict, on the basis of UN Security Council Resolutions 242, 338, 1397, 1515, 1850, and the Madrid principles. The Quartet has reiterated that the only viable solution to the Israeli\u2013Palestinian conflict is an agreement that ends the occupation that began in 1967; resolves all permanent status issues as previously defined by the parties; and fulfils the aspirations of both parties for independent homelands through two states for two peoples, Israel and an independent, contiguous and viable state of Palestine, living side by side in peace and security.\nOn April 14, 2004, US President George W. Bush said to Israeli Prime Minister Ariel Sharon, \"The United States reiterates its steadfast commitment to Israel's security, including secure, defensible borders.\" Israeli officials argue that the pre-1967 armistice line is not a defensible border, since Israel would be nine miles wide at the thinnest point, subjected to rocket fire from the highlands of the West Bank, and unable to stop smuggling from Jordan across the Jordan Valley. Thus, Israeli officials have been arguing for the final-status borders to be readjusted to reflect security concerns.\nResolution 1860 (2009) recalled resolution 242 and stressed that the Gaza Strip constitutes an integral part of the territory occupied in 1967 that will be a part of the Palestinian state.\nSettlement of the refugee problem.\nThe resolution advocates a \"just settlement of the refugee problem\". Lord Caradon said \"It has been said that in the Resolution we treated Palestinians only as refugees, but this is unjustified. We provided that Israel should withdraw from occupied territories and it was together with that requirement for a restoration of Arab territory that we also called for a settlement of the refugee problem.\" Upon the adoption of Resolution 242, French President Charles de Gaulle stressed this principle during a press conference on November 27, 1967, and confirmed it in his letter of January 9, 1968, to David Ben-Gurion. De Gaulle cited \"the pitiful condition of the Arabs who had sought refuge in Jordan or were relegated to Gaza\" and stated that provided Israel withdrew her forces, it appeared it would be possible to reach a solution \"within the framework of the United Nations that included the assurance of a dignified and fair future for the refugees and minorities in the Middle East.\"\nAlexander Orakhelashvili said that \u2018Just settlement\u2019 can only refer to a settlement guaranteeing\nthe return of displaced Palestinians. He explained that it must be presumed that the Council did not adopt decisions that validated mass deportation or displacement, since expulsion or deportation are crimes against humanity or an exceptionally serious war crime.\nAccording to M. Avrum Ehrlich, 'Resolution 242 called for \"a just solution to the refugee problem,\" a term covering Jewish refugees from Arab countries as stated by President Carter in 1978 at Camp David'.\nAccording to John Quigley, however, it is clear from the context in which it was adopted, and from the statements recounted by the delegates, that Resolution 242 contemplates the Palestine Arab refugees only.\nArthur Goldberg, the United States ambassador to the U.N. at the time, wrote on the 20th anniversary that the \"language presumably refers both to Arab and Jewish refugees\".\nFrench version vs. English version of text.\nThe English version of the clause:Withdrawal of Israeli armed forces from territories occupied in the recent conflictis given in French as:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Retrait des forces arm\u00e9es isra\u00e9liennes des territoires occup\u00e9s lors du r\u00e9cent conflit.\nThe difference between the two versions lies in the absence of a definite article (\"the\") in the English version, while the word \"des\" present in the French version in the expression \"des territoires occup\u00e9s\" can only mean \"from the occupied territories\" (the \"des\" in front of \"territoires occup\u00e9s\" can only be the contraction \"from the\" because of the use of the word \"retrait\" which entails an object \u2013 \"des forces isra\u00e9liennes\" where the \"des\" is the contraction of \"of the\" (of the Israeli forces) and a location \"des territoires occup\u00e9s\" where \"des\" is the contraction of \"from the\" (from the occupied territories)). If the meaning of \"from some occupied territories\" were intended, the only way to say so in French would have been \"de territoires occup\u00e9s\".\nAlthough some have dismissed the controversy by suggesting that the use of the word \"des\" in the French version is a translation error and should therefore be ignored in interpreting the document, the debate has retained its force since both versions are of equal legal force, as recognized languages of the United Nations and in international law.\nSolicitor John McHugo, a partner at Trowers &amp; Hamlins and a visiting fellow at the Scottish Centre for International Law at Edinburgh University, draws a comparison to phrases such as:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Dogs must be kept on the lead near ponds in the park.\nIn spite of the lack of definite articles, according to McHugo, it is clear that such an instruction cannot legitimately be taken to imply that some dogs need not be kept on the lead or that the rule applies only near some ponds. Further, McHugo points out a potential consequence of the logic employed by advocates of a \"some\" reading. Paragraph 2 (a) of the resolution, which guarantees \"freedom of navigation through international waterways in the area,\" may allow Arab states to interfere with navigation through \"some\" international waterways of their choosing.\nGlenn Perry asserts that because the French version resolves ambiguities in the English text, and is more consistent with the other clauses of the treaty, it is the correct interpretation. He argues that \"it is an accepted rule that the various language versions must be considered together, with the ambiguities of one version elucidated by the other\". He cites Article 33 of the Vienna Convention on the Law of Treaties, which states that except when a treaty provides that one text shall prevail \"the meaning which best reconciles the texts, having regard to the object and purpose of the treaty, shall be adopted\". He furthermore argues that the context of the passage, in a treaty that reaffirms \"'territorial integrity', 'territorial inviolability,' and 'the inadmissibility of the acquisition of territory by war' \u2013 taken together cannot be reconciled with anything less than full withdrawal\". He argues that the reference to \"secure and recognized borders\" can be interpreted in several ways, and only one of them contradicts the principle of full withdrawal.\nShabtai Rosenne, former Permanent Representative of Israel to the United Nations Office at Geneva and member of the UN's International Law Commission, wrote that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It is a historical fact, which nobody has ever attempted to deny, that the negotiations between the members of the Security Council, and with the other interested parties, which preceded the adoption of that resolution, were conducted on the basis of English texts, ultimately consolidated in Security Council document S/8247. [...] Many experts in the French language, including academics with no political axe to grind, have advised that the French translation is an accurate and idiomatic rendering of the original English text, and possibly even the only acceptable rendering into French.\nOnly English and French were the Security Council's working languages (Arabic, Russian, Spanish and Chinese were official but not the working languages).\nThe Committee for Accuracy in Middle East Reporting in America argues the practice at the UN is that the binding version of any resolution is the one voted upon. In the case of 242 that version was in English, so they assert the English version the only binding one. David A. Korn asserts that this was indeed the position held by the United States and United Kingdom:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...\u00a0both the British and the Americans pointed out that 242 was a British resolution; therefore, the English language text was authoritative and would prevail in any dispute over interpretation.\nThe French representative to the Security Council, in the debate immediately after the vote, asserted (in the official translation from French):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;the French text, which is equally authentic with the English, leaves no room for any ambiguity, since it speaks of withdrawal \"des territoires occup\u00e9s,\" which indisputably corresponds to the expression \"occupied territories\". We were likewise gratified to hear the United Kingdom representative stress the link between this paragraph of his resolution and the principle of inadmissibility of the acquisition of territories by force...\nOpponents of the \"all territories\" reading remind that the UN Security Council declined to adopt a draft resolution, including the definite article, far prior to the adoption of Resolution 242. They argue that, in interpreting a resolution of an international organization, one must look to the process of the negotiation and adoption of the text. This would make the text in English, the language of the discussion, take precedence.\nThe negotiating and drafting process.\nA Congressional Research Service (CRS) Issue Brief quotes policy statements made by President Johnson in a speech delivered on September 10, 1968, and by Secretary of State Rogers in a speech delivered on December 9, 1969: \"The United States has stated that boundaries should be negotiated and mutually recognized, 'should not reflect the weight of conquest,' and that adjustments in the pre-1967 boundaries should be 'insubstantial.'\"\nPresident Carter asked for a State Department report \"to determine if there was any justice to the Israeli position that the resolution did not include all the occupied territories\". The State Department report concluded:Support for the concept of total withdrawal was widespread in the Security Council, and it was only through intensive American efforts that a resolution was adopted which employed indefinite language in the withdrawal clause. In the process of obtaining this result, the United States made clear to the Arab states and several other members of the Security Council that the United States envisioned only insubstantial revisions of the 1949 armistice lines. Israel did not protest the approach.\nRuth Lapidoth describes the view, adopted by Israel, which holds that the resolution allowed Israel to retain \"some territories\". She argues \"The provision on the establishment of \u201csecure and recognized boundaries\u201d would have been meaningless if there had been an obligation to withdraw from all the territories.\nU.S. Secretary of State Henry Kissinger recalled the first time he heard someone invoke \"the sacramental language of United Nations Security Council Resolution 242, mumbling about the need for a just and lasting peace within secure and recognized borders\". He said the phrase was so platitudinous that he thought the speaker was pulling his leg. Kissinger said that, at that time, he did not appreciate how the flood of words used to justify the various demands obscured rather than illuminated the fundamental positions. Kissinger said those \"clashing perspectives\" prevented any real bargaining and explained:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Jordan\u2019s acquiescence in Resolution 242 had been obtained in 1967 by the promise of our United Nations Ambassador Arthur Goldberg that under its terms we would work for the return of the West Bank of Jordan with minor boundary rectifications and that we were prepared to use our influence to obtain a role for Jordan in Jerusalem.\nHowever, speaking to Henry Kissinger, President Richard Nixon said \"You and I both know they can\u2019t go back to the other [1967] borders. But we must not, on the other hand, say that because the Israelis win this war, as they won the '67 War, that we just go on with status quo. It can't be done.\" Kissinger replied \"I couldn't agree more\"\nMoreover, President Gerald Ford said: \"The U.S. further supports the position that a just and lasting peace, which remains our objective, must be acceptable to both sides. The U.S. has not developed a final position on the borders. Should it do so it will give great weight to Israel's position that any peace agreement with Syria must be predicated on Israel remaining on the Golan Heights.\"\nFurthermore, Secretary of State George Shultz declared: \"Israel will never negotiate from, or return to, the lines of partition or to the 1967 borders.\"\nSecretary of State Christopher's letter to Netanyahu states: \"I would like to reiterate our position that Israel is entitled to secure and defensible borders, which should be directly negotiated and agreed with its neighbors.\"\nA key part of the case in favour of a \"some territories\" reading is the claim that British and American officials involved in the drafting of the resolution omitted the definite article deliberately in order to make it less demanding on the Israelis. As George Brown, British Foreign Secretary in 1967, said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Israelis had by now annexed de facto, if not formally, large new areas of Arab land, and there were now very many more Arab refugees. It was clear that what Israel or at least many of her leaders, really wanted was permanently to colonize much of this newly annexed Arab territory, particularly the Jordan valley, Jerusalem, and other sensitive areas. This led me into a flurry of activity at the United Nations, which resulted in the near miracle of getting the famous resolution\u00a0\u2013 Resolution 242\u00a0\u2013 unanimously adopted by the Security Council. It declares \"the inadmissibility of territory by war\" and it also affirms the necessity \"for guaranteeing the territorial inviolability and political independence of every state in the area\". It calls for \"withdrawal of Israeli forces from territories occupied during the recent conflict.\" It does not call for Israeli withdrawal from \u201cthe\u201d territories recently occupied, nor does it use the word \u201call\u201d. It would have been impossible to get the resolution through if either of these words had been included, but it does set out the lines on which negotiations for a settlement must take place. Each side must be prepared to give up something: the resolution doesn\u2019t attempt to say precisely what, because that is what negotiations for a peace-treaty must be about.\nLord Caradon, chief author of the resolution, takes a subtly different slant. His focus seems to be that the lack of a definite article is intended to deny permanence to the \"unsatisfactory\" pre-1967 border, rather than to allow Israel to retain land taken by force. Border rectification by mutual agreement is allowed:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Q. \"But how would one change the previous border without the acquisition of territory by war? Are you suggesting mutual concessions, that is, that both Israel and the Arabs would rationalize the border by yielding up small parcels of territory?\" A. Yes, I'm suggesting that... Q. \"And that this should be mutually done, with mutual territorial concessions?\" A. Yes, yes. To the benefit of all.\nArthur J. Goldberg, another of the resolution's drafters, argued that Resolution 242 does not dictate the extent of the withdrawal, and added that this matter should be negotiated between the parties:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Does Resolution 242 as unanimously adopted by the UN Security Council require the withdrawal of Israeli armed forces from all of the territories occupied by Israel during the 1967 war? The answer is no. In the resolution, the words the and all are omitted. Resolution 242 calls for the withdrawal of Israeli armed forces from territories occupied in the 1967 conflict, without specifying the extent of the withdrawal. The resolution, therefore, neither commands nor prohibits total withdrawal.\nIf the resolution is ambiguous, and purposely so, on this crucial issue, how is the withdrawal issue to be settled? By direct negotiations between the concerned parties. Resolution 242 calls for agreement between them to achieve a peaceful and accepted settlement. Agreement and acceptance necessarily require negotiations.\nMr. Michael Stewart, Secretary of State for Foreign and Commonwealth Affairs, in a reply to a question in Parliament, 9 December 1969: \"As I have explained before, there is reference, in the vital United Nations Security Council Resolution, both to withdrawal from territories and to secure and recognized boundaries. As I have told the House previously, we believe that these two things should be read concurrently and that the omission of the word 'all' before the word 'territories' is deliberate.\"\nMr. Joseph J. Sisco, Assistant Secretary of State, 12 July 1970 (NBC \"Meet the Press\"): \"That Resolution did not say 'withdrawal to the pre-June 5 lines'. The Resolution said that the parties must negotiate to achieve agreement on the so-called final secure and recognized borders. In other words, the question of the final borders is a matter of negotiations between the parties.\" Mr. Sisco was actively involved in drafting the resolution in his capacity as Assistant Secretary of State for International Organization Affairs in 1967.\nPresident Lyndon B. Johnson:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Fifth, the crisis underlines the importance of respect for political independence and territorial integrity of all the states of the area. We reaffirmed that principle at the height of this crisis. We reaffirm it again today on behalf of all.\nThis principle can be effective in the Middle East only on the basis of peace between the parties. The nations of the region have had only fragile and violated truce lines for 20 years. What they now need are recognized boundaries and other arrangements that will give them security against terror, destruction, and war.\nThere are some who have urged, as a single, simple solution, an immediate return to the situation as it was on June 4. As our distinguished and able Ambassador, Mr. Arthur Goldberg, has already said, this is not a prescription for peace but for renewed hostilities. Certainly troops must be withdrawn, but there must also be recognized rights of national life, progress in solving the refugee problem, freedom of innocent maritime passage, limitation of the arms race, and respect for political independence and territorial integrity.\"\nU.S. position.\nOn June 19, 1967, President Johnson declared the five principles, including land for peace, that he believed comprised the components of any United Nations settlement of the Middle East crisis. He pledged the U.S. Government would \"do its part for peace in every forum, at every level, at every hour\". On July 12, 1967, Secretary of State Rusk announced that the U.S. position on the Near East crisis was outlined in the President's statement of June 19 and that it provided the basis for a just and equitable settlement between the Arab states and Israel. On August 16, 1967, the Israeli Foreign Office stated that Israel agreed with the principles set forth by the President on June 19 and indicated that no resolution would be acceptable if it deviated from them.\nOn June 9, 1967, Israeli Foreign Minister Eban assured Arthur Goldberg, US Ambassador to the UN, that Israel was not seeking territorial aggrandizement and had no \"colonial\" aspirations. Secretary of State Rusk stressed to the Government of Israel that no settlement with Jordan would be accepted by the world community unless it gave Jordan some special position in the Old City of Jerusalem. The US also assumed Jordan would receive the bulk of the West Bank as that was regarded as Jordanian territory.\nOn November 3, 1967, Ambassador Goldberg, accompanied by Mr. Sisco and Mr. Pedersen, called on King Hussein of Jordan. Goldberg said the US was committed to the principle of political independence and territorial integrity and was ready to reaffirm it bilaterally and publicly in the Security Council resolution. Goldberg said the US believes in territorial integrity, withdrawal, and recognition of secure boundaries. Goldberg said the principle of territorial integrity has two important sub-principles: there must be a withdrawal to recognized and secure frontiers for all countries, not necessarily the old armistice lines, and there must be mutuality in adjustments.\nWalt Rostow advised President Johnson that Secretary Rusk had explained to Mr. Eban that US support for secure permanent frontiers does not mean the US supports territorial changes. The record of a meeting between Under Secretary of State Eugene Rostow and Israeli Ambassador Harmon stated that Rostow made clear the US view that there should be movement from General Armistice Agreements to conditions of peace and that this would involve some adjustments of armistice lines as foreseen in the Armistice Agreements. Rostow told Harmon that he had already stressed to Foreign Minister Eban that the US expected the thrust of the settlement would be toward security and demilitarization arrangements rather than toward major changes in the Armistice lines. Harmon said the Israeli position was that Jerusalem should be an open city under unified administration but that the Jordanian interest in Jerusalem could be met through arrangements including \"sovereignty\". Rostow said the US government assumed (and Harman confirmed) that despite public statements to the contrary, the Government of Israel position on Jerusalem was that which Eban, Harman, and Evron had given several times, that Jerusalem was negotiable.\nAmbassador Goldberg briefed King Hussein on US assurances regarding territorial integrity. Goldberg said the US did not view Jordan as a country that consisted only of the East Bank, and that the US was prepared to support a return of the West Bank to Jordan with minor boundary rectifications. The US would use its influence to obtain compensation to Jordan for any territory it would be required to give up. Finally, although as a matter of policy the US did not agree with Jordan's position on Jerusalem, nor with the Israeli position on Jerusalem, the US was prepared to use its influence to obtain for Jordan a role in Jerusalem. Secretary Rusk advised President Johnson that he confirmed Goldberg's pledge regarding territorial integrity to King Hussein.\nDuring a subsequent meeting between President Johnson, King Hussein, and Secretary of State Rusk, Hussein said the phrasing of the resolution calling for withdrawal from occupied territories could be interpreted to mean that the Egyptians should withdraw from Gaza and the Jordanians should withdraw from the West Bank. He said this possibility was evident from a speech given by Prime Minister Eshkol in which it had been claimed that both Gaza and the West Bank had been \"occupied territory\". The President agreed, and promised he would talk to Ambassador Goldberg about inserting Israel in that clause. Ambassador Goldberg told King Hussein that after taking into account legitimate Arab concerns and suggestions, the US would be willing to add the word \"Israeli\" before \"Armed Forces\" in the first operative paragraph.\nA State Department study noted that when King Hussein met on 8 November with President Johnson, who had been briefed by Secretary Rusk on the US interpretation, the Jordanian monarch asked how soon the Israeli troops would withdraw from most of the occupied lands. The President replied \"In six months.\"\nWilliam Quandt wrote about Johnson's meeting with Eban on October 24, 1967, and noted that Israel had annexed East Jerusalem. He said Johnson forcefully told Eban he thought Israel had been unwise when it went to war and that he still thought they were unwise. The President stressed the need to respect the territorial integrity of the Arab states. Quandt said \"'The President wished to caution the Israelis that the further they get from June 5 the further they get from peace.' Meaning the more territory they insisted on holding beyond the 1967 lines, the worse would be the odds of getting a peace agreement with the Arabs.\"\nInterpretations.\nIsrael interprets Resolution 242 as calling for withdrawal from territories as part of a negotiated peace and full diplomatic recognition. The extent of withdrawal would come as a result of comprehensive negotiations that led to durable peace not before Arabs start to meet their own obligations under Resolution 242.\nInitially, the resolution was accepted by Egypt, Jordan and Israel but not by the Palestine Liberation Organization. The Arab position was initially that the resolution called for Israel to withdraw from all the territory it occupied during the Six-Day War prior to peace agreements.\nIsrael and the Arab states have negotiated before the Israeli withdrawal. Israel and Jordan made peace without Israel withdrawing from the West Bank, since Jordan had already renounced its claims and recognized the PLO as the sole representative of the Palestinians. Egypt began negotiations before Israel withdrew from the Sinai. Negotiations ended without Egypt ever resuming control of the Gaza Strip, which Egypt held until 1967.\nSupporters of the \"Palestinian viewpoint\" focus on the phrase in the resolution's preamble emphasizing the \"inadmissibility of the acquisition of territory by war\", and note that the French version called for withdrawal from \"des territoires occup\u00e9s\" \u2013 \"\"the\" territories occupied\". The French UN delegation insisted on this interpretation at the time, but both English and French are the Secretariat's working languages.\nSupporters of the \"Israeli viewpoint\" note that the second part of that same sentence in the preamble explicitly recognizes the need of existing states to live in security.\nThey focus on the operative phrase calling for \"secure and recognized boundaries\" and note that the resolution calls for a withdrawal \"from territories\" rather than \"from the territories\" or \"from all territories,\" as the Arabs and others proposed; the latter two terms were rejected from the final draft of Resolution 242.\nAlexander Orakhelashvili cites a number of cases in which international tribunals have ruled that international organizations, including the Security Council, are bound by general international law. He says that inclusion of explicit clauses about the inadmissibility of acquisition of territory by war and requiring respect of territorial integrity and sovereignty of a state demonstrates that the Council does not intend to offend peremptory norms in these specific ways. The resolution also acknowledges that these principles must be part of an accepted settlement. That is confirmed by the Vienna Convention on the Law of Treaties which reiterates the prohibition on the use of force and provides that any settlement obtained by the threat or use of force in violation of the principles of international law embodied in the Charter of the United Nations or conflicting with a peremptory norm of general international law is invalid. According to Hans-Paul Gasser, \u2018doubtful\u2019 wording of the Council's resolutions must always be construed in such a way as to avoid conflict with fundamental international obligations.\nThe USSR, India, Mali, Nigeria and Arab States all proposed that the resolution be changed to read \"all territories\" instead of \"territories.\" Their request was discussed by the UN Security Council and \"territories\" was adopted instead of \"all territories\", after President Johnson told Premier Alexei Kosygin that the delegates should not try to negotiate the details of a Middle East settlement in the corridors and meeting halls of the United Nations, and Ambassador Goldberg stipulated that the exact wording of the resolution would not affect the position of any of the parties. Per Lord Caradon, the chief author of the resolution:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nLord Caradon also maintained,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;They're bad boundaries; they're just where the troops happened to be at a cease-fire line twenty years before, just where they happened to be sitting. The Arab legion was sitting across the road from Tel Aviv to Jerusalem and therefore there had to be a detour right up to 1967. So we knew\u2014we all knew\u2014that the boundaries of '67 were not drawn as permanent frontiers, they were a cease-fire line of a couple of decades earlier. So we deliberately did not say\u2014I'm glad to be able to say that\u2014we did not say that the '67 boundaries must be forever. We thought there should be a boundary commission to hear both sides and to deal with the thing in a sensible manner.\nDuring a symposium on the subject Lord Caradon said that Israel was in clear defiance of resolution 242. He specifically cited the \"annexation of East Jerusalem\" and \"the creeping colonialism on the West Bank and in Gaza and in the Golan.\"\nHowever, British Foreign Secretary George Brown said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I have been asked over and over again to clarify, modify or improve the wording, but I do not intend to do that. The phrasing of the Resolution was very carefully worked out, and it was a difficult and complicated exercise to get it accepted by the UN Security Council. I formulated the Security Council Resolution. Before we submitted it to the Council, we showed it to Arab leaders. The proposal said 'Israel will withdraw from territories that were occupied', and not from 'the' territories, which means that Israel will not withdraw from all the territories.\nThe PLO.\nThe day after Resolution 242 was adopted, the Palestine Liberation Organization (PLO) rejected it as \"fundamentally and gravely inconsistent with the Arab character of Palestine, the essence of the Palestine cause and the right of the Palestinian people to their homeland.\" and \"disappoints the hopes of the Arab nation and ignores its national aspirations [... and] ignores the existence of the Palestinian people and their right of self-determination.\"\nReplacing the National Charter of 1964 formulated by the first Palestine National Council (PNC), a revised National Charter was drawn up by the fourth PNC at Cairo in July 1968.\nAt the 12th PNC in Cairo on 8 June 1974, the PLO adopted the Ten-Point Program. Some hardline factions split away to form the Rejectionist Front.https:// On the same day the PNC recommended to the PLO executive committee participation in the Geneva process. While reiterating its rejection of UN 242 the PLO should engage in a \"framework other than that of resolution 242.\" The Program, a compromise with rejectionists, marked the first official PLO document that suggested the feasibility of a two-state solution. While Israel was not likely to accept such conditions, the document suggested compromise. According to scholar Shaul Mishal, \"a real shift in the PLO position towards the occupied territories;unequivocal support for military struggle has ever since been supplemented by a willingness to consider political means as well.\" Although a minority, the creation of the Rejectionist Front enabled an argument that the PLO did not speak for all Palestinians and so should not participate at Geneva.\nOn 22 November 1974, United Nations General Assembly Resolution 3236 recognized the right of the Palestinian people to self-determination, national independence and sovereignty in Palestine. It also recognized the PLO as the sole legitimate representative of the Palestinian people, and accorded it observer status in the United Nations. In 1975, as part of the Sinai II agreement, Kissinger had promised Israel that the United States would not deal with the PLO until it recognized Israel's right to exist and accepted United Nations Security Council Resolutions 242 and 338. The 1978 Camp David Accords attempted to address the Palestinian problem but there continued to be no direct Palestinian representation.\nThe 1988 Palestinian Declaration of Independence included a PNC call for multilateral negotiations on the basis of UN Security Council Resolution 242 later known as \"the Historic Compromise\", implying acceptance of a two-state solution and no longer questioning the legitimacy of the State of Israel. The PNC called only for withdrawal from Arab Jerusalem and \"Arab territories occupied.\" Together with Yasser Arafat's later statements in Geneva this was accepted by the United States as a basis for dialogue.\nFor the Madrid Conference of 1991 Israel still refused to deal directly with the PLO and the Palestinians formed part of a joint delegation with Jordan. Finally, in the 1993 Declaration of Principles and the subsequent Israeli-Palestinian agreements, Israel and the PLO each recognized the other and agreed terms of reference as Resolutions 242 and 338.\nStatements by Security Council representatives.\nThe representative for India stated to the Security Council:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe representatives from Nigeria, France, Soviet Union, Bulgaria, United Arab Republic (Egypt), Ethiopia, Jordan, Argentina and Mali supported this view, as worded by the representative from Mali: \"[Mali] wishes its vote today to be interpreted in the light of the clear and unequivocal interpretation which the representative of India gave of the provisions of the United Kingdom text.\" The Russian representative Vasili Kuznetsov stated:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nIsrael was the only country represented at the Security Council to express a contrary view. The United States, United Kingdom, Canada, Denmark, China and Japan were silent on the matter, but the US and UK did point out that other countries' comments on the meaning of 242 were simply their own views. The Syrian representative was strongly critical of the text's \"vague call on Israel to withdraw\".\nThe statement by the Brazilian representative perhaps gives a flavour of the complexities at the heart of the discussions:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nHowever, the Soviet delegate Vasily Kuznetsov argued: \" ... phrases such as 'secure and recognized boundaries'. ... make it possible for Israel itself arbitrarily to establish new boundaries and to withdraw its forces only to those lines it considers appropriate.\" [1373rd meeting, para. 152.]\nU.S. Supreme Court Justice Arthur Goldberg, who represented the US in discussions, later stated: \"The notable omissions in regard to withdrawal are the word 'the' or 'all' and 'the June 5, 1967, lines' the resolution speaks of withdrawal from occupied territories, without defining the extent of withdrawal\".\nImplementation.\nOn November 23, 1967, the Secretary General appointed Gunnar Jarring as Special Envoy to negotiate the implementation of the resolution with the parties, the so-called Jarring Mission. The governments of Israel, Egypt, Jordan and Lebanon recognized Jarring's appointment and agreed to participate in his shuttle diplomacy, although they differed on key points of interpretation of the resolution. The government of Syria rejected Jarring's mission on grounds that total Israeli withdrawal was a prerequisite for further negotiations. The talks under Jarring's auspices lasted until 1973, but bore no results. After 1973, the Jarring mission was replaced by bilateral and multilateral peace conferences.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47542", "revid": "8260261", "url": "https://en.wikipedia.org/wiki?curid=47542", "title": "Buffy the Vampire Slayer", "text": "American supernatural TV series (1997\u20132003)\nBuffy the Vampire Slayer is an American supernatural drama television series created by Joss Whedon. The show's concept is based on the 1992 film, also written by Whedon, although they are separate and distinct productions. Whedon served as executive producer and showrunner of the series under his production tag Mutant Enemy Productions. It aired on The WB from March 10, 1997, to May 22, 2001, and later on UPN from October 2, 2001, to May 20, 2003.\nThe series follows Buffy Summers (played by Sarah Michelle Gellar), the latest in a succession of young women known as \"Vampire Slayers\". Slayers are chosen by fate to battle against vampires, demons and other forces of darkness. Buffy wants to live a normal life, but learns to embrace her destiny as the series progresses. Like previous Slayers, she is aided by a Watcher, part of the Watcher's Council based in England, who guides, teaches and trains her. Unlike her predecessors, Buffy surrounds herself with loyal friends who become known as the \"Scoobies\". The show primarily takes place in the fictional setting of Sunnydale, a small Southern California city located on a \"Hellmouth\"; a portal \"between this reality and the next\", and a convergence point of mystical energies. Because of this, supernatural creatures and beings with magical powers, both good and evil, are drawn to Sunnydale or rise from below ground to menace the town and the world.\nThe series received critical and popular acclaim, and is often listed among the greatest television series of all time. Original airings often reached four to six million viewers. Although lower than successful shows on the \"big four\" networks (ABC, CBS, NBC and Fox), these ratings were a success for the relatively new and smaller WB Television Network. Despite being mostly ignored in above-the-line categories by the Emmys, the series was nominated for the American Film Institute Award for Drama Series of the Year, Gellar was nominated for the Golden Globe Award for Best Actress \u2013 Television Series Drama for her performance in the show and the series was nominated five times for Television Critics Association Awards, winning in 2003 for the Television Critics Association Heritage Award.\nThe success of \"Buffy\" has led to hundreds of tie-in products, including novels, comics and video games. The series has received attention in fandom (including fan films), parody, and academia, and has influenced the direction of other television series. \"Buffy\" was part of a wave of television series from the late 1990s and early 2000s that featured strong female characters, alongside \"Charmed\", \"\", \"La Femme Nikita\", \"Dark Angel\", and \"Alias\". The series, as well as its spin-off series, \"Angel\", and extensions thereof, have been collectively termed the \"Buffyverse\".\nPremise.\nCharacters.\nBuffy Summers (played by Sarah Michelle Gellar) is the \"Slayer\", one in a long line of young women chosen by fate to battle evil forces. This mystical calling grants her powers that dramatically increase physical strength, endurance, agility, accelerated healing, intuition, and a limited degree of precognition, usually in the form of prophetic dreams. She is known as a reluctant hero who wants to live a normal life. However, she learns to embrace her destiny as the vampire slayer.\nBuffy receives guidance from her Watcher, Rupert Giles (Anthony Stewart Head). Giles, rarely referred to by his first name (it is later revealed that in his rebellious younger days he went by \"Ripper\"), is a member of the Watchers' Council, whose job is to train and guide the Slayers. Giles researches the supernatural creatures that Buffy must face, offers insights into their origins, advice on how to defeat them, and helps her train to stay in fighting form.\nBuffy also receives help from the friends she meets at Sunnydale High School: Willow Rosenberg (Alyson Hannigan) and Xander Harris (Nicholas Brendon). Willow is originally a wallflower who excels at academics, providing a contrast to Buffy's outgoing personality and less-than-stellar educational record. They share the social isolation that comes with being different, and especially from being exceptional young women. As the series progresses, Willow becomes a more assertive character and a powerful witch, and realizes she is a lesbian. In contrast, Xander, with no supernatural abilities, provides comic relief and a grounded perspective. Buffy and Willow are the only characters who appear in all 144 episodes, with Xander appearing in 143.\nThe cast of characters grew over the course of the series. Buffy first arrives in Sunnydale with her mother, Joyce Summers (Kristine Sutherland), who functions as an anchor of normality in the Summers' lives even after she learns of Buffy's role in the supernatural world (\"Becoming, Part Two\"). Buffy's younger sister Dawn Summers (Michelle Trachtenberg) is introduced in season five (\"Buffy vs. Dracula\"). Angel (David Boreanaz), a vampire cursed with a soul, is Buffy's love interest throughout the first three seasons. He leaves Buffy after realizing he will never be able to give her a normal life. He goes on to make amends for his sins and to search for redemption in his own spin-off television series, \"Angel\". He makes several guest appearances in the remaining seasons, and is present in \"Buffy\"'s final episode.\nAt Sunnydale High, Buffy meets several other students besides Willow and Xander willing to join her fight for good, an informal group eventually tagged the \"Scooby Gang\" or \"Scoobies\". Cordelia Chase (Charisma Carpenter), the archetypal shallow cheerleader, reluctantly becomes involved. Daniel \"Oz\" Osbourne (Seth Green), a fellow student, rock guitarist and werewolf, joins the group through his relationship with Willow. Jenny Calendar (Robia LaMorte), Sunnydale's computer science teacher, joins the group after helping destroy a demon trapped in cyberspace during season 1; she later becomes Giles' love interest. Anya (Emma Caulfield) is a former vengeance demon called Anyanka who specialized in avenging scorned women; after losing her powers she became Xander's lover, then joined the Scoobies in season four.\nIn Buffy's senior year at high school, she meets Faith (Eliza Dushku), another Slayer called forth when Slayer Kendra Young (Bianca Lawson) was killed by vampire Drusilla (Juliet Landau) in season two. Although Faith initially fights on the side of good with Buffy and the rest of the group, she later joins forces with Mayor Richard Wilkins (Harry Groener) after accidentally killing a human. She reappears briefly in the fourth season, looking for vengeance, and moves to \"Angel\" where she voluntarily goes to jail for her crimes. Faith reappears in season seven of \"Buffy\", after having helped Angel and his crew, and fights alongside Buffy against the First Evil.\nBuffy gathers other allies throughout the series: Spike (James Marsters), a vampire, is an old companion of Angelus (Angel) and one of Buffy's major enemies in early seasons, although he and Buffy later become allies and lovers. At the end of season six, Spike regains his soul. Spike is known for his Billy Idol-style peroxide blond hair and his black leather coat, stolen from a previous Slayer, Nikki Wood. Nikki's son, Robin Wood (D. B. Woodside), joins the group in the final season. Tara Maclay (Amber Benson) is a fellow member of Willow's Wicca group during season four, and their friendship evolves into a romantic relationship. Buffy becomes involved personally and professionally with Riley Finn (Marc Blucas), a military operative in \"the Initiative\", which hunts demons using science and technology. The seventh and final season sees geeky wannabe-villain Andrew Wells (Tom Lenk) side with the Scoobies after initially being their captive/hostage; they regard him more as a nuisance than an ally.\n\"Buffy\" featured dozens of major and minor recurring characters. For example, the \"Big Bad\" (villain) characters were featured for at least one season (for example, Glory is a character who appeared in 12 episodes, spanning much of season five). Similarly, characters who allied themselves to the group and characters who attended the same institutions were sometimes featured in multiple episodes.\nSetting and filming locations.\nThe show is set in the fictional California town of Sunnydale, whose suburban Sunnydale High School sits on top of a \"Hellmouth\", a gateway to demon realms. The Hellmouth, located beneath the school library, is a source of mystical energies as well as a nexus for a wide variety of evil creatures and supernatural phenomena. Joss Whedon cited the Hellmouth and \"high school as hell\" as one of the primary metaphors in creating the series.\nMost of \"Buffy\" was shot on location in Los Angeles, California. The high school used in the first three seasons is actually Torrance High School, in Torrance, California, the same high school used for \"Beverly Hills, 90210\". The show was initially very dependent on location shooting, because the production budget allowed for few permanent sets to be built. In the first season this was limited to the interior of Sunnydale High (the library, hallways, and classrooms), Buffy's bedroom, and the Master's underground lair. Starting in the second season, more permanent sets were built, including the full interior of Buffy's house, Angel's mansion, and Giles's apartment, as well as extensions to the high school set (the addition of a dining hall and commons area). A driveway area near the gated entrance to Fox Studios was transformed into a graveyard. In the third season the Sunnydale \"Main Street\" was constructed on the backlot, which would be a staple location for the rest of the series. When the show transitioned to college in the fourth season, the hallway sets from Sunnydale High were remodeled to appear as the interior hallways of UC Sunnydale.\nSome of the exterior shots of the college Buffy attends, UC Sunnydale, were filmed at UCLA. Several episodes include shots from the Oviatt Library at CSUN. The exterior of the Crawford Street mansion where Angelus, Spike, and Drusilla lived was Frank Lloyd Wright's Ennis House.\nFormat.\n\"Buffy\" is told in a serialized format, mixing complex, season-long storylines with a villain-of-the-week conflict revolving around Buffy and her friends as they struggle to balance the fight against supernatural evils with their complicated social lives. A typical episode contains one or more villains, or supernatural phenomena, that are thwarted or defeated by the end of the episode. Though elements and relationships are explored and ongoing subplots are included, the show focuses primarily on Buffy and her role as an archetypal heroine. Gellar described the show as \"the ultimate metaphor: horrors of adolescence manifesting through these actual monsters. It's the hardest time of life.\" Each season's storyline is broken down into season-long narratives marked by the rise and defeat of a powerful antagonist, commonly referred to as the \"Big Bad\".\nWhile the show is mainly a drama with frequent comic relief, most episodes blend different genres, including horror, martial arts, romance, melodrama, farce, fantasy, supernatural, comedy, and, in one episode, musical comedy.\nIn the first few seasons, the most prominent monsters in the \"Buffy\" bestiary are vampires based on traditional myths, lore, and literary conventions. As the series continues, Buffy and her companions fight an increasing variety of demons, as well as ghosts, werewolves, zombies, and unscrupulous humans. They frequently save the world from annihilation by a combination of physical combat, magic, and detective-style investigation, and are guided by an extensive collection of ancient and mystical reference books. \nEpisodes.\nPlot summary.\nSeason one exemplifies the \"high school is hell\" concept. Buffy Summers has just moved to Sunnydale after burning down her old school's gym and hopes to escape her Slayer duties. Her plans are complicated by Rupert Giles, her new Watcher, who reminds her of the inescapable presence of evil. Sunnydale High is built atop a Hellmouth, a portal to demon dimensions that attracts supernatural phenomena to the area. A mysterious man, Angel, warns Buffy of upcoming danger. She eventually discovers that he is a vampire cursed with a soul, which prevents him from feeding off living humans. Buffy befriends two schoolmates, Xander Harris and Willow Rosenberg, who help her fight evil throughout the series. Buffy, her Watcher and friends later start to collectively call themselves the \"Scooby Gang\". Their first major threat is the Master, an ancient and especially threatening vampire, who was trapped in the hellmouth underground. When he escapes, Buffy defeats him and saves Sunnydale.\nThe emotional stakes are raised in season two. Vampire couple Spike and Drusilla come to town. A new slayer, Kendra, who is activated as a result of Buffy's brief death in season one, also arrives in Sunnydale. Popular schoolmate, Cordelia Chase, who resented Buffy and her friends, joins the Scooby Gang and becomes involved with Xander. Willow learns witchcraft and becomes involved with schoolmate Daniel \"Oz\" Osbourne, who is a werewolf. The romantic relationship between Buffy and the vampire Angel develops. But after they have sex, Angel experiences a moment of true happiness, breaking the curse that gave him his soul, thus reverting him to a sadistic killer. The evil vampire, famously known as Angelus, joins the other vampires Spike and Drusilla, and he torments Buffy and her friends. He murders multiple innocents and Giles's new girlfriend Jenny Calendar, a Romani woman who was sent to maintain Angel's curse. Kendra is murdered by Drusilla. To avert an apocalypse, Buffy is forced to banish Angel to a hell dimension just moments after Willow has restored his soul. The ordeal leaves Buffy emotionally shattered, and she leaves Sunnydale.\nAfter attempting to start a new life in Los Angeles, Buffy returns to town in season three. Angel has been mysteriously released from the demon dimension but is close to insanity due to the torment he suffered there. He recovers, but he and Buffy realize that a relationship between them can never happen and Angel leaves Sunnydale at the end of the season. Giles is fired from the Watchers' Council because he has developed a \"father's love\" for Buffy and he is replaced by Wesley Wyndam-Pryce. Towards the end of the season, Buffy announces that she will no longer be working for the council. Early in the season, she meets Faith, the Slayer activated after Kendra's death. She also encounters the affable Mayor Richard Wilkins III, who secretly has plans to \"ascend\" (become a \"pure\" demon) on Sunnydale High's graduation day. Although Faith initially works well with Buffy, she becomes increasingly unstable after accidentally killing a human and forms a relationship with the paternal yet manipulative mayor. The rivalry between Buffy and Faith eventually lands Faith in a coma. At the end of the season, after the mayor becomes a huge snake-like demon, Buffy, Angel, the Scooby Gang and the entire graduating class destroy him by blowing up Sunnydale High. At the end of the season, Angel and Cordelia leave the series to star in the spin-off series, \"Angel\".\nSeason four sees Buffy and Willow enroll at UC Sunnydale, while Xander joins the workforce and begins dating Anya, a former vengeance demon. Spike returns as a series regular and is abducted by The Initiative, a top-secret military installation based beneath the UC Sunnydale campus. They implant a microchip in his head that prevents him from harming humans. Every time he tries to harm a human, he suffers excruciating pain. Upon learning that he can still harm other demons, he joins in with the Scooby Gang, purely for the joy of fighting. Oz leaves town after realizing that he is too dangerous as a werewolf, and Willow falls in love with Tara Maclay, another witch. Faith awakens from her coma and escapes from Sunnydale to L.A. Buffy begins dating Riley Finn, a graduate student and US Army Ranger seconded to The Initiative. Although appearing to be a well-meaning anti-demon operation, The Initiative's sinister plans are revealed when Adam, a monster secretly built from parts of humans, demons and machinery, escapes and begins to wreak havoc on the town. Adam is destroyed by a magical composite of Buffy and her three friends, and The Initiative is shut down.\nDuring season five, a younger sister, Dawn, suddenly appears in Buffy's life; although she is new to the series, to the characters it is as if she has always been there. Buffy is confronted by Glory, an exiled Hell God who is searching for a \"Key\" that will allow her to return to her Hell dimension and in the process blur the lines between dimensions and unleash Hell on Earth. It is later discovered that the Key's protectors have used Buffy's blood to turn the Key into human form\u2013Dawn\u2013concurrently implanting everybody with lifelong memories of her. The Watchers' Council aids in Buffy's research on Glory, and she and Giles are both reinstated on their own terms. Riley leaves early in the season after realizing that Buffy does not love him and joins a military demon-hunting operation. Spike, still implanted with his chip from The Initiative, realizes he is in love with Buffy and increasingly helps the Scoobies in their fight. Buffy's mother Joyce dies of a brain aneurysm, while at the end of the season, Xander proposes to Anya. Glory finally discovers that Dawn is the key and kidnaps her, using Dawn's blood to open a portal to the Hell dimension. To save Dawn, Buffy sacrifices her own life by diving into the portal, thus closing it with her death.\nAt the beginning of season six, Buffy has been dead for 147 days, but Buffy's friends resurrect her through a powerful spell, believing they have rescued her from a Hell dimension. Buffy returns in a deep depression, explaining (several episodes later) that she had been in heaven and is devastated to be pulled back to earth. Giles returns to England because he has concluded that Buffy has become too reliant on him, while Buffy takes up a fast-food job to support herself and Dawn and develops a secret, mutually abusive sexual relationship with Spike. Dawn suffers from kleptomania and feelings of alienation, Xander leaves Anya at the altar (after which she once again becomes a vengeance demon), and Willow becomes addicted to magic, causing Tara to temporarily leave her. They also begin to deal with the Trio, a group of nerds led by Warren Mears who use their proficiency in technology and magic to attempt to kill Buffy and take over Sunnydale. Warren is shown to be the only competent villain of the group and, after Buffy thwarts his plans multiple times, the Trio breaks apart. Warren becomes unhinged and attacks Buffy with a gun, accidentally killing Tara in the process. This causes Willow to descend into nihilistic darkness and unleash all of her dark magical powers, killing Warren and attempting to kill his friends. Giles returns to face her in battle and infuses her with light magic, tapping into her remaining humanity. This overwhelms Willow with guilt and pain, whereupon she attempts to destroy the world to end everyone's suffering, although it eventually allows Xander to reach through her pain and end her rampage. Late in the season, after losing control and trying to rape Buffy, Spike leaves Sunnydale and travels to see a demon and asks him to \"return him to what he used to be\" so that he can \"give Buffy what she deserves\". After Spike passes a series of brutal tests, the demon restores his soul.\nDuring season seven, it is revealed that Buffy's second resurrection caused instability in the slayer line which also allowed the First Evil to begin tipping the balance between good and evil. It begins by hunting down and killing inactive Potential Slayers and soon raises an army of ancient, powerful Turok-Han vampires. After the Watchers' Council is destroyed, a number of Potential Slayers (some brought by Giles) take refuge in Buffy's house. Faith returns to help fight the First Evil, and the new Sunnydale High School principal, Robin Wood, also joins the cause. The Turok-Han vampires and a sinister, misogynistic preacher known as Caleb begin causing havoc for the Scoobies. As the Hellmouth becomes more active, nearly all of Sunnydale's population\u2013humans and demons alike\u2013flee. In the series finale, Buffy kills Caleb and Angel returns to Sunnydale with an amulet, which Buffy gives to Spike; the Scoobies then surround the Hellmouth, and the Potential Slayers descend into its cavern while Willow casts a spell that activates their Slayer powers. Anya dies in the fight, as do some of the new Slayers. Spike's amulet channels the power of the sun to destroy the Hellmouth and all the vampires within it, including himself. The collapse of the cavern creates a crater that swallows all of Sunnydale, while the survivors of the battle escape in a school bus. In the final scene, as the survivors survey the crater, Dawn asks, \"What are we going to do now?\" Buffy slowly begins to enigmatically smile as she contemplates the future ahead of her, ending the series on a hopeful note.\nProduction.\nOrigins.\nWriter Joss Whedon says that \"Rhonda the Immortal Waitress\" was really the first incarnation of the \"Buffy\" concept, \"the idea of some woman who seems to be completely insignificant who turns out to be extraordinary\". This early, unproduced idea evolved into \"Buffy\", which Whedon developed to invert the Hollywood formula of \"the little blonde girl who goes into a dark alley and gets killed in every horror movie\". Whedon wanted \"to subvert that idea and create someone who was a hero\". He explained, \"The very first mission statement of the show was the joy of female power: having it, using it, sharing it.\"\nThe idea was first visited through Whedon's script for the 1992 movie \"Buffy the Vampire Slayer\", which featured Kristy Swanson in the title role. The director, Fran Rubel Kuzui, saw it as a \"pop culture comedy about what people think about vampires\". Whedon disagreed: \"I had written this scary film about an empowered woman, and they turned it into a broad comedy. It was crushing.\" The script was praised within the industry, but the movie was not.\nSome years later, Gail Berman (later a Fox executive, but at that time president and CEO of the production company Sandollar Television, who owned the TV rights to the movie) approached Whedon to develop his \"Buffy\" concept into a television series. Whedon explained that \"They said, 'Do you want to do a show?' And I thought, 'High school as a horror movie.' And so the metaphor became the central concept behind \"Buffy\", and that's how I sold it.\" The supernatural elements in the series stood as metaphors for personal anxieties associated with adolescence and young adulthood. Early in its development, the series was going to be simply titled \"Slayer\". Whedon went on to write and partly fund a 25-minute non-broadcast pilot that was shown to networks and eventually sold to the WB Network. The latter promoted the premiere with a series of \"History of the Slayer\" clips, and the first episode aired on March 10, 1997. Whedon has declared in June 2003 that the non-broadcast pilot would not be included with DVDs of the series \"while there is strength in these bones\", stating that it \"sucks on ass\". Dolly Parton was an uncredited producer of the series.\nExecutive producers.\nJoss Whedon was credited as executive producer throughout the run of the series, and for the first five seasons (1997\u20132001) he was also the showrunner, supervising the writing and all aspects of production. Marti Noxon took on the role for seasons six and seven (2001\u20132003), but Whedon continued to be involved with writing and directing \"Buffy\" alongside projects such as \"Angel\", \"Fray\", and \"Firefly\". Fran Rubel Kuzui and her husband, Kaz Kuzui, were credited as executive producers but were not involved in the show. Their credit, rights, and royalties over the franchise relate to their funding, producing, and directing of the original movie version of \"Buffy\".\nWriting.\nScript-writing was done by Mutant Enemy, a production company created by Whedon in 1997. The writers with the most writing credits are Joss Whedon, Steven S. DeKnight, Jane Espenson, David Fury, Drew Goddard, Drew Greenberg, David Greenwalt, Rebecca Rand Kirshner, Marti Noxon and Doug Petrie. Other authors with writing credits include Dean Batali, Carl Ellsworth, Tracey Forbes, Ashley Gable, Howard Gordon, Diego Gutierrez, Elin Hampton, Rob Des Hotel, Matt Kiene, Ty King, Thomas A. Swyden, Joe Reinkemeyer, Dana Reston and Dan Vebber.\nJane Espenson has explained how scripts came together. First, the writers talked about the emotional issues facing Buffy Summers and how she would confront them through her battle against evil supernatural forces. Then the episode's story was broken into acts and scenes. Act breaks were designed as key moments to intrigue viewers so that they would stay with the episode following the commercial break. The writers collectively filled in scenes surrounding these act breaks for a more fleshed-out story. A whiteboard marked their progress by mapping brief descriptions of each scene. Once breaking was complete, the credited author wrote an outline for the episode, which was checked by Whedon or Noxon. The writer then wrote a full script, which went through a series of drafts, and finally a quick rewrite from the showrunner. The final article was used as the shooting script.\nMusic.\n\"Buffy\" features a mix of original, indie, rock, and pop music. The composers spent around seven days scoring between fourteen and thirty minutes of music for each episode. Christophe Beck revealed that the \"Buffy\" composers used computers and synthesizers and were limited to recording one or two \"real\" samples. Despite this, their goal was to produce \"dramatic\" orchestration that would stand up to film scores.\nAlongside the score, most episodes featured indie rock music, usually at the characters' venue of choice, The Bronze. \"Buffy\" music supervisor John King explained that \"we like to use unsigned bands\" that \"you would believe would play in this place\". For example, the fictional group Dingoes Ate My Baby were portrayed on screen by front group Four Star Mary. Pop songs by famous artists were rarely featured prominently, but several episodes spotlighted the sounds of more famous artists such as Sarah McLachlan, The Brian Jonestown Massacre, Blink-182, Third Eye Blind, Aimee Mann (who also had a line of dialogue), The Dandy Warhols, Cibo Matto, Coldplay, Lisa Loeb, K's Choice, and Michelle Branch. The popularity of music used in \"Buffy\" has led to the release of four soundtrack albums: ', ', the \"\"Once More, with Feeling\" Soundtrack\", and \"\".\nSpecial effects.\n\"Buffy\" features a variety of monsters and supernatural creatures. Monster suits were created by John Vulich and his special effects company Optic Nerve, while blending and beauty makeup was created by makeup supervisor Todd McIntosh. McIntosh is credited with creating the iconic vampire face \"Buffy\" is known for, inspired by McIntosh's love for shows like the Gothic soap opera \"Dark Shadows\".\nInspirations and metaphors.\nDuring the first year of the series, Whedon described the show as \"\"My So-Called Life\" with \"The X-Files\"\". Alongside these series, Whedon has cited cult film \"Night of the Comet\" as a \"big influence\", and credited the \"X-Men\" character Kitty Pryde as a significant influence on the character of Buffy. The authors of the unofficial guidebook \"Dusted\" point out that the series was often a pastiche, borrowing elements from previous horror novels, movies, and short stories and from such common literary stock as folklore and mythology. Nevitt and Smith describe \"Buffy\"'s use of pastiche as \"postmodern Gothic\". For example, the Adam character parallels the \"Frankenstein\" monster, the episode \"Bad Eggs\" parallels \"Invasion of the Body Snatchers\", \"Out of Mind, Out of Sight\" parallels \"The Invisible Man\", and so on.\n\"Buffy\" episodes frequently include a deeper meaning or metaphor. Whedon explained, \"We think very carefully about what we're trying to say emotionally, politically, and even philosophically while we're writing it... it really is, apart from being a pop-culture phenomenon, something that is deeply layered textually episode by episode.\" Academics Wilcox and Lavery provide examples of episodes dealing with real life issues portrayed as supernatural metaphors:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the world of \"Buffy\" the problems that teenagers face become literal monsters. A mother can take over her daughter's life (\"Witch\"); a strict stepfather-to-be really is a heartless machine (\"Ted\"); a young lesbian fears that her nature is demonic (\"Goodbye Iowa\" and \"Family\"); a girl who has sex with even the nicest-seeming guy may discover that he afterward becomes a monster (\"Innocence\").\nThe love affair between the vampire Angel and Buffy was fraught with metaphors. For example, their night of passion cost the vampire his soul. Sarah Michelle Gellar said: \"That's the ultimate metaphor. You sleep with a guy and he turns bad on you\". Marsters said that his character was part of an audience-and network-forced change for the show; themes about overcoming adolescent problems gave way to \"problems that are kind of sexy\", frustrating Whedon.\nBuffy struggles throughout the series with her calling as Slayer and the loss of freedom this entails, frequently sacrificing teenage experiences for her Slayer duties. Her difficulties and eventual empowering realizations are reflections of several dichotomies faced by modern women and echo feminist issues within society.\nIn the episode \"Becoming (Part 2)\", when Joyce learns that Buffy is the Slayer, her reaction has strong echoes of a parent discovering her child is gay, including denial, suggesting that she tries \"not being a Slayer\", before ultimately kicking Buffy out of the house.\nCasting.\nKatie Holmes and Selma Blair were in the running for the role of Buffy in 1996. Natasha Lyonne was also considered for the role but declined it due to not wanting to commit to a series at the age of 16. Other actresses who originally auditioned for the role of Buffy and got other roles in the show include Julie Benz (Darla), Elizabeth Anne Allen (Amy Madison), Julia Lee (Chantarelle/Lily Houston), Charisma Carpenter (Cordelia Chase) and Mercedes McNab (Harmony Kendall). Bianca Lawson, who played slayer Kendra Young in season 2 of the show, originally auditioned for the role of Cordelia before Carpenter was cast in the role.\nThe title role went to Sarah Michelle Gellar, who had appeared as Sydney Rutledge on \"Swans Crossing\" and Kendall Hart on \"All My Children\". At age 18 in 1995, Gellar had already won a Daytime Emmy Award for Outstanding Younger Leading Actress in a Drama Series. In 1996, she originally auditioned for the role of Cordelia. After watching her audition, Whedon asked her to come back in and audition for the lead role of Buffy.\nA talent agent spotted David Boreanaz on the sidewalk walking his dog. He immediately contacted casting director Marcia Shulman, saying that he had found Angel.\nAnthony Stewart Head had already led a prolific acting and singing career but remained best known in the United States for a series of twelve coffee commercials with Sharon Maughan for Taster's Choice instant coffee. He accepted the role of Rupert Giles.\nNicholas Brendon, unlike other \"Buffy\" regulars, had little acting experience, instead working various jobs\u2014including production assistant, plumber's assistant, veterinary janitor, food delivery, script delivery, day care counselor, and waiter\u2014before breaking into acting and overcoming his stutter. He landed the role of Xander Harris following four days of auditioning. Ryan Reynolds was offered a undisclosed role, but declined due to his real-life experience in high school. Danny Strong also auditioned for the part, he later played the role of Jonathan Levinson, a recurring character for much of the series run.\nAlyson Hannigan was the last of the original six to be cast. Following her role in \"My Stepmother Is an Alien\", she appeared in commercials and supporting roles on television shows throughout the early 1990s. In 1996, the role of Willow Rosenberg was originally played by Riff Regan for the unaired \"Buffy\" pilot, but Hannigan auditioned when the role was being recast for the series proper. Hannigan described her approach to the character through Willow's reaction to a particular moment: Willow sadly tells Buffy that her Barbie doll was taken from her as a child. Buffy asks her if she ever got it back. Willow's line was to reply \"most of it\". Hannigan decided on an upbeat and happy delivery of the line \"most of it\", as opposed to a sad, depressed delivery. Hannigan figured Willow would be happy and proud that she got \"most of it\" back. That indicated how she was going to play the rest of the scene, and the role, for that matter, and defined the character. Her approach subsequently got her the role.\nOpening sequence.\nThe \"Buffy\" opening sequence provides credits at the beginning of each episode, with the accompanying music performed by Californian rock band Nerf Herder. In the DVD commentary for the first \"Buffy\" episode, Whedon said his decision to go with Nerf Herder's theme was influenced by Hannigan, who had urged him to listen to the band's music. Nerf Herder later recorded a second version of the theme which was used for the opening titles from season 3 on. Janet Halfyard, in her essay \"Music, Gender, and Identity in \"Buffy the Vampire Slayer\" and \"Angel\"\", describes the opening:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Firstly ... we have the sound of an organ, accompanied by a wolf's howl, with a visual image of a flickering night sky overlaid with unintelligible archaic script: the associations with both the silent era and films such as \"Nosferatu\" and with the conventions of the Hammer House of Horror and horror in general are unmistakable.\nBut the theme quickly changes: \"It removes itself from the sphere of 1960s and 70s horror by replaying the same motif, the organ now supplanted by an aggressively strummed electric guitar, relocating itself in modern youth culture ...\" Halfyard describes sequences, in which the action and turbulence of adolescence are depicted, as the visual content of the opening credits, and which provide a postmodern twist on the horror genre.\nBroadcast history and syndication.\n\"Buffy the Vampire Slayer\" first aired on March 10, 1997 (as a mid-season replacement for the series \"Savannah\") on The WB, and played a key role in the growth of the Warner Bros. television network in its early years. After five seasons, it transferred to UPN for its final two seasons. In 2001, the show went into syndication in the United States on local stations and on cable channel FX; the local airings ended in 2005, and the FX airings lasted until 2008 but returned to the network in 2013. Beginning in January 2010, it began to air in syndication in the United States on Logo. Reruns also briefly aired on MTV. In March 2010, it began to air in Canada on MuchMusic and MuchMore. On November 7, 2010, it began airing on Chiller with a 24-hour marathon; the series airs weekdays. Chiller also aired a 14-hour Thanksgiving Day marathon on November 25, 2010. In 2011, it began airing on Oxygen and TeenNick. On June 22, 2015, it began airing on ABC Family.\nWhile the seventh season was still being broadcast, Sarah Michelle Gellar told \"Entertainment Weekly\" she was not going to sign on for an eighth year; \"When we started to have such a strong year this year, I thought: 'This is how I want to go out, on top, at our best.'\" Whedon and UPN gave some considerations to production of a spin-off series that would not require Gellar, including a rumored Faith series, but nothing came of those plans. The \"Buffy\" canon continued outside the television medium in the Dark Horse Comics series, \"Buffy\" Season Eight. This was produced starting March 2007 by Whedon, who also wrote the first story arc, \"The Long Way Home\".\nIn the United Kingdom, the entire series aired on Sky One and BBC Two. After protests from fans about early episodes being edited for their pre-watershed time-slot, from the second run (mid-second season onwards), the BBC gave the show two time slots: the early-evening slot (typically Thursday at 6:45 pm) for a family-friendly version with violence, objectionable language and other stronger material cut out, and a late-night uncut version (initially late-night Sundays, but for most of the run, late-night Fridays; exact times varied). Sky1 aired the show typically at 8:00 pm on Thursdays. From the fourth season onwards, the BBC aired the show in anamorphic 16:9 widescreen format. Whedon later said that \"Buffy\" was never intended to be viewed this way. Despite his claims, Syfy now airs repeat showings in the widescreen format.\nIn August 2014, Pivot announced that, for the first time, episodes of \"Buffy\" would be broadcast in high-definition and in a widescreen format authorized by the studio, but not by any of the series' principals. The transfer was poorly received by some fans, owing to a number of technical and format changes that were viewed as detrimental to the show's presentation; various scenes were heavily cropped to fit the 16:9 format, and shots were altered to have a brighter look, often with color levels altered. Other problems included missing filters, editing errors, and poorly re-rendered CGI. Series creator Joss Whedon and other members of the original team also expressed their displeasure.\nThe series became available on Disney+ (under the Star brand) beginning February 2021, and was added to Comet's digital network in 2022.\nSpin-offs.\n\"Buffy\" has inspired a range of official works, including television shows, books, comics, games, and podcasts. This expansion of the series encouraged use of the term \"Buffyverse\" to describe the franchise and the fictional universe in which \"Buffy\" and related stories take place.\nThe franchise has inspired \"Buffy\" action figures and merchandise such as official \"Buffy/Angel\" magazines and \"Buffy\" companion books. Eden Studios has published a \"Buffy\" role-playing game, while Score Entertainment has released a \"Buffy\" Collectible Card Game.\nContinuations.\nThe story line was continued in a series of comic books produced by Joss Whedon and published by Dark Horse Comics, which serve as a canonical continuation of the television series. The series began in 2007 with \"Buffy the Vampire Slayer Season Eight\" and was followed by \"Buffy the Vampire Slayer Season Nine\" in 2011, \"Buffy the Vampire Slayer Season Ten\" in 2014, \"Buffy the Vampire Slayer Season Eleven\" in 2016, and \"Buffy the Vampire Slayer Season Twelve\" in 2018.\nJoss Whedon was interested in a film continuation in 1998, but such a film never materialized.\nInitial attempts and upcoming revival.\nIn July 2018, 20th Century Fox Television reportedly began development on a television reboot of the series. Monica Owusu-Breen was to serve as showrunner and had been working on the script with Whedon, who was to be an executive producer. News of Whedon's involvement was seen as reassuring by fans, though the extent of his involvement was unclear; other executive producers reported to be involved included Gail Berman, Fran Kuzui, and Kaz Kuzui, all credited as executive producers for the original series. According to anonymous sources who spoke with \"The Hollywood Reporter\" and \"Deadline Hollywood\", the producers wanted the new series to be \"richly diverse ... [and] some aspects of the series could be seen as metaphors for issues facing society today\"\u2013similar to the way Gellar described the original series as the \"ultimate metaphor\" for coping with adolescence. The producers intended \"for the new slayer to be African American\", an example of the diversity they wish to portray. The report from \"Deadline Hollywood\" cautioned that \"the project is still in nascent stages with no script, and many details are still in flux\".\nAt the time of \"Buffy\"'s 20th anniversary in 2017, Whedon expressed fear of reboots, commenting that when \"something [is brought] back, and even if it's exactly as good as it was, the experience can't be. You've already experienced it, and part of what was great was going through it for the first time. You have to meet expectations and adjust it for the climate, which is not easily [done].\" Similar concerns were expressed about the decision to reboot the series, rather than to revive it or further expand the Buffyverse. Reports that a black actress was to assume the iconic role of Buffy, rather than having a new character or Slayer created, have been met with questions and concerns. Vox noted that \"the original series already had multiple characters of color who could factor into an 'inclusive' reboot\u2013including the black slayer Kendra and the 'First Slayer'\" \u2013 leaving fans wondering \"why a reboot has to racebend Buffy, when it could simply focus on a different character\". A Twitter message posted by Owusu-Breen on July 26, 2018, was interpreted by media outlets as indicating that the new series would not recast the role of Buffy and instead would focus on a new Slayer. In August 2022, executive producer Gail Berman announced that the series was put \"on pause\" indefinitely. In January 2024, Dolly Parton stated that the producers were still working on the reboot and were \"revamping it.\"\nIn February 2025, \"Variety\" reported that a \"Buffy\" sequel series was nearing a pilot order at Hulu without Whedon's involvement. Gellar was set to reprise her role and serve as an executive producer alongside Gail Berman, Fran Kuzui, Kaz Kuzui, and Parton. Chlo\u00e9 Zhao was appointed as the pilot's director, with Nora and Lilla Zuckerman credited as the writers. The new series would feature a new Slayer as the primary protagonist, while Buffy Summers would appear in a recurring role. In May 2025, Ryan Kiera Armstrong was cast in the lead role. In July 2025, Faly Rakotohavana, Ava Jean, Sarah Bock, Daniel Di Tomasso and Jack Cutmore-Scott were announced as series regulars in the pilot. Filming for the pilot began in Los Angeles in August 2025 and additional cast members were announced, including Merrin Dungey, Audrey Hsieh, Audrey Grace Marshall and Chase Sui Wonders.\n\"Angel\".\nThe spin-off \"Angel\" was introduced in October 1999, at the start of \"Buffy\" season four. The series was created by \"Buffy\"'s creator Joss Whedon in collaboration with David Greenwalt. Like \"Buffy\", it was produced by the production company Mutant Enemy. At times, it performed better in the Nielsen ratings than its parent series did.\nThe series was given a darker tone, focusing on the ongoing trials of Angel in Los Angeles. His character is tormented by guilt following the return of his soul, punishment for more than a century of murder and torture. During the first four seasons of the show, he works as a private detective in a fictionalized version of Los Angeles, California, where he and his associates work to \"help the helpless\", to restore the faith and \"save the souls\" of those who have lost their way. Typically, this mission involves doing battle with demons or demonically allied humans (primarily the law firm Wolfram &amp; Hart), while Angel must also contend with his own violent nature. In season five, the Senior Partners of Wolfram and Hart take a bold gamble in their campaign to corrupt Angel, giving him control of their Los Angeles office. Angel accepts the deal as an opportunity to fight evil from the inside.\nIn addition to Boreanaz, \"Angel\" inherited \"Buffy\" series cast regular Charisma Carpenter (Cordelia Chase). When Glenn Quinn (Doyle) left the series during its first season, Alexis Denisof (Wesley Wyndam-Pryce), who played a recurring character in the last nine episodes of season three of \"Buffy\", took his place. Carpenter and Denisof were followed later by Mercedes McNab (Harmony Kendall) and James Marsters (Spike). Several actors and actresses who played \"Buffy\" characters made guest appearances on \"Angel\", including Seth Green (Daniel \"Oz\" Osbourne), Sarah Michelle Gellar (Buffy Summers), Eliza Dushku (Faith), Tom Lenk (Andrew Wells), Alyson Hannigan (Willow Rosenberg), Julie Benz (Darla), Mark Metcalf (The Master), Julia Lee (Anne Steele) and Juliet Landau (Drusilla). Angel also continued to appear occasionally on \"Buffy\".\nOther actors that appeared in both the \"Buffy the Vampire Slayer\" and \"Angel\" series but as different characters include: Bob Fimiani as Mr. Ward, a head of the Department of Defense in \"Buffy\" and Glith-roo, a Codger Demon in \"Angel\"; Carlos Jacott as a demon named Ken in \"Buffy\" and a different demon named Richard Straley in \"Angel\"; Jonathan M. Woodward as a vampire and former classmate in \"Buffy\" named Holden Webster and Knox, a Wolfram and Hart scientist in \"Angel\"; and Andy Umberger who played a demon named D'Hoffryn in \"Buffy\" and a predator named Ronald Meltzer in \"Angel\".\nThe storyline has been continued in the comic book series \"\" published by IDW Publishing and later \"Angel and Faith\" published by Dark Horse Comics.\nExpanded universe.\nThe series' fiction has been officially expanded and elaborated on by authors and artists in the so-called \"Buffyverse Expanded Universe\". The creators of these works may or may not keep to established continuity. Similarly, writers for the TV series were under no obligation to use information which had been established by the Expanded Universe, and sometimes contradicted such continuity.\nDark Horse has published the \"Buffy\" comics since 1998. In 2003, Whedon wrote an eight-issue miniseries for Dark Horse Comics titled \"Fray\", about a Slayer in the future. Following the publication of \"Tales of the Vampires\" in 2004, \"Dark Horse Comics\" halted publication on Buffyverse-related comics and graphic novels. The company produced Whedon's \"Buffy the Vampire Slayer Season Eight\" with forty issues from March 2007 to January 2011, picking up where the television show left off\u2014taking the place of an eighth canonical season. The first story arc is also written by Whedon and is called \"The Long Way Home\", which has been widely well-received, with circulation rivaling industry leaders DC and Marvel's top-selling titles. After \"The Long Way Home\" came other story arcs like Faith's return in \"No Future for You\" and a \"Fray\" crossover in \"Time of Your Life\". Dark Horse later followed \"Season Eight\" with \"Buffy the Vampire Slayer Season Nine\", starting in 2011, and \"Buffy the Vampire Slayer Season Ten\", which began in 2014. Dark Horse continued to publish \"Buffy\" comics continuing the story after the television show until September 2018, when it released the final issue of \"Buffy the Vampire Slayer Season Twelve\", which intended to bring closure to the series. Following the end of Dark Horse's \"Buffy\" series, Boom! Studios acquired the license to publish \"Buffy\" comics. Taking a different approach from Dark Horse, Boom! Studios decided to publish a new rebooted \"Buffy\" series in 2019 with many elements updated to be more contemporary. Boom! Studio's approach to rebooting \"Buffy\" has been stylistically compared to the \"Ultimate Marvel\" series by the creators. Joss Whedon is not as involved in the rebooted \"Buffy\" comic as he was in Dark Horse's continuation, however he did take part in the initial development stages for the series and gave his blessing to the creators.\nSimon &amp; Schuster holds the license to produce \"Buffy\" novels, of which they published more than sixty between 1998 and 2008, under their Pocket Books and Simon Pulse imprints. These sometimes flesh out background information on characters; for example, \"Go Ask Malice\" details the events that lead up to Faith arriving in Sunnydale. The most recent novels include \"Carnival of Souls\", \"Blackout\", \"Portal Through Time\", \"Bad Bargain\", \"The Deathless\" and \"One Thing or Your Mother\". After a ten-year hiatus, two additional novels were published in 2019 and 2020, following on from story threads in the comic book series.\nFive official \"Buffy\" video games have been released on portable and home consoles. Most notably, \"Buffy the Vampire Slayer\" for Xbox in 2002 and \"\" for GameCube, Xbox and PlayStation 2 in 2003.\nIn September 2023, an audio series titled \"Slayers: A Buffyverse Story\" was announced, to premiere on October 12, 2023, on Audible. The series is set 10 years after the events of the series finale and the story is led by Spike (James Marsters); also returning are Charisma Carpenter, Anthony Head, Juliet Landau, Emma Caulfield, Amber Benson, James C. Leary, and Danny Strong. The series was written by Benson and Christopher Golden, and directed by Benson, Golden, and Kc Wayland. In February 2024, Audible canceled the series after one season.\nUndeveloped spinoffs.\nThe popularity of \"Buffy\" and \"Angel\" has led to attempts to develop more on-screen ventures in the fictional 'Buffyverse'. These projects remain undeveloped and may never be greenlit. In 2002, two potential spinoffs were in discussion: \"\" and \"Ripper\". \"Buffy: The Animated Series\" was a proposed animated TV show based on \"Buffy\"; Whedon and Jeph Loeb were to be executive producers for the show, and most of the cast from \"Buffy\" were to return to voice their characters. 20th Century Fox showed an interest in developing and selling the show to another network. A three-minute pilot was completed in 2004 but was never picked up. Whedon revealed to \"The Hollywood Reporter\": \"We just could not find a home for it. We had six or seven hilarious scripts from our own staff\u2013and nobody wanted it.\" Writer Jane Espenson has teased small extracts from some of her scripts for the show.\n\"Ripper\" was originally a proposed television show based upon the character of Rupert Giles portrayed by Anthony Stewart Head. More recent information has suggested that if \"Ripper\" were ever made, it would be a TV movie or a DVD movie. There was little heard about the series until 2007 when Joss Whedon confirmed that talks were almost completed for a 90-minute \"Ripper\" special on the BBC with both Head and the BBC completely on board.\nIn 2003, a year after the first public discussions on \"Buffy: The Animated Series\" and \"Ripper\", \"Buffy\" was nearing its end. Espenson said during the time spin-offs were being discussed, \"I think Marti talked with Joss about \"Slayer School\" and Tim Minear talked with him about Faith on a motorcycle. I assume there was some back-and-forth pitching.\" Espenson has revealed that \"Slayer School\" might have used new slayers and potentially included Willow Rosenberg, but Whedon did not think that such a spinoff felt right.\nDushku declined the pitch for a Buffyverse TV series based on Faith and instead agreed to a deal to produce \"Tru Calling\". Dushku explained to IGN: \"It would have been a really hard thing to do, and not that I would not have been up for a challenge, but with it coming on immediately following \"Buffy\", I think that those would have been really big boots to fill.\" Tim Minear explained some of the ideas behind the aborted series: \"The show was basically going to be Faith meets \"Kung Fu\". It would have been Faith, probably on a motorcycle, crossing the earth, trying to find her place in the world.\"\nFinally, during the summer of 2004 after the end of \"Angel\", a movie about Spike was proposed. The movie would have been directed by Tim Minear and starred Marsters and Amy Acker and featured Alyson Hannigan. Outside the 2006 Saturn Awards, Whedon announced that he had pitched the concept to various bodies but had yet to receive any feedback.\nIn September 2008, \"Sci-Fi Wire\" ran an interview with Sarah Michelle Gellar in which she said she would not rule out returning to her most iconic role: \"Never say never\", she said. \"One of the reasons the original \"Buffy\" movie did not really work on the big screen\u2013and people blamed Kristy, but that's not what it was\u2013the story was better told over a long arc\", Gellar said. \"And I worry about Buffy as a 'beginning, middle and end' so quickly. ... You show me a script; you show me that it works, and you show me that [the] audience can accept that, [and] I'd probably be there. Those are what my hesitations are.\"\nLegacy and cultural impact.\nAcademia.\n\"Buffy\" is notable for attracting the interest of scholars of popular culture, as a subset of popular culture studies, and some academic settings include the show as a topic of literary study and analysis. National Public Radio describes \"Buffy\" as having a \"special following among academics, some of whom have staked a claim in what they call 'Buffy Studies.'\" Though not widely recognized as a distinct discipline, the term \"Buffy studies\" is commonly used amongst the peer-reviewed academic \"Buffy\"-related writings. The influence of \"Buffy\" on the depiction of vampires across popular culture has also been noted by anthropologists such as A. Asbj\u00f8rn J\u00f8n. Popular media researcher Rob Cover argued that Buffy and Angel speak to contemporary attitudes to identity, inclusion, and diversity, and that critiquing the characters' long-narrative stories lends insight into the complexity of identity in the current era and the landscape of social issues in which those identities are performed.\nCritics have responded to the academic attention the series has received. For example, Jes Battis, who authored \"\", admits that study of the Buffyverse \"invokes an uneasy combination of enthusiasm and ire\", and meets \"a certain amount of disdain from within the halls of the academy\". Nonetheless, \"Buffy\" eventually led to the publication of around twenty books and hundreds of articles examining the themes of the show from a wide range of disciplinary perspectives, including sociology, Speech Communication, psychology, philosophy, and women's studies. In a 2012 study by \"Slate\", \"Buffy the Vampire Slayer\" was named the most studied pop culture work by academics, with more than 200 papers, essays, and books devoted to the series.\nThe Whedon Studies Association produces the online academic journal \"Slayage\" and sponsors a biennial academic conference on the works of Whedon. The sixth \"Biennial Slayage Conference\", titled \"Much Ado About Whedon\", was held at California State University-Sacramento in late June 2014.\nFandom and fan films.\nThe popularity of \"Buffy\" has led to the creation of websites, online discussion forums, works of \"Buffy\" fan fiction, and several unofficial fan-made productions. Since the end of the series, Whedon has stated that his intention was to produce a cult television series and has acknowledged the \"rabid, almost insane fan base\" that the show has created. In 2016, Jenny Owen Youngs and Kristin Russo began the \"Buffering the Vampire Slayer\" podcast, recognized as one of the top podcasts in production by \"Time\" and \"Esquire\" magazines. In 2017 the 20th anniversary of the show attracted even more writers to create their own adventures of the series' characters.\n\"Buffy\" in popular culture.\nThe series employed pop culture references as a frequent humorous device, and has itself become a frequent pop culture reference in video games, comics and television shows. The series has also been parodied and spoofed. Sarah Michelle Gellar has participated in several parody sketches, including a \"Saturday Night Live\" sketch in which the Slayer is relocated to the \"Seinfeld\" universe, and adding her voice to an episode of \"Robot Chicken\" that parodied a would-be eighth season of \"Buffy\".\n\"Buffy\" was the code-name used for an early HTC mobile phone which integrated the social networking website Facebook.\nIn March 2017, in honor of \"Buffy the Vampire Slayer's\" 20th anniversary, \"Entertainment Weekly\" reunited Joss Whedon and the whole cast for their first joint interview and photo shoot in over a decade.\nU.S. television ratings.\n\"Buffy\" helped put The WB on the ratings map, but by the time the series landed on UPN in 2001, viewing figures had fallen. The series' high came during the third season, with 5.3 million viewers (including repeats), possibly due to the fact that both Gellar and Hannigan had hit movies out during the season (\"Cruel Intentions\" and \"American Pie\" respectively). The series' low came in season one at 3.7 million. The series finale \"Chosen\" pulled in a season high of 4.9 million viewers on the UPN network.\nThe WB was impressed with the young audience the show was bringing in, and ordered a full season of 22 episodes for season two. \"Buffy\" was moved from Monday at 9:00 pm to launch The WB's new night of programming on Tuesday, starting with the episode \"Innocence\", watched by 8.2 million people. Due to its success in that time slot, it remained on Tuesdays at 8:00 pm for the remainder of its original run, and became one of the network's highest-rated shows.\nIn the 2001\u20132002 season, the show moved to UPN after a negotiation dispute with The WB. While it was still one of the highest rated shows on their network, The WB felt that it had peaked and thus declined a salary increase to the cast and crew. UPN then picked the series up for a two-season renewal, dedicating a two-hour premiere to help re-launch it. The following season premiere attracted the second highest rating of the series, with 7.7 million viewers.\nImpact on television.\n\"Buffy the Vampire Slayer\" became a pop culture phenomenon and is considered as iconic. Commentators of the entertainment industry including \"AllMovie\", \"The Hollywood Reporter\", \"PopMatters\", \"The Village Voice\", and \"The Washington Post\" cite \"Buffy\" as \"influential\", with some describing it as the ascent of television into its golden age. Stephanie Zacharek, in the \"Village Voice\", wrote, \"If we really are in a golden age of television, \"Buffy the Vampire Slayer\" was a harbinger.\" Robert Moore of \"PopMatters\" expressed similar sentiments, writing \"TV was not art before \"Buffy\", but it was afterwards\", suggesting that it was responsible for re-popularizing long story arcs on primetime television. The show is often seen as one of the greatest shows of all-time and for being groundbreaking and influential for the landscape of television. David Simon, creator of \"The Wire\", considered \"Buffy\" as \"the best show in years\". Stephen Daisley of \"The Spectator\" stated that \"Buffy\" was a \"generation-defining TV hit\", following this by saying that \"\"Buffy\" was steeped in literary allusions and crackled with pop culture references and became the first TV series to attract serious scholarship.\" He also wrote, \"Renowned professors wrestled with this high school set study of the human condition while linguists tried to pin down Buffy Speak, the distinctive and playful grammar which animated Whedon\u2019s dialogue. (Inventive, much?)\".\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"Buffy the Vampire Slayer\" showed the whole world, and an entire sprawling industry, that writing monsters and demons and end-of-the world is not hack-work, it can challenge the best. Joss Whedon raised the bar for every writer\u2014not just genre/niche writers, but every single one of us.\n\u2013Russell T Davies\n\"Buffy\"'s effect on programming was quickly evident. Autumn 2003 saw several new shows going into production in the U.S. that featured strong females forced to come to terms with supernatural power or destiny while trying to maintain a normal life. These post-\"Buffy\" shows include \"Dead Like Me\", \"Joan of Arcadia\", \"Tru Calling\", \"Veronica Mars\" and \"Teen Wolf\". Bryan Fuller, the creator of \"Dead Like Me\", said that \"\"Buffy\" showed that young women could be in situations that were both fantastic and relatable, and instead of shunting women off to the side, it puts them at the center.\" In the United Kingdom, the lessons learned from the impact of \"Buffy\" influenced the revived \"Doctor Who\" series (2005\u2013present), as well as its spinoff series \"Torchwood\". Reviewers noted that shows such as \"Legacies\" and \"Riverdale\" took inspiration from \"Buffy\" involving the \"Dark Willow\" story arc. Adam B. Vary of \"Variety\", by talking about the potential reboot of the show, wrote how \"\"Buffy\" presaged the next 25 years of genre-bending entertainment. \"Supernatural\", \"True Blood\", \"Alias\", \"Once Upon a Time\", \"The Vampire Diaries\", \"Veronica Mars\", \"Teen Wolf\", \"The Magicians\", \"Jessica Jones\", \"Orphan Black\", \"Chilling Adventures of Sabrina\", \"Wynonna Earp\", \"Riverdale,\" \"Wednesday\", \"Game of Thrones\" \u2014 none of these shows, and many more besides, would be what they are without \"Buffy\".\nSeveral \"Buffy\" alumni have gone on to write for or create other shows. Such endeavors include \"Tru Calling\" (Douglas Petrie, Jane Espenson and lead actress Eliza Dushku), \"Wonderfalls\" (Tim Minear), \"Point Pleasant\" (Marti Noxon), \"Jake 2.0\" (David Greenwalt), \"The Inside\" (Tim Minear), \"Smallville\" (Steven S. DeKnight), \"Once Upon a Time\" (Jane Espenson), \"Lost\" (Drew Goddard and David Fury), and \"Daredevil\" (Goddard, DeKnight, and Petrie). TV Tropes, a website devoted to pop culture tropes, claimed that \"Buffy the Vampire Slayer\" is the reason why the site exists. The show also had a significant impact on slang in popular culture. The series also served as inspiration for television writers such as Shonda Rhimes, Eric Kripke, Rob Thomas and Amy Sherman-Palladino. In 2015, \"The Atlantic\" wrote that \"Buffy\" is \"still revolutionary\" and \"subversive\". \"The Daily Orange\" wrote \"Buffy led the third-wave feminist movement in pop culture\", talking about how the series led the cause for women leads on TV.\nSeveral critics have noted series such as \"Orphan Black\", \"The Magicians\", \"Jessica Jones\" and \"Wynonna Earp\" as being worthy successors to \"Buffy\". At the 2015 San Diego Comic-Con, the authors Rachel Hawkins, Kiersten White, Rae Carson, Brittany Geragotelis and Valerie Tejeda talked about the Buffy effect on heroines in fiction and how Buffy was a big influence on writing their books.\nMeanwhile, the Parents Television Council complained of efforts to \"deluge their young viewing audiences with adult themes\". The U.S. Federal Communications Commission (FCC), however, rejected the council's indecency complaint concerning the violent sex scene between Buffy and Spike in \"Smashed\". The BBC, however, chose to censor some of the more controversial sexual content when it was shown on the pre-watershed 6:45 pm slot.\nShow characters Willow Rosenberg and Tara Maclay were one of the first lesbian couples to be shown on public broadcast television. This was important representation at the time, as it challenged many social stereotypes about gay women. It did not over-sexualize them and instead allowed them to be seen as independent people in a fairly healthy relationship. Creator Joss Whedon has said in interviews that he was initially told by the network he could not include a bisexual character in the show, however, in later seasons as cultural opinions on LGBT issues began to shift, he was allowed to introduce Willow and Tara as being in a relationship with one another. At first they were only seen talking and holding hands as they were not allowed to be shown kissing, until in 2002, the show showed the girlfriends in bed together, which though not a sex scene was considered the first scene of its kind for a broadcast network series. The following year, the show featured the first lesbian sex scene in broadcast TV history.\nAwards and nominations.\n\"Buffy\" has gathered a number of awards and nominations which include an Emmy Award nomination for the 1999 episode \"Hush,\" which featured an extended sequence with no character dialogue. The 2001 episode \"The Body\" was filmed with no musical score, only diegetic music; it was nominated for a Nebula Award in 2002. The 2001 musical episode \"Once More, with Feeling\" received plaudits, but was omitted from Emmy nomination ballots by \"accident\". It since was featured on \"Channel 4's \"100 Greatest Musicals\"\". In 2001, Sarah Michelle Gellar received a Golden Globe-nomination for Best Actress in a TV Series-Drama for her role in the show, as well nominations for the Teen Choice Awards and the Saturn Award for Best Genre TV Actress. The series won the Drama Category for Television's Most Memorable Moment at the 60th Primetime Emmy Awards for \"The Gift\" beating \"The X-Files\", \"Grey's Anatomy\", \"Brian's Song\" and \"Dallas\", although the sequence for this award was not aired.\nIt was nominated for Emmy and Golden Globe awards, winning a total of three Emmys. However, snubs in lead Emmy categories resulted in outrage among TV critics and the decision by the academy to hold a tribute event in honor of the series after it had gone off the air in 2003.\nHome media.\nBy 2004, before the release of the final season, the series earned $123.3 million in sales.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47543", "revid": "120807", "url": "https://en.wikipedia.org/wiki?curid=47543", "title": "Zero Wing", "text": "1989 video game\n is a 1989 horizontally scrolling shooter video game developed by Toaplan and published by Namco for Japanese arcades; in North America, it was distributed by Williams Electronics. Controlling the ZIG space fighter craft, players assume the role of protagonist Trent in a last-ditch effort to overthrow the alien space pirate organization CATS (Abigor in the PC-Engine version). It was the eighth shoot 'em up game from Toaplan, and their fourteenth video game overall.\nHeaded by development chief Toshiaki \u014cta, \"Zero Wing\" was created by most of the same team that previously worked on several projects at Toaplan, initially starting as a project not intended for commercial release but to train new recruits before being ultimately released to the market. The game was later ported to other platforms, each one featuring several changes or additions compared with the original version.\n\"Zero Wing\" enjoyed a degree of success in arcades and its home conversions were met with mostly positive reception from critics. The European Mega Drive version later gained renewed popularity due to the \"All your base are belong to us\" internet meme, which plays off the badly translated introductory cutscene. The rights to the title are owned by Tatsujin, a Japanese company formed by Masahiro Yuge. The Mega Drive version was later released in North America by independent publisher Retro-Bit in 2020 as well as the Nintendo Classics service.\nGameplay.\n\"Zero Wing\" is a science fiction-themed side-scrolling shooter similar to \"Hellfire\", where players assume the role of Trent taking control of the ZIG space fighter craft through eight increasingly difficult levels, each with a boss at the end that must be fought before progressing any further, in a last-ditch effort to overthrow the alien cyborg CATS as the main objective. As far as side-scrolling shooters go, the title initially appears to be very standard, as players control their craft over a constantly scrolling background and the scenery never stops moving until the stage boss is reached.\nA unique gameplay feature is the \"Seizer Beam\" system; during gameplay, players can grab certain enemies and hold them as shield against enemy fire or launch them against enemies. There are three types of weapons in the game that can be switched between after destroying incoming carriers by picking up a color-changing item ranging from the \"Red Cannon\" shot, the \"Blue Laser\" and the \"Green Homing\" missiles. Each weapon can be upgraded by picking up an item of the same color. Other items can also be grabbed along the way such as speed increasers, 1UPs and a bomb module capable of obliterating any enemy caught within its blast radius that can also be triggered after taking enemy hits.\nDepending on the settings in the arcade version, the title uses either a checkpoint system in which a downed single player will start off at the beginning of the checkpoint they managed to reach before dying, or a respawn system where their ship immediately starts at the location where they died. Getting hit by enemy fire or colliding against solid stage obstacles will result in losing a life, as well as a penalty of decreasing the ship's firepower and speed to his original state and once all lives are lost, the game is over unless the players insert more credits into the arcade machine to continue playing. The game loops back to the first stage after completing the last stage as with previous titles from Toaplan, with each one increasing the difficulty and enemies fire denser bullet patterns as well as spawning extra bullets when destroyed.\nSynopsis.\nThe backstory of \"Zero Wing\" varies between each version, but the plot within the game itself remains consistent. Set in 2101, the game follows the signing of a peace treaty between the United Nations (also translated as the Milky Way Federation) and CATS, a space pirate organization, who later breaks the covenant and takes control of the Japanese/Federation space colonies. \nThe protagonist, Trent, pilots a ZIG spacecraft that managed to escape from a mothership destroyed by a representative of CATS. His aim is to defeat enemy forces, avenge the mothership and its crew, and liberate Earth. However, in the PC Engine version, the story is different. CATS is replaced by an organization called Abigor, led by a man named Ludwig and his right-hand woman, Seiren. Abigor attacks the Galactic Federation, and the ZIG, piloted by Masato Tachibana, is sent to repel them. It is eventually revealed that the Federation dispatcher's sister, Airen, who had been working as an undercover spy to alert the Federation about Abigor's attack, is arrested by Seiren and turned over to Ludwig. After Ludwig chooses to spare Airen's life, Tachibana communicates with Ludwig, who challenges him to a battle with the condition that Airen will be returned if Tachibana wins. Tachibana accepts and destroys Ludwig's ship, after which Ludwig relinquishes Airen before committing suicide. Airen and Tachibana then celebrate their victory before returning to Earth offscreen.\nDevelopment.\n\"Zero Wing\" was created by most of the same team that previously worked on several projects at Toaplan, with members of the development staff recounting its history through various Japanese publications. Toshiaki \u014cta was at the helm as development chief and also served as programmer alongside Hiroaki Furukawa and Tatsuya Uemura. Uemura also acted as composer along with Masahiro Yuge and Toshiaki Tomizawa. Artists Miho Hayashi, Naoki Ogiwara and Shintar\u014d Nakaoka created the artwork while Sanae Nit\u014d and Yuko Tataka served as character designers.\nUemura stated that \"Zero Wing\" originally started as a project not intended for commercial launch to train new recruits at Toaplan, handling training for new hires while using his work and engine from \"Hellfire\" before ultimately deciding with releasing the game to the market, which made it a more practical learning experience for the new developers. Uemura, however, felt that both stage design and characters were \"cobbled together\", leading the game's world being \"kind of a mess\" and he also stated the project turned into a \"battle royale\", as staff from both \"Hellfire\" and \"Truxton\" were mixed with the new recruits. Sound also proved to be very divisive as Uemura, Yuge and Tomizawa wrote several songs for the game with their own individual styles, though Uemura claimed this was due to dividing the work, while Yuge stated he would go to rest and drink after being stuck when composing for the title during work hours. Due to being a training project, Uemura stated that the team had freedom to \"just fool around\" and several features were integrated into the title such as warps, which was taken from \"Slap Fight\". Uemura also revealed that the reason for enemies spawning suicide bullets during loops of higher difficulty was in response to hardware limitations regarding sprites. Both the single-player and co-op versions were also planned from the beginning of development due to pressure to make two-player games at the time. The alien Pipiru was designed by Ogiwara, though Uemura claimed such character was not planned.\nThe Mega Drive version of \"Zero Wing\" was created by the same staff responsible for the arcade version, with Uemura overseeing development in-house. Both Uemura and Tataka have stated that working with the Mega Drive proved to be difficult due to several restrictions imposed by the hardware. According to both Uemura and Yuge, the poor English translation of the Mega Drive version was handled by a member of Toaplan in charge of export and overseas business whose English was \"really terrible\". The Mega Drive port features arranged music by Noriyuki Iwadare. The PC Engine CD-ROM\u00b2 version was outsourced by an \"acquaintance\" from defunct developer Orca, with Uemura handling sound.\nTo expand the plot, the Mega Drive version added an introductory sequence to the game. This introduction does not appear in the arcade original nor in the PC Engine CD-ROM\u00b2 versions; rather, a different intro takes place with a blue-windowed ZIG. The PC Engine CD-ROM\u00b2 also added two new levels: 5th (Deeva) and 10th (Vacura).\nRelease.\n\"Zero Wing\" was first released in arcades in October 1989 by Namco in Japan, and then by Williams Electronics for North America in April 1990. In 1989, an album containing music from the title was co-published exclusively in Japan by Scitron and Pony Canyon.\n\"Zero Wing\" was first ported to the Mega Drive by Toaplan and was first published in Japan on 31 May 1991 and later in Europe by Sega in July 1991. Despite never being released in North America, this version is playable on American Sega Genesis consoles; like most early titles, it had no region protection, nor had the European release been PAL-optimized. The game was later converted to the PC Engine CD-ROM\u00b2 add-on and published exclusively in Japan by Naxat Soft on 18 September 1992.\nThe Mega Drive port was later released in North America by independent publisher Retro-Bit in 2020. \"Zero Wing\" was included as part of the \"Toaplan Arcade 1\" compilation for Evercade, released in December 2022. \"Zero Wing\" was re-released on the Nintendo Classics service in June 2022.\nBitwave Games and Toaplan jointly released \"Zero Wing\" on Steam and GOG.com in February 2023, both as a standalone title and as a game bundle with \"Twin Cobra\", \"Truxton\", and \"Out Zone\". The Steam version includes several game enhancements, such as an updated graphics, rewind option, achievements, online leaderboards, sidebar indicators, and a new Very Easy mode.\nReception.\nIn Japan, \"Game Machine\" listed \"Zero Wing\" as the fourth most successful table arcade unit of November 1989, outperforming titles such as \"Jitsuryoku!! Pro Yaky\u016b\" and \"U.N. Squadron\".\nThe game received a positive critical reception upon release. \"Computer and Video Games\" scored it 93%, including ratings of 92% for graphics, 93% for sound, 90% for playability, and 89% for lastability. They praised \"the great intro sequence\", \"super-smooth gameplay, beautifully defined graphics, rocking sound track, amazing explosions and incredible end-of-level bosses\", concluding that it is \"the game which breaths (\"sic\") new life into shoot 'em ups on the Megadrive\". \"Mean Machines\" scored it 91%, including ratings of 92% for presentation and graphics, 88% for sound, 90% for playability, and 89% for lastability. They praised the \"ace\" opening sequence, \"detailed\" graphics, \"real good\" music, and skill-based gameplay, and called it one of the best games for Mega Drive. \"Sega Force\" scored it 86%, including ratings of 84% for presentation, 89% for visuals, 83% for sound, 89% for playability, and 82% for lastability. They praised the \"animated intro\" sequence, \"smart\" graphics, \"up-beat Jap tune\" music, and the accessible but \"highly involved\" gameplay, concluding that it is \"almost as good as \"Hellfire\"\" but \"not quite\".\n\"Complex\" ranked \"Zero Wing\" 98th on their list of \"The 100 Best Sega Genesis Games\".\nLegacy.\n\"Zero Wing\" was the last side-scrolling shoot 'em up title to be developed by Toaplan, as the developers believed they did not know how to make a side-scrolling shooter interesting, despite positive reception from players. \n\"Zero Wing\" is one of the most widely-known examples of poor linguistic translation in video games. Translations were handled internally by the design team, not a professional translator. In 1999, the introduction cutscene for the Sega Mega Drive version of \"Zero Wing\" was re-discovered, culminating in the wildly popular \"All your base are belong to us\" Internet meme. \n\"Zero Wing\" is also of interest in the field of translation studies in the context of its multiple endings. As noted by Clyde Mandelin, who reverse engineered the original game, while the English-language version had three different post-credit endings, the Japanese version had thirty-five. Many of those endings referenced then-current Japanese popular culture in ways that would have been hard to translate across cultures, and suggests that the text was written by someone who grew up in the 1960s or 1970s.\nIn more recent years, the rights to \"Zero Wing\" and many other IPs from Toaplan are now owned by Tatsujin, a company named after \"Truxton\"'s Japanese title that was founded in 2017 by former Toaplan employee Masahiro Yuge, and is part of Embracer Group since 2022.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47544", "revid": "41840956", "url": "https://en.wikipedia.org/wiki?curid=47544", "title": "Carrying capacity", "text": "Maximum population size of a species that an ecosystem can support\nThe carrying capacity of an ecosystem is the maximum population size of a biological species that can be sustained by that specific environment, given the food, habitat, water, and other resources available. The carrying capacity is defined as the environment's maximal load, which in population ecology corresponds to the population equilibrium, when the number of deaths in a population equals the number of births (as well as immigration and emigration). Carrying capacity of the environment implies that the resources extraction is not above the rate of regeneration of the resources and the wastes generated are within the assimilating capacity of the environment. The effect of carrying capacity on population dynamics is modelled with a logistic function. Carrying capacity is applied to the maximum population an environment can support in ecology, agriculture and fisheries. The term carrying capacity had been applied to a few different processes in the past before finally being applied to human population limits in the 1950s. The notion of carrying capacity for humans is covered by the notion of sustainable population.\nAn early detailed examination of global limits on human population was published in the 1972 book \"Limits to Growth\", which has prompted follow-up commentary and analysis, including much criticism. A 2012 review in the journal \"Nature\" by 22 international researchers expressed concerns that the Earth may be \"approaching a state shift\" in which the biosphere may become less hospitable to human life, and in which the human carrying capacity may diminish. This concern that humanity may be passing beyond \"tipping points\" for safe use of the biosphere has increased in subsequent years. Although the global population has now passed 8 billion, recent estimates of Earth's carrying capacity run from two to four billion people, depending on how optimistic researchers are about the prospects for international cooperation to solve problems requiring collective action.\nOrigins.\nIn terms of population dynamics, the term 'carrying capacity' was not explicitly used in 1838 by the Belgian mathematician Pierre Fran\u00e7ois Verhulst when he first published his equations based on research on modelling population growth.\nThe origins of the term \"carrying capacity\" are uncertain, with sources variously stating that it was originally used \"in the context of international shipping\" in the 1840s, or that it was first used during 19th-century laboratory experiments with micro-organisms. A 2008 review finds the first use of the term in English was an 1845 report by the US Secretary of State to the US Senate. It then became a term used generally in biology in the 1870s, being most developed in wildlife and livestock management in the early 1900s. It had become a staple term in ecology used to define the biological limits of a natural system related to population size in the 1950s.\nNeo-Malthusians and eugenicists popularised the use of the words to describe the number of people the Earth can support in the 1950s, although American biostatisticians Raymond Pearl and Lowell Reed had already applied it in these terms to human populations in the 1920s.\nHadwen and Palmer (1923) defined carrying capacity as the density of stock that could be grazed for a definite period without damage to the range.\nIt was first used in the context of wildlife management by the American Aldo Leopold in 1933, and a year later by the American Paul Lester Errington, a wetlands specialist. They used the term in different ways, Leopold largely in the sense of grazing animals (differentiating between a 'saturation level', an intrinsic level of density a species would live in, and carrying capacity, the most animals which could be in the field) and Errington defining 'carrying capacity' as the number of animals above which predation would become 'heavy' (this definition has largely been rejected, including by Errington himself). The important and popular 1953 textbook on ecology by Eugene Odum, \"Fundamentals of Ecology\", popularised the term in its modern meaning as the equilibrium value of the logistic model of population growth.\nMathematics.\nThe specific reason why a population stops growing is known as a limiting or regulating factor.\nThe difference between the birth rate and the death rate is the natural increase. If the population of a given organism is below the carrying capacity of a given environment, this environment could support a positive natural increase; should it find itself above that threshold the population typically decreases. Thus, the carrying capacity is the maximum number of individuals of a species that an environment can support in long run.\nPopulation size decreases above carrying capacity due to a range of factors depending on the species concerned, but can include insufficient space, food supply, or sunlight. The carrying capacity of an environment varies for different species.\nIn the standard ecological algebra as illustrated in the simplified of population dynamics, carrying capacity is represented by the constant formula_1:\n formula_2\nwhere\nThus, the equation relates the growth rate of the population N to the current population size, incorporating the effect of the two constant parameters r and K. (Note that decrease is negative growth.) The choice of the letter K came from the German \"Kapazit\u00e4tsgrenze\" (capacity limit).\nThe is a first-order ordinary differential equation. Combined with an initial value formula_3 for the population at time formula_4, the solution takes the form of the logistic growth curve:\n formula_5\nwhere\nThe logistic growth curve depicts how population growth rate and carrying capacity are inter-connected. As illustrated in the logistic growth curve model, when the population size is small, the population increases exponentially. However, as population size nears carrying capacity, the growth decreases and reaches zero at K.\nWhat determines a specific system's carrying capacity involves a limiting factor; this may be available supplies of food or water, nesting areas, space, or the amount of waste that can be absorbed without degrading the environment and decreasing carrying capacity.\nPopulation ecology.\nCarrying capacity is a commonly used concept for biologists when trying to better understand biological populations and the factors which affect them. When addressing biological populations, carrying capacity can be seen as a stable dynamic equilibrium, taking into account extinction and colonization rates. In population biology, logistic growth assumes that population size fluctuates above and below an equilibrium value.\nNumerous authors have questioned the usefulness of the term when applied to actual wild populations. Although useful in theory and in laboratory experiments, carrying capacity as a method of measuring population limits in the environment is less useful as it sometimes oversimplifies the interactions between species.\nAgriculture.\nIt is important for farmers to calculate the carrying capacity of their land so they can establish a sustainable stocking rate. For example, calculating the carrying capacity of a paddock in Australia is done in Dry Sheep Equivalents (DSEs). A single DSE is 50\u00a0kg Merino wether, dry ewe or non-pregnant ewe, which is maintained in a stable condition. Not only sheep are calculated in DSEs, the carrying capacity for other livestock is also calculated using this measure. A 200\u00a0kg weaned calf of a British style breed gaining 0.25\u00a0kg/day is 5.5DSE, but if the same weight of the same type of calf were gaining 0.75\u00a0kg/day, it would be measured at 8DSE. Cattle are not all the same, their DSEs can vary depending on breed, growth rates, weights, if it is a cow ('dam'), steer or ox ('bullock' in Australia), and if it weaning, pregnant or 'wet' (i.e. lactating).\nIn other parts of the world different units are used for calculating carrying capacities. In the United Kingdom the paddock is measured in LU, livestock units, although different schemes exist for this. New Zealand uses either LU, EE (ewe equivalents) or SU (stock units). In the US and Canada the traditional system uses animal units (AU). A French/Swiss unit is \"Unit\u00e9 de Gros B\u00e9tail\" (UGB).\nIn some European countries such as Switzerland the pasture (\"alm\" or \"alp\") is traditionally measured in \"Sto\u00df\", with one \"Sto\u00df\" equaling four \"F\u00fc\u00dfe\" (feet). A more modern European system is (GV or GVE), corresponding to 500\u00a0kg in liveweight of cattle. In extensive agriculture 2 GV/ha is a common stocking rate, in intensive agriculture, when grazing is supplemented with extra fodder, rates can be 5 to 10 GV/ha. In Europe average stocking rates vary depending on the country, in 2000 the Netherlands and Belgium had a very high rate of 3.82 GV/ha and 3.19 GV/ha respectively, surrounding countries have rates of around 1 to 1.5 GV/ha, and more southern European countries have lower rates, with Spain having the lowest rate of 0.44 GV/ha.\nThis system can also be applied to natural areas. Grazing megaherbivores at roughly 1 GV/ha is considered sustainable in central European grasslands, although this varies widely depending on many factors. In ecology it is theoretically (i.e. cyclic succession, patch dynamics, \"Megaherbivorenhypothese\") taken that a grazing pressure of 0.3 GV/ha by wildlife is enough to hinder afforestation in a natural area. Because different species have different ecological niches, with horses for example grazing short grass, cattle longer grass, and goats or deer preferring to browse shrubs, niche differentiation allows a terrain to have slightly higher carrying capacity for a mixed group of species, than it would if there were only one species involved.\nSome niche market schemes mandate lower stocking rates than can maximally be grazed on a pasture. In order to market ones' meat products as 'biodynamic', a lower \"Gro\u00dfvieheinheit\" of 1 to 1.5 (2.0) GV/ha is mandated, with some farms having an operating structure using only 0.5 to 0.8 GV/ha.\nThe Food and Agriculture Organization has introduced three international units to measure carrying capacity: FAO Livestock Units for North America, FAO Livestock Units for sub-Saharan Africa, and Tropical Livestock Units.\nAnother rougher and less precise method of determining the carrying capacity of a paddock is simply by looking objectively at the condition of the herd. In Australia, the national standardized system for rating livestock conditions is done by body condition scoring (BCS). An animal in a very poor condition is scored with a BCS of 0, and an animal which is extremely healthy is scored at 5: animals may be scored between these two numbers in increments of 0.25. At least 25 animals of the same type must be scored to provide a statistically representative number, and scoring must take place monthly -if the average falls, this may be due to a stocking rate above the paddock's carrying capacity or too little fodder. This method is less direct for determining stocking rates than looking at the pasture itself, because the changes in the condition of the stock may lag behind changes in the condition of the pasture.\nFisheries.\nIn fisheries, carrying capacity is used in the formulae to calculate sustainable yields for fisheries management. The maximum sustainable yield (MSY) is defined as \"the highest average catch that can be continuously taken from an exploited population (=stock) under average environmental conditions\". MSY was originally calculated as half of the carrying capacity, but has been refined over the years, now being seen as roughly 30% of the population, depending on the species or population. Because the population of a species which is brought below its carrying capacity due to fishing will find itself in the exponential phase of growth, as seen in the Verhulst model, the harvesting of an amount of fish at or below MSY is a surplus yield which can be sustainably harvested without reducing population size at equilibrium, keeping the population at its maximum recruitment. However, annual fishing can be seen as a modification of \"r\" in the equation -i.e. the environment has been modified, which means that the population size at equilibrium with annual fishing is slightly below what \"K\" would be without it.\nNote that mathematically and in practical terms, MSY is problematic. If mistakes are made and even a tiny amount of fish are harvested each year above the MSY, populations dynamics imply that the total population will eventually decrease to zero. The actual carrying capacity of the environment may fluctuate in the real world, which means that practically, MSY may actually vary from year to year (annual sustainable yields and maximum average yield attempt to take this into account). Other similar concepts are optimum sustainable yield and maximum economic yield; these are both harvest rates below MSY.\nThese calculations are used to determine fishing quotas.\nHumans.\nHuman carrying capacity is a function of how people live and the technology at their disposal. The two great economic revolutions that marked human history up to 1900\u2014the agricultural and industrial revolutions\u2014greatly increased the Earth's human carrying capacity, allowing human population to grow from 5 to 10 million people in 10,000 BCE to 1.5 billion in 1900. The immense technological improvements of the past 100 years\u2014in applied chemistry, physics, computing, genetic engineering, and more\u2014have further increased Earth's human carrying capacity, at least in the short term. Without the Haber-Bosch process for fixing nitrogen, modern agriculture could not support 8 billion people. Without the Green Revolution of the 1950s and 60s, famine might have culled large numbers of people in poorer countries during the last three decades of the twentieth century.\nRecent technological successes, however, have come at grave environmental costs. Climate change, ocean acidification, and the huge dead zones at the mouths of many of world's great rivers, are a function of the scale of contemporary agriculture and the many other demands 8 billion people make on the planet. Scientists now speak of humanity exceeding or threatening to exceed 9 planetary boundaries for safe use of the biosphere. Humanity's unprecedented ecological impacts threaten to degrade the ecosystem services that people and the rest of life depend on\u2014potentially decreasing Earth's human carrying capacity. The signs that we have crossed this threshold are increasing.\nThe fact that degrading Earth's essential services is obviously possible, and happening in some cases, suggests that 8 billion people may be above Earth's human carrying capacity. But human carrying capacity is always a function of a certain number of people living a certain way. This was encapsulated by Paul Ehrlich and James Holdren's (1972) IPAT equation: environmental impact (I) = population (P) x affluence (A) x the technologies used to accommodate human demands (T). IPAT has found spectacular confirmation in recent decades within climate science, where the Kaya identity for explaining changes in CO2 emissions is essentially IPAT with two technology factors broken out for ease of use.\nThis suggests to technological optimists that new technological discoveries (or the deployment of existing ones) could continue to increase Earth's human carrying capacity, as it has in the past. Yet technology has unexpected side effects, as we have seen with stratospheric ozone depletion, excessive nitrogen deposition in the world's rivers and bays, and global climate change. This suggests that 8 billion people may be sustainable for a few generations, but not over the long term, and the term 'carrying capacity' implies a population that is sustainable indefinitely. It is possible, too, that efforts to anticipate and manage the impacts of powerful new technologies, or to divide up the efforts needed to keep global ecological impacts within sustainable bounds among more than 200 nations all pursuing their own self-interest, may prove too complicated to achieve over the long haul.\nOne issue with applying carrying capacity to any species is that ecosystems are not constant and change over time, therefore changing the resources available. Research has shown that sometimes the presence of human populations can increase local biodiversity, demonstrating that human habitation does not always lead to deforestation and decreased biodiversity.\nAnother issue to consider when applying carrying capacity, especially to humans, is that measuring food resources is arbitrary. This is due to choosing what to consider (e.g., whether or not to include plants that are not available every year), how to classify what is considered (e.g., classifying edible plants that are not usually eaten as food resources or not), and determining if caloric values or nutritional values are privileged. Additional layers to this for humans are their cultural differences in culinary preferences (e.g., some consume flying termites) and individual choices on what to invest their labor into (e.g., fishing vs. farming), both of which vary over time. This leads to the need to determine whether or not to include all food resources or only those the population considered will actually consume.\nMeasuring water resources carrying capacity is likewise highly arbitrary: choices in temporal resolution (e.g., monthly versus annual), spatial resolution (e.g., how subbasins are delineated), and whether to include human activities such as inter-basin water transfers and reservoir storage in the assessment can all lead to vastly different WRCC results.\nCarrying capacity measurements over large areas also assumes homogeneity in the resources available but this does not account for how resources and access to them can greatly vary within regions and populations. They also assume that the populations in the region only rely on that region's resources even though humans exchange resources with others from other regions and there are few, if any, isolated populations. Variations in standards of living which directly impact resource consumption are also not taken into account. These issues show that while there are limits to resources, a more complex model of how humans interact with their ecosystem needs to be used to understand them.\nRecent warnings that humanity may have exceeded Earth's carrying capacity.\nBetween 1900 and 2020, Earth's human population increased from 1.6 billion to 7.8 billion (a 390% increase). These successes greatly increased human resource demands, generating significant environmental degradation.\nMillennium ecosystem assessment.\nThe Millennium Ecosystem Assessment (MEA) of 2005 was a massive, collaborative effort to assess the state of Earth's ecosystems, involving more than 1,300 experts worldwide. Their first two of four main findings were the following. The first finding is:Over the past 50 years, humans have changed ecosystems more rapidly and extensively than in any comparable period of time in human history, largely to meet rapidly growing demands for food, fresh water, timber, fiber, and fuel. This has resulted in a substantial and largely irreversible loss in the diversity of life on Earth.The second of the four main findings is:The changes that have been made to ecosystems have contributed to substantial net gains in human well-being and economic development, but these gains have been achieved at growing costs in the form of the degradation of many ecosystem services, increased risks of nonlinear changes, and the exacerbation of poverty for some groups of people. These problems, unless addressed, will substantially diminish the benefits that future generations obtain from ecosystems.According to the MEA, these unprecedented environmental changes threaten to reduce the Earth's long-term human carrying capacity. \"The degradation of ecosystem services could grow significantly worse during the first half of this [21st] century,\" they write, serving as a barrier to improving the lives of poor people around the world.\nEcological footprint accounting.\nEcological footprint accounting measures the demands people make on nature and compares them to available supplies, for both individual countries and the world as a whole. Developed originally by Mathis Wackernagel and William Rees, it has been refined and applied in a variety of contexts over the years by Global Footprint Network (GFN). On the demand side, the ecological footprint measures how fast a population uses resources and generates wastes, with a focus on five main areas: carbon emissions (or carbon footprint); land devoted to direct settlement; timber and paper use; food and fiber use; and seafood consumption. It converts these into per capita or total hectares used. On the supply side, national or global biocapacity represents the productivity of ecological assets in a particular nation or the world as a whole; this includes \"cropland, grazing land, forest land, fishing grounds, and built-up land.\" Again the various metrics to capture biocapacity are translated into the single term of hectares of available land. As Global Footprint Network (GFN) states:Each city, state or nation's Ecological Footprint can be compared to its biocapacity, or that of the world. If a population's Ecological Footprint exceeds the region's biocapacity, that region runs a biocapacity deficit. Its demand for the goods and services that its land and seas can provide\u2014fruits and vegetables, meat, fish, wood, cotton for clothing, and carbon dioxide absorption\u2014exceeds what the region's ecosystems can regenerate. In more popular communications, this is called \"an ecological deficit.\" A region in ecological deficit meets demand by importing, liquidating its own ecological assets (such as overfishing), and/or emitting carbon dioxide into the atmosphere. If a region's biocapacity exceeds its Ecological Footprint, it has a biocapacity reserve.According to the GFN's calculations, humanity has been using resources and generating wastes in excess of sustainability since approximately 1970: currently humanity use Earth's resources at approximately 170% of capacity. This implies that humanity is well over Earth's human carrying capacity for our current levels of affluence and technology use. According to Global Footprint Network:In 2024, [Earth Overshoot Day] fell on August 1. Earth Overshoot Day marks the date when humanity has exhausted nature's budget for the year. For the rest of the year, we are maintaining our ecological deficit by drawing down local resource stocks and accumulating carbon dioxide in the atmosphere. We are operating in overshoot.The concept of 'ecological overshoot' can be seen as equivalent to exceeding human carrying capacity. According to the most recent calculations from Global Footprint Network, most of the world's residents live in countries in ecological overshoot (see the map on the right). \nThis includes countries with dense populations (such as China, India, and the Philippines), countries with high per capita consumption and resource use (France, Germany, and Saudi Arabia), and countries with both high per capita consumption and large numbers of people (Japan, the United Kingdom, and the United States).\nPlanetary boundaries framework.\nAccording to its developers, the planetary boundaries framework defines \"a safe operating space for humanity based on the intrinsic biophysical processes that regulate the stability of the Earth system.\" Human civilization has evolved in the relative stability of the Holocene epoch; thus crossing planetary boundaries for safe levels of atmospheric carbon, ocean acidity, or one of the other stated boundaries could send the global ecosystem spiraling into novel conditions that are less hospitable to life\u2014possibly reducing global human carrying capacity.\nThis framework, developed in an article published in 2009 in \"Nature\" and then updated in two articles published in 2015 in \"Science\" and in 2018 in \"PNAS\", \u00a0identifies nine stressors of planetary support systems that need to stay within critical limits to preserve stable and safe biospheric conditions (see figure below). Climate change and biodiversity loss are seen as especially crucial, since on their own, they could push the Earth system out of the Holocene state: \"transitions between time periods in Earth history have often been delineated by substantial shifts in climate, the biosphere, or both.\" \nThe scientific consensus is that humanity has exceeded three to five of the nine planetary boundaries for safe use of the biosphere and is pressing hard on several more. By itself, crossing one of the planetary boundaries does not prove humanity has exceeded Earth's human carrying capacity; perhaps technological improvements or clever management might reduce this stressor and bring us back within the biosphere's safe operating space. But when several boundaries are crossed, it becomes harder to argue that carrying capacity has not been breached.\nBecause fewer people helps reduce all nine planetary stressors, the more boundaries are crossed, the clearer it appears that reducing human numbers is part of what is needed to get back within a safe operating space. Population growth regularly tops the list of causes of humanity's increasing impact on the natural environment in Earth system science literature. Recently, planetary boundaries developer Will Steffen and co-authors ranked global population change as the leading indicator of the influence of socio-economic trends on the functioning of the Earth system in the modern era, post-1750.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "47546", "revid": "50052465", "url": "https://en.wikipedia.org/wiki?curid=47546", "title": "Vardar Macedonia", "text": "Former territory in the Balkans\nVardar Macedonia (Macedonian and ) is a historical term referring to the northern part of the broader Macedonian region, roughly corresponding to present-day North Macedonia. The name derives from the Vardar River and is primarily associated with the period of Serbian (1912\u20131918) and later Yugoslav rule (1918\u20131991).\nHistory.\nVardar Macedonia refers to the northern part of the broader Macedonian region, which became part of the Kingdom of Serbia following the Balkan Wars (1912\u20131913) and was formally assigned to Serbia by the Treaty of Bucharest. It was named after the Vardar River, distinguishing it from Aegean Macedonia in Greece and Pirin Macedonia in Bulgaria.\nThe region was initially known as Serbian Macedonia although the use of the name \"Macedonia\" was prohibited later in the Kingdom of Yugoslavia, due to the implemented policy of Serbianisation of the local Slavic-speakers. From 1919 to 1922, the area (including parts of today Kosovo and Eastern Serbia) was part of South Serbia (, \"Ju\u017ena Srbija\"), \nIn 1929, the Kingdom of Yugoslavia was divided into provinces called \"banovinas\". Vardar Macedonia as part of South Serbia then became part of Vardar Banovina.\nDuring World War I it was occupied by Bulgaria as part of the Military Inspection Area of Macedonia. After the war the present-day Strumica and Novo Selo municipalities were broken away from Bulgaria and ceded to Yugoslavia. During the Second World War, Bulgaria established two administrative districts in the region \u2013 Bitola and Skopje. In August 1944 the Democratic Federal Macedonia was proclaimed with Vardar Macedonia as part of it. In 1945, it became one of the six constituent countries of SFR Yugoslavia and later was renamed in the People's Republic of Macedonia (1946\u20131963), and finally to Socialist Republic of Macedonia (1963\u20131991). Before the independence of the Republic of Macedonia, the region was also called Yugoslav Macedonia.\nAfter the breakup of Yugoslavia, besides North Macedonia, the region encompasses also Trgovi\u0161te and Pre\u0161evo municipalities in Central Serbia, as well the Elez Han municipality in Kosovo.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
