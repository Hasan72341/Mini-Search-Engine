{"id": "44060", "revid": "11055690", "url": "https://en.wikipedia.org/wiki?curid=44060", "title": "Barents Sea", "text": "Marginal sea of the Arctic Ocean, off the northern coasts of Norway and Russia\nThe Barents Sea ( , ; , ; ) is a marginal sea of the Arctic Ocean, located off the northern coasts of Norway and Russia and divided between Norwegian and Russian territorial waters. It was known earlier among Russians as the Northern Sea, Pomorsky Sea or Murman Sea (\"Norse Sea\"); the current name of the sea is after the historical Dutch navigator Willem Barentsz.\nThe Barents Sea is a rather shallow shelf sea with an average depth of , and it is an important site for both fishing and hydrocarbon exploration. It is bordered by the Kola Peninsula to the south, the shelf edge towards the Norwegian Sea to the west, the archipelagos of Svalbard to the northwest, Franz Josef Land to the northeast and Novaya Zemlya to the east. The islands of Novaya Zemlya, an extension of the northern end of the Ural Mountains, separate the Barents Sea from the Kara Sea.\nAlthough part of the Arctic Ocean, the Barents Sea has been characterised as \"turning into the Atlantic\" or in the process of being \"Atlantified\" because of its status as \"the Arctic warming hot spot.\" Hydrologic changes due to global warming have led to a reduction in sea ice and in the stratification of the water column, which could produce major changes in weather in Eurasia. One prediction is that, as the Barents Sea's permanent ice-free area grows, evaporation will increase, leading to increased winter snowfalls in much of continental Europe.\nGeography.\nThe southern half of the Barents Sea, including the ports of Murmansk (Russia) and Vard\u00f8 (Norway) remain ice-free year-round due to the warm North Atlantic drift. In September, the entire Barents Sea is more or less completely ice-free. From 1920 to 1944, Finland's territory also reached the Barents Sea. The Liinakhamari harbour in the Pechengsky District was Finland's only ice-free winter harbour until 1944 when it was ceded to the Soviet Union.\nThere are three main types of water masses in the Barents Sea: Warm, salty Atlantic water (temperature &gt;3\u00a0\u00b0C, salinity &gt;35) from the North Atlantic drift; cold Arctic water (temperature &lt;0\u00a0\u00b0C, salinity &lt;35) from the north; and warm, but not very salty, coastal water (temperature &gt;3\u00a0\u00b0C, salinity &lt;34.7). Between the Atlantic and Polar waters, a front called the Polar Front is formed. In the western parts of the sea (close to Bear Island), this front is determined by the bottom topography and is therefore relatively sharp and stable from year to year, while in the east (towards Novaya Zemlya), it can be quite diffuse and its position can vary markedly between years.\nThe lands of Novaya Zemlya attained most of their early Holocene coastal deglaciation approximately 10,000 years before the present.\nExtent.\nThe International Hydrographic Organization defines the limits of the \"Barentsz Sea\" [\"sic\"] as follows:\n\"On the west\": The northeastern limit of the Norwegian Sea [A line joining the southernmost point of West Spitzbergen [\"sic\"] to North Cape of Bear Island, through this island to Cape Bull and thence on to North Cape in Norway (25\u00b045'E)].\n\"On the northwest\": The eastern shore of West Spitzbergen [\"sic\"], Hinlopen Strait up to 80\u00b0 latitude north; south and east coasts of North-East Land [the island of Nordaustlandet] to Cape Leigh Smith ().\n\"On the north\": Cape Leigh Smith across the Islands Bolshoy Ostrov (Great Island) [Stor\u00f8ya], Gilles [Kvit\u00f8ya] and Victoria; Cape Mary Harmsworth (southwestern extremity of Alexandra Land) along the northern coasts of Franz-Josef Land as far as Cape Kohlsaat ().\n\"On the east\": Cape Kohlsaat to Cape Zhelaniya (Desire); west and southwest coast of Novaya Zemlya to Cape Kussov Noss and thence to western entrance Cape, Dolgaya Bay () on Vaigach Island. Through Vaigach Island to Cape Greben; thence to Cape Belyi Noss on the mainland.\n\"On the south\": The northern limit of the White Sea [A line joining Svyatoi Nos (Murmansk Coast, 39\u00b047'E) and Cape Kanin].\nOther islands in the Barents Sea include Chaichy and Timanets.\nGeology.\nThe Barents Sea was originally formed from two major continental collisions: the Caledonian orogeny, in which the Baltica and Laurentia collided to form Laurasia, and a subsequent collision between Laurasia and Western Siberia. Most of its geological history is dominated by extensional tectonics, caused by the collapse of the Caledonian and Uralian orogenic belts and the break-up of Pangaea. These events created the major rift basins that dominate the Barents Shelf, along with various platforms and structural highs. The later geological history of the Barents Sea is dominated by Late Cenozoic uplift, particularly that caused by Quaternary glaciation, which has resulted in erosion and deposition of significant sediment.\nEcology.\nDue to the North Atlantic drift, the Barents Sea has a high biological production compared to other oceans of similar latitude. The spring bloom of phytoplankton can start quite early near the ice edge because the fresh water from the melting ice makes up a stable water layer on top of the seawater. The phytoplankton bloom feeds zooplankton such as \"Calanus finmarchicus\", \"Calanus glacialis\", \"Calanus hyperboreus\", \"Oithona\" spp., and krill. The zooplankton feeders include young cod, capelin, polar cod, whales, and little auk. The capelin is a key food for top predators such as the north-east Arctic cod, harp seals, and seabirds such as the common guillemot and Brunnich's guillemot. The fisheries of the Barents Sea, in particular the cod fisheries, are of great importance for both Norway and Russia.\nSIZEX-89 was an international winter experiment in 1989 for which the main objectives were to perform sensor signature studies of different ice types to develop SAR algorithms for ice variables, such as ice types, ice concentrations and ice kinematics. Although previous research suggested that predation by whales may be the cause of depleting fish stocks, more recent research suggests that marine mammal consumption has only a trivial influence on fisheries. A model assessing the effects of fisheries and climate was far more accurate at describing trends in fish abundance. There is a genetically distinct polar bear population associated with the Barents Sea.\nPollution.\nThe Barents Sea is \"among the most polluted places on Earth\" due to accumulated marine garbage, decades of Soviet nuclear tests, radioactive waste dumping and industrial pollution. The elevated pollution has caused elevated rates of disease among locals. With rising military buildup and increased use of shipping lanes heading east through the Arctic, there are concerns that a further increase in pollution is likely, not least from the increased risk of future oil spills from ships not properly equipped for the environment.\nHistory.\nName.\nThe Barents Sea was formerly known to Russians as Murmanskoye More, or the \"Sea of Murmans\" (i.e., their term for Norwegians). It appears with this name in sixteenth-century maps, including Gerard Mercator's \"Map of the Arctic\" published in his 1595 atlas. Its eastern corner, in the region of the Pechora River's estuary, has been known as Pechorskoye Morye, that is, Pechora Sea. It was also known as Pomorsky Morye, after the first inhabitants of its shores, the Pomors.\nThis sea was given its present name by Europeans in honour of Willem Barentsz, a Dutch navigator and explorer. Barentsz was the leader of early expeditions to the far north, at the end of the sixteenth century.\nThe Barents Sea has been called by sailors \"\"The Devil's Dance Floor\" due to its unpredictability and difficulty level.\nOcean rowers call it \"Devil's Jaw\"\". In 2017, after the first recorded complete man-powered crossing of the Barents Sea from Troms\u00f8 to Longyearbyen in a rowboat by the Polar Row expedition, captain Fiann Paul was asked by Norwegian TV2 how a rower would name the Barents Sea. Fiann responded that he would name it \"Devil's Jaw\", adding that the winds you constantly battle are like breath from the devil's nostrils while he holds you in his jaws.\nModern era.\nSeabed mapping was completed in 1933; the first full map was produced by Russian marine geologist Maria Klenova.\nThe Barents Sea was the site of a notable World War II engagement which later became known as the Battle of the Barents Sea. Under the command of Oskar Kummetz, German warships sank minelayer HMS \"Bramble\" and destroyer but lost destroyer . Also, the German cruiser was severely damaged by British gunfire. The Germans later retreated and the British convoy arrived safely at Murmansk shortly afterwards.\nDuring the Cold War, the Soviet Red Banner Northern Fleet used the southern reaches of the sea as a ballistic missile submarine bastion, a strategy that Russia continued. Nuclear contamination from dumped Russian naval reactors is an environmental concern in the Barents Sea.\nEconomy.\nPolitical status.\nFor decades there was a boundary dispute between Norway and Russia regarding the position of the boundary between their respective claims to the Barents Sea. The Norwegians favoured a median line, based on the Geneva Convention of 1958, whereas the Russians favoured a meridian- based sector line, based on a Soviet decision of 1926. A neutral \"grey\" zone between the competing claims had an area of , which is approximately 12% of the total area of the Barents Sea. The two countries started negotiations on the location of the boundary in 1974 and agreed to a moratorium on hydrocarbon exploration in 1976.\nTwenty years after the fall of the Soviet Union, in 2010 Norway and Russia signed an agreement that placed the boundary equidistant from their competing claims. This was ratified and went into force on 7 July 2011, opening the grey zone for hydrocarbon exploration.\nOil and gas.\nEncouraged by the success of oil exploration and production in the North Sea in the 1960s, Norway began hydrocarbon exploration in the Barents Sea in 1969. They acquired seismic reflection surveys through the following years, which were analysed to understand the location of the main sedimentary basins. NorskHydro drilled the first well in 1980, which was a dry hole, and the first discoveries were made the following year: the Alke and Askeladden gas fields. Several more discoveries were made on the Norwegian side of the Barents Sea throughout the 1980s, including the important Sn\u00f8hvit field.\nHowever, interest in the area began to wane due to a succession of dry holes, wells containing only gas (which was cheap at the time), and the prohibitive costs of developing wells in such a remote area. Interest in the area was reignited in the late 2000s after the Snovhit field was finally brought into production and two new large discoveries were made.\nThe Russians began exploration in their territory around the same time, encouraged by their success in the Timan-Pechora Basin. They drilled their first wells in the early 1980s, and some very large gas fields were discovered throughout this decade. The Shtokman field was discovered in 1988 and is classed as a giant gas field: currently the 5th-largest gas field in the world. Similar practical difficulties Barents Sea resulted in a decline in Russian exploration, aggravated by the nation's political instability of the 1990s.\nFishing.\nThe Barents Sea contains the world's largest remaining cod population, as well as important stocks of haddock and capelin. Fishing is managed jointly by Russia and Norway in the form of the Joint Norwegian\u2013Russian Fisheries Commission, established in 1976, in an attempt to keep track of how many fish are leaving the ecosystem due to fishing. The Joint Norwegian-Russian Fisheries Commission sets Total Allowable Catches (TACs) for multiple species throughout their migratory tracks. Through the Commission, Norway and Russia also exchange fishing quotas and catch statistics to ensure the TACs are not being violated.\nHowever there are problems with reporting under this system, and researchers believe that they do not have accurate data for the effects of fishing on the Barents Sea ecosystem. Cod is one of the major catches. A large portion of catches are not reported when the fishing boats land, to account for profits that are being lost to high taxes and fees. Since many fishermen do not strictly follow the TACs and rules set forth by the Commission, the amount of fish being extracted annually from the Barents Sea is underestimated.\nBarents Sea biodiversity and marine bioprospecting.\nThe Barents Sea, where temperate waters from the Gulf Stream and cold waters from the Arctic meet, is home to an enormous diversity of organisms, which are well-adapted to the extreme conditions of their marine habitats. This makes these arctic species very attractive for marine bioprospecting. Marine bioprospecting may be defined as the search for bioactive molecules and compounds from marine sources that have new, unique properties and the potential for commercial applications. Amongst others, applications include medicines, food and feed, textiles, cosmetics and the process industry.\nThe Norwegian government strategically supports the development of marine bioprospecting as it has the potential to contribute to new and sustainable wealth creation. Troms\u00f8 and the northern areas of Norway play a central role in this strategy. They have excellent access to unique Arctic marine organisms, existing marine industries, and R&amp;D competence and infrastructure in this region. Since 2007, science and industry have cooperated closely on bioprospecting and the development and commercialization of new products.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44061", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=44061", "title": "Convulsion", "text": "Medical condition where body muscles contract and relax rapidly and repeatedly\nMedical condition&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nA convulsion is a medical condition where the body muscles contract and relax rapidly and repeatedly, resulting in uncontrolled shaking. Because epileptic seizures typically include convulsions, the term \"convulsion\" is often used as a synonym for \"seizure\". However, not all epileptic seizures result in convulsions, and not all convulsions are caused by epileptic seizures. Non-epileptic convulsions have no relation with epilepsy, and are caused by non-epileptic seizures.\nConvulsions can be caused by epilepsy, infections (including a severe form of listeriosis which is caused by eating food contaminated by Listeria monocytogenes), brain trauma, or other medical conditions. They can also occur from an electric shock or improperly enriched air for scuba diving.\nThe word \"fit\" is sometimes used to mean a convulsion or epileptic seizure.\nSigns and symptoms.\nA person having a convulsion may experience several different symptoms, such as a brief blackout, confusion, drooling, loss of bowel or bladder control, sudden shaking of the entire body, uncontrollable muscle spasms, or temporary cessation of breathing. Symptoms usually last from a few seconds to several minutes, although they can last longer.\nConvulsions in children are not necessarily benign, and may lead to brain damage if prolonged. In these patients, the frequency of occurrence should not downplay their significance, as a worsening seizure state may reflect the damage caused by successive attacks. Symptoms may include:\nCauses.\nMost convulsions are the result of abnormal electrical activity in the brain. Often, a specific cause is not clear. Numerous conditions can cause a convulsion.\nConvulsions can be caused by specific chemicals in the blood, as well as infections like meningitis or encephalitis. Other possibilities include celiac disease, head trauma, stroke, or lack of oxygen to the brain. Sometimes the convulsion can be caused by genetic defects or brain tumors. Convulsions can also occur when the blood sugar is too low or there is a deficiency of vitamin B6 (pyridoxine). The pathophysiology of convulsion remains ambiguous.\nConvulsions are often caused by epileptic seizures, febrile seizures, non-epileptic seizures, or paroxysmal kinesigenic dyskinesia. In rare cases, it may be triggered by reactions to certain medications, such as antidepressants, stimulants, and antihistamines.\nEpileptic seizures.\nEpilepsy is a neuronal disorder with multifactorial manifestations. It is a noncontagious illness and is usually associated with sudden attacks of seizures, which are an immediate and initial anomaly in the electrical activity of the brain that disrupts part or all of the body. Various areas of the brain can be disturbed by epileptic events. Epileptic seizures can have contrary clinical features. Epileptic seizures can have long-lasting effects on cerebral blood flow.\nVarious kinds of epileptic seizures affect 60 million people worldwide.\nGeneralized seizures.\nThe most common type of seizure is called a generalized seizure, also known as a generalized convulsion. This is characterized by a loss of consciousness which may lead to the person collapsing. The body stiffens for about a minute and then jerks uncontrollably for the next minute. During this, the patient may fall and injure themselves or bite their tongue, may lose control of their bladder, and their eyes may roll back. A familial history of seizures puts a person at a greater risk of developing them. Generalized seizures have been broadly classified into two categories: motor and non-motor.\nA generalized tonic-clonic seizure (GTCS), also known as a grand mal seizure, is a whole-body seizure that has a tonic phase followed by clonic muscle retrenchments. GTCSs can happen in people of all ages. GTCSs are very hazardous, and they increase the risk of injuries and sudden unexpected death in epilepsy (SUDEP). SUDEP is a sudden, unexpected, nontraumatic death in patients with epilepsy. Strong convulsions that are related to GTCSs can also cause falls and severe injuries.\nNot all generalized seizures produce convulsions. For example, in an absence seizure, also known as a petit mal seizure, the brain experiences electrical disturbances but the body remains motionless and unresponsive.\nFebrile convulsion.\nA common cause of convulsions in children is febrile seizures, a type of seizure associated with a high body temperature. This high temperature is a usual immune response to infection, and in febrile convulsions, the reason for the fever is extra-cranial (such as a body-wide viral infection). In Nigeria, malaria\u2014which can cause sudden, high fevers\u2014is a significant cause of convulsions among children under 5 years of age.\nFebrile seizures fall into two categories: simple and complex. A simple febrile seizure is generalized, occurs singularly, and lasts less than 15 minutes. A complex febrile seizure can be focused in an area of the body, occur more than once, and lasts for more than 15 minutes. Febrile seizures affect 2\u20134% of children in the United States and Western Europe. It is the most common childhood seizure. The exact reason for febrile convulsion is unidentified, though it might be the outcome of the interchange between environmental and genetic factors.\nPsychogenic non-epileptic seizures.\nPsychogenic non-epileptic seizures (PNES) are described as neurobehavioral conditions or \"psychogenic illnesses\" which occur not due to the electrical disturbances in a person's brain but due to mental and emotional stress. PNES are an important differential diagnosis and a common occurrence in epilepsy centers. According to the \"5th Edison of Diagnostic and Statistical Manual of Mental Disorders\" (DSM 5), PNES is classified as a \"conversion disorder\" or Functional Neurologic Symptom Disorder characterized by alterations in behavior, motor activity, consciousness, and sensation. A few neuroimaging (functional and structural) studies suggest that PNES may replicate sensorimotor alterations, emotional regulation, cognitive control, and integration of neural circuits.\nParoxysmal kinesigenic dyskinesia.\nThere is a linkage between infantile convulsion and paroxysmal dyskinesia. Paroxysmal kinesigenic dyskinesia (PKD) is characterized by sudden involuntary movement caused by sudden stress or excitement. The relationship between convulsion and PKD is mainly due to the common mechanism of pathophysiology.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44062", "revid": "47033785", "url": "https://en.wikipedia.org/wiki?curid=44062", "title": "X-ray astronomy", "text": "Branch of astronomy that uses X-ray observation\nX-ray astronomy is an observational branch of astronomy which deals with the study of X-ray observation and detection from astronomical objects. X-radiation is absorbed by the Earth's atmosphere, so instruments to detect X-rays must be taken to high altitude by balloons, sounding rockets, and satellites. X-ray astronomy uses a type of space telescope that can see x-ray radiation which standard optical telescopes, such as the Mauna Kea Observatories, cannot.\nX-ray emission is expected from astronomical objects that contain extremely hot gases at temperatures from about a million kelvin (K) to hundreds of millions of kelvin (MK). Moreover, the maintenance of the E-layer of ionized gas high in the Earth's thermosphere also suggested a strong extraterrestrial source of X-rays. Although theory predicted that the Sun and the stars would be prominent X-ray sources, there was no way to verify this because Earth's atmosphere blocks most extraterrestrial X-rays. It was not until ways of sending instrument packages to high altitudes were developed that these X-ray sources could be studied.\nThe existence of solar X-rays was confirmed early in the mid-twentieth century by V-2s converted to sounding rockets, and the detection of extra-terrestrial X-rays has been the primary or secondary mission of multiple satellites since 1958. The first cosmic (beyond the Solar System) X-ray source was discovered by a sounding rocket in 1962. Called Scorpius X-1 (Sco X-1) (the first X-ray source found in the constellation Scorpius), the X-ray emission of Scorpius X-1 is 10,000 times greater than its visual emission, whereas that of the Sun is about a million times less. In addition, the energy output in X-rays is 100,000 times greater than the total emission of the Sun in all wavelengths.\nMany thousands of X-ray sources have since been discovered. In addition, the intergalactic space in galaxy clusters is filled with a hot, but very dilute gas at a temperature between 100 and 1000 megakelvins (MK). The total amount of hot gas is five to ten times the total mass in the visible galaxies.\nHistory of X-ray astronomy.\nIn 1927, E.O. Hulburt of the US Naval Research Laboratory and associates Gregory Breit and Merle A. Tuve of the Carnegie Institution of Washington explored the possibility of equipping Robert H. Goddard's rockets to explore the upper atmosphere. \"Two years later, he proposed an experimental program in which a rocket might be instrumented to explore the upper atmosphere, including detection of ultraviolet radiation and X-rays at high altitudes\".\nIn the late 1930s, the presence of a very hot, tenuous gas surrounding the Sun was inferred indirectly from optical coronal lines of highly ionized species. The Sun has been known to be surrounded by a hot tenuous corona. In the mid-1940s radio observations revealed a radio corona around the Sun.\nThe beginning of the search for X-ray sources from above the Earth's atmosphere was on August 5, 1948 12:07 GMT. A US Army (formerly German) V-2 rocket as part of Project Hermes was launched from White Sands Proving Grounds. The first solar X-rays were recorded by T. Burnight.\nThrough the 1960s, 70s, 80s, and 90s, the sensitivity of detectors increased greatly during the 60 years of X-ray astronomy. In addition, the ability to focus X-rays has developed enormously\u2014allowing the production of high-quality images of many fascinating celestial objects.\nObservational platforms.\nSounding rocket flights.\nThe first sounding rocket flights for X-ray research were accomplished at the White Sands Missile Range in New Mexico with a V-2 rocket on January 28, 1949. A detector was placed in the nose cone section and the rocket was launched in a suborbital flight to an altitude just above the atmosphere. X-rays from the Sun were detected by the U.S. Naval Research Laboratory Blossom experiment on board. \nAn Aerobee 150 rocket launched on June 19, 1962 (UTC) detected the first X-rays emitted from a source outside the Solar System (Scorpius X-1). It is now known that such X-ray sources as Sco X-1 are compact stars, such as neutron stars or black holes. Material falling into a black hole may emit X-rays, but the black hole itself does not. The energy source for the X-ray emission is gravity. Infalling gas and dust is heated by the strong gravitational fields of these and other celestial objects. Based on discoveries in this new field of X-ray astronomy, starting with Scorpius X-1, Riccardo Giacconi received the Nobel Prize in Physics in 2002.\nThe largest drawback to rocket flights is their very short duration (just a few minutes above the atmosphere before the rocket falls back to Earth) and their limited field of view. A rocket launched from the United States will not be able to see sources in the southern sky; a rocket launched from Australia will not be able to see sources in the northern sky.\nX-ray Quantum Calorimeter (XQC) project.\nIn astronomy, the interstellar medium (or ISM) is the gas and cosmic dust that pervade interstellar space: the matter that exists between the star systems within a galaxy. It fills interstellar space and blends smoothly into the surrounding intergalactic medium. The interstellar medium consists of an extremely dilute (by terrestrial standards) mixture of ions, atoms, molecules, larger dust grains, cosmic rays, and (galactic) magnetic fields. The energy that occupies the same volume, in the form of electromagnetic radiation, is the interstellar radiation field.\nOf interest is the hot ionized medium (HIM) consisting of a coronal cloud ejection from star surfaces at 106-107 K which emits X-rays. The ISM is turbulent and full of structure on all spatial scales. Stars are born deep inside large complexes of molecular clouds, typically a few parsecs in size. During their lives and deaths, stars interact physically with the ISM. Stellar winds from young clusters of stars (often with giant or supergiant HII regions surrounding them) and shock waves created by supernovae inject enormous amounts of energy into their surroundings, which leads to hypersonic turbulence. The resultant structures are stellar wind bubbles and superbubbles of hot gas. The Sun is currently traveling through the Local Interstellar Cloud, a denser region in the low-density Local Bubble.\nTo measure the spectrum of the diffuse X-ray emission from the interstellar medium over the energy range 0.07 to 1 keV, NASA launched a Black Brant 9 from White Sands Missile Range, New Mexico on May 1, 2008. The Principal Investigator for the mission is Dr. Dan McCammon of the University of Wisconsin\u2013Madison.\nBalloons.\nBalloon flights can carry instruments to altitudes of up to 40\u00a0km above sea level, where they are above as much as 99.997% of the Earth's atmosphere. Unlike a rocket where data are collected during a brief few minutes, balloons are able to stay aloft for much longer. However, even at such altitudes, much of the X-ray spectrum is still absorbed. X-rays with energies less than 35 keV (5,600 aJ) cannot reach balloons. On July 21, 1964, the Crab Nebula supernova remnant was discovered to be a hard X-ray (15\u201360 keV) source by a scintillation counter flown on a balloon launched from Palestine, Texas, United States. This was likely the first balloon-based detection of X-rays from a discrete cosmic X-ray source.\nHigh-energy focusing telescope.\nThe high-energy focusing telescope (HEFT) is a balloon-borne experiment to image astrophysical sources in the hard X-ray (20\u2013100 keV) band. Its maiden flight took place in May 2005 from Fort Sumner, New Mexico, USA. The angular resolution of HEFT is c. 1.5'. Rather than using a grazing-angle X-ray telescope, HEFT makes use of a novel tungsten-silicon multilayer coatings to extend the reflectivity of nested grazing-incidence mirrors beyond 10 keV. HEFT has an energy resolution of 1.0 keV full width at half maximum at 60 keV. HEFT was launched for a 25-hour balloon flight in May 2005. The instrument performed within specification and observed Tau X-1, the Crab Nebula.\nHigh-resolution gamma-ray and hard X-ray spectrometer (HIREGS).\nA balloon-borne experiment called the High-resolution gamma-ray and hard X-ray spectrometer (HIREGS) observed X-ray and gamma-rays emissions from the Sun and other astronomical objects. It was launched from McMurdo Station, Antarctica in December 1991 and 1992. Steady winds carried the balloon on a circumpolar flight lasting about two weeks each time.\nRockoons.\nThe rockoon, a blend of \"rocket\" and \"balloon\", was a solid fuel rocket that, rather than being immediately lit while on the ground, was first carried into the upper atmosphere by a gas-filled balloon. Then, once separated from the balloon at its maximum height, the rocket was automatically ignited. This achieved a higher altitude, since the rocket did not have to move through the lower thicker air layers that would have required much more chemical fuel.\nThe original concept of \"rockoons\" was developed by Cmdr. Lee Lewis, Cmdr. G. Halvorson, S. F. Singer, and James A. Van Allen during the Aerobee rocket firing cruise of the on March 1, 1949.\nFrom July 17 to July 27, 1956, the Naval Research Laboratory (NRL) shipboard launched eight Deacon rockoons for solar ultraviolet and X-ray observations at ~30\u00b0 N ~121.6\u00b0 W, southwest of San Clemente Island, apogee: 120\u00a0km.\nSatellite observations.\nThe orbiting of satellites with solar X-ray sensors provided the opportunity for continuous, long-term observation. The first such experiments, on Vanguard 3 and Explorer 7, failed as they were under-calibrated and quickly became saturated. SOLRAD 1, launched in 1960, was the first satellite to succeed in measuring solar X-rays.\nInstruments.\nX-ray telescopes and mirrors.\nSatellites are needed because X-rays are absorbed by the Earth's atmosphere, so instruments to detect X-rays must be taken to high altitude by balloons, sounding rockets, and satellites. X-ray telescopes (XRTs) have varying directionality or imaging ability based on glancing angle reflection rather than refraction or large deviation reflection.\nThis limits them to much narrower fields of view than visible or UV telescopes. The mirrors can be made of ceramic or metal foil.\nThe first X-ray telescope in astronomy was used to observe the Sun. The first X-ray picture (taken with a grazing incidence telescope) of the Sun was taken in 1963, by a rocket-borne telescope. On April 19, 1960, the very first X-ray image of the sun was taken using a pinhole camera on an Aerobee-Hi rocket.\nThe utilization of X-ray mirrors for extrasolar X-ray astronomy simultaneously requires:\nX-ray astronomy detectors.\nX-ray astronomy detectors have been designed and configured primarily for energy and occasionally for wavelength detection using a variety of techniques usually limited to the technology of the time.\nX-ray detectors collect individual X-rays (photons of X-ray electromagnetic radiation) and count the number of photons collected (intensity), the energy (0.12 to 120 keV) of the photons collected, wavelength (c. 0.008\u20138\u00a0nm), or how fast the photons are detected (counts per hour), to tell us about the object that is emitting them.\nAstrophysical sources of X-rays.\nSeveral types of astrophysical objects emit, fluoresce, or reflect X-rays, from galaxy clusters, through black holes in active galactic nuclei (AGN) to galactic objects such as supernova remnants, stars, and binary stars containing a white dwarf (cataclysmic variable stars and super soft X-ray sources), neutron star or black hole (X-ray binaries). Some Solar System bodies emit X-rays, the most notable being the Moon, although most of the X-ray brightness of the Moon arises from reflected solar X-rays. A combination of many unresolved X-ray sources is thought to produce the observed X-ray background. The X-ray continuum can arise from bremsstrahlung, black-body radiation, synchrotron radiation, or what is called inverse Compton scattering of lower-energy photons by relativistic electrons, knock-on collisions of fast protons with atomic electrons, and atomic recombination, with or without additional electron transitions.\nAn intermediate-mass X-ray binary (IMXB) is a binary star system where one of the components is a neutron star or a black hole. The other component is an intermediate mass star.\nHercules X-1 is composed of a neutron star accreting matter from a normal star (HZ Herculis) probably due to Roche lobe overflow. X-1 is the prototype for the massive X-ray binaries although it falls on the borderline, ~2\u00a0M\u2609, between high- and low-mass X-ray binaries.\nIn July 2020, astronomers reported the observation of a \"hard tidal disruption event candidate\" associated with ASASSN-20hx, located near the nucleus of galaxy NGC 6297, and noted that the observation represented one of the \"very few tidal disruption events with hard powerlaw X-ray spectra\".\nCelestial X-ray sources.\nThe celestial sphere has been divided into 88 constellations. The International Astronomical Union (IAU) constellations are areas of the sky. Each of these contains remarkable X-ray sources. Some of them have been identified from astrophysical modeling to be galaxies or black holes at the centers of galaxies. Some are pulsars. As with sources already successfully modeled by X-ray astrophysics, striving to understand the generation of X-rays by the apparent source helps to understand the Sun, the universe as a whole, and how these affect us on Earth. Constellations are an astronomical device for handling observation and precision independent of current physical theory or interpretation. Astronomy has been around for a long time. Physical theory changes with time. With respect to celestial X-ray sources, X-ray astrophysics tends to focus on the physical reason for X-ray brightness, whereas X-ray astronomy tends to focus on their classification, order of discovery, variability, resolvability, and their relationship with nearby sources in other constellations.\nWithin the constellations Orion and Eridanus and stretching across them is a soft X-ray \"hot spot\" known as the Orion\u2013Eridanus Superbubble, the Eridanus Soft X-ray Enhancement, or simply the Eridanus Bubble, a 25\u00b0 area of interlocking arcs of H\u03b1 emitting filaments. Soft X-rays are emitted by hot gas (T ~ 2\u20133 MK) in the interior of the superbubble. This bright object forms the background for the \"shadow\" of a filament of gas and dust. The filament is shown by the overlaid contours, which represent 100 micrometre emission from dust at a temperature of about 30 K as measured by IRAS. Here the filament absorbs soft X-rays between 100 and 300 eV, indicating that the hot gas is located behind the filament. This filament may be part of a shell of neutral gas that surrounds the hot bubble. Its interior is energized by ultraviolet (UV) light and stellar winds from hot stars in the Orion OB1 association. These stars energize a superbubble about 1200 lys across which is observed in the visual (H\u03b1) and X-ray portions of the spectrum.\nExplorational X-ray astronomy.\nUsually observational astronomy is considered to occur on Earth's surface (or beneath it in neutrino astronomy). The idea of limiting observation to Earth includes orbiting the Earth. As soon as the observer leaves the cozy confines of Earth, the observer becomes a deep space explorer. Except for Explorer 1 and Explorer 3 and the earlier satellites in the series, usually if a probe is going to be a deep space explorer it leaves the Earth or an orbit around the Earth.\nFor a satellite or space probe to qualify as a deep space X-ray astronomer/explorer or \"astronobot\"/explorer, all it needs to carry aboard is an XRT or X-ray detector and leave Earth's orbit.\n\"Ulysses\" was launched October 6, 1990, and reached Jupiter for its \"gravitational slingshot\" in February 1992. It passed the south solar pole in June 1994 and crossed the ecliptic equator in February 1995. The solar X-ray and cosmic gamma-ray burst experiment (GRB) had 3 main objectives: study and monitor solar flares, detect and localize cosmic gamma-ray bursts, and in-situ detection of Jovian aurorae. \"Ulysses\" was the first satellite carrying a gamma burst detector which went outside the orbit of Mars. The hard X-ray detectors operated in the range 15\u2013150 keV. The detectors consisted of 23-mm thick \u00d7 51-mm diameter CsI(Tl) crystals mounted via plastic light tubes to photomultipliers. The hard detector changed its operating mode depending on (1) measured count rate, (2) ground command, or (3) change in spacecraft telemetry mode. The trigger level was generally set for 8-sigma above background and the sensitivity is 10\u22126 erg/cm2 (1 nJ/m2). When a burst trigger is recorded, the instrument switches to record high resolution data, recording it to a 32-kbit memory for a slow telemetry read out. Burst data consist of either 16 s of 8-ms resolution count rates or 64 s of 32-ms count rates from the sum of the 2 detectors. There were also 16 channel energy spectra from the sum of the 2 detectors (taken either in 1, 2, 4, 16, or 32 second integrations). During 'wait' mode, the data were taken either in 0.25 or 0.5 s integrations and 4 energy channels (with shortest integration time being 8 s). Again, the outputs of the 2 detectors were summed.\nThe \"Ulysses\" soft X-ray detectors consisted of 2.5-mm thick \u00d7 0.5\u00a0cm2 area Si surface barrier detectors. A 100\u00a0mg/cm2 beryllium foil front window rejected the low energy X-rays and defined a conical FOV of 75\u00b0 (half-angle). These detectors were passively cooled and operate in the temperature range \u221235 to \u221255\u00a0\u00b0C. This detector had 6 energy channels, covering the range 5\u201320 keV.\nTheoretical X-ray astronomy.\nTheoretical X-ray astronomy is a branch of theoretical astronomy that deals with the theoretical astrophysics and theoretical astrochemistry of X-ray generation, emission, and detection as applied to astronomical objects.\nLike theoretical astrophysics, theoretical X-ray astronomy uses a wide variety of tools which include analytical models to approximate the behavior of a possible X-ray source and computational numerical simulations to approximate the observational data. Once potential observational consequences are available they can be compared with experimental observations. Observers can look for data that refutes a model or helps in choosing between several alternate or conflicting models.\nTheorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.\nMost of the topics in astrophysics, astrochemistry, astrometry, and other fields that are branches of astronomy studied by theoreticians involve X-rays and X-ray sources. Many of the beginnings for a theory can be found in an Earth-based laboratory where an X-ray source is built and studied.\nDynamos.\nDynamo theory describes the process through which a rotating, convecting, and electrically conducting fluid acts to maintain a magnetic field. This theory is used to explain the presence of anomalously long-lived magnetic fields in astrophysical bodies. If some of the stellar magnetic fields are really induced by dynamos, then field strength might be associated with rotation rate.\nAstronomical models.\nFrom the observed X-ray spectrum, combined with spectral emission results for other wavelength ranges, an astronomical model addressing the likely source of X-ray emission can be constructed. For example, with Scorpius X-1 the X-ray spectrum steeply drops off as X-ray energy increases up to 20 keV, which is likely for a thermal-plasma mechanism. In addition, there is no radio emission, and the visible continuum is roughly what would be expected from a hot plasma fitting the observed X-ray flux. The plasma could be a coronal cloud of a central object or a transient plasma, where the energy source is unknown, but could be related to the idea of a close binary.\nIn the Crab Nebula X-ray spectrum there are three features that differ greatly from Scorpius X-1: its spectrum is much harder, its source diameter is in light-years (ly)s, not astronomical units (AU), and its radio and optical synchrotron emission are strong. Its overall X-ray luminosity rivals the optical emission and could be that of a nonthermal plasma. However, the Crab Nebula appears as an X-ray source that is a central freely expanding ball of dilute plasma, where the energy content is 100 times the total energy content of the large visible and radio portion, obtained from the unknown source.\nThe \"Dividing Line\" as giant stars evolve to become red giants also coincides with the Wind and Coronal Dividing Lines. To explain the drop in X-ray emission across these dividing lines, a number of models have been proposed:\nAnalytical X-ray astronomy.\nHigh-mass X-ray binaries (HMXBs) are composed of OB supergiant companion stars and compact objects, usually neutron stars (NS) or black holes (BH). Supergiant X-ray binaries (SGXBs) are HMXBs in which the compact objects orbit massive companions with orbital periods of a few days (3\u201315 d), and in circular (or slightly eccentric) orbits. SGXBs show typical the hard X-ray spectra of accreting pulsars and most show strong absorption as obscured HMXBs. X-ray luminosity (\"L\"x) increases up to 1036 erg\u00b7s\u22121 (1029 watts).\nThe mechanism triggering the different temporal behavior observed between the classical SGXBs and the recently discovered supergiant fast X-ray transients (SFXT)s is still debated.\nStellar X-ray astronomy.\nThe first detection of stellar x-rays occurred on April 5, 1974, with the detection of X-rays from Capella. A rocket flight on that date briefly calibrated its attitude control system when a star sensor pointed the payload axis at Capella (\u03b1 Aur). During this period, X-rays in the range 0.2\u20131.6 keV were detected by an X-ray reflector system co-aligned with the star sensor. The X-ray luminosity of \"L\"x = 1031 erg\u00b7s\u22121 (1024 W) is four orders of magnitude above the Sun's X-ray luminosity.\nStellar coronae.\nCoronal stars, or stars within a coronal cloud, are ubiquitous among the stars in the cool half of the Hertzsprung-Russell diagram. Experiments with instruments aboard Skylab and Copernicus have been used to search for soft X-ray emission in the energy range ~0.14\u20130.284 keV from stellar coronae. The experiments aboard ANS succeeded in finding X-ray signals from Capella and Sirius (\u03b1 CMa). X-ray emission from an enhanced solar-like corona was proposed for the first time. The high temperature of Capella's corona as obtained from the first coronal X-ray spectrum of Capella using HEAO 1 required magnetic confinement unless it was a free-flowing coronal wind.\nIn 1977 Proxima Centauri is discovered to be emitting high-energy radiation in the XUV. In 1978, \u03b1 Cen was identified as a low-activity coronal source. With the operation of the Einstein observatory, X-ray emission was recognized as a characteristic feature common to a wide range of stars covering essentially the whole Hertzsprung-Russell diagram. The Einstein initial survey led to significant insights:\nTo fit the medium-resolution spectrum of UX Arietis, subsolar abundances were required.\nStellar X-ray astronomy is contributing toward a deeper understanding of\nCurrent wisdom has it that the massive coronal main sequence stars are late-A or early F stars, a conjecture that is supported both by observation and by theory.\nYoung, low-mass stars.\nNewly formed stars are known as pre-main-sequence stars during the stage of stellar evolution before they reach the main-sequence. Stars in this stage (ages &lt;10 million years) produce X-rays in their stellar coronae. However, their X-ray emission is 103 to 105 times stronger than for main-sequence stars of similar masses.\nX-ray emission for pre\u2013main-sequence stars was discovered by the Einstein Observatory. This X-ray emission is primarily produced by magnetic reconnection flares in the stellar coronae, with many small flares contributing to the \"quiescent\" X-ray emission from these stars. Pre\u2013main sequence stars have large convection zones, which in turn drive strong dynamos, producing strong surface magnetic fields. This leads to the high X-ray emission from these stars, which lie in the saturated X-ray regime, unlike main-sequence stars that show rotational modulation of X-ray emission. Other sources of X-ray emission include accretion hotspots and collimated outflows.\nX-ray emission as an indicator of stellar youth is important for studies of star-forming regions. Most star-forming regions in the Milky Way Galaxy are projected on Galactic-Plane fields with numerous unrelated field stars. It is often impossible to distinguish members of a young stellar cluster from field-star contaminants using optical and infrared images alone. X-ray emission can easily penetrate moderate absorption from molecular clouds, and can be used to identify candidate cluster members.\nUnstable winds.\nGiven the lack of a significant outer convection zone, theory predicts the absence of a magnetic dynamo in earlier A stars. In early stars of spectral type O and B, shocks developing in unstable winds are the likely source of X-rays.\nCoolest M dwarfs.\nBeyond spectral type M5, the classical \u03b1\u03c9 dynamo can no longer operate as the internal structure of dwarf stars changes significantly: they become fully convective. As a distributed (or \u03b12) dynamo may become relevant, both the magnetic flux on the surface and the topology of the magnetic fields in the corona should systematically change across this transition, perhaps resulting in some discontinuities in the X-ray characteristics around spectral class dM5. However, observations do not seem to support this picture: long-time lowest-mass X-ray detection, VB 8 (M7e V), has shown steady emission at levels of X-ray luminosity (\"L\"X)\u00a0\u2248 1026 erg\u00b7s\u22121 (1019 W) and flares up to an order of magnitude higher. Comparison with other late M dwarfs shows a rather continuous trend.\nStrong X-ray emission from Herbig Ae/Be stars.\nHerbig Ae/Be stars are pre-main sequence stars. As to their X-ray emission properties, some are\nThe nature of these strong emissions has remained controversial with models including\nK giants.\nThe FK Com stars are giants of spectral type K with an unusually rapid rotation and signs of extreme activity. Their X-ray coronae are among the most luminous (\"L\"X \u2265 1032 erg\u00b7s\u22121 or 1025 W) and the hottest known with dominant temperatures up to 40 MK. However, the current popular hypothesis involves a merger of a close binary system in which the orbital angular momentum of the companion is transferred to the primary.\nPollux is the brightest star in the constellation Gemini, despite its Beta designation, and the 17th brightest in the sky. Pollux is a giant orange K star that makes an interesting color contrast with its white \"twin\", Castor. Evidence has been found for a hot, outer, magnetically supported corona around Pollux, and the star is known to be an X-ray emitter.\nEta Carinae.\nNew X-ray observations by the Chandra X-ray Observatory show three distinct structures: an outer, horseshoe-shaped ring about 2 light years in diameter, a hot inner core about 3 light-months in diameter, and a hot central source less than 1 light-month in diameter which may contain the superstar that drives the whole show. The outer ring provides evidence of another large explosion that occurred over 1,000 years ago. These three structures around Eta Carinae are thought to represent shock waves produced by matter rushing away from the superstar at supersonic speeds. The temperature of the shock-heated gas ranges from 60 MK in the central regions to 3 MK on the horseshoe-shaped outer structure. \"The Chandra image contains some puzzles for existing ideas of how a star can produce such hot and intense X-rays,\" says Prof. Kris Davidson of the University of Minnesota. Davidson is principal investigator for the Eta Carina observations by the Hubble Space Telescope. \"In the most popular theory, X-rays are made by colliding gas streams from two stars so close together that they'd look like a point source to us. But what happens to gas streams that escape to farther distances? The extended hot stuff in the middle of the new image gives demanding new conditions for any theory to meet.\"\nAmateur X-ray astronomy.\nCollectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. The United States Air Force Academy (USAFA) is the home of the US's only undergraduate satellite program, and has and continues to develop the FalconLaunch sounding rockets. In addition to any direct amateur efforts to put X-ray astronomy payloads into space, there are opportunities that allow student-developed experimental payloads to be put on board commercial sounding rockets as a free-of-charge ride.\nThere are major limitations to amateurs observing and reporting experiments in X-ray astronomy: the cost of building an amateur rocket or balloon to place a detector high enough and the cost of appropriate parts to build a suitable X-ray detector.\nMajor questions in X-ray astronomy.\nAs X-ray astronomy uses a major spectral probe to peer into the source, it is a valuable tool in efforts to understand many puzzles.\nStellar magnetic fields.\nMagnetic fields are ubiquitous among stars, yet we do not understand precisely why, nor have we fully understood the bewildering variety of plasma physical mechanisms that act in stellar environments. Some stars, for example, seem to have magnetic fields, fossil stellar magnetic fields left over from their period of formation, while others seem to generate the field anew frequently.\nExtrasolar X-ray source astrometry.\nWith the initial detection of an extrasolar X-ray source, the first question usually asked is \"What is the source?\" An extensive search is often made in other wavelengths such as visible or radio for possible coincident objects. Many of the verified X-ray locations still do not have readily discernible sources. X-ray astrometry becomes a serious concern that results in ever greater demands for finer angular resolution and spectral radiance.\nThere are inherent difficulties in making X-ray/optical, X-ray/radio, and X-ray/X-ray identifications based solely on positional coincidents, especially with handicaps in making identifications, such as the large uncertainties in positional determinants made from balloons and rockets, poor source separation in the crowded region toward the galactic center, source variability, and the multiplicity of source nomenclature.\nX\u2010ray source counterparts to stars can be identified by calculating the angular separation between source centroids and the position of the star. The maximum allowable separation is a compromise between a larger value to identify as many real matches as possible and a smaller value to minimize the probability of spurious matches. \"An adopted matching criterion of 40\" finds nearly all possible X\u2010ray source matches while keeping the probability of any spurious matches in the sample to 3%.\"\nSolar X-ray astronomy.\nAll of the detected X-ray sources at, around, or near the Sun appear to be associated with processes in the corona, which is its outer atmosphere.\nCoronal heating problem.\nIn the area of solar X-ray astronomy, there is the coronal heating problem. The photosphere of the Sun has an effective temperature of 5,570 K yet its corona has an average temperature of 1\u20132 million K. However, the hottest regions are 8\u201320 million K. The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.\nIt is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient gas in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events\u2014nanoflares.\nCurrently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfv\u00e9n waves have been found to dissipate or refract before reaching the corona. In addition, Alfv\u00e9n waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.\nCoronal mass ejection.\nA coronal mass ejection (CME) is an ejected plasma consisting primarily of electrons and protons (in addition to small quantities of heavier elements such as helium, oxygen, and iron), plus the entraining coronal closed magnetic field regions. Evolution of these closed magnetic structures in response to various photospheric motions over different time scales (convection, differential rotation, meridional circulation) somehow leads to the CME. Small-scale energetic signatures such as plasma heating (observed as compact soft X-ray brightening) may be indicative of impending CMEs.\nThe soft X-ray sigmoid (an S-shaped intensity of soft X-rays) is an observational manifestation of the connection between coronal structure and CME production. \"Relating the sigmoids at X-ray (and other) wavelengths to magnetic structures and current systems in the solar atmosphere is the key to understanding their relationship to CMEs.\"\nThe first detection of a Coronal mass ejection (CME) as such was made on December 1, 1971, by R. Tousey of the US Naval Research Laboratory using OSO 7. Earlier observations of coronal transients or even phenomena observed visually during solar eclipses are now understood as essentially the same thing.\nThe largest geomagnetic perturbation, resulting presumably from a \"prehistoric\" CME, coincided with the first-observed solar flare, in 1859. The flare was observed visually by Richard Christopher Carrington and the geomagnetic storm was observed with the recording magnetograph at Kew Gardens. The same instrument recorded a crotchet, an instantaneous perturbation of the Earth's ionosphere by ionizing soft X-rays. This could not easily be understood at the time because it predated the discovery of X-rays (by Roentgen) and the recognition of the ionosphere (by Kennelly and Heaviside).\nExotic X-ray sources.\nA microquasar is a smaller cousin of a quasar that is a radio emitting X-ray binary, with an often resolvable pair of radio jets.\nLSI+61\u00b0303 is a periodic, radio-emitting binary system that is also the gamma-ray source, CG135+01.\nObservations are revealing a growing number of recurrent X-ray transients, characterized by short outbursts with very fast rise times (tens of minutes) and typical durations of a few hours that are associated with OB supergiants and hence define a new class of massive X-ray binaries: Supergiant Fast X-ray Transients (SFXTs).\nObservations made by Chandra indicate the presence of loops and rings in the hot X-ray emitting gas that surrounds Messier 87. A magnetar is a type of neutron star with an extremely powerful magnetic field, the decay of which powers the emission of copious amounts of high-energy electromagnetic radiation, particularly X-rays and gamma rays.\nX-ray dark stars.\nDuring the solar cycle, as shown in the sequence of images at right, at times the Sun is almost X-ray dark, almost an X-ray variable. Betelgeuse, on the other hand, appears to be always X-ray dark. Hardly any X-rays are emitted by red giants. There is a rather abrupt onset of X-ray emission around spectral type A7-F0, with a large range of luminosities developing across spectral class F. Altair is spectral type A7V and Vega is A0V. Altair's total X-ray luminosity is at least an order of magnitude larger than the X-ray luminosity for Vega. The outer convection zone of early F stars is expected to be very shallow and absent in A-type dwarfs, yet the acoustic flux from the interior reaches a maximum for late A and early F stars provoking investigations of magnetic activity in A-type stars along three principal lines. Chemically peculiar stars of spectral type Bp or Ap are appreciable magnetic radio sources, most Bp/Ap stars remain undetected, and of those reported early on as producing X-rays only few of them can be identified as probably single stars. X-ray observations offer the possibility to detect (X-ray dark) planets as they eclipse part of the corona of their parent star while in transit. \"Such methods are particularly promising for low-mass stars as a Jupiter-like planet could eclipse a rather significant coronal area.\"\nX-ray dark planets and comets.\nX-ray observations offer the possibility to detect (X-ray dark) planets as they eclipse part of the corona of their parent star while in transit. \"Such methods are particularly promising for low-mass stars as a Jupiter-like planet could eclipse a rather significant coronal area.\"\nAs X-ray detectors have become more sensitive, they have observed that some planets and other normally X-ray non-luminescent celestial objects under certain conditions emit, fluoresce, or reflect X-rays.\nComet Lulin.\nNASA's Swift Gamma-Ray Burst Mission satellite was monitoring Comet Lulin as it closed to 63 Gm of Earth. For the first time, astronomers can see simultaneous UV and X-ray images of a comet. \"The solar wind\u2014a fast-moving stream of particles from the sun\u2014interacts with the comet's broader cloud of atoms. This causes the solar wind to light up with X-rays, and that's what Swift's XRT sees\", said Stefan Immler, of the Goddard Space Flight Center. This interaction, called charge exchange, results in X-rays from most comets when they pass within about three times Earth's distance from the Sun. Because Lulin is so active, its atomic cloud is especially dense. As a result, the X-ray-emitting region extends far sunward of the comet.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n\"The content of this article was adapted and expanded from http://imagine.gsfc.nasa.gov/ (Public Domain)\""}
{"id": "44063", "revid": "50526930", "url": "https://en.wikipedia.org/wiki?curid=44063", "title": "Extragalactic astronomy", "text": "Study of astronomical objects outside the Milky Way Galaxy\nExtragalactic astronomy is the branch of astronomy concerned with objects outside the Milky Way galaxy. In other words, it is the study of all astronomical objects which are not covered by galactic astronomy.\nThe closest objects in extragalactic astronomy include the galaxies of the Local Group, which are close enough to allow very detailed analyses of their contents (e.g. supernova remnants, stellar associations). As instrumentation has improved, distant objects can now be examined in more detail and so extragalactic astronomy includes objects at nearly the edge of the observable universe. Research into distant galaxies (outside of our local group) is valuable for studying aspects of the universe such as galaxy evolution and Active Galactic Nuclei (AGN) which give insight into physical phenomena (e.g. super massive black hole accretion and the presence of dark matter). It is through extragalactic astronomy that astronomers and physicists are able to study the effects of General Relativity such as gravitational lensing and gravitational waves, that are otherwise impossible (or nearly impossible) to study on a galactic scale.\nA key interest in extragalactic astronomy is the study of how galaxies behave and interact through the universe. Astronomer's methodologies depend \u2014 from theoretical to observation based methods.\nGalaxies form in various ways. In most cosmological \"N\"-body simulations, the earliest galaxies in the cosmos formed in the first hundreds of millions of years.\nThese primordial galaxies formed as the enormous reservoirs of gas and dust in the early universe collapsed in on themselves, giving birth to the first stars, now known as Population III Stars. These stars were of enormous masses in the range of 300 to perhaps 3 million solar masses. Due to their large mass, these stars had extremely short lifespans.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44069", "revid": "678051", "url": "https://en.wikipedia.org/wiki?curid=44069", "title": "Vulcan (hypothetical planet)", "text": "Hypothetical planet between the Sun and Mercury\nVulcan () was a proposed planet that some pre-20th century astronomers thought existed in an orbit between Mercury and the Sun. Speculation about, and even purported observations of, intermercurial bodies or planets date back to the beginning of the 17th century.\nThe case for their probable existence was bolstered by the support of the French mathematician Urbain Le Verrier, who had predicted the existence of Neptune using disturbances in the orbit of Uranus. By 1859, he had confirmed unexplained peculiarities in Mercury's orbit and predicted that they had to be the result of the gravitational influence of another unknown nearby planet or series of asteroids. A French amateur astronomer's report that he had observed an object passing in front of the Sun that same year led Le Verrier to announce that the long sought after planet, which he gave the name Vulcan, had been discovered at last.\nMany searches were conducted for Vulcan over the following decades but, despite several claimed observations, its existence could not be confirmed. The need for the planet as an explanation for Mercury's orbital peculiarities was later rendered unnecessary when Einstein's 1915 theory of general relativity showed that Mercury's departure from an orbit predicted by Newtonian physics was explained by effects arising from the curvature of spacetime caused by the Sun's mass.\nHypotheses and observations.\nCelestial bodies interior to the orbit of Mercury had been hypothesized, searched for, and were even claimed to have been observed, for centuries.\nClaims of seeing objects passing in front of the Sun included those made by the German astronomer Christoph Scheiner in 1611 (which turned out to be the discovery of sunspots), British lawyer, writer and amateur astronomer Capel Lofft's observations of 'an opaque body traversing the sun's disc' on 6 January 1818, and Bavarian physician and astronomer Franz von Paula Gruithuisen's 26 June 1819 report of seeing \"two small spots...on the Sun, round, black and unequal in size\". German astronomer J. W. Pastorff reported many observations also claiming to have seen two spots, with the first observation on 23 October 1822 and subsequent observations in 1823, 1834, 1836, and 1837; in 1834 the larger spot was recorded as 3 arcseconds across, and the smaller 1.25 arcseconds.\nProposals that there could be planets orbiting inside Mercury's orbit were put forward by British scientist Thomas Dick in 1838264 and by French physicist, mathematician, and astronomer Jacques Babinet in 1846 who suggested there may be \"incandescent clouds of a planetary kind, circling the Sun\" and proposed the name \"Vulcan\" (after the god Vulcan from Roman mythology) for a planet close to the Sun.156\nAs a planet near the Sun would be lost in its glare, several observers mounted systematic searches to try to catch it during \"transit\", i.e. when it passes in front of the Sun's disc. German amateur astronomer Heinrich Schwabe searched unsuccessfully on every clear day from 1826 to 1843 and Yale scientist Edward Claudius Herrick conducted observations twice daily starting in 1847, hoping to catch a planet in transit.264 French physician and amateur astronomer Edmond Modeste Lescarbault began searching the Sun's disk in 1853, and more systematically after 1858, with a 3.75 inch (95\u00a0mm) refractor in an observatory he set up outside his surgery.146\nLe Verrier's prediction.\nIn 1840, Fran\u00e7ois Arago, the director of the Paris Observatory, suggested to mathematician Urbain Le Verrier that he work on the topic of Mercury's orbit around the Sun. The goal of the study was to construct a model based on Sir Isaac Newton's laws of motion and gravitation. By 1843, Le Verrier published his provisional theory regarding Mercury's motion, with a detailed presentation published in 1845, which would be tested during a transit of Mercury across the face of the Sun in 1848. Predictions from Le Verrier's theory failed to match the observations.\nDespite that, Le Verrier continued his work and, in 1859, published a more thorough study of Mercury's motion. That was based on a series of meridian observations of the planet and 14 transits. The study's rigor meant that any differences between the motion predicted and what was observed would point to the influence of an unknown factor. Indeed, some discrepancies remained. During Mercury's orbit, its perihelion advances by a small amount, something called perihelion precession. The observed value exceeds the classical mechanics prediction by the small amount of 43 arcseconds per century.\nLe Verrier postulated that the excess precession could be explained by the presence of some unidentified object or objects inside the orbit of Mercury. He calculated that it was either another Mercury-sized planet or, since it was unlikely that astronomers were failing to see such a large object, an unknown asteroid belt near the Sun.\nThe fact that Le Verrier had predicted the existence of the planet Neptune in 1846, using the same techniques, lent veracity to his claim.\nClaimed discovery.\nOn 22 December 1859, Le Verrier received a letter from Lescarbault, saying that he had seen a transit of the hypothetical planet on 26 March of that year. Le Verrier took the train to the village of Org\u00e8res-en-Beauce, some south-west of Paris, to Lescarbault's home-made observatory. Le Verrier arrived unannounced and proceeded to interrogate the man.\nLescarbault described in detail how, on 26 March 1859, he observed a small black dot on the face of the Sun. After some time had passed, he realized that it was moving. He thought it looked similar to the transit of Mercury which he had observed in 1845. He estimated the distance it had already traveled, made some measurements of its position and direction of motion and, using an old clock and a pendulum with which he took his patients' pulses, estimated the total duration of the transit (coming up with 1 hour, 17 minutes, and 9 seconds).\nLe Verrier was not happy about Lescarbault's crude equipment but was satisfied the physician had seen the transit of a previously unknown planet. On 2 January 1860, he announced the discovery of the new planet with the proposed name from mythology, \"Vulcan\", at the meeting of the Acad\u00e9mie des Sciences in Paris. Lescarbault, for his part, was awarded the L\u00e9gion d'honneur and invited to appear before numerous learned societies.\nHowever, not everyone accepted the veracity of Lescarbault's \"discovery\". An eminent French astronomer, Emmanuel Liais, who was working for the Brazilian government in Rio de Janeiro in 1859, claimed to have been studying the surface of the Sun with a telescope twice as powerful as Lescarbault's, at the very moment that Lescarbault said he observed his mysterious transit. Liais, therefore, was \"in a condition to deny, in the most positive manner, the passage of a planet over the sun at the time indicated\".\nBased on Lescarbault's \"transit\", Le Verrier computed Vulcan's orbit: it supposedly revolved about the Sun in a nearly circular orbit at a distance of . The period of revolution was 19 days and 17 hours, and the orbit was inclined to the ecliptic by 12 degrees and 10 minutes (an incredible degree of precision). As seen from the Earth, Vulcan's greatest elongation from the Sun was 8 degrees.\nAttempts to confirm the discovery.\nNumerous reports reached Le Verrier from other amateurs who claimed to have seen unexplained transits. Some of these reports referred to observations made many years earlier, and many were not dated, let alone accurately timed. Nevertheless, Le Verrier continued to tinker with Vulcan's orbital parameters as each newly reported sighting reached him. He frequently announced dates of future Vulcan transits. When these failed to materialize, he tinkered with the parameters some more.\nShortly after 08:00 on 29 January 1860, F.A.R. Russell and three other people in London saw an alleged transit of an intra-Mercurial planet. Many years later, an American observer, Richard Covington, claimed to have seen a well-defined black spot progress across the Sun's disk around 1860 when he was stationed in Washington Territory.\nNo observations of Vulcan were made in 1861. Then, on the morning of 20 March 1862, between 08:00 and 09:00 Greenwich Time, another amateur astronomer, a Mr. Lummis of Manchester, England, saw a transit. His colleague, whom he alerted, also saw the event. Based on these two men's reports, two French astronomers, Benjamin Valz and Rodolphe Radau, independently calculated the object's supposed orbital period, with Valz deriving a figure of 17 days and 13 hours and Radau a figure of 19 days and 22 hours.168\nOn 8 May 1865 another French astronomer, Aristide Coumbary, observed an unexpected transit from Istanbul, Turkey.\nBetween 1866 and 1878, no reliable observations of the hypothetical planet were made. Then, during the total solar eclipse of July 29, 1878, two experienced astronomers, Professor James Craig Watson, the director of the Ann Arbor Observatory in Michigan, and Lewis Swift, from Rochester, New York, both claimed to have seen a Vulcan-type planet close to the Sun. Watson, observing from Separation Point, Wyoming, placed the planet about 2.5 degrees south-west of the Sun and estimated its magnitude at 4.5. Swift, observing the eclipse from a location near Denver, Colorado, saw what he took to be an intra-mercurial planet about 3 degrees south-west of the Sun. He estimated its brightness to be the same as that of Theta Cancri, a fifth-magnitude star which was also visible during totality, about six or seven minutes from the \"planet\". Theta Cancri and the planet were nearly in line with the Sun's centre.\nWatson and Swift had reputations as excellent observers. Watson had already discovered more than twenty asteroids, while Swift had several comets named after him. Both described the colour of their hypothetical intra-mercurial planet as \"red\". Watson reported that it had a definite disk\u2014unlike stars, which appear in telescopes as mere points of light\u2014and that its phase indicated that it was on the far side of the Sun approaching superior conjunction.\nBoth Watson and Swift had observed two objects they believed were not known stars, but after Swift corrected an error in his coordinates, none of the coordinates matched each other, nor known stars. The idea that \"four\" objects were observed during the eclipse generated controversy in scientific journals and mockery from Watson's rival C. H. F. Peters. Peters noted that the margin of error in the pencil and cardboard recording device Watson had used was large enough to plausibly include a bright known star. A skeptic of the Vulcan hypothesis, Peters dismissed all the observations as mistaking known stars as planets. \nAstronomers continued searching for Vulcan during total solar eclipses in 1883, 1887, 1889, 1900, 1901, 1905, and 1908.219 Finally, in 1908, William Wallace Campbell, Director, and Charles Dillon Perrine, Astronomer, of the Lick Observatory, after comprehensive photographic observations at three solar eclipse expeditions in 1901, 1905, and 1908, stated: \"In our opinion, the work of the three Crocker Expeditions ... brings the observational side of the intermercurial planet problem\u2014famous for half a century\u2014definitely to a close.\"\nHypothesis disproved.\nIn 1915 Einstein's theory of relativity, an approach to understanding gravity entirely differently from classical mechanics, removed the need for Le Verrier's hypothetical planet. It showed that the peculiarities in Mercury's orbit were the results of the curvature of spacetime caused by the mass of the Sun. This added a predicted 0.1 arc-second advance of Mercury's perihelion each orbital revolution, or 43 arc-seconds per century, exactly the observed amount (without any recourse to the existence of a hypothetical Vulcan).\nThe new theory modified the predicted orbits of all planets, but the magnitude of the differences from Newtonian theory diminishes rapidly as one gets farther from the Sun. Also, Mercury's fairly eccentric orbit makes it much easier to detect the perihelion shift than is the case for the nearly circular orbits of Venus and Earth. Einstein's theory was empirically verified in the Eddington experiment during the solar eclipse of May 29, 1919, during which photographs showed the curvature of spacetime was bending starlight around the Sun. Most astronomers quickly accepted that a large planet inside the orbit of Mercury could not exist, given the corrected equation of gravity.220\nThe International Astronomical Union has reserved the name 'Vulcanoid\" for asteroids that may exist inside the orbit of the planet Mercury. So far, however, earth- and space-based telescopes and the NASA Parker Solar Probe have detected no such asteroids.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44070", "revid": "8839503", "url": "https://en.wikipedia.org/wiki?curid=44070", "title": "Avro Vulcan", "text": "British jet-powered delta wing strategic bomber\nThe Avro Vulcan (later Hawker Siddeley Vulcan from July 1963) is a jet-powered, tailless, delta-wing, high-altitude strategic bomber, which was operated by the Royal Air Force (RAF) from 1956 until 1984. Aircraft manufacturer A.V. Roe and Company (Avro) designed the Vulcan in response to Specification B.35/46. Of the three V bombers produced, the Vulcan was considered the most technically advanced, and therefore the riskiest option. Several reduced-scale aircraft, designated Avro 707s, were produced to test and refine the delta-wing design principles.\nThe Vulcan B.1 was first delivered to the RAF in 1956; deliveries of the improved Vulcan B.2 started in 1960. The B.2 featured more powerful engines, a larger wing, an improved electrical system, and electronic countermeasures, and many were modified to accept the Blue Steel missile. As a part of the V-force, the Vulcan was the backbone of the United Kingdom's airborne nuclear deterrent during much of the Cold War. Although the Vulcan was typically armed with nuclear weapons, it could also carry out conventional bombing missions, which it did in Operation Black Buck during the Falklands War between the United Kingdom and Argentina in 1982.\nThe Vulcan had no defensive weaponry, initially relying upon high-speed, high-altitude flight to evade interception. Electronic countermeasures were employed by the B.1 (designated B.1A) and B.2 from around 1960. A change to low-level tactics was made in the mid-1960s. In the mid-1970s, nine Vulcans were adapted for maritime radar reconnaissance operations, redesignated as B.2 (MRR). In the final years of service, six Vulcans were converted to the K.2 tanker configuration for aerial refuelling.\nAfter retirement by the RAF, one example, B.2 XH558, named \"The Spirit of Great Britain\", was restored for use in display flights and air shows, whilst two other B.2s, XL426 and XM655, have been kept in taxiable condition for ground runs and demonstrations. B.2 XH558 flew for the last time in October 2015 and is also being kept in taxiable condition.\nXM612 is on display at Norwich Aviation Museum.\nDevelopment.\nOrigins.\nThe origin of the Vulcan and the other V bombers is linked with early British atomic weapon programme and nuclear deterrent policies. Britain's atom bomb programme began with Air Staff Operational Requirement OR.1001 issued in August 1946. This anticipated a government decision in January 1947 to authorise research and development work on atomic weapons, the U.S. Atomic Energy Act of 1946 (McMahon Act) having prohibited exporting atomic knowledge, even to countries that had collaborated on the Manhattan Project. OR.1001 envisaged a weapon not to exceed in length, in diameter and in weight. The weapon had to be suitable for release from .\nIn January 1947, the Ministry of Supply distributed Specification B.35/46 to UK aviation companies to satisfy Air Staff Operational Requirement OR.229 for \"a medium range bomber landplane capable of carrying one bomb to a target from a base which may be anywhere in the world.\" A cruising speed of at altitudes between was specified. The maximum weight when fully loaded should not exceed . Alternatively, the aircraft was to be capable of carrying a conventional bomb load of . The similar OR.230 required a \"long-range bomber\" with a radius of action with a maximum weight of when fully loaded; this requirement was considered too exacting. Six companies submitted technical brochures to this specification, including Avro.\nRequired to tender by the end of April 1947, work began on receipt of Specification B.35/46 at Avro, led by technical director Roy Chadwick and chief designer Stuart Davies; the type designation was \"Avro 698\". As was obvious to the design team, conventional aircraft could not satisfy the specification. No worthwhile information about high-speed flight was available from the Royal Aircraft Establishment (RAE) or the US. Avro were aware that Alexander Lippisch had designed a delta-wing fighter and considered the same delta configuration would be suitable for their bomber. The team estimated that an otherwise conventional aircraft, with a swept wing of 45\u00b0, would have doubled the weight requirement. Realising that swept wings increase longitudinal stability, the team deleted the tail (empennage) and the supporting fuselage, it thus became a swept-back flying wing with only a rudimentary forward fuselage and a fin (vertical stabilizer) at each wingtip. The estimated weight was now only 50% over the requirement; a delta shape resulted from reducing the wingspan and maintaining the wing area by filling in the space between the wingtips, which enabled the specification to be met. Although Alexander Lippisch is generally credited as the pioneer of the delta wing, Chadwick's team had followed its own logical design process. The initial design submission had four large turbojets stacked in pairs buried in the wings on either side of the centreline. Outboard of the engines were two bomb bays.\nIn August 1947, Chadwick was killed in the crash of the Avro Tudor 2 prototype, and was succeeded by Sir William Farren. Reductions in wing thickness made incorporating the split bomb bays and stacked engines impossible, thus the engines were placed side by side in pairs on either side of a single bomb bay, with the fuselage growing somewhat. The wingtip fins gave way to a single fin on the aircraft's centreline. Rival manufacturer Handley Page received a prototype contract for its crescent-winged HP.80 B.35/46 tender in November 1947. Although considered the best option, the contract award for Avro's design was delayed while its technical strength was established. Instructions to proceed with the construction of two Avro 698 prototypes were received in January 1948. As an insurance measure against both radical designs failing, Short Brothers received a contract for the prototype SA.4 to the less-stringent Specification B.14/46. The SA.4, later named Sperrin, was not required. In April 1948, Vickers also received authority to proceed with their Type 660, which, although falling short of the B.35/46 Specification, but being of a more conventional design, would be available sooner. This plane entered service as the Valiant.\nAvro 707 and Avro 710.\nAs Avro had no flight experience of the delta wing, the company planned two smaller experimental aircraft based on the 698, the one-third scale model 707 for low-speed handling and the one-half scale model 710 for high-speed handling. Two of each were ordered. The 710 was cancelled when it was considered too time-consuming to develop; a high-speed variant of the 707 was designed in its place, the 707A. The first 707, VX784, flew in September 1949, but crashed later that month, killing the Aeroplane and Armament Experimental Establishment's chief test pilot Squadron Leader Samuel Eric Esler, DFC, AE. The second low-speed 707, VX790, built with the still uncompleted 707A's nose section (containing an ejection seat) and redesignated 707B, flew in September 1950 piloted by Avro test pilot Wg Cdr Roland \"Roly\" Falk. The high-speed 707A, WD280, followed in July 1951.\nDue to the delay of the 707 programme, the contribution of the 707B and 707A towards the basic design of the 698 was not considered significant, though it did highlight a need to increase the length of the nosewheel to give a ground incidence of 3.5\u00b0, the optimum take-off attitude. The 707B and 707A proved the design's validity and gave confidence in the delta planform. A second 707A, WZ736, and a two-seat 707C, WZ744, were also constructed, but they played no part in the 698's development.\nPrototypes and type certification.\nFirst prototype VX770 and name.\nMore influential than the Avro 707 in the 698's design was the wind-tunnel testing performed at RAE Farnborough. This necessitated a wing redesign incorporating a cranked and drooped leading edge and vortex generators to avoid the onset of compressibility drag, which would have restricted the maximum speed. This wing modification resulted in the \"phase 2\" wing which was first investigated on Avro 707A WD480. This modification was too late to be incorporated on the two prototype 698s and the first three B.1 aircraft before their first flights. (The B.1s were quickly retrofitted).\nPainted gloss white, the 698 prototype VX770, with its pure delta wing, flew for the first time on 30 August 1952 piloted by Roly Falk flying solo. VX770, fitted with only the first pilot's ejection seat and a conventional control wheel, was powered by four Rolls-Royce RA.3 Avon engines of thrust, its intended Bristol Olympus engines not being available. The prototype had fuselage fuel tanks but no wing tanks, so temporary additional tankage was carried in the bomb bay. VX770 made an appearance at the 1952 Society of British Aircraft Constructors' (SBAC) Farnborough Air Show the next month when Falk demonstrated an almost vertical bank.\nAfter its Farnborough appearance, the future name of the Avro 698 was a subject of speculation. Avro had strongly recommended the name \"Ottawa\", in honour of the company's connection with Avro Canada. The weekly magazine \"Flight\" suggested \"Albion\" after rejecting \"Avenger\", \"Apollo\", and \"Assegai\". The chief of the air staff preferred a V-class of bombers, and the Air Council announced the following month that the 698 would be called \"Vulcan\" after the Roman god of fire and destruction.\nIn January 1953, VX770 was grounded for the installation of wing fuel tanks, Armstrong Siddeley ASSa.6 Sapphire engines of thrust and other systems; it flew again in July 1953.\nFrom 1957, VX770 was used as the flying testbed for the Rolls-Royce Conway by-pass engine. It crashed at a flying display at RAF Syerston in September 1958.\nSecond prototype VX777.\nThe second prototype, VX777, first flew on 3 September 1953. More representative of production aircraft, it was lengthened to accommodate a longer nose undercarriage leg to increase the angle of attack of the wing, shortening the take-off run. It featured a visual bomb-aiming blister under the cabin and was fitted with Bristol Olympus 100 engines of thrust. At Falk's suggestion, a fighter-style control stick had replaced the control wheel. Like VX770, VX777 had the original wing with straight leading edges. VX777 was joined in formation by the first prototype VX770 and four Avro 707s at the 1953 Farnborough Air Show. During trials in July 1954, VX777 was substantially damaged in a heavy landing at Farnborough. It was repaired, fitted with Olympus 101 engines of thrust before resuming trials with Avro and the Aeroplane and Armament Experimental Establishment (A&amp;AEE) at Boscombe Down.\nTesting and type certification.\nWhile exploring VX777's high-speed and high-altitude flight envelope at the A&amp;AEE, mild buffeting and other undesirable flight characteristics were experienced while approaching the limiting Mach number, including an alarming tendency to enter an uncontrollable dive. This was judged unacceptable for an unarmed bomber. Fitting the phase 2 wing removed the buffeting and an auto-mach trimmer countered the high-speed dive. The latter applied up-elevator as the speed critically increased. This up-elevator force was greater than the force required to counter the dive. Consequently, as speed increased, the control column had to be pushed rather than pulled to maintain level flight. This artificial pitch-up made the Vulcan handle more like other aircraft as its speed increased.\nThe first production B.1 XA889 first flew in February 1955 with the original wing and joined the trials in June. In September 1955, Falk, flying the second production B.1 XA890 (which had remained at Woodford as part of the MoS's Air Fleet on radio trials), amazed crowds at the Farnborough Air Show by executing a barrel roll on his second flypast in front of the SBAC president's tent. After two days of flying, he was called in front of service and civil aviation authorities and ordered to refrain from carrying out this \"dangerous\" manoeuvre. Now fitted with a phase 2 wing, XA889 was delivered in March 1956 to the A&amp;AEE for trials for the type's initial Certificate of Airworthiness which it received the following month.\nInto service.\nVulcan B.1 and B1A.\nThe first 15 production B.1s were powered by the Olympus 101. Many of these early examples in a metallic finish remained the property of the Ministry of Supply, being retained for trials and development purposes. Those entering RAF service were delivered to No 230 Operational Conversion Unit (OCU), the first in July 1956. Later aircraft, painted in anti-flash white and powered by the Olympus 102 with thrust, began to enter squadron service in July 1957. The Olympus 102s were modified during overhaul to the Olympus 104 standard, ultimately rated at thrust.\nRebuilding B.1s as B.2s was considered but rejected over cost. Nevertheless, to extend the B.1's service life, 28 (the surviving B1 aircraft fitted with Olympus 102/104 engines) were upgraded by Armstrong Whitworth between 1959 and 1963 to the B.1A standard, including features of the B.2 such as ECM equipment, in-flight refuelling receiving equipment, and UHF radio. However, the B.1As were not strengthened for low-level operations and all were withdrawn by 1968.\nVulcan B.2.\nAs far back as 1952, Bristol Aero Engines had begun development of the BOl.6 (Olympus 6) rated at thrust but if fitted to the B.1, this would have reintroduced the buffet requiring further redesign of the wing.\nThe decision to proceed with the B.2 versions of the Vulcan was made in May 1956, being developed by Avro's chief designer Roy Ewans. The first B.2 was anticipated to be around the 45th aircraft of the 99 then on order. As well as being able to achieve greater heights over targets, operational flexibility was believed to be extended by the provision of in-flight refuelling equipment and tanker aircraft. The increasing sophistication of Soviet air defences required the fitting of electronic countermeasure (ECM) equipment, and vulnerability could be reduced by the introduction of the Avro Blue Steel stand-off missile, then in development. To develop these proposals, the second Vulcan prototype VX777 was rebuilt with the larger and thinner phase-2C wing, improved flying control surfaces, and Olympus 102 engines, first flying in this configuration in August 1957. Several Vulcan B.1s were used for the development of the B.2: development of the BOl.6 (later Olympus 200), XA891; a new AC electrical system, XA893; ECM including jammers within a bulged tail cone and a tail warning radar, XA895: and for Blue Steel development work, XA903.\nThe 46th production aircraft and first B.2, XH533, first flew in September 1958 using Olympus 200 engines, six months before the last B.1 XH532 was delivered in March 1959. The second B.2, XH534, flew in January 1959. Powered by production Olympus 201s with thrust, it was more representative of a production aircraft, being fitted with an in-flight refuelling probe and a bulged ECM tail cone. Some subsequent B.2s were initially lacking probes and ECM tail cones, but these were retrofitted. The first 10 B.2s outwardly showed their B.1 ancestry, retaining narrow engine air intakes. Anticipating even more powerful engines, the air intakes were deepened on the 11th (XH557) and subsequent aircraft. Many of the early aircraft were retained for trials, and the 12th B.2, XH558, was the first to be delivered to the RAF in July 1960. Coincidentally, XH558 was also the last Vulcan in service with the RAF, before being retired in 1992.\nThe 26th B.2, XL317, the first of a production batch ordered in February 1956, was the first Vulcan, apart from development aircraft, capable of carrying the Blue Steel missile; 33 aircraft were delivered to the RAF with these modifications. When the Mk.2 version of Blue Steel was cancelled in favour of the Douglas GAM-87 Skybolt air-launched ballistic missile in December 1959, fittings were changed in anticipation of the new missile, one under each wing. Though Skybolt was cancelled in November 1962, many aircraft were delivered or retrofitted with \"Skybolt\" blisters. Later aircraft were delivered with Olympus 301 engines with thrust. Two earlier aircraft were re-engined (XH557 and XJ784) for trials and development work; another seven aircraft were converted around 1963.\nThe last B.2 XM657 was delivered in 1965 and the type served until 1984. Whilst in service, the B.2 was continuously updated with modifications, including rapid engine starting, bomb-bay fuel tanks, wing strengthening to give the fatigue life to enable the aircraft to fly at low level (a tactic introduced in the mid-1960s), upgraded navigation equipment, terrain-following radar, standardisation on a common weapon (WE.177) and improved ECM equipment. Nine B.2s were modified for a maritime radar reconnaissance role and six for an airborne tanker role. An updated bomb rack assembly allowing the carriage of 30 1,000\u00a0lb bombs, up from 21 was demonstrated by Avro but was not introduced. In 1982, during the Falklands War, the updated B.2 made a bombing run against Port Stanley Airport, flying a distance of 4,000 mi (6,437 km).\nProposed developments and cancelled projects.\nThe Avro 718 was a 1951 proposal for a delta-winged military transport based on the Type 698 to carry 80 troops or 110 passengers. It would have been powered by four Bristol Olympus BOl.3 engines.\nThe Avro Type 722 Atlantic was a 1952 proposal (announced in June 1953) for a 120-passenger delta-winged airliner based on the Type 698.\nThe Avro 732 was a 1956 proposal for a supersonic development of the Vulcan and would have been powered by 8 de Havilland Gyron Junior engines. Unlike the proposed Avro 721 low-level bomber of 1952 or the Avro 730 supersonic stainless steel canard bomber dating from 1954 (cancelled in 1957 before completion of the prototype), the Type 732 showed its Vulcan heritage.\nIn 1960, the Air Staff approached Avro with a request into a study for a patrol missile carrier armed with up to six Skybolt missiles capable of a mission length of 12 hours. Avro's submission in May 1960 was the Phase 6 Vulcan, which would have been the Vulcan B.3. The aircraft was fitted with an enlarged wing of span with increased fuel capacity; additional fuel tanks in a dorsal spine; a new main undercarriage to carry an all-up-weight of ; and reheated Olympus 301s of thrust. An amended proposal of October 1960 inserted a plug into the forward fuselage with capacity for six crew members including a relief pilot, all facing forwards on ejection seats, and aft-fan versions of the Olympus 301.\nTo counter improving Soviet defences after the cancellation of Skybolt, Avro proposed a Vulcan with three Gnat fighters slung underneath. The Gnats were to have been released in enemy airspace to provide fighter cover, and they were expected to land \"in friendly territory\" or return to the Vulcan to replenish their tanks by means of a specially installed flight-refuelling drogue.\nExport proposals.\nOther countries expressed interest in purchasing Vulcans, but as with the other V-bombers, no foreign sales materialised.\nAs early as 1954, the Royal Australian Air Force (RAAF) recognised that the English Electric Canberra might soon become outdated. Potential replacements, such as the Boeing B-47E, Handley-Page Victor and Vulcan were considered.\nPolitical pressure for a Canberra replacement came to a head in 1962, by which time agile, supersonic bombers/strategic strike aircraft, such as the North American A-5 Vigilante, BAC TSR-2, General Dynamics F-111, had become available. Had the Australian government pre-ordered the TSR-2, several V-bombers, including Vulcans, would have been made available, for interim use by the RAAF; however, the F-111C was ordered. (The UK government almost followed that decision \u2013 after the cancellation of the TSR-2 \u2013 it was offered the similar F-111K.)\nIn the early 1980s, Argentina approached the UK with a proposal to buy a number of Vulcans. An application, made in September 1981, requested the 'early availability' of a 'suitable aircraft'. With some reluctance, British ministers approved the export of a single aircraft but emphasised that clearance had not been given for the sale of a larger number. A letter from the British Foreign and Commonwealth Office to the Ministry of Defence in January 1982 stated that little prospect was seen of this happening without ascertaining the Argentine interest and whether such interest was genuine: 'On the face of it, a strike aircraft would be entirely suitable for an attack on the Falklands.' Argentina invaded the Falkland Islands less than three months later, after which a British embargo on the sale of any military equipment was quickly imposed.\nDesign.\nOverview.\nDespite its radical and unusual shape, the airframe was built along traditional lines. Except for the most highly stressed parts, the whole structure was manufactured from standard grades of light alloy. The airframe was broken down into a number of major assemblies: The centre section, a rectangular box containing the bomb bay and engine bays bounded by the front and rear spars and the wing transport joints; the intakes and centre fuselage; the front fuselage, incorporating the pressure cabin; the nose; the outer wings; the leading edges; the wing trailing edge and rear end of the fuselage; and a single swept tail fin with a single rudder was on the trailing edge.\nA five-man crew was accommodated within the pressure cabin on two levels; the first pilot and co-pilot sitting on Martin-Baker 3K (3KS on the B.2) ejection seats whilst on the lower level the navigator radar, navigator plotter, and air electronics officer (AEO) sat facing rearwards and would abandon the aircraft via the entrance door. The original B35/46 specification sought a jettisonable crew compartment, but this requirement was removed in a subsequent amendment; the rear crew's escape system was often an issue of controversy, such as when a practical refit scheme was rejected. A rudimentary sixth seat forward of the navigator radar was provided for an additional crew member; the B.2 had an additional seventh seat opposite the sixth seat and forward of the AEO. The visual bomb-aimer's compartment could be fitted with a T4 (Blue Devil) bombsight, in many B.2s, this space housed a vertically mounted Vinten F95 Mk.10 camera for assessing simulated low-level bombing runs.\nFuel was carried in 14 bag tanks, four in the centre fuselage above and to the rear of the nosewheel bay, and five in each outer wing. The tanks were split into four groups of almost equal capacity, each normally feeding its respective engine, though cross-feeding was possible. The centre of gravity was automatically maintained by electric timers, which sequenced the booster pumps on the tanks. B.2 aircraft could be fitted with one or two additional fuel tanks in the bomb bay.\nDespite being designed before a low radar cross-section and other stealth factors were ever a consideration, an RAE technical note of 1957 stated that of all the aircraft so far studied, the Vulcan appeared by far the simplest radar-echoing object, due to its shape; only one or two components contributed significantly to the echo at any aspect, compared with three or more on most other types.\nColour schemes.\nThe two prototype Vulcans were finished in gloss white. Early Vulcan B.1s left the factory in a natural metal finish; the front half of the nose radome was painted black, the rear half painted silver. Front-line Vulcan B.1s had a finish of anti-flash white and RAF \"type D\" roundels. Front-line Vulcan B.1As and B.2s were similar, but with pale roundels.\nWith the adoption of low-level attack profiles in the mid-1960s, B.1As and B.2s were given a glossy sea grey medium and dark green disruptive pattern camouflage on the upper surfaces, white undersurfaces, and \"type D\" roundels. (The last 13 Vulcan B.2s, XM645 onwards, were delivered thus from the factory). In the mid-1970s, Vulcan B.2s received a similar scheme with matte camouflage, light aircraft grey undersides, and \"low-visibility\" roundels. B.2(MRR)s received a similar scheme in gloss; and the front halves of the radomes were no longer painted black. Beginning in 1979, 10 Vulcans received a wrap-around camouflage of dark sea grey and dark green because, during Red Flag exercises in the US, defending SAM forces had found that the grey-painted undersides of the Vulcan became much more visible against the ground at high angles of bank.\nAvionics.\nThe original Vulcan B.1 radio fit was: two 10-channel VHF transmitter/receivers (TR-1985/TR-1986) and an STR-18, 24-channel HF transmitter-receiver (R4187/T4188). The Vulcan B.1A also featured a UHF transmitter-receiver (ARC-52). The initial B.2 radio fit was similar to the B.1A though it was ultimately fitted with the ARC-52, a V/UHF transmitter/receiver (PTR-175), and a single-sideband modulation HF transmitter-receiver (Collins 618T).\nThe navigation and bombing system comprised an H2S Mk9 radar and a navigation bombing computer Mk1. The Mark 2 aircraft had a modified navigation bombing computer Mk2. The updated computer would allow for operation up to 60,000ft and also a modified syncro system. The greater height was at the expense of a miniumum height being increased to 17,200ft. With the switch to low altitude operations the ballistic from Mk1 system was retrofitted restoring the height range from 7,200ft to 50,000ft.\nOther navigation aids included a Marconi radio compass (ADF), GEE Mk3, Green Satin Doppler radar to determine the groundspeed and drift angle, radio and radar altimeters, and an instrument landing system. TACAN replaced GEE in the B.1A and B.2 in 1964. Decca Doppler 72 replaced Green Satin in the B.2 around 1969&lt;ref name='PB&amp;E102/3'&gt;Price, Blackman and Edmonson 2010, pp. 102, 103.&lt;/ref&gt; A continuous display of the aircraft's position was maintained by a ground position indicator.\nVulcan B.2s were eventually fitted with the free-running dual-gyroscopic heading reference system (HRS) Mk.2, based upon the inertial platform of the Blue Steel missile, which had been integrated into the system when the missile had been carried. With the HRS a navigator's heading unit was provided, which enabled the navigator plotter to adjust the aircraft heading, through the autopilot, by as little as 0.1 degrees. The B.2 (MRR) was additionally fitted with the LORAN C navigation system.\nThe original ECM fit of the B.1A and B.2 was one Green Palm voice communications jammer; two Blue Diver metric jammers; three Red Shrimp S-band jammers; a Blue Saga passive warning receiver with four aerials; a Red Steer tail warning radar; and chaff dispensers. The bulk of the equipment was carried in a large, extended tail cone, and a flat ECM aerial counterpoise plate was mounted between the starboard tailpipes. Later equipment on the B.2 included: an L band jammer (replacing a Red Shrimp); the ARI 18146 X-band jammer; replacing the Green Palm; the improved Red Steer Mk.2; infra-red decoys (flares); and the ARI 18228 PWR with its aerials that gave a squared top to the fin.\nControls.\nThe aircraft was controlled by a fighter-type control stick and rudder bar, which operated the powered flying controls, which each had a single electrohydraulic-powered flying control unit, except the rudder, which had two, one running as a back-up. Artificial feel and auto stabilisation in the form of pitch and yaw dampers were provided, as well as an auto Mach trimmer.\nThe flight instruments in the B.1 were traditional and included \"G4B\" compasses; Mk.4 artificial horizons; and zero reader flight display instruments. The B.1 had a Smiths Mk10 autopilot. In the B.2, these features were incorporated into the Smiths Military Flight System (MFS), the pilots' components being: two beam compasses; two director-horizons; and an Mk.10A or Mk.10B autopilot. From 1966, B.2s were fitted with the \"ARI 5959\" TFR, built by General Dynamics, its commands being fed into the director-horizons.\nThe B.1 had four elevators (inboard) and four ailerons (outboard). In the B.2, these were replaced by eight elevons. The Vulcan was also fitted with six electrically operated three-position (retracted, medium drag, high drag) airbrakes, four in the upper centre section and two in the lower. Originally, four lower airbrakes were used, but the outboard two were deleted before the aircraft entered service. A brake parachute was installed inside the tail cone.\nElectrical and hydraulic systems.\nThe main electrical system on the B.1/B.1A was 112 V DC supplied by four 22.5 kW engine-driven starter\u2013generators. Backup power was provided by four 24 V 40 Ah batteries connected in series providing 96 V. Secondary electrical systems were 28 V DC, single-phase 115 V AC at 1600\u00a0Hz, and three-phase 115 V AC at 400\u00a0Hz, driven by transformers and inverters from the main system. The 28 V DC system was backed up by a single 24 V battery.\nFor greater efficiency and higher reliability, the main system on the B.2 was changed to three-phase 200 V AC at 400\u00a0Hz supplied by four 40 kVA engine-driven constant-speed alternators. Engine starting was then by air-starters supplied from a Palouste compressor on the ground. Standby supplies in the event of a main AC failure were provided by two primary systems: A ram air turbine driving a 17 kVA alternator was stowed in the underside of the port wing and could operate from high altitudes down to . In addition an airborne auxiliary power plant, a Rover gas turbine driving a 40 kVA alternator was fitted within the starboard wing, and could be started once the aircraft was below an altitude of . Secondary electrical supplies were by transformer-rectifier units for 28 V DC and rotary frequency converters for the 115 V 1600\u00a0Hz single-phase supplies.\nThe change to an AC system was a significant improvement. Each PFCU had a hydraulic pump that was driven by an electric motor, in modern terminology, this is an electro-hydraulic actuator. Because no manual reversion existed, a total electrical failure would result in a loss of control. The standby batteries on the B.1 were designed to give enough power for twenty minutes of flying time, but this proved to be optimistic and two aircraft, XA891 and XA908, crashed as a result.\nThe main hydraulic system provided pressure for undercarriage raising and lowering and bogie trim; nosewheel centring and steering; wheel brakes (fitted with Maxarets); bomb doors opening and closing; and (B.2 only) AAPP air scoop lowering. Hydraulic pressure was provided by three hydraulic pumps fitted to Nos. 1, 2 and 3 engines. An electrically operated hydraulic power pack (EHPP) could be used to operate the bomb doors and recharge the brake accumulators. A compressed air (later nitrogen) system was provided for emergency undercarriage lowering.\nEngine.\nThe Rolls-Royce Olympus, originally known as the \"Bristol BE.10 Olympus\", is a two-spool, axial-flow turbojet that powered the Vulcan. Each Vulcan had four engines buried in the wings, positioned in pairs close to the fuselage. The engine's design began in 1947, intended to power the Bristol Aeroplane Company's own rival design to the Vulcan.\nAs the prototype Vulcan VX770 was ready for flight prior to the Olympus being available, it first flew using Rolls-Royce Avon RA.3 engines of thrust. These were quickly replaced by Armstrong Siddeley Sapphire ASSa.6 engines of thrust. VX770 later became a flying test bed for the Rolls-Royce Conway. The second prototype VX777 first flew with Olympus 100s of thrust. It was subsequently re-engined with Olympus 101 engines. When VX777 flew with a Phase 2C (B.2) wing in 1957, it was fitted with Olympus 102 engines of thrust.\nEarly B.1s were equipped with the Olympus 101. Later aircraft were delivered with Olympus 102s. All Olympus 102s became the Olympus 104 on overhaul and ultimately thrust on uprating. The first B.2 flew with the second-generation Olympus 200, design of which began in 1952. Subsequent B.2s were engined with either the uprated Olympus 201 or the Olympus 301. The Olympus 201 was designated 202 on being fitted with a rapid air starter. The engine would later be developed into a reheated (afterburning) powerplant for the cancelled TSR-2 strike/reconnaissance aircraft and the supersonic passenger transport Concorde.\nAround 90% power, the engines in the Vulcan would emit a distinctive \"howl\"-like noise due to the air intake arrangement, which became an attraction at public airshows.\nOperational history.\nIntroduction.\nIn September 1956, the RAF received its first Vulcan B.1, XA897, which immediately embarked upon a round-the-world tour. The tour was to be an important demonstration of the range and capabilities of the aircraft, but it also had other benefits in the form of conducting goodwill visits in various countries; during their service, Vulcans routinely visited various nations and distant parts of the Commonwealth as a show of support and military protection. This first tour, however, was struck by misfortune; on 1 October 1956, while landing in bad weather at London Heathrow Airport at the completion of the world tour, XA897 was destroyed in a fatal accident.\nThe first two aircraft were delivered to 230 OCU in January 1957 and the training of crews started on 21 February 1957. The first OCU course to qualify was No. 1 Course, on 21 May 1957, and they went on to form the first flight of No. 83 Squadron. No. 83 Squadron was the first operational squadron to use the bomber, at first using borrowed Vulcans from the OCU, and on 11 July 1956 it received the first aircraft of its own. By September 1957, several Vulcans had been handed over to No. 83 Squadron. The second OCU course also formed a Flight of 83 Squadron, but subsequent trained crews were also used to form the second bomber squadron, 101 Squadron. The last aircraft from the first batch of 25 aircraft had been delivered by the end of 1957 to 101 Squadron.\nTo increase the mission range and flight time for Vulcan operations, in-flight refuelling capabilities were added from 1959 onwards; several Valiant bombers were refurbished as tankers to refuel the Vulcans. Continuous airborne patrols proved untenable, however, and the refuelling mechanisms across the Vulcan fleet fell into disuse in 1965 after the Valiant tanker aircraft were withdrawn from Service.\nBoth Vulcans and the other V-force aircraft routinely visited the Far East, in particular Singapore, where a fully equipped nuclear-weapons storage facility had been constructed in 1959. These deployments were part of the UK's contribution to SEATO operations, often to test the defences of friendly nations in joint exercises. During the Indonesia\u2013Malaysia confrontation, Britain planned to deploy three squadrons of V-bomber aircraft and 48 Red Beard tactical nuclear weapons to the region, although this was ultimately decided against. Vulcans trained in the region for both conventional and nuclear missions. In the early 1970s, the RAF decided to permanently deploy two squadrons of Vulcans overseas in the Near East Air Force Bomber Wing, based at RAF Akrotiri in Cyprus. The Vulcans were withdrawn in the mid-1970s, however, as Cypriot intercommunal violence intensified.\nVulcans flew some long-range missions. In June 1961, one flew 18,507\u00a0km from RAF Scampton to Sydney in just over 20 hours, facilitated by three air refuellings. Vulcans visited the United States in the 1960s and 1970s to participate in air shows and static displays, as well as to participate in the Strategic Air Command's (SAC) Annual Bombing and Navigation Competition at such locations as Barksdale AFB, Louisiana, and the former McCoy AFB, Florida. Vulcans also took part in the large Operation Sky Shield exercise in 1961, in which NORAD defences were tested against possible Soviet air attack: B-47s, B-52s, and a relatively small number of Vulcans simulated Soviet fighter/bomber attacks against New York, Chicago, and Washington, D.C. The results of the tests were classified until 1997. The Vulcan avoided USAF interceptors during the 1974 \"Giant Voice\" exercise.\nNuclear deterrent.\nAs part of Britain's independent nuclear deterrent, the Vulcan initially carried Britain's first nuclear weapon, the \"Blue Danube\" gravity bomb. \"Blue Danube\" was a low-kiloton yield fission bomb designed before the United States detonated the first hydrogen bomb. These were supplemented by U.S.-owned \"Mk 5\" bombs (made available under the Project E programme) and later by the British \"Red Beard\" tactical nuclear weapon. The UK had already embarked on its own hydrogen bomb programme, and to bridge the gap until these were ready the V-bombers were equipped with an Interim Megaton Weapon based on the \"Blue Danube\" casing containing \"Green Grass\", a large pure-fission warhead of yield. This bomb was known as \"Violet Club\". Only five were deployed before the \"Green Grass\" warhead was incorporated into a developed weapon as \"Yellow Sun Mk.1.\"\nThe later \"Yellow Sun Mk 2\", was fitted with \"Red Snow\", a British-built variant of the U.S. W28 warhead. \"Yellow Sun Mk 2\" was the first British thermonuclear weapon to be deployed, and was carried on both the Vulcan and Handley Page Victor. The Valiant retained U.S. nuclear weapons assigned to SACEUR under the dual-key arrangements. \"Red Beard\" was positioned in Singapore for use by Vulcan and Victor bombers. From 1963, three squadrons of Vulcan B.2s and two squadrons of Victor B.2s were armed with the \"Blue Steel\" missile, a rocket-powered stand-off bomb, which was also fitted with the yield \"Red Snow\" warhead.\nOperationally, RAF Bomber Command and the SAC cooperated in the Single Integrated Operational Plan to ensure coverage of all major Soviet targets from 1958; 108 of the RAF's V-bombers were assigned targets under the plan by the end of 1959. From 1962 onwards, one aircraft per squadron in every RAF bomber base were armed with nuclear weapons and on standby permanently under the principle of Quick Reaction Alert (QRA). Vulcans on QRA were to be airborne within four minutes of receiving an alert, as this was identified as the amount of time between warning of a USSR nuclear strike being launched and it arriving in Britain. The closest the Vulcan came to taking part in potential nuclear conflict was during the Cuban Missile Crisis in October 1962, where Bomber Command was moved to Alert Condition 3, an increased state of preparedness from normal operations; however, it stood down in early November.\nThe Vulcans were intended to be equipped with the Skybolt missile to replace the Blue Steel, with Vulcan B.2s carrying two Skybolts under the wings. The last 28 B.2s were modified on the production line to fit pylons to carry the Skybolt. A B.3 variant with increased wingspan to carry up to six Skybolts was proposed in 1960. When the Skybolt missile system was cancelled by U.S. President John F. Kennedy on the recommendation of his Secretary of Defense, Robert McNamara in 1962, precipitating the Skybolt Crisis, \"Blue Steel\" was retained. To supplement it until the Royal Navy took on the deterrent role with Polaris SLBM-equipped submarines, the Vulcan bombers adopted a new mission profile of flying high during clear transit, dropping down low to avoid enemy defences on approach, and deploying a parachute-retarded bomb, the WE.177B. Before the availability of the WE 177 laydown mode the attack profile involved a pop up manoeuvre to allow the Yellow Sun time to arm. Initially this was a pop up to 11,000 feet and release in level flight. To further reduce exposure to SAM at release in the climb was developed. Depending on the Mk 1 or Mk 2 and the different engine configurations the aircraft was to pop up at a specified distance from the target, climb at a set angle, and release the weapon at 10,500 feet. In the case of the Mk 2 Vulcan with 301 series engines and the Yellow Sun this was 18,450 yards and 15\u00b0. Since the aircraft had been designed for high-altitude flight, at low altitudes training cruise speed was limited to 240 knots with a dash not exceeding 350 knots during practice attacks in order to preserve airframe fatigue life. War missions were planned with a low altitude speed of 325 knots and an upper speed of 375 knots. The aircraft was also cleared for a once only dash to 415 knots for 10 minutes. This was not stated but would probably been the final phase of a laydown attack. On one occasion prior to proper Release to Service clearance the author experienced just over 400 knots. We could certainly feel the additional turbulence. Subsequently, on a Bomber Command Trial we flew for 350 knots at 10,000 feet prior to general release clearance at that speed. RAF Air Vice Marshal Ron Dick, a former Vulcan pilot, said \"it is [thus] questionable whether it could have been effective flying at low level in a war against ... the Soviet Union.\"\nAfter the British Polaris submarines became operational and Blue Steel was taken out of service in 1970, the Vulcan continued to carry WE.177B in a tactical nuclear strike role as part of the British contribution to Europe's standing NATO forces, although they no longer held aircraft at 15 minutes' readiness in peacetime. Two squadrons were also stationed in Cyprus as part of the Near East Air Force and assigned to Central Treaty Organization in a strategic strike role. With the eventual demise of the WE.177B and the Vulcan bombers, the Blackburn Buccaneer, SEPECAT Jaguar, and Panavia Tornado continued with the WE.177C until its retirement in 1998. While not a like-for-like replacement, the multi-role Tornado interdictor/strike bomber is the successor for the roles previously filled by the Vulcan.\nConventional role.\nAlthough in operational use the Vulcan typically carried various nuclear armaments, the type also had a secondary conventional role. While performing conventional combat missions, the Vulcan could carry up to 21\u00a0 bombs inside its bomb bay. From the 1960s, the various Vulcan squadrons routinely conducted conventional training missions; the aircrews were expected to be able to perform conventional bombing missions, in addition to the critical nuclear strike mission. Conventional bombing was practised with different profiles depending on expected target defences. Against a target defended by aircraft and SAM the most likely profile would have been a laydown profile with a retard tail unit (Mk 117). Where the target was defended by Soviet SA2 Guideline missiles the aircraft could pop up to 2,500 feet, below the SAM and above the self-damage limit for free-fall bombs. If the target defence was air defence artillery the attack might pop up to 8,000 feet to minimise risk from guns and with the advantage that automatic bombing computation with the Navigation and Bombing System Calc 3 could be used.\nThe Vulcan's only combat missions took place towards the end of the type's service in 1982. During the Falklands War, the Vulcan was deployed against Argentinian forces which had occupied the Falkland Islands. The missions performed by the Vulcan became known as the \"Black Buck\" raids, each aircraft had to fly from Ascension Island to reach Stanley on the Falklands. Victor tankers conducted the necessary air-to-air refuelling for the Vulcan to cover the distance involved; approximately of fuel was used in each mission.\nEngineering work to prepare the five Vulcans that would conduct the missions began on 9 April. Each aircraft required modifications to the bomb bay, the reinstatement of the long-out-of-use in-flight refuelling system, the installation of a new navigational system derived from the Vickers VC10, and the updating of several onboard electronics. Underneath the wings, new pylons were fitted to carry an ECM pod and Shrike antiradar missiles at wing hardpoint locations.\nOn 1 May, the first mission was conducted by a single Vulcan (XM607) that flew over Port Stanley and dropped its bombs on the airfield, concentrating on the single runway, with one direct hit, making it unsuitable for fighter aircraft. The Vulcan's mission was quickly followed up by strikes against anti-air installations, flown by British Aerospace Sea Harriers from Royal Navy aircraft carriers. A further two missions saw missiles launched against radar installations and two additional missions were cancelled. At the time, these missions held the record for the world's longest-distance raids. The Vulcans' ECM systems proved to be effective at jamming Argentine radars; while a Vulcan was within the theatre, other British aircraft in the vicinity had a reduced chance of coming under effective fire.\nOn 3 June 1982, Vulcan B.2 XM597 of No. 50 Squadron took part in the \"Black Buck 6\" mission against Argentinian radar sites at Stanley airfield on the Falkland Islands. While attempting to refuel for its return journey to Ascension Island, the probe broke, leaving the Vulcan with insufficient fuel, forcing a diversion to Gale\u00e3o Air Force Base, Rio de Janeiro, in neutral Brazil. En route, secret papers were dumped along with the two remaining AGM-45 Shrike missiles, although one failed to launch. After a mayday call, the Vulcan, escorted by Brazilian Air Force Northrop F-5 fighters, was permitted an emergency landing at Rio with very little fuel left on board. The Vulcan and her crew were detained until the end of hostilities nine days later.\nReconnaissance.\nIn November 1973, as a result of the planned closure of the Victor SR.2 equipped No. 543 Squadron, No. 27 Squadron reformed at RAF Scampton equipped with the Vulcan as a replacement in the maritime radar reconnaissance role. The squadron carried out patrols of the seas around the British Isles, including the strategically important GIUK gap between Iceland and the United Kingdom, flying at high level and using the Vulcan's H2S radar to monitor shipping. In peacetime, this could be followed up by visual identification and photography of targets of interest at low level. In the event of war, a Vulcan would leave visual identification of potential targets to Buccaneers or Canberras and could coordinate attacks by Buccaneers against hostile shipping. Though initially equipped with a number of B.2 aircraft, the Squadron eventually operated nine B.2 (MRR) aircraft (also known by the unofficial designation SR.2). The aircraft were modified for the role by removing the TFR (and its thimble radome) and adding the LORAN C radio navigation aid. The main external visual difference was the presence of a gloss paint finish, with a light grey undersurface, to protect against sea spray.\nThe squadron also inherited its secondary role of air sampling from No. 543 Squadron. This involved flying through plumes of airborne contamination and using onboard equipment to collect fallout released from both above ground and underground nuclear tests for later analysis at the Atomic Weapons Research Establishment at Aldermaston. Five aircraft had small pylons fitted to the redundant Skybolt hardpoints, which could be used to carry sampling pods modified from drop tanks. These pods would collect the needed samples on a filter, while an additional smaller \"localiser\" pod was fitted to the port wing, inboard of the main pylons.\nThe squadron disbanded at Scampton in March 1982, passing on its radar reconnaissance duties to the RAF's Nimrods.\nAerial refuelling role.\nAfter the end of the Falklands War in 1982, the Vulcan B.2 was due to be withdrawn from RAF service that year. The Falklands campaign, however, had consumed much of the airframe fatigue life of the RAF's Victor tankers. While Vickers VC10 tanker conversions had been ordered in 1979 and Lockheed TriStar tankers would be ordered after the conflict, as a stopgap measure six Vulcans were converted into single-point tankers. The Vulcan tanker conversion was accomplished by removing the jammers from the ECM bay in the tail of the aircraft and replacing them with a single hose drum unit. An additional cylindrical bomb-bay tank was fitted, giving a fuel capacity of almost .\nThe go-ahead for converting the six aircraft was given on 4 May 1982. Just 50 days after being ordered, the first Vulcan tanker, \"XH561\", was delivered to RAF Waddington. The Vulcan K.2s were operated by No. 50 Squadron, along with three Vulcan B.2s, in support of UK air defence activities until it was disbanded in March 1984.\nVulcan Display Flight.\nAfter the disbandment of No. 50 Squadron, two Vulcans continued flying with the RAF in air displays as part of the Vulcan Display Flight, based at Waddington but administered through No. 55 Squadron, based at RAF Marham. Initially displaying using XL426, in 1986 that aircraft was sold, having been replaced by XH558, which began displays in 1985. The VDF continued with XH558 until 1992, finishing operations after the Ministry of Defence determined it was too costly to run in light of budget cuts. Both aircraft subsequently entered preservation and survived, although a third, XH560, kept in reserve in the first years, was later scrapped.\nThe initial production aircraft. The first few had straight leading edges, later retrofitted with phase 2 (kinked) wings. Early examples were finished in silver, later changed to \"anti-flash\" white. Many were converted to B.1A standard 1959\u20131963. The last few unmodified B.1s in RAF service with No. 230 OCU retired by 1966. Last flight by any B.1, an engine testbed XA903, March 1979.\nThe B.1 with an ECM system in a new larger tail cone (as in B.2). Unlike the B.2, the B.1As did not undergo extensive wing strengthening for low-level flying and were withdrawn from service 1966\u201367.\nDeveloped version of the B.1. Larger, thinner wing than the B.1 (Phase 2C wing) and fitted with Olympus 201-202 engines, or Olympus 301 engines. Uprated electrics with AAPP and Ram Air Turbine (RAT). ECM similar to B.1A. TFR in nose thimble radome fitted to most aircraft in the mid-60s. New Radar warning receiver aerials on tail fin giving it a square top from the mid-1970s.\nNine B.2s converted to maritime radar reconnaissance (MRR). TFR deleted. Five aircraft were further modified for the air sampling role. Distinctive gloss finish with light grey underside.\nSix B.2s converted for air-to-air refuelling with Mark 17 hose drum unit (HDU) mounted semi-recessed in the tail cone. TFR deleted. Fitted with three bomb-bay drum tanks, it was the only mark of Vulcan that could jettison fuel in an emergency.\nProposed version, intended as a long-endurance missile carrier capable of carrying up to six Skybolt missiles on flights of up to 12 hours duration. Never built.\nVariants.\nProduction.\nA total of 134 production Vulcans were assembled at Woodford Aerodrome, 45 to the B.1 design and 89 were B.2 models, the last being delivered to the RAF in January 1965.\nOperators.\nV-Bomber dispersal airfields.\nIn the event of transition to war, the V Bomber squadrons were to deploy four aircraft at short notice to each of 26 pre-prepared dispersal airfields around the United Kingdom. In the early 1960s the RAF ordered 20 Beagle Basset communication aircraft to move the crews to dispersal airfields; the importance of these aircraft was only brief, diminishing when the primary nuclear deterrent switched to the Royal Navy's Polaris (UK nuclear programme).\nSurviving aircraft.\nSeveral Vulcans survive, housed in museums in both the United Kingdom and North America (USA and Canada). One Vulcan, XH558 (G-VLCN) \"Spirit of Great Britain\", was used as a display aircraft by the RAF as part of the Vulcan Display Flight until 1993. After being grounded, it was later restored to flight by the Vulcan To The Sky Trust and displayed as a civilian aircraft from 2008 until 2015, before being retired a second time for engineering reasons. In retirement, XH558 is to be retained at its base at Doncaster Sheffield Airport as a taxiable aircraft, a role already performed by two other survivors, XL426 (G-VJET) based at Southend Airport, and XM655 (G-VULC), based at Wellesbourne Mountford Airfield. XJ823, a B.2, can be seen at the Solway Aviation Museum at Carlisle Lake District Airport. XM607 is currently being restored at RAF Waddington, where it has been gate guardian since being retired. XM594 is on display at the Newark Air Museum, Newark, Nottinghamshire, England.\nSpecifications (B.1).\n\"Data from\" Polmar, LamingGeneral characteristics* Crew: 5 (pilot, co-pilot, AEO, navigator radar, navigator plotter)* Aspect ratio: * Airfoil: root: NACA 0010 mod.; tip: NACA 0008 mod.* Fuel capacity: ;\nPerformance* Maximum speed: Mach 0.96* Endurance: * g limits: * Roll rate: * Maximum glide ratio: * Thrust/weight: 0.31\nArmament\n * 21 \u00d7 1,000 pounds (454\u00a0kg) of conventional bombs\nSee also.\nAircraft of comparable role, configuration, and era\nRelated lists\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "44071", "revid": "50548976", "url": "https://en.wikipedia.org/wiki?curid=44071", "title": "M61 Vulcan", "text": "20 mm Gatling-type rotary cannon\nThe M61 Vulcan is a hydraulically, electrically, or pneumatically driven, six-barrel, air-cooled, electrically fired Gatling-style rotary cannon which fires rounds at an extremely high rate (typically 6,000 rounds per minute). The M61 and its derivatives have been the principal cannon armament of United States military fixed-wing aircraft for over sixty years.\nThe M61 was originally produced by General Electric. After several mergers and acquisitions, it is produced by General Dynamics as of 2000[ [update]]. It is also manufactured under license in Japan by Sumitomo Heavy Industries for Japan's Self-Defense Force and by SNT Dynamics in South Korea.\nDevelopment.\nAt the end of World War II, the United States Army Air Forces began to consider new directions for future military aircraft guns. The higher speeds of jet-powered fighter aircraft meant that achieving an effective number of hits would be extremely difficult without a much higher volume of fire. While captured German designs (principally the Mauser MG 213C) showed the potential of the single-barrel revolver cannon, the practical rate of fire of such a design was still limited by ammunition feed and barrel wear concerns. The Army wanted something more advanced, combining an extremely high rate of fire with exceptional reliability.\nIn 1947, the Air Force became a separate branch of the military. The new Air Force made a request for a new aircraft gun. A lesson of World War II air combat was that German, Italian, and Japanese fighters could attack American aircraft from long range with their cannon main armament. American fighters with .50\u00a0caliber (12.7\u00a0mm) main armament, such as the P-51 and P-47, had to be close to enemy aircraft in order to hit and damage them. The Hispano cannon carried by the P-38 and P-61, while formidable against propeller-driven planes, had a relatively low rate of fire in the age of jets, while other cannons were notoriously unreliable.\nIn response to this requirement, the Armament Division of General Electric resurrected an old idea: the multi-barrel Gatling gun. The original Gatling gun had fallen out of favor because of the need for an external power source to rotate the barrel assembly, but the new generation of turbojet-powered fighters offered sufficient electric power to operate the gun, and electric operation was more reliable than gas-operated reloading.\nWith multiple barrels, the rate of fire per barrel could be lower than a single-barrel revolver cannon while providing a greater overall rate of fire. The idea of powering a Gatling gun from an external electric power source was not a novel idea at the end of World War II, as Richard Jordan Gatling himself had done just that with a patent he filed in 1893.\nDuring World War I, a similar 12-barreled Fokker-Leimberger aircraft rotary machine gun, powered by either the aircraft engine or an electric motor, had been under development by the German Empire.\nIn 1946, the Army issued General Electric a contract for \"Project Vulcan\", a six-barrel weapon capable of firing 7,200 rounds per minute (rpm). Although European designers were moving towards heavier weapons for better hitting power, the U.S. initially concentrated on a powerful cartridge designed for a pre-war anti-tank rifle, expecting that the cartridge's high muzzle velocity would be beneficial for improving hit ratios on high-speed targets. The first GE prototypes of the caliber T45 were ground-fired in 1949; it achieved 2,500\u00a0rpm, which was increased to 4,000\u00a0rpm by 1950. Due to air combat experience in the Korean War, the USAF decided that high velocity alone might not be sufficient to ensure target destruction and tested alternatives based on the caliber cartridge. These variants of the T45 were known as the T171 and T150 respectively and were first tested in 1952. Eventually, the standard 20\u00d7102\u00a0mm cartridge was determined to have the desired balance of projectile/explosive mass and muzzle velocity, resulting in an optimum balance of range, accuracy and kinetic energy on target.\nThe development of the Lockheed F-104 Starfighter revealed that the T171 Vulcan (later redesignated \"M61\") suffered problems with its linked ammunition, being prone to misfeed and presenting a foreign object damage hazard with discarded links. A linkless ammunition feed system was developed for the upgraded \"M61A1\", which subsequently became the standard cannon armament of U.S. fighters.\nIn 1993, General Electric sold its aerospace division, including GE Armament Systems along with the design and production tooling for the M61 and GE's other rotary cannon, to Martin Marietta. After Martin's merger with Lockheed, the rotary cannon became the responsibility of Lockheed Martin Armament Systems. Lockheed Martin Armament Systems was later acquired by General Dynamics, which produces the M61 and its variants as of 2000[ [update]].\nDescription.\nEach of the cannon's six barrels fires once in turn during each revolution of the barrel cluster. The multiple barrels provide both a very high rate of fire\u2014around 100 rounds per second\u2014and contribute to prolonged weapon life by minimizing barrel erosion and heat generation. The average time between jams or failures is in excess of 10,000 rounds, making it an extremely reliable weapon. The success of the Vulcan Project and its progeny, the very-high-speed Gatling gun, has led to guns of the same configuration being referred to as \"Vulcan cannons\", which can sometimes confuse nomenclature on the subject.\nMost aircraft versions of the M61 are hydraulically driven and electrically primed. The gun rotor, barrel assembly and ammunition feed system are rotated by a hydraulic drive motor through a system of flexible drive shafts. The round is fired by an electric priming system where an electric current from a firing lead passes through the firing pin to the primer as each round is rotated into the firing position.\nThe self-powered version, the GAU-4 (called M130 in Army service), is gas-operated, tapping gun gas from three of the six barrels to operate the gun gas-driven mechanism. The self-powered Vulcan weighs about more than its electric counterpart, but requires no external power source to operate, except for an electric inertia starter to initiate gun rotation, allowing the first rounds to be chambered and fired.\nThe initial M61 used linked ammunition, but the ejection of spent links created considerable (and ultimately insuperable) problems. The original weapon was soon replaced by the M61A1, with a linkless feed system. Depending on the application, the feed system can be either single-ended (ejecting spent cases and unfired rounds) or double-ended (returning casings back to the magazine). A disadvantage of the M61 is that the bulk of the weapon, its feed system, and ammunition drum make it difficult to fit it into a densely packed airframe.\nThe feed system must be custom-designed for each application, adding to the complete weapon. Most aircraft installations are double-ended, because the ejection of empty cartridges can cause a foreign-object damage hazard for jet engines and because the retention of spent cases assists in maintaining the center of gravity of the aircraft. The first aircraft to carry the M61A1 was the C model of the F-104, starting in 1959.\nA lighter version of the Vulcan developed for use on the F-22 Raptor, designated M61A2, is mechanically the same as the M61A1, but with thinner barrels to reduce overall weight to . The rotor and housing have also been modified to remove any piece of metal not absolutely needed for operation and replaces some metal components with lighter-weight materials. The F/A-18E/F Super Hornet also uses this version.\nThe Vulcan's rate of fire is typically 6,000 rounds per minute, although some versions (such as that of the AMX and the F-106 Delta Dart) are limited to a lower rate, and others (A-7 Corsair, F-15 Eagle) have a selectable rate of fire of either 4,000 or 6,000\u00a0rounds per minute. The M61A2's lighter barrels allow a somewhat higher rate of fire, up to 6,600\u00a0rounds per minute.\nAmmunition.\nPractically no powered rotary cannon is supplied with sufficient ammunition for a full minute of firing, due to its weight (at 6,000\u00a0rpm, the projectiles alone would represent a mass of about for one minute of firing; and by including the brass shell, filling and primer the weight is slightly double that at ). In order to avoid using the 600 to 1,000 rounds carried by aircraft all at once, a burst controller is generally used to limit the number of rounds fired at each trigger pull. Bursts of from two or three up to 40 or 50 can be selected. The size of the airframe and available internal space limits the size of the ammunition drum and thus limits the ammunition capacity. When vehicle-mounted, the only limiting factor is the vehicle's safe carry weight, so commensurately larger ammo storage is available.\nUntil the late 1980s, the M61 primarily used the M50 series of ammunition in various types, typically firing a projectile at a muzzle velocity of about . A variety of armor-piercing incendiary (API), high-explosive incendiary (HEI), and training rounds are available.\nA new PGU-28/B round was developed in the mid-1980s. It is a semi-armor-piercing high-explosive incendiary (SAPHEI) round, providing improvements in range, accuracy, and power over the preceding M56A3 HEI round. The PGU-28/B is a \"low-drag\" round designed to reduce in-flight drag and deceleration, and has a slightly increased muzzle velocity of . However, the PGU-28/B has not been without problems. A 2000 USAF safety report noted 24 premature detonation mishaps (causing serious damage in many cases) in 12 years with the SAPHEI round, compared to only two such mishaps in the entire recorded history of the M56 round. The report estimated that the PGU-28/B had a potential failure rate 80 times higher than USAF standards permit. Due to safety issues, it was limited to emergency wartime use in 2000.\nThe main types of combat rounds and their main characteristics are listed in the table:\nApplications and first combat use.\nThe Vulcan was first used in aerial combat on 4 April 1965, when four North Vietnamese Vietnam People's Air Force (VPAF) MiG-17s) attacked a force of 10 North American F-100 Super Sabres (two of which were assigned weather reconnaissance duties) escorting 48 Vulcan-armed and \"bomb-laden\" F-105 Thunderchiefs, shooting down two of the latter. The MiG leader and only survivor from the four MiGs, Captain Tran Hanh, reported that U.S. jets had pursued them and that F-105s had shot down three of his aircraft, killing lieutenants Pham Giay, Le Minh Huan and Tran Nguyen Nam. Captain Donald Kilgus, piloting an F-100, received an official probable kill with his four M39 20\u00a0mm cannons during the engagement; however no other US pilot reported destroying any MiGs during the battle, leaving open the possibility that at least two of the MiG-17s may have been downed by their own anti-aircraft fire.\nThe first confirmed Vulcan gun kill occurred on 29 June 1966 when Major Fred Tracy, flying his F-105 with the 421st TFS, fired 200 rounds of 20\u00a0mm into a MiG-17 that had just fired a 23\u00a0mm shell which entered one side of his cockpit and exited the other. When the VPAF MiG flew in front of him after making its pass, Tracy opened fire on it.\nThe gun was installed in the Air Force's A-7D version of the LTV A-7 Corsair II where it replaced the earlier United States Navy A-7's Colt Mk 12 cannon and was adopted by the Navy on the A-7C and A-7E. It was integrated into the newer F-4E Phantom II variants. The F-4 was originally designed without a cannon as it was believed that missiles had made guns obsolete. Combat experience in Vietnam showed that a gun could be more effective than guided missiles in many combat situations and that an externally carried gun pod was less effective than an internal gun; the first generation of gun pods such as the SUU-16 were not oriented with the sights of the fighter. The improved pods were self-powered and properly synchronized to the sights, while the USAF versions of the F-4 were hastily fitted with internal M61 cannons in a prominent fairing under the nose, well before the war ended (Navy Phantoms never received cannons, continuing to rely on air-to-air missiles alone). The next generation of fighters built post-Vietnam incorporated the M61 gun internally.\nThe Vulcan was later fitted into the weapons bay of some Convair F-106 Delta Dart and General Dynamics F-111 Aardvark models. It was also adopted as standard in the \"teen\"-series air superiority fighters: the Grumman F-14 Tomcat, the McDonnell Douglas F-15 Eagle, General Dynamics F-16 Fighting Falcon, and McDonnell Douglas F/A-18 Hornet. Other aircraft include the Italian/Brazilian AMX International AMX (on Italian aircraft only), and the F-22 Raptor. It was fitted in a side-firing installation on the Fairchild AC-119 and some marks of the Lockheed AC-130 gunships, and was used in the tail turrets of both the Convair B-58 Hustler and Boeing B-52H Stratofortress bombers. Japan's Mitsubishi F-1 carried one internally mounted JM61A1 Vulcan with 750 rounds.\nTwo gun pod versions, the SUU-16/A (also designated M12 by the US Army) and improved SUU-23/A (US Army M25), were developed in the 1960s, often used on gunless versions of the F-4. The SUU-16/A uses the electric M61A1 with a ram-air turbine to power the motor. This proved to cause serious aerodynamic drag at higher speeds, while speeds under did not provide enough airflow for the maximum rate of fire.\nThe subsequent SUU-23/A uses the GAU-4/A self-powered Vulcan, with an electric inertia starter to bring it up to speed. Both pods ejected empty cases and unfired rounds rather than retaining them. Both pods contained 1,200 rounds of ammunition, with a loaded weight of respectively. During service in the Vietnam War, the pods proved to be relatively inaccurate: the pylon mounting was not rigid enough to prevent deflection when firing, and repeated use would misalign the pod on its pylon, making matters worse.\nA variant with much shorter barrels, designated the M195, was also developed for use on the M35 Armament Subsystem as used on the AH-1G Cobra helicopter. This variant fed from ammunition boxes fitted to the landing skid and was developed to provide the AH-1 helicopter with a longer-range suppressive fire system before the adoption of the M97 universal turret mounting the M197 cannon.\nThe M61 is also the basis of the US Navy Mk 15 Phalanx close-in weapon system and the M163 VADS Vulcan Air Defense System, using the M168 variant.\nOn 13 December 2024, the Ukrainian Air Force claimed an F-16 shot down 6 Russian cruise missiles. Two were shot down with \u201cmedium-range missiles\u201d, another two with \u201cshort-range missiles\u201d and finally two with the M61 Vulcan 20\u00a0mm cannon. The pilot wasn't identified for security reasons but said: \"A few bursts from the cannon \u2014 and an explosion... then another one! 'A secondary detonation,' I thought, but, as it turned out, there were two missiles,\" \nSee also.\nSoviet Union/Russian Federation/CIS\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44073", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=44073", "title": "Electronic configuration", "text": ""}
{"id": "44075", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=44075", "title": "Tumour", "text": ""}
{"id": "44077", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=44077", "title": "Captains Courageous", "text": "1897 adventure novel by Rudyard Kipling\nCaptains Courageous: A Story of the Grand Banks is an 1897 novel by Rudyard Kipling that follows the adventures of fifteen-year-old Harvey Cheyne Jr., the spoiled son of a railroad tycoon, after he is saved from drowning by an American fishing schooner in the North Atlantic. The novel originally appeared as a serialisation in \"McClure's\", beginning with the November 1896 edition with the last instalment appearing in May 1897. In that year, it was published in its entirety as a novel, first in the United States by Doubleday, and a month later in the United Kingdom by Macmillan. It is Kipling's only novel set entirely in North America. In 1900, Teddy Roosevelt extolled the book in his essay \"What We Can Expect of the American Boy\", praising Kipling for describing \"in the liveliest way just what a boy should be and do\".\nThe book's title comes from the ballad \"Mary Ambree\", which starts, \"Then captains courageous, whom death could not daunt\". Kipling had previously used the same title for an article on businessmen as the new adventurers, published in \"The Times\" on 23 November 1892.\nPlot.\nProtagonist Harvey Cheyne Jr. is the spoiled son of a wealthy California railroad magnate. Washed overboard from a transatlantic steamship and rescued by the crew of the fishing schooner \"We're Here\", off the Grand Banks of Newfoundland, Harvey can neither persuade them to take him quickly to port, nor convince them of his wealth. Harvey accuses the captain, Disko Troop, of taking his money (which is revealed to be on the deck from which Harvey fell). Troop bloodies his nose, but takes him in as a boy on the crew until they return to port. Harvey comes to accept his situation.\nThrough a series of trials and adventures, Harvey, with the help of the captain's son, Dan Troop, becomes acclimated to the fishing lifestyle, and even skillful, such as becoming responsible for the ship's accounts of its catch. Great stories of the cod fishery with references to New England whaling and 19th-century steam and sailing are intertwined with the \"We're Here\"'s adventures during a season at sea. Eventually, the \"We're Here\" returns to port and Harvey wires his parents, who immediately hasten to Boston, Massachusetts, and thence to the fishing town of Gloucester to recover him. The Cheynes are amazed by their son's newfound maturity, and reward the Portuguese seaman Manuel, who initially rescued Harvey. Harvey's father hires Dan to work on his prestigious tea clipper fleet, and Harvey goes to Stanford to prepare for taking over his father's shipping lines.\nNotes.\nThe book was written during Kipling's time living in Brattleboro, Vermont. Kipling recalled in his autobiography:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Now our Dr. [James] Conland had served in [the Gloucester] fleet when he was young. One thing leading to another, as happens in this world, I embarked on a little book which was called \"Captains Courageous\". My part was the writing; his the details. This book took us (he rejoicing to escape from the dread respectability of our little town) to the shore-front, and the old T-wharf of Boston Harbour, and to queer meals in sailors' eating-houses, where he renewed his youth among ex-shipmates or their kin. We assisted hospitable tug-masters to help haul three- and four-stick schooners of Pocahontas coal all round the harbour; we boarded every craft that looked as if she might be useful, and we delighted ourselves to the limit of delight. ... Old tales, too, he dug up, and the lists of dead and gone schooners whom he had loved, and I revelled in profligate abundance of detail\u2014not necessarily for publication but for the joy of it. ...I wanted to see if I could catch and hold something of a rather beautiful localised American atmosphere that was already beginning to fade. Thanks to Conland I came near this.\nKipling also recalled:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe resulting account, in Chapter 9, of the Cheynes' journey from San Diego to Boston, is a classic of railway literature. The couple travel in the Cheynes' private rail car, the \"Constance\", and are taken from San Diego to Chicago as a special train, hauled by sixteen locomotives in succession. It takes precedence over 177 other trains. \"Two and one-half minutes would be allowed for changing engines; three for watering and two for coaling\". The \"Constance\" is attached to the scheduled express \"New York Limited\" to Buffalo, New York, and transferred to the New York Central for the trip across the state to Albany. Switched to the Boston and Albany Railroad, the Cheynes complete the trip to Boston in their private car, with the entire cross-country run taking 87 hours 35 minutes.\nKipling also recalled:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;My characters arrived triumphantly; and, then, a real live railway magnate was so moved after reading the book that he called out his engines and called out his men, hitched up his own private car, and set himself to beat \"my\" time on paper over the identical route, and succeeded.\nDisko Troop claims to receive his given name for his birth on board his father's ship near Disko Island on the west coast of Greenland. His crewman, \"Long Jack\", once calls him \"Discobolus\".\nA claim that Kipling used the United States Fish Commission fisheries research ship as the model for \"We\u2032re Here\" is unproven.\nFilm, TV, theatrical, or other adaptations.\n\"Captains Courageous\" has been adapted for film three times:\nMusical theatre:\nOther adaptations:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44078", "revid": "22651524", "url": "https://en.wikipedia.org/wiki?curid=44078", "title": "You Can't Take it With You", "text": ""}
{"id": "44081", "revid": "10044298", "url": "https://en.wikipedia.org/wiki?curid=44081", "title": "Oceanology", "text": "Scientific study of the ocean"}
{"id": "44082", "revid": "18336458", "url": "https://en.wikipedia.org/wiki?curid=44082", "title": "Marine science", "text": ""}
{"id": "44083", "revid": "39446300", "url": "https://en.wikipedia.org/wiki?curid=44083", "title": "Utopian fiction", "text": ""}
{"id": "44084", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=44084", "title": "Prepositional phrase", "text": ""}
{"id": "44085", "revid": "211905", "url": "https://en.wikipedia.org/wiki?curid=44085", "title": "John Byron", "text": "Royal Navy officer, explorer and colonial administrator\nVice-Admiral of the White John Byron (8 November 1723 \u2013 1 April 1786) was a Royal Navy officer, explorer and colonial administrator. He earned the nickname \"Foul-Weather Jack\" in the British press due to his frequent encounters with bad weather at sea. As a midshipman, Byron sailed in a squadron under George Anson on his voyage around the world, though Byron's ship, HMS \"Wager\", made it only to southern Chile, where it was wrecked. He returned to England with the captain of the ship. \nByron was appointed governor of Newfoundland following Hugh Palliser, who left in 1768. He circumnavigated the world as a commodore with his own squadron in 1764\u20131766. Byron fought in several battles of the Seven Years' War and the American War of Independence, and rose to vice admiral before his death in 1786. His grandsons include the poet Lord Byron and the admiral and explorer George Byron, 7th Baron Byron. One of Byron's great-granddaughters was the mathematician and informatics pioneer Ada Lovelace.\nEarly career.\nByron was the second son of William Byron, 4th Baron Byron and Frances Berkeley, the daughter of William, 4th Baron Berkeley. After studying at Westminster School he joined the Royal Navy at the age of 14, making his first voyage aboard HMS \"Romney\" in 1738\u201340.\nAnson's voyage around the world.\nIn 1740, he accompanied George Anson on his voyage around the world as a midshipman aboard one of the several ships in the squadron. \nOn 14 May 1741, HMS \"Wager\" was shipwrecked on the coast of Chile on what is now called Wager Island and Byron was one of the survivors. Under the tenuous command of Captain David Cheape, who was only promoted to the position mid-voyage following the death of his predecessor, the survivors bickered amongst themselves and split into factions. A large group of sailors, including Byron, eventually defied Cheape's authority and sailed east to Portuguese Brazil, targeting Rio Grande do Sul on the Atlantic coast. Days into the journey, Byron and several others returned to the Captain and his remaining small party. \nCheape's party consisted of 19 men after the deserters rejoined the camp. This included the surgeon Elliot and Lieutenant Hamilton, as well as Byron and fellow midshipman Alexander Campbell. They rowed up the coast but were punished by continuous rain, headwinds and waves that threatened the boats. One night while the men slept on shore, one of the boats was capsized while at anchor and was swept out to sea with its two boatkeepers. One of the men got ashore but the other drowned. As it was now impossible for them all to fit in the remaining boat, four marines were left ashore with muskets to fend for themselves. The winds prevented them from getting around the headland so they returned to pick up the marines only to find them gone. They returned to Wager Island in early February 1742. With one death on the journey, there were now 13 in the group.\nMart\u00edn Olleta, a Chono chieftain, guided the men up the coast to the Spanish settlements of Chilo\u00e9 Island so they set out again. Two men died; after burying the bodies, the six seamen rowed off in the boat never to be seen again while Cheape, Hamilton, Byron, Campbell and the dying Elliot were on shore looking for food. Olleta then agreed to take the remaining four on by canoe for their only remaining possession, a musket. It is likely the party travelled across Presidente R\u00edos Lake in inland Taitao Peninsula, a lake Chile regarded as officially discovered in 1945. Eventually they made it to be taken prisoner by the Spanish. The Spaniards treated them well and they were eventually taken to the inland capital of Santiago where they were released on parole. The Spaniards heard that Anson had been generous in the treatment of the prisoners he had taken and this kindness was returned.\nByron and the other three men stayed in Santiago till late 1744 and were offered passage on a French ship bound for Spain. Three accepted the passage. Campbell elected to take a mule across the Andes and joined the Spanish Admiral Pizarro in Montevideo on the \"Asia\" only to find Isaac Morris and the two seamen who had been abandoned in Freshwater Bay on the Atlantic coast. After time in prison in Spain, Campbell reached Britain in May 1746, followed by the other three two months later.\nIn England, the official court martial examined only the loss of the \"Wager\" in which Baynes, in nominal charge at the time, was acquitted of blame but reprimanded for omissions of duty. Disputes over what happened after the wreck were instead played out as Bulkeley and Cummins, Campbell, Morris, the cooper Young and later Byron published their own accounts, the last of which was the only one that in any way defended Cheap who had since died. Twenty-nine crew members plus seven marines made it back to England.\nByron's account of his adventures and the \"Wager\" Mutiny are recounted in \"The Narrative of the Honourable John Byron\" (1768). His book sold well enough to be printed in several editions.\nByron was appointed captain of in December 1746.\nSeven Years' War.\nIn 1760, during the Seven Years' War, Byron commanded a squadron sent to destroy the fortifications at Louisbourg, Cape Breton Island, Nova Scotia, which had been captured by the British two years before. They wanted to ensure it could not be used by the French in Canada. In July of that year he defeated the French flotilla sent to relieve New France at the Battle of Restigouche.\nCommodore, governor, and vice admiral.\nIn early 1764 the British Admiralty determined that it would require a permanent naval settlement off the South American coast, in order to resupply naval vessels seeking to enter the Pacific via Cape Horn. Captain Byron was selected to explore the South Atlantic for a suitable island upon which to establish such a settlement. The South American mainland was controlled by Spain, which was hostile to local expansion of British interests; to disguise Byron's mission it was announced that he had been appointed the new Navy Commander-in-Chief, East Indies. Byron set sail in June 1764, ostensibly to take up the East Indies post. For the voyage he was granted command of the 24-gun frigate and the 16-gun sloop .\nByron's two-vessel flotilla crossed the Atlantic over the winter of 1764 and made its way slowly down the South American coast. The Admiralty had ordered Byron to first seek Pepys Island, reputedly discovered off the Patagonian coast by the corsair Ambrose Cowley in 1683. Byron reached the co-ordinates given by Cowley in January 1765, but there was no sign of the island and the search was swiftly abandoned. On 5 February Byron reached the Patagonian settlement of Port Desire where he resupplied his vessels from the storeship HMS \"Florida\".\nBetween June 1764 and May 1766, Byron completed his own circumnavigation of the globe as captain of HMS \"Dolphin\". This was the first such circumnavigation that was accomplished in less than 2 years. His actions nearly caused a war between Great Britain and Spain, as both countries had armed fleets ready to contest the sovereignty of the Falkland Islands. Later Byron encountered islands and extant residents of the Tuamotus and Tokelau Islands, and Nikunau in the southern Gilbert Islands; he also visited Tinian in the Northern Marianas Islands. A notable member of Byron's crew was Master's Mate Erasmus Gower whom Byron chose to 'take a significant part' in the ceremony when he took possession of the Falkland Islands. Byron had examined Gower for his lieutenant's examination in 1762 and was so impressed that he chose him to accompany him on his own circumnavigation (1764\u201365) and ensured that he was appointed as lieutenant to Commander Philip Carteret immediately afterwards in the next circumnavigation (1766\u201369).\nIn 1769 he was appointed governor of Newfoundland off the mainland of Canada, an office he held for the next three years.\nHe was promoted to rear admiral on 31 March 1775. In 1779, he served as Commander-in-chief of the Leeward Islands Station during the American War of Independence. After being severely injured during a storm on his way to the West Indies, Byron unsuccessfully attacked a French fleet under the Comte d'Estaing at the Battle of Grenada in July 1779. He subsequently resigned his post and returned to England, where he suffered from poor health for the rest of his life.\nByron was briefly Commander-in-Chief, North American Station from 1 October 1779. He was made vice admiral of the white in September 1780.\nFamily.\nOn 8 September 1748 he married his first cousin Sophia Trevanion, daughter of John Trevanion of Caerhays in Cornwall and Barbara Berkeley, the sister of his mother. They had two sons and seven daughters:\nJohn was the brother of Hon. George Byron, married to Frances Levett, daughter of Elton Levett of Nottingham, a descendant of Ambrose Elton, Esq., High Sheriff of Herefordshire in 1618 and a surgeon in Nottingham.\nDeath and legacy.\nAccording to a note written by his wife Sophia to their financial agent, John Byron died on 1 April 1786 at home in Bolton Row, London (not 10 April, as subsequent biographies claim). On that date nine days later his remains were buried in the Berkeley family vault situated beneath the chancel of the Church of St Mary the Virgin, Twickenham.\nJohn's life was a great inspiration for his grandson the poet George Gordon Byron, though they never met. The poet both drew from his grandfather's experiences in his writing, using his 'Narrative' for the shipwreck scene in \"Don Juan\", and wrote of the kinship he felt in having such a turbulent, unlucky life: he wrote in an epistle to his half-sister Augusta Leigh that \"he had no rest at sea, nor I on shore\".\nIn fiction.\nJohn Byron's experiences in the Anson voyage form the basis of the novel \"The Unknown Shore\" by Patrick O'Brian. It closely follows Byron's account in \"The Narrative of the Honourable John Byron\" (1768).\nIn \"The Dark Design\" by Philip Jos\u00e9 Farmer, John Byron is a crewmember of the schooner \"The Razzle Dazzle\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44086", "revid": "1063167", "url": "https://en.wikipedia.org/wiki?curid=44086", "title": "Lew Wallace", "text": "American general, politician, and author (1827\u20131905)\nLewis Wallace (April 10, 1827\u00a0\u2013 February 15, 1905) was an American lawyer, Union general in the American Civil War, governor of New Mexico Territory, politician, diplomat, artist, and author from Indiana. Among his novels and biographies, Wallace is best known for his historical adventure story, \"\" (1880), a bestselling novel that has been called \"the most influential Christian book of the nineteenth century.\"\nWallace's military career included service in the Mexican\u2013American War and the American Civil War. He was appointed Indiana's adjutant general and commanded the 11th Indiana Infantry Regiment. Wallace, who attained the rank of major general, participated in the Battle of Fort Donelson, the Battle of Shiloh, and the Battle of Monocacy. He also served on the military commission for the trials of the Lincoln assassination conspirators, and presided over the trial of Henry Wirz, the Confederate commandant of the Andersonville prison camp.\nWallace resigned from the U.S. Army in November 1865 and briefly served as a major general in the Mexican Army, before returning to the United States. Wallace was appointed governor of the New Mexico Territory (1878\u20131881) and served as U.S. minister to the Ottoman Empire (1881\u20131885). Wallace retired to his home in Crawfordsville, Indiana, where he continued to write until his death in 1905.\nEarly life and education.\nLewis \"Lew\" Wallace was born on April 10, 1827, in Brookville, Indiana. He was the second of four sons born to Esther French Wallace (n\u00e9e Test) and David Wallace. Lew's father, a graduate of the U.S. Military Academy in West Point, New York, left the military in 1822 and moved to Brookville, where he established a law practice and entered Indiana politics. David served in the Indiana General Assembly and later as the state's lieutenant governor, and governor, and as a member of Congress. Lew Wallace's maternal grandfather was circuit court judge and Congressman John Test.\nIn 1832 the family moved to Covington, Indiana, where Lew's mother died from tuberculosis on July 14, 1834. In December 1836, David married nineteen-year-old Zerelda Gray Sanders Wallace, who later became a prominent suffragist and temperance advocate. In 1837, after David's election as governor of Indiana, the family moved to Indianapolis.\nLew began his formal education at the age of six at a public school in Covington, but he much preferred the outdoors. Wallace had a talent for drawing and loved to read, but he was a discipline problem at school. In 1836, at the age of nine, Lew joined his older brother in Crawfordsville, Indiana, where he briefly attended the preparatory school division of Wabash College, but soon transferred to another school more suitable for his age. In 1840, when Wallace was thirteen, his father sent him to a private academy at Centerville, Indiana, where his teacher encouraged Lew's natural affinity for writing. Wallace returned to Indianapolis the following year.\nSixteen-year-old Lew went out to earn his own wages in 1842, after his father refused to pay for more schooling. Wallace found a job copying records at the Marion County clerk's office and lived in an Indianapolis boardinghouse. He also joined the Marion Rifles, a local militia unit, and began writing his first novel, \"The Fair God\", but it was not published until 1873. Wallace said in his autobiography that he had never been a member of any organized religion, but he did believe \"in the Christian conception of God\".\nBy 1846, at the start of the Mexican\u2013American War, the nineteen-year-old Wallace was studying law at his father's law office, but left that pursuit to establish a recruiting office for the Marion Volunteers in Indianapolis. He was appointed a second lieutenant, and on June 19, 1846, mustered into military service with the Marion Volunteers (also known as Company H, 1st Indiana Volunteer Infantry). Wallace rose to the position of regimental adjutant and the rank of first lieutenant while serving in the army of Zachary Taylor, but Wallace personally did not participate in combat. Wallace was mustered out of the volunteer service on June 15, 1847, and returned to Indiana, where he intended to practice law. After the war, Wallace and William B. Greer operated a Free Soil newspaper, \"The Free Soil Banner,\" in Indianapolis.\nMarriage and family.\nIn 1848 Wallace met Susan Elston at the Crawfordsville home of Henry S. Lane, Wallace's former commander during the Mexican War. Susan was the daughter of Major Isaac Compton Elston, a wealthy Crawfordsville merchant, and Maria Akin Elston, whose family were Quakers from upstate New York. Susan accepted Wallace's marriage proposal in 1849, and they were married in Crawfordsville on May 6, 1852. The Wallaces had one son, Henry Lane Wallace, who was born on February 17, 1853.\nEarly law and military career.\nWallace was admitted to the bar in February 1849, and moved from Indianapolis to Covington, Indiana, where he established a law practice. In 1851 Wallace was elected prosecuting attorney of Indiana's 1st congressional district, but he resigned in 1853 and moved his family to Crawfordsville, in Montgomery County, Indiana. Wallace continued to practice law and was elected as a Democrat to a two-year term in the Indiana Senate in 1856. From 1849 to 1853, his office was housed in the Fountain County Clerk's Building.\nWhile living in Crawfordsville, Wallace organized the Crawfordsville Guards Independent Militia, later called the Montgomery Guards. During the winter of 1859\u201360, after reading about elite units of the French Army in Algeria, Wallace adopted the Zouave uniform and their system of training for the group. The Montgomery Guards would later form the core of his first military command, the 11th Indiana Volunteer Infantry, during the American Civil War.\nCivil War service.\nWallace, a staunch supporter of the Union, became a member of the Republican party, and began his full-time military career soon after the Confederate attack on Fort Sumter, South Carolina, on April 12, 1861. Indiana's governor, the Republican Oliver P. Morton, asked Wallace to help recruit Indiana volunteers for the Union army. Wallace, who also sought a military command, agreed to become the state's adjutant general on the condition that he would be given command of a regiment of his choice. Indiana's quota of six regimental units was filled within a week, and Wallace took command of the 11th Indiana Volunteer Infantry Regiment, which was mustered into the Union army on April 25, 1861. Wallace received his formal commission as a colonel in the Union army the following day.\nOn June 5, 1861, Wallace went with the 11th Indiana to Cumberland, Maryland, and on June 12, the regiment won a minor battle at Romney, Virginia, (in present-day West Virginia). The rout boosted morale for Union troops and led to the Confederate evacuation of Harpers Ferry on June 18. On September 3, 1861, Wallace was promoted to brigadier general of U.S. Army volunteers and given command of a brigade.\nForts Henry and Donelson.\nOn February 4 and 5, 1862, prior to the advance against Fort Henry, Union troops under the command of Brig. Gen. Ulysses S. Grant and a flotilla of Union ironclads and timberclad gunboats under the command of Flag Officer Andrew Hull Foote made their way toward the Confederate fort along the Tennessee River in western Tennessee. Wallace's brigade, which was attached to Brig. Gen. Charles F. Smith's division, was ordered to occupy Fort Heiman, an uncompleted Confederate fort across the river from Fort Henry. Wallace's troops secured the deserted fort and watched the Union attack on Fort Henry from their hilltop position. On February 6, after more than an hour of bombardment from the Union gunboats, Confederate Brig. Gen. Lloyd Tilghman surrendered Fort Henry to Foote.\nGrant's superior, Maj. Gen. Henry W. Halleck, was concerned that Confederate reinforcements would try to retake the two forts when the Union troops moved overland toward Fort Donelson, so Wallace was left in command at Fort Henry to keep the forts secure. Displeased to have been left behind, Wallace prepared his troops to move out at a moment's notice. The order came at midnight on February 13. Wallace arrived in front of Fort Donelson the following day and was placed in charge of the newly forming 3rd Division. Many of the men in the division were untested reinforcements. Wallace's three brigades took up position in the center of the Union line, facing Fort Donelson.\nDuring the fierce Confederate assault on February 15, and with Grant's absence from the battlefield, Wallace acted on his own initiative to send Cruft's brigade to reinforce the beleaguered division of Brig. Gen. John A. McClernand, despite orders from Grant to hold his position and prevent the enemy from escaping and without Grant's authority to take the offensive. With the Confederates continuing to advance, Wallace led a second brigade to the right and engaged the Confederates with infantry and artillery. Wallace's decision stopped their forward movement and was key in stabilizing a defensive line for the Union troops. After the Confederate assault had been checked, Wallace led a counterattack that regained the lost ground on the Union right. On March 21, 1862, McClernand, C. F. Smith, and Wallace were promoted to major general in that order for their efforts. Wallace, who was age thirty-four at the time of his promotion, became the youngest major general in the Union army.\nShiloh.\nWallace's most controversial command came at the battle of Shiloh, where he continued as the 3rd Division commander under Maj. Gen. Grant. What was to become a long-standing controversy developed around the contents of Wallace's written orders on April 6, the 3rd Division's movements on the first day of battle, and its late arrival on the field. The next day, the reinforcement by Wallace's division and the juncture of Maj. Gen. Don Carlos Buell's Army of the Ohio permitted the Union forces to push back the enemy all day long to gain the victory.\nPrior to the battle, Wallace's division had been detached and was encamped near Crump's Landing, five miles downstream from Pittsburg Landing and the bulk of Grant's army. Wallace's orders were to guard the Union's rear and to cover the road leading west to Bethel Station, Tennessee, where railroad lines led to Corinth, Mississippi, to the south. To protect the road from Crump's Landing and Bethel Station, Wallace sent Col. John M. Thayer's 2nd Brigade to Stoney Lonesome, west of Crump's Landing, and the 3rd Brigade, commanded by Col. Charles Whittlesey to Adamsville, west of Crump's Landing. Col. Morgan L. Smith's 1st Brigade remained with Wallace at Crump's Landing, north of Pittsburg Landing, Tennessee.\nAround 5 a.m. on April 6, 1862, the Battle of Shiloh began in which Grant's army at Pittsburg Landing was surprised and began to be pushed back by a sudden attack from the Confederate army under Gen. Albert Sidney Johnston. Grant, who heard the early morning artillery fire, took a steamboat upriver from his headquarters at Savannah, Tennessee, briefly stopping at Crump's Landing, where he gave Wallace orders to wait, but be ready to move in any direction. Grant proceeded to Pittsburg Landing, where he arrived around 9:00 or 9:30\u00a0a.m. Grant's new orders to Wallace, which arrived between 11 and 11:30\u00a0a.m., were given verbally to Grant's quartermaster, who transcribed them before they were delivered. The written orders were lost during the battle, so their exact wording cannot be confirmed; however, most eyewitness accounts agree that Grant ordered Wallace to join the right side of the Union army, presumably in support of Brig. Gen. William Tecumseh Sherman's 5th Division, which was encamped near Shiloh Church on the morning of April 6.\nKnowledge of the area's roads played a critical role in Wallace's journey to the battlefield on April 6. In late March, after heavy rains made transportation difficult between Crump's Landing and Pittsburg Landing, Wallace's men had opened a route to Pittsburg Landing along Shunpike road, which connected to a road near Sherman's camp. Brig. Gen. W. H. L. Wallace's men at Pittsburg Landing opened the River Road (also known as the Hamburg-Savannah Road), a route farther east.\nOf the two main routes that Wallace could use to move his men to the front, he chose the Shunpike road, the more direct route to reach the right of Sherman's division near Shiloh Church. The day before the battle, Wallace wrote a letter to W. H. L. Wallace, recommending this route to reinforce the 3rd Division. Lew Wallace and his staff maintained after the battle that Grant's order did not specify Pittsburg Landing as their destination, and that it did not specify which route the 3rd Division was ordered to take. However, Grant claimed in his memoirs that he had ordered Wallace to take the route nearest to the river to reach Pittsburg Landing. Historians are divided, with some stating that Wallace's explanation is the most logical.\nAfter a second messenger from Grant arrived around noon with word to move out, Wallace's division of approximately 5,800 men began their march toward the battlefield. Between 2 and 2:30\u00a0p.m., Colonel William R. Rowley, sent by Grant, rode to where Wallace's division first was; there was only a supply wagon departing the scene. Riding on further, Rowley found Wallace along the Shunpike road at the head of his column near Clear Creek, positioned on high ground. He informed Wallace that Sherman had been forced back from Shiloh Church and was fighting closer to the river, near Pittsburg Landing. Grant had ordered Rowley to \"tell him to come up at once\" and that \"if he should require a written order of you, you will give it to him at once\". Rowley pulled Wallace off to the side and warned him of the danger that lay just ahead, exclaiming, \"Don't you know that Sherman has been driven back? Why, the whole army is within half a mile of the river, and it's a question if we are not all going to be driven into it.\" Wallace, stunned by the news, sent his cavalry ahead to assess the situation, and upon returning, it had confirmed Rowley's claim. The Union army had been pushed back so far that Wallace was heading toward the rear of the advancing Southern troops.\nWallace briefly considered attacking the Confederates, but abandoned the idea. Instead he made a controversial decision to countermarch his first two brigades along the Shunpike road, follow a crossroads to the River Road, and then move south to Pittsburg Landing. Rather than realigning his troops, so that the rear guard would be in the front, Wallace countermarched his column to maintain their original order, keeping his artillery in position to support the Union infantry on the field. After the time-consuming maneuver was completed, Wallace's troops returned to the midpoint on the Shunpike road, crossed east over a path to the River Road, and followed it south to join Grant's army on the field. Progress was slow due to the atrocious road conditions and the countermarch. Wallace's division arrived at Pittsburg Landing about 6:30\u00a0p.m., after having marched about in nearly seven hours over roads that had been left in terrible conditions by recent rainstorms and previous Union marches. They gathered at the battlefield at dusk, about 7 p.m., with the fighting basically over for the day, and took up a position on the right of the Union line.\nThe next day, April 7, Wallace's division held the extreme right of the Union line. Two of Wallace's batteries with the aid of a battery from the 1st Illinois Light Artillery were the first to attack at about 5:30\u00a0a.m. Sherman's and Wallace's troops helped force the Confederates to fall back, and by 3 p.m. the Confederates were retreating southwest, toward Corinth.\nHistorian Timothy B. Smith noted that on the second day Wallace's division sustained far fewer casualties (296) than any of Buell's three divisions. The number of casualties does not always show the effectiveness of troops. Wallace had his soldiers lie down when they were under fire, which minimized casualties. He also maneuvered his division so that it repeatedly turned the Confederate left flank. Wallace advanced his division at 6:30 am, reached the south side of Tilghman Branch about 8:00 am, and occupied a commanding ridge by 9:00 am, all with little opposition. Here he paused to wait for Union troops to appear on his left. Up to this point, Wallace's movements were slow. Once Grant's and Buell's soldiers reached the Confederate main line of defense they were stopped in heavy fighting. Noting that the Confederate left did not reach as far as Owl Creek, Wallace wheeled his division to outflank the enemy line. Finding Wallace's troops to their left and rear, the left-hand Confederate brigade hurriedly fell back. This unhinged the entire line and the Confederate troops soon retreated to a second position around noon. At around 1:00 pm, Wallace worked a few regiments around the Confederate left flank, forcing their withdrawal to a third position. After the Confederates left the battlefield, Wallace's division went the farthest south of the Union forces, but he pulled his troops back before going into camp that evening.\nShiloh controversy.\nAt first, the battle was viewed by the North as a victory; however, on April 23, after civilians began hearing news of the surprise and resulting high number of casualties, the Lincoln administration asked the Union army for further explanation. Grant, who was accused of poor leadership at Shiloh, and his superior, Halleck, tried to place the blame on Wallace by asserting that his failure to follow orders and the delay in moving up his division on April 6 had nearly cost the Union the battle.\nAfter hearing reports that Wallace refused to obey anything but \"written\" orders, an angry General Grant asserted that a division general \"ought to take his troops to wherever the firing may be, even without orders\".\nOn April 30, 1862, Halleck reorganized his army and removed Wallace and John McClernand from the front lines, placing both of them in reserve, with McClernand commanding.\nWallace's reputation and career as a military leader suffered a significant setback from controversy over Shiloh. He spent the remainder of his life trying to resolve the accusations and change public opinion about his role in the battle. On March 14, 1863, Wallace wrote a letter to Halleck that provided an official explanation of his actions. He also wrote Grant several letters and met with him in person more than once in an attempt to vindicate himself. On August 16, 1863, Wallace wrote Sherman for advice on the issue. Sherman urged Wallace to be patient and not to request a formal inquiry. Although Sherman brought Wallace's concerns to Grant's attention, Wallace was not given another active duty command until March 1864.\nFor many years Grant stood by his original version of the orders to Wallace. As late as 1884, when Grant wrote an article on Shiloh for \"The Century Magazine\" that appeared in its February 1885 issue, he maintained that Wallace had taken the wrong road on the first day of battle. After W. H. L. Wallace's widow gave Grant a letter that Lew Wallace had written to her husband the day before the battle (the one indicating his plans to use the Shunpike road to pass between Shiloh and his position west of Crump's Landing), Grant changed his mind. Grant wrote a letter to the editors at \"Century\", which was published in its September 1885 issue, and added a note to his memoirs to explain that Wallace's letter \"modifies very materially what I have said, and what has been said by others, about the conduct of General Lew Wallace at the battle of Shiloh.\" While reaffirming that he had ordered Wallace to take the River Road, Grant stated that he could not be sure the exact content of Wallace's written orders, since his verbal orders were given to one of his aides and transcribed.\nGrant's article in the February 1885 issue of \"Century\" became the basis of his chapter on Shiloh in his memoirs, which were published in 1886, and influenced many later accounts of Wallace's actions on the first day of battle. Grant acknowledged in his memoirs: \"If the position of our front had not changed, the road which Wallace took would have been somewhat shorter to our right than the River road.\" Wallace's account of the events appeared in his autobiography, which was published posthumously in 1906. Despite his later fame and fortune as the writer of \"Ben-Hur\", Wallace continued to lament, \"Shiloh and its slanders! Will the world ever acquit me of them? If I were guilty I would not feel them as keenly.\"\nThe Kentucky Campaign and Defense of Cincinnati.\nFollowing his loss of a field command, Wallace returned to Indiana and spent time at his retreat on the Kankakee River. It was there that he received a telegram from Governor Morton to take command of an Indiana regiment in the Department of the Ohio to help with the defense of Kentucky during Braxton Bragg's incursion into Kentucky and to report to Louisville. Presenting himself with his new regiment to Brig. Gen. Jeremiah Boyle in Louisville, Boyle was uncomfortable having a superior officer under his command. Boyle ordered Wallace to take his regiment to Lexington, take command of the hastily created Army of Kentucky, and march to the relief of the men at Cumberland Gap. Wallace began a defensive plan that would place his army on the north side of the Kentucky River, about 15 miles from Boonesboro to defend against the advance of Gen. Edmund Kirby Smith's army from the direction of Cumberland Gap. He had all of the locks on the river in the area opened to flood the fords, confiscated every boat in the area and moved them to the north bank, and the position was secured by sheer limestone cliffs on his flanks. But Wallace was soon relieved of command by Maj. Gen. William \"Bull\" Nelson, who took command of the Army of Kentucky on August 24 on orders from Wright. Nelson altered Wallace's defensive plan, and engaged Smith's Confederate Army of Kentucky at the Battle of Richmond on August 30, and was soundly defeated.\nWallace and his staff started a return to Cincinnati to await any orders. Maj. Gen. Horatio Wright sent a telegram ordering Wallace to return to Lexington to take command of what remained of the Army of Kentucky. Traveling by train from Cincinnati, Wallace received another telegram from Wright when he arrived at Paris, Kentucky, ordering him to remain in Cincinnati. He immediately returned to Cincinnati and began vigorous efforts for the defense of Cincinnati.\nUpon his arrival in the city, Wallace immediately began organizing the defenses of Cincinnati, Ohio and the Kentucky cities of Covington and Newport south of Cincinnati. Wallace ordered martial law, set a strict curfew, closed all businesses, and began putting male citizens to work on rifle pits, felling trees for makeshift abatis and clear fields of fire, and improving the 1861 earthwork defenses. It was during this hasty defensive preparation that the Black Brigade of Cincinnati was formed, by Wallace's orders.\nIn response to calls from Ohio's Governor Tod, approximately 15,000 so-called \"Squirrel Hunters\"\u2014untrained volunteers who carried outdated equipment\u2014reported to Cincinnati. Additionally, newly created regiments from Indiana and Ohio were rushed to Cincinnati; most had not completed their training.\nBecause the arriving regiments could not be ferried quickly enough across the Ohio River, Wallace ordered the construction of a pontoon bridge, which was constructed using coal barges in under 48 hours.\nWhile at Lexington, Gen. Smith gave Brig. Gen. Henry Heth permission to make a \"demonstration\" on Cincinnati, granting him approximately 8,000 men. Heth moved within a few miles of Fort Mitchell and exchanged skirmish fire with men from the 101st Ohio Infantry, 103rd Ohio Infantry, and 104th Ohio Infantry on September 10\u201311, then returned to Lexington on September 12, 1862.\nWallace's leadership during the defense of Cincinnati earned him the nickname by local newspapers as the \"Savior of Cincinnati\". On September 12, Wallace telegraphed Wright from Cincinnati: \"The skedaddle is complete; every sign of a rout. If you say so I will organize a column of 20,000 men to pursue to-night.\" Instead, Wright relieved Wallace of a field command.\nOther military assignments.\nWallace was ordered to take command of Camp Chase, a prisoner-of-war camp at Columbus, Ohio, where he remained until October 30, 1862. His instructions there were to recruit and train Confederate prisoners of war for U.S. Army service (also known as \"Galvanized Yankees\") to aid in the Sioux Uprising. The Battle of Wood Lake on September 23 essentially ended the uprising and Wallace was again without a command.\nThe following month, Wallace was placed in charge of the five-member commission Buell Military Commission to investigate Maj. Gen. Don Carlos Buell's conduct in response to the Confederate invasion of Kentucky. The commission criticized Buell for his retreat, but it did not find him disloyal to the Union. When the commission's work was completed on May 6, 1863, Wallace returned to Indiana to wait for a new command. In mid-July 1863, while Wallace was home, he helped protect the railroad junction at North Vernon, Indiana, from Confederate general John Hunt Morgan's raid into southern Indiana.\nMonocacy.\nWallace's most notable service came on Saturday, July 9, 1864, at the Battle of Monocacy part of the Valley Campaigns of 1864. Although Confederate General Jubal A. Early and an estimated 15,000 troops defeated Wallace's troops at Monocacy Junction, Maryland, forcing them to retreat to Baltimore, the effort cost Early a chance to capture Washington, D.C. Wallace's men were able to delay the Confederate advance toward Washington for an entire day, giving the city time to organize its defenses. Early arrived in Washington at around noon on July 11, two days after defeating Wallace at Monocacy, the northernmost Confederate victory of the war, but Union reinforcements had already arrived at Fort Stevens to repel the Confederates and force their retreat to Virginia.\nWallace, who had returned to active duty on March 12, 1864, assumed command of VIII Corps, which was headquartered in Baltimore. On July 9, a combined Union force of approximately 5,800 men under Wallace's command (mostly hundred days men from VIII Corps) and a division under James B. Ricketts from VI Corps encountered Confederate troops at Monocacy Junction between 9 and 10 a.m. Although Wallace was uncertain whether Baltimore or Washington, D.C., was the Confederate objective, he knew his troops would have to delay the advance until Union reinforcements arrived. Wallace's men repelled the Confederate attacks for more than six hours before retreating to Baltimore.\nAfter the battle Wallace informed Halleck that his forces fought until 5 p.m., but the Confederate troops, which he estimated at 20,000 men, had overwhelmed them. When Grant learned of the defeat, he named Maj. Gen. E. O. C. Ord as Wallace's replacement in command of VIII Corps. On July 28, after officials learned how Wallace's efforts at Monocacy helped save Washington D.C. from capture, he was reinstated as commander of VIII Corps. In Grant's memoirs, he praised Wallace's delaying tactics at Monocacy:\nIf Early had been but one day earlier, he might have entered the capital before the arrival of the reinforcements I had sent.\u00a0... General Wallace contributed on this occasion by the defeat of the troops under him, a greater benefit to the cause than often falls to the lot of a commander of an equal force to render by means of a victory.\nLater military service.\nOn January 22, 1865, Grant ordered Wallace to the Rio Grande in southern Texas to investigate Confederate military operations in the area. Although Wallace was not officially authorized to offer terms, he did discuss proposals for the surrender of the Confederate troops in the Trans-Mississippi Department. Wallace provided Grant with copies of his proposals and reported on the negotiations, but no agreement was made. Before returning to Baltimore, Wallace also met with Mexican military leaders to discuss the U.S. government's unofficial efforts to aid in expelling Maximilian's French occupation forces from Mexico.\nFollowing President Lincoln's death on April 15, 1865, Wallace was appointed to the military commission that investigated the Lincoln assassination conspirators. The commission, which began in May, was dissolved on June 30, 1865, after all eight conspirators were found guilty. In mid-August 1865, Wallace was appointed head of an eight-member military commission that investigated the conduct of Henry Wirz, the Confederate commandant in charge of the South's Andersonville prison camp. The court-martial which took nearly two months, opened on August 21, 1865. At its conclusion Wirz was found guilty and sentenced to death.\nOn April 30, 1865, Wallace had accepted an offer to become a major general in the Mexican army, but the agreement, which was contingent upon his resignation from the U.S. Army, was delayed by Wallace's service on the two military commissions. Wallace tendered his resignation from the U.S. Army on November 4, 1865, effective November 30, and returned to Mexico to assist the Mexican army. Although the Ju\u00e1rez government promised Wallace $100,000 for his services, he returned to the United States in 1867 in deep financial debt.\nAfter the war, Wallace became a companion of the Indiana Commandery of the Military Order of the Loyal Legion of the United States.\nPolitical and diplomatic career.\nWallace returned to Indiana in 1867 to practice law, but the profession did not appeal to him, and he turned to politics. Wallace made two unsuccessful bids for a seat in Congress (in 1868 and 1870), and supported Republican presidential candidate Rutherford B. Hayes in the 1876 election. As a reward for his political support, Hayes appointed Wallace as governor of the New Mexico Territory, where he served from August 1878 to March 1881. His next assignment came in March 1881, when Republican president James A. Garfield appointed Wallace to an overseas diplomatic post in Constantinople as U.S. Minister to the Ottoman Empire. Wallace remained in this post until 1885.\nTerritorial governor of New Mexico.\nWallace arrived in Santa Fe on September 29, 1878, to begin his service as governor of the New Mexico Territory during a time of lawless violence and political corruption. Wallace was involved in efforts to resolve New Mexico's Lincoln County War, a contentious and violent disagreement among the county's residents, and tried to end a series of Apache raids on territorial settlers. In 1880, while living at the Palace of the Governors in Santa Fe, Wallace also completed the manuscript for \"\".\nOn March 1, 1879, after previous efforts to restore order in Lincoln County had failed, Wallace ordered the arrest of those responsible for local killings. One of the outlaws was William Henry McCarty Jr. (alias William H. Bonney), better known as Billy the Kid. On March 17, 1879, Wallace secretly met with Bonney, who had witnessed the murder of a Lincoln County lawyer named Huston Chapman. Wallace wanted him to testify in the trial of Chapman's accused murderers, but Bonney wanted Wallace's protection from his enemies and amnesty for his earlier crimes. During their meeting, the pair arranged for Bonney to become an informant in exchange for a full pardon of his previous crimes. Wallace supposedly assured the Kid that he would be \"scot free with a pardon in your pocket for all your misdeeds.\" On March 20 Bonney agreed to provide grand jury testimony against those involved in Chapman's murder. Wallace arranged for a \"fake\" arrest and Bonney's detention in a local jail to assure his safety. Bonney testified in court on April 14, as agreed. However, the local district attorney revoked Wallace's bargain and refused to set the outlaw free. After spending several weeks in jail, Bonney escaped and returned to his criminal ways, which included killing additional men. He was shot and killed on July 14, 1881, by Sheriff Pat Garrett, who had been appointed by local ranching interests who had tired of his rustling their herds. In the meantime, Wallace had resigned from his duties as territorial governor on March 9, 1881, and was waiting for a new political appointment.\nOn December 31, 2010, on his last day in office, then-Governor Bill Richardson of New Mexico declined a pardon request from Bonney's supporters, citing a \"lack of conclusiveness and the historical ambiguity\" over Wallace's promise of amnesty. Descendants of Wallace and Garrett were among those who opposed the pardon.\nU.S. diplomat in the Ottoman Empire.\nOn May 19, 1881, Wallace was appointed U.S. Minister to the Ottoman Empire in Constantinople (present-day Istanbul, Turkey). Wallace remained at the diplomatic post until 1885, and became a trusted friend of Sultan Abdul Hamid II. When a crisis developed between the Turkish and British governments over control of Egypt, Wallace served as an intermediary between the sultan and Lord Dufferin, the British ambassador. Although Wallace's efforts were unsuccessful, he earned respect for his efforts and a promotion in the U.S. diplomatic service.\nIn 1883, an editorial aimed at Wallace appeared in the newspaper \"Havatzelet\" (xiii. No. 6) titled \"An American and yet a Despot\". The editorial caused the \"Havatzelet\" to be suspended and its editor Israel Dov Frumkin to be imprisoned for forty-five days by order from Constantinople, directed to the pasha of the Mutasarrifate of Jerusalem. The incident that led to the editorial was the dismissal, made at Wallace's request, of Joseph Kriger, the Jewish secretary and interpreter to the pasha of Jerusalem. Wallace complained that Kriger had failed to receive him with the honor due to his rank, and refused to issue any apology for the alleged shortcoming. \"Havatzelet\" claimed that the proceeding was instigated by missionaries, whom Wallace strongly supported.\nIn addition to Wallace's diplomatic duties, which included protection of U.S. citizens and U.S. trade rights in the area, Wallace found time to travel and do historical research. Wallace visited Jerusalem and the surrounding area, a setting in his previous novel, \"Ben-Hur\", and did research in Constantinople, the locale for \"The Prince of India; or, Why Constantinople Fell\", which he began writing in 1887.\nThe election of Grover Cleveland, the Democratic candidate for president, ended Wallace's political appointment. He resigned from the U.S. diplomatic service on March 4, 1885. The sultan wanted Wallace to continue to work in the Ottoman Empire, and even made a proposal to have him represent Ottoman interests in England or France, but Wallace declined and returned home to Crawfordsville.\nWriting career.\nWallace confessed in his autobiography that he took up writing as a diversion from studying law. Although he wrote several books, Wallace is best known for his historical adventure story, \"\" (1880), which established his fame as an author.\nIn 1843, Wallace began writing his first novel, \"The Fair God\", but it was not published until 1873. The popular historical novel, with Cortez's conquest of Mexico as its central theme, was based on William H. Prescott's \"History of the Conquest of Mexico\". Wallace's book sold seven thousand copies in its first year. Its sales continued to rise after Wallace's reputation as an author was established with the publication of subsequent novels.\nWallace wrote the manuscript for \"Ben-Hur\", his second and best-known novel, during his spare time at Crawfordsville, and completed it in Santa Fe, while serving as the territorial governor of New Mexico. \"Ben-Hur\", an adventure story of revenge and redemption, is told from the perspective of a Jewish nobleman named Judah Ben-Hur. Because Wallace had not been to the Holy Land before writing the book, he began research to familiarize himself with the area's geography and its history at the Library of Congress in Washington, D.C., in 1873. Harper and Brothers published the book on November 12, 1880.\n\"Ben-Hur\" made Wallace a wealthy man and established his reputation as a famous author. Sales were slow at first; only 2,800 copies were sold in the first seven months after its release, but the book became popular among readers around the world. By 1886, it was earning Wallace about $11,000 in annual royalties (equivalent to $290,000 in 2015 dollars), and provided Wallace's family with financial security. By 1889, Harper and Brothers had sold 400,000 copies and the book had been translated into several languages.\nIn 1900, \"Ben-Hur\" became the best-selling American novel of the 19th century, surpassing Harriet Beecher Stowe's \"Uncle Tom's Cabin\". Amy Lifson, an editor for \"Humanities\", identified it as the most influential Christian book of the 19th century. Others named it one of the best-selling novels of all time. At the time of \"Ben-Hur\"'s one hundredth anniversary in 1980, it had \"never been out of print\" and had been adapted for the stage and several motion pictures. One historian, Victor Davis Hanson, has argued that \"Ben-Hur\" drew from Wallace's life, particularly his experiences at Shiloh, and the damage it did to his reputation. The book's main character, Judah Ben-Hur, accidentally causes injury to a high-ranking Roman commander, for which he and his family suffer tribulations and calumny.\nWallace wrote subsequent novels and biographies, but \"Ben-Hur\" remained his most important work. Wallace considered \"The Prince of India; or, Why Constantinople Fell\" (1893) as his best novel. He also wrote a biography of President Benjamin Harrison, a fellow Hoosier and Civil War general, and \"The Wooing of Malkatoon\" (1898), a narrative poem. Wallace was writing his autobiography when he died in 1905. His wife Susan completed it with the assistance of Mary Hannah Krout, another author from Crawfordsville. It was published posthumously in 1906.\nLater years.\nWallace continued to write after his return from the Ottoman Empire. He also patented several of his own inventions, built a seven-story apartment building in Indianapolis, the Blacherne, and drew up plans for a private study at his home in Crawfordsville. Wallace remained active in veterans groups, including writing a speech for the dedication of the battlefield at the Chickamauga.\nWallace's elaborate writing study, which he described as \"a pleasure-house for my soul\", served as his private retreat. Now called the General Lew Wallace Study and Museum, it was built between 1895 and 1898, adjacent to his residence in Crawfordsville, and set in an enclosed park. The study along with three and one-half acres of its grounds were designated a National Historic Landmark in 1976. The property is operated as a museum, open to the public. Wallace had a moat on two sides of the Study and stocked it so he could fish from the back porch and a landing. In winter, he would fire up the coal furnace in the Study basement and fish from the windows. He loved fishing so much he invented and patented a special traveler's fishing pole. After just a few years he had the moat drained as it was negatively affecting the Study foundation and he worried about his grandchildren and neighborhood children falling into the water.\nOn April 5, 1898, at the outbreak of the Spanish\u2013American War, Wallace, at age seventy-one, offered to raise and lead a force of soldiers, but the war office refused. Undeterred, he went to a local recruiting office and attempted to enlist as a private, but was rejected again, presumably because of his age.\nWallace's service at the battle of Shiloh continued to haunt him in later life. The debate persisted in book publications, magazine articles, pamphlets, speeches, and in private correspondence. Wallace attended a reunion at Shiloh in 1894, his first return since 1862, and retraced his journey to the battlefield with veterans from the 3rd Division. He returned to Shiloh for a final time in 1901 to walk the battlefield with David W. Reed, the Shiloh Battlefield Commission's historian, and others. Wallace died before the manuscript of his memoirs was fully completed, and it is unknown whether he would have revised his final account of the battle.\nDeath.\nWallace died at home in Crawfordsville, on February 15, 1905, of atrophic gastritis. He was 77. Wallace is buried in Crawfordsville Oak Hill Cemetery.\nLegacy and honors.\nWallace was a man of many interests and a lifelong adventure seeker, who remained a persistent, self-confident man of action. He was also impatient and highly sensitive to personal criticisms, especially those related to his command decisions at Shiloh. Despite Wallace's career in law and politics, combined with years of military and diplomatic service, he achieved his greatest fame as a novelist, most notably for his best-selling biblical tale, \"Ben-Hur\".\nFollowing Wallace's death, the State of Indiana commissioned the sculptor Andrew O'Connor to create a marble statue of Wallace dressed in a military uniform for the National Statuary Hall Collection in the U.S. Capitol. The statue was unveiled during a ceremony held on January 11, 1910. A bronze copy of the statue is installed on the grounds of Wallace's study in Crawfordsville.\nLew Wallace High School opened in 1926 at 415 West 45th Avenue in Gary, Indiana. On June 3, 2014, the Gary School Board voted 4 to 2 to close Lew Wallace, along with five other schools.\nA Knights of Pythias lodge was established in Franklin, Indiana at the Masonic Home to be known as the General Lewis Wallace Lodge #2019.\nPopular culture.\nUSL Indianapolis-based team The Indy Eleven pays homage to the 11th Regiment of Indiana Volunteers, which fought for the Union Army during the Civil War. The inspiration for the name came from Donna Schmink, the Collection Manager at the Indiana War Museum, who, when asked by team officials for ideas on a team name connected to Indiana history, suggested \"the Eleventh\" in honor of the regiment that valiantly fought under the initial direction of Colonel Lew Wallace.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44088", "revid": "44144402", "url": "https://en.wikipedia.org/wiki?curid=44088", "title": "Vittorio Gassman", "text": "Italian actor and director (1922\u20132000)\nVittorio Gassman (; born Gassmann; 1 September 1922 \u2013 29 June 2000), popularly known as , was an Italian actor, director, and screenwriter.\nHe is considered one of the greatest Italian actors, whose career includes both important productions as well as dozens of \"divertissements\".\nEarly life.\nGassman was born in Genoa to a German father, Heinrich Gassmann (an engineer from Karlsruhe), and an Italian Jewish mother, Luisa Ambron, born in Pisa. While still very young, he moved to Rome, where he studied at the Silvio D'Amico National Academy of Dramatic Arts.\nCareer.\nGassman's stage debut was in Milan, in 1942, with Alda Borelli in Niccodemi's \"La Nemica\". He then moved to Rome and acted at the Teatro Eliseo joining Tino Carraro and Ernesto Calindri in a stage company that remained famous for some time; with them he acted in a range of plays from bourgeois comedy to sophisticated intellectual theatre. In 1946, he made his film debut in \"Preludio d'amore\", while only one year later he appeared in five films. In 1948, he played in \"Bitter Rice\".\nIt was with Luchino Visconti's company that Gassman achieved his mature successes, together with Paolo Stoppa, Rina Morelli and Paola Borboni. He played Stanley Kowalski in Tennessee Williams' \"Un tram che si chiama desiderio\" (\"A Streetcar Named Desire\"), as well as in \"Come vi piace\" (\"As You Like It\") by Shakespeare and \"Oreste\" (by Vittorio Alfieri). He joined the \"Teatro Nazionale\" with Tommaso Salvini, Massimo Girotti, Arnoldo Fo\u00e0 to create a successful \"Peer Gynt\" (by Henrik Ibsen). With Luigi Squarzina in 1952 he co-founded and co-directed the \"Teatro d'Arte Italiano\", producing the first complete version of \"Hamlet\" in Italy, followed by rare works such as Seneca's \"Thyestes\" and Aeschylus's \"The Persians\".\nIn 1956, Gassman played the title role in a production of \"Othello\". He was so well received by his acting in the television series entitled \"Il Mattatore (Spotlight Chaser)\" that \"Il Mattatore\" became the nickname that accompanied him for the rest of his life. Gassman's debut in the commedia all'italiana genre was rather accidental, in Mario Monicelli's \"Big Deal on Madonna Street\" (1958). The \"Istituto Italiano di Cultura\" in London describes the film as \"considered among the masterpieces of Italian cinema \u2026 The careers of both Gassman and Mastroianni were considerably helped by the success of the film, Gassman in particular, since before this point he was not deemed suitable for comedic roles.\"\nSubsequent acclaimed films featuring Gassman include: \"The Easy Life\" (1962), \"The Great War\" (1962), \"I mostri\" (1963), \"For Love and Gold\" (1966), \"Scent of a Woman\" (1974) and \"We All Loved Each Other So Much\" (1974).\nHe directed \"Adelchi\", a lesser-known work by Alessandro Manzoni. Gassman brought this production to half a million spectators, crossing Italy with his \"Teatro Popolare Itinerante\" (a newer edition of the famous \"Carro di Tespi\"). His productions have included many of the famous authors and playwrights of the 20th century, with repeated returns to the classics of Shakespeare, Dostoyevsky and the Greek tragicians. He also founded a theatre school in Florence (Bottega Teatrale di Firenze), which educated many of the more talented actors of the current generation of Italian thespians.\nIn cinema, he worked frequently both in Italy and abroad. He met and fell in love with American actress Shelley Winters while she was touring Europe with fianc\u00e9 Farley Granger. When Winters was forced to return to Hollywood to fulfil contractual obligations, he followed her there and married her. With his natural charisma and his fluency in English, he scored a number of roles in Hollywood, including \"Rhapsody\" with Elizabeth Taylor and \"The Glass Wall\" before returning to Italy and the theatre.\nIn the 1990s he took part in the popular Italian Rai 3 TV show \"Tunnel\" in which he very formally and \"seriously\"' recited documents such as utility bills, yellow pages and similar trivial texts, such as washing instructions for a wool sweater or cookies ingredients. He rendered them with the same professional skill that made him famous while reciting Dante's \"Divine Comedy\".\nIn 1994, Gassman voiced Mufasa in the Italian dubbed version of \"The Lion King\". Gassman's voice was redubbed in several of his films by historical Italian actors and dubbers which include Emilio Cigoli, Sandro Ruffini, Gualtiero De Angelis, Stefano Sibaldi, Enrico Maria Salerno and Pino Locchi.\nPersonal life.\nGassman married three times, all to actresses: Nora Ricci (with whom he had Paola, an actress and wife of Ugo Pagliai); Shelley Winters (mother of his daughter Vittoria); and Diletta D'Andrea (mother of his son Jacopo).\nWhile rehearsing \"Hamlet\", he began an affair with Anna Maria Ferrero, his 16-year-old Ophelia, which ended his marriage to Winters. He and Winters were forced to work together on \"Mambo\" just as their marriage was unraveling, providing fodder for tabloids all over the world.\nFrom 1964 to 1968 he was the partner of French actress Juliette Mayniel (mother of his son Alessandro, also an actor). Through Alessandro, he is the grandfather of singer-songwriter Leo Gassmann.\nGassman suffered from bipolar disorder.\nDeath.\nOn 29 June 2000, Gassman died of a heart attack in his sleep at his home in Rome at the age of 77. He was buried at Campo Verano.\nFilmography.\nActor.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44093", "revid": "10202399", "url": "https://en.wikipedia.org/wiki?curid=44093", "title": "Electroconvulsive therapy", "text": "Medical procedure in which electrical current is passed through the brain\nElectroconvulsive therapy (ECT) is a psychiatric treatment that causes a generalized seizure by passing electrical current through the brain. ECT is often used as an intervention for mental disorders when other treatments are inadequate. Conditions responsive to ECT include major depressive disorder, mania, and catatonia. \nThe general physical risks of ECT are similar to those of brief general anesthesia. Immediately following treatment, the most common adverse effects are confusion and transient memory loss. Among treatments for severely depressed pregnant women, ECT is one of the least harmful to the fetus.\nThe usual course of ECT involves multiple administrations, typically given two or three times per week until the patient no longer has symptoms. ECT is administered under anesthesia with a muscle relaxant. ECT can differ in its application in three ways: electrode placement, treatment frequency, and the electrical waveform of the stimulus. Differences in these parameters affect symptom remission and adverse side effects.\nPlacement can be bilateral, where the electric current is passed from one side of the brain to the other, or unilateral, in which the current is solely passed across one hemisphere of the brain. High-dose unilateral ECT has some cognitive advantages compared to moderate-dose bilateral ECT while showing no difference in antidepressant efficacy.\nHistory.\nAs early as the 16th century, agents to induce seizures were used to treat psychiatric conditions. In 1785, the therapeutic use of seizure induction (by administering camphor orally) was documented in the \"London Medical and Surgical Journal\". As to its earliest antecedents one doctor claims 1744 as the dawn of electricity's therapeutic use, as documented in the first issue of \"Electricity and Medicine\". Treatment and cure of hysterical blindness was documented eleven years later. Benjamin Franklin wrote that an electrostatic machine cured \"a woman of hysterical fits.\" By 1801, James Lind as well as Giovanni Aldini had used galvanism to treat patients with various mental disorders. G.B.C. Duchenne, the mid-19th century \"Father of Electrotherapy\", said its use was integral to a neurological practice.\nIn the second half of the 19th century, such efforts were frequent enough in British asylums as to make it notable.\nConvulsive therapy was introduced in 1934 by Hungarian neuropsychiatrist Ladislas J. Meduna who, believing mistakenly that schizophrenia and epilepsy were antagonistic disorders, induced seizures first with camphor and then metrazol (cardiazol). Meduna is thought to be the father of convulsive therapy. \nIn 1937, the first international meeting on schizophrenia and convulsive therapy was held in Switzerland by the Swiss psychiatrist Max M\u00fcller. The proceedings were published in the \"American Journal of Psychiatry\" and, within three years, cardiazol convulsive therapy was being used worldwide. \nThe ECT procedure was first conducted in 1938 by Italian neuro-psychiatrist Ugo Cerletti and rapidly replaced less safe and effective forms of biological treatments in use at the time. Cerletti, who had been using electric shocks to produce seizures in animal experiments, and his assistant Lucio Bini at Sapienza University of Rome developed the idea of using electricity as a substitute for metrazol in convulsive therapy and, in 1938, experimented for the first time on a person affected by delusions. \nIt was believed early on that inducing convulsions aided in helping those with severe schizophrenia but later found to be most useful with affective disorders such as depression. Cerletti had noted a shock to the head produced convulsions in dogs. The idea of a procedure on humans came to Cerletti when he saw how pigs were given an electric shock before being butchered to put them in an anesthetized state. Cerletti and Bini practiced until they felt they had the right parameters needed to have a successful human trial. Once they started trials on patients, they found that after 10\u201320 treatments the results were significant. Patients had much improved. \nA positive side effect to the treatment was retrograde amnesia and it was because of this side effect that patients could not remember the treatments and had no ill feelings toward it. \nECT soon replaced metrazol therapy all over the world because it was cheaper, less frightening and more convenient. Cerletti and Bini were nominated for a Nobel Prize but did not receive one. By 1940, the procedure was introduced to both England and the US. In Germany and Austria, it was promoted by Friedrich Meggendorfer. Through the 1940s and 1950s, the use of ECT became widespread. At the time the ECT device was patented and commercialized abroad, the two Italian inventors had competitive tensions that damaged their relationship. In the 1960s, despite a climate of condemnation, the original Cerletti-Bini ECT apparatus prototype was contended by scientific museums between Italy and the US. The ECT apparatus prototype is now owned and displayed by the Sapienza in Rome.\nIn the early 1940s, in an attempt to reduce the memory disturbance and confusion associated with treatment, two modifications were introduced: the use of unilateral electrode placement and the replacement of sinusoidal current with brief pulse. It took many years for brief-pulse equipment to be widely adopted. \nIn the 1940s and early 1950s, ECT was usually given in an \"unmodified\" form, without muscle relaxants, and the seizure resulted in a full-scale convulsion. A rare but serious complication of unmodified ECT was fracture or dislocation of the long bones. In the 1940s, psychiatrists began to experiment with curare, the muscle-paralysing South American poison, in order to modify the convulsions. The introduction of suxamethonium (succinylcholine), a safer synthetic alternative to curare, in 1951 led to the more widespread use of \"modified\" ECT. A short-acting anesthetic was usually given in addition to the muscle relaxant in order to spare patients the terrifying feeling of suffocation that can be experienced with muscle relaxants.\nThe steady growth of antidepressant use along with negative depictions of ECT in the mass media led to a marked decline in the use of ECT during the 1950s to the 1970s. The Surgeon General stated there were problems with ECT in the initial years before anesthesia was routinely given, and that \"these now-antiquated practices contributed to the negative portrayal of ECT in the popular media.\" \"The New York Times\" described the public's negative perception of ECT as being caused mainly by one fictional work: \"For Big Nurse in \"One Flew Over the Cuckoo's Nest,\" it was a tool of terror, and, in the public mind, \"shock therapy\" has retained the tarnished image given it by Ken Kesey's novel: dangerous, inhumane and overused\".\nIn 1976, Dr. Blatchley demonstrated the effectiveness of his constant current, brief pulse device ECT. This device eventually largely replaced earlier devices because of the reduction in cognitive side effects, although as of 2012 some ECT clinics still were using sine-wave devices. \nThe 1970s saw the publication of the first American Psychiatric Association (APA) task force report on electroconvulsive therapy (to be followed by further reports in 1990 and 2001). The report endorsed the use of ECT in the treatment of depression. The decade also saw criticism of ECT. Specifically, critics pointed to shortcomings such as noted side effects, the procedure being used as a form of abuse, and uneven application of ECT. The use of ECT declined until the 1980s, \"when use began to increase amid growing awareness of its benefits and cost-effectiveness for treating severe depression\". In 1985, the National Institute of Mental Health and National Institutes of Health convened a consensus development conference on ECT and concluded that, while ECT was the most controversial treatment in psychiatry and had significant side-effects, it had been shown to be effective for a narrow range of severe psychiatric disorders.\nBecause of the backlash noted previously, national institutions reviewed past practices and set new standards. In 1978, the American Psychiatric Association released its first task force report in which new standards for consent were introduced and the use of unilateral electrode placement was recommended. The 1985 NIMH Consensus Conference confirmed the therapeutic role of ECT in certain circumstances. The American Psychiatric Association released its second task force report in 1990 where specific details on the delivery, education, and training of ECT were documented. Finally, in 2001 the American Psychiatric Association released its latest task force report. This report emphasizes the importance of informed consent, and the expanded role that the procedure has in modern medicine. By 2017, ECT was routinely covered by insurance companies for providing the \"biggest bang for the buck\" for otherwise intractable cases of severe mental illness, was receiving favorable media coverage, and was being provided in regional medical centers.\nThough ECT use declined with the advent of modern antidepressants, there has been a resurgence of ECT with new modern technologies and techniques. Modern shock voltage is given for a shorter duration of 0.5 milliseconds where conventional brief pulse is 1.5 milliseconds.\nIn a review from 2022 of neuroimaging studies based on a global data collaboration, ECT was suggested to work via a temporary disruption of neural circuits followed by augmented neuroplasticity and rewiring.\nModern use.\nECT is used, where possible, with informed consent in treatment-resistant major depressive disorder, bipolar depression, treatment-resistant catatonia, prolonged or severe mania, and in conditions where \"there is a need for rapid, definitive response because of the severity of a psychiatric or medical condition (e.g., when illness is characterized by suicidality, psychosis, stupor, marked psychomotor retardation, depressive delusions or hallucinations, or life-threatening physical exhaustion associated with mania).\" It has also been used to treat autism in adults with an intellectual disability, yet findings from a systematic review found this an unestablished intervention.\nMajor depressive disorder.\nFor major depressive disorder, despite a Canadian guideline and some experts arguing for using ECT as a first line treatment, ECT is generally used only when one or other treatments have failed, or in emergencies, such as imminent suicide. ECT has also been used in selected cases of depression occurring in the setting of multiple sclerosis, Parkinson's disease, Huntington's chorea, developmental delay, brain arteriovenous malformations, and hydrocephalus.\nEfficacy.\nA meta-analysis on the effectiveness of ECT in unipolar and bipolar depression indicated that although patients with unipolar depression and bipolar depression responded to other medical treatments very differently, both groups responded equally well to ECT. Overall remission rate for patients given a round of ECT treatment was 50.9% for those with unipolar depression and 53.2% for those with bipolar depression. Most severely depressed patients respond to ECT.\nIn 2004, a meta-analysis found in terms of efficacy, \"a significant superiority of ECT in all comparisons: ECT versus simulated ECT, ECT versus placebo, ECT versus antidepressants in general, ECT versus tricyclics and ECT versus monoamine oxidase inhibitors.\"\nIn 2003, the UK ECT Review Group published a systematic review and meta-analysis comparing ECT to placebo and antidepressant drugs. This meta-analysis demonstrated a large effect size (high efficacy relative to the mean in terms of the standard deviation) for ECT versus placebo, and versus antidepressant drugs.\nCompared with repetitive transcranial magnetic stimulation (rTMS) for people with treatment-resistant major depressive disorder, ECT relieves depression as shown by reducing the score on the Hamilton Rating Scale for Depression by about 15 points, while rTMS reduced it by 9 points.\nOther estimates regarding the response rate in treatment resistant depression vary between 60\u201380%, with a remission rate of 50\u201360%. In addition to reducing symptoms of depression and inducing relapse, ECT has also been shown to reduce the risk of suicide, improve functional outcomes and quality of life as well as reduce the risk of re-hospitalization. Efficacy does not depend on depression subtype. With regards to treatment resistant schizophrenia, the response rate is 40\u201370%.\nFollow-up.\nThere is little agreement on the most appropriate follow-up to ECT for people with major depressive disorder. The initial course of ECT is then transitioned to maintenance ECT, pharmacotherapy or both. When ECT is stopped abruptly, without a bridge to maintenance ECT or medications (usually antidepressants and Lithium), it is associated with a relapse rate of 84%. There is no defined schedule for maintenance ECT, however it is usually started weekly with intervals extended permissibly with the goal of maintaining remission. When ECT is followed by treatment with antidepressants, about 50% of people relapsed by 12 months following successful initial treatment with ECT, with about 37% relapsing within the first 6 months. About twice as many relapsed with no antidepressants. Most of the evidence for continuation therapy is with tricyclic antidepressants; evidence for relapse prevention with newer antidepressants is lacking. Adjunct maintenance ECT paired with cognitive behavioral therapy has also been shown to reduce relapse rates. Maintenance ECT may safely continue indefinitely, with no set maximum treatment interval established.\nLithium has also been found to reduce the risk of relapse, especially in younger patients.\nCatatonia.\nECT is generally a second-line treatment for people with catatonia who do not respond to other treatments, but is a first-line treatment for severe or life-threatening catatonia. There is a plethora of evidence for its efficacy, notwithstanding a lack of randomised controlled trials, such that \"the excellent efficacy of ECT in catatonia is generally acknowledged\". For people with autism spectrum disorders who have catatonia, there is little published evidence about the efficacy of ECT.\nMania.\nECT is used to treat people who have severe or prolonged mania; NICE recommends it only in life-threatening situations or when other treatments have failed and as a second-line treatment for bipolar mania.\nSchizophrenia.\nECT is widely used worldwide in the treatment of schizophrenia. However, in North America and Western Europe it is invariably used only in treatment resistant schizophrenia when symptoms show little response to antipsychotics. ECT may improve medium-term clinical response relative to standard care, but may not affect other outcomes. Evidence is lacking to support the practice's superiority to placebo treatment (sham ECT) or antipsychotic supplementation. It is useful in the case of severe exacerbations of catatonic schizophrenia, whether excited or stuporous. There are also case reports of ECT improving persistent psychotic symptoms associated with stimulant-induced psychosis.\nEffects and adverse effects.\nAside from effects on the brain, the general risk for adverse effects stemming from ECT are similar to those of brief general anesthesia; a Surgeon General of the United States's report stated that there are \"no absolute health contraindications\" to its use. Immediately following treatment, the most common adverse effects are confusion and memory loss. Some patients experience muscle soreness after ECT. Other common adverse effects of ECT include headache, jaw soreness, nausea, vomiting, and fatigue. These side effects are transient and respond to treatment. There is evidence and rationale to support giving low doses of benzodiazepines or lower doses of general anesthetics, which induce sedation but not unconsciousness, to patients to reduce the likelihood of adverse effects of ECT. Severe adverse cardiac events occur in between one in forty and, maximally, one in fifteen patients administered ECT.\nWhile there are no absolute contraindications for ECT, there is an increased risk for patients who have unstable or severe cardiovascular conditions or aneurysms; who have recently had a stroke; who have increased intracranial pressure (for instance, due to a solid brain tumor); who have severe pulmonary conditions; or who are generally at high risk for adverse effects from anesthesia.\nIn adolescents, ECT is highly efficient for several psychiatric disorders, with few and relatively benign adverse effects.\nRisk of death.\nA meta-analysis from 2017 found that the death rate of ECT was around 2.1 per 100,000 procedures. A review from 2011 reported an estimated ECT mortality rate of fewer than one death per 73,440 treatments.\nCognitive impairment.\nCognitive impairment sometimes occurs after ECT. The American Psychiatric Association (APA) report in 2001 acknowledged: \"In some patients the recovery from retrograde amnesia will be incomplete, and evidence has shown that ECT can result in persistent or permanent memory loss\". It is the purported effects of ECT on long-term memory that give rise to much of the concern surrounding its use. However, the methods used to measure memory loss are non-specific, and their application to people with depressive disorders, who have cognitive deficits related to depression, including problems with memory, may further limit their utility.\nThe acute effects of ECT can include amnesia, both retrograde (for events occurring before the treatment) and anterograde (for events occurring after the treatment). Memory loss and confusion are more pronounced with bilateral electrode placement rather than unilateral and with outdated sine-wave rather than brief-pulse currents. Using either constant or pulsing electrical impulses also varied the memory loss results in patients. Patients who received pulsing electrical impulses, as opposed to a steady flow, seemed to incur less memory loss. The vast majority of modern treatments use brief pulse currents. A greater number of treatments and higher electrical charges (stimulus charges) have also been associated with a greater risk of memory impairment.\nRetrograde amnesia is most marked for events occurring in the weeks or months before treatment. Anterograde memory loss usually resolves 2\u20134 weeks after treatment, whereas retrograde amnesia (which develops gradually after repeated treatments in the initial course) usually takes weeks to months to resolve; amnesia rarely persists for more than 1 year. Retrograde amnesia after ECT usually affects autobiographical memory rather than semantic memory. One published review summarizing the results of questionnaires about subjective memory loss found that between 29% and 55% of respondents believed they experienced long-lasting or permanent memory changes. In 2000, American psychiatrist Sarah Lisanby and colleagues found that bilateral ECT left patients with more persistently impaired memory of public events as compared to right unilateral ECT. However, bilateral ECT may be more efficacious than unilateral in the treatment of mood disorders.\nECT has not been found to increase the risk of dementia or cause structural brain damage.\nEffects on brain structure.\nConsiderable controversy exists over the effects of ECT on brain tissue, although a number of mental health associations\u2014including the APA\u2014have concluded that there is no evidence that ECT causes structural brain damage. A 1999 report by the US Surgeon General states: \"The fears that ECT causes gross structural brain pathology have not been supported by decades of methodologically sound research in both humans and animals.\"\nMany expert proponents of ECT maintain that the procedure is safe and does not cause brain damage. Dr. Charles Kellner, a prominent ECT researcher and former chief editor of the \"Journal of ECT\", stated in a 2007 interview that, \"There are a number of well-designed studies that show ECT does not cause brain damage and numerous reports of patients who have received a large number of treatments over their lifetime and have suffered no significant problems due to ECT.\" Kellner cites a study purporting to show an absence of cognitive impairment in eight subjects after more than 100 lifetime ECT treatments. Kellner stated, \"Rather than cause brain damage, there is evidence that ECT may reverse some of the damaging effects of serious psychiatric illness.\" Two meta-analyses find that ECT is associated with brain matter growth.\nEffects in pregnancy.\nIf steps are taken to decrease potential risks, ECT is generally considered relatively safe during all trimesters of pregnancy, particularly when compared to pharmacological treatments. Suggested preparation for ECT during pregnancy includes a pelvic examination, discontinuation of nonessential anticholinergic medication, uterine tocodynamometry, intravenous hydration, and administration of a nonparticulate antacid. During ECT, elevation of the pregnant woman's right hip, external fetal cardiac monitoring, intubation, and avoidance of excessive hyperventilation are recommended. In many instances of active mood disorder during pregnancy, the risks of untreated symptoms may outweigh the risks of ECT. Modifications in technique can minimize potential complications of ECT during pregnancy. The use of ECT during pregnancy requires a thorough evaluation of the patient's capacity for informed consent.\nEffects on the heart.\nECT can cause a lack of blood flow and oxygen to the heart, heart arrhythmia, and \"persistent asystole\". A 2019 systematic review and meta-analysis of 82 studies found that the rate of major adverse cardiac events with ECT was 1 in 39 patients or about 1 in 200 to 500 procedures. The risk of death with ECT however is low. If death does occur, cardiovascular complications are considered as causal in about 30% of individuals.\nProcedure.\nThe placement of electrodes, as well as the dose and duration of the stimulation is determined on a per-patient basis.\nIn unilateral ECT, both electrodes are placed on the same side of the patient's head. Unilateral ECT may be used first to minimize side effects such as memory loss.\nIn bilateral ECT, the two electrodes are placed on opposite sides of the head. Usually bitemporal placement is used, whereby the electrodes are placed on the temples. Uncommonly bifrontal placement is used; this involves positioning the electrodes on the patient's forehead, roughly above each eye.\nUnilateral ECT is thought to cause fewer cognitive effects than bilateral treatment, but is less effective unless administered at higher doses. Most patients in the US and almost all in the UK receive bilateral ECT.\nThe electrodes deliver an electrical stimulus. The stimulus levels recommended for ECT are in excess of an individual's seizure threshold: about one and a half times seizure threshold for bilateral ECT and up to 12 times for unilateral ECT. Below these levels treatment may not be effective in spite of a seizure, while doses massively above threshold level, especially with bilateral ECT, expose patients to the risk of more severe cognitive impairment without additional therapeutic gains. Seizure threshold is determined by trial and error (\"dose titration\"). Some psychiatrists use dose titration, some still use \"fixed dose\" (that is, all patients are given the same dose) and others compromise by roughly estimating a patient's threshold according to age and sex. Older men tend to have higher thresholds than younger women, but it is not a hard and fast rule, and other factors, for example drugs, affect seizure threshold.\nImmediately prior to treatment, a patient is given a short-acting anesthetic such as methohexital, propofol, etomidate, or thiopental, a muscle relaxant such as suxamethonium (succinylcholine), and occasionally atropine to inhibit salivation. Studies have shown that adding ketamine, an NMDA receptor antagonist, to the anesthesia regimen produced greater decreases in depression scores when compared to propofol, methohexital, and thiopental alone. In a minority of countries such as Japan, India, and Nigeria, ECT may be used without anesthesia. The Union Health Ministry of India recommended a ban on ECT without anesthesia in India's Mental Health Care Bill of 2010 and the Mental Health Care Bill of 2013. The practice was abolished in Turkey's largest psychiatric hospital in 2008.\nThe patient's EEG, ECG, and blood oxygen levels are monitored during treatment.\nECT is usually administered three times a week, on alternate days, over a course of two to four weeks.\nNeuroimaging prior to ECT.\nNeuroimaging prior to ECT may be useful for detecting intracranial pressure or mass given that patients respond less when one of these conditions exist. Nonetheless, it is not indicated due to high cost and low prevalence of these conditions in patients needing ECT.\nConcurrent pharmacotherapy.\nWhether psychiatric medications are terminated prior to treatment or maintained, varies. However, drugs that are known to cause toxicity in combination with ECT, such as lithium, are discontinued, and benzodiazepines, which increase the seizure threshold, are either discontinued, a benzodiazepine antagonist is administered at each ECT session, or the ECT treatment is adjusted accordingly.\nA 2009 RCT provides some evidence indicating that concurrent use of some antidepressant improves ECT efficacy.\nCourse.\nECT is usually done from 6 to 12 times in 2 to 4 weeks but can sometimes exceed 12 rounds. It is also recommended to not do ECT more than 3 times per week. Evidence suggest that ECTs for depression may be stopped if there is no improvement during the first six sessions.\nTreatment team.\nIn the US, the medical team performing the procedure typically consists of a psychiatrist, an anesthetist, an ECT treatment nurse or qualified assistant, and one or more recovery nurses. Medical trainees may assist, but only under the direct supervision of credentialed attending physicians and staff.\nDevices.\nMost modern ECT devices deliver a brief-pulse current, which is thought to cause fewer cognitive effects than the sine-wave currents which were originally used in ECT. A small minority of psychiatrists in the US still use sine-wave stimuli. Sine-wave is no longer used in the UK or Ireland.\nTypically, the electrical stimulus used in ECT is about 800 milliamps and has up to several hundred watts, and the current flows for between one and six seconds.\nTypically, 70 to 120 volts are applied externally to the patient's head, resulting in approximately 800 milliamperes of direct current passing between the electrodes, for a duration of 100 milliseconds to 6 seconds, either from temple to temple (bilateral ECT) or from front to back of one side of the head (unilateral ECT). However, only about 1% of the electrical current crosses the bony skull into the brain because skull impedance is about 100 times higher than skin impedance.\nIn the US, ECT devices are manufactured by two companies, Somatics, which is owned by psychiatrists Richard Abrams and Conrad Swartz, and Mecta. In the UK, the market for ECT devices was long monopolized by Ectron Ltd, which was set up by psychiatrist Robert Russell.\nMechanism of action.\nDespite decades of research, the exact mechanism of action of ECT remains elusive. A review from 2022 of neuroimaging studies based on a global data collaboration, resulted in a model of temporary disruption of neural circuits followed by augmented neuroplasticity and rewiring. Other brain changes observed after ECT include increased gray matter volume in the frontolimbic areas including the hippocampus and amygdala, increased white matter tracts in the frontal and temporal lobes, increased monoamine neurotransmitters and increased neurogenesis in the dentate gyrus. Changes in sleep architecture due to the induced seizures have also been hypothesized as a mechanism of action.\nUse.\nAs of 2001, it was estimated that about one million people received ECT annually.\nThere is wide variation in ECT use between different countries, different hospitals, and different psychiatrists. International practice varies considerably from widespread use of the therapy in many Western countries to a small minority of countries that do not use ECT at all, such as Slovenia and Luxembourg.\nAbout 70 percent of ECT patients are women. This may be because women are more likely to be diagnosed with depression. Older and more affluent patients are also more likely to receive ECT. The use of ECT is not as common in ethnic minorities.\nIn Sweden, which has a complete register of all ECT treatments in the country, in 2013 the rate of persons treated in that year per 100,000 inhabitants was 41. Almost the same rate had already been present in 1975 with 42 patients per 100,000 inhabitants.\nUnited States.\nECT became popular in the US in the 1940s. At the time, psychiatric hospitals were overrun with patients whom doctors were desperate to treat and cure. Whereas lobotomies would reduce a patient to a more manageable submissive state, ECT helped to improve mood in those with severe depression. A survey of psychiatric practice in the late 1980s found that an estimated 100,000 people received ECT annually, with wide variation between metropolitan statistical areas.\nAccurate statistics about the frequency, context and circumstances of ECT in the US are difficult to obtain because only a few states have reporting laws that require the treating facility to supply state authorities with this information. In 13 of the 50 states, the practice of ECT is regulated by law.\nIn the mid-1990s in Texas, ECT was used in about one third of psychiatric facilities and given to about 1,650 people annually.\nUsage of ECT has since declined slightly; in 2000\u201301 ECT was given to about 1,500 people aged from 16 to 97 (in Texas it is illegal to give ECT to anyone under sixteen). ECT is more commonly used in private psychiatric hospitals than in public hospitals, and minority patients are underrepresented in the ECT statistics.\nIn the United States, ECT is usually given three times a week; in the United Kingdom, it is usually given twice a week. Occasionally it is given on a daily basis. A course usually consists of 6\u201312 treatments, but may be more or fewer. Following a course of ECT some patients may be given continuation or maintenance ECT with further treatments at weekly, fortnightly or monthly intervals. A few psychiatrists in the US use multiple-monitored ECT (MMECT), where patients receive more than one treatment per anesthetic. Electroconvulsive therapy is not a required subject in US medical schools and not a required skill in psychiatric residency training. Privileging for ECT practice at institutions is a local option: no national certification standards are established, and no ECT-specific continuing training experiences are required of ECT practitioners.\nUnited Kingdom.\nIn the UK in 1980, an estimated 50,000 people received ECT annually, with use declining steadily since then to about 12,000 per annum in 2002. It is still used in nearly all psychiatric hospitals, with a survey of ECT use from 2002 finding that 71 percent of patients were women and 46 percent were over 65 years of age. Eighty-one percent had a diagnosis of mood disorder; schizophrenia was the next most common diagnosis. Sixteen percent were treated without their consent. In 2003, the National Institute for Health and Care Excellence, a government body which was set up to standardize treatment throughout the National Health Service in England and Wales, issued guidance on the use of ECT. Its use was recommended \"only to achieve rapid and short-term improvement of severe symptoms after an adequate trial of treatment options has proven ineffective and/or when the condition is considered to be potentially life-threatening in individuals with severe depressive illness, catatonia or a prolonged manic episode\".\nThe guidance received a mixed reception. It was welcomed by an editorial in the \"British Medical Journal\" but the Royal College of Psychiatrists launched an unsuccessful appeal. The NICE guidance, as the \"British Medical Journal\" editorial points out, is only a policy statement and psychiatrists may deviate from it if they see fit. Adherence to standards has not been universal in the past. A survey of ECT use in 1980 found that more than half of ECT clinics failed to meet minimum standards set by the Royal College of Psychiatrists, with a later survey in 1998 finding that minimum standards were largely adhered to, but that two-thirds of clinics still fell short of current guidelines, particularly in the training and supervision of junior doctors involved in the procedure. A voluntary accreditation scheme, ECTAS, was set up in 2004 by the Royal College, and as of 2017[ [update]] the vast majority of ECT clinics in England, Wales, Northern Ireland and the Republic of Ireland have signed up.\nThe Mental Health Act 2007 allows people to be treated against their will. This law has extra protections regarding ECT. A patient capable of making the decision can decline the treatment, and in that case treatment cannot be given unless it will save that patient's life or is immediately necessary to prevent deterioration of the patient's condition. A patient may not be capable of making the decision (they \"lack capacity\"), and in that situation ECT can be given if it is appropriate and also if there are no advance directives that prevent the use of ECT.\nChina.\nECT was introduced in China in the early 1950s and while it was originally practiced without anesthesia, as of 2012 almost all procedures were conducted with it. As of 2012, there are approximately 400 ECT machines in China, and 150,000 ECT treatments are performed each year. Chinese national practice guidelines recommend ECT for the treatment of schizophrenia, depressive disorders, and bipolar disorder and in the Chinese literature, ECT is an effective treatment for schizophrenia and mood disorders.\nAlthough the Chinese government stopped classifying homosexuality as an illness in 2001, electroconvulsive therapy is still used by some establishments as a form of \"conversion therapy\". Alleged Internet addiction (or general unruliness) in adolescents is also known to have been treated with ECT, sometimes without anesthesia, most notably by Yang Yongxin. The practice was banned in 2009 after a news story featuring Yang was published.\nSociety and culture.\nControversy.\nSurveys of public opinion, the testimony of former patients, legal restrictions on the use of ECT and disputes as to the efficacy, ethics and adverse effects of ECT within the psychiatric and wider medical community indicate that the use of ECT remains controversial. This is reflected in the January 2011 vote by the FDA's Neurological Devices Advisory Panel to recommend that FDA maintain ECT devices in the Class III device category for high risk devices, except for patients with catatonia, major depressive disorder, and bipolar disorder. This may result in the manufacturers of such devices having to do controlled trials on their safety and efficacy for the first time. In justifying their position, panelists referred to the memory loss associated with ECT and the lack of long-term data.\nLegal status.\nInformed consent.\nThe World Health Organization (2005) advises that ECT should be used only with the informed consent of the patient (or their guardian if their incapacity to consent has been established).\nIn the US, this doctrine places a legal obligation on a doctor to make a patient aware of the reason for treatment, the risks and benefits of a proposed treatment, the risks and benefits of alternative treatment, and the risks and benefits of receiving no treatment. The patient is then given the opportunity to accept or reject the treatment. The form states how many treatments are recommended and also makes the patient aware that consent may be revoked and treatment discontinued at any time during a course of ECT. The US Surgeon General's Report on Mental Health states that patients should be warned that the benefits of ECT are short-lived without active continuation treatment in the form of drugs or further ECT, and that there may be some risk of permanent, severe memory loss after ECT. The report advises psychiatrists to involve patients in discussion, possibly with the aid of leaflets or videos, both before and during a course of ECT.\nAccording to the US Surgeon General, involuntary treatment is uncommon in the US and is typically used only in cases of great extremity, and only when all other treatment options have been exhausted. The use of ECT is believed to be a potentially life-saving treatment.\nIn one of the few jurisdictions where recent statistics on ECT usage are available, a national audit of ECT by the Scottish ECT Accreditation Network indicated that 77% of patients who received the treatment in 2008 were capable of giving informed consent.\nIn the UK, in order for consent to be valid it requires an explanation in \"broad terms\" of the nature of the procedure and its likely effects. One review from 2005 found that only about half of patients felt they were given sufficient information about ECT and its adverse effects and another survey found that about fifty percent of psychiatrists and nurses agreed with them.\nA 2005 study published in the \"British Journal of Psychiatry\" described patients' perspectives on the adequacy of informed consent before ECT. The study found that \"About half (45\u201355%) of patients reported they were given an adequate explanation of ECT, implying a similar percentage felt they were not.\" The authors also stated:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nInvoluntary ECT.\nProcedures for involuntary ECT vary from country to country depending on local mental health laws.\nUnited States.\nIn most states in the US, a judicial order following a formal hearing is needed before a patient can be forced to undergo involuntary ECT. However, ECT can also be involuntarily administered in situations with less immediate danger. Suicidal intent is a common justification for its involuntary use, especially when other treatments are ineffective.\nIn 2007, a psychiatric patient in the Creedmoor Psychiatric Center in New York, given the pseudonym of Simone D., won a court ruling which set aside a two-year-old court order to undergo ECT against her will.\nUnited Kingdom.\nUntil 2007 in England and Wales, the Mental Health Act 1983 allowed the use of ECT on detained patients whether or not they had capacity to consent to it. However, following amendments which took effect in 2007, ECT may not generally be given to a patient who has capacity and refuses it, irrespective of his or her detention under the Act. In fact, even if a patient is deemed to lack capacity, if they made a valid advance decision refusing ECT then they should not be given it; and even if they do not have an advance decision, the psychiatrist must obtain an independent second opinion (which is also the case if the patient is under age of consent). However, there is an exception regardless of consent and capacity; under Section 62 of the Act, if the treating psychiatrist says the need for treatment is urgent they may start a course of ECT without authorization. From 2003 to 2005, about 2,000 people a year in England and Wales were treated without their consent under the Mental Health Act. Concerns have been raised by the official regulator that psychiatrists are too readily assuming that patients have the capacity to consent to their treatments, and that there is a worrying lack of independent advocacy. In Scotland, the Mental Health (Care and Treatment) (Scotland) Act 2003 also gives patients with capacity the right to refuse ECT.\nRegulation.\nIn the US, ECT devices came into existence prior to medical devices being regulated by the Food and Drug Administration. In 1976, the Medical Device Regulation Act required the FDA to retrospectively review already existing devices, classify them, and determine whether clinical trials were needed to prove efficacy and safety. The FDA initially classified the devices used to administer ECT as . In 2014, the American Psychiatric Association petitioned the FDA to reclassify ECT devices from Class III (high-risk) to Class II (medium-risk). A similar reclassification proposal in 2010 did not pass. In 2018, the FDA re-classified ECT devices as Class II devices when used to treat catatonia or a severe major depressive episode associated with major depressive disorder or bipolar disorder.\nBy country.\nAustralia.\nIn Western Australia, ECT has been heavily restricted since 2014, after a bill passed with bipartisan support introducing restrictions on ECT, which were welcomed by mental health experts. Children under 14 are prohibited from receiving ECT, while those aged 14 to 18 must have informed consent approval from the Mental Health Tribunal. The law imposes a $15,000 fine on anyone who performs ECT on a child under the age of 14.\nSimilarly, ECT is also banned on children under the age of 12 in the Australian Capital Territory (ACT).\nUnited States.\nMany mental health facilities offer ECT for specific diagnoses, such as chronic depression, mania, catatonia and schizophrenia. However, ECT is often only used as a treatment of last resort. To be considered for ECT, often testing such as an EKG and lab tests are required, in addition to a physical and neurological exam. Certain medications and conditions, such as cardiac conditions or hypertension, may disqualify a patient from ECT. Patients should give proper informed consent before ECT is performed. In the United States, ECT is performed under general anesthesia. Both trained health professionals with experience in ECT administration as well as a specifically trained and certified anesthesiologist should administer the procedure and anesthesia respectively.\nPublic perception.\nA questionnaire survey of 379 members of the general public in Australia indicated that more than 60% of respondents had some knowledge about the main aspects of ECT. Participants were generally opposed to the use of ECT on depressed individuals with psychosocial issues, on children, and on involuntary patients. Public perceptions of ECT were found to be mainly negative. A sample of the general public, medical students, and psychiatry trainees in the United Kingdom found that the psychiatry trainees were more knowledgeable and had more favorable opinions of ECT than did the other groups. More members of the general public believed that ECT was used for control or punishment purposes than medical students or psychiatry trainees.\nFictional examples.\nElectroconvulsive therapy has been depicted in fiction, including fictional works partly based on true experiences. These include Sylvia Plath's semi-autobiographical novel \"The Bell Jar\", Ken Loach's film \"Family Life\", and Ken Kesey's novel \"One Flew Over the Cuckoo's Nest\"; Kesey's novel was a direct product of his time working the graveyard shift as an orderly at a mental health facility in Menlo Park, California.\nTwo analyses of large numbers of films using ECT scenes found that almost all presented fictional settings that were unrelated to real treatment routines and were apparently aimed at stigmatizing ECT as a tool of repression and of mind and behavior control - having effects of memory-erosion, pain and damage.\nThe song \"The Mind Electric\" by Miracle Musical is typically interpreted as depicting someone undergoing ECT.\nIn the television series \"Mr Bates vs The Post Office\", which is based on true events, the character of Saman Kaur receives ECT following a deep depression and attempted suicide.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44095", "revid": "50079837", "url": "https://en.wikipedia.org/wiki?curid=44095", "title": "Sui generis", "text": "Latin phrase meaning in its own class\n is a Latin phrase that means \"of its/their own kind\" or \"in a class by itself\", therefore \"unique\". It denotes an exclusion to the larger system an object is in relation to.\nSeveral disciplines use the term to refer to unique entities. These include:\nBiology.\nIn the taxonomical structure \"genus \u2192 species\", a species is described as if its genus was created to classify it (i.e., its uniqueness at the time of classification merited the creation of a new genus, the sole member of which was initially the species). A species that is the sole \"extant\" member of its genus (e.g., the genus \"Homo\") is not necessarily ; extinctions may have eliminated other congeneric species. A \"sui generis\" genus may also be called a monotypic genus.\nCreative arts.\nA book, movie, television series, or other artistic creation is called when it does not fit into standard genre boundaries. Movie critic Richard Schickel identifies\" Joe Versus the Volcano\" as a movie. Film critic Michael Brooke used the term to describe \"Fantastic Planet\", a 1973 Franco-Czech sci-fi animated film directed by Ren\u00e9 Laloux.\nLaw.\nIn law, it is a term of art used to identify a legal classification that exists independently of other categorizations, either because of its singularity or due to the specific creation of an entitlement or obligation. For example, a court's contempt powers arise\" sui generis\" and not from statute or rule. The New York Court of Appeals has used the term in describing cooperative apartment corporations, mostly because this form of housing is considered real property for some purposes and personal property for other purposes.\nWhen citing cases and other authorities, lawyers and judges may refer to \"a case\", or \"a authority\", meaning it is a special one confined to \"its own\" facts and, therefore, may not be of broader application.\nIntellectual property law.\nGenerally speaking, protection for intellectual property extends to intellectual creations to incentivize innovation and depends upon the nature of the work and its \"characteristics\". The main types of intellectual property law are: copyright, which protects creative works; patent, which protects invention; trade secret, which protects information not generally known or readily ascertainable that is valuable to the secret holder; and trademark, which protects branding and other exclusive properties of products and services. Any matter that meets these criteria can be protected.\n statutes exist in many countries that extend intellectual property protection to matter that does not meet characteristic definitions. For example, integrated circuit layouts, ship hull designs, fashion designs in France, databases, and plant varieties require statutes because of their unique characteristics. The United States, Japan, Australia, and many EU countries protect the topography of semiconductor chips and integrated circuits under \"sui generis\" laws, which borrow some aspects from patent or copyright law. In the U.S., this law is known as the Semiconductor Chip Protection Act of 1984.\nStatutory law.\nIn statutory interpretation, \"sui generis\" refers to the problem of giving meaning to groups of words where one of the words is ambiguous or inherently unclear.\nIn road traffic law, a statute may require consideration of large vehicles separately from other vehicles. The word \"large\" is ambiguous \"per se\" but may be considered \"heavy\". The relevant legislation (in Australian law) contain sections called \"Terms used\" or \"Definitions\" that itemise all words deemed ambiguous and confers specific interpretations consistent with natural language. One indicates \"heavy vehicle\" means a vehicle with a GVM over 4.5 tons, and GVM means \"gross vehicle mass\", the maximum loaded mass of the vehicle. Further explanations cover various contingencies. Thus, \"large\" is equivalent to \"heavy\" and is (for this unique case) clearly defined \"sui generis\".\nIn U.S. attorney admissions, an applicant for admission to the practice of law may be referred to the state committee on character and fitness, where proceedings are \"neither civil nor criminal, but are sui generis\".\nTown planning law.\nIn town and country planning in the United Kingdom, in particular, relating to the Town and Country Planning (Use Classes) Order 1987, many common types of land use are categorised into specific \"use classes\". Change of use of land within a use class does not require planning permission; changing between use classes might require planning permission, and approval is always needed if the new use is .\nExamples of uses include embassies, theatres, amusement arcades, laundrettes, taxi or vehicle hire businesses, petrol filling stations, scrapyards, nightclubs, motor car showrooms, retail warehouses, clubs and hostels.\nAs of 1 September 2020 following the Town and Country Planning (Use Classes) (Amendment) (England) Regulations 2020, the following uses were added as sui generis:\nThe grant of private hire vehicle (taxicab) operators licences by local authorities frequently has a condition that the appropriate \"sui generis\" change of use planning permission is granted to those premises to ensure those businesses cannot trade lawfully without the relevant planning consent.\nInternational law.\nWhen applied to international law, \"\" refers to situations which are distinct and thus not easily categorized under existing legal frameworks or conventions. in international law may suggest novel legal frameworks to address unprecedented issues using a new set of legal principles. For example, the legal status of the internet or space law.\n\"Sui generis\" systems can be crucial in international law because they allow the international community to develop adaptive legal responses to emerging global challenges and contexts that are not adequately addressed by traditional international law. They often also serve as precursors to the evolution of wider customary international law or the development of new treaties and conventions. The uniqueness of legal regimes can sometimes make them challenging to interpret, enforce, or harmonize with existing legal frameworks.\nAnother example of \"Sui Generis\" can be seen in the field of laws of war in international law. According to scholar Noura Erakat in \"Justice for Some: Law and Question of Palestine (2019)\", especially within the period of the Second Intifada of the Israeli\u2013Palestinian conflict, Israel uses the concept of \"Sui Generis\" to justify its military actions in the occupied territories, in contravention of pre-existing legal norms. She refers to its use of military pre-emptive strikes towards Palestinians suspected of being involved with Palestinian militant organisations such as in the West Bank and Gaza Strip. Erakat highlights its use of \"Sui Generis\" through the legal inconsistencies that arises when balancing Israel\u00b4s responsibility as the occupying power of the west bank and its justification for its use of preemptive strikes. Another example Erakat refers to is Israel's legal interpretation of the laws of war in The Second Intifada. Erakat argues that, to not grant Palestinian militant groups the rights of belligerent groups and to non-combatants in the occupied territories given by the international laws of conflict, it had argued that conflict as \"short of war\", under the concept of \"Sui Generis\". This for Erakat, allowed Israel to wield international law in order to fulfill its aims during the conflict without following the pre-existing legal conventions regarding armed conflict.\nPhilosophy.\nAnalytic philosophy often uses the expression to indicate an idea, an entity, or a reality that cannot be reduced to a lower concept or included in a higher concept. G.\u00a0E. Moore, for example, refuted reductive ethical naturalism in moral theories like utilitarianism by arguing that moral properties (like good or bad) could not be reduced to or equated with non-moral properties (like pleasure) because moral properties are sui generis. This can be seen in Moore's open-question argument.\nPolitics and society.\nIn political philosophy, the unparalleled development of the European Union as compared with other international organizations has led to its designation as a geopolitical entity. The legal nature of the EU is widely debated because its mixture of intergovernmental and supranational elements causes it to share characteristics with both confederal and federal entities. It is generally considered more than a confederation but less than a federation, thus being appropriately classified as an instance of neither political form. Compared to other international organizations, the EU is often considered \"sui generis\" because its legal system comprehensively rejects any use of retaliatory sanctions by one member state against another.\nA similar case that led to the use of the label is the relationship of New Caledonia relative to France because the legal status of New Caledonia can aptly be said to lie somewhere between a French overseas collectivity and a sovereign nation. Although other examples of such status for further dependent or disputed territories may exist, this arrangement is unique within the French realm.\nThe legal status of the Holy See is also described as a entity possessing an international personality. The Sovereign Military Order of Malta has likewise been described as a \" primary subject of public international law\". Another entity widely considered to have \"sui generis\" international legal personality is the International Committee of the Red Cross.\nIn local government, a entity does not fit with a country's general scheme of local governance. For example, in England, the City of London and the Isles of Scilly are the two \"sui generis\" localities, as their forms of local government are both (for historical or geographical reasons) very different from those of elsewhere in the country.\nIn a press conference during which reporters were trying to analyse his political personality, Huey Long said \"say that I am \"sui generis\", and let it go at that\".\nThe Joint Council of Municipalities in Croatia is a council of municipalities in the east of the country that was formed after the Erdut Agreement and UNTAES mission aimed at protection of the rights of the ethnic Serb community in the region and is, as such, a unique form of local cooperation and minority self-government in Croatia.\nSociology.\nIn sociology, methodological holists argue that social phenomena exist in their own right (sui generis) and are not reducible to the actions of individuals. For example, Emile Durkheim argued that the suicide rate was a social phenomenon sui generis (existing over and above the actions of individuals) In a social constructionist perspective, \" is what has been externalized, then internalized in the overall public and becomes a part of society that exists in its construct. It is not something that is not thought to have been created because it is embedded in everyone's way of thinking and being. Instances include love, going to school, or clothing belonging to a specific gender. These examples are \"sui generis\" for they exist in society and are widely accepted without thoughts of where they come from or how they were created.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44096", "revid": "14965160", "url": "https://en.wikipedia.org/wiki?curid=44096", "title": "Hans von Bodeck", "text": "Prussian diplomat\nHans von Bodeck (1582 \u2013 23 June 1658) was a Prussian diplomat and chancellor of the Hohenzollern Prince-electors of Brandenburg-Prussia.\nBiography.\nBodeck came from a prominent patrician family of Elbing (Elbl\u0105g) in the Polish province of Royal Prussia. His grandfather was the burgomaster, while his father was a city councilman. His ancestor Johann III von Bodeck (1542\u20131595) received imperial status from Emperor Rudolf II and was allowed to improve the family's coat of arms. The family also held offices in Danzig (Gda\u0144sk).\nIn order to find a trading partner Bodeck was sent on a diplomatic mission from Elbing throughout Europe. During that time he wrote \"liber amicorum\", which is now studied by musicologists. During his diplomatic tour Bodeck visited the Netherlands, France, Switzerland, and England, where he attended the Universities of Oxford and Cambridge. He attended the funeral ceremony for Queen Elizabeth I of England and the coronation of the new English king, James I. The council of Elbing had sent two delegates with dual missions: firstly, to pay its respects to the new king and secondly to oppose the transfer of English trade from Elbing to nearby Danzig.\nIn 1604 Bodeck left for London and met John Dowland, Philip Rosseter, and Thomas Campion. All three composers of lute songs lived in the same district of London. Bodeck befriended them, and Campion wrote a song dedicated to Bodeck. Many people from England and Scotland came to live in Elbling.\nLater that year Bodeck left for Paris and met Count Christopher von Dohna, a nobleman of Prussia, who lived 15\u00a0km from Elbing. Bodeck then became the chancellor to Elector Joachim Frederick of Brandenburg. He died in 1658 in Hamburg.\nA collection of pieces for lute was purchased by Dohna and kept at the Elbing library. In 1929 Hans Bauer wrote a full description of the register of Bodeck. During the capture of Elbing by the Soviet Red Army in 1945 during World War II and the subsequent expulsion of the city's German populace, the library was destroyed. Many Prussian documents and original manuscripts have since been discovered in Krak\u00f3w, leading music researchers to hope that some of Bodeck's works might resurface.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44099", "revid": "26830857", "url": "https://en.wikipedia.org/wiki?curid=44099", "title": "HMS Antelope", "text": "Eleven ships of the Royal Navy have been named HMS \"Antelope\", after the Antelope:\nBattle honours.\nShips named \"Antelope\" have earned the following battle honours:\nReferences.\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n List of ships with the same or similar names\nThis article includes a with the same or similar names. If an [ internal link] for a specific ship led you here, you may wish to change the link to point directly to the intended ship article, if one exists."}
{"id": "44101", "revid": "50464812", "url": "https://en.wikipedia.org/wiki?curid=44101", "title": "Satellite state", "text": "Country which is nominally sovereign but under extensive influence from a larger state\nA satellite state or dependent state is a country that is formally independent but under heavy political, economic, and military influence or control from another country. The term was coined by analogy to planetary objects orbiting a larger object, such as smaller moons revolving around larger planets, and is used mainly to refer to Central and Eastern European member states of the Warsaw Pact during the Cold War, as well as to Mongolia and Tuva between 1924 and 1990, all of which were economically, culturally, and politically dominated by the Soviet Union. While primarily referring to the Soviet-controlled states in Central and Eastern Europe or Asia, in some contexts the term also refers to other countries under Soviet hegemony during the Cold War, such as North Korea (especially in the years surrounding the Korean War of 1950\u20131953), Cuba (particularly after it joined the Comecon in 1972), North Vietnam during the Vietnam War, and some countries in the American sphere of influence, such as South Vietnam during 1964\u20131973. In Western usage, the term has seldom been applied to states other than those in the Soviet orbit. In Soviet usage, the term applied to states in the orbit of Nazi Germany, Fascist Italy, and Imperial Japan, whereas in the West the term to refer to those has typically been \"client states\".\nThe \"Oxford English Dictionary\" traces the concept of satellite states in English back as early as 1780. In times of war or political tension, satellite states sometimes served as buffers between an enemy country and the nation exerting control over the satellites.\nSoviet satellite states.\nInterwar period.\nWhen the Mongolian Revolution of 1921 broke out, Mongolian revolutionaries expelled the Russian White Guards (during the Russian Civil War of 1917\u20131923 following the October Revolution of 1917) from Mongolia, with the assistance of the Soviet Red Army. The revolution also officially ended Manchurian sovereignty over Mongolia, which had existed since 1691. Although the theocratic Bogd Khanate of Mongolia still nominally continued, with successive series of violent struggles, Soviet influence grew stronger. In 1924, after the Bogd Khan died of laryngeal cancer or, as some sources suggest, at the hands of Soviet spies, the Mongolian People's Republic was proclaimed on November 26, 1924. A nominally independent and sovereign country, it has been described as being a satellite state of the Soviet Union in the years from 1924 until 1990. This is supported by the fact that the Mongolian PR collapsed less than two months after the dissolution of the Soviet Union.\nDuring the Russian Civil War, Red Army troops occupied Tuva in January 1920, which had also been part of the Qing Empire of China and a protectorate of Imperial Russia. The Tuvan People's Republic was proclaimed a nominally independent state in 1921, although it was tightly controlled by Moscow and is considered a satellite state of the Soviet Union until 1944, when the USSR annexed it into the Russian SFSR.\nAnother early Soviet satellite state in Asia was the short-lived Far Eastern Republic in Siberia.\nPost-World War II.\nAt the end of World War II, most Eastern and Central European countries were occupied by the Soviet Union, and along with the Soviet Union made up what is called the Soviet empire. Soviet forces remained in these countries after the war's end. Through a series of coalition governments including communist parties, and then a forced liquidation of coalition members opposed by the Soviets, Stalinist systems were established in each country. Stalinists gained control of existing governments, police, press and radio outlets in these countries. Soviet satellite states of the Cold War included:\nAlbania, Romania, and Yugoslavia ceased to be satellites before the revolutions of 1989. The Federal People's Republic of Yugoslavia is considered an early Soviet satellite, as it broke from Soviet orbit in the 1948 Tito\u2013Stalin split, with the Cominform offices being moved from Belgrade to Bucharest, and Yugoslavia subsequently formed the Non-Aligned Movement. The People's Socialist Republic of Albania, under the leadership of Enver Hoxha, broke ties with the Soviet Union in the Albanian\u2013Soviet split following the Soviet de-Stalinisation process, and removed itself from Soviet influence in 1961. Romania's de-satellization process started in 1956 and ended by 1965, with serious economic disagreements with Moscow resulting in a final rejection of Soviet hegemony in 1964.\nFrom 1945 to 1948 North Korea was under Soviet Civil Administration, following this provisional governments were established under the Provisional People's Committee of North Korea and People's Committee of North Korea resulting in the establishment of the Democratic People's Republic of Korea in 1948. Some scholars consider North Korea a satellite state under the Soviet Union from 1948 until the 1958 August faction incident.\nThe short-lived East Turkestan Republic (1944\u20131949) was a Soviet satellite until it was absorbed into the People's Republic of China. Between 1945 and the Iran crisis of 1946 the Azerbaijan People's Government and Republic of Mahabad existed as satellite states in Soviet-occupied Iran. The Democratic Republic of Afghanistan was a satellite regime of the Soviet Union from 1978 to 1991. Between 1979 and 1989, Afghanistan was also under Soviet military occupation.\nPost-Cold War usage of the term.\nSome commentators have expressed concern that United States military and diplomatic interventions in the Balkans, in the Middle East, and elsewhere might lead, or perhaps have already led, to the existence of American satellite states. William Pfaff warned that a permanent American presence in Iraq would \"turn Iraq into an American satellite state\". In the Asia-Pacific, John Pilger accused ex Australian Prime Minister John Howard of turning the country into America's 51st state and South Korea has regularly been described by North Korea for being a \"puppet state\" of the United States.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44103", "revid": "50221419", "url": "https://en.wikipedia.org/wiki?curid=44103", "title": "Tribute in Light", "text": "Annual tribute to 9/11 victims\nThe Tribute in Light is an art installation created in remembrance of the September 11 attacks. It consists of 88 vertical searchlights arranged in two columns of light to represent the Twin Towers. It stands six blocks south of the World Trade Center on top of the Battery Parking Garage in New York City. \"Tribute in Light\" began as a temporary commemoration of the attacks in early 2002, but it became an annual event, currently produced on September 11 by the Municipal Art Society of New York. The \"Tribute in Light\" was conceived by artists John Bennett, Gustavo Bonevardi, Richard Nash Gould, Julian LaVerdiere, and Paul Myoda, and lighting consultant Paul Marantz.\nThe Tribute's illumination begins at dusk and ends at dawn, with the lights being turned off for 20-minute periods to allow migratory birds to escape as needed. On clear nights, the lights can be seen from away, visible in all of New York City and most of suburban Northern New Jersey and Long Island. The lights can also be seen in Fairfield County, Connecticut, as well as Westchester, Orange, and Rockland counties in New York.\nThe 88 xenon spotlights (44 for each tower) each consume 7,000 watts. As of 2011[ [update]], the annual cost for the entire project was about $500,000.\nA similar \"Tribute in Light\" has also appeared on occasion at the Pentagon in Arlington County, Virginia and at the crash site of United 93 in Shanksville, Pennsylvania, which were also targeted during the 9/11 attacks.\nBackground.\nAfter the September 11 attacks, several people independently conceived the idea of using lights for remembrance. These efforts were merged under the umbrella of the Municipal Art Society and Creative Time.\n\"Tribute in Light\" began construction on March 11, 2002, and initially ran as a temporary installation from March 11 to April 14, 2002, and it ran again on September 11, 2003, to mark the second anniversary of the attack. Since then, it has been repeated every year on September 11. It was announced that 2008 would be its final year, but the tribute was continued in 2009.\nOn December 17, 2009, it was confirmed that the tribute would continue through the tenth anniversary of the attacks in 2011. In 2012, plans were underway for the National September 11 Memorial &amp; Museum to assume the lease for the MTA property used during this tribute, and to begin transitioning operation of the tribute from the Municipal Art Society parking garage at The Battery to the National September 11 memorial foundation. However, the moving of the tribute has not yet occurred as of 2025.\nThe lights are produced by an Italian company named Space Cannon, which sends a team every year to help with the installation. A Las Vegas-based company, Light America, was also part of the team who implemented the project.\nEach year, about 30 technicians, electricians, and stagehands work for about ten days to install the lights. During a testing phase of several days, observers in Brooklyn, Staten Island, New Jersey, and uptown Manhattan help make sure that the beams are adjusted accurately.\nThe project was originally going to be named \"Towers of Light\", but the victims' families felt that the name emphasized the buildings destroyed instead of the people killed.\nA permanent fixture of the \"Tribute in Light\" was at one point intended to be installed on the roof of One World Trade Center, but it was not included in the finished design.\nSince 2008, the generators that power \"Tribute in Light\" have been fueled with biodiesel made from used cooking oil collected from local restaurants.\nEffects on birds.\nThe light pollution from \"Tribute in Light\" has caused confusion for over a million migrating birds, trapping them in the beams. Even at an altitude of several miles, birds can be affected by the lights. As a result of this effect, the beams are switched off for 20-minute periods to allow the birds to escape. To ensure the lights do not affect migrating birds, the Municipal Art Society works with the NYC Bird Alliance on the illumination. A 2017 study found that the installation \"dramatically altered multiple behaviors of nocturnally migrating birds\u2014but these effects disappeared when lights were extinguished\".\nIn popular culture.\n\"Tribute in Light\" was featured in Boyz II Men's music video for \"Color of Love\". It made a notable appearance during the opening credits of Spike Lee's 2002 film \"25th Hour\". The tribute was also shown and referenced in the CBS series \"Blue Bloods\". These lights were featured in the music video of U2's \"You're the Best Thing About Me\".\nThe video game adaptation of the film \"Spider-Man 2\" features the lights at the approximate location of the WTC site, while another video game adaptation of the film \"The Amazing Spider-Man 2\", the lights are seen on the construction site of One World Trade Center at night.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44104", "revid": "30552921", "url": "https://en.wikipedia.org/wiki?curid=44104", "title": "Sigismund von Herberstein", "text": "Carniolan writer and diplomat (1486\u20131566)\nSiegmund (Sigismund) Freiherr von Herberstein (or Baron Sigismund von Herberstein; 23 August 1486 \u2013 28 March 1566) was a Carniolan diplomat, writer, historian and member of the Holy Roman Empire Imperial Council. He was most noted for his extensive writing on the geography, history and customs of Russia, and contributed greatly to early Western European knowledge of that area.\nEarly life.\nHerberstein was born in 1486 in Vipava () in the Duchy of Carniola, now in Slovenia, then part of the Habsburg monarchy. His parents were Leonhard von Herberstein and Barbara von Lueg, members of the prominent German-speaking family which had already resided in Herberstein Castle for nearly 200 years. Little is known of his early life apart from the fact that he became familiar with the Slovene language spoken in the region. This knowledge became significant later in his life.\nIn 1499, he entered the University of Vienna to study philosophy and law. In 1506, he entered the army as an officer and served in a number of campaigns. In 1508, he was knighted by the Holy Roman Emperor, Maximilian I, in person. In 1515, he entered the Imperial council, or Parliament, and began a long and illustrious diplomatic career.\nDiplomatic career.\nBetween 1515 and 1553, Herberstein carried out approximately 69 missions abroad, travelling throughout much of Europe, including Turkey. He was feted by the ruling Habsburgs and rewarded with titles and estates. He was twice sent to Russia as ambassador of the Holy Roman Emperor, in 1517 to attempt to arrange a truce between Russia and Lithuania, and in 1526 to renew a treaty between the two signed in 1522. These extended visits (nine months in his 1517 visit) provided him with the opportunity to study a hitherto largely unknown Russian society.\nWriting on Russia.\nHerberstein's knowledge of Slovene, acquired in his youth, allowed him to communicate with Russians, as Slovene and Russian both belong to the Slavic languages. He used this ability to question a variety of people in Russia on a wide range of topics. This gave him an insight into Russia and Russians unavailable to the few previous visitors to Russia. \nHe probably wrote his first account of life in Russia between 1517 and 1527, but no copy of this survives. In 1526, he was asked to produce a formal report on his experiences in Russia, but this remained relatively unnoticed in the archives until he was able to find time to revise and expand it, which he possibly started in the 1530s.\nThe evidence suggests that Herberstein was an energetic and capable ethnographer. He investigated in depth both by questioning locals and by critically examining the scarce existing literature on Russia. The result was his major work, a book written in Latin titled \"Rerum Moscoviticarum Commentarii\" (\"Notes on Muscovite Affairs\"), published in 1549. This became the main early source of knowledge in Western Europe on Russia.\nHe was the first to record the spelling of \"tsar\" as \"czar\" (both spellings are meant to express the same pronunciation). Later, English and French began to move from the 'cz' spelling to the 'ts' spelling in the 19th century.\nHerberstein also referred to a \"Russian road guide\" that came into his possession for geographical data on the northern Urals and Siberia, which indicated that the territories were well known to the Russians at the time."}
{"id": "44105", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=44105", "title": "September 11, 2001 Terrorist Attack/Timeline March 2002", "text": ""}
{"id": "44106", "revid": "44361418", "url": "https://en.wikipedia.org/wiki?curid=44106", "title": "Marwan al-Shehhi", "text": "Emirati terrorist and 9/11 hijacker (1978\u20132001)\nMarwan al-Shehhi (9 May 1978\u00a0\u2013 11 September 2001) was an Emirati terrorist hijacker for al-Qaeda. He was the hijacker-pilot on United Airlines Flight 175 (Boeing 767). As part of the September 11 attacks, he crashed the plane into the South Tower of the World Trade Center in a suicide attack. He was one of five hijackers aboard the aircraft and one of two Emiratis to take part in the attacks, the other being Fayez Banihammad, who helped him hijack the same plane.\nAl-Shehhi was a university student in applied sciences from the United Arab Emirates. In 1996, at the age of 18, he moved to Germany to pursue his university education. There, he met Mohamed Atta and Ziad Jarrah, who were also studying applied sciences and engineering. Shortly thereafter, Ramzi bin al-Shibh, who provided them with financial support, joined them, and they formed the Hamburg cell. Together, after pledging their lives to martyrdom (\"Jihad\"), they became the leaders of the September 11 attacks.\nIn late 1999, Ramzi bin al-Shibh invited three of them to al-Qaeda training camps in Afghanistan, and they went. They met with Osama bin Laden, who assigned tasks to three members of the Hamburg cell for the attacks in the United States. He arrived in the United States in May 2000, one month before Atta. Atta, al-Shehhi, and Jarrah had been trained as pilots in Florida at Huffman Aviation, receiving their commercial pilot licenses in December 2000 and January 2001 from the FAA.\nAl-Shehhi spent his time making preparations for the attack itself, such as meeting with crucial planners abroad, assisting with the arrival of hijackers aboard the other flights, and travelling on surveillance flights determining details on how the hijacking would take place. On 9 September 2001, he traveled from Florida to Boston, where he stayed at the Milner Hotel until 11 September. After boarding United Airlines Flight 175 at Logan International Airport, al-Shehhi and 4 other hijackers waited 30 minutes into the flight to make their attack, which then allowed al-Shehhi to take over control as pilot, and at 9:03\u00a0am, 17 minutes after Mohamed Atta crashed American Airlines Flight 11 into the North Tower, Al-Shehhi crashed the Boeing 767 into the South Tower of the World Trade Center between floors 77 to 85. At 23 years of age, he was the youngest hijacker-pilot to participate in the attacks. The impact of the Boeing 767 into the South Tower was seen live on television around the world as it happened. At 9:59\u00a0am, after 56 minutes of burning, the 110-story skyscraper collapsed, killing around 900 between office workers and first responders.\nEarly life.\nMarwan Yousef Mohamed Rashid Lekrab al-Shehhi was born on 9 May 1978, in Al Qusaidat, Ras Al Khaimah, United Arab Emirates, the son of an Arab imam father and an Egyptian mother. \nDescribed as a quiet and devout Muslim, details about al-Shehhi's life in the UAE, however, are difficult to acquire. He was a part of the Shihuh tribe through his father's side. According to an October 2001 article in \"The New York Times\", \"If residents of Mr. Shehhi's hometown had heard of him before now, they were certainly not telling strangers. Four hours spent in the community yielded no address and no one \u2013 policemen, firemen, pedestrians or local officials \u2013 who did anything more than shrug at the mention of his name.\"\nHis teacher in Germany, Gabriele Bock, recalls him as someone who seemed to be struggling to have plans for the future while studying there.\nWhile in Germany, al-Shehhi enrolled in the University of Bonn after completing a German course. He left Germany in June 1997 to attend to problems at home although the university forbade him. In early 1998, al-Shehhi transferred to the Technical University of Hamburg. A poor student, al-Shehhi was directed by the Scholarship program administrators to repeat a semester of his studies back in Bonn beginning in August 1998. Al-Shehhi did not enroll back at Bonn until January 1999 and continued to struggle with his studies. By July 1999, Marwan returned to Hamburg to study shipbuilding.\nIt has been reported al-Shehhi also married in 1999, holding a belated celebration in January 2000, in an arranged marriage by his half-brother with a young woman named Fawzeya.\nRadicalization.\nAfter moving to Hamburg in 1998, al-Shehhi helped form the Hamburg cell with Mohamed Atta and Ramzi bin al-Shibh. There, his views became more and more radical. They met three or four times a week to discuss anti-American feelings and plot possible attacks. When someone asked why he and Atta never laughed, al-Shehhi retorted, \"How can you laugh when people are dying in Palestine?\"\nOn 9 October 1999, Marwan al-Shehhi was filmed at Said Bahaji's wedding in Germany with other 9/11 hijackers including Ziad Jarrah.\nIn late 1999, al-Shehhi, Atta, Ziad Jarrah, Said Bahaji, and Ramzi bin al-Shibh decided to travel to Chechnya to fight against the Russians, but were convinced by Khalid al-Masri and Mohamedou Ould Slahi at the last minute to change their plans. They instead traveled to Afghanistan to meet with Osama bin Laden and trained for terrorist attacks. Immediately afterwards, Atta, al-Shehhi, and Jarrah reported their passports stolen, possibly to erase travel visas to Afghanistan. After their training, the hijackers began to attempt to hide their radicalism. al-Shehhi shaved his beard and seemed to his old friends like he had become less religious. After the attacks, a librarian in Hamburg reported that al-Shehhi boasted to her \"There will be thousands of dead. You will think of me ... You will see, in America something is going to happen. There will be many people killed.\"\nIn the United States.\nFlight education and preparation.\nAl-Shehhi was the first of the Hamburg group to leave for the United States. He arrived in Newark, New Jersey on 29 May 2000. Atta and Jarrah joined him the next month, and the three men began to search for flight schools. Al-Shehhi and Jarrah posed as body guards of Atta, who was posing as a \"Saudi Arabian royal family member\" while the three of them took flying lessons in Venice, Florida. They logged hundreds of hours on a Boeing 727 flight simulator. They received their licenses by December 2000. Their expenses were paid for by Ali Abdul Aziz Ali. On either 26 or 27 December, Atta, Jarrah, and al-Shehhi abandoned a Piper Cherokee that had stalled on the runway of Miami International Airport. On 31 December, the trio went to the Opa-Locka Airport and practiced on a Boeing 727 simulator. \nTravels in early 2001.\nAfter completing their flight training Atta, Jarrah, and al-Shehhi all took foreign trips during the holiday season of 2000\u20132001. Atta went to Berlin, Germany to coordinate with Ramzi bin al-Shibh about their flight training, Jarrah went to Lebanon and Germany to visit his family and girlfriend, while al-Shehhi went to Morocco then to the United Arab Emirates to visit his wife. After not hearing from al-Shehhi for a long time, his family reported him missing to the government of the UAE. As a response, the embassy in UAE contacted Hamburg to inform the authorities. After learning that his family were concerned for him, al-Shehhi finally called them on 20 January 2001 to assure them that he was okay.\nAtta and al-Shehhi both encountered some difficulty reentering the United States on 10 and 19 January, respectively. As neither had presented a student visa, both of them had to persuade INS inspectors that they should be admitted so that they could continue their flight training. Neither operative had any problem clearing customs. After returning to Florida from their trips, Atta and al-Shehhi visited Georgia, staying briefly in Norcross and Decatur, and renting a single-engine plane to fly with an instructor in Lawrenceville. By 19 February, Atta and al-Shehhi were in Virginia. They had rented a mailbox in Virginia Beach, cashed a check, and then promptly returned to Georgia, staying in Stone Mountain. In mid-March, Ziad Jarrah was in Georgia as well, staying in Decatur. At the end of the month, Jarrah left the United States again and visited Seng\u00fcn in Germany for two weeks. In early April, Atta and al-Shehhi returned to Virginia Beach and closed the mailbox they had opened in February.\nAtta and al-Shehhi returned to Virginia Beach from their travels in Georgia, making their way to a large Dar Al-Hijrah mosque, sometime in early April. They were joined there by 9/11 hijackers Nawaf al-Hazmi and Hani Hanjour who had moved out of San Diego and Arizona after living in or visiting Abdussattar Shaikh's house, where Khalid al-Mihdhar also stayed. The mosque had recently hired the same imam Anwar al-Awlaki with whom Hazmi had spent time at the Rabat mosque in San Diego. He remembered Hazmi from San Diego but denied having contact with Hazmi or Hanjour in Virginia. Atta and al-Shehhi returned to Florida and moved into an apartment in Coral Springs. Atta stayed in Florida, awaiting the arrival of the first muscle hijackers. Al-Shehhi, on the other hand, bought a ticket to Cairo and flew there from Miami on 18 April. Al-Shehhi met with Atta's father, who stated in a post-9/11 interview that al-Shehhi wanted to pick up Atta's international driver's license and some money.\nAl-Shehhi returned to Miami on 2 May. That day, Atta and Jarrah were together, about 30 miles to the north, visiting a Department of Motor Vehicles office in Lauderdale Lakes, Florida, to get Florida driver's licenses. 4 days later on 6 May 2001 Atta, Jarrah, and al-Shehhi signed up at the US1 Fitness Center, a gym in Dania Beach, Florida where the men learned how to fight in hand-to-hand combat. Al-Shehhi began to take \"surveillance flights\" in the summer of 2001, watching the operations of flight crews and making final preparations.\nAugust 2001.\nOn 26 August, al-Shehhi signed into the Panther Motel in Deerfield Beach, Florida, paying US$500, saying he wanted to stay until 2 September, and listing a Mailboxes Etc. as his permanent address. His register entry indicated that he was driving a blue Chevrolet Malibu, assumed to be the one rented by Atta two weeks prior, and manager Richard Surma said that he bent rules to allow al-Shehhi to have another man as an overnight guest. On 28 August, al-Shehhi went to the Miami International Airport, accompanied by an unknown man, where he purchased his ticket for Flight 175.\nSeptember 11 attacks and death.\nOn 9 September, al-Shehhi flew from Florida to Boston and stayed in the Milner Hotel where he shared a room with 3 other hijackers, Satam al-Suqami, Fayez Banihammad, and Mohand al-Shehri, and it is also known that on 10 September al-Shehhi had received a phone call from Abdulaziz al-Omari. The call lasted 4 minutes and it remains unknown to authorities as to what the men discussed.\nAt 5:01 a.m. on the morning of 11 September, Al-Shehhi in Boston received a phone call from Flight 93 hijacker pilot Ziad Jarrah in Newark. The call would be the last time Jarrah and al-Shehhi spoke and is believed by authorities to be the two confirming to one another that the attacks were ready to begin.\nAfter arriving at Logan International Airport later that morning, al-Shehhi made a call to Mohamed Atta lasting from 6:52 to 6:55, who was elsewhere in the same airport as both American 11 and United 175 were to fly from Logan to Los Angeles International Airport. The call is believed to have served the same purpose as Al-Shehhi's earlier call to Jarrah.\nBetween 7:23 and 7:28, the five hijackers each boarded the plane, with al-Shehhi taking his seat in 6C. The plane became airborne at 8:14, only to be hijacked 28 minutes after takeoff. The terrorists gained access to the cockpit through unknown means and murdered both pilots, allowing al-Shehhi to assume control of the flight. Shortly after the hijacking, the plane came close to colliding with another aircraft in the vicinity, Delta Air Lines Flight 2315. Approaching New York City, al-Shehhi saw smoke and fire pouring southeast from the World Trade Center's North Tower after it was struck by Flight 11 at 8:46\u00a0am, and narrowly avoided a second mid-air collision with Midwest Express Flight 7.\nFlying at speeds of around while carrying approximately 9,100 gallons (approximately 34,447 liters) of jet fuel, al-Shehhi crashed the plane into the South Tower at 9:03:02\u00a0am between floors 77 and 85, instantly killing himself and everyone else aboard the flight in addition to many more inside the South Tower. More than 600 people were above the impact zone when the plane struck, half of whom were killed instantaneously. Thus, the estimated 300 people still alive following the impact were left stranded in the upper floors of the catastrophically damaged skyscraper, now set ablaze and rapidly filling with smoke. Because all eyes were on the Twin Towers following the crash of Flight 11 seventeen minutes earlier, the impact of Flight 175 and the explosion that followed was seen by millions of people worldwide on live television, being filmed and photographed from numerous vantage points. Although the angle at which al-Shehhi crashed did not sever all means of escape from the impact zone, most of the people who survived the crash were unable to use the single intact stairwell before the tower fell less than an hour later.\nAl-Shehhi flew the plane faster and lower into the tower than Atta did, into the eastern half of the South Tower's southern facade close to the southeast corner, causing the tower's structural integrity to be compromised far more severely than the North Tower's. At 9:59 a.m., the South Tower became the first tower to collapse, having stood for 56 minutes after the plane crash.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44107", "revid": "6569922", "url": "https://en.wikipedia.org/wiki?curid=44107", "title": "September 11, 2001 Terrorist Attack hijacker", "text": ""}
{"id": "44108", "revid": "39450321", "url": "https://en.wikipedia.org/wiki?curid=44108", "title": "Minuet", "text": "Social dance and musical form\nA minuet (; also spelled menuet) is a social dance of French origin for two people, usually written in time. The English word was adapted from the Italian \"minuetto\" and the French \"menuet\". \nThe term also describes the musical form that accompanies the dance, which subsequently developed more fully, often with a longer musical form called the minuet and trio, and was much used as a movement in the early classical symphony. While often stylized in instrumental forms, composers of the period would have been familiar with the popular dance. \nDance.\nThe name may refer to the short steps, \"pas menus\", taken in the dance, or else be derived from the \"branle \u00e0 mener\" or \"amener\", popular group dances in early 17th-century France. The minuet was traditionally said to have descended from the \"bransle de Poitou\", though there is no evidence making a clear connection between these two dances. The earliest treatise to mention the possible connection of the name to the expression \"pas menus\" is Gottfried Taubert's \"Rechtschaffener Tantzmeister\", published in Leipzig in 1717, but this source does not describe the steps as being particularly small or dainty. At the period when it was most fashionable it was controlled, ceremonious and graceful.\nMusic.\nRhythm and form.\nThe name of this dance is also given to a musical composition written in the same time and rhythm, though when not accompanying an actual dance the pace was quicker. Stylistically refined minuets, apart from the social dance context, were introduced\u2014to opera at first\u2014by Jean-Baptiste Lully, who included no fewer than 92 of them in his theatrical works and in the late 17th century the minuet was adopted into the suite, such as some of the suites of Johann Sebastian Bach and George Frideric Handel. Among Italian and some French composers the minuet was often considerably quicker and livelier and was sometimes written in or time Because the tempo of a minuet was not standard, the tempo direction \"tempo di minuetto\" was ambiguous unless qualified by another direction, as it sometimes was.\nInitially, before its adoption in contexts other than social dance, the minuet was usually in binary form, with two repeated sections of usually eight bars each. But the second section eventually expanded, resulting in a kind of ternary form. The second (or middle) minuet provided a form of contrast by means of different key (although in many works, the second minuet stayed in the same key as the first minuet), orchestration, and thematic material. On a larger scale, two such minuets might be further combined, so that the first minuet was followed by a second one and then by a repetition of the first. The whole form might in any case be repeated as long as the dance lasted.\nMinuet and trio.\nAround the time of Jean-Baptiste Lully, it became a common practice to score this middle section for a trio (such as two oboes and a bassoon, as is common in Lully). As a result, this middle section came to be called the minuet's \"trio\", even when no trace of such an orchestration remains. The overall structure is called rounded binary or minuet form:\nAfter these developments by Lully, composers occasionally inserted a modified repetition of the first (A) section or a section that contrasted with both the A section and what was thereby rendered the third or C section, yielding the form A\u2013A\u2032\u2013B\u2013A or A\u2013B\u2013C\u2013A, respectively; an example of the latter is the third movement of Mozart's Serenade No. 13 in G major, K. 525, popularly known under the title \"Eine kleine Nachtmusik\".\nA livelier form of the minuet simultaneously developed into the scherzo (which was generally also coupled with a trio). This term came into existence approximately from Beethoven onwards, but the form itself can be traced back to Haydn.\nThe minuet and trio eventually became the standard third movement in the four-movement classical symphony, Johann Stamitz being the first to employ it thus with regularity.\nAn example of the true form of the minuet is to be found in \"Don Giovanni\".\nA famous example of a more recent instrumental work in minuet form is Ignacy Jan Paderewski's Minuet in G.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "44111", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=44111", "title": "Foley artist", "text": ""}
{"id": "44112", "revid": "39494085", "url": "https://en.wikipedia.org/wiki?curid=44112", "title": "John Dalton", "text": "British chemist and physicist (1766\u20131844)\nJohn Dalton (; 5 or 6 September 1766 \u2013 27 July 1844) was an English chemist, physicist, and meteorologist whose work laid the foundations of modern atomic theory and stoichiometric chemistry. Building on earlier ideas about the indivisibility of matter and his own precise measurements of combining ratios, Dalton proposed that each chemical element consists of identical atoms of characteristic weight, and that compounds are formed when atoms of different elements combine in fixed whole-number proportions. His \"A New System of Chemical Philosophy\" (1808) presented a coherent atomic model, supplied relative atomic weights and symbolic notation, and established the quantitative framework that shaped nineteenth-century chemistry and remains the basis of modern chemical thought.\nDalton was also a pioneering meteorologist and physicist, keeping daily weather observations for over fifty years, formulating the first empirical law of partial pressures (later known as Dalton\u2019s Law), and studying the behavior of gases through his work on vapor pressure and gas solubility. His investigations into his own color blindness led to the first scientific description of the condition\u2014still called Daltonism in several languages\u2014and helped establish experimental methods for linking perception with physiology. Elected a Fellow of the Royal Society in 1822 and awarded its Royal Medal in 1826, Dalton became the first British scientist to develop a quantitative atomic theory and one of the key figures in the transition of chemistry from a qualitative to a mathematical science.\nIn honour of Dalton's work, a unit of atomic mass, the dalton, symbol Da, is officially accepted for use with the SI.\nEarly life.\nJohn Dalton was born on 5 or 6 September 1766 into a Quaker family in Eaglesfield, near Cockermouth, in Cumberland, England. His father was a weaver. He received his early education from his father and from Quaker John Fletcher, who ran a private school in the nearby village of Pardshaw Hall. Dalton's family was too poor to support him for long and he began to earn his living, from the age of ten, in the service of wealthy local Quaker Elihu Robinson.\nScientific work.\nMeteorology.\nDalton's early life was influenced by a prominent Quaker, Elihu Robinson, a competent meteorologist and instrument maker, from Eaglesfield, Cumberland, who interested him in problems of mathematics and meteorology. During his years in Kendal, Dalton contributed solutions to problems and answered questions on various subjects in \"The Ladies' Diary\" and the \"Gentleman's Diary\". In 1787 at age 21 he began his meteorological diary in which, during the succeeding 57 years, he entered more than 200,000 observations. He rediscovered George Hadley's theory of atmospheric circulation (now known as the Hadley cell) around this time. In 1793 Dalton's first publication, \"Meteorological Observations and Essays\", contained the seeds of several of his later discoveries but despite the originality of his treatment, little attention was paid to them by other scholars. A second work by Dalton, \"Elements of English Grammar\" (or \"A new system of grammatical instruction: for the use of schools and academies\"), was published in 1801.\nMeasuring mountains.\nAfter leaving the Lake District, Dalton returned annually to spend his holidays studying meteorology, something which involved a lot of hill-walking. Until the advent of aeroplanes and weather balloons, the only way to make measurements of temperature and humidity at altitude was to climb a mountain. Dalton estimated the height using a barometer. The Ordnance Survey did not publish maps for the Lake District until the 1860s. Before then, Dalton was one of the few authorities on the heights of the region's mountains. He was often accompanied by Jonathan Otley, who also made a study of the heights of the local peaks, using Dalton's figures as a comparison to check his work. Otley published his information in his map of 1818. Otley became both an assistant and a friend to Dalton.\nColour blindness.\nIn 1794, shortly after his arrival in Manchester, Dalton was elected a member of the Manchester Literary and Philosophical Society, the \"Lit &amp; Phil\", and a few weeks later he communicated his first paper on \"Extraordinary facts relating to the vision of colours\", in which he postulated that shortage in colour perception was caused by discoloration of the liquid medium of the eyeball. As both he and his brother were colour blind, he recognised that the condition must be hereditary.\nAlthough Dalton's theory was later disproven, his early research into colour vision deficiency was recognized after his lifetime. Examination of his preserved eyeball in 1995 demonstrated that Dalton had deuteranopia, a type of congenital red-green color blindness in which the gene for medium wavelength sensitive (green) photopsins is missing. Individuals with this form of colour blindness see every colour as mapped to blue, yellow or gray, or, as Dalton wrote in his seminal paper,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;That part of the image which others call red, appears to me little more than a shade, or defect of light; after that the orange, yellow and green seem one colour, which descends pretty uniformly from an intense to a rare yellow, making what I should call different shades of yellow.\nGas laws.\nIn 1800, Dalton became secretary of the Manchester Literary and Philosophical Society, and in the following year he presented an important series of lectures, entitled \"Experimental Essays\" on the constitution of mixed gases; the pressure of steam and other vapours at different temperatures in a vacuum and in air; on evaporation; and on the thermal expansion of gases. The four essays, presented between 2 and 30 October 1801, were published in the \"Memoirs of the Literary and Philosophical Society of Manchester\" in 1802.\nThe second essay opens with the remark,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There can scarcely be a doubt entertained respecting the reducibility of all elastic fluids of whatever kind, into liquids; and we ought not to despair of effecting it in low temperatures and by strong pressures exerted upon the unmixed gases further.\nAfter describing experiments to ascertain the pressure of steam at various points between , Dalton concluded from observations of the vapour pressure of six different liquids, that the variation of vapour pressure for all liquids is equivalent, for the same variation of temperature, reckoning from vapour of any given pressure.\nIn the fourth essay he remarks,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I see no sufficient reason why we may not conclude, that all elastic fluids under the same pressure expand equally by heat\u2014and that for any given expansion of mercury, the corresponding expansion of air is proportionally something less, the higher the temperature. ... It seems, therefore, that general laws respecting the absolute quantity and the nature of heat, are more likely to be derived from elastic fluids than from other substances.\nHe enunciated Gay-Lussac's law, published in 1802 by Joseph Louis Gay-Lussac (Gay-Lussac credited the discovery to unpublished work from the 1780s by Jacques Charles). In the two or three years following the lectures, Dalton published several papers on similar topics. \"On the Absorption of Gases by Water and other Liquids\" (read as a lecture on 21 October 1803, first published in 1805) contained his law of partial pressures now known as Dalton's law.\nAtomic theory.\nArguably the most important of all Dalton's investigations are concerned with the atomic theory in chemistry. While his name is inseparably associated with this theory, the origin of Dalton's atomic theory is not fully understood. The theory may have been suggested to him either by researches on ethylene (\"olefiant gas\") and methane (\"carburetted hydrogen\") or by analysis of nitrous oxide (\"protoxide of azote\") and nitrogen dioxide (\"deutoxide of azote\"), both views resting on the authority of Thomas Thomson.\nFrom 1814 to 1819, Irish chemist William Higgins claimed that Dalton had plagiarised his ideas, but Higgins' theory did not address relative atomic mass. Recent evidence suggests that Dalton's development of thought may have been influenced by the ideas of another Irish chemist Bryan Higgins, who was William's uncle. Bryan believed that an atom was a heavy central particle surrounded by an atmosphere of caloric, the supposed substance of heat at the time. The size of the atom was determined by the diameter of the caloric atmosphere. Based on the evidence, Dalton was aware of Bryan's theory and adopted very similar ideas and language, but he never acknowledged Bryan's anticipation of his caloric model. However, the essential novelty of Dalton's atomic theory is that he provided a method of calculating relative atomic weights for the chemical elements, which provides the means for the assignment of molecular formulas for all chemical substances. Neither Bryan nor William Higgins did this, and Dalton's priority for that crucial innovation is uncontested.\nA study of Dalton's laboratory notebooks, discovered in the rooms of the Manchester Literary and Philosophical Society, concluded that Dalton was not led by his search for an explanation of the law of multiple proportions to the idea that chemical combination consists in the interaction of atoms of definite and characteristic weight, but rather the idea of atoms arose in his mind as a purely physical concept, forced on him by study of the physical properties of the atmosphere and other gases. The first published indications of this idea are to be found at the end of his paper \"On the Absorption of Gases by Water and other Liquids\" already mentioned. There he says:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Why does not water admit its bulk of every kind of gas alike? This question I have duly considered, and though I am not able to satisfy myself completely I am nearly persuaded that the circumstance depends on the weight and number of the ultimate particles of the several gases.\nHe then proposes relative weights for the atoms of a few elements, without going into further detail. However, a recent study of Dalton's laboratory notebook entries concludes he developed the chemical atomic theory in 1803 to reconcile Henry Cavendish's and Antoine Lavoisier's analytical data on the composition of nitric acid, not to explain the solubility of gases in water.\nThe main points of Dalton's atomic theory, as it eventually developed, are:\nIn his first extended published discussion of the atomic theory (1808), Dalton proposed an additional (and controversial) \"rule of greatest simplicity\". This rule could not be independently confirmed, but some such assumption was necessary in order to propose formulas for a few simple molecules, upon which the calculation of atomic weights depended. This rule dictated that if the atoms of two different elements were known to form only a single compound, like hydrogen and oxygen forming water or hydrogen and nitrogen forming ammonia, the molecules of that compound shall be assumed to consist of one atom of each element. For elements that combined in multiple ratios, such as the then-known two oxides of carbon or the three oxides of nitrogen, their combinations were assumed to be the simplest ones possible. For example, if two such combinations are known, one must consist of an atom of each element, and the other must consist of one atom of one element and two atoms of the other.\nThis was merely an assumption, derived from faith in the simplicity of nature. No evidence was then available to scientists to deduce how many atoms of each element combine to form molecules. But this or some other such rule was absolutely necessary to any incipient theory, since one needed an assumed molecular formula in order to calculate relative atomic weights. Dalton's \"rule of greatest simplicity\" caused him to assume that the formula for water was OH and ammonia was NH, quite different from our modern understanding (H2O, NH3). On the other hand, his simplicity rule led him to propose the correct modern formulas for the two oxides of carbon (CO and CO2). Despite the uncertainty at the heart of Dalton's atomic theory, the principles of the theory survived.\nRelative atomic weights.\nDalton published his first table of relative atomic weights containing six elements (hydrogen, oxygen, nitrogen, carbon, sulfur and phosphorus), relative to the weight of an atom of hydrogen conventionally taken as 1. Since these were only relative weights, they do not have a unit of weight attached to them. Dalton provided no indication in this paper how he had arrived at these numbers, but in his laboratory notebook, dated 6 September 1803, is a list in which he set out the relative weights of the atoms of a number of elements, derived from analysis of water, ammonia, carbon dioxide, etc. by chemists of the time.\nThe extension of this idea to substances in general necessarily led him to the law of multiple proportions, and the comparison with experiment brilliantly confirmed his deduction. In the paper \"On the Proportion of the Several Gases in the Atmosphere\", read by him in November 1802, the law of multiple proportions appears to be anticipated in the words:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The elements of oxygen may combine with a certain portion of nitrous gas or with twice that portion, but with no intermediate quantity.\nBut there is reason to suspect that this sentence may have been added some time after the reading of the paper, which was not published until 1805.\nCompounds were listed as binary, ternary, quaternary, etc. (molecules composed of two, three, four, etc. atoms) in the \"New System of Chemical Philosophy\" depending on the number of atoms a compound had in its simplest, empirical form.\nDalton hypothesised the structure of compounds can be represented in whole number ratios. So, one atom of element X combining with one atom of element Y is a binary compound. Furthermore, one atom of element X combining with two atoms of element Y or vice versa, is a ternary compound. Many of the first compounds listed in the \"New System of Chemical Philosophy\" correspond to modern views, although many others do not.\nDalton used his own symbols to visually represent the atomic structure of compounds. They were depicted in the \"New System of Chemical Philosophy\", where he listed 21 elements and 17 simple molecules.\nOther investigations.\nDalton published papers on such diverse topics as rain and dew and the origin of springs (hydrosphere); on heat, the colour of the sky, steam and the reflection and refraction of light; and on the grammatical subjects of the auxiliary verbs and participles of the English language.\nExperimental approach.\nAs an investigator, Dalton was often content with rough and inaccurate instruments, even though better ones were obtainable. Sir Humphry Davy described him as \"a very coarse experimenter\", who \"almost always found the results he required, trusting to his head rather than his hands.\" On the other hand, historians who have replicated some of his crucial experiments have confirmed Dalton's skill and precision.\nIn the preface to the second part of Volume I of his \"New System\", he says he had so often been misled by taking for granted the results of others that he determined to write \"as little as possible but what I can attest by my own experience\", but this independence he carried so far that it sometimes resembled lack of receptivity. Thus he distrusted, and probably never fully accepted, Gay-Lussac's conclusions as to the combining volumes of gases.\nHe held unconventional views on chlorine. Even after its elementary character had been settled by Davy, he persisted in using the atomic weights he himself had adopted, even when they had been superseded by the more accurate determinations of other chemists.\nHe always objected to the chemical notation devised by J\u00f6ns Jacob Berzelius, although most thought that it was much simpler and more convenient than his own cumbersome system of circular symbols.\nOther publications.\nFor \"Rees's Cyclop\u00e6dia\" Dalton contributed articles on Chemistry and Meteorology, but the topics are not known.\nHe contributed 117 \"Memoirs of the Literary and Philosophical Society of Manchester\" from 1817 until his death in 1844 while president of that organisation. Of these the earlier are the most important. In one of them, read in 1814, he explains the principles of volumetric analysis, in which he was one of the earliest researchers. In 1840 a paper on phosphates and arsenates, often regarded as a weaker work, was refused by the Royal Society, and he was so incensed that he published it himself. He took the same course soon afterwards with four other papers, two of which (\"On the quantity of acids, bases and salts in different varieties of salts\" and \"On a new and easy method of analysing sugar\") contain his discovery, regarded by him as second in importance only to atomic theory, that certain anhydrates, when dissolved in water, cause no increase in its volume, his inference being that the salt enters into the pores of the water.\nPublic life.\nEven before he had propounded the atomic theory, Dalton had attained a considerable scientific reputation. In 1803, he was chosen to give a series of lectures on natural philosophy at the Royal Institution in London, and he delivered another series of lectures there in 1809\u20131810. Some witnesses reported that he was deficient in the qualities that make an attractive lecturer, being harsh and indistinct in voice, ineffective in the treatment of his subject, and singularly wanting in the language and power of illustration.\nIn 1810, Sir Humphry Davy asked him to offer himself as a candidate for the fellowship of the Royal Society, but Dalton declined, possibly for financial reasons. In 1822 he was proposed without his knowledge, and on election paid the usual fee. Six years previously he had been made a corresponding member of the French Acad\u00e9mie des Sciences, and in 1830 he was elected as one of its eight foreign associates in place of Davy. In 1833, Earl Grey's government conferred on him a pension of \u00a3150, raised in 1836 to \u00a3300 (equivalent to \u00a3 and \u00a3 in 2023, respectively). Dalton was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1834.\nA young James Prescott Joule, who later studied and published (1843) on the nature of heat and its relationship to mechanical work, was a pupil of Dalton in his last years.\nPersonal life.\nDalton never married and had only a few close friends. As a Quaker, he lived a modest and unassuming personal life.\nFor the 26 years prior to his death, Dalton lived in a room in the home of the Rev W. Johns, a published botanist, and his wife, in George Street, Manchester. Dalton and Johns died in the same year (1844).\nDalton's daily round of laboratory work and tutoring in Manchester was broken only by annual excursions to the Lake District and occasional visits to London. In 1822 he paid a short visit to Paris, where he met many distinguished resident men of science. He attended several of the earlier meetings of the British Association at York, Oxford, Dublin and Bristol.\nDisability and death.\nDalton suffered a minor stroke in 1837, and a second in 1838 left him with a speech impairment, although he remained able to perform experiments. In May 1844 he had another stroke; on 26 July, while his hand was trembling, he recorded his last meteorological observation. On 27 July, in Manchester, Dalton fell from his bed and was found dead by his attendant.\nDalton was accorded a civic funeral with full honours. His body lay in state in Manchester Town Hall for four days and more than 40,000 people filed past his coffin. The funeral procession included representatives of the city's major civic, commercial, and scientific bodies. He was buried in Manchester in Ardwick Cemetery; the cemetery is now a playing field, but pictures of the original grave may be found in published materials.\nLegacy.\n&lt;templatestyles src=\"Botanist/styles.css\"/&gt;The standard author abbreviation Jn.Dalton is used to indicate this person as the author when citing a botanical name.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44114", "revid": "49687814", "url": "https://en.wikipedia.org/wiki?curid=44114", "title": "Symphony", "text": "Type of extended musical composition\nA symphony is an extended musical composition in Western classical music, most often for orchestra. Although the term has had many meanings from its origins in the ancient Greek era, by the late 18th century the word had taken on the meaning common today: a work usually consisting of multiple distinct sections or movements, often four, with the first movement in sonata form. Symphonies are almost always scored for an orchestra consisting of a string section (violin, viola, cello, and double bass), brass, woodwind, and percussion instruments which altogether number about 30 to 100 musicians. Symphonies are notated in a musical score, which contains all the instrument parts. Orchestral musicians play from parts which contain just the notated music for their own instrument. Some symphonies also contain vocal parts (e.g., Beethoven's Ninth Symphony, or Mahler's Second Symphony).\nEtymology and origins.\nThe word \"symphony\" is derived from the Greek word (), meaning \"agreement or concord of sound\", \"concert of vocal or instrumental music\", from (), \"harmonious\". The word referred to a variety of different concepts before ultimately settling on its current meaning designating a musical form.\nIn late Greek and medieval theory, the word was used for consonance, as opposed to (), which was the word for \"dissonance\". In the Middle Ages and later, the Latin form \"symphonia\" was used to describe various instruments, especially those capable of producing more than one sound simultaneously. Isidore of Seville was the first to use the word symphonia as the name of a two-headed drum, and from c.\u20091155 to 1377 the French form \"symphonie\" was the name of the \"organistrum\" or hurdy-gurdy. In late medieval England, \"symphony\" was used in both of these senses, whereas by the 16th century it was equated with the dulcimer. In German, \"Symphonie\" was a generic term for spinets and virginals from the late 16th century to the 18th century.\nIn the sense of \"sounding together\", the word begins to appear in the titles of some works by 16th- and 17th-century composers including Giovanni Gabrieli's \"Sacrae symphoniae\", and \"Symphoniae sacrae, liber secundus\", published in 1597 and 1615, respectively; Adriano Banchieri's \"Eclesiastiche sinfonie, dette canzoni in aria francese, per sonare, et cantare\", Op. 16, published in 1607; Lodovico Grossi da Viadana's \"Sinfonie musicali\", Op. 18, published in 1610; and Heinrich Sch\u00fctz's \"Symphoniae sacrae\", Op. 6, and \"Symphoniarum sacrarum secunda pars\", Op. 10, published in 1629 and 1647, respectively. Except for Viadana's collection, which contained purely instrumental and secular music, these were all collections of sacred vocal works, some with instrumental accompaniment.\nBaroque era.\nIn the 17th century, for most of the Baroque era, the terms \"symphony\" and \"sinfonia\" were used for a range of different compositions, including instrumental pieces used in operas, sonatas and concertos\u2014usually part of a larger work. The \"opera sinfonia\", or \"Italian overture\" had, by the 18th century, a standard structure of three contrasting movements: fast, slow, fast and dance-like. It is this form that is often considered as the direct forerunner of the orchestral symphony. The terms \"overture\", \"symphony\" and \"sinfonia\" were widely regarded as interchangeable for much of the 18th century.\nIn the 17th century, pieces scored for large instrumental ensemble did not precisely designate which instruments were to play which parts, as is the practice from the 19th century to the current period. When composers from the 17th century wrote pieces, they expected that these works would be performed by whatever group of musicians were available. To give one example, whereas the bassline in a 19th-century work is scored for cellos, double basses and other specific instruments, in a 17th-century work, a basso continuo part for a sinfonia would not specify which instruments would play the part. A performance of the piece might be done with a basso continuo group as small as a single cello and harpsichord. However, if a bigger budget was available for a performance and a larger sound was required, a basso continuo group might include multiple chord-playing instruments (harpsichord, lute, etc.) and a range of bass instruments, including cello, double bass, bass viol or even a serpent, an early bass wind instrument.\nGalant and classical eras.\nLaRue, Bonds, Walsh, and Wilson write in the second edition of \"The New Grove Dictionary of Music and Musicians\" that \"the symphony was cultivated with extraordinary intensity\" in the 18th century. It played a role in many areas of public life, including church services, but a particularly strong area of support for symphonic performances was the aristocracy. In Vienna, perhaps the most important location in Europe for the composition of symphonies, \"literally hundreds of noble families supported musical establishments, generally dividing their time between Vienna and their ancestral estate [elsewhere in the Empire]\". Since the normal size of the orchestra at the time was quite small, many of these courtly establishments were capable of performing symphonies. The young Joseph Haydn, taking up his first job as a music director in 1757 for the Morzin family, found that when the Morzin household was in Vienna, his own orchestra was only part of a lively and competitive musical scene, with multiple aristocrats sponsoring concerts with their own ensembles.\nLaRue, Bonds, Walsh, and Wilson's article traces the gradual expansion of the symphonic orchestra through the 18th century. At first, symphonies were string symphonies, written in just four parts: first violin, second violin, viola, and bass (the bass line was taken by cello(s), double bass(es) playing the part an octave below, and perhaps also a bassoon). Occasionally the early symphonists even dispensed with the viola part, thus creating three-part symphonies. A basso continuo part including a bassoon together with a harpsichord or other chording instrument was also possible.\nThe first additions to this simple ensemble were a pair of horns, occasionally a pair of oboes, and then both horns and oboes together. Over the century, other instruments were added to the classical orchestra: flutes (sometimes replacing the oboes), separate parts for bassoons, clarinets, and trumpets and timpani. Works varied in their scoring concerning which of these additional instruments were to appear. The full-scale classical orchestra, deployed at the end of the century for the largest-scale symphonies, has the standard string ensemble mentioned above, pairs of winds (flutes, oboes, clarinets, bassoons), a pair of horns, and timpani. A keyboard continuo instrument (harpsichord or piano) remained an option.\nThe \"Italian\" style of symphony, often used as overture and entr'acte in opera houses, became a standard three-movement form: a fast movement, a slow movement, and another fast movement. Over the course of the 18th century it became the custom to write four-movement symphonies, along the lines described in the next paragraph. The three-movement symphony died out slowly; about half of Haydn's first thirty symphonies are in three movements; and for the young Mozart, the three-movement symphony was the norm, perhaps under the influence of his friend Johann Christian Bach. An outstanding late example of the three-movement Classical symphony is Mozart's \"Prague Symphony\", from 1786.\nThe four-movement form that emerged from this evolution was as follows:\nVariations on this layout, like changing the order of the middle movements or adding a slow introduction to the first movement, were common. Haydn, Mozart and their contemporaries restricted their use of the four-movement form to orchestral or multi-instrument chamber music such as quartets, though since Beethoven solo sonatas are as often written in four as in three movements.\nThe composition of early symphonies was centred on Milan, Vienna, and Mannheim. The Milanese school centred around Giovanni Battista Sammartini and included Antonio Brioschi, Ferdinando Galimberti and Giovanni Battista Lampugnani. Early exponents of the form in Vienna included Georg Christoph Wagenseil, Wenzel Raimund Birck and Georg Matthias Monn, while later significant Viennese composers of symphonies included Johann Baptist Wanhal, Carl Ditters von Dittersdorf and Leopold Hofmann. The Mannheim school included Johann Stamitz.\nThe most important symphonists of the latter part of the 18th century are Haydn, who wrote at least 106 symphonies over the course of 36 years, and Mozart, with at least 47 symphonies in 24 years.\nRomantic era.\nAt the beginning of the 19th century, Beethoven elevated the symphony from an everyday genre produced in large quantities to a supreme form in which composers strove to reach the highest potential of music in just a few works. Beethoven began with two works directly emulating his models Mozart and Haydn, then seven more symphonies, starting with the Third Symphony (\"Eroica\") that expanded the scope and ambition of the genre. His Symphony No. 5 is perhaps the most famous symphony ever written; its transition from the emotionally stormy C minor opening movement to a triumphant major-key finale provided a model adopted by later symphonists such as Brahms and Mahler. His Symphony No. 6 is a programmatic work, featuring instrumental imitations of bird calls and a storm; and, unconventionally, a fifth movement (symphonies at that time usually had at most four movements). His Symphony No. 9 includes parts for vocal soloists and choir in the last movement, making it a choral symphony.\nOf the symphonies by Schubert, two are core repertory items and are frequently performed. Of the Eighth Symphony (1822), Schubert completed only the first two movements; this highly Romantic work is usually called by its nickname \"The Unfinished\". His last completed symphony, the Ninth (1826) is a massive work in the Classical idiom.\nOf the early Romantics, Felix Mendelssohn (five symphonies, plus thirteen string symphonies) and Robert Schumann (four) continued to write symphonies in the classical mould, though using their own musical language. In contrast, Berlioz favored programmatic works, including his \"dramatic symphony\" \"Rom\u00e9o et Juliette\", the viola symphony \"Harold en Italie\" and the highly original \"Symphonie fantastique\". The latter is also a programme work and has both a march and a waltz and five movements instead of the customary four. His fourth and last symphony, the \"Grande symphonie fun\u00e8bre et triomphale\" (originally titled \"Symphonie militaire\") was composed in 1840 for a 200-piece marching military band, to be performed out of doors, and is an early example of a band symphony. Berlioz later added optional string parts and a choral finale. In 1851, Richard Wagner declared that all of these post-Beethoven symphonies were no more than an epilogue, offering nothing substantially new. Indeed, after Schumann's last symphony, the \"Rhenish\" composed in 1850, for two decades the Lisztian symphonic poem appeared to have displaced the symphony as the leading form of large-scale instrumental music. However, Liszt also composed two programmatic choral symphonies during this time, \"Faust\" and \"Dante\". If the symphony had otherwise been eclipsed, it was not long before it re-emerged in a \"second age\" in the 1870s and 1880s, with the symphonies by Bruckner, Brahms, Tchaikovsky, Saint-Sa\u00ebns, Borodin, Dvo\u0159\u00e1k, and Franck\u2014works which largely avoided the programmatic elements of Berlioz and Liszt and dominated the concert repertory for at least a century.\nOver the course of the 19th century, composers continued to add to the size of the symphonic orchestra. Around the beginning of the century, a full-scale orchestra would consist of the string section plus pairs of flutes, oboes, clarinets, bassoons, horns, trumpets, and lastly a set of timpani. This is, for instance, the scoring used in Beethoven's symphonies numbered 1, 2, 4, 7, and 8. Trombones, which had previously been confined to church and theater music, came to be added to the symphonic orchestra, notably in Beethoven's 5th, 6th, and 9th symphonies. The combination of bass drum, triangle, and cymbals (sometimes also: piccolo), which 18th-century composers employed as a coloristic effect in so-called \"Turkish music\", came to be increasingly used during the second half of the 19th century without any such connotations of genre. By the time of Mahler (see below), it was possible for a composer to write a symphony scored for \"a veritable compendium of orchestral instruments\". In addition to increasing in variety of instruments, 19th-century symphonies were gradually augmented with more string players and more wind parts, so that the orchestra grew substantially in sheer numbers, as concert halls likewise grew.\nLate-Romantic, modernist and postmodernist eras.\nTowards the end of the 19th century, Gustav Mahler began writing long, large-scale symphonies that he continued composing into the early 20th century. His Third Symphony, completed in 1896, is one of the longest regularly performed symphonies at around 100 minutes in length for most performances. The Eighth Symphony was composed in 1906 and is nicknamed the \"Symphony of a Thousand\" because of the large number of voices required to perform the work.\nThe 20th century saw further diversification in the style and content of works that composers labeled \"symphonies\". Some composers, including Dmitri Shostakovich, Sergei Rachmaninoff, and Carl Nielsen, continued to write in the traditional four-movement form, while other composers took different approaches: Jean Sibelius' Symphony No. 7, his last, is in one movement, Richard Strauss' Alpine Symphony, in one movement, split into twenty-two parts, detailing an eleven hour hike through the mountains and Alan Hovhaness's Symphony No. 9, \"Saint Vartan\"\u2014originally Op. 80, changed to Op. 180\u2014composed in 1949\u201350, is in twenty-four.\nA concern with unification of the traditional four-movement symphony into a single, subsuming formal conception had emerged in the late 19th century. This has been called a \"two-dimensional symphonic form\", and finds its key turning point in Arnold Schoenberg's Chamber Symphony No. 1, Op. 9 (1909), which was followed in the 1920s by other notable single-movement German symphonies, including Kurt Weill's First Symphony (1921), Max Butting's Chamber Symphony, Op. 25 (1923), and Paul Dessau's 1926 Symphony.\nAlongside this experimentation, other 20th-century symphonies deliberately attempted to evoke the 18th-century origins of the genre, in terms of form and even musical style, with prominent examples being Sergei Prokofiev's Symphony No. 1 \"Classical\" of 1916\u201317 and the Symphony in C by Igor Stravinsky of 1938\u201340.\nThere remained, however, certain tendencies. Designating a work a \"symphony\" still implied a degree of sophistication and seriousness of purpose. The word \"sinfonietta\" came into use to designate a work that is shorter, of more modest aims, or \"lighter\" than a symphony, such as Sergei Prokofiev's Sinfonietta for orchestra.\nIn addition to those composers listed above, other symphonists from the first half of the twentieth century include Edward Elgar, Bohuslav Martin\u016f, Roger Sessions, William Walton, and Rued Langgaard. The symphonies of this period were \"extraordinary in scope, richness, originality, and urgency of expression\". One measure of a symphony's significance is how much it reflects particular to its age. Twentieth-century composers who fulfil this measure include Jean Sibelius, Igor Stravinsky, Luciano Berio (in his Sinfonia, 1968\u201369), Elliott Carter (in his \"Symphony of Three Orchestras\", 1976), and Pelle Gudmundsen-Holmgreen (in \"Symphony/Antiphony\", 1980).\nFrom the mid-20th century into the 21st there has been a resurgence of interest in the symphony with many postmodernist composers adding substantially to the canon, not least in the United Kingdom: Peter Maxwell Davies (10), Robin Holloway (1), David Matthews (9), James MacMillan (5), Peter Seabourne (6), and Philip Sawyers (6). British composer Derek Bourgeois has surpassed the number of symphonies written by Haydn, with 116 symphonies. The greatest number of symphonies to date has been composed by the Finn Leif Segerstam, whose list of works includes 371 symphonies.\nSymphonies for concert band.\nHector Berlioz originally wrote the \"Grande symphonie fun\u00e8bre et triomphale\" for military band in 1840. Anton Reicha had composed his four-movement 'Commemoration' Symphony (also known as \"Musique pour c\u00e9l\u00e9brer le M\u00e9morie des Grands Hommes qui se sont Illustr\u00e9s au Service de la Nation Fran\u00e7aise\") for large wind ensemble even earlier, in 1815, for ceremonies associated with the reburial of Louis XVI and Marie Antoinette\nAfter those early efforts, few symphonies were written for wind bands until the 20th century when more symphonies were written for concert band than in past centuries. Although examples exist from as early as 1932, the first such symphony of importance is Nikolai Myaskovsky's Symphony No. 19, Op. 46, composed in 1939. Some further examples are Paul Hindemith's Symphony in B-flat for Band, composed in 1951; Morton Gould's Symphony No. 4 \"West Point\", composed in 1952; Vincent Persichetti's Symphony No. 6, Op. 69, composed in 1956; Vittorio Giannini's Symphony No. 3, composed in 1958; Alan Hovhaness's Symphonies No. 4, Op.\u00a0165, No.\u00a07, \"Nanga Parvat\", Op.\u00a0175, No.\u00a014, \"Ararat\", Op.\u00a0194, and No.\u00a023, \"Ani\", Op.\u00a0249, composed in 1958, 1959, 1961, and 1972 respectively; John Barnes Chance's Symphony No. 2, composed in 1972; Alfred Reed's 2nd, 3rd, 4th, and 5th symphonies, composed in 1979, 1988, 1992, and 1994 respectively; eight of the ten numbered symphonies of David Maslanka; six symphonies to date by Julie Giroux (although she is currently working on a seventh); Johan de Meij's Symphony No. 1 \"The Lord of the Rings\", composed in 1988, and his Symphony No. 2 \"The Big Apple\", composed in 1993; Yasuhide Ito's Symphony in Three Scenes 'La Vita', composed in 1998, which is his third symphony for wind band; John Corigliano's Symphony No. 3 'Circus Maximus, composed in 2004; Denis Levaillant's PachaMama Symphony, composed in 2014 and 2015, and James M. Stephenson's Symphony No. 2 which was premiered by the United States Marine Band (\"The President's Own\") and received both the National Band Association's William D. Revelli (2017) and the American Bandmasters Association's Sousa/Ostwald (2018) awards.\nOther modern usages of \"symphony\".\nIn some forms of English, the word \"symphony\" is also used to refer to the orchestra, the large ensemble that often performs these works. The word \"symphony\" appears in the name of many orchestras, for example, the London Symphony Orchestra, the Boston Symphony Orchestra, the St. Louis Symphony, the Houston Symphony, or Miami's New World Symphony. For some orchestras, \"(city name) Symphony\" provides a shorter version of the full name; for instance, the OED gives \"Vancouver Symphony\" as a possible abbreviated form of Vancouver Symphony Orchestra. Additionally, in common usage, a person may say they are going out to hear a symphony perform, a reference to the orchestra and not the works on the program. These usages are not common in British English.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "44116", "revid": "3138265", "url": "https://en.wikipedia.org/wiki?curid=44116", "title": "Concerto", "text": "Musical composition typically for a soloist with accompaniment\nA concerto (; plural \"concertos\", or \"concerti\" from the Italian plural) is, from the late Baroque era, mostly understood as an instrumental composition, written for one or more soloists accompanied by an orchestra or other ensemble. The typical three-movement structure, a slow movement (e.g., lento or adagio) preceded and followed by fast movements (e.g., presto or allegro), became a standard from the early 18th century.\nThe concerto originated as a genre of vocal music in the late 16th century: the instrumental variant appeared around a century later, when Italians such as Giuseppe Torelli and Arcangelo Corelli started to publish their concertos. A few decades later, Venetian composers, such as Antonio Vivaldi, had written hundreds of violin concertos, while also producing solo concertos for other instruments such as a cello or a woodwind instrument, and concerti grossi for a group of soloists. The first keyboard concertos, such as George Frideric Handel's organ concertos and Johann Sebastian Bach's harpsichord concertos, were written around the same time.\nIn the second half of the 18th century, the piano became the most used keyboard instrument, and composers of the Classical Era such as Joseph Haydn, Wolfgang Amadeus Mozart and Ludwig van Beethoven each wrote several piano concertos, and, to a lesser extent, violin concertos, and concertos for other instruments. In the Romantic Era, many composers, including Niccol\u00f2 Paganini, Felix Mendelssohn, Fr\u00e9d\u00e9ric Chopin, Robert Schumann, Johannes Brahms, Pyotr Ilyich Tchaikovsky and Sergei Rachmaninoff, continued to write solo concertos, and, more exceptionally, concertos for more than one instrument; 19th century concertos for instruments other than the piano, violin and cello remained comparatively rare, however. In the first half of the 20th century, concertos were written by, among others, Maurice Ravel, Edward Elgar, Richard Strauss, Sergei Prokofiev, George Gershwin, Heitor Villa-Lobos, Joaqu\u00edn Rodrigo and B\u00e9la Bart\u00f3k, the latter also composing a concerto for orchestra, that is without soloist. During the 20th century concertos appeared by major composers for orchestral instruments which had been neglected in the 19th century such as the clarinet, viola and French horn.\nIn the second half of the 20th century and onwards into the 21st a great many composers have continued to write concertos, including Alfred Schnittke, Gy\u00f6rgy Ligeti, Dmitri Shostakovich, Philip Glass and James MacMillan among many others. An interesting feature of this period is the proliferation of concerti for less usual instruments, including orchestral ones such as the double bass (by composers like Eduard Tubin or Peter Maxwell Davies) and cor anglais (like those by MacMillan and Aaron Jay Kernis), but also folk instruments (such as Tubin's concerto for Balalaika, Serry's \"Concerto in C Major for Bassetti Accordion\", or the concertos for Harmonica by Villa-Lobos and Malcolm Arnold), and even Deep Purple's \"Concerto for Group and Orchestra\", a concerto for a rock band.\nConcertos from previous ages have remained a conspicuous part of the repertoire for concert performances and recordings. Less common has been the previously common practice of the composition of concertos by a performer to be performed personally, though the practice has continued via certain composer-performers such as Daniil Trifonov.\nGenre.\nThe Italian word \"concerto\", meaning accord or gathering, derives from the Latin verb \"concertare\", which indicates a competition or battle.\nBaroque Era.\nCompositions were for the first time indicated as concertos in the title of a music print when the \"Concerti\" by Andrea and Giovanni Gabrieli were published in 1587.\nConcerto as a genre of vocal music.\nIn the 17th century, sacred works for voices and orchestra were typically called concertos, as reflected by J.\u00a0S. Bach's usage of the title \"concerto\" for many of the works that are now known as cantatas. The term \"concerto\" was initially used to denote works that involved voices and instruments in which the instruments had independent parts\u2014as opposed to the Renaissance common practice in which instruments that accompanied voices only doubled the voice parts. Examples of this earlier form of concerto include Giovanni Gabrieli's \"In Ecclesiis\" or Heinrich Sch\u00fctz's \"Saul, Saul, was verfolgst du mich\".\nInstrumental concerto.\nThe concerto began to take its modern shape in the late-Baroque period, beginning with the \"concerto grosso\" form developed by Arcangelo Corelli. Corelli's concertino group was two violins, a cello and basso continuo. In J. S. Bach's Fifth Brandenburg Concerto, for example, the concertino is a flute, a violin, and a harpsichord; although the harpsichord is a featured solo instrument, it also sometimes plays with the \"ripieno\", functioning as a continuo keyboard accompaniment.\nLater, the concerto approached its modern form, in which the concertino usually reduces to a single solo instrument playing with (or against) an orchestra. The main composers of concertos of the baroque were Tommaso Albinoni, Antonio Vivaldi (e.g., published in \"L'estro armonico\", \"La stravaganza\", Six Violin Concertos, Op. 6, Twelve Concertos, Op. 7, \"Il cimento dell'armonia e dell'inventione\", Six Flute Concertos, Op. 10, Six Concertos, Op. 11 and Six Violin Concertos, Op. 12), Georg Philipp Telemann, Johann Sebastian Bach, George Frideric Handel, Pietro Locatelli, Jean-Marie Leclair, Giuseppe Tartini, Francesco Geminiani and Johann Joachim Quantz.\nThe concerto was intended as a composition typical of the Italian style of the time, and all the composers were studying how to compose in the Italian fashion (\"all'Italiana\").\nThe Baroque concerto was mainly for a string instrument (violin, viola, cello, seldom viola d'amore or harp) or a wind instrument (flute, recorder, oboe, bassoon, horn, or trumpet,). Bach also wrote a concerto for two violins and orchestra. During the Baroque period, before the invention of the piano, keyboard concertos were comparatively rare, with the exception of the twelve organ concertos by George Frideric Handel and the thirteen harpsichord concertos by Johann Sebastian Bach.\nClassical era.\nThe concertos of the sons of Johann Sebastian Bach, such as C. P. E. Bach, are perhaps the best links between those of the Baroque period and those of the Classical era. It is conventional to state that the first movements of concertos from the Classical period onwards follow the structure of sonata form. Final movements are often in rondo form, as in J.S. Bach's E Major Violin Concerto.\nMozart wrote five violin concertos, all in 1775, except the first in 1773. They show a number of influences, notably Italian and Austrian. Several passages have leanings towards folk music, as manifested in Austrian serenades. Mozart also wrote the \"Sinfonia Concertante\" for violin, viola and orchestra. Haydn wrote three concertos for violin and above all two for cello. Beethoven wrote only one violin concerto that remained obscure until revealed as a masterpiece in a performance by violin virtuoso Joseph Joachim on 27 May 1844.\nC.P.E. Bach's keyboard concertos contain some virtuosic solo writing. Some of them have movements that run into one another without a break, and there are frequent cross-movement thematic references. Mozart, as a child, made arrangements for keyboard and orchestra of four sonatas by now little-known composers. Then he arranged three sonata movements by Johann Christian Bach. By the time he was twenty, Mozart was able to write concerto ritornelli that gave the orchestra admirable opportunity for asserting its character in an exposition with some five or six sharply contrasted themes, before the soloist enters to elaborate on the material. Of his 27 piano concertos, the last 17 are highly appreciated. Eleven cataloged keyboard concertos are attributed to Haydn, of which seven are considered genuine. Beethoven wrote five concertos for piano and orchestra.\nC. P. E. Bach wrote five flute concertos and two oboe concertos. Mozart wrote four horn concertos, two for flute, one for oboe (later rearranged for flute and known as Flute Concerto No. 2), one for clarinet, one for bassoon, one for flute and harp, and \"Exsultate, jubilate\", a \"de facto\" concerto for soprano voice. They all exploit and explore the characteristics of the solo instrument(s). Haydn wrote an important trumpet concerto and a \"Sinfonia Concertante\" for violin, cello, oboe, bassoon and orchestra, as well as one horn concerto. Haydn also wrote a concerto for double bass, but it was lost to history in the great fire of Eszterh\u00e1za in 1779.\nRomantic era.\nIn the 19th century, the concerto as a vehicle for virtuosic display flourished, and concertos became increasingly complex and ambitious works. Whilst performances of typical concertos in the baroque era lasted about ten minutes, those by Beethoven could last half an hour or longer. The term concertino, or the German \"Konzertstuck\" (\"Concert Piece\") began to be used to designate smaller pieces not considered large enough to be considered a full concerto, though the distinction has never been formalised and many Concertinos are still longer than the original Baroque concertos.\nDuring the Romantic era the cello became increasingly used as a concerto instrument; though the violin and piano remained the most frequently used. Beethoven contributed to the repertoire of concertos for more than one soloist with a \" Triple Concerto\" for piano, violin, cello and orchestra while later in the century, Brahms wrote a \" Double Concerto\" for violin, cello and orchestra.\n20th and 21st century.\nMany of the concertos written in the early 20th century belong more to the late Romantic school, hence modernistic movement. Masterpieces were written by Edward Elgar (a violin concerto and a cello concerto), Sergei Rachmaninoff and Nikolai Medtner (four and three piano concertos, respectively), Jean Sibelius (a violin concerto), Frederick Delius (a violin concerto, a cello concerto, a piano concerto and a double concerto for violin and cello), Karol Szymanowski (two violin concertos and a \"Symphonie Concertante\" for piano), and Richard Strauss (two horn concertos, a violin concerto, \"Don Quixote\"\u2014a tone poem that features the cello as a soloist\u2014and among later works, an oboe concerto).\nHowever, in the first decades of the 20th century, several composers such as Debussy, Schoenberg, Berg, Hindemith, Stravinsky, Prokofiev and Bart\u00f3k started experimenting with ideas that were to have far-reaching consequences for the way music is written and, in some cases, performed. Some of these innovations include a more frequent use of modality, the exploration of non-western scales, the development of atonality and neotonality, the wider acceptance of dissonances, the invention of the twelve-tone technique of composition and the use of polyrhythms and complex time signatures.\nThese changes also affected the concerto as a musical form. Beside more or less radical effects on musical language, they led to a redefinition of the concept of virtuosity that included new and extended instrumental techniques and a focus on previously neglected aspects of sound such as pitch, timbre and dynamics. In some cases, they also brought about a new approach to the role of soloists and their relation to the orchestra.\nTwo great innovators of early 20th-century music, Schoenberg and Stravinsky, both wrote violin concertos. The material in Schoenberg's concerto, like that in Berg's, is linked by the twelve-tone serial method. In the 20th century, particularly after the Second World War, the cello enjoyed an unprecedented popularity. As a result, its concertante repertoire caught up with those of the piano and the violin both in terms of quantity and quality.\nThe 20th century also witnessed a growth of the concertante repertoire of instruments, some of which had seldom or never been used in this capacity, and even a concerto for wordless coloratura soprano by Reinhold Gli\u00e8re. As a result, almost all classical instruments now have a concertante repertoire. Among the works of the prolific composer Alan Hovhaness may be noted \"Prayer of St. Gregory\" for trumpet and strings, though it is not a concerto in the usual sense of the term. In the later 20th century the concerto tradition was continued by composers such as Maxwell Davies, whose series of Strathclyde Concertos exploit some of the instruments less familiar as soloists.\nIn addition, the 20th century gave rise to several composers who experimented further by showcasing a variety of nontraditional orchestral instruments within the center of the orthodox concerto form. Included within this group are: Paul Hindemith (\"Concerto for Trautonium and String Orchestra\" in 1931), Andre Jolivet (\"Concerto of Ondes Martenot\" in 1947), Heitor Villa-Lobos (\"Concerto for Harmonica\" in 1956), John Serry Sr. (\"Concerto in C Major for Bassetti Accordion\" in 1966), Astor Piazzolla (\"Concerto for Bandoneon, String Orchestra and Percussion\", \"Aconcagua\" in 1979), Peter Maxwell Davies (\"Concerto for Piccolo and Orchestra,\" Op. 182 in 1996), and Tan Dun (\"Concerto for Water Percussion and Orchestra\" in 1998)\nOther composers of this era adopted a neoclassical rejection of specific features which typically characterized the concerto form during the Baroque or Romantic periods. Several of them achieved this objective by incorporating various musical elements from the realm of jazz within the structure of the concerto. Included in this group were: Aaron Copland (\"Concerto for Piano\", 1926), Maurice Ravel (\"Concerto for the Left Hand\", 1929), Igor Stravinsky (\"Ebony Concerto\" for clarinet and jazz band, 1945) and George Gershwin (\"Concerto in F\", 1925). Still others called upon the orchestra itself to function as the primary virtuosic force within the concerto form. This approach was adopted by B\u00e9la Bart\u00f3k in his \"Concerto for Orchestra\" as well by other composers of the period including: Walter Piston (1933), Zoltan Kodaly (1939), Michael Tippet (1962) and Elliott Carter (1969).\nConcertos with concert band include:\nConcertos created with Artificial Intelligence.\nThe first full concertos composed with the assistance of artificial intelligence were published by the Romanian musician Sore In. Between May and June 2024, he released three works: a violin concerto (\"Respiro\", in B-flat major), a guitar concerto (\"Nimble\", in G major), and a flute concerto (\"Susur\", in C-sharp major). In January 2025, he followed these with a cello concerto, \"Porta Calida\" (in D major). Although other artists had previously experimented with AI-generated sketches or partial drafts, Sore In\u2019s works represented the first coherent fully structured concertos developed with AI technology.\nBy type.\nVocal concerto.\n20th century:\nWithout orchestra.\nSingle solo instrument.\nBaroque era:\n20th century:\nMultiple instruments.\nBaroque era:\n20th century:\nFor one instrumental soloist and orchestra.\nFor bowed string instrument and orchestra.\nViolin concerto.\nBaroque era:\nClassical era:\nEarly Romantic traits can be found in the violin concertos of Viotti, but it is Spohr's twelve violin concertos, written between 1802 and 1827, that truly embrace the Romantic spirit with their melodic as well as their dramatic qualities.\n20th century:\n21st century:\nViola concerto.\nBaroque era:\nClassical era:\n20th century:\nCello concerto.\nThe 'core' repertoire\u2014performed the most of any cello concertos\u2014are by Elgar, Dvo\u0159\u00e1k, Saint-Sa\u00ebns, Haydn, Shostakovich and Schumann, but many more concertos are performed nearly as often.\nBaroque era:\nClassical era:\nRomantic era:\n20th century:\nDouble bass concerto.\n20th century:\nOther bowed string instruments.\n20th century:\nFor plucked string instrument and orchestra.\nHarp concerto.\nBaroque era:\nClassical era:\nRomantic era:\n20th century:\nMandolin concerto.\nBaroque era:\n20th century:\nGuitar concerto.\n20th century:\nOther plucked string instruments.\nBaroque era:\n20th century:\nFor woodwind instrument and orchestra.\nFlute concerto.\nBaroque era:\nClassical era:\n20th century:\nOboe concerto.\nBaroque era:\nClassical era:\nRomantic era:\n20th century:\nEnglish horn.\n20th century:\nBassoon concerto.\n20th century:\nClarinet concerto.\n20th century:\n21st century:\nSaxophone concerto.\n20th century:\nOther woodwind instruments.\n20th century:\nFor brass instrument and orchestra.\nTrumpet concerto.\n20th century:\nHorn concerto.\nClassical era:\n20th century:\nTrombone concerto.\n20th century:\nOther brass instruments.\n20th century:\nKeyboard concerto.\nHarpsichord concerto.\nBaroque era:\n20th century:\nOrgan concerto.\nBaroque era:\n20th century:\nPiano concerto.\nClassical era:\nRomantic era:\n20th century:\nAccordion concerto.\n20th century:\nOther keyboard instruments.\n20th century:\nOther instrumental soloist.\nPercussion instrument.\n20th century:\nFree reed aerophone.\n20th century:\nElectronic musical instrument.\n20th century:\nFor multiple instruments and orchestra.\nIn the Baroque era, two violins and one cello formed the standard concertino of a concerto grosso. In the classical era, the sinfonia concertante replaced the concerto grosso genre, although concertos for two or three soloists were still composed too. From the Romantic era works for multiple instrumental soloists and orchestra were again commonly called concerto.\nTwo soloists.\nBaroque era:\nClassical era:\nRomantic era:\n20th century:\nThree soloists.\nBaroque era:\nClassical era:\nRomantic era:\n21st century:\nFour or more soloists.\nBaroque era:\n20th century:\nConcerto for orchestra.\nSymphonic orchestra.\nIn the 20th and 21st centuries, several composers wrote concertos for orchestra. In these works, different sections and/or instruments of the orchestra or concert band are treated at one point or another as soloists with emphasis on solo sections and/or instruments changing during the piece. Some examples include those written by:\nDutilleux has also described his \"M\u00e9taboles\" as a concerto for orchestra.\nChamber orchestra or string orchestra.\nBaroque era:\n20th century:\nMore than one orchestra.\nBaroque era:\n20th century:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44118", "revid": "1314170617", "url": "https://en.wikipedia.org/wiki?curid=44118", "title": "Sonata", "text": "Type of instrumental composition\nIn music, a sonata (; pl. \"sonate\") is a piece that consists of 3 or 4 movements that can be for different musical instruments\n.17 The term evolved through the history of music, designating a variety of forms until the Classical era, when it took on increasing importance. Sonata is a vague term, with varying meanings depending on the context and time period. By the early 19th century it came to represent a principle of composing large-scale works. It was applied to most instrumental genres and regarded\u2014alongside the fugue\u2014as one of two fundamental methods of organizing, interpreting and analyzing concert music. Though the musical style of sonatas has changed since the Classical era, most 20th- and 21st-century sonatas maintain the overarching structure.\nThe term sonatina, pl. \"sonatine\", the diminutive form of sonata, is often used for a short or technically easy sonata.\nInstrumentation.\nIn the Baroque period, a sonata was for one or more instruments, almost always with continuo. After the Baroque period most works designated as sonatas specifically are performed by a solo instrument, most often a keyboard instrument, or by a solo instrument accompanied by a keyboard instrument.\nSonatas for a solo instrument other than keyboard have been composed, as have sonatas for other combinations of instruments.\nThere are some general guidelines a typical sonata might follow, however, it is important to acknowledge the term sonata still hadn\u2019t taken shape yet in the 17th century because of the \"sinfonia\" conflating the term. A sinfonia were pieces played by multiple instruments together, upholding the characteristics of the imitative canzona. The sinfonia showed precursors to the introductory movement of sonata form today. As newer types of canzonas and concertos began to form (called \"stile moderno\"), the sonata was still an ambiguous genre because many characteristics of other forms became entangled with early sonatas.\nThe sonata finally began to become a separate entity starting in the 17th to 18th centuries when the canzona became less popular and the suite, concerto, and sonata all developed in different directions. In short, a suite is a sequence of movements based on dance movements, whereas sonatas do not possess complete dance like movements. Although it is important to note that sonatas can contain movements assembled from parts of dance movements, but the passages are not formal enough to be called a suite. Sonatas were standardized to either fall into being a \"sonata da camera\", \u201cchamber sonata,\u201d or a \"sonata da chiesa\", \u201cchurch sonata.\u201d Corelli\u2019s twelve trio sonatas, Op. 2, were foundational to the development of the sonata and an example of 12 chamber trio sonatas, Op. 2, in 1685. Corelli\u2019s prolific work in his trio sonatas inspired Bach, Vivaldi, Handel, and Telemann.\nThe sonata and the suite were two forms that experienced overlap in France, Germany, and England; however, remained separate in Italy because the scoring criteria was different. Beste writes that during this time period, the keyboard repertoire evolved with the sonata as Bach was writing his keyboard suites, with BWV 825-30 being called \u201cpartitas.\u201d Beste writes on the partita that \u201cBy the late seventeenth century, however, [the partita] had come to denote a multi-movement instrumental cycle, either still as a set of variations or as a succession of dances. Only in its latter connotation does it overlap with the sonata, and only in a specific instrumental and geographical context: its widespread currency is limited to Germany, and to the solo keyboard repertoire (12). The overlap between sonata and partita are interesting to consider looking at Bach\u2019s unaccompanied sonatas for violin, as Beste writes \u201cthey conform to the four-movement \u2018church sonata\u2019 pattern established by Corelli, for which no other generic term was available. The partitas, on the other hand, borrow their designation from the keyboard repertoire, as multi-movement dance cycles for solo instrument.\u201d\nHistory.\nBaroque.\nIn the works of Arcangelo Corelli and his contemporaries, two broad classes of sonata were established, and were first described by S\u00e9bastien de Brossard in his \"Dictionaire de musique\" (third edition, Amsterdam, ca. 1710): the sonata da chiesa (that is, suitable for use in church), which was the type \"rightly known as \"Sonatas\"\", and the sonata da camera (proper for use at court), which consists of a prelude followed by a succession of dances, all in the same key. Although the four, five, or six movements of the sonata da chiesa are also most often in one key, one or two of the internal movements are sometimes in a contrasting tonality.\nThe sonata da chiesa, generally for one or two violins and basso continuo, consisted normally of a slow introduction, a loosely fugued allegro, a cantabile slow movement, and a lively finale in some binary form suggesting affinity with the dance-tunes of the suite. This scheme, however, was not very clearly defined, until the works of Arcangelo Corelli when it became the essential sonata and persisted as a tradition of Italian violin music.\nThe sonata da camera consisted almost entirely of idealized dance-tunes. On the other hand, the features of \"sonata da chiesa\" and \"sonata da camera\" then tended to be freely intermixed. Although nearly half of Johann Sebastian Bach's 1,100 surviving compositions, arrangements, and transcriptions are instrumental works, only about 4% are sonatas.\nThe term \"sonata\" is also applied to the series of over 500 works for harpsichord solo, or sometimes for other keyboard instruments, by Domenico Scarlatti, originally published under the name \"Essercizi per il gravicembalo\" (Exercises for the Harpsichord). Most of these pieces are in one binary-form movement only, with two parts that are in the same tempo and use the same thematic material, though occasionally there will be changes in tempo within the sections. They are frequently virtuosic, and use more distant harmonic transitions and modulations than were common for other works of the time. They were admired for their great variety and invention.\nBoth the solo and trio sonatas of Vivaldi show parallels with the concerti he was writing at the same time. He composed over 70 sonatas, the great majority of which are of the solo type; most of the rest are trio sonatas, and a very small number are of the multivoice type.\nThe sonatas of Domenico Paradies are mild and elongated works with a graceful and melodious little second movement included.\nClassical period.\nThe practice of the Classical period would become decisive for the sonata; the term moved from being one of many terms indicating genres or forms, to designating the fundamental form of organization for large-scale works. This evolution stretched over fifty years. The term came to apply both to the structure of individual movements (see Sonata form and History of sonata form) and to the layout of the movements in a multi-movement work. In the transition to the Classical period there were several names given to multimovement works, including divertimento, serenade, and partita, many of which are now regarded effectively as sonatas. The usage of \"sonata\" as the standard term for such works began somewhere in the 1770s. Haydn labels his first piano sonata as such in 1771, after which the term \"divertimento\" is used sparingly in his output. The term \"sonata\" was increasingly applied to either a work for keyboard alone (see piano sonata), or for keyboard and one other instrument, often the violin or cello. It was less and less frequently applied to works with more than two instrumentalists; for example, piano trios were not often labelled \"sonata for piano, violin, and cello.\"\nInitially the most common layout of movements was:\nHowever, two-movement layouts also occur, a practice Haydn uses as late as the 1790s. There was also in the early Classical period the possibility of using four movements, with a dance movement inserted before the slow movement, as in Haydn's piano sonatas No. 6 and No. 8. Mozart's sonatas were also primarily in three movements. Of the works that Haydn labelled \"piano sonata\", \"divertimento\", or \"partita\" in Hob XIV, seven are in two movements, thirty-five are in three, and three are in four; and there are several in three or four movements whose authenticity is listed as \"doubtful.\" Composers such as Boccherini would publish sonatas for piano and obbligato instrument with an optional third movement\u2014\u2013in Boccherini's case, 28 cello sonatas.\nBut increasingly instrumental works were laid out in four, not three movements, a practice seen first in string quartets and symphonies, and reaching the sonata proper in the early sonatas of Beethoven. But two- and three-movement sonatas continued to be written throughout the Classical period: Beethoven's opus 102 pair has a two-movement C major sonata and a three-movement D major sonata. Nevertheless, works with fewer or more than four movements were increasingly felt to be exceptions; they were labelled as having movements \"omitted,\" or as having \"extra\" movements.\nThe four-movement layout was by this point standard for the string quartet, and overwhelmingly the most common for the symphony. The usual order of the four movements was:\nWhen movements appeared out of this order they would be described as \"reversed\", such as the scherzo coming before the slow movement in Beethoven's 9th Symphony. This usage would be noted by critics in the early 19th century, and it was codified into teaching soon thereafter.\nIt is difficult to overstate the importance of Beethoven's output of sonatas: 32 piano sonatas, plus sonatas for cello and piano or violin and piano, forming a large body of music that would over time increasingly be thought essential for any serious instrumentalist to master.\nRomantic period.\nIn the early 19th century, the current usage of the term \"sonata\" was established, both as regards form \"per se\", and in the sense that a fully elaborated sonata serves as a norm for concert music in general, which other forms are seen in relation to. From this point forward, the word \"sonata\" in music theory labels as much the abstract musical form as particular works. Hence there are references to a symphony as a \"sonata for orchestra\". This is referred to by William Newman as the \"sonata idea\".\nAmong works expressly labeled \"sonata\" for the piano, there are the three of Fr\u00e9d\u00e9ric Chopin, those of Felix Mendelssohn, the three of Robert Schumann, Franz Liszt's Sonata in B minor, and later the sonatas of Johannes Brahms and Sergei Rachmaninoff.\nIn the early 19th century, the sonata form was defined, from a combination of previous practice and the works of important Classical composers, particularly Haydn, Mozart, Beethoven, but composers such as Clementi also. It is during this period that the differences between the three- and the four-movement layouts became a subject of commentary, with emphasis on the concerto being laid out in three movements, and the symphony in four.\nErnest Newman wrote in the essay \"Brahms and the Serpent\":\nThat, perhaps, will be the ideal of the instrumental music of the future; the way to it, indeed, seems at last to be opening out before modern composers in proportion as they discard the last tiresome vestiges of sonata form. This, from being what it was originally, the natural mode of expression of a certain eighteenth century way of thinking in music, became in the nineteenth century a drag upon both individual thinking and the free unfolding of the inner vital force of an idea, and is now simply a shop device by which a bad composer may persuade himself and the innocent reader of textbooks that he is a good one.\nAfter the Romantic period.\nThe role of the sonata as an extremely important form of extended musical argument would inspire composers such as Hindemith, Prokofiev, Shostakovich, Tailleferre, Ustvolskaya, and Williams to compose in sonata form, and works with traditional sonata structures continue to be composed and performed.\nScholarship and musicology.\nSonata idea or principle.\nResearch into the practice and meaning of sonata form, style, and structure has been the motivation for important theoretical works by Heinrich Schenker, Arnold Schoenberg, and Charles Rosen among others; and the pedagogy of music continued to rest on an understanding and application of the rules of sonata form as almost two centuries of development in practice and theory had codified it.\nThe development of the classical style and its norms of composition formed the basis for much of the music theory of the 19th and 20th centuries. As an overarching formal principle, sonata was accorded the same central status as Baroque fugue; generations of composers, instrumentalists, and audiences were guided by this understanding of sonata as an enduring and dominant principle in Western music. The sonata idea begins before the term had taken on its present importance, along with the evolution of the Classical period's changing norms. The reasons for these changes, and how they relate to the evolving sense of a new formal order in music, is a matter to which research is devoted. Some common factors which were pointed to include: the shift of focus from vocal music to instrumental music; changes in performance practice, including the loss of the continuo.\nCrucial to most interpretations of the sonata form is the idea of a tonal center; and, as the \"Grove Concise Dictionary of Music\" puts it: \"The main form of the group embodying the 'sonata principle', the most important principle of musical structure from the Classical period to the 20th century: that material first stated in a complementary key be restated in the home key\".(\nThe sonata idea has been thoroughly explored by William Newman in his monumental three-volume work \"Sonata in the Classic Era (A History of the Sonata Idea)\", begun in the 1950s and published in what has become the standard edition of all three volumes in 1972.\n20th-century theory.\nHeinrich Schenker argued that there was an \"Urlinie\" or basic tonal melody, and a basic bass figuration. He held that when these two were present, there was basic structure, and that the sonata represented this basic structure in a whole work with a process known as \"interruption\".\nAs a practical matter Schenker applied his ideas to the editing of the piano sonatas of Beethoven, using original manuscripts and his own theories to \"correct\" the available sources. The basic procedure was the use of tonal theory to infer meaning from available sources as part of the critical process, even to the extent of completing works left unfinished by their composers. While many of these changes were and are controversial, that procedure has a central role today in music theory, and is an essential part of the theory of sonata structure as taught in most music schools.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "44120", "revid": "82432", "url": "https://en.wikipedia.org/wiki?curid=44120", "title": "Jelly Roll Morton", "text": "American ragtime and jazz musician (1890\u20131941)\nFerdinand Joseph LaMothe (n\u00e9 Lemott, later Morton; c. September 20, 1890 \u2013 July 10, 1941), known professionally as Jelly Roll Morton, was an American blues and jazz pianist, bandleader, and composer of Louisiana Creole descent. Morton was jazz's first arranger, proving that a genre rooted in improvisation could retain its essential characteristics when notated. His composition \"Jelly Roll Blues\", published in 1915, was one of the first published jazz compositions. He also claimed to have invented the genre.\nMorton also wrote \"King Porter Stomp\", \"Wolverine Blues\", \"Black Bottom Stomp\", and \"I Thought I Heard Buddy Bolden Say\", the last being a tribute to New Orleans musicians from the turn of the 20th century.\nMorton's claim to have invented jazz in 1902 was criticized. Music critic Scott Yanow wrote, \"Jelly Roll Morton did himself a lot of harm posthumously by exaggerating his worth ... Morton's accomplishments as an early innovator are so vast that he did not really need to stretch the truth.\" On the other hand, Gunther Schuller says of Morton's \"hyperbolic assertions\" that there is \"no proof to the contrary\" and that Morton's \"considerable accomplishments in themselves provide reasonable substantiation.\u201d\nBiography.\nEarly life.\nMorton was born Ferdinand Joseph LaMothe (or Lemott), into the Creole community in the Faubourg Marigny neighborhood of New Orleans around 1890; he claimed to have been born in 1884 on his WWI draft registration card in 1918. Both parents traced their Creole ancestry four generations to the 18th century. Morton's birth date and year of birth are uncertain, given that no birth certificate was ever issued for him. The law requiring birth certificates for citizens was not enforced until 1914. His parents were Martin-Edouard Joseph Lamothe, also known as Edward Joseph Lamothe, a bricklayer and occasional trombonist, and Louise Hermance Monette, a domestic worker. His parents were never legally married and his father left his mother when Morton was around three years old. After his mother married William Mouton in 1894, Ferdinand adopted his stepfather's surname, anglicizing it to Morton, adapting \"Ferd\" as an unofficial forename. Ferd had two sisters, one of whom, Eug\u00e9nie, married Ignace Colas, in 1913.\nCareer.\nAt the age of fourteen, Morton began as a piano player in a brothel. He often sang smutty lyrics and used the nickname \"Jelly Roll\", which was African-American slang for female genitalia. While working there, he was living with his churchgoing great-grandmother. He convinced her that he worked as a night watchman in a barrel factory. After Morton's grandmother found out he was playing music in a brothel, she disowned him. \"When my grandmother found out that I was playing jazz in one of the sporting houses in the District, she told me that I had disgraced the family and forbade me to live at the house. She told me that devil music would surely bring about my downfall...\" The cornetist Rex Stewart recalled that Morton had chosen \"the nom de plume 'Morton' to protect his family from disgrace if he was identified as a whorehouse 'professor'.\"\nAround 1904, Morton started touring in the US South, working in minstrel shows such as Will Benbow's Chocolate Drops, gambling, and composing. His songs \"Jelly Roll Blues\", \"New Orleans Blues\", \"Frog-I-More Rag\", \"Animule Dance\", and \"King Porter Stomp\" were composed during this period. Stride pianists James P. Johnson and Willie \"The Lion\" Smith saw him perform in Chicago in 1910 and New York City in 1911.\nIn 1912\u201314, Morton toured with his girlfriend Rosa Brown as a vaudeville act before living in Chicago for three years. By 1914, he was putting his compositions on paper. In 1915 \"Jelly Roll Blues\" was one of the first jazz compositions to be published. Jelly Roll Morton was employed by Ben Shook Jr. around 1916. Shook was associated with a Jubilee club led by Mabel Lewis, a contralto singer and former member of the original Fisk University Jubilee Singers. In 1917 he went to California with bandleader William Manuel Johnson and Johnson's sister Anita Gonzalez, born Bessie Julia Johnson. Morton's tango \"The Crave\" was popular in Hollywood. He was invited to perform at the Hotel Patricia nightclub in Vancouver, Canada. Author Mark Miller described his arrival as \"an extended period of itinerancy as a pianist, vaudeville performer, gambler, hustler, and, as legend would have it, pimp\". Morton returned to Chicago in 1923 to claim authorship of \"The Wolverines\", which had become popular as \"Wolverine Blues\". He released the first of his commercial recordings, first as piano rolls, then on record, both as a piano soloist and with jazz bands.\nIn 1926, Morton signed a contract with the Victor Talking Machine Company, giving him the opportunity to bring a well-rehearsed band to play his arrangements in the Victor recording studios in Chicago. These recordings by Jelly Roll Morton and His Red Hot Peppers included Kid Ory, Omer Simeon, George Mitchell, Johnny St. Cyr, Barney Bigard, Johnny Dodds, Baby Dodds, and Andrew Hilaire.\nAfter Morton moved to New York City, he continued to record for Victor. Although he had trouble finding musicians who wanted to play his style of jazz, he recorded with Omer Simeon, George Baquet, Albert Nicholas, Barney Bigard, Russell Procope, Lorenzo Tio and Artie Shaw, the trumpeters Ward Pinkett, Bubber Miley, Johnny Dunn and Henry \"Red\" Allen, Sidney Bechet, Paul Barnes, Bud Freeman, Pops Foster, Paul Barbarin, Cozy Cole, and Zutty Singleton. His New York sessions failed to produce a hit.\nDue in part to the Great Depression, RCA Victor did not renew Morton's recording contract for 1931. He continued playing in New York but struggled financially. He briefly had a radio show in 1934, then toured in a burlesque band. In 1935, his 30-year-old composition \"King Porter Stomp\", arranged by Fletcher Henderson, became Benny Goodman's first hit and a swing standard, but Morton received no royalties from the recordings.\nMusic Box interviews.\nIn 1935, Morton moved to Washington, D.C., to become the manager and piano player at a bar called, at various times, the Music Box, Blue Moon Inn, and Jungle Inn, at 1211 U Street NW in Shaw, an African-American neighborhood. Morton was master of ceremonies, bouncer, and bartender. The club owner allowed her friends free admission and drinks, which prevented Morton from making the business a success. During Morton's brief residency at the Music Box, the folklorist Alan Lomax heard him play. In May 1938, Lomax invited Morton to record music and interviews for the Library of Congress. The sessions were intended to be a short interview with musical examples for researchers at the Library of Congress, but the sessions expanded to over eight hours, with Morton talking and playing piano. Lomax conducted longer interviews, taking notes but not recording. Lomax was interested in Morton's days in Storyville, New Orleans, and the ribald songs of the time. Although reluctant to record these, Morton obliged Lomax. Because of the suggestive nature of the songs, some of the Library of Congress recordings were not released until 2005.\nIn these interviews, Morton claimed to have been born in 1885. Morton scholars, such as Lawrence Gushee, say that Morton was aware that if he had been born in 1890, he would have been too young to claim to be the inventor of jazz. However, Morton may not have known his actual birthdate, and there remains the possibility that he was telling the truth. He said Buddy Bolden played ragtime but not jazz, a view not accepted by some of Bolden's contemporaries in New Orleans. The contradictions may stem from different definitions of \"ragtime\" and \"jazz\".\nStabbing, later life, and death.\nIn 1938, Morton was stabbed by a friend of the Music Box's owner and suffered wounds to the head and chest. A nearby whites-only hospital refused to treat him, as the city had racially segregated facilities. He was transported to a black hospital farther away. When he was in the hospital, doctors left ice on his wounds for several hours before attending to the injury. His recovery from his wounds was incomplete, and thereafter he was often ill and became short of breath easily. After this incident, his wife Mabel demanded they leave Washington.\nWorsening asthma sent him to a hospital in New York for three months. He continued to suffer from respiratory problems when he travelled to Los Angeles with the intent to restart his career. He died on July 10, 1941, after an eleven-day stay in Los Angeles County General Hospital. He was generally believed to be 50 years old. According to the jazz historian David Gelly in 2000, Morton's arrogance and \"bumptious\" persona alienated so many musicians that few of them attended his funeral.\nAn article about the funeral in the August 1, 1941, issue of \"DownBeat\" reported that his pallbearers were Kid Ory, Mutt Carey, Fred Washington, and Ed Garland. It noted that Duke Ellington and Jimmie Lunceford were absent, though both were appearing in Los Angeles at the time. Mercer Ellington, Duke Ellington's son, did attend the funeral. The article was reproduced in \"Mister Jelly Roll\", a 1950 biography of Morton by Alan Lomax.\nPersonal life.\nMorton married Mabel Bertrand, a showgirl, in November 1928 in Gary, Indiana.\nHe was a \"very devout Catholic\", according to Anita Gonzales, his long-term companion. His gravesite features a large Rosary rather than any music imagery.\nForm and compositions.\nMorton's piano style was formed from early secondary ragtime and \"shout\", which also evolved separately into the New York school of stride piano. Morton's playing was also close to barrelhouse, which produced boogie-woogie.\nMorton often played the melody of a tune with his right thumb, while sounding a harmony above these notes with the fingers of the right hand. This could add a rustic or \"out-of-tune\" sound due to the playing of a diminished 5th above the melody. This technique may still be recognized as belonging to New Orleans. Morton also walked in major and minor sixths in the bass, instead of tenths or octaves. He played basic swing rhythms with both the left and the right hand.\nSeveral of Morton's compositions were musical tributes to himself, including \"Winin' Boy\", \"The Jelly Roll Blues\" (subtitled \"The Original Jelly-Roll\"); and \"Mr. Jelly Lord\". In the big-band era, his \"King Porter Stomp\", which Morton had written decades earlier, was a big hit for Fletcher Henderson and Benny Goodman; it became a standard covered by most other swing bands of that time. Morton claimed to have written some tunes that were copyrighted by others, including \"Alabama Bound\" and \"Tiger Rag\". \"Sweet Peter\", which Morton recorded in 1926, appears to be the source of the melody of the hit song \"All of Me\", which was credited to Gerald Marks and Seymour Simons in 1931.\nHis musical influence continues in the work of Dick Hyman and Reginald Robinson.\nLegacy.\nIn 2013, Katy Martin published an article arguing that Alan Lomax's book of interviews put Morton in a negative light. Martin disagreed that Morton was an egotist.In being called a supreme egotist, Jelly Roll was often a victim of loose and lurid reporting. If we read the words that he himself wrote, however, we learn that he almost had an inferiority complex and said that he created his own style of jazz piano because 'All my fellow musicians were much faster in manipulations, I thought than I, and I did not feel as though I was in their class.' So he used a slower tempo to permit flexibility through the use of more notes, a pinch of Spanish to give a number of right seasoning, the avoidance of playing triple forte continuously, and many other points.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44122", "revid": "50977304", "url": "https://en.wikipedia.org/wiki?curid=44122", "title": "American Beauty (1999 film)", "text": "American dark comedy-drama film by Sam Mendes\nAmerican Beauty is a 1999 American psychological black comedy-drama film written by Alan Ball and directed by Sam Mendes in his feature directorial debut. Kevin Spacey stars as Lester Burnham, an advertising executive who has a midlife crisis when he becomes infatuated with his teenage daughter's best friend, played by Mena Suvari. Annette Bening stars as Lester's materialistic wife, Carolyn, and Thora Birch plays their insecure daughter, Jane. Wes Bentley, Chris Cooper, Peter Gallagher, and Allison Janney co-star. Academics have described the film as satirizing how beauty and personal satisfaction are perceived by the American middle class; further analysis has focused on the film's explorations of romantic and paternal love, sexuality, materialism and self-liberation.\nAfter being filmed in California from December 1998 to February 1999, \"American Beauty\" was released by DreamWorks Pictures in North America on September 17, 1999, receiving widespread critical and popular acclaim. It was the second-best-reviewed American film of the year behind \"Being John Malkovich\" and grossed over $350\u00a0million worldwide against its $15-million budget, becoming the ninth highest-grossing film of 1999. DreamWorks launched a major campaign to increase \"American Beauty\"'s chances of Oscar success following its controversial Best Picture snub for \"Saving Private Ryan\" (1998) the previous year.\nAt the 72nd Academy Awards, the film won five Oscars, including Best Picture, along with Best Director for Mendes, Best Actor for Spacey, Best Original Screenplay for Ball, and Best Cinematography for Conrad L. Hall. The film was nominated for and won many other awards and honors, mainly for directing, writing, and acting.\nPlot.\nLester Burnham, a middle-aged media executive in suburbia, despises his job and is unhappily married to neurotic, status-obsessed Carolyn, a real estate saleswoman. Their 16-year-old daughter and only child, Jane, resents her parents and has low self-esteem. Retired Marine colonel Frank Fitts, his near-catatonic wife Barbara, and their teenage son Ricky move in next door. Ricky documents the world around him with a camcorder, collecting recordings on videotape in his bedroom, as he finds beauty in unexpected places and things; he pays for supplies by dealing marijuana, using his part-time catering and waitstaff jobs as a front. A strict and abusive disciplinarian, Frank previously had Ricky sent to a psychiatric hospital and military school. Gay couple Jim Olmeyer and Jim Berkley, also neighbors, welcome the Fitts family, angering the homophobic Frank.\nOne evening during a cheerleading routine at a school basketball game, Lester becomes infatuated with Jane's friend Angela Hayes, who brags to her classmates about being sexually experienced. He starts having sexual fantasies about her, in which red rose petals are a recurring motif. Carolyn begins an affair with married real estate \"King\" Buddy Kane. Lester quits his job, blackmails his supervisor into giving him a generous severance package, and starts working as a fry cook at a fast food restaurant. He also buys his dream car, a 1970 Pontiac Firebird, and starts regularly exercising after overhearing Angela teasing Jane that she would have sex with Lester if he improved his physique. He begins smoking marijuana supplied by Ricky and returning Angela's flirts. The girls' friendship wanes when Angela and Lester start having sexual interests in one another and when Jane starts a relationship with Ricky; while Angela thinks he is strange, Jane appreciates him for focusing on the beauty he sees within her.\nBuddy and Carolyn have a date at a shooting range. Lester then discovers Carolyn's infidelity when they order a meal at the fast food's drive through. Buddy fears a costly divorce and ends the affair, while Carolyn is humiliated and simultaneously frustrated by her lack of professional success. Frank finds Ricky's recording of a nude Lester working out and becomes suspicious of their friendship, assuming that they are sexually involved after spying on their drug session. He viciously accuses Ricky of being gay and expels him from the house, to which Ricky defiantly agrees. Carolyn, driving home, withdraws a handgun from the glove box as she listens to a self-help tape. At home, Jane argues with Lester and Angela over Angela's sexual interest in Lester, when Ricky interrupts to ask Jane to leave with him for New York City before he dismisses Angela as uninteresting and unattractive.\nFrank tentatively approaches Lester in the Burnhams' garage, then breaks down and tearfully embraces him. Lester comforts Frank and gently rebuffs Frank's attempts to kiss him. Lester finds Angela alone and consoles her, then professes his attraction to her during their conversation. As he begins to undress her on the couch, she admits her virginity. Lester realizes that she had feigned being sexually experienced, and he cannot continue. He bonds with her as they share their frustrations. Angela goes to the bathroom as Lester smiles at a family photograph, when an unseen figure shoots him in the back of the head at point-blank range.\nRicky and Jane, who had earlier considered killing Lester themselves, find his body. Discovering the body herself, Carolyn hides in the master closet, discards her gun, and hugs Lester's clothing. A blood-soaked Frank returns home, a gun missing from his collection. Lester's closing narration describes meaningful experiences during his life, for which he expresses gratitude. Despite being murdered, he is finally happy, having found beauty in the world.\nCast.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nThemes and analysis.\nInterpretations.\nScholars and academics have offered many possible readings of \"American Beauty\"; film critics are similarly divided, not so much about the quality of the film, as their interpretations of it. Described by many as about \"the meaning of life\" or \"the hollow existence of the American suburbs\", the film has defied categorization by even the filmmakers. Mendes is indecisive, saying the script seemed to be about something different each time he read it: \"a mystery story, a kaleidoscopic journey through American suburbia, a series of love stories;\u00a0... it was about imprisonment,\u00a0... loneliness, [and] beauty. It was funny; it was angry, sad.\"\nLiterary critic and author Wayne C. Booth concludes that the film resists any one interpretation: \"[\"American Beauty\"] cannot be adequately summarized as 'here is a satire on what's wrong with American life'; that plays down the celebration of beauty. It is more tempting to summarize it as 'a portrait of the beauty underlying American miseries and misdeeds', but that plays down the scenes of cruelty and horror, and Ball's disgust with mores. It cannot be summarized with either Lester or Ricky's philosophical statements about what life is or how one should live.\" He argues that the problem of interpreting the film is tied with that of finding its center\u2014a controlling voice who \"[unites] all of the choices\". He contends that in \"American Beauty\"'s case, it is neither Mendes nor Ball. Mendes considers the voice to be Ball's, but even while the writer was \"strongly influential\" on set, he often had to accept deviations from his vision, particularly ones that transformed the cynical tone of his script into something more optimistic. With \"innumerable voices intruding on the original author's,\" Booth says, those who interpret \"American Beauty\" \"have forgotten to probe for the elusive center\". According to Booth, the film's true controller is the creative energy \"that hundreds of people put into its production, agreeing and disagreeing, inserting and cutting\".\nImprisonment and redemption.\nMendes called \"American Beauty\" a rite of passage film about imprisonment and escape from imprisonment. The monotony of Lester's existence is established through his gray, nondescript workplace and characterless clothing. In these scenes, he is often framed as if trapped, \"reiterating rituals that hardly please him\". He masturbates in the confines of his shower; the shower stall evokes a jail cell and the shot is the first of many where Lester is confined behind bars or within frames, such as when he is reflected behind columns of numbers on a computer monitor, \"confined [and] nearly crossed out\".\nThe academic and author Jody W. Pennington argues that Lester's journey is the story's center. His sexual reawakening through meeting Angela is the first of several turning points as he begins to \"[throw] off the responsibilities of the comfortable life he has come to despise\". After Lester shares a joint with Ricky, his spirit is released and he begins to rebel against Carolyn. Changed by Ricky's \"attractive, profound confidence\", he is convinced that Angela is attainable and sees that he must question his \"banal, numbingly materialist suburban existence\"; he takes a job at a fast-food outlet, which allows him to regress to a point when he could \"see his whole life ahead of him\".\nWhen Lester is caught masturbating by Carolyn, his angry retort about their lack of intimacy is the first time he says aloud what he thinks about her. By confronting the issue and Carolyn's \"superficial investments in others\", he is trying to \"regain a voice in a home that [only respects] the voices of mother and daughter\". His final turning point comes when he and Angela almost have sex; after she confesses her virginity, he no longer thinks of her as a sex object, but as a daughter. He holds her close and \"wraps her up\". Mendes called it \"the most satisfying end to [Lester's] journey there could possibly have been\". With these final scenes, Mendes intended to show him at the conclusion of a \"mythical quest\". After Lester gets a beer from the refrigerator, the camera pushes toward him, then stops facing a hallway down which he walks \"to meet his fate\". Having begun to act his age again, Lester achieves closure. As he smiles at a family photo, the camera pans slowly from Lester to the kitchen wall, onto which blood spatters as a gunshot rings out; the slow pan reflects the peace of his death. His body is discovered by Jane and Ricky. Mendes said that Ricky's staring into Lester's dead eyes is \"the culmination of the theme\" of the film: that beauty is found where it is least expected.\nConformity and beauty.\nLike other American films of 1999\u2014such as \"Fight Club\", \"Bringing Out the Dead\" and \"Magnolia\", \"American Beauty\" instructs its audience to \"[lead] more meaningful lives\". The film argues the case against conformity, but does not deny that people need and want it; even the gay characters just want to fit in. Jim and Jim, the Burnhams' other neighbors, are a satire of \"gay bourgeois coupledom\", who \"[invest] in the numbing sameness\" that the film criticizes in heterosexual couples. The feminist academic and author Sally R. Munt argues that \"American Beauty\" uses its \"art house\" trappings to direct its message of nonconformity primarily to the middle classes, and that this approach is a \"\"clich\u00e9\" of bourgeois preoccupation;\u00a0... the underlying premise being that the luxury of finding an individual 'self' through denial and renunciation is always open to those wealthy enough to choose, and sly enough to present themselves sympathetically as a rebel.\"\nProfessor Roy M. Anker argues that the film's thematic center is its direction to the audience to \"look closer\". The opening combines an unfamiliar viewpoint of the Burnhams' neighborhood with Lester's narrated admission that this is the last year of his life, forcing audiences to consider their own mortality and the beauty around them. It also sets a series of mysteries; Anker asks, \"from what place exactly, and from what state of being, is he telling this story? If he's already dead, why bother with whatever it is he wishes to tell about his last year of being alive? There is also the question of how Lester has died\u2014or will die.\" Anker believes the preceding scene\u2014Jane's discussion with Ricky about the possibility of his killing her father\u2014adds further mystery.\nProfessor Ann C. Hall disagrees; she says by presenting an early resolution to the mystery, the film allows the audience to put it aside \"to view the film and its philosophical issues\". Through this examination of Lester's life, rebirth and death, \"American Beauty\" satirizes American middle class notions of meaning, beauty and satisfaction. Even Lester's transformation only comes about because of the possibility of sex with Angela; he therefore remains a \"willing devotee of the popular media's exaltation of pubescent male sexuality as a sensible route to personal wholeness\". Carolyn is similarly driven by conventional views of happiness; from her belief in \"house beautiful\" domestic bliss to her car and gardening outfit, Carolyn's domain is a \"fetching American millennial vision of Pleasantville, or Eden\". The Burnhams are unaware that they are \"materialists philosophically, and devout consumers ethically\" who expect the \"rudiments of American beauty\" to give them happiness. Anker argues that \"they are helpless in the face of the prettified economic and sexual stereotypes\u00a0... that they and their culture have designated for their salvation.\"\nThe film presents Ricky as its \"visionary,\u00a0... spiritual and mystical center\". He sees beauty in the minutiae of everyday life, videoing as much as he can for fear of missing it. He shows Jane what he considers the most beautiful thing he has filmed: a plastic bag, tossing in the wind in front of a wall. He says capturing the moment was when he realized that there was \"an entire life behind things\"; he feels that \"sometimes there's so much beauty in the world I feel like I can't take it... and my heart is going to cave in.\" Anker argues that Ricky, in looking past the \"cultural dross\", has \"[grasped] the radiant splendor of the created world\" to see God. As the film progresses, the Burnhams move closer to Ricky's view of the world. Lester only forswears personal satisfaction at the film's end. On the cusp of having sex with Angela, he returns to himself after she admits her virginity. Suddenly confronted with a child, he begins to treat her as a daughter; in doing so, Lester sees himself, Angela, and his family \"for the poor and fragile but wondrous creatures they are\". He looks at a picture of his family in happier times, and dies having had an epiphany that infuses him with \"wonder, joy, and soul-shaking gratitude\"\u2014he has finally seen the world as it is.\nAccording to Patti Bellantoni, colors are used symbolically throughout the film, none more so than red, which is an important thematic signature that drives the story and \"[defines] Lester's arc\". First seen in drab colors that reflect his passivity, Lester surrounds himself with red as he regains his individuality. The American Beauty rose is repeatedly used as symbol; when Lester fantasizes about Angela, she is usually naked and surrounded by rose petals. In these scenes, the rose symbolizes Lester's desire for her. When associated with Carolyn, the rose represents a \"fa\u00e7ade for suburban success\". Roses are included in almost every shot inside the Burnhams' home, where they signify \"a mask covering a bleak, unbeautiful reality\". Carolyn feels that \"as long as there can be roses, all is well\". She cuts the roses and puts them in vases, where they adorn her \"meretricious vision of what makes for beauty\" and begin to die. The roses in the vase in the Angela\u2013Lester seduction scene symbolize Lester's previous life and Carolyn; the camera pushes in as Lester and Angela get closer, finally taking the roses\u2014and thus Carolyn\u2014out of the shot. Lester's epiphany at the end of the film is expressed by rain and the use of red, building to a crescendo that is a deliberate contrast to the release Lester feels. The constant use of red \"lulls [the audience] subliminally\" into becoming used to it; consequently, it leaves the audience unprepared when Lester is shot and his blood spatters on the wall.\nSexuality and repression.\nPennington argues that \"American Beauty\" defines its characters through their sexuality. Lester's attempts to relive his youth are a direct result of his lust for Angela, and the state of his relationship with Carolyn is in part shown through their lack of sexual contact. Also sexually frustrated, Carolyn has an affair that takes her from \"cold perfectionist\" to more of a carefree soul who \"[sings] happily along with\" the music in her car. Jane and Angela constantly reference sex, through Angela's descriptions of her supposed sexual encounters and the way the girls address each other. Their nude scenes are used to communicate their vulnerability. By the end of the film, Angela's hold on Jane has weakened until the only power she has over her friend is Lester's attraction to her. Col. Fitts reacts with disgust to meeting Jim and Jim; he asks, \"How come these faggots always have to rub it in your face? How can they be so shameless?\" To which Ricky replies, \"That's the thing, Dad\u2014they don't feel like it's anything to be ashamed of.\" Pennington argues that Col. Fitts's reaction is not homophobic, but an \"anguished self-interrogation\".\nWith other turn-of-the-millennium films such as \"Fight Club\" (1999), \"In the Company of Men\"\u00a0(1997), \"American Psycho\"\u00a0(2000), and \"Boys Don't Cry\"\u00a0(1999), \"American Beauty\" \"raises the broader, widely explored issue of masculinity in crisis\". Professor Vincent Hausmann charges that in their reinforcement of masculinity \"against threats posed by war, by consumerism, and by feminist and queer challenges\", these films present a need to \"focus on, and even to privilege\" aspects of maleness \"deemed 'deviant'\". Lester's transformation conveys \"that he, and not the woman, has borne the brunt of [lack of being]\" and he will not stand for being emasculated. Lester's attempts to \"strengthen traditional masculinity\" conflict with his responsibilities as a father. Although the film portrays the way Lester returns to that role positively, he does not become \"the hypermasculine figure implicitly celebrated in films like \"Fight Club\"\". Hausmann concludes that Lester's behavior toward Angela is \"a misguided but nearly necessary step toward his becoming a father again\".\nHausmann says the film \"explicitly affirms the importance of upholding the prohibition against incest\"; a recurring theme of Ball's work is his comparison of the taboos against incest and homosexuality. Instead of making an overt distinction, \"American Beauty\" looks at how their repression can lead to violence. Col. Fitts is so ashamed of his homosexuality that it drives him to murder Lester. Ball said, \"The movie is in part about how homophobia is based in fear and repression and about what [they] can do.\" The film implies two unfulfilled incestuous desires: Lester's pursuit of Angela is a manifestation of his lust for his own daughter, while Col. Fitts's repression is exhibited through the almost sexualized discipline with which he controls Ricky. Consequently, Ricky realizes that he can only hurt his father by falsely telling him he is homosexual, while Angela's vulnerability and submission to Lester reminds him of his responsibilities and the limits of his fantasy. Col. Fitts represents Ball's father, whose repressed homosexual desires led to his own unhappiness. Ball rewrote Col. Fitts to delay revealing him as homosexual.\nTemporality and music.\n\"American Beauty\" follows a traditional narrative structure, only deviating with the displaced opening scene of Jane and Ricky from the middle of the story. Although the plot spans one year, the film is narrated by Lester at the moment of his death. Jacqueline Furby says that the plot \"occupies\u00a0... no time [or] all time\", citing Lester's claim that life did not flash before his eyes, but that it \"stretches on forever like an ocean of time\". Furby argues that a \"rhythm of repetition\" forms the core of the film's structure. For example, two scenes have the Burnhams sitting down to an evening meal, shot from the same angle. Each image is broadly similar, with minor differences in object placement and body language that reflect the changed dynamic brought on by Lester's new-found assertiveness. Another example is the pair of scenes in which Jane and Ricky film each other. Ricky films Jane from his bedroom window as she removes her bra, and the image is reversed later for a similarly \"voyeuristic and exhibitionist\" scene in which Jane films Ricky at a vulnerable moment.\nLester's fantasies are emphasized by slow- and repetitive-motion shots; Mendes uses double-and-triple cutbacks in several sequences, and the score alters to make the audience aware that it is entering a fantasy. One example is the gymnasium scene\u2014Lester's first encounter with Angela. While the cheerleaders perform their half-time routine to \"On Broadway\", Lester becomes increasingly fixated on Angela. Time slows to represent his \"voyeuristic hypnosis\" and Lester begins to fantasize that Angela's performance is for him alone. \"On Broadway\"\u2014which provides a conventional underscore to the onscreen action\u2014is replaced by discordant, percussive music that lacks melody or progression. This nondiegetic score is important to creating the narrative stasis in the sequence; it conveys a moment for Lester that is stretched to an indeterminate length. The effect is one that Stan Link likens to \"vertical time\", described by the composer and music theorist Jonathan Kramer as music that imparts \"a single present stretched out into an enormous duration, a potentially infinite 'now' that nonetheless feels like an instant\". The music is used like a visual cue, so that Lester and the score are staring at Angela. The sequence ends with the sudden reintroduction of \"On Broadway\" and teleological time.\nAccording to Drew Miller of \"Stylus\", the soundtrack \"[gives] unconscious voice\" to the characters' psyches and complements the subtext. The most obvious use of pop music \"accompanies and gives context to\" Lester's attempts to recapture his youth; reminiscent of how the counterculture of the 1960s combated American repression through music and drugs, Lester begins to smoke cannabis and listen to rock music. Mendes's song choices \"progress through the history of American popular music\". Miller argues that although some may be over familiar, there is a parodic element at work, \"making good on [the film's] encouragement that viewers look closer\". Toward the end of the film, Thomas Newman's score features more prominently, creating \"a disturbing tempo\" that matches the tension of the visuals. The exception is \"Don't Let It Bring You Down\", which plays during Angela's seduction of Lester. At first appropriate, its tone clashes as the seduction stops. The lyrics, which speak of \"castles burning\", can be seen as a metaphor for Lester's view of Angela\u2014\"the rosy, fantasy-driven exterior of the 'American Beauty'\"\u2014as it burns away to reveal \"the timid, small-breasted girl who, like his wife, has willfully developed a false public self\".\nProduction.\nDevelopment.\nAlan Ball began writing \"American Beauty\" as a play in the early 1990s, partly inspired by the media circus that accompanied the Amy Fisher trial in 1992. He shelved the play after deciding that the story would not work on stage. After spending the next few years writing for television, Ball revived the idea in 1997 when attempting to break into the film industry after several frustrating years writing for the television sitcoms \"Grace Under Fire\" and \"Cybill\". He joined the United Talent Agency, where his representative, Andrew Cannava, suggested he write a spec script to \"reintroduce [himself] to the town as a screenwriter\". Ball pitched three ideas to Cannava: two conventional romantic comedies and \"American Beauty.\" Despite the story's lack of an easily marketable concept, Cannava selected \"American Beauty\" because he felt it was the one for which Ball had the most passion. While developing the script, Ball created another television sitcom, \"Oh, Grow Up\". He channeled his anger and frustration at having to accede to network demands on that show\u2014and during his tenures on \"Grace Under Fire\" and \"Cybill\"\u2014into writing \"American Beauty\".\nBall did not expect to sell the script, believing it would act as more of a calling card, but \"American Beauty\" drew interest from several production bodies. Cannava passed the script to several producers, including Dan Jinks and Bruce Cohen, who took it to DreamWorks. With the help of executives Glenn Williamson and Bob Cooper, and Steven Spielberg in his capacity as studio partner, Ball was convinced to develop the project at DreamWorks; he received assurances from the studio\u2014known at the time for its more conventional fare\u2014that it would not \"iron the [edges] out\". In an unusual move, DreamWorks decided not to option the script; instead, in April 1998, the studio bought it outright for $250,000, outbidding Fox Searchlight Pictures, October Films, The Samuel Goldwyn Company, and Lakeshore Entertainment. DreamWorks planned to make the film for $6\u20138\u00a0million.\nJinks and Cohen involved Ball throughout the film's development, including casting and director selection. The producers met with about twenty interested directors, several of whom were considered A-list at the time. Ball was not keen on the more well-known directors because he believed their involvement would increase the budget and lead DreamWorks to become \"nervous about the content\". Nevertheless, DreamWorks offered the film to Mike Nichols and Robert Zemeckis; neither accepted. In the same year, Sam Mendes (then a theater director) revived the musical \"Cabaret\" in New York with fellow director Rob Marshall. Beth Swofford of the Creative Artists Agency arranged meetings for Mendes with studio figures in Los Angeles to see if film direction was a possibility. Mendes came across \"American Beauty\" in a pile of eight scripts at Swofford's house, and knew immediately that it was the one he wanted to make; early in his career, he had been inspired by how the film \"Paris, Texas\"\u00a0(1984) presented contemporary America as a mythic landscape and he saw the same theme in \"American Beauty\", as well as parallels with his own childhood. Mendes later met with Spielberg; impressed by Mendes's productions of \"Oliver!\" and \"Cabaret\", Spielberg encouraged him to consider \"American Beauty\".\nMendes found that he still had to convince DreamWorks's production executives to let him direct. He had already discussed the film with Jinks and Cohen, and felt they supported him. Ball was also keen; having seen \"Cabaret\", he was impressed with Mendes's \"keen visual sense\" and thought he did not make obvious choices. Ball felt that Mendes liked to look under the story's surface, a talent he felt would be a good fit with the themes of \"American Beauty\". Mendes's background also reassured him, because of the prominent role the playwright usually has in a theater production. Over two meetings\u2014the first with Cooper, Walter Parkes, and Laurie MacDonald, the second with Cooper alone\u2014Mendes pitched himself to the studio. The studio soon approached Mendes with a deal to direct for the minimum salary allowed under Directors Guild of America rules\u2014$150,000. Mendes accepted, and later recalled that after taxes and his agent's commission, he only earned $38,000. In June 1998, DreamWorks confirmed that it had contracted Mendes to direct the film.\nWriting.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"I think I was writing about ... how it's becoming harder and harder to live an authentic life when we live in a world that seems to focus on appearance. ... For all the differences between now and the [1950s], in a lot of ways this is just as oppressively conformist a time. ... You see so many people who strive to live the unauthentic life and then they get there and they wonder why they're not happy. ... I didn't realize it when I sat down to write [\"American Beauty\"], but these ideas are important to me.\"\n\u2014Alan Ball, 2000\nBall was partly inspired by two encounters he had in the early 1990s. In about 1991\u201392, Ball saw a plastic bag blowing in the wind outside the World Trade Center. He watched the bag for ten minutes, saying later that it provoked an \"unexpected emotional response\". In 1992, Ball became preoccupied with the media circus that accompanied the Amy Fisher trial. Discovering a comic book telling of the scandal, he was struck by how quickly it had become commercialized. He said he \"felt like there was a real story underneath [that was] more fascinating and way more tragic\" than the story presented to the public, and attempted to turn the idea into a play. Ball produced around 40 pages, but stopped when he realized it would work better as a film. He felt that because of the visual themes, and because each character's story was \"intensely personal\", it could not be done on a stage. All the main characters appeared in this version, but Carolyn did not feature strongly; Jim and Jim instead had much larger roles.\nBall based Lester's story on aspects of his own life. Lester's re-examination of his life parallels feelings Ball had in his mid-30s; like Lester, Ball put aside his passions to work in jobs he hated for people he did not respect. Scenes in Ricky's household reflect Ball's own childhood experiences. Ball suspected his father was homosexual and used the idea to create Col. Fitts, a man who \"gave up his chance to be himself\". Ball said the script's mix of comedy and drama was not intentional, but that it came unconsciously from his own outlook on life. He said the juxtaposition produced a starker contrast, giving each trait more impact than if they appeared alone.\nThe script was written between June 1997 and February 1998. In the script that was sent to prospective actors and directors, Lester and Angela had sex; by the time of shooting, Ball had rewritten the scene to the final version. Ball initially rebuffed counsel from others that he change the script, feeling they were being puritanical; the final impetus to alter the scene came from DreamWorks's then-president Walter Parkes. He convinced Ball by indicating that in Greek mythology, the hero \"has a moment of epiphany before\u00a0... tragedy occurs\". Ball later said his anger when writing the first draft had blinded him to the idea that Lester needed to refuse sex with Angela to complete his emotional journey\u2014to achieve redemption. Jinks and Cohen asked Ball not to alter the scene right away, as they felt it would be inappropriate to make changes to the script before a director had been hired. Early drafts also included a flashback to Col. Fitts's service in the Marines, a sequence that unequivocally established his homosexual leanings. In love with another Marine, Col. Fitts sees the man die and comes to believe that he is being punished for the \"sin\" of being gay. Ball removed the sequence because it did not fit the structure of the rest of the film\u2014Col. Fitts was the only character to have a flashback\u2014and because it removed the element of surprise from Col. Fitts's later pass at Lester. Ball said he had to write it for his own benefit to know what happened to Col. Fitts, though all that remained in later drafts was subtext.\nBall remained involved throughout production; he had signed a television show development deal, so had to get permission from his producers to take a year off to be close to \"American Beauty\". Ball was on-set for rewrites and to help interpret his script for all but two days of filming. His original bookend scenes\u2014in which Ricky and Jane are prosecuted for Lester's murder after being framed by Col. Fitts\u2014were excised in post-production; the writer later felt the scenes were unnecessary, saying they were a reflection of his \"anger and cynicism\" at the time of writing &lt;templatestyles src=\"Crossreference/styles.css\" /&gt;. Ball and Mendes revised the script twice before it was sent to the actors, and twice more before the first read-through.\nThe shooting script features a scene in Angela's car in which Ricky and Jane talk about death and beauty; the scene differed from earlier versions, which set it as a \"big scene on a freeway\" in which the three witness a car crash and see a dead body. The change was a practical decision, as the production was behind schedule and they needed to cut costs. The schedule called for two days to be spent filming the crash, but only half a day was available. Ball agreed, but only if the scene could retain a line of Ricky's where he reflects on having once seen a dead homeless woman: \"When you see something like that, it's like God is looking right at you, just for a second. And if you're careful, you can look right back.\" Jane asks: \"And what do you see?\" Ricky: \"Beauty.\" Ball said, \"They wanted to cut that scene. They said it's not important. I said, 'You're out of your fucking mind. It's one of the most important scenes in the movie!'\u00a0... If any one line is the heart and soul of this movie, that is the line.\" Another scene was rewritten to accommodate the loss of the freeway sequence; set in a schoolyard, it presents a \"turning point\" for Jane in that she chooses to walk home with Ricky instead of going with Angela. By the end of filming, the script had been through ten drafts.\nCasting.\nMendes had Kevin Spacey and Bening in mind for the leads from the beginning, but DreamWorks executives were unenthusiastic. DreamWorks suggested several alternatives, including Bruce Willis, Kevin Costner, and John Travolta to play Lester (the role was also offered to Chevy Chase, but he turned it down), while Helen Hunt or Holly Hunter were proposed to play Carolyn. Mendes did not want a big star \"weighing the film down\"; he felt Spacey was the right choice based on his performances in the 1995 films \"The Usual Suspects\" and \"Seven\", and 1992's \"Glengarry Glen Ross\". Spacey was surprised; he said, \"I usually play characters who are very quick, very manipulative and smart. ... I usually wade in dark, sort of treacherous waters. This is a man living one step at a time, playing by his instincts. This is actually much closer to me, to what I am, than those other parts.\" Mendes offered Annette Bening the role of Carolyn without DreamWorks' consent; although executives were upset at Mendes, by September 1998, DreamWorks had entered negotiations with Spacey and Bening.\nSpacey loosely based Lester's early \"schlubby\" deportment on Walter Matthau. During the film, Lester's physique improves from flabby to toned; Spacey worked out during filming to improve his body, but because Mendes shot the scenes out of chronological order, Spacey varied postures to portray the stages. Before filming, Mendes and Spacey analyzed Jack Lemmon's performance in \"The Apartment\"\u00a0(1960), because Mendes wanted Spacey to emulate \"the way [Lemmon] moved, the way he looked, the way he was in that office and the way he was an ordinary man and yet a special man\". Spacey's voiceover is a throwback to \"Sunset Boulevard\"\u00a0(1950), which is also narrated in retrospect by a dead character. Mendes felt it evoked Lester's\u2014and the film's\u2014loneliness. Bening recalled women from her youth to inform her performance: \"I used to babysit constantly. You'd go to church and see how people present themselves on the outside, and then be inside their house and see the difference.\" Bening and a hair stylist collaborated to create a \"PTA president coif\" hairstyle, and Mendes and production designer Naomi Shohan researched mail-order catalogs to better establish Carolyn's environment of a \"spotless suburban manor\". To help Bening get into Carolyn's mindset, Mendes gave her music that he believed Carolyn would like. He lent Bening the Bobby Darin version of the song \"Don't Rain on My Parade\", which she enjoyed and persuaded the director to include it for a scene in which Carolyn sings in her car.\nKirsten Dunst was offered the role of Angela Hayes, but she turned it down because of the character and Lester's sexual behaviors. Sarah Michelle Gellar also declined the same role, due to scheduling conflicts with \"Buffy The Vampire Slayer\".\nFor the roles of Jane, Ricky, and Angela, DreamWorks gave Mendes \"carte blanche\". By November 1998, Thora Birch, Wes Bentley, and Mena Suvari had been cast in the parts\u2014in Birch's case, despite the fact she was 16 years old and was deemed underage for a brief nude scene, which her parents had to approve. Child labor representatives accompanied Birch's parents on set during the filming of the nude scene. Bentley overcame competition from top actors under the age of 25 to be cast. The 2009 documentary \"My Big Break\" followed Bentley, and several other young actors, before and after he landed the part. To prepare, Mendes provided Bentley with a video camera, telling the actor to film what Ricky would. Peter Gallagher and Allison Janney were cast (as Buddy Kane and Barbara Fitts) after filming began in December 1998. Mendes gave Janney a book of paintings by Edvard Munch. He told her, \"Your character is in there somewhere.\" Mendes cut much of Barbara's dialogue, including conversations between Colonel Frank Fitts and her, as he felt that what needed to be said about the pair\u2014their humanity and vulnerability\u2014was conveyed successfully through their shared moments of silence. Chris Cooper plays Colonel Frank Fitts, Scott Bakula plays Jim Olmeyer, and Sam Robards plays Jim Berkley. Jim and Jim were deliberately depicted as the most normal, happy\u2014and boring\u2014couple in the film. Ball's inspiration for the characters came from a thought he had after seeing a \"bland, boring, heterosexual couple\" who wore matching clothes: \"I can't wait for the time when a gay couple can be just as boring.\" Ball also included aspects of a gay couple he knew who had the same forename.\nMendes insisted on two weeks of cast rehearsals, although the sessions were not as formal as he was used to in the theater, and the actors could not be present at every one. Several improvisations and suggestions by the actors were incorporated into the script. An early scene showing the Burnhams leaving home for work was inserted later on to show the low point that Carolyn and Lester's relationship had reached. Spacey and Bening worked to create a sense of the love that Lester and Carolyn once had for one another; for example, the scene in which Lester almost seduces Carolyn after the pair argues over Lester's buying a car was originally \"strictly contentious\".\nFilming.\nPrincipal photography lasted about 50 days, from December 14, 1998 to February 1999. \"American Beauty\" was filmed on soundstages at the Warner Bros. backlot in Burbank, California, and at Hancock Park and Brentwood in Los Angeles. The aerial shots at the beginning and end of the film were captured in Sacramento, California, and many of the school scenes were shot at South High School in Torrance, California; several extras in the gym crowd were South High students. The film is set in an upper middle-class neighborhood in an unidentified American town. Production designer Naomi Shohan likened the locale to Evanston, Illinois, but said, \"it's not about a place, it's about an archetype... The milieu was pretty much Anywhere, USA\u2014upwardly mobile suburbia.\" The intent was for the setting to reflect the characters, who are also archetypes. Shohan said, \"All of them are very strained, and their lives are constructs.\" The Burnhams' household was designed as the reverse of the Fitts'\u2014the former a pristine ideal, but graceless and lacking in \"inner balance\", leading to Carolyn's desire to at least give it the appearance of a \"perfect all-American household\"; the Fitts's home is depicted in \"exaggerated darkness [and] symmetry\".\nThe production selected two adjacent properties on the Warner backlot's \"Blondie Street\" for the Burnham and Fitts's homes. The crew rebuilt the houses to incorporate false rooms that established lines of sight\u2014between Ricky and Jane's bedroom windows, and between Ricky's bedroom and Lester's garage. The garage windows were designed specifically to obtain the crucial shot toward the end of the film in which Col. Fitts\u2014watching from Ricky's bedroom\u2014mistakenly assumes that Lester is paying Ricky for sex. Mendes made sure to establish the line of sight early on in the film to make the audience feel a sense of familiarity with the shot. The house interiors were filmed on the backlot, on location, and on soundstages when overhead shots were needed. The inside of the Burnhams' home was shot at a house close to Interstate 405 and Sunset Boulevard in Los Angeles; the inside of the Fitts's home was shot in the city's Hancock Park neighborhood. Ricky's bedroom was designed to be cell-like to suggest his \"monkish\" personality, while at the same time blending with the high-tech equipment to reflect his voyeuristic side. The production deliberately minimized the use of red, as it was an important thematic signature elsewhere. The Burnhams' home uses cool blues, while the Fitts's is kept in a \"depressed military palette\".\nMendes's dominating visual style was deliberate and composed, with a minimalist design that provided \"a sparse, almost surreal feeling\u2014a bright, crisp, hard edged, near Magritte-like take on American suburbia\"; Mendes constantly directed his set dressers to empty the frame. He made Lester's fantasy scenes \"more fluid and graceful\", and Mendes made minimal use of steadicams, feeling that stable shots generated more tension. For example, when Mendes used a slow push in to the Burnhams' dinner table, he held the shot because his training as a theater director taught him the importance of putting distance between the characters. He wanted to keep the tension in the scene, so he only cut away when Jane left the table. Mendes used a hand-held camera for the scene in which Col. Fitts beats Ricky. Mendes said the camera provided the scene with a \"kinetic\u00a0... off-balance energy\". He also went hand-held for the excerpts of Ricky's camcorder footage. Mendes took a long time to get the quality of Ricky's footage to the level he wanted. For the plastic-bag footage, Mendes used wind machines to move the bag in the air. The scene took four takes; two by the second unit did not satisfy Mendes, so he shot the scene himself. He felt his first take lacked grace, but for the last attempt, he changed the location to the front of a brick wall and added leaves on the ground. Mendes was satisfied by the way the wall gave definition to the outline of the bag.\nMendes avoided using close-ups, believing the technique was overused. He also mentioned Spielberg's advice to imagine an audience silhouetted at the bottom of the camera monitor, to keep in mind that it was being shot for display on a screen. Spielberg\u2014who visited the set a few times\u2014also advised Mendes not to worry about costs if he had a \"great idea\" toward the end of a long working day. Mendes said, \"That happened three or four times, and they are all in the movie.\" Despite Spielberg's support, DreamWorks and Mendes fought constantly over the schedule and budget, although the studio interfered little with the film's content. Spacey, Bening and Hall worked for significantly less than their usual rates. \"American Beauty\" cost DreamWorks $15\u00a0million to produce, slightly above their projected sum. Mendes was so dissatisfied with his first three days' filming that he obtained permission from DreamWorks to reshoot the scenes. He said, \"I started with a wrong scene, actually, a comedy scene. And the actors played it way too big:\u00a0... it was badly shot, my fault, badly composed, my fault, bad costumes, my fault\u00a0...; and everybody was doing what I was asking. It was all my fault.\" Aware that he was a novice, Mendes drew on the experience of Hall: \"I made a very conscious decision early on, if I didn't understand something technically, to say, without embarrassment, 'I don't understand what you're talking about, please explain it.'\"\nMendes encouraged some improvisation; for example, when Lester masturbates in bed beside Carolyn, the director asked Spacey to improvise several euphemisms for the act in each take. Mendes said, \"I wanted that not just because it was funny\u00a0... but because I didn't want it to seem rehearsed. I wanted it to seem like he was blurting it out of his mouth without thinking. [Spacey] is so in control\u2014I wanted him to break through.\" Spacey obliged, eventually coming up with 35 phrases, but Bening could not always keep a straight face, which meant the scene had to be shot ten times. The production used small amounts of computer-generated imagery. Most of the rose petals in Lester's fantasies were added in post-production, although some were real and had the wires holding them digitally removed. When Lester fantasizes about Angela in a rose-petal bath, the steam was real, save for in the overhead shot. To position the camera, a hole had to be cut in the ceiling, through which the steam escaped; it was instead added digitally.\nEditing.\n\"American Beauty\" was edited by Christopher Greenbury and Tariq Anwar; Greenbury began in the position, but had to leave halfway through post-production because of a scheduling conflict with \"Me, Myself &amp; Irene\"\u00a0(2000). Mendes and an assistant edited the film for ten days between the appointments. Mendes realized during editing that the film was different from the one he had envisioned. He believed he had been making a \"much more whimsical,\u00a0... kaleidoscopic\" film than what came together in the edit suite. Instead, Mendes was drawn to the emotion and darkness; he began to use the score and shots he had intended to discard to craft the film along these lines. In total, he cut about 30 minutes from his original edit.\nThe opening included a dream in which Lester imagines himself flying above the town. Mendes spent two days filming Spacey against bluescreen, but removed the sequence as he believed it to be too whimsical\u2014\"like a Coen brothers movie\"\u2014and therefore inappropriate for the tone he was trying to set. The opening in the final cut reused a scene from the middle of the film where Jane tells Ricky to kill her father. This scene was to be the revelation to the audience that the pair was not responsible for Lester's death, as the way it was scored and acted made it clear that Jane's request was not serious. However, in the portion he used in the opening\u2014and when the full scene plays out later\u2014Mendes used the score and a reaction shot of Ricky to leave a lingering ambiguity as to his guilt. The subsequent shot\u2014an aerial view of the neighborhood\u2014was originally intended as the plate shot for the bluescreen effects in the dream sequence.\nMendes spent more time recutting the first ten minutes than the rest of the film taken together. He trialed several versions of the opening; the first edit included bookend scenes in which Jane and Ricky are convicted of Lester's murder, but Mendes excised these in the last week of editing because he felt they made the film lose its mystery, and because they did not fit with the theme of redemption that had emerged during production. Mendes believed the trial drew focus away from the characters and turned the film \"into an episode of \"NYPD Blue\"\". Instead, he wanted the ending to be \"a poetic mixture of dream and memory and narrative resolution\". When Ball first saw a completed edit, it was a version with truncated versions of these scenes. He felt that they were so short that they \"didn't really register\". Mendes and he argued, but Ball was more accepting after Mendes cut the sequences completely; Ball felt that without the scenes, the film was more optimistic and had evolved into something that \"for all its darkness had a really romantic heart\".\nCinematography.\nConrad Hall was not the first choice for director of photography; Mendes believed he was \"too old and too experienced\" to want the job, and he had been told that Hall was difficult to work with. Instead, Mendes asked Frederick Elmes, who turned the job down because he did not like the script. Hall was recommended to Mendes by Tom Cruise, because of Hall's work on \"Without Limits\"\u00a0(1998), which Cruise had executive produced. Mendes was directing Cruise's then-wife Nicole Kidman in the play \"The Blue Room\" during preproduction on \"American Beauty\", and had already storyboarded the whole film. Hall was involved for one month during preproduction; his ideas for lighting the film began with his first reading of the script, and further passes allowed him to refine his approach before meeting Mendes. Hall was initially concerned that audiences would not like the characters; he only felt able to identify with them during cast rehearsals, which gave him fresh ideas on his approach to the visuals.\nHall's approach was to create peaceful compositions that evoked classicism, to contrast with the turbulent on-screen events and allow audiences to take in the action. Hall and Mendes first discussed the intended mood of a scene, but he was allowed to light the shot in any way he felt necessary. In most cases, Hall first lit the scene's subject by \"painting in\" the blacks and whites, before adding fill light, which he reflected from beadboard or white card on the ceiling. This approach gave Hall more control over the shadows while keeping the fill light unobtrusive and the dark areas free of spill. Hall shot \"American Beauty\" in a 2.39:1 aspect ratio in the Super 35 format, primarily using Kodak Vision 500T 5279 35\u00a0mm film stock. He used Super 35 partly because its larger scope allowed him to capture elements such as the corners of the petal-filled pool in its overhead shot, creating a frame around Angela within. He shot the whole film at the same T-stop (T1.9); given his preference for shooting that wide, Hall favored high-speed stocks to allow for more subtle lighting effects.\nHall used Panavision Platinum cameras with the company's Primo series of prime and zoom lenses. Hall employed Kodak Vision 200T 5274 and EXR 100T 5248 stock for scenes with daylight effects. He had difficulty adjusting to Kodak's newly introduced Vision release print stock, which, combined with his contrast-heavy lighting style, created a look with too much contrast. Hall contacted Kodak, who sent him a batch of 5279 that was five percent lower in contrast. Hall used a 1/8th strength Tiffen Black ProMist filter for almost every scene, which he said in retrospect may not have been the best choice, as the optical steps required to blow Super 35 up for its anamorphic release print led to a slight amount of degradation; therefore, the diffusion from the filter was not required. When he saw the film in a theater, Hall felt that the image was slightly unclear and that had he not used the filter, the diffusion from the Super 35\u2013anamorphic conversion would have generated an image closer to what he originally intended.\nA shot where Lester and Ricky share a cannabis joint behind a building came from a misunderstanding between Hall and Mendes. Mendes asked Hall to prepare the shot in his absence; Hall assumed the characters would look for privacy, so he placed them in a narrow passage between a truck and the building, intending to light from the top of the truck. When Mendes returned, he explained that the characters did not care if they were seen. He removed the truck and Hall had to rethink the lighting; he lit it from the left, with a large light crossing the actors, and with a soft light behind the camera. Hall felt the consequent wide shot \"worked perfectly for the tone of the scene\". Hall made sure to keep rain, or the suggestion of it, in every shot near the end of the film. In one shot during Lester's encounter with Angela at the Burnhams' home, Hall created rain effects on the foreground cross lights; in another, he partly lit the pair through French windows to which he had added material to make the rain run slower, intensifying the light (although the strength of the outside light was unrealistic for a night scene, Hall felt it justified because of the strong contrasts it produced). For the close-ups when Lester and Angela move to the couch, Hall tried to keep rain in the frame, lighting through the window onto the ceiling behind Lester. He also used rain boxes to produce rain patterns where he wanted without lighting the entire room.\nMusic.\nThomas Newman's score was recorded in Santa Monica, California. He used mainly percussion instruments to create the mood and rhythm, the inspiration for which was provided by Mendes. Newman \"favored pulse, rhythm, and color over melody\", making for a more minimalist score than he had previously created. He built each cue around \"small, endlessly repeating phrases\"\u2014often, the only variety through a \"thinning of the texture for eight bars\". The percussion instruments included tablas, bongos, cymbals, piano, xylophones, and marimbas; also featured were guitars, flute, and world music instruments. Newman also used electronic music and on \"quirkier\" tracks employed more unorthodox methods, such as tapping metal mixing bowls with a finger and using a detuned mandolin. Newman believed the score helped move the film along without disturbing the \"moral ambiguity\" of the script: \"It was a real delicate balancing act in terms of what music worked to preserve [that].\"\nThe soundtrack features songs by Newman, Bobby Darin, the Who (\"The Seeker\"), Free, Eels, the Guess Who, Bill Withers, Betty Carter, Peggy Lee, the Folk Implosion, Gomez, and Bob Dylan, as well as two cover versions\u2014the Beatles' \"Because\", performed by Elliott Smith, and Neil Young's \"Don't Let It Bring You Down\", performed by Annie Lennox. Produced by the film's music supervisor Chris Douridas, an abridged soundtrack album was released on October 5, 1999, and was nominated for a Grammy Award for Best Soundtrack Album. An album featuring 19 tracks from Newman's score was released on January 11, 2000, and won the Grammy Award for Best Score Soundtrack Album. \"Filmmaker\" considered the score one of Newman's best, saying it \"[enabled] the film's transcendentalist aspirations\". In 2006, \"Filmmaker\" chose the score as one of twenty essential soundtracks it believed spoke to the \"complex and innovative relationships between music and screen storytelling\".\nRelease.\nPublicity.\nDreamWorks contracted Amazon.com to create the official website, marking the first time that Amazon had created a special section devoted to a feature film. The website included an overview, a photo gallery, cast and crew filmographies, and exclusive interviews with Spacey and Bening. The film's tagline\u2014\"look closer\"\u2014originally came from a cutting pasted on Lester's workplace cubicle by the set dresser. DreamWorks ran parallel marketing campaigns and trailers\u2014one aimed at adults, the other at teenagers. Both trailers ended with the poster image of a girl holding a rose.\nReviewing the posters of several films of the year, David Hochman of \"Entertainment Weekly\" rated \"American Beauty\"'s highly, saying it evoked the tagline; he said, \"You return to the poster again and again, thinking, this time you're gonna find something.\" DreamWorks did not want to test screen the film; according to Mendes, the studio was pleased with it, but he insisted on one where he could question the audience afterward. The studio reluctantly agreed and showed the film to a young audience in San Jose, California. Mendes claimed the screening went very well.\nTheatrical run.\nThe film had its world premiere on September 8, 1999, at Grauman's Egyptian Theatre in Los Angeles. Three days later, the film appeared at the Toronto International Film Festival. With the filmmakers and cast in attendance, it screened at several American universities, including the University of California at Berkeley, New York University, the University of California at Los Angeles, the University of Texas at Austin, the University of North Carolina at Chapel Hill, and Northwestern University.\nOn September 15, 1999, \"American Beauty\" opened to the public in limited release at three theaters in Los Angeles and three in New York. More theaters were added during the limited run, and on October 1, the film officially entered wide release by screening in 706\u00a0theaters across North America. The film grossed $8,188,587 over the weekend, ranking third at the box office behind \"Double Jeopardy\" and \"Three Kings\". Audiences polled by the market research firm CinemaScore gave \"American Beauty\" a \"B+\" grade on average. The theater count hit a high of 1,528 at the end of the month, before a gradual decline. Following \"American Beauty\"'s wins at the 57th Golden Globe Awards, DreamWorks re-expanded the theater presence from a low of 7 in mid-February, to a high of 1,990 in March. The film ended its North American theatrical run on June 4, 2000, having grossed $130.1\u00a0million.\n\"American Beauty\" had its European premiere at the London Film Festival on November 18, 1999; in January 2000, it began to screen in various territories outside North America. It debuted in Israel to \"potent\" returns, and limited releases in Germany, Italy, Austria, Switzerland, the Netherlands and Finland followed on January 21. After January 28 opening weekends in Australia, the United Kingdom, Spain and Norway, \"American Beauty\" had earned $7\u00a0million in 12\u00a0countries for a total of $12.1\u00a0million outside North America. On February 4, \"American Beauty\" debuted in France and Belgium. Expanding to 303\u00a0theaters in the United Kingdom, the film ranked first at the box office with $1.7\u00a0million. There, it surpassed \"Alien Resurrection\" for scoring the highest opening weekend for an 18 certificate film. This record would be overtaken by \"Snatch\" later that year. On the weekend of February 18\u2014following \"American Beauty\"'s eight nominations for the 72nd Academy Awards\u2014the film grossed $11.7\u00a0million from 21\u00a0territories, for a total of $65.4\u00a0million outside North America. The film had \"dazzling\" debuts in Hungary, Denmark, the Czech Republic, Slovakia, and New Zealand.\nAs of February 18, the most successful territories were the United Kingdom ($15.2\u00a0million), Italy ($10.8\u00a0million), Germany ($10.5\u00a0million), Australia ($6\u00a0million), and France ($5.3\u00a0million). The Academy Award nominations meant strong performances continued across the board; the following weekend, \"American Beauty\" grossed $10.9\u00a0million in 27\u00a0countries, with strong debuts in Brazil, Mexico, and South Korea. Other high spots included robust returns in Argentina, Greece, and Turkey. On the weekend of March 3, 2000, \"American Beauty\" debuted strongly in Hong Kong, Taiwan, and Singapore, markets traditionally \"not receptive to this kind of upscale fare\". The impressive South Korean performance continued, with a return of $1.2\u00a0million after nine days. In total, \"American Beauty\" grossed $130.1\u00a0million in North America and $226.2\u00a0million internationally, for $356.3\u00a0million worldwide.\nHome media.\n\"American Beauty\" was released on VHS on May 9, 2000, and on DVD with the DTS format on October 24, 2000. The film also received a Japan-only LaserDisc release on October 27, 2000, making it one of the last films to ever be released on the format. The last North American LaserDisc release had occurred earlier in October 2000.\nBefore the North American rental release on May 9, Blockbuster Video wanted to purchase hundreds of thousands of extra copies for its \"guaranteed title\" range, whereby anyone who wanted to rent the film would be guaranteed a copy. Blockbuster and DreamWorks could not agree on a profit-sharing deal, so Blockbuster ordered two-thirds the number of copies it originally intended. DreamWorks made around one million copies available for rental; Blockbuster's share would usually have been about 400,000 of these. Some Blockbuster stores only displayed 60 copies, and others did not display the film at all, forcing customers to ask for it. The strategy required staff to read a statement to customers explaining the situation; Blockbuster claimed it was only \"[monitoring] customer demand\" due to the reduced availability. Blockbuster's strategy leaked before May 9, leading to a 30 percent order increase from other retailers.\nIn its first week of rental release, \"American Beauty\" made $6.8\u00a0million. This return was lower than would have been expected had DreamWorks and Blockbuster reached an agreement. In the same year, \"The Sixth Sense\" made $22\u00a0million, while \"Fight Club\" made $8.1\u00a0million, though the latter's North American theatrical performance was just 29 percent that of \"American Beauty\". Blockbuster's strategy also affected rental fees; \"American Beauty\" averaged $3.12, compared with $3.40 for films that Blockbuster fully promoted. Only 53 percent of the film's rentals were from large outlets in the first week, compared with the usual 65 percent.\nThe DVD release included a behind-the-scenes featurette, film audio commentary from Mendes and Ball, and a storyboard presentation with discussion from Mendes and Hall. In the film commentary, Mendes refers to deleted scenes he intended to include in the release. However, these scenes are not on the DVD, as he changed his mind after recording the commentary; Mendes felt that to show scenes he previously chose not to use would detract from the film's integrity.\nIn February 2006, Viacom (now known as Paramount Skydance) acquired the rights to \"American Beauty\" and all other 58 live-action films DreamWorks had released since 1997, following their billion-dollar acquisition of the company's live-action assets. On September 21, 2010, Paramount Home Entertainment released \"American Beauty\" on Blu-ray, as part of Paramount's Sapphire Series. All the extras from the DVD release were present, with the theatrical trailers upgraded to HD. In March 2021, Paramount Home Entertainment also released \"American Beauty\" on a ten film Blu-ray set. The set featured nine other Paramount-owned films which won Academy Award for Best Picture, including \"Gladiator\" (another title acquired from DreamWorks in 2006) and \"The English Patient\" (one of 700 titles Paramount acquired from Miramax in 2020). The film became available on Paramount's subscription streaming service Paramount+, which launched in March 2021, in addition to being made available on Paramount's free streaming service Pluto TV.\nReception and legacy.\nInitial.\n\"American Beauty\" received overwhelming praise upon release, chiefly for Spacey, Mendes and Ball. \"Variety\" reported that \"no other 1999 movie has benefited from such universal raves.\" It was the best-received title at the Toronto International Film Festival (TIFF), where it won the People's Choice award after a ballot of the festival's audiences. TIFF's director, Piers Handling, said, \"\"American Beauty\" was the buzz of the festival, the film most talked-about.\"\nReview aggregator Rotten Tomatoes reports that 87% of 193 critics gave the film a positive review. The website's critics' consensus reads: \"Flawlessly cast and brimming with dark, acid wit, \"American Beauty\" is a smart, provocative high point of late '90s mainstream Hollywood film.\" According to Metacritic, which assigned a weighted average score of 84 out of 100 based on 34 critics, the film received \"universal acclaim\".\nRoger Ebert of the \"Chicago Sun-Times\", who awarded the film four out of four stars, singled Spacey out for successfully portraying a man who \"does reckless and foolish things [but who] doesn't deceive himself\". Kevin Jackson of \"Sight &amp; Sound\" said Spacey impressed in ways distinct from his previous performances, the most satisfying aspect being his portrayal of \"both sap and hero\".\nRetrospective.\nA few months after the release of \"American Beauty\", reports of a backlash appeared in the American press. In the years since the film's release, its critical regard has waned. A significant factor was its themes being seen as trivial after the September 11 attacks and the Great Recession of late 2007 to 2009.\nIn 2005, \"Premiere\" named \"American Beauty\" as one of 20 \"most overrated movies of all time.\" Mendes accepted the inevitability of the critical reappraisal, saying in 2008, \"I thought some of it was entirely justified\u2014it was a little overpraised at the time.\"\nIn 2017, allegations of sexual assault and misconduct against actor Kevin Spacey surfaced at the height of the MeToo movement, including by children who were underage at the time of the allegations. This led many critics and one of Spacey's accusers, actor Anthony Rapp, to find uncomfortable parallels between Spacey and Lester Burnham, his character in \"American Beauty\".\nIn 2019, on the twentieth anniversary of the film's release, \"The Huffington Post\"'s Matthew Jacobs wrote that \"the film's reputation has tumbled precipitously,\" adding, \"Plenty of classics undergo cultural reappraisals [...] but few have turned into such a widespread punchline.\"\nSimilar to Matthew Jacobs's assessment, Stephanie Zacharek for \"Time\" wrote, \"In 2019, beating up on Sam Mendes' multi-Oscar-winning \"American Beauty\" [...] is so painfully easy that it seems unfair. The Best Picture winner has fallen largely out of fashion; it rarely appears on critics' lists of favorite movies, and its memory seems to have faded for most moviegoers, too.\" Zacharek concluded, \"\"American Beauty\" is a movie about a privileged white guy who feels bad about himself and tries to rectify that by exploding his life\u2014only to lose it all in the end. It's about a man who thought he had control, but didn't\u2014and who can't, at the very least, relate to that? In the context of his own crisis of self-absorption, Lester Burnham couldn't see the real collision course looming ahead, a future of lost jobs and foreclosures, of madhouse doublespeak issuing from the mouths of people whose job it is to lead us, of wars that can't be won and thus keep being fought. Maybe it takes a look back at a ridiculous movie to show us how much we've really lost. Whatever Ball's 'authentic life' really is, you can bet it's not being lived on Instagram.\"\nNonetheless, other critics still defend the artistic value of the film. In 2014, on the occasion of the fifteenth anniversary of the movie, \"Entertainment Weekly's\" Ashley Fetters stated that \"American Beauty\" stands as \"a classic, if not a masterpiece.\"\nIn popular culture.\nThe film was spoofed by the animated sitcom \"Family Guy\", the 2001 Todd Solondz film \"Storytelling\", the teen movie spoof \"Not Another Teen Movie\", the 2011 animated cartoon series \"The Amazing World of Gumball\" and in the 2005 DreamWorks animated film \"Madagascar\". In the highly publicized disappearance of North Carolina resident Leah Roberts, Roberts went to a theater to watch \"American Beauty\" on March 13, 2000, shortly before vanishing. A ticket stub from a screening at Bellis Fair Mall in Bellingham, Washington was found among her possessions in a Jeep which had been abandoned in a remote area.\nAccolades.\n\"American Beauty\" was not considered an immediate favorite to dominate the American awards season. Several other contenders opened at the end of 1999, and US critics spread their honors among them when compiling their end-of-year lists. The Chicago Film Critics Association and the Broadcast Film Critics Association named the film the best of 1999, but while the New York Film Critics Circle, the National Society of Film Critics and the Los Angeles Film Critics Association recognized \"American Beauty\", they gave their top awards to other films. By the end of the year, reports of a critical backlash suggested \"American Beauty\" was the underdog in the race for Best Picture; however, at the Golden Globe Awards in January 2000, \"American Beauty\" won Best Film, Best Director and Best Screenplay.\nAs the nominations for the 72nd Academy Awards approached, a frontrunner had not emerged. DreamWorks had launched a major campaign for \"American Beauty\" five weeks before ballots were due to be sent to the 5,600 Academy Award voters. Its campaign combined traditional advertising and publicity with more focused strategies. Although direct mail campaigning was prohibited, DreamWorks reached voters by promoting the film in \"casual, comfortable settings\" in voters' communities. The studio's candidate for Best Picture the previous year, \"Saving Private Ryan\", lost to \"Shakespeare in Love\", so the studio took a new approach by hiring outsiders to provide input for the campaign. It hired three veteran consultants, who told the studio to \"think small\".\nNancy Willen encouraged DreamWorks to produce a special about the making of \"American Beauty\", to set up displays of the film in the communities' bookstores, and to arrange a question-and-answer session with Mendes for the British Academy of Film and Television Arts. Dale Olson advised the studio to advertise in free publications that circulated in Beverly Hills\u2014home to many voters\u2014in addition to major newspapers. Olson arranged to screen \"American Beauty\" to about 1,000 members of the Actors Fund of America, as many participating actors were also voters. Bruce Feldman took writer Alan Ball to the Santa Barbara International Film Festival, where Ball attended a private dinner in honor of Anthony Hopkins, meeting several voters who were in attendance.\nIn February 2000, \"American Beauty\" was nominated for eight Academy Awards; its closest rivals, \"The Cider House Rules\" and \"The Insider\", received seven nominations each. In March 2000, the major industry labor organizations all awarded their top honors to \"American Beauty\"; perceptions had shifted\u2014the film was now the favorite to dominate the Academy Awards. \"American Beauty\"'s closest rival for Best Picture was still \"The Cider House Rules\", from Miramax. Both studios mounted aggressive campaigns; DreamWorks bought 38 percent more advertising space in \"Variety\" than Miramax. On March 26, 2000, \"American Beauty\" won five Academy Awards: Best Picture, Best Director, Best Actor (Spacey), Best Original Screenplay and Best Cinematography.\nAt the 53rd British Academy Film Awards, \"American Beauty\" won six of the 14 awards for which it was nominated: Best Film, Best Actor, Best Actress (Bening), Best Cinematography, Best Film Music and Best Editing.\nIn March 2000, the Publicists Guild of America recognized DreamWorks for the best film publicity campaign.\nMTV Movie Awards controversy.\nIn April 2000, controversy arose when the 2000 MTV Movie Awards initially nominated \"American Beauty\" for their annual award for Best Kiss between Kevin Spacey as Lester Burnham and Mena Suvari as Angela Hayes. DreamWorks rejected MTV's request for a clip, stating that they did not want to glorify the \"inappropriate\" kiss between the characters, a 42-year-old man and a 16-year-old girl. MTV rescinded the film's nomination afterward.\nRetrospective accolades.\nIn 2006, the Writers Guild of America ranked the screenplay number 38 on its list of the 101\u00a0greatest screenplays.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nFootnotes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "44123", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=44123", "title": "Hubble Constant", "text": ""}
{"id": "44125", "revid": "5846", "url": "https://en.wikipedia.org/wiki?curid=44125", "title": "Gyroscope", "text": "Device for measuring or maintaining the orientation and angular velocity\nA gyroscope (from Ancient Greek \u03b3\u1fe6\u03c1\u03bf\u03c2 \"g\u0177ros\" 'round' and \u03c3\u03ba\u03bf\u03c0\u03ad\u03c9 \"skop\u00e9\u014d\" 'to look') is a device used for measuring or maintaining orientation and angular velocity. It is a spinning wheel or disc in which the axis of rotation (spin axis) is free to assume any orientation by itself. When rotating, the orientation of this axis is unaffected by tilting or rotation of the mounting, due to the conservation of angular momentum.\nGyroscopes based on other operating principles also exist, such as the microchip-packaged MEMS gyroscopes found in electronic devices (sometimes called gyrometers), solid-state ring lasers, fibre optic gyroscopes, and the extremely sensitive quantum gyroscope.\nApplications of gyroscopes include inertial navigation systems, such as in the Hubble Space Telescope, or inside the steel hull of a submerged submarine. Due to their precision, gyroscopes are also used in gyrotheodolites to maintain direction in tunnel mining. Gyroscopes can be used to construct gyrocompasses, which complement or replace magnetic compasses (in ships, aircraft and spacecraft, vehicles in general), to assist in stability (bicycles, motorcycles, and ships) or be used as part of an inertial guidance system.\nMEMS (Micro-Electro-Mechanical System) gyroscopes are popular in some consumer electronics, such as smartphones.\nDescription and diagram.\nA gyroscope is an instrument, consisting of a wheel mounted into two or three gimbals providing pivoted supports, for allowing the wheel to rotate about a single axis. A set of three gimbals, one mounted on the other with orthogonal pivot axes, may be used to allow a wheel mounted on the innermost gimbal to have an orientation remaining independent of the orientation, in space, of its support.\nIn the case of a gyroscope with two gimbals, the outer gimbal, which is the gyroscope frame, is mounted so as to pivot about an axis in its own plane determined by the support. This outer gimbal possesses one degree of rotational freedom and its axis possesses none. The second (inner) gimbal is mounted in the gyroscope frame (outer gimbal) so as to pivot about an axis in its own plane that is always perpendicular to the pivotal axis of the gyroscope frame (outer gimbal). This inner gimbal has two degrees of rotational freedom.\nThe axle of the spinning wheel (the rotor) defines the spin axis. The rotor is constrained to spin about an axis, which is always perpendicular to the axis of the inner gimbal. So the rotor possesses three degrees of rotational freedom and its axis possesses two.\nThe rotor responds to a force applied to the input axis by a reaction force to the output axis.\nA gyroscope flywheel will roll or resist about the output axis depending upon whether the output gimbals are of a free or fixed configuration. An example of some free-output-gimbal devices is the attitude control gyroscopes used to sense or measure the pitch, roll and yaw attitude angles in a spacecraft or aircraft.\nThe centre of gravity of the rotor can be in a fixed position. The rotor simultaneously spins about one axis and is capable of oscillating about the two other axes, and it is free to turn in any direction about the fixed point (except for its inherent resistance caused by rotor spin). Some gyroscopes have mechanical equivalents substituted for one or more of the elements. For example, the spinning rotor may be suspended in a fluid, instead of being mounted in gimbals. A control moment gyroscope (CMG) is an example of a fixed-output-gimbal device that is used on spacecraft to hold or maintain a desired attitude angle or pointing direction using the gyroscopic resistance force.\nIn some special cases, the outer gimbal (or its equivalent) may be omitted so that the rotor has only two degrees of freedom. In other cases, the centre of gravity of the rotor may be offset from the axis of oscillation, and thus the centre of gravity of the rotor and the centre of suspension of the rotor may not coincide.\nHistory.\nEarly similar devices.\nEssentially, a gyroscope is a top combined with a pair of gimbals. Tops were invented in many different civilizations, including classical Greece, Rome, and China. Most of these were not utilized as instruments.\nThe first known apparatus similar to a gyroscope (the \"Whirling Speculum\" or \"Serson's Speculum\") was invented by John Serson in 1743. It was used as a level, to locate the horizon in foggy or misty conditions.\nThe first instrument used more like an actual gyroscope was made by Johann Bohnenberger of Germany, who first wrote about it in 1817. At first he called it the \"Machine\". Bohnenberger's machine was based on a rotating massive sphere. In 1832, American Walter R. Johnson developed a similar device that was based on a rotating disc. The French mathematician Pierre-Simon Laplace, working at the \u00c9cole Polytechnique in Paris, recommended the machine for use as a teaching aid, and thus it came to the attention of L\u00e9on Foucault.\nFoucault's gyroscope.\nIn 1852, Foucault used it in an experiment demonstrating the rotation of the Earth.\nIt was Foucault who gave the device its modern name, in an experiment to see (Greek \"skopeein\" 'to see') the Earth's rotation (Greek \"gyros\" 'circle, rotation'), which was visible in the 8 to 10 minutes before friction slowed the spinning rotor.\nCommercialization.\nIn the 1860s, the advent of electric motors made it possible for a gyroscope to spin indefinitely; this led to the first prototype heading indicators, and a rather more complicated device, the gyrocompass. The first functional gyrocompass was patented in 1904 by German inventor Hermann Ansch\u00fctz-Kaempfe. American Elmer Sperry followed with his own design later that year, and other nations soon realized the military importance of the invention\u2014in an age in which naval prowess was the most significant measure of military power\u2014and created their own gyroscope industries. The Sperry Gyroscope Company quickly expanded to provide aircraft and naval stabilizers as well, and other gyroscope developers followed suit.\nCirca 1911 the L. T. Hurst Mfg Co of Indianapolis started producing the \"Hurst gyroscope\" a toy gyroscope with a pull string and pedestal. Manufacture was at some point switched to Chandler Mfg Co (still branded Hurst). The product was later renamed to a \"Chandler gyroscope\", presumably because Chandler Mfg Co. took over rights to the gyroscope. Chandler continued to produce the toy until the company was purchased by TEDCO Inc. in 1982. The gyroscope is still produced by TEDCO today.\nIn the first several decades of the 20th century, other inventors attempted (unsuccessfully) to use gyroscopes as the basis for early black box navigational systems by creating a stable platform from which accurate acceleration measurements could be performed (in order to bypass the need for star sightings to calculate position). Similar principles were later employed in the development of inertial navigation systems for ballistic missiles.\nDuring World War II, the gyroscope became the prime component for aircraft and anti-aircraft gun sights. After the war, the race to miniaturize gyroscopes for guided missiles and weapons navigation systems resulted in the development and manufacturing of so-called midget gyroscopes that weighed less than and had a diameter of approximately . Some of these miniaturized gyroscopes could reach a speed of 24,000 revolutions per minute in less than 10 seconds.\nGyroscopes continue to be an engineering challenge. For example, the axle bearings have to be extremely accurate. A small amount of friction is deliberately introduced to the bearings, since otherwise an accuracy of better than formula_1 of an inch (2.5\u00a0nm) would be required.\nThree-axis MEMS-based gyroscopes are also used in portable electronic devices such as tablets, smartphones, and smartwatches. This adds to the 3-axis acceleration sensing ability available on previous generations of devices. Together these sensors provide 6 component motion sensing; accelerometers for X, Y, and Z movement, and gyroscopes for measuring the extent and rate of rotation in space (roll, pitch and yaw). Some devices additionally incorporate a magnetometer to provide absolute angular measurements relative to the Earth's magnetic field. Newer MEMS-based inertial measurement units incorporate up to all nine axes of sensing in a single integrated circuit package, providing inexpensive and widely available motion sensing.\nGyroscopic principles.\nAll spinning objects have gyroscopic properties. The main properties that an object can experience in any gyroscopic motion are rigidity in space and precession.\nRigidity in space.\nRigidity in space describes the principle that a gyroscope remains in the fixed position on the plane in which it is spinning, unaffected by the Earth's rotation. For example, a bike wheel. Early forms of gyroscope (not then known by the name) were used to demonstrate the principle.\nPrecession.\nA simple case of precession, also known as steady precession, can be described by the following relation to Moment:\nformula_2\nwhere formula_3 represents precession, formula_4 is represented by spin, formula_5 is the nutation angle, and formula_6 represents inertia along its respective axis. This relation is only valid with the Moment along the Y and Z axes are equal to 0.\nThe equation can be further reduced noting that the angular velocity along the z-axis is equal to the sum of the Precession and the Spin: formula_7, Where formula_8 represents the angular velocity along the z axis.\nformula_9\nor\nformula_10\nGyroscopic precession is torque induced. It is the rate of change of the angular momentum that is produced by the applied torque. Precession produces counterintuitive dynamic results such as a spinning top not falling over. Precession is used in aerospace applications for sensing changes of attitude and direction.\nContemporary uses.\nSteadicam.\nA Steadicam rig was employed during the filming of the 1983 film \"Return of the Jedi\", in conjunction with two gyroscopes for extra stabilization, to film the background plates for the speeder bike chase. Steadicam inventor Garrett Brown operated the shot, walking through a redwood forest, running the camera at one frame per second. When projected at 24 frames per second, it gave the impression of flying through the air at perilous speeds.\nHeading indicator.\nThe heading indicator or directional gyro has an axis of rotation that is set horizontally, pointing north. Unlike a magnetic compass, it does not seek north. When being used in an airplane, for example, it will slowly drift away from north and will need to be reoriented periodically, using a magnetic compass as a reference.\nGyrocompass.\nUnlike a directional gyro or heading indicator, a gyrocompass seeks north. It detects the rotation of the Earth about its axis and seeks the \"true\" north, rather than the \"magnetic\" north. Gyrocompasses usually have built-in damping to prevent overshoot when re-calibrating from sudden movement.\nAccelerometer.\nBy determining an object's acceleration and integrating over time, the velocity of the object can be calculated. Integrating again, position can be determined. The simplest accelerometer is a weight that is free to move horizontally, which is attached to a spring and a device to measure the tension in the spring. This can be improved by introducing a counteracting force to push the weight back and to measure the force needed to prevent the weight from moving. A more complicated design consists of a gyroscope with a weight on one of the axes. The device will react to the force generated by the weight when it is accelerated, by integrating that force to produce a velocity.\nVariations.\nGyrostat.\nA gyrostat consists of a massive flywheel concealed in a solid casing. Its behaviour on a table, or with various modes of suspension or support, serves to illustrate the curious reversal of the ordinary laws of static equilibrium due to the gyrostatic behaviour of the interior invisible flywheel when rotated rapidly. The first gyrostat was designed by Lord Kelvin to illustrate the more complicated state of motion of a spinning body when free to wander about on a horizontal plane, like a top spun on the pavement, or a bicycle on the road. Kelvin also made use of gyrostats to develop mechanical theories of the elasticity of matter and of the ether. In modern continuum mechanics there is a variety of these models, based on ideas of Lord Kelvin. They represent a specific type of Cosserat theories (suggested for the first time by Eug\u00e8ne Cosserat and Fran\u00e7ois Cosserat), which can be used for description of artificially made smart materials as well as of other complex media. One of them, so-called Kelvin's medium, has the same equations as magnetic insulators near the state of magnetic saturation in the approximation of quasimagnetostatics.\nIn modern times, the gyrostat concept is used in the design of attitude control systems for orbiting spacecraft and satellites. For instance, the Mir space station had three pairs of internally mounted flywheels known as \"gyrodynes\" or control moment gyroscopes.\nIn physics, there are several systems whose dynamical equations resemble the equations of motion of a gyrostat. Examples include a solid body with a cavity filled with an inviscid, incompressible, homogeneous liquid, the static equilibrium configuration of a stressed elastic rod in elastica theory, the polarization dynamics of a light pulse propagating through a nonlinear medium, the Lorenz system in chaos theory, and the motion of an ion in a Penning trap mass spectrometer.\nMEMS gyroscope.\nA microelectromechanical systems (MEMS) gyroscope is a miniaturized gyroscope found in electronic devices. It takes the idea of the Foucault pendulum and uses a vibrating element. This kind of gyroscope was first used in military applications but has since been adopted for increasing commercial use.\nHRG.\nThe hemispherical resonator gyroscope (HRG), also called a wine-glass gyroscope or mushroom gyro, makes use of a thin solid-state hemispherical shell, anchored by a thick stem. This shell is driven to a flexural resonance by electrostatic forces generated by electrodes which are deposited directly onto separate fused-quartz structures that surround the shell. Gyroscopic effect is obtained from the inertial property of the flexural standing waves.\nVSG or CVG.\nA vibrating structure gyroscope (VSG), also called a Coriolis vibratory gyroscope (CVG), uses a resonator made of different metallic alloys. It takes a position between the low-accuracy, low-cost MEMS gyroscope and the higher-accuracy and higher-cost fiber optic gyroscope. Accuracy parameters are increased by using low-intrinsic damping materials, resonator vacuumization, and digital electronics to reduce temperature dependent drift and instability of control signals.\nHigh quality wine-glass resonators are used for precise sensors like HRG.\nDTG.\nA dynamically tuned gyroscope (DTG) is a rotor suspended by a universal joint with flexure pivots. The flexure spring stiffness is independent of spin rate. However, the dynamic inertia (from the gyroscopic reaction effect) from the gimbal provides negative spring stiffness proportional to the square of the spin speed (Howe and Savet, 1964; Lawrence, 1998). Therefore, at a particular speed, called the tuning speed, the two moments cancel each other, freeing the rotor from torque, a necessary condition for an ideal gyroscope.\nRing laser gyroscope.\nA ring laser gyroscope relies on the Sagnac effect to measure rotation by measuring the shifting interference pattern of a beam split into two separate beams which travel around the ring in opposite directions.\nWhen the Boeing 757-200 entered service in 1983, it was equipped with the first suitable ring laser gyroscope. This gyroscope took many years to develop, and the experimental models went through many changes before it was deemed ready for production by the engineers and managers of Honeywell and Boeing. It was an outcome of the competition with mechanical gyroscopes, which kept improving. The reason Honeywell, of all companies, chose to develop the laser gyro was that they were the only one that did not have a successful line of mechanical gyroscopes, so they would not be competing against themselves. The first problem they had to solve was that with laser gyros rotations below a certain minimum could not be detected at all, due to a problem called \"lock-in\", whereby the two beams act like coupled oscillators and pull each other's frequencies toward convergence and therefore zero output. The solution was to shake the gyro rapidly so that it never settled into lock-in. Paradoxically, too regular of a dithering motion produced an accumulation of short periods of lock-in when the device was at rest at the extremities of its shaking motion. This was cured by applying a random white noise to the vibration. The material of the block was also changed from quartz to a new glass ceramic Cer-Vit, made by Owens Corning, because of helium leaks.\nFiber optic gyroscope.\nA fiber optic gyroscope also uses the interference of light to detect mechanical rotation. The two-halves of the split beam travel in opposite directions in a coil of fiber optic cable as long as 5\u00a0km. Like the ring laser gyroscope, it makes use of the Sagnac effect.\nLondon moment.\nA London moment gyroscope relies on the quantum-mechanical phenomenon, whereby a spinning superconductor generates a magnetic field whose axis lines up exactly with the spin axis of the gyroscopic rotor. A magnetometer determines the orientation of the generated field, which is interpolated to determine the axis of rotation. Gyroscopes of this type can be extremely accurate and stable. For example, those used in the Gravity Probe B experiment measured changes in gyroscope spin axis orientation to better than 0.5 milliarcseconds (1.4\u00d710-7 degrees, or about ) over a one-year period. This is equivalent to an angular separation the width of a human hair viewed from away.\nThe GP-B gyro consists of a nearly-perfect spherical rotating mass made of fused quartz, which provides a dielectric support for a thin layer of niobium superconducting material. To eliminate friction found in conventional bearings, the rotor assembly is centered by the electric field from six electrodes. After the initial spin-up by a jet of helium which brings the rotor to 4,000 RPM, the polished gyroscope housing is evacuated to an ultra-high vacuum to further reduce drag on the rotor. Provided the suspension electronics remain powered, the extreme rotational symmetry, lack of friction, and low drag will allow the angular momentum of the rotor to keep it spinning for about 15,000 years.\nA sensitive DC SQUID that can discriminate changes as small as one quantum, or about 2 \u00d710-15 Wb, is used to monitor the gyroscope. A precession, or tilt, in the orientation of the rotor causes the London moment magnetic field to shift relative to the housing. The moving field passes through a superconducting pickup loop fixed to the housing, inducing a small electric current. The current produces a voltage across a shunt resistance, which is resolved to spherical coordinates by a microprocessor. The system is designed to minimize Lorentz torque on the rotor.\nOther examples.\nHelicopters.\nThe main rotor of a helicopter acts like a gyroscope. Its motion is influenced by the principle of gyroscopic precession which is the concept that a force applied to a spinning object will have a maximum reaction approximately 90 degrees later. The reaction may differ from 90 degrees when other stronger forces are in play. To change direction, helicopters must adjust the pitch angle and the angle of attack.\nGyro X.\nGyro X prototype vehicle created by Alex Tremulis and Thomas Summers in 1967. The car utilized gyroscopic precession to drive on two wheels. An assembly consisting of a flywheel mounted in a gimbal housing under the hood of the vehicle acted as a large gyroscope. The flywheel was rotated by hydraulic pumps creating a gyroscopic effect on the vehicle. A precessional ram was responsible for rotating the gyroscope to change the direction of the precessional force to counteract any forces causing the vehicle imbalance. The one-of-a-kind prototype is now at the Lane Motor Museum in Nashville, Tennessee.\nConsumer electronics.\nIn addition to being used in compasses, aircraft, computer pointing devices, etc., gyroscopes have been introduced into consumer electronics.\nSince the gyroscope allows the calculation of orientation and rotation, designers have incorporated them into modern technology. The integration of the gyroscope has allowed for more accurate recognition of movement within a 3D space than the previous lone accelerometer within a number of smartphones. Gyroscopes in consumer electronics are frequently combined with accelerometers for more robust direction- and motion-sensing. Examples of such applications include smartphones such as the Samsung Galaxy Note 4, HTC Titan, Nexus 5, iPhone 5s, Nokia 808 PureView and Sony Xperia, game console peripherals such as the PlayStation 3 controller and the Wii Remote, and virtual reality headsets such as the Oculus Rift. Some features of Android phones like PhotoSphere or 360 Camera and to use VR gadget do not work without a gyroscope sensor in the phone.\nNintendo has integrated a gyroscope into the Wii console's Wii Remote controller by an additional piece of hardware called \"Wii MotionPlus\". It is also included in the 3DS, Wii U GamePad, and Nintendo Switch Joy-Con and Pro controllers, which detect movement when turning and shaking.\nCruise ships use gyroscopes to level motion-sensitive devices such as self-leveling pool tables.\nAn electric powered flywheel gyroscope inserted in a bicycle wheel is sold as an alternative to training wheels.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44126", "revid": "14672439", "url": "https://en.wikipedia.org/wiki?curid=44126", "title": "Hitch", "text": "Hitch may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44127", "revid": "23939382", "url": "https://en.wikipedia.org/wiki?curid=44127", "title": "Military incompetence", "text": "Failures of military organisations\nMilitary incompetence refers to incompetencies and failures of military organisations, whether through incompetent individuals or through a flawed institutional culture.\nThe effects of isolated cases of \"personal\" incompetence can be disproportionately significant in military organisations. Strict hierarchies of command provide the opportunity for a single decision to direct the work of thousands, whilst an institutional culture devoted to following orders without debate can help ensure that a bad or miscommunicated decision is implemented without being challenged or corrected.\nHowever, the most common cases of \"military incompetence\" can be attributable to a flawed organisational culture. Perhaps the most marked of these is a conservative and traditionalist attitude, where innovative ideas or new technology are discarded or left untested. A tendency to believe that a problem can be solved by applying an earlier (failed) solution \"better\", be that with more men, more firepower, or simply more zeal, is common. A strict hierarchical system often discourages the devolution of power to junior commanders, and can encourage micromanagement by senior officers.\nThe nature of warfare provides several factors which exacerbate these effects; the fog of war means that information about the enemy forces is often limited or inaccurate, making it easy for the intelligence process to interpret the information to agree with existing assumptions, or to fit it to their own preconceptions and expectations. Communications tend to deteriorate in battlefield situations, with the flow of information between commanders and combat units being disrupted, making it difficult to react to changes in the situation as they develop.\nAfter operations have ceased, military organisations often fail to learn effectively from experience. In victory, whatever methods have been used\u2014no matter how inefficient\u2014appear to have been vindicated (see victory disease), whilst in defeat there is a tendency to select scapegoats and to avoid looking in detail at the broader reasons for failure."}
{"id": "44128", "revid": "2543165", "url": "https://en.wikipedia.org/wiki?curid=44128", "title": "Lashing (ropework)", "text": "Way of fastening two or more items together with rope\nA lashing is an arrangement of rope, wire, or webbing with linking device used to secure and fasten two or more items together in a somewhat rigid manner. Lashings are most commonly applied to timber poles, and are commonly associated with cargo, containerisation, the Scouting movement, sailors, and gardeners.\nIt has been imagined that the first lashing made by humans was wrapping a few strips of bark around a stone to hold it to a tree branch to make an ax to hunt and build with. In modern times, the same methods are used, but strips of bark and vines have been replaced with natural and synthetic fiber ropes. Scouts and campers use lashings to build camp gadgets and improve their campsites for comfort and convenience, including the building of rafts for transport and competitive events. Lashings are also used in pioneering, the art of creating structures such as bridges and towers, using ropes and wooden spars.\nThere are still areas in the world where lashing spars (or poles) is the basic means of building.\nTypes.\nSquare lashing.\nSquare lashing is a type of lashing used to bind spars together, at right angles to one another. There are different types, but all consist of a series of wraps around the spars, and frapping around the line running between the spars.\nDiagonal lashing.\nDiagonal lashing is a type of lashing used to bind spars or poles together, to prevent racking. It gets its name from the fact that the wrapping turns cross the poles diagonally and is used to spring poles together where they do not touch as in the X-brace of a trestle.\nShear lashing.\nShear lashing (two-spar shear lashing) also spelled \"sheer lashing\" is used for lashing together two parallel spars which will be opened out of the parallel to form sheer legs as in the formation of an A-frame. The clove hitch is tied around one leg only and frapping turns are taken between the poles.\nRound lashing.\nThe round lashing is most frequently used to join two poles together to extend their length. Typically, two lashings are used a reasonable distance apart for extra strength. In the simple version, a clove hitch is tied around both poles and there are no frapping turns.\nThe nautical term gammon means a round lashing of rope or iron hardware to attach a mast to a boat or ship.\nTripod lashing.\nThe tripod lashing (also known as gyn lashing, figure of eight lashing, and three-spar shear lashing) is used to join several spars together to form a self supporting structure. If the lashing is tied around three spars, then the structure is called a tripod, but quadpods can also be made by using four spars.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44129", "revid": "8", "url": "https://en.wikipedia.org/wiki?curid=44129", "title": "Linus Carl Pauling", "text": ""}
{"id": "44130", "revid": "4968133", "url": "https://en.wikipedia.org/wiki?curid=44130", "title": "Plait", "text": "Plait may refer to:\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44132", "revid": "44360971", "url": "https://en.wikipedia.org/wiki?curid=44132", "title": "Ramallah", "text": "City in the West Bank, Palestine\nRamallah ( , ; ; ; lit.\u2009'God's Height') is a city in the central West Bank, Palestine. It serves as the administrative capital of Palestine, as well as capital of the Ramallah and al-Bireh Governorate. The city is situated on the Judaean Mountains, north of Jerusalem, at an average elevation of above sea level, adjacent to al-Bireh.\nRamallah has buildings containing masonry from the period of Herod the Great, but no complete building predates the Crusades of the 11th century.\u00a0The modern city was founded during the 16th century by the Hadadeens, an Arab Christian clan descended from Ghassanids. In 1517, the city was incorporated into the Ottoman Empire, and in 1920, it became part of British Mandatory Palestine after it was captured by the United Kingdom during World War I. The 1948 Arab\u2013Israeli War saw the entire West Bank, including Ramallah, occupied and annexed by Transjordan. Ramallah was later captured by Israel in the 1967 Six-Day War. Since the 1995 Oslo Accords, Ramallah has been governed by the Palestinian Authority, as part of Area A in the West Bank.\nRamallah has emerged as a key political, cultural, and economic center in recent years. It houses various Palestinian governmental bodies, including the Mukataa, the official residence of the president of the Palestinian National Authority, the Palestinian Legislative Council, and the headquarters of the Palestinian Security Services. It is also home to several museums and cultural centers, and has a notable nightlife scene. Historically, the city was a predominantly Christian town, however the population of Muslims has increased to constitute a majority of Ramallah's 38,998 residents by 2017, while Christians make up a significant minority.\nHistory.\nEarly history.\nRamallah was founded in the 16th century by the \"Hadadeens\", an Arab Christian clan. The city boasts archaeological remnants from earlier epochs. Ancient rock-cut tombs have been found near Ramallah. Located just south of the built-up area is Tell en-Nasbeh, an archeological site where biblical Mizpah in Benjamin is likely to have been located.\nSeveral Ramallah buildings incorporate masonry dating back to the reign of Herod the Great (37\u20134 BCE). Potsherds from the Crusader/Ayyubid and early Ottoman period have also been found there. Ramallah has been identified with the Crusader place called \"Ramalie\". Remains of a building with an arched doorway from the Crusader era, called \"al-Burj\", have been identified, but the original use of the building is undetermined.\nOttoman period.\nThe area of Ramallah was incorporated into the Ottoman Empire in 1517 with all of Palestine. Modern Ramallah was founded in the mid-1500s by the Haddadins (also: Haddad\"ee\"n), a clan of brothers descended from Ghassanid Christians. The Haddadins (ancestors of the present-day Jadallah family, among others), and their leader Rashid el-Haddadin, arrived from east of the Jordan River from the areas of Karak and Shoubak. The Haddadin migration is attributed to fighting and unrest among clans in that area.\nHaddadin was attracted to the mountainous site of Ramallah because it was similar to the mountainous areas he came from. In addition, the heavily forested area could supply him with plenty of fuel for his forges. In 1596, Ramallah was listed in the tax registers as being in the \"nahiya\" of Quds (Jerusalem), part of the \"Liwa of Quds\". It had a population of 71 Christian households and 9 Muslim households. It paid a fixed tax rate of 25% on wheat, barley, olives, vines or fruit trees, and goats or beehives; a total of 9,400 ak\u00e7e. All of the revenue went to a waqf.\nIn 1838, American biblical scholar Edward Robinson visited the area, noting that the inhabitants were Christian \"of the Greek rite\". There were 200 taxable men, which gives an estimated total population of 800\u2013900 people. The village \"belonged\" to the Haram al-Sharif, Jerusalem, to which it paid an annual tax of 350 Mids of grain. In 1883, the PEF's \"Survey of Western Palestine\" described Ramallah as\nA large Christian village, of well-built stone houses, standing on a high ridge, with a view on the west extending to the sea. It stands amongst gardens and olive-yards, and has three springs to the south and one on the west; on the north there are three more, within a mile from the village. On the east there is a well. There are rock-cut tombs to the north-east with well-cut entrances, but completely blocked with rubbish. In the village is a Greek church, and on the east a Latin convent and a Protestant schoolhouse, all modern buildings. The village lands are Wakuf, or ecclesiastical property, belonging to the Haram of Jerusalem. About a quarter of the inhabitants are Roman Catholics, the rest Orthodox Greeks.\nBritish Mandate.\nDuring World War I, the British Army captured and occupied Ramallah in December 1917. The city remained occupied until the designation of the Palestine Mandate in 1920, resulting in Ramallah falling under British Mandatory control until 1948. In the 1920s, the economy of Ramallah started to improve, resulting in the local Arab upper class (consisting primarily of landowners and merchants) ordering the construction of several multi-storied villas, many of which still stand today. In 1939, the Jerusalem Electric Company introduced electricity to Ramallah, and a majority of the city's homes became wired shortly thereafter. On the same year, the British Mandatory authorities inaugurated the state-owned Palestine Broadcasting Service in Ramallah, with BBC members training local radio staff to deliver daily broadcasts in Arabic, Hebrew, and English. The station was later renamed Jerusalem Calling.\nThe city became a center of insurgent activity when the 1936\u201339 Arab revolt in Palestine broke out. The rebels subsequently established a court near Ramallah, in order to provide legal alternatives to the courts of the British Mandate. One British schoolteacher noted that the Ramallah court judge began to produce \"news sheets on typewriters and duplicators, aimed at publicizing the alternative rebel regime.\"\nJordanian and Israeli occupation.\nFollowing the creation of the State of Israel and the ensuing conflict, Jordan seized part of the territory, which they named the West Bank, which included Ramallah. Jordan annexed the West Bank, applying its national law to the conquered territory. The West Bank was relatively peaceful during the years of Jordanian occupation between 1948 and 1967, with its residents enjoying freedom of movement between the West Bank, Jordan, Lebanon, and Syria. However, many Palestinians were arrested and jailed for being members of \"illegal political parties\", which included the Palestine Communist Party, and other socialist and pro-independence groups. The city's population had doubled by 1953, but the economy and infrastructure could not accommodate the influx of poor villagers. Natives of Ramallah began to emigrate, primarily to the United States. About one fourth of Ramallah's 6,000 natives had left by 1956, with Arabs from the surrounding towns and villages (particularly Hebron) buying the homes and land the \u00e9migr\u00e9s left behind.\nDuring the Six-Day War in 1967, Israel captured Ramallah from Jordan, imposing a military closure and conducting a census a few weeks later. Every person registered in the census was given an Israeli identity card which allowed the bearer to continue to reside there. Those who were abroad during the census lost their residency rights. Because of Israeli control of the West Bank and Gaza Strip, for the first time in 19 years, residents of Ramallah could freely visit the Gaza Strip, as well as Israel, and engage in commerce there.\nUnlike the Jordanians, Israel did not offer citizenship to the residents. Ramallah residents were issued permits to work in Israel, but did not gain the rights associated with Israeli citizenship. The city remained under Israeli military rule for more than four decades. The Israeli Civil Administration (CA), established in 1981, was in charge of civilian and day-to-day services such as issuing permission to travel, build, export or import, and host relatives from abroad. The CA reprinted Jordanian textbooks for distribution in schools but did not update them. The CA was in charge of tax collection and land expropriation, which sometimes included Israeli seizure of olive groves that Arab villagers had tended for generations.\nAccording to the Israeli Human Rights activists, the development of Jewish settlements in the Ramallah area, such as Beit El and Psagot, prevented the expansion of the city and cut it off from the surrounding Arab villages. As resistance increased, Ramallah residents who were members of the Palestine Liberation Organization were jailed or deported to neighboring countries. The popular uprising known as the First Intifada erupted in December 1987, protesting against the continued Israeli occupation.\nRamallah residents were among the early joiners of the First Intifada. The Intifada Unified Leadership, an umbrella organization of various Palestinian factions, distributed weekly bulletins on the streets of Ramallah with a schedule of the daily protests, strikes and action against Israeli patrols in the city. At the demonstrations, tires were burned in the street, and the crowds threw stones and Molotov cocktails. The IDF responded with tear gas and rubber bullets. Schools in Ramallah were forcibly shut down, and opened gradually for a few hours a day. The Israelis conducted house arrests, imposing curfews that restricted travel and exports in what Palestinians regarded as collective punishment. In response to the closure of schools, residents organized home schooling sessions to help students make up missed material; this became one of the few symbols of civil disobedience. Following the Oslo Accords, the Israeli army abandoned the Mukataa in December 1995 and withdrew to the city outskirts. The newly established Palestinian Authority assumed civilian and security responsibility for the city, which was designated \"Area A\" under the accords.\nPalestinian Authority control.\nThe years between 1993 and 2000 (known locally as the \"Oslo Years\") brought relative prosperity to Ramallah. Ramallah and its immediate environs were classified as Area A in the Oslo Accords, under full civil and security control of the Palestinian Authority (PA) administration in September 1995. Many expatriates returned to establish businesses there, and the atmosphere was one of optimism. In 2000, unemployment began to rise and the economy of Ramallah declined. The Israel Defense Forces remained in control of the territories and its government did not restore the freedom of movement enjoyed by Ramallah residents prior to the first Intifada. Travel to Jerusalem required special permits. The number and size of Israeli settlements around Ramallah increased dramatically. A network of bypass roads for use of Israeli citizens only was built around Ramallah, and Israel expropriated land for settlements. Many official documents previously handled by the Israeli Civil Administration were now handled by the Palestinian Authority but still required Israeli approval. A Palestinian passport issued to Ramallah residents was not valid unless the serial number was registered with the Israeli authorities, who controlled border crossings.The failure of the Camp David summit in July 2000 led to the outbreak of the Second Intifada (al-Aqsa Intifada) in September 2000. Young Ramallah residents demonstrated daily against the Israeli army, with marches to the Israeli checkpoints at the outskirts of the city. Over time, the marches were replaced by sporadic use of live ammunition against Israeli soldiers; and various attacks targeting Jewish settlers, particularly on the Israeli-only bypass roads. Army checkpoints were established to restrict movement in and out of Ramallah. On October 12, 2000, two Israeli army reservists, Vadim Norzhich and Yosef Avrahami were lynched in Ramallah. They had taken a wrong turn, and were set upon by a mob, enraged in particular by the Muhammad al-Durrah incident in Gaza. A frenzied crowd killed the two IDF reservists, mutilated their bodies, and dragged them through the streets. Later that afternoon, the Israeli army carried out an air strike on Ramallah, demolishing the police station. Israel later succeeded in capturing and prosecuting some of those involved in the deaths of the reservists.The IDF has occasionally operated inside Ramallah, in breach of the 1995 Oslo Accords. The first and largest incursion was the 2002 Operation Defensive Shield, with a more recent intervention coming in March 2017 while attempting to arrest a suspected terrorist. In 2002, the army imposed curfews, electricity cuts, school closures and disruptions of commercial life. Many Ramallah institutions, including government ministries, were vandalized, and equipment was destroyed or stolen. The IDF took over local Ramallah television stations, and social and economic conditions deteriorated. Many expatriates left, as did many other Palestinians who complained that the living conditions had become intolerable. Construction of the Israeli West Bank barrier has added to Ramallah's isolation. Yasser Arafat established his West Bank headquarters, the Mukataa, in Ramallah. Although considered an interim solution, Ramallah became the \"de facto\" capital of the Palestinian Authority, now officially known as the State of Palestine. It hosts almost all governmental headquarters. In December 2001, Arafat held meetings at the Mukataa, but lived with his wife and daughter in Gaza City. After suicide bombings in Haifa, Arafat was confined to the Ramallah compound. In 2002, the compound was partly demolished by the Israeli Defense Forces and Arafat's building was cut off from the rest of the compound.\nOn November 11, 2004, Arafat died at the Percy training hospital of the Armies near Paris. He was buried in the courtyard of the Mukataa on November 12, 2004. The site still serves as the Ramallah headquarters of the Palestinian Authority, as well the official West Bank office of Mahmoud Abbas. Throughout 2005, while the Disengagement Plan was underway, some US government officials suggested to the Palestinian leadership to move the provisional capital back to Gaza, where it had been when the Palestinian Authority was first established in 1994. President Abbas, however, refrained from doing so, arguing that at this point, it was important to keep the administrative center in the West Bank in order to remind the international community that the West Bank was still awaiting a territorial solution.\nIn December 2005, local elections were held in Ramallah in which candidates from three different factions competed for a four-year term on the fifteen-seat municipal council. The council elected Janet Mikhail as mayor, the first woman to hold the post.\nMunir Hamdan, a member of Fatah and a Ramallah businessman, discussed the concentration of government offices with a journalist. He said, \"The president and prime minister have their offices here. So do the parliament and all the government ministries\", representing a \"collusion\" between the Palestinian Authority and Israel to turn Ramallah into the political as well as the financial capital of the Palestinians. He is particularly worried by the construction of a large new governmental complex by the PA. Hatem Abdel Kader, a Jerusalem resident, Fatah legislator and former Minister for Jerusalem Affairs, complained that \"If they are building a new government compound here, that means they have no plans to be based in Jerusalem... Unfortunately, the Palestinian government of Salam Fayyad has abandoned Jerusalem in favor of Ramallah.\" In November 2011, King Abdullah II of Jordan visited Ramallah for the first time since 2000.\nGeography and climate.\nThis area enjoys a Mediterranean climate of a dry summer and mild, rainy winter with occasional snowfall. The recorded average of Ramallah's rainfall is about and minimum rainfall is and maximum rainfall is .\nThe K\u00f6ppen climate classification places Ramallah in the Csa category. Climates of this class generally occur on the western sides of continents between the latitudes of 30\u00b0 and 45\u00b0. These climates are in the polar front region in winter, and thus have moderate temperatures and changeable, rainy weather. Summers are hot and dry, due to the domination of the subtropical high pressure systems, except in the immediate coastal areas, where summers are milder due to the nearby presence of cold ocean currents that may bring fog but prevent rain.\nEconomy.\nRamallah is chief financial and commercial center for the Palestinian Authority, home to the country's numerous financial institutions. Currently Ramallah is seat of power of the Palestinian Authority, whose most of the offices are located within the city. The city serves as the headquarters for most international NGOs and embassies. Hundreds of millions of dollars in aid flowing into the city have boosted Ramallah's economy greatly since the end of the Second Intifada. Ramallah's buoyant economy continues to draw Palestinians from other West Bank towns where jobs are fewer. The built-up area has grown fivefold since 2002.\nConstruction boom is one of the most obvious signs of West Bank economic growth, estimated at an annual rate of 8 percent. This has been attributed to relative stability and Western donor support to the Palestinian Authority. The PIF have begun work on a $400 million commercial center comprising 13 towers which will be some of the tallest in Ramallah. The Ersal Commercial Center has drawn investment from a Saudi Arabian firm, The Land Holding, which has a 10% stake. It is not the only Gulf Arab firm investing in Ramallah and its outskirts. The Qatari Diar Real Estate Investment Company has a stake in Rawabi, a completely new town being constructed in the hills outside Ramallah at a cost of $1 billion.\nRamallah has highest concentration of high-tech companies. ASAL technologies, an information technology company in Ramallah, has 120 employees and is looking forward to \"exponential growth\". In collaboration with the Republic of India, a new tech park named, the India Palestine Techno Park is located in Birzeit. Apple Inc operates a research &amp; development center in Rawabi with ASAL Technologies. A large number multinational companies operates facilities in Ramallah, which outsource Palestinians.\nBy 2010, Ramallah had become the leading center of economic and political activity in the territories under the control of the Palestinian Authority. During a building boom in the early years of the 21st century, apartment buildings and \"five-star\" hotels were erected, particularly in the Al-Masyoun neighborhood. In 2010, \"more than one hundred\" Palestinian businesses were reported to have moved to Ramallah from East Jerusalem, because \"Here they pay less taxes and have more customers.\" One local boasted to a journalist that \"Ramallah is becoming the de facto capital of Palestine.\" This boast was seconded by \"The New York Times\" which, in 2010, called Ramallah the \"de facto capital of the West Bank. According to Sani Meo, the publisher of \"This Week in Palestine\", \"Capital or no capital, Ramallah has done well and Palestine is proud of its achievements.\" Some Palestinians allege that Ramallah's prosperity is part of an Israeli \"conspiracy\" to make Ramallah the capital of a Palestinian state, instead of Jerusalem.\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nAn Ottoman village list of about 1870 showed that Ramallah had 249 houses and a population of 635, though the population count included men only. The village was described as being in the \"Bire\" area, \"north of Mikhmas, on a rocky hill.\" In 1896, the population of Ramallah was estimated to be about 2,061 persons.\nIn the 1922 census of Palestine conducted by the British Mandate authorities, Ramallah had a population of 3,104 (2,972 Christians, 125 Muslims, and seven Jews), the Christians consisting of 2,162 Orthodox, 1 Syriac Orthodox (Jacobite), 332 Roman Catholics, 144 Greek Catholic (Melkite Catholic), 211 Church of England, and 122 \"other\". The population increased at the time of the 1931 census to 4,286 (3,766 Christians, 519 Muslims and one Jew) in a total of 1,014 houses. In the 1938 village statistics, the population is listed as 4,900. In the 1945 statistics, the population stood at 5,080, with Christians forming the majority of the population (4,440 Christians and 640 Muslims).\nHowever, the demographic makeup of the town changed drastically under Jordanian occupation, when considerable emigration of Christians took place. This left slightly more than half of the city's 12,134 inhabitants Christian by 1967, and the remainder Muslim.\nRamallah expatriates created one of the largest Arab communities in the United States, settling mainly in Washington, New York, Florida, California, Texas, and especially in Michigan. Many worked in the auto industry. In 1959, the American Federation of Ramallah, Palestine (AFRP) was established in Detroit. The AFRP has several branches in the United States, and holds an annual convention every summer attended by a sizable number of former Ramallah residents and their offspring.\nRamallah's population drastically decreased in the late 20th century from 24,722 inhabitants in 1987 to 17,851 in 1997. In the Palestinian Central Bureau of Statistics (PCBS) census in 1997, Palestinian refugees accounted for 60.3% of the population, which was 17,851. There were 8,622 males and 9,229 females. People younger than 20 years of age made up 45.9% of the population, while those aged between 20 and 64 were 45.4%, and residents aged over 64 constituted 4.7%.\nOnly in 2005 did the population reach more than 24,000. In a PCBS projection in 2006, Ramallah had a population of 25,467 inhabitants. In the 2007 PCBS census, there were 27,460 people living in the city. Sources vary about the current Christian population in the city, ranging around 25%.\nHealth.\nIn the aftermath of the 1936\u20131939 Arab revolt, the Ramallah Hospital Foundation was established and registered as a tax exempt organization in New York in 1944. It bought large pieces of land in the south-eastern fringes of the city dedicated for the future hospital. In 1963, a hospital was opened.\nReligion.\nThe Jamal Abdel Nasser Mosque is one of the city's largest mosques. The Orthodox Church of Ramallah, an Orthodox Christian convent, Melkite Catholic Church, Evangelical Lutheran Church, Arab Episcopal (Anglican) Church, Ramallah Local Church (Evangelical\\Born Again) and Ramallah Baptist Church all operate schools in the city.\nChristian presence.\nRamallah grew dramatically throughout the 17th and 18th centuries as an agricultural village, attracting more (predominantly Christian) inhabitants from all around the region. In 1700, Yacoub Elias was the first Ramallah native to be ordained by the Greek Orthodox Church of Jerusalem, the Christian denomination that prevailed in the Holy Land at the time. In the early 19th century, the first Jerusalemite Greek Orthodox Christian church was built. Later, in 1852, the Greek Orthodox Church of the Transfiguration, was built to replace it; it is the sole Eastern Orthodox Church in Ramallah today.\nThere is also a Melkite Greek Catholic Church in Ramallah, built in 1895. The Roman Catholic Church also established its presence in Ramallah the 19th century and constitutes today the second-largest Christian denomination in the city. The Roman Catholic Church established the St. Joseph's Girls' School run by St. Joseph sisters, as well as the co-educational Al-Ahliyyah College high school run by Rosary sisters. In 1913, construction of the Catholic Holy Family Church was started.\nAs of 2022[ [update]], Ramallah also has a Coptic Orthodox Church, an Evangelical Lutheran Church and an Episcopalian (Anglican) Church. In the 19th century, the Religious Society of Friends (Quakers) established a presence in Ramallah and built the Ramallah Friends Schools, one for girls and later a boys' school, to alleviate the dearth of education for women and girls. Eli and Sybil Jones opened \"The Girls Training Home of Ramallah\" in 1869. A medical clinic was established in 1883, with Dr. George Hassenauer serving as the first doctor in Ramallah. In 1889, the girls academy became the Friends Girls School (FGS). As the FGS was also a boarding school, it attracted a number of girls from surrounding communities, including Jerusalem, Lydda, Jaffa, and Beirut. The Friends Boys School (FBS) was founded in 1901 and opened in 1918. The Quakers opened a Friends Meeting House for worship in the city center in 1910. According to the school's official website, most high school students choose to take the International Baccalaureate exams (IBE) instead of the traditional \"Tawjihi\" university exams.\nThe activity of foreign churches in Palestine in the late 19th century increased awareness of prosperity in the West. In Ramallah and Bethlehem, a few miles south, local residents began to seek economic opportunity overseas. In 1901, merchants from Ramallah emigrated to the United States and established import-export businesses, selling handmade rugs and other exotic wares across the Atlantic. Increased trade dramatically improved living standards for Ramallah's inhabitants. American cars, mechanized farming equipment, radios, and later televisions became attainable luxuries for upper-class families. As residents of Jaffa and Lydda moved to Ramallah, the balance of Muslims and Christians began to change.\nIn the 21st century, a large community of people with direct descent from the Haddadins who founded Ramallah live in the United States. The town is now predominately Muslim, but still contains a Christian minority. The change in demographics is due mostly to new migration of Muslims to the area, and emigration of Christians from the area.\nCulture.\nRamallah is generally considered the most affluent and cultural, as well as the most liberal, of all Palestinian cities, and is home to a number of popular Palestinian activists, poets, artists, and musicians.\nIn 2004 the Ramallah Cultural Palace opened in the city. The only cultural center of its kind in the Palestinian-governed areas, it houses a 736-seat auditorium, as well as conference rooms, exhibit halls, and movie-screening rooms. It was a joint venture of the Palestinian Authority, the United Nations Development Programme (UNDP), and the Japanese government. Ramallah hosted its first annual international film festival in 2004.\nPalestinian costume.\nUntil the 1940s, traditional Palestinian costumes reflected a woman's economic and marital status and her town or district of origin, with knowledgeable observers discerning this information from the fabric, colours, cut, and embroidery motifs (or lack thereof) used in the apparel.\nVillages in the Levant under Ottoman rule remained isolated due to the difficulty of travel in the 19th century. As a result, clothing and accessories became a statement of region. In Ramallah, the back panels of dresses often incorporated a palm tree motif embroidered in cross-stitch.\nInternational relations.\nMany foreign nations have located their diplomatic missions to the Palestinian Authority in Ramallah, including, as of 2010[ [update]], Argentina, Australia, Austria, Korea, South Africa, Norway, Sri Lanka, Switzerland, China, Poland, Portugal, The Netherlands, Russia, Jordan, Brazil, Finland, Denmark, Ireland, Germany, India, Japan, the Czech Republic, Canada and Mexico.\nTwin towns \u2013 sister cities.\nRamallah is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "44133", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=44133", "title": "Overture", "text": "Instrumental introduction to an opera, ballet, or oratorio\nOverture (from French \"ouverture\", lit. \"opening\") is a music instrumental introduction to a ballet, opera, or oratorio in the 17th century. During the early Romantic era, composers such as Beethoven and Mendelssohn composed overtures which were independent, self-existing, instrumental, programmatic works that foreshadowed genres such as the symphonic poem. These were \"at first undoubtedly intended to be played at the head of a programme\".\nThe idea of an instrumental opening to opera existed during the 17th century. Peri's \"Euridice\" opens with a brief instrumental ritornello, and Monteverdi's \"L'Orfeo\" (1607) opens with a toccata, in this case a fanfare for muted trumpets. More important was the prologue, consisting of sung dialogue between allegorical characters which introduced the overarching themes of the stories depicted.\nFrench overture.\nAs a musical form, the French overture first appears in the court ballet and operatic overtures of Jean-Baptiste Lully, which he elaborated from a similar, two-section form called \"ouverture\", found in the French ballets de cour as early as 1640. This French overture consists of a slow introduction in a marked \"dotted rhythm\" (i.e., exaggerated iambic, if the first chord is disregarded), followed by a lively movement in fugato style. The overture is frequently followed by a series of dance tunes before the curtain rises, and often returns following the Prologue to introduce the action proper. This ouverture style was also used in English opera, most notably Henry Purcell's \"Dido and \u00c6neas\". Its distinctive rhythmic profile and function thus led to the French overture style found in the works of late Baroque composers such as Johann Sebastian Bach, Georg Friedrich H\u00e4ndel, and Georg Philipp Telemann. The style is most often used in preludes to suites, and can be found in non-staged vocal works such as cantatas, for example in the opening chorus of Bach's cantata \"Nun komm, der Heiden Heiland, BWV 61\". Handel also uses the French overture form in some of his Italian operas, such as \"Giulio Cesare\".\nItalian overture.\nIn Italy, a distinct form called \"overture\" arose in the 1680s, became established particularly through the operas of Alessandro Scarlatti, and spread throughout Europe, supplanting the French form as the standard operatic overture by the mid-18th century. Its stereotypical form is in three generally homophonic movements: fast\u2013slow\u2013fast. The opening movement is normally in duple metre and a major key; the slow movement in earlier examples is usually quite short, and sometimes in a contrasting key; the concluding movement is dancelike, most often with rhythms of the gigue or minuet, and returns to the key of the opening section. As the form evolved, the first movement often incorporated fanfare-like elements and took on the pattern of so-called \"sonatina form\" (sonata form without a development section), and the slow section became more extended and lyrical. Italian overtures were often detached from their operas and played as independent concert pieces. In this context, they became important in the early history of the symphony.\n18th century.\nPrior to the 18th century, the symphony and the overture were almost interchangeable, with overtures being extracted from operas to serve as stand-alone instrumental works, and symphonies being tagged to the front of operas as overtures. With the reform of \"opera seria,\" the overture began to distinguish itself from the symphony, and composers began to link the content of overtures to their operas dramatically and emotionally. Elements from the opera are foreshadowed in the overture, following the reform ideology that the music and every other element on stages serves to enhance the plot. One such overture was that of \"La Magnifique\" by Andr\u00e9-Ernest-Modeste Gr\u00e9try, in which several of the arias are quoted. This \"medley form\" persists in the overtures to many works of musical theatre written in the 20th and 21st centuries.\n19th-century opera.\nIn 19th-century opera the overture, \"Vorspiel\", \"Einleitung,\" Introduction, or whatever else it may be called, is generally nothing more definite than that portion of the music which takes place before the curtain rises. Richard Wagner's \"Vorspiel\" to \"Lohengrin\" is a short self-contained movement founded on the music of the Grail.\nIn Italian opera after about 1800, the \"overture\" became known as the \"sinfonia\". Fisher also notes the term \"Sinfonia avanti l'opera\" (literally, the \"symphony before the opera\") was \"an early term for a sinfonia used to begin an opera, that is, as an overture as opposed to one serving to begin a later section of the work\".\nConcert overture.\nEarly 19th century.\nAlthough by the end of the eighteenth century opera overtures were already beginning to be performed as separate items in the concert hall, the \"concert overture\", intended specifically as an individual concert piece without reference to stage performance and generally based on some literary theme, began to appear early in the Romantic era. Carl Maria von Weber wrote two concert overtures, \"Der Beherrscher der Geister\" ('The Ruler of the Spirits', 1811, a revision of the overture to his unfinished opera \"R\u00fcbezahl\" of 1805), and \"Jubel-Ouvert\u00fcre\" ('Jubilee Overture', 1818, incorporating \"God Save the King\" at its climax).\nHowever, the overture \"A Midsummer Night's Dream\" (1826) by Felix Mendelssohn is generally regarded as the first concert overture. Mendelssohn's other contributions to this genre include his \"Calm Sea and Prosperous Voyage\" overture (1828), his overture \"The Hebrides\" (1830; also known as \"Fingal's Cave\") and the overtures \"Die sch\u00f6ne Melusine\" (\"The Fair Melusine\", 1834) and \"Ruy Blas\" (1839). Other notable early concert overtures were written by Hector Berlioz (e.g., \"Les Francs juges\" (1826), and \"Le corsaire\" (1828)).\nLater 19th century.\nIn the 1850s the concert overture began to be supplanted by the symphonic poem, a form devised by Franz Liszt in several works that began as dramatic overtures. The distinction between the two genres was the freedom to mould the musical form according to external programmatic requirements. The symphonic poem became the preferred form for the more \"progressive\" composers, such as C\u00e9sar Franck, Camille Saint-Sa\u00ebns, Richard Strauss, Alexander Scriabin, and Arnold Schoenberg, while more conservative composers like Anton Rubinstein, Pyotr Ilyich Tchaikovsky, Johannes Brahms, Robert Schumann and Arthur Sullivan remained faithful to the overture.\nIn the age when the symphonic poem had already become popular, Brahms wrote his \"Academic Festival Overture\", Op. 80, as well as his \"Tragic Overture\", Op. 81. An example clearly influenced by the symphonic poem is Tchaikovsky's \"1812 Overture\". His equally well-known \"Romeo and Juliet\" is also labelled a 'fantasy-overture'.\n20th century.\nIn European music after 1900, an example of an overture displaying a connection with the traditional form is Dmitri Shostakovich's \"Festive Overture\", Op. 96 (1954), which is in two linked sections, \"Allegretto\" and \"Presto\" (Temperley 2001). Malcolm Arnold's \"A Grand, Grand Overture\", Op. 57 (1956), is a 20th-century parody of the late 19th century concert overture, scored for an enormous orchestra with organ, additional brass instruments, and obbligato parts for four rifles, three Hoover vacuum cleaners (two uprights in B\u266d, one horizontal with detachable sucker in C), and an electric floor polisher in E\u266d; it is dedicated \"to President Hoover\".\nOne song of the Who's rock opera \"Tommy\" is designated as \"Underture\".\nFilm.\nIn motion pictures, an overture is a piece of music setting the mood for the film before the opening credits start. Famous examples include \"Gone with the Wind\" (1939) and \"Lawrence of Arabia\" (1962). For a comprehensive list, see the list of films with overtures.\nList of standard repertoire.\nSome well-known or commonly played overtures:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nGeneral and cited references.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "44136", "revid": "43660803", "url": "https://en.wikipedia.org/wiki?curid=44136", "title": "John Huston", "text": "American filmmaker (1906\u20131987)\nJohn Marcellus Huston ( ; August 5, 1906\u00a0\u2013 August 28, 1987) was an American film director, screenwriter, and actor. He wrote the screenplays for most of the 37 feature films he directed, many of which are today considered classics. He received numerous accolades including two Academy Awards and three Golden Globe Awards. He also received a star on the Hollywood Walk of Fame in 1960 and the BAFTA Fellowship in 1980.\nSon of actor Walter Huston, he studied and worked as a fine art painter in Paris. He then moved to Mexico and began writing, first plays and short stories, and later working in Los Angeles as a Hollywood screenwriter, and was nominated for several Academy Awards writing for films directed by William Dieterle and Howard Hawks, among others. His directorial debut came with \"The Maltese Falcon\" (1941), which despite its small budget became a commercial and critical hit; he continued to be a successful, if iconoclastic, Hollywood director for the next 45 years.\nHuston directed acclaimed films such as \"The Treasure of the Sierra Madre\" (1948), \"Key Largo\" (1948), \"The Asphalt Jungle\" (1950), \"The African Queen\" (1951), \"Moulin Rouge\" (1952), \"Heaven Knows, Mr. Allison\" (1957), \"The Misfits\" (1961), \"The Night of the Iguana\" (1964), \"Fat City\" (1972), \"The Man Who Would Be King\" (1975), \"Annie\" (1982), \"Prizzi's Honor\" (1985) and \"The Dead\" (1987). During his 46-year career, Huston received 14 Academy Award nominations, winning twice. Huston acted in numerous films, receiving nominations for an Academy Award and a Golden Globe Award for \"The Cardinal\" (1963) and \"Chinatown\" (1974) respectively. He also acted in \"Casino Royale\" (1967), \"Myra Breckinridge\" (1970) and \"Battle for the Planet of the Apes\" (1973). He voiced the wizard Gandalf in \"The Hobbit\" (1977) and \"The Return of the King\" (1980).\nHuston has been referred to as \"a titan\", \"a rebel\", and a \"renaissance man\" in the Hollywood film industry. He traveled widely, settling at various times in France, Mexico, and Ireland. Huston was a citizen of the United States by birth but renounced this to become an Irish citizen and resident in 1964. He eventually returned to the United States, where he lived the rest of his life. He was the father of actress Anjelica Huston, whom he directed to an Oscar win in \"Prizzi's Honor\".\nEarly life.\nJohn Huston was born on August 5, 1906, in Nevada, Missouri. He was the only child of Reah (n\u00e9e Gore) and Canadian-born Walter Huston. His father was an actor, initially in vaudeville, and later in films. His mother worked as a sports editor for various publications but stopped after John was born. Similarly, his father ended his stage acting career for steady employment as a civil engineer, although he returned to stage acting within a few years. He later became highly successful on both Broadway and then in motion pictures. He had Scottish, Scotch-Irish, English and Welsh ancestry.\nHuston's parents divorced in 1913 when he was six years old. For much of his childhood, he lived and studied in boarding schools. During summer vacations, he traveled separately with each of his parents \u00a0\u2013 with his father on vaudeville tours, and with his mother to horse races and other sports events. Young Huston benefited greatly from seeing his father act on stage, and he was later drawn to acting.\nSome critics, such as Lawrence Grobel, surmise that his relationship with his mother may have contributed to his marrying five times, and seeming to have difficulty in maintaining relationships. Grobel wrote, \"When I interviewed some of the women who had loved him, they inevitably referred to his mother as the key to unlocking Huston's psyche.\" According to actress Olivia de Havilland, \"she [his mother] was the central character. I always felt that John was ridden by witches. He seemed pursued by something destructive. If it wasn't his mother, it was his idea of his mother.\"\nAs a child, Huston was often ill; he was treated for an enlarged heart and kidney ailments. He recovered after an extended bedridden stay in Arizona and moved with his mother to Los Angeles, where he attended Abraham Lincoln High School. He dropped out after two years to become a professional boxer. By age 15 he was a top-ranking amateur lightweight boxer in California. He ended his brief boxing career after suffering a broken nose.\nHe also engaged in many interests, including ballet, English and French literature, opera, horseback riding, and studying painting at the Art Students League of Los Angeles. Living in Los Angeles, Huston became infatuated with the new film industry and motion pictures, as a spectator only. To Huston, \"Charlie Chaplin was a god.\"\nHuston returned to New York City to live with his father, who was acting in off-Broadway productions, and had a few small roles. He later remembered that while watching his father rehearse, he became fascinated with the mechanics of acting:\nWhat I learned there, during those weeks of rehearsal, would serve me for the rest of my life.\nAfter a short period of acting on stage, and having undergone surgery, Huston travelled alone to Mexico. During two years there, among other adventures, he obtained a position as an honorary member of the Mexican cavalry. He returned to Los Angeles and married Dorothy Harvey, a girlfriend from high school. Their marriage lasted seven years (1926\u20131933).\nCareer.\n1930\u20131939: Early career and directorial debut.\nDuring his stay in Mexico, Huston wrote a play called \"Frankie and Johnny\", based on the ballad of the same title. After selling it easily, he decided that writing would be a viable career, and he focused on it. His self-esteem was enhanced when H. L. Mencken, editor of the popular magazine \"American Mercury,\" bought two of his stories, \"Fool\" and \"Figures of Fighting Men.\" During subsequent years, Huston's stories and feature articles were published in \"Esquire,\" \"Theatre Arts,\" and \"The New York Times.\" He also worked for a period on the \"New York Graphic.\" In 1931, when he was 25, he moved back to Los Angeles in hopes of writing for the blossoming film industry. The silent films had given way to \"talkies\", and writers were in demand. His father had earlier moved there and already gained success in a number of films.\nHuston received a script editing contract with Samuel Goldwyn Productions but, after six months of receiving no assignments, quit to work for Universal Studios, where his father was a star. At Universal, he got a job in the script department, and began by writing dialogue for a number of films in 1932, including \"Murders in the Rue Morgue\", \"A House Divided\", and \"Law and Order\". The last two also starred his father, Walter Huston. \"A House Divided\" was directed by William Wyler, who gave Huston his first real \"inside view\" of the filmmaking process during all stages of production. Wyler and Huston became close friends and collaborators on a number of leading films.\nHuston gained a reputation as a \"lusty, hard-drinking libertine\" during his first years as a writer in Hollywood. Huston described those years as a \"series of misadventures and disappointments\". In 1933 he was in a romantic relationship with actress Zita Johann. While driving drunk, with Johann as passenger, he hit a parked car sending Johann through the glass windshield. She suffered head trauma and Huston was charged with driving while intoxicated. His brief career as a Hollywood writer ended suddenly when he struck and killed actress Tosca Roulien, wife of actor Raul Roulien, while driving. There is a rumor that actor Clark Gable was responsible for the accident, but that MGM general manager Eddie Mannix paid Huston to take the blame. Gable was on location filming a movie, however, proving that rumor untrue. A coroner's jury absolved Huston of blame, but the incident left him \"traumatized\". He moved to London and Paris, living as a \"drifter.\"\nBy 1937, the 31-year-old Huston returned to Hollywood intent on being a \"serious writer.\" He married again, to Lesley Black. His first job was as scriptwriter with Warner Brothers Studio, and he formed his personal longterm goal to direct his own scripts. For the next four years, he co-wrote scripts for major films such as \"Jezebel, The Amazing Dr. Clitterhouse\", \"Juarez\", \"Dr. Ehrlich's Magic Bullet,\" and \"Sergeant York\" (1941). He was nominated for Academy Awards for his screenplays for both \"Ehrlich\" and \"Sergeant York.\" Huston wrote that \"Sergeant York\", which was directed by Howard Hawks, has \"gone down as one of Howard's best pictures, and Gary Cooper had a triumph playing the young mountaineer.\"\nHuston was recognized and respected as a screenwriter. He persuaded the Warners to give him a chance to direct, under the condition that his next script also became a hit.\nHuston wrote:\nThey indulged me rather. They liked my work as a writer and they wanted to keep me on. If I wanted to direct, why, they'd give me a shot at it, and if it didn't come off all that well, they wouldn't be too disappointed as it was to be a very small picture.\nHis next script was \"High Sierra\" (1941), to be directed by Raoul Walsh. The film became the hit Huston wanted. It also made Humphrey Bogart a star with his first major role, as a gunman on the run. Warners kept their end of the bargain and gave Huston his choice of subject. For his first directing assignment, Huston chose Dashiell Hammett's detective thriller, \"The Maltese Falcon\", a film which failed at the box office in two earlier versions by Warners. However, studio head Jack L. Warner approved of Huston's treatment of Hammett's 1930 novel, and he stood by his word to let Huston choose his first subject.\nHuston kept the screenplay close to the novel, keeping much of Hammett's dialogue, and directing it in an uncluttered style, much like the book's narrative. He did unusual preparation for his first directing job by sketching out each shot beforehand, including camera positions, lighting, and compositional scale, for such elements as closeups.\nHe especially benefited by selecting a superior cast, giving Humphrey Bogart the lead role. Bogart was happy to take the role, as he liked working with Huston. The supporting cast included other noted actors: Mary Astor, Peter Lorre, Sydney Greenstreet (his first film role), and his own father, Walter Huston. The film was given only a small B-movie budget, and received minimal publicity by Warners, as they had low expectations. The entire film was made in eight weeks for only $300,000.\nWarners was surprised by the immediate enthusiastic response by the public and critics, who hailed the film as a \"classic\", with many ranking it as the \"best detective melodrama ever made.\" \"Herald Tribune\" critic Howard Barnes called it a \"triumph.\" Huston received an Academy Award nomination for the screenplay. After this film, Huston directed all of his screenplays, except for one, \"Three Strangers\" (1946). In 1942, he directed two more hits, \"In This Our Life\" (1942), starring Bette Davis, and \"Across the Pacific\", another thriller starring Humphrey Bogart.\n1942\u20131946: Army years during World War II.\nIn 1942 Huston served in the United States Army during World War II, making films for the Army Signal Corps. While in uniform with the rank of captain, he directed and produced three films that some critics rank as \"among the finest made about World War II: \"Report from the Aleutians\" (1943), about soldiers preparing for combat; \"The Battle of San Pietro\" (1945), the story (censored by the Army) of a failure by America's intelligence agencies that resulted in many deaths, and \"Let There Be Light\" (1946), about psychologically damaged veterans. It was censored and suppressed for 35 years, until 1981.\nHuston was promoted to the rank of major and received the Legion of Merit award for \"courageous work under battle conditions.\" All of his films made for the Army were \"controversial\", and were either not released, were censored, or banned outright, as they were considered \"demoralizing\" to soldiers and the public. \"Let There Be Light\" was the most controversial as the Army banned the film from public viewing due to the ethics of filming the soldiers' recovery and the lack of written permission supplied by Huston. Years later, after Huston moved to Ireland, his daughter, actress Anjelica Huston, recalled that the \"main movies we watched were the war documentaries.\"\nHuston performed an uncredited rewrite of Anthony Veiller's screenplay for \"The Stranger\" (1946), a film he was to have directed. When Huston became unavailable, the film's star, Orson Welles, directed instead; Welles had the lead role of a high-ranking Nazi fugitive who settles in New England under an assumed name.\n1947\u20131951: Breakthrough and acclaim.\n \"The Treasure of the Sierra Madre\" (1948)\nHuston's next picture, which he wrote, directed, and briefly appeared in as an American asked to \"help out a fellow American, down on his luck\", was \"The Treasure of the Sierra Madre\" (1948). It would become one of the films that established his reputation as a leading filmmaker. The film, also starring Humphrey Bogart, was the story of three drifters who band together to prospect for gold. Huston gave a supporting role to his father, Walter Huston.\nWarners studio was initially uncertain what to make of the film. They had allowed Huston to film on location in Mexico, which was a \"radical move\" for a studio at the time. They also knew that Huston was gaining a reputation as \"one of the wild men of Hollywood.\" In any case, studio boss Jack L. Warner initially \"detested it.\" But whatever doubts Warners had were soon removed, as the film achieved widespread public and critical acclaim. Hollywood writer James Agee called it \"one of the most beautiful and visually alive movies I have ever seen.\" \"Time\" magazine described it as \"one of the best things Hollywood has done since it learned to talk.\" Huston won Oscars for Best Director and Best Adapted Screenplay; his father won for Best Supporting Actor. The film also won other awards in the U.S. and overseas.\nDecades later, \"Film Comment\" magazine devoted four pages to the film in its May\u2013June 1980 edition, with author Richard T. Jameson offering his impressions:\nThis film has impressed itself on the heart and mind and soul of anyone who has seen it, to the extent that filmmakers of great originality and distinctiveness like Robert Altman and Sam Peckinpah can be said to have remade it again and again\u00a0... without compromising its uniqueness.\n \"Key Largo\" (1948)\nAlso in 1948, Huston directed \"Key Largo\", again starring Humphrey Bogart. It was the story about a disillusioned veteran who clashes with gangsters on a remote Florida key. It co-starred Lauren Bacall, Claire Trevor, Edward G. Robinson, and Lionel Barrymore. The film was an adaptation of the stage play by Maxwell Anderson. Some viewers complained that it was still overly stage-bound. But the \"outstanding performances\" by all the actors saved the film, and Claire Trevor won an Oscar for best supporting actress. Huston was annoyed that the studio cut several scenes from the final release without his agreement. That, along with some earlier disputes, angered Huston enough that he left the studio when his contract expired.\n \"The Asphalt Jungle\" (1950)\nIn 1950 he wrote and directed \"The Asphalt Jungle\", a film which broke new ground by depicting criminals as somewhat sympathetic characters, simply doing their professional work, \"an occupation like any other\". Huston described their work as \"a left-handed form of human endeavor.\" Huston achieved that effect by giving \"deep attention\" to the plot, involving a large jewelry theft, by examining the minute, step-by-step details and difficulties each of the characters had of carrying it out. Some critics felt that, by this technique, Huston had achieved an almost \"documentary\" style.\nHis assistant director Albert Band explains further:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I'll never forget it. We got on that set and he composed a shot in which ten elements were working all at the same time. Took half a day to do it, but it was fantastic. He knew \"exactly\" how to shoot a picture. His shots were all painted on the spot ... He had a great eye and he never lost his sense of composition.\nFilm critic Andrew Sarris considered it to be \"Huston's best film\", and the film that made Marilyn Monroe a recognized actress. Sarris also notes the similar themes in many of Huston's films, as exemplified by this one: \"His protagonists almost invariably fail at what they set out to do.\" This theme was also expressed in \"Treasure of the Sierra Madre\", where the group foundered on their own greed.\nIt starred Sterling Hayden and Sam Jaffe, a personal friend of Huston. Marilyn Monroe had her first serious role in this film. Huston said, \"it was, of course, where Marilyn Monroe got her start.\" Monroe said Huston was the first genius she had ever met; and he made her feel that she finally had a chance of becoming a professional actress:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe film succeeded at the box office, and Huston was again nominated for an Oscar for best screenplay and best director, along with winning the Screen Directors Guild Award. This became a model for many similar movies by other filmmakers.\n \"The Red Badge of Courage\" (1951)\nHuston's next film, \"The Red Badge of Courage\" (1951), was of a completely different subject: war and its effect on soldiers. While in the army during World War II, he became interested in Stephen Crane's classic American Civil War novel of the same title. For the starring role, Huston chose World War II hero Audie Murphy to play the young Union soldier who deserts his company out of fear, but later returns to fight alongside them. MGM was concerned that the movie seemed too antiwar for the postwar period. Without Huston's input, they cut down the running time of the film from eighty-eight minutes to sixty-nine, added narration, and deleted what Huston felt was a crucial scene.\nThe film performed poorly at the box office. Huston suggests that it was possibly because it \"brought war very close to home.\" Huston recalls that at the preview showing, before the film was halfway through, \"damn near a third of the audience got up and walked out of the theater.\" Despite the \"butchering\" and weak public response, film historian Michael Barson describes the movie as \"a minor masterpiece.\"\nAt the same time, the film was also the cause of a growing feud between MGM founder Louis B. Mayer and Producer Dore Schary to the point where Huston felt like stepping down to avoid growing the conflict. However, Mayer encouraged Huston to stay on telling him to fight for the picture regardless of what he thought of it.\n \"The African Queen\" (1951)\nBefore \"The Red Badge of Courage\" opened in theaters, Huston was already in Africa shooting \"The African Queen\" (1951), a story based on C. S. Forester's popular novel. It starred Humphrey Bogart and Katharine Hepburn in a combination of romance, comedy and adventure. Barson calls it \"one of the most popular Hollywood movies of all time.\" The film's producer, Sam Spiegel, urged Huston to change the ending to allow the protagonists to survive, instead of dying. Huston agreed, and the ending was rewritten. It became Huston's most successful film financially, and \"it remains one of his finest works.\" Huston was nominated for two Academy Awards\u2014Best Director and Best Adapted Screenplay. Bogart, meanwhile, won his only Oscar for Best Actor for his role as Charlie Allnut.\nHepburn wrote about her experiences shooting the film in her memoir, \"The Making of the African Queen: Or How I went to Africa with Bogart, Bacall, and Huston and almost lost my mind\". Clint Eastwood directed and starred in the film \"White Hunter, Black Heart\", based on Peter Viertel's novel of the same name, which tells a fictional version of the making of the film.\n1952\u20131966: HUAC period.\nIn 1952 Huston moved to Ireland as a result of his \"disgust\" at the \"witch-hunt\" and the \"moral rot\" he felt was created by investigation and hearings by the House Committee on Un-American Activities (HCUA), which had affected many of his friends in the movie industry. Huston had, with friends including director William Wyler and screenwriter Philip Dunne, established the \"Committee for the First Amendment\", as a response to the ongoing government investigations into communists within the film industry. The HCUA was calling numerous filmmakers, screenwriters, and actors to testify about any past affiliations.\nHe later described, in general, the types of people who were alleged communists:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The people who did get caught up in it were, for the most part, well-intentioned boobs from a poor background. A number of them had come from the Lower East Side of Manhattan, and out in Hollywood, they sort of felt guilty for living the good life. Their social conscience was more acute than the next fellow's.\nHuston took producing, writing, and directing credits for his next two films: \"Moulin Rouge\" (1952); and \"Beat the Devil\" (1953). \"Moby Dick\" (1956), however, was written by Ray Bradbury, although Huston had his name added to the screenplay credit after the completion of the project. Although Huston had personally hired Bradbury to adapt Herman Melville's novel into a screenplay, Bradbury and Huston did not get along during pre-production. Bradbury later dramatized their relationship in the short story \"Banshee\". When this was adapted as an episode of \"The Ray Bradbury Theater,\" Peter O'Toole played the role based on John Huston. Bradbury wrote more poems, essays, and stories on his time in Ireland, but was reluctant to write a book because he did not want to gossip about Huston. It was not until after he read Katharine Hepburn's memoir, \"The Making of the African Queen,\" that he decided that he could write \"a book which is fair, which presents the Huston that I loved along with the one that I began to fear on occasion.\" He published \"Green Shadows, White Whale\", a novel about his time in Ireland with Huston, almost 40 years after he wrote the screenplay for \"Moby Dick\".\nHuston had been planning to film Herman Melville's \"Moby-Dick\" for the previous ten years, and originally thought the starring role of Captain Ahab would be an excellent part for his father, Walter Huston. After his father died in 1950, Huston chose Gregory Peck to play the role. The movie was filmed over a three-year period on location in Ireland, where Huston was living. The fishing village of New Bedford, Massachusetts, was recreated along the waterfront; the sailing ship in the film was fully constructed to be seaworthy; and three 100-foot whales were built out of steel, wood, and plastic. In the film, Huston's voice was dubbed for the voice of actor Joseph Tomelty and a Pequod lookout. But the film failed at the box office. Critics such as David Robinson suggested that the movie lacked the \"mysticism of the book\" and thereby \"loses its significance.\"\nOf Huston's next five films, only \"The Misfits\" (1961), gained critical approval. Critics have since noted the \"retrospective atmosphere of doom\" associated with the film. Clark Gable, the star, died of a heart attack a few weeks after the filming was completed; Marilyn Monroe never finished another film, and died a year later after being suspended during the filming of \"Something's Got to Give\"; and costars Montgomery Clift (1966) and Thelma Ritter (1969) also died over the next decade. But two of the \"Misfits\" stars, Eli Wallach and Kevin McCarthy, lived another 50 years. During the filming, Monroe was sometimes taking prescribed drugs, which led to her arriving late on the set. Monroe also sometimes forgot her lines. Monroe's personal problems eventually led to the breakup of her marriage to playwright Arthur Miller, the scriptwriter, \"virtually on set.\" Miller dramatized the making of \"The Misfits\" in his final play, \"Finishing the Picture\", where Huston is represented as the director. Huston later commented about this period in Monroe's career: \"Marilyn was on her way out. Not only of the picture, but of life.\"\nHe followed \"The Misfits\" with \"\", a film quite different from most of his others. Besides directing, he also narrates portions of the story. Film historian Stuart M. Kaminsky notes that Huston presents Sigmund Freud, played by Montgomery Clift, \"as a kind of savior and messiah\", with an \"almost Biblical detachment.\" As the film begins, Huston describes Freud as a \"kind of hero or God on a quest for mankind\":\nThis is the story of Freud's descent into a region as black as hell, man's unconscious, and how he let in the light.\nHuston explains how he became interested in psychotherapy, the subject of the film:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I first got into that through an experience in a hospital during the war, where I made a documentary about patients suffering from battle neuroses. I was in the army and made the picture \"Let There Be Light\". That experience started my interest in psychotherapy, and to this day Freud looms as the single huge figure in that field.\nFor his next film, Huston again traveled to Puerto Vallarta, Mexico, after meeting an architect, Guillermo Wulff, who owned property and businesses in the town. The filming of \"The Night of the Iguana\" took place in a beach cove called Mismaloya, about thirty minutes south of town. Huston adapted the stage play by Tennessee Williams. The film stars Richard Burton and Ava Gardner, and was nominated for several Academy Awards. The production attracted intense worldwide media attention, due to Burton bringing his celebrity mistress, actress Elizabeth Taylor (who was still married to singer Eddie Fisher at the time) to Puerto Vallarta. Huston liked the town where filming took place so much that he bought a house near there, as did Burton and Taylor. Guillermo Wulff and Huston became friends and always spent time together while Huston was in town, more frequently at Wulff's El Dorado Restaurant on Los Muertos Beach.\nProducer Dino De Laurentis traveled to Ireland to ask Huston to direct \"\". Although De Laurentis had ambitions for a broader story, he realized that the subject could not be adequately covered and limited the story to less than the first half of the Book of Genesis. Huston enjoyed directing the film, as it gave him a chance to indulge his love of animals. Besides directing he also played the role of Noah and the voice of God. \"The Bible\" earned rentals of $15 million in North America, making it the second highest-grossing film of 1966. However, because of its bloated budget of $18 million (which made it the most expensive movie of Huston's career), 20th Century Fox ended up losing $1.5 million.\nHuston enjoyed describing details about the filming:\nEvery morning before beginning work, I visited the animals. One of the elephants, Candy, loved to be scratched on the belly behind her foreleg. I'd scratch her and she would lean farther and farther toward me until there was some danger of her toppling over on me. One time I started to walk away from her, and she reached out and took my wrist with her trunk and pulled me back to her side. It was a command: \"Don't stop!\" I used it in the picture. Noah scratches the elephant's belly and walks away, and the elephant pulls him back to her time after time.\n1967\u20131969: Involvement with the Irish film industry.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I think the politicians who supported building the studio can take consolation in the fact that it's brought a lot of money to Ireland. We're spending more than a million dollars in Ireland and we wouldn't be here if it weren't for Ardmore.\u2014\u200a\nWhile working on \"Casino Royale\" (1967), Huston took interest in the Irish film industry, which had historically struggled to attain domestic or international success. There were rumours that he would buy Ireland's premiere film location, Ardmore Studios in Bray, County Wicklow. In 1967, Huston gave Taoiseach Jack Lynch a tour of Ardmore and asked to form a committee to help foster a productive Irish film industry. Huston served on the resulting committee with Irish filmmakers and journalists.\nLynch also ultimately agreed to offer tax breaks to foreign production companies if they shot on location in Ireland, and signed the Film Act of 1970.\nHuston was interviewed in Irish journalist Peter Lennon's \"Rocky Road to Dublin\" (1967), where he argued that it was more important for Irish filmmakers to make films in Ireland than for foreign production companies to make international films.\nIn 1969, he shot \"Sinful Davey\" in Ireland using a mixed Irish and British cast.\n1972\u20131987: Later career and final films.\nAfter several films that were not well received, Huston returned to critical acclaim with \"Fat City\". Based on Leonard Gardner's 1969 novel of the same name, it was about an aging, washed-up alcoholic boxer in Stockton, California, trying to get his name back on the map, while having a new relationship with a world-weary alcoholic. It also featured an amateur boxer trying to find success in boxing. The film was nominated for several awards. It starred Stacy Keach, a young Jeff Bridges, and Susan Tyrrell; she was nominated for an Academy Award for Best Supporting Actress. Roger Ebert stated \"Fat City\" was one of Huston's best films, giving it four out of four stars.\nPerhaps Huston's most highly regarded film of the 1970s, \"The Man Who Would Be King\" was both a critical and commercial success. Huston had been planning to make this film since the '50s, originally with his friends Humphrey Bogart and Clark Gable. Eventually, the lead roles went to Sean Connery and Michael Caine. The movie was partly filmed on location in Morocco and the French Alps. The film was praised for its use of old-fashioned escapism and entertainment. Steven Spielberg has cited the film as one of the inspirations for his film \"Raiders of the Lost Ark\".\nAfter filming \"The Man Who Would Be King\", Huston took his longest break between directing films. He returned with an offbeat and somewhat controversial film based on the novel \"Wise Blood\". Here, Huston showed his skills as a storyteller, and boldness when it came to difficult subjects such as religion. \"Under the Volcano\", Huston's last film set in Mexico, stars Albert Finney as an alcoholic ambassador during the beginnings of World War II. Adapted from the 1947 novel by Malcolm Lowry, the film was highly praised by critics, most notably for Finney's portrayal of a desperate and depressed alcoholic. The film was a success on the independent circuit.\nJohn Huston's final film, 1987's \"The Dead\", is an adaptation of the classic short story by James Joyce. This may have been one of Huston's most personal films, due to his citizenship in Ireland and his passion for classic literature. Huston directed most of the film from a wheelchair, as he needed an oxygen tank to breathe during the last few months of his life. The film was nominated for two Academy Awards and was praised by critics. Roger Ebert eventually placed it in his Great Movies list; a section of movies he claimed to be some of the best ever made. Huston died nearly four months before the film's release date. In the 1996 RT\u00c9 documentary \"John Huston: An t-\u00c9ireannach\", Anjelica Huston said that\n\"it was very important for my father to make that film.\" She contends that Huston did not think that it was going to be his last film, but that it was his love letter to Ireland and the Irish.\nAs an actor.\nWhile he had done some stage acting in his youth and had occasionally cast himself in bit parts in his own films. Earlier in his career, he had played bit parts in his own films, such as the unnamed rich American in \"The Treasure of the Sierra Madre\". Towards the end of his career, Huston began to play more prominent roles in films by other directors. In 1963, director Otto Preminger asked if he would portray a Boston prelate in \"The Cardinal\", and, writes author Philip Kemp, he \"virtually stole the picture.\" He was nominated for an Academy Award for Best Supporting Actor for his role. He had a little participation (as did many others) in 1967's \"Casino Royale\" as actor and director. He acted in Roman Polanski's \"Chinatown\" (1974) as the film's master villain, Noah Cross. For his performance he earned a nomination for the Golden Globe Award for Best Supporting Actor \u2013 Motion Picture. He also acted as President Teddy Roosevelt's secretary of state John Hay in \"The Wind and the Lion\". Huston enjoyed acting and denied that he took it all that seriously. \"It's a cinch,\" he once said, \"and they pay you damn near as much as you make directing.\"\nHe continued to take prominent supporting roles for the next two decades, including 1974's \"Chinatown\" (directed by Roman Polanski), and he lent his booming baritone voice as a voice actor and narrator to a number of prominent films. His last two films, 1985's \"Prizzi's Honor\", and 1987's \"The Dead\", filmed while he was in failing health at the end of his life, were both nominated for multiple Academy Awards. He died shortly after completing his last film. Huston said he did not regard himself very highly as an actor, saying he was proud only of his performance in \"Chinatown.\" But he had also greatly enjoyed acting in \"Winter Kills\". He also played the Lawgiver in \"Battle for the Planet of the Apes\".\nHuston is famous to a generation of fans of J. R. R. Tolkien's Middle-earth stories as the voice of the wizard Gandalf in the Rankin/Bass animated adaptations of \"The Hobbit\" (1977) and \"The Return of the King\" (1980).\nHuston played the lead in Orson Welles's last completed film, \"The Other Side of the Wind\". In it he played an aging filmmaker named Jake Hannaford who was having great problems getting financing for his latest uncompleted film. Much of his portrayal was filmed in the spring of 1974 in Carefree, Arizona, at Southwestern Studio and a nearby mansion. But due to political and financial complications, \"The Other Side of the Wind\" was not released until the fall of 2018.\nMovie themes.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nI miss the order that old Hollywood had. It was much easier then to get a picture made than it is today. It's become a clich\u00e9 that the studio people were picture makers then, but there is a large element of truth in it. They were people who wanted to make pictures, and they knew how to make them. They weren't accountants and bookkeepers, tax consultants and efficiency experts who don't know how to make pictures, or wheeler-dealers; that element just seems to have taken over today\u2014promoters who just want to get a part of the action rather than people who want to make good movies.\n\u2014John Huston, \"Playboy\" interview, 1985\nAuthor Ian Freer describes him as \"cinema's Ernest Hemingway\"\u2014a filmmaker who was \"never afraid to tackle tough issues head on.\" Huston's films were insightful about human nature and human predicaments. They also sometimes included scenes or brief dialogue passages that were remarkably prescient concerning environmental issues that came to public awareness in the future, in the period starting about 1970; examples include \"The Misfits\" and \"The Night of the Iguana\" (1964). Huston spent long evenings carousing in the Nevada casinos after filming, surrounded by reporters and beautiful women, gambling, drinking, and smoking cigars.\nAccording to Kaminsky, Huston's stories were often about \"failed quests\" by a group of different people. The group would persist in the face of poor odds, doomed at the outset by the circumstances created by an impossible situation. However, some members of the doomed group usually survive, those who are \"cool\" and \"intelligent\", or someone who \"will sacrifice everything for self-understanding and independence\". Those types of characters are exemplified by Bogart in \"The Maltese Falcon\", and Montgomery Clift in \"Freud.\"\nAnother type of quest often seen in Huston's films involves a pair of potential lovers trying to face a hostile world. Flint adds, however, that he \"bucked Hollywood's penchant for happy endings\", and many of his stories ended with \"love unsatisfied\".\nFilm historian James Goodwin adds that in virtually all of his films, there is some type of \"heroic quest\u00a0\u2013 even if it involves questionable motives or destructive alliances\". In addition, the quest \"is preferable to the spiritless, amoral routines of life\". As a result, his best films, according to Flint, \"have lean, fast-paced scripts and vibrant plots and characterizations, and many of them deal ironically with vanity, avarice and unfulfilled quests\".\nIn the opinion of critics Tony Tracy and Roddy Flynn, \"...\u00a0what fundamentally fascinated Huston was not movies \"per se\"\u00a0\u2013 that is, form\u00a0\u2013 but the human condition\u00a0... and literature offered a road map for exploring that condition.\" In many of his films, therefore, he tried to express his interest by developing themes involving some of the \"grand narratives\" of the twentieth century, such as \"faith, meaning, truth, freedom, psychology, colonialism, war and capitalism\".\nTo Jameson, all of Huston's films are adaptations, and he believes that through his films there was a \"cohesive world-view, not only thematically but also stylistically; there is the Huston look\". The \"Huston look\" was also noted by screenwriter James Agee, who adds that this \"look proceeds from Huston's sense of what is natural to the eye and his delicate, simple feeling for space relationships.\" In any case, notes Flint, Huston took \"uncommon care to preserve the writer's styles and values\u00a0... and sought repeatedly to transpose the interior essence of literature to film with dramatic and visual tension\", as he did in \"Red Badge of Courage,\" \"Moby Dick,\" and \"Under the Volcano.\"\nReligion is also a theme that runs through many of Huston's films. In \"The Night of the Iguana,\" Kaminsky notes how Richard Burton, while preaching a sermon to his congregation, seems \"lost, confused, his speech is gibberish\", and leads his congregation to turn away from him. In other films, adds Kaminsky, religion is seen as \"part of the fantasy world\", that the actors must overcome to survive physically or emotionally. \"These religious zealots counsel a move away from the pleasure of the world and human love, a world that Huston believes in,\" concludes Kaminsky. Such religious themes were also seen in \"The Bible,\" and \"Wise Blood,\" for example.\nTo Barson, however, Huston was among the \"least consistent\" filmmakers, although he concludes that he was one of the \"most interesting directors of the past sixty years\". Throughout his long career, many of his films did poorly and were criticized as a result. To a writer in 1972 he commented, \"Criticism isn't a new experience for me. Pictures that are now thought of as, forgive the term, classics, weren't all that well thought of at the time they came out.\" After an interview a few years before he died, the reporter writes that \"Huston said he missed the major studio era when people savored making movies, not just money.\"\nAccording to Roger Ebert, in his review of \"Fat City\", \"His fascination with underdogs and losers. The characters in Huston movies hardly ever set out to achieve what they're aiming for. Sam Spade, in \"The Maltese Falcon\", Huston's first film, ends up minus one partner and one woman he thought he could trust. Everyone is a loser in \"The Treasure of the Sierra Madre\", and the gold blows back into the dust and is lost in it. Ahab, in \"Moby Dick\". Marlon Brando's career Army officer in \"Reflections in a Golden Eye\", even Bogart and Hepburn in \"The African Queen\" \u2013 they all fall short of their plans. \"The African Queen\" does have a happy ending, but it feels tacked-on and ridiculous, and the Queen destroys itself in destroying the German steamer. So this \"[Fat City]\" is a theme we find in Huston's work, but rarely does he fit it to characters and a time and place so well as in \"Fat City\". Maybe that's because Huston knows the territory: he was a professional boxer himself for a while, and not a very good one.\"\nDirecting techniques.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nJohn has meant a great deal in my life. Nobody would have heard of me if it hadn't been for him. Working with John ten years later is very good. He's a different kind of director than the people I've been working with. He's an artist with a camera\u2014he sees it like a painter.\n \u2014Marilyn Monroe\nHe explored the visual aspects of his films throughout his career, sketching each scene on paper beforehand, then carefully framing his characters during the shooting. Some of Huston's films were adaptations of important novels, often depicting a \"heroic quest\", as in \"Moby Dick\", or \"The Red Badge of Courage\". In many films, different groups of people, while struggling toward a common goal, would become doomed, forming \"destructive alliances\", giving the films a dramatic and visual tension. Many of his films involved themes such as religion, meaning, truth, freedom, psychology, colonialism, and war.\nGeorge Stevens, Jr. notes that while many directors rely on post-production editing to shape their final work, Huston instead created his films while they were being shot: \"I don't even know the editor of my films most of the time,\" Huston said. Actor Michael Caine also observed the same technique: \"Most directors don't know what they want so they shoot everything they can think of\u00a0\u2014 they use the camera like a machine gun. John uses it like a sniper.\" Danny Huston confirmed as much when he recalled what Huston said to him as the then-youngster was fooling around with a Kodak Super 8: \"and I was shooting all these various things. He said, 'Stop it, stop doing that.' I said, 'What?' He said, 'When you go from left to right and right to left, what do you do?' So I looked from left to right and right to left. I said, 'I give up. What do I do?' He said, 'You blink. That's a cut.'\"\nFilm writer Peter Flint pointed out other benefits to Huston's style: \"He shot economically, eschewing the many protective shots favored by timid directors, and edited cerebrally so that financial backers would have trouble trying to cut scenes.\" Huston shot most of his films on location, working \"intensely\" six days a week, and \"on Sundays, played equally intense poker with the cast and crew.\"\nWhen asked how he envisions his films while directing and what his goals are, Huston replied:\nTo me the ideal film\u00a0\u2014 which I've never succeeded in making\u00a0\u2014 would be as though the reel were behind one's eyes and you were projecting it yourself, seeing what you wish to see. This has a great deal in common with thought processes\u00a0... That's why I think the camera is an eye as well as a mind. Everything we do with the camera has physiological and mental significance.\nAccording to Kaminsky, much of Huston's vision probably came from his early experience as a painter on the streets of Paris. While there, he studied art and worked at it for a year and a half. Huston continued painting as a hobby for most of his life. Kaminsky also notes that most of Huston's films \"reflected this prime interest in the image, the moving portrait and the use of color.\" Huston explored the use of \"stylistic framing\", especially well-planned close-ups, in much of his directing. In his first film, \"The Maltese Falcon\", for instance, Huston sketched out all of his scenes beforehand, \"like canvases of paintings\". Anjelica Huston recalled that even for his subsequent films, he sketched storyboards \"constantly... it was a form of study, and my father was a painter, a very good one... there was an extremely developed sensory quality about my father, he didn't miss a trick.\"\nPersonal life.\nTo producer George Stevens Jr., Huston symbolized \"intellect, charm and physical grace\" within the film industry. He adds, \"He was the most charismatic of the directors I knew, speaking with a soothing, melodic voice that was often mimicked, but was unique to him.\" Actor Richard Burton, whom Huston directed in \"The Night of the Iguana\", opined in his diaries that \"Huston is a simpleton. But believes himself to be a genius. And a self aggrandizing liar.\" Huston loved the outdoors, especially hunting while living in Ireland. Among his life's adventures before becoming a Hollywood filmmaker, he had been an amateur boxer, reporter, short-story writer, portrait artist in Paris, a cavalry rider in Mexico, and a documentary filmmaker during World War II. Besides sports and adventure, he enjoyed hard liquor and relationships with women. Stevens describes him as someone who \"lived life to its fullest\". Barson even suggests that Huston's \"flamboyant life\" as a rebel would possibly make for \"an even more engaging tale than most of his movies\".\n1933 car accident.\nWhile driving on Sunset Boulevard on September 25, 1933, Huston struck and killed a pedestrian, a Brazilian dancer named Tosca Roulien, wife of Raul Roulien. The resulting media frenzy forced Huston to retreat temporarily from public performance and instead work as a screenwriter. A subsequent inquest absolved Huston of any blame for the accident. \nAbout six months prior to this accident, while driving drunk, Huston crashed into a parked car injuring his passenger and his partner at the time, actress Zita Johann. Johann suffered head trauma as she was thrown through the windshield. Huston was charged with driving while intoxicated.\nBeliefs.\nIt has been suggested that John Huston was an atheist, but his religious beliefs are hard to determine. He claimed that he had no orthodox religion. His daughter, Anjelica, was raised Roman Catholic, and said that he did not like Hollywood, and \"especially despised Beverly Hills\u00a0... he thought it was just fake from the ground up. He didn't like any of that; he was not intrigued or attracted by it.\" She said that, in contrast, \"he liked to be in the wild places; he liked animals as much as he liked people.\"\nMarriages.\nHuston married five times. His wives were:\nHuston visited Ireland in 1951 and stayed at Luggala, County Wicklow, the home of Garech Browne, a member of the Guinness family. He visited Ireland several times afterwards and on one of these visits, he purchased and restored a Georgian home, St Clerans, of Craughwell, County Galway. Between 1960 and 1971 he served as Master of Fox Hounds (MFH) of the County Galway Hunt, whose kennels are at Craughwell. He renounced his U.S. citizenship and became an Irish citizen in 1964. His daughter Anjelica attended school in Ireland at Kylemore Abbey for a number of years. A film school is now dedicated to him on the NUI Galway campus.\nPainting.\nHuston was an accomplished painter who wrote in his autobiography, \"Nothing has played a more important role in my life\". As a young man, he studied at the Smith School of Art in Los Angeles but dropped out within a few months. He later studied at the Art Students League of New York. He painted throughout his life and had studios in each of his homes. One of the last pictures that Huston painted, his colorful watercolor including a ram and a bunch of grapes, along with his handwritten inscription, \"In celebration of my beloved friend Baron Philippe's 60th harvest at Mouton -- John Huston,\" illustrated the label of the 1982 vintage of Ch\u00e2teau Mouton Rothschild, in one of the greatest years of one of the world's greatest wines, putting him in the company of the renowned artists who illustrated that wine's label in other years. He had owned a wide collection of art, including a notable collection of Pre-Columbian art.\nIllness and death.\nA heavy smoker, Huston was diagnosed with emphysema in 1978. By the last year of his life he could not breathe for more than twenty minutes without needing oxygen. He died on August 28, 1987, in his rented home in Middletown, Rhode Island, from pneumonia as a complication of lung disease, aged 81. Huston is interred in the Hollywood Forever Cemetery in Hollywood with his mother.\nArchives.\nThe moving image collection of John Huston is held at the Academy Film Archive. The film material at the Academy Film Archive is complemented by production files, photographs, and personal correspondence found in the John Huston papers, 1932\u20131981, at the Academy's Margaret Herrick Library. The film archive preserved several of John Huston's home movies in 2001.\nAwards and honors.\nHuston received 15 Oscar nominations in the course of his career and became the oldest person ever to be nominated for the Best Director Oscar when, at 79 years old, he was nominated for \"Prizzi's Honor\" (1985), a record he would hold until 2023, when Martin Scorsese attained a nomination at the age of 81 for directing \"Killers of the Flower Moon\". Huston won two Oscars, for directing and writing the screenplay for \"The Treasure of the Sierra Madre\". Huston also won a Golden Globe for that film. He received the Life Achievement Award from the American Film Institute in 1983, and the Career Achievement Award from the U.S. National Board of Review of Motion Pictures in 1984.\nHe also has the unique distinction of directing both his father Walter and his daughter Anjelica in Oscar-winning performances (in \"The Treasure of the Sierra Madre\" and \"Prizzi's Honor\", respectively), making the Hustons the first family to have three generations of Academy Award winners. He also directed her in \"Sinful Davey\" in 1969.\nIn addition, he also directed 13 other actors in Oscar-nominated performances: Sydney Greenstreet, Claire Trevor, Sam Jaffe, Humphrey Bogart, Katharine Hepburn, Jos\u00e9 Ferrer, Colette Marchand, Deborah Kerr, Grayson Hall, Susan Tyrrell, Albert Finney, Jack Nicholson and William Hickey.\nIn 1960, Huston was honored with a star on the Hollywood Walk of Fame for his contribution to motion pictures. In 1965, Huston received the Laurel Award for Screenwriting Achievement from the Writers Guild of America. In 1981, his film \"Escape to Victory\" was nominated for the Golden Prize at the 12th Moscow International Film Festival. A statue of Huston, sitting in his director's chair, stands in Plaza John Huston in Puerto Vallarta, Mexico.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44138", "revid": "724850", "url": "https://en.wikipedia.org/wiki?curid=44138", "title": "Cantata", "text": "Vocal composition with an instrumental accompaniment\nA cantata (; ; literally \"sung\", past participle feminine singular of the Italian verb \"cantare\", \"to sing\") is a vocal composition with an instrumental accompaniment, typically in several movements, often involving a choir.\nThe meaning of the term changed over time, from the simple single-voice madrigal of the early 17th century, to the multi-voice \"cantata da camera\" and the \"cantata da chiesa\" of the later part of that century, from the more substantial dramatic forms of the 18th century to the usually sacred-texted 19th-century cantata, which was effectively a type of short oratorio. Cantatas for use in the liturgy of church services are called church cantata or sacred cantatas; other cantatas can be indicated as secular cantatas. Several cantatas were, and still are, written for special occasions, such as Christmas cantatas. Christoph Graupner, Georg Philipp Telemann and Johann Sebastian Bach composed cycles of church cantatas for the occasions of the liturgical year.\nHistorical context.\nThe term originated in the early 17th century, simultaneously with opera and oratorio. Prior to that, all \"cultured\" music was vocal. With the rise of instrumental music, the term appeared, while instrumental art became sufficiently developed to be embodied in sonatas. From the beginning of the 17th century until late in the 18th, the cantata for one or two solo voices with accompaniment of basso continuo (and perhaps a few solo instruments) was a principal form of Italian vocal chamber music.\nA cantata consisted first of a declamatory narrative or scene in recitative, held together by a primitive aria repeated at intervals. Fine examples may be found in the church music of Giacomo Carissimi; and the English vocal solos of Henry Purcell (such as \"Mad Tom\" and \"Mad Bess\") show the utmost that can be made of this archaic form. With the rise of the da capo aria, the cantata became a group of two or three arias joined by recitative. George Frideric Handel's numerous Italian duets and trios are examples on a rather large scale. His Latin motet \"Silete Venti\", for soprano solo, shows the use of this form in church music.\nDifferences from other musical forms.\nThe Italian solo cantata tended, when on a large scale, to become indistinguishable from a scene in an opera, in the same way the church cantata, solo or choral, is indistinguishable from a small oratorio or portion of an oratorio. This is equally evident whether one examines the church cantatas of Bach, of which nearly 200 are extant (see List of Bach cantatas) or the \"Chandos Anthems\" of Handel. In Johann Sebastian Bach's case, some of the larger cantatas are actually called oratorios, and the \"Christmas Oratorio\" is a collection of six church cantatas actually intended for performance on six different days, though together they form as complete an artistic whole as any classical oratorio.\nBaroque.\nDuring the baroque era, the term \"cantata\" generally retained its original Italian usage to describe a secular vocal piece of extended length, often in different sections, and usually Italianate in style. At the same time, vocal pieces of similar scope, often with several singers, and various instruments, were in great demand for the services of the Lutheran church. Such pieces were usually called \"geistliche Konzerte\" (singular: \"geistliches Konzert\", meaning sacred concerto). Many of these pieces were simply called by their opening text. Such pieces for the liturgy or other occasions were not only composed by Bach but also by Dieterich Buxtehude, Christoph Graupner, Gottfried Heinrich St\u00f6lzel and Georg Philipp Telemann, to name a few. The editors of the Bach Gesellschaft adopted \"sacred cantata\" as a convenient catchall for most of Bach's liturgical pieces. The term was then retroactively applied by Philipp Spitta to refer to comparable works by composers from Heinrich Sch\u00fctz onwards. Many secular cantatas were composed for events in the nobility. They were so similar in form to the sacred ones that many of them were parodied (in parts or completely) to sacred cantatas, for example in Bach's \"Christmas Oratorio\".\nBach cantatas.\nJohann Sebastian Bach, almost 200 of whose cantatas survive, is a notable contributor to the genre.\nHis cantatas are usually written for a baroque orchestra consisting of a string section, an oboe section, and a continuo group, timpani and brass were sometimes added on festive occasions such as Christmas or Easter. The vocal forces consisted of a four-part choir and soloists. Bach also wrote some cantatas for only one solo singer (ex. BWV 51).\nGraupner's cantatas.\nChristoph Graupner was Hofkapellmeister at the court of Hesse-Darmstadt and provided over 1,400 cantatas during his nearly 50 years of employment there, making him the most significant contributor to the genre. While only a handful of Bach's cantatas contain accompanied chorales (the vocal parts are usually doubled by the instrumental parts), nearly all of Graupner's chorales feature elaborate ritornello sections. This is possibly because Bach's Leipzig congregation was expected to sing along with them, but the Darmstadt court was not. Also, many of Graupner's cantatas exploit elaborate orchestral effects and use exotic instrumentation, such as chalumeau, fl\u00fbte d'amour, oboe d'amore, viola d'amore, trumpets, horns and timpani. See: List of cantatas by Christoph Graupner.\nClassical and romantic period.\nThe term \"cantata\" came to be applied almost exclusively to choral works, as distinguished from solo vocal music. In early 19th-century cantatas, the chorus is the vehicle for music more lyric and songlike than in oratorio, not excluding the possibility of a brilliant climax in a fugue as in Ludwig van Beethoven's Der glorreiche Augenblick, Carl Maria von Weber's \"Jubel-Kantate\", and Felix Mendelssohn's \"Die erste Walpurgisnacht\". Anton Bruckner composed several Name-day cantatas, a Festive Cantata and two secular cantatas (\"Germanenzug\" and \"Helgoland\"). Bruckner's Psalm 146 is also in cantata form. Mendelssohn's Symphony Cantata, the \"Lobgesang\", is a hybrid work, partly in the oratorio style. It is preceded by three symphonic movements, a device avowedly suggested by Beethoven's Ninth Symphony; but the analogy is not accurate, as Beethoven's work is a symphony of which the fourth movement is a choral finale of essentially single design, whereas Mendelssohn's \"Symphony Cantata\" is a cantata with three symphonic preludes. The full lyric possibilities of a string of choral songs were realized by Johannes Brahms in his \"Rinaldo\", which, like the \"Walpurgisnacht\"\u2014was set to a text by Goethe. Other cantatas, Beethoven's \"Meeresstille\", works of Brahms and many notable small English choral works, such as cantatas of John Henry Maunder and John Stanley, find various ways to set poetry to choral music. The competition for the French requires that each candidate submit a cantata. Hector Berlioz failed in three attempts before finally winning in 1830 with \"Sardanapale\". While almost all of the cantatas have long since been forgotten (along with their composers, for the most part), Debussy's prize-winning \"L'enfant prodigue\" (1884, following his unsuccessful \"Le gladiateur\" of 1883) is still performed occasionally today. Late in the century, Gustav Mahler wrote his early \"Das klagende Lied\" on his own words between 1878 and 1880, and Samuel Coleridge-Taylor created a successful trilogy of cantatas, \"The Song of Hiawatha\" between 1898 and 1900.\nTwentieth century and beyond.\nCantatas, both of the chamber variety and on a grand scale, were composed after 1900 as well. Indeed, it would not be an exaggeration to claim that one of the most popular pieces of classical music of the 20th century to the layman's ears, is a cantata, namely \"Carmina Burana\" (1935\u20131936) by the German composer Carl Orff.\nIn the early part of the century, secular cantatas once again became prominent, while the 19th-century tradition of sacred cantatas also continued. Ralph Vaughan Williams composed both kinds: \"festival\" cantatas such as \"Toward the Unknown Region\" (1907), \"Five Mystical Songs\" (1911), and \"Five Tudor Portraits\" (1936), and sacred cantatas including \"Sancta civitas\" (1926), \"Benedicite\" (1930), \"Dona nobis pacem\" (1936), and \"Hodie\" (1954). Joseph Ryelandt also composed secular and sacred cantatas, such as \"Le chant de la pauvret\u00e9\" Op.\u00a092 in 1928 and \"Veni creator\" Op.\u00a0123 in 1938. B\u00e9la Bart\u00f3k composed the secular \"Cantata Profana\", subtitled \"The Nine Splendid Stags\" and based on a Romanian folk tale, in 1930. Although it began as a song cycle (as reflected also by its title), Arnold Schoenberg's \"Gurre-Lieder\" (1900\u20131903/1910\u201311) evolved into one of the century's largest secular cantatas. Paul Hindemith composed three works he designated as cantatas: \"Die Serenaden\", Op.\u00a035, for soprano, oboe, viola, and cello (1924), \"Mahnung an die Jugend, sich der Musik zu befleissigen\" (from the \"Pl\u00f6ner Musiktage\", 1932), and \"Ite angeli veloces\" for alto and tenor, mixed chorus, and orchestra, with audience participation (1953\u201355). Of Anton Webern's last three compositions, two are secular cantatas: Cantata No. 1, Op.\u00a029 (1938\u201339), and Cantata No. 2, Op.\u00a031 (1941\u201343), both setting texts by Hildegard Jone. Webern had begun sketching a Third Cantata by the time he was killed in 1945. Ernst Krenek also composed two examples: a \"scenic cantata\", \"Die Zwingburg\", Op.\u00a014 (1922), and a \"Cantata for Wartime\", Op.\u00a095, for women's voices and orchestra (1943). Sergei Prokofiev composed \"Semero ikh\" (1917\u201318; rev. 1933), and in 1939 premiered a cantata drawn from the film music for \"Alexander Nevsky\". He wrote two festival cantatas, the \"Cantata for the Twentieth Anniversary of the October Revolution\", Op.\u00a074, and \"Flourish, Mighty Homeland\", Op.\u00a0114, for the thirtieth anniversary of the same event\nPatriotic cantatas celebrating anniversaries of events in the Revolution or extolling state leaders were frequently commissioned in the Soviet Union between 1930 and the middle of the century, though these occasional works were seldom among their composers' best. Examples include Dmitri Shostakovich's \"Poem of the Motherland\", Op.\u00a047 (1947) and \"The Sun Shines over Our Motherland\", Op.\u00a090 (1952), and three works by Prokofiev, \"Zdravitsa!\" [Hail to Stalin] (1939). Dmitry Kabalevsky also composed four such cantatas, \"The Great Homeland\", Op.\u00a035 (1941\u201342), \"The Song of Morning, Spring and Peace\", Op.\u00a057 (1957\u201358), \"Leninists\", Op.\u00a063 (1959), and \"About Our Native Land\", Op.\u00a082 (1965).\nPatriotic cantatas were also created in China during the civil war and the Second Sino-Japanese War. For example, the Yellow River Cantata was composed in 1939.\nIn 1940, the Brazilian composer Heitor Villa-Lobos created a secular cantata titled \"Mandu \u00e7arar\u00e1\", based on an Indian legend collected by Barbosa Rodrigues. Francis Poulenc composed in 1943 \"Figure humaine\", FP 120, a cantata for double mixed choir of 12 voices on poems by Paul \u00c9luard. Igor Stravinsky composed a work titled simply \"Cantata\" in 1951\u201352, which used stanzas from the 15th-century \"Lyke-wake Dirge\" as a narrative frame for other anonymous English lyrics, and later designated \"A Sermon, a Narrative and a Prayer\" (1961) as \"a cantata for alto and tenor soli, speaker, chorus, and orchestra\". Luigi Nono wrote \"Il canto sospeso\" in 1955\u201356. Hans Werner Henze composed a \"Cantata della fiaba estrema\" and \"Novae de infinito laudes\" (both in 1963), as well as a number of other works that might be regarded as cantatas, such as \"Kammermusik\" (1958, rev. 1963), \"Muzen Siziliens\" (1966), and \"El Cimarr\u00f3n\" (1969\u201370). \"Momente\" (1962\u201364/1969), one of the most important works of Karlheinz Stockhausen, is often described as a cantata. Benjamin Britten composed at least six works he designated as cantatas: \"The Company of Heaven\" (1937), \"Rejoice in the Lamb\", Op.\u00a030 (1943), \"Saint Nicolas\", Op.\u00a042 (1949), the \"Cantata academica\", Op.\u00a062 (1959), the \"Cantata Misericordium\", Op.\u00a069 (1963), and \"Phaedra\", Op.\u00a093 (1975). Alberto Ginastera also composed three works in this form: the \"Cantata para Am\u00e9rica M\u00e1gica\", Op.\u00a027 (1960), \"Bomarzo\", Op.\u00a032 (1964), and \"Milena\", Op.\u00a037 (1971), and Gottfried von Einem composed in 1973 \"An die Nachgeborenen\" based on diverse texts, the title taken from a poem of Bertolt Brecht. Mikis Theodorakis composed the cantatas \"According to the Sadducees\" and \"Canto Olympico\". Herbert Blendinger's \"Media in vita\" was premiered in 1980, his \"Mich ruft zuweilen eine Stille\" (Sometimes a silence calls me) in (1992), and \"Allein den Betern kann es noch gelingen\" (It can only be achieved by those who pray) in 1995. Iv\u00e1n Er\u0151d wrote in 1988/89) \"Vox Lucis\" (Voice of the Light), Op.\u00a056. Ivan Moody wrote in 1995 \"Revelation\".\nCantatas were also composed by Mark Alburger, Erik Bergman, Dave Brubeck, Carlos Ch\u00e1vez, Osvald Chlubna, Peter Maxwell Davies, Norman Dello Joio, Lukas Foss, Roy Harris, Arthur Honegger, Alan Hovhaness, Dmitry Kabalevsky, Libby Larsen, J\u00f3n Leifs, Peter Mennin, Dimitri Nicolau, Krzysztof Penderecki, Allan Pettersson, Daniel Pinkham, Earl Robinson, Ned Rorem, William Schuman (\"A Free Song\"), Roger Sessions, Siegfried Strohbach, Michael Tippett, Kurt Weill, J\u00f6rg Widmann (\"Friedenskantate\", \"Cantata in tempore belli\") and Jan Ryant D\u0159\u00edzal (\"Christmas Cantata\").\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44141", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=44141", "title": "Medieval Climate Optimum", "text": ""}
{"id": "44142", "revid": "13369807", "url": "https://en.wikipedia.org/wiki?curid=44142", "title": "Metric system", "text": "Decimal-based systems of measurement\nThe metric system is a system of measurement that standardises a set of base units and a nomenclature for describing relatively large and small quantities via decimal-based multiplicative unit prefixes. Though the rules governing the metric system have changed over time, the modern definition, the International System of Units (SI), defines the metric prefixes and seven base units: metre (m), kilogram (kg), second (s), ampere (A), kelvin (K), mole (mol), and candela (cd).\nAn SI derived unit is a named combination of base units such as hertz (cycles per second), newton (kg\u22c5m/s2), and tesla (1\u00a0kg\u22c5s\u22122\u22c5A\u22121) and in the case of Celsius a shifted scale from Kelvin. Certain units have been officially accepted for use with the SI. Some of these are decimalised, like the litre and electronvolt, and are considered \"metric\". Others, like the astronomical unit are not. Ancient non-metric but SI-accepted multiples of time, minute and hour, are base 60 (sexagesimal). Similarly, the angular measure degree and submultiples, \narcminute, and arcsecond, are also sexagesimal and SI-accepted.\nThe SI system derives from the older metre, kilogram, second (MKS) system of units, though the definition of the base units has changed over time. Today, all base units are defined by physical constants; not by prototypes in the form of physical objects as they were in the past.\nOther metric system variants include the centimetre\u2013gram\u2013second system of units, the metre\u2013tonne\u2013second system of units, and the gravitational metric system. Each has unaffiliated metric units. Some of these systems are still used in limited contexts.\nAdoption.\nThe SI system has been adopted as the official system of weights and measures by most countries in the world.\nA notable outlier is the United States (US). Although used in some contexts, the US has resisted full adoption; continuing to use different measurement systems.\nAdopting the metric system is known as metrication.\nMultiplicative prefixes.\nIn the SI system and generally in older metric systems, multiples and fractions of a unit can be described via a prefix on a unit name that implies a decimal (base-10), multiplicative factor. The only exceptions are for the SI-accepted units of time (minute and hour) and angle (degree, arcminute, arcsecond) which, based on ancient convention, use base-60 multipliers.\nThe prefix \"kilo\", for example, implies a factor of 1000 (103), and the prefix \"milli\" implies a factor of 1/1000 (10\u22123). Thus, a \"kilometre\" is a thousand metres, and a \"milligram\" is one thousandth of a gram. These relations can be written symbolically as:\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;1\u00a0km = 1000\u00a0m\n&lt;templatestyles src=\"Block indent/styles.css\"/&gt;1\u00a0mg = 0.001\u00a0g\nBase units.\nThe decimalised system is based on the metre, which had been introduced in France in the 1790s. The historical development of these systems culminated in the definition of the International System of Units (SI) in the mid-20th century, under the oversight of an international standards body.\nThe historical evolution of metric systems has resulted in the recognition of several principles. A set of independent dimensions of nature is selected, in terms of which all natural quantities can be expressed, called base quantities. For each of these dimensions, a representative quantity is defined as a base unit of measure. The definition of base units has increasingly been realised in terms of fundamental natural phenomena, in preference to copies of physical artefacts. A unit derived from the base units is used for expressing quantities of dimensions that can be derived from the base dimensions of the system\u2014e.g., the square metre is the derived unit for area, which is derived from length. These derived units are coherent, which means that they involve only products of powers of the base units, without any further factors. For any given quantity whose unit has a name and symbol, an extended set of smaller and larger units is defined that are related by factors of powers of ten. The unit of time should be the second; the unit of length should be either the metre or a decimal multiple of it; and the unit of mass should be the gram or a decimal multiple of it.\nMetric systems have evolved since the 1790s, as science and technology have evolved, in providing a single universal measuring system. Before and in addition to the SI, other metric systems include: the MKS system of units and the MKSA systems, which are the direct forerunners of the SI; the centimetre\u2013gram\u2013second (CGS) system and its subtypes, the CGS electrostatic (cgs-esu) system, the CGS electromagnetic (cgs-emu) system, and their still-popular blend, the Gaussian system; the metre\u2013tonne\u2013second (MTS) system; and the gravitational metric systems, which can be based on either the metre or the centimetre, and either the gram, gram-force, kilogram or kilogram-force.\nAttributes.\nEase of learning and use.\nThe metric system is intended to be easy to use and widely applicable, including units based on the natural world, decimal ratios, prefixes for multiples and sub-multiples, and a structure of base and derived units.\nIt is a coherent system with derived units built from base units using logical rather than empirical relationships and with multiples and submultiples of both units based on decimal factors and identified by a common set of prefixes.\nExtensibility.\nThe metric system is extensible since the governing body reviews, modifies and extends it needs arise. For example, the katal, a derived unit for catalytic activity equivalent to one mole per second (1\u00a0mol/s), was added in 1999.\nRealisation.\nThe base units used in a measurement system must be realisable. To that end, the definition of each SI base unit is accompanied by a \"mise en pratique\" (practical realisation) that describes at least one way that the unit can be measured. Where possible, definitions of the base units were developed so that any laboratory equipped with proper instruments would be able to realise a standard without reliance on an artefact held by another country. In practice, such realisation is done under the auspices of a mutual acceptance arrangement.\nIn 1791 the commission originally defined the metre based on the size of the earth, equal to one ten-millionth of the distance from the equator to the North Pole. In the SI, the standard metre is now defined as exactly of the distance that light travels in a second. The metre can be realised by measuring the length that a light wave travels in a given time, or equivalently by measuring the wavelength of light of a known frequency.\nThe kilogram was originally defined as the mass of one cubic decimetre of water at 4\u00a0\u00b0C, standardised as the mass of a man-made artefact of platinum\u2013iridium held in a laboratory in France, which was used until a new definition was introduced in May 2019. Replicas made in 1879 at the time of the artefact's fabrication and distributed to signatories of the Metre Convention serve as \"de facto\" standards of mass in those countries. Additional replicas have been fabricated since as additional countries have joined the convention. The replicas were subject to periodic validation by comparison to the original, called the IPK. It became apparent that either the IPK or the replicas or both were deteriorating, and are no longer comparable: they had diverged by 50\u00a0\u03bcg since fabrication, so figuratively, the accuracy of the kilogram was no better than 5 parts in a hundred million or a relative accuracy of . The revision of the SI replaced the IPK with an exact definition of the Planck constant as expressed in SI units, which defines the kilogram in terms of fundamental constants.\nBase and derived unit structure.\nA base quantity is one of a conventionally chosen subset of physical quantities, where no quantity in the subset can be expressed in terms of the others. A base unit is a unit adopted for expressing a base quantity. A derived unit is used for expressing any other quantity, and is a product of powers of base units. For example, in the modern metric system, length has the unit metre and time has the unit second, and speed has the derived unit metre per second.15 Density, or mass per unit volume, has the unit kilogram per cubic metre.434\nDecimal ratios.\nA significant characteristic of the metric system is its use of decimal multiples \u2013 powers of 10. For example, a length that is significantly longer or shorter than 1 metre can be represented in units that are a power of 10 or 1000 metres. This differs from many older systems in which the ratio of different units varied. For example, 12 inches is one foot, but the larger unit in the same system, the mile is not a power of 12 feet. It is 5,280 feet \u2013 which is hard to remember for many.17\nIn the early days, multipliers that were positive powers of ten were given Greek-derived prefixes such as \"kilo-\" and \"mega-\", and those that were negative powers of ten were given Latin-derived prefixes such as \"centi-\" and \"milli-\". However, 1935 extensions to the prefix system did not follow this convention: the prefixes \"nano-\" and \"micro-\", for example have Greek roots. During the 19th century the prefix \"myria-\", derived from the Greek word \u03bc\u03cd\u03c1\u03b9\u03bf\u03b9 (\"m\u00fdrioi\"), was used as a multiplier for .\nWhen applying prefixes to derived units of area and volume that are expressed in terms of units of length squared or cubed, the square and cube operators are applied to the unit of length including the prefix, as illustrated below.\nFor the most part, the metric prefixes are used uniformly for SI base, derived and accepted units. A notable exception is that for a large measure of seconds, the non-SI units of minute, hour and day are customary instead. Units of duration longer than a day are problematic since both month and year have varying number of days. Sub-second measures are often indicated via submultiple prefixes. For example, millisecond.\nCoherence.\nEach variant of the metric system has a degree of coherence\u2014the derived units are directly related to the base units without the need for intermediate conversion factors. For example, in a coherent system the units of force, energy, and power are chosen so that the equations\nhold without the introduction of unit conversion factors. Once a set of coherent units has been defined, other relationships in physics that use this set of units will automatically be true. Therefore, Einstein's mass\u2013energy equation, \"E\" = \"mc\"2, does not require extraneous constants when expressed in coherent units.\nThe CGS system had two units of energy, the erg that was related to mechanics and the calorie that was related to thermal energy; so only one of them (the erg) could bear a coherent relationship to the base units. Coherence was a design aim of SI, which resulted in only one unit of energy being defined \u2013 the joule.\nRationalisation.\nMaxwell's equations of electromagnetism contained a factor of formula_1 relating to steradians, representative of the fact that electric charges and magnetic fields may be considered to emanate from a point and propagate equally in all directions, i.e. spherically. This factor made equations more awkward than necessary, and so Oliver Heaviside suggested adjusting the system of units to remove it.\nEveryday notions.\nThe basic units of the metric system have always represented commonplace quantities or relationships in nature; even with modern refinements of definition and methodology. In cases where laboratory precision may not be required or available, or where approximations are good enough, the commonplace notions may suffice.\nTime.\nThe second is readily determined from the Earth's rotation period. Unlike other units, time multiples are not decimal. A second is of a minute, which is of an hour, which is of a day, so a second is of a day.\nLength.\nThe length of the equator is close to (more precisely ). In fact, the dimensions of our planet were used by the French Academy in the original definition of the metre. A dining tabletop is typically about 0.75 metres high. A very tall human is about 2 metres tall.\nMass.\nA 1-euro coin weighs 7.5\u00a0g; a Sacagawea US 1-dollar coin weighs 8.1\u00a0g; a UK 50-pence coin weighs 8.0\u00a0g.\nTemperature.\nIn everyday use, Celsius is more commonly used than Kelvin, however a temperature difference of one Kelvin is the same as one degree Celsius and that is defined as of the temperature differential between the freezing and boiling points of water at sea level. A temperature in Kelvin is the temperature in Celsius plus about 273. Human body temperature is about 37\u00a0\u00b0C or 310\u00a0K.\nLength, mass, volume relationship.\nThe mass of a litre of cold water is 1 kilogram. 1 millilitre of water occupies 1 cubic centimetre and weighs 1 gram.\nRelationship between candela and watt.\nCandela is about the luminous intensity of a moderately bright candle, or 1 candle power. A 60\u00a0Watt tungsten-filament incandescent light bulb has a luminous intensity of about 800 lumens which is radiated equally in all directions (i.e. 4\u03c0 steradians), thus is equal to \"I\"v \n \u2248 64 cd.\nRelationship between watt, volt and ampere.\nA 60\u00a0W incandescent light bulb consumes 0.5\u00a0A at 120\u00a0V (US mains voltage). A 60\u00a0W bulb rated at 230\u00a0V (European mains voltage) consumes 0.26\u00a0A at this voltage. This is evident from the formula \"P\" \n \"I\" \"V\".\nMole and mass relationship.\nA mole of a substance has a mass that is its molecular mass expressed in units of grams. The mass of a mole of carbon is 12.0\u00a0g, and the mass of a mole of table salt is 58.4\u00a0g.\nSince all gases have the same volume per mole at a given temperature and pressure far from their points of liquefaction and solidification (see Perfect gas), and air is about oxygen (molecular mass 32) and nitrogen (molecular mass 28), the density of any near-perfect gas relative to air can be obtained to a good approximation by dividing its molecular mass by 29 (because \u00d7 28 + \u00d7 32 \n 28.8 \u2248 29). For example, carbon monoxide (molecular mass 28) has almost the same density as air.\nHistory.\nThe metric system was the result of multiple attempts at standardisation of weights and measures over many centuries, as earlier (Chinese, Egyptian, Greek, Roman, and other) systems of weights and measures interacted with one another through global trade. \n17th &amp; 18th centuries.\nAround 1665\u20131670, Gabriel Mouton, a priest of the Collegiate Church of St Paul in Lyon, France, suggested that the many different systems of units then in use in France should be replaced by a single all-embracing decimal system. He proposed that linear measurements would be based on a natural standard of length, namely the length of one minute of arc of the largest circle that could be drawn round the Earth, and which he proposed to call one milliare. Mouton suggested that the unit of mass should be based on the unit of length, and on the mass density of water. While Moulton's contribution is sometimes overlooked, these ideas were clearly the basis over 120 years later, when the metric system was devised and standardised in France.\nDuring the final days of France's \"Ancien r\u00e9gime\" (1775-1788) and the early days of the French Revolution (1789\u201399), the first metric system was devised by chemists Antoine and Anne-Marie Lavoisier for the benefit of the Ferme g\u00e9n\u00e9rale in return for financial support of Antoine Lavoisier's research. \nFollowing the model established by Moulton, Lavoisier's unit of length, the metre, was based on the dimensions of the Earth, and the unit of mass, the kilogram, was based on the mass of 1 litre of water (the volume of water which occupies 1 cubic decimetre of space, using the same unit of length).\nThe French Revolution left France with a need to reform its numerous systems of various local weights and measures. In 1790, Charles Maurice de Talleyrand-P\u00e9rigord proposed Lavoisier's system based on natural units to the French National Assembly, without reference to the prior work of Mouton and Lavoisier. The aim was global adoption of the metric system. With the United Kingdom not responding to requests to collaborate in the development of the system, the French Academy of Sciences established a commission to implement this new standard alone, and in 1799, the new system was launched in France.\nA number of different metric systems have since been developed, all using the \"M\u00e8tre des Archives\" and \"Kilogramme des Archives\" (or their descendants) as their base units, but differing in the definitions of the various derived units.\n19th century.\nIn 1832, Gauss used the astronomical second as a base unit in defining the gravitation of the Earth, and together with the milligram and millimetre, this became the first system of mechanical units. He showed that the strength of a magnet could also be quantified in terms of these units, by measuring the oscillations of a magnetised needle and finding the quantity of \"magnetic fluid\" that produces an acceleration of one unit when applied to a unit mass. The centimetre\u2013gram\u2013second system of units (CGS) was the first coherent metric system, having been developed in the 1860s and promoted by James Clerk Maxwell and Lord Kelvin. In 1874, this system was formally promoted by the British Association for the Advancement of Science (BAAS). The system's characteristics are that density is expressed in g/cm3, force expressed in dynes and mechanical energy in ergs. Thermal energy was defined in calories, one calorie being the energy required to raise the temperature of one gram of water from 15.5\u00a0\u00b0C to 16.5\u00a0\u00b0C. The Association also recognised two sets of units for electrical and magnetic properties \u2013 the electrostatic set of units and the electromagnetic set of units.\nThe CGS units of electricity were cumbersome to work with. This was remedied at the 1893 International Electrical Congress held in Chicago by defining the \"international\" ampere and ohm using definitions based on the metre, kilogram and second, in the International System of Electrical and Magnetic Units. During the same period in which the CGS system was being extended to include electromagnetism, other systems were developed, distinguished by their choice of coherent base unit, including the Practical System of Electric Units, or QES (quad\u2013eleventhgram\u2013second) system. Here, the base units are the quad, equal to (approximately a quadrant of the Earth's circumference), the eleventhgram, equal to , and the second. These were chosen so that the corresponding electrical units of potential difference, current and resistance had a convenient magnitude.\n20th century.\nIn 1901, Giovanni Giorgi showed that by adding an electrical unit as a fourth base unit, the various anomalies in electromagnetic systems could be resolved. The metre\u2013kilogram\u2013second\u2013coulomb (MKSC) and metre\u2013kilogram\u2013second\u2013ampere (MKSA) systems are examples of such systems.\nThe metre\u2013tonne\u2013second system of units (MTS) was based on the metre, tonne and second \u2013 the unit of force was the sth\u00e8ne and the unit of pressure was the pi\u00e8ze. It was invented in France for industrial use and from 1933 to 1955 was used both in France and in the Soviet Union. Gravitational metric systems use the kilogram-force (kilopond) as a base unit of force, with mass measured in a unit known as the hyl, \"Technische Masseneinheit\" (TME), mug or metric slug. Although the CGPM passed a resolution in 1901 defining the standard value of acceleration due to gravity to be 980.665\u00a0cm/s2, gravitational units are not part of the International System of Units (SI).\nCurrent.\nThe International System of Units is the modern metric system. It is based on the metre\u2013kilogram\u2013second\u2013ampere (MKSA) system of units from early in the 20th century. It also includes numerous coherent derived units for common quantities like power (watt) and irradience (lumen). Electrical units were taken from the International system then in use. Other units like those for energy (joule) were modelled on those from the older CGS system, but scaled to be coherent with MKSA units. Two additional base units \u2013 the \"kelvin\", which is equivalent to degree Celsius for change in thermodynamic temperature but set so that 0\u00a0K is absolute zero, and the \"candela\", which is roughly equivalent to the international candle unit of illumination \u2013 were introduced. Later, another base unit, the \"mole\", a unit of amount of substance equivalent to the Avogadro number number of specified molecules, was added along with several other derived units.\nThe system was promulgated by the General Conference on Weights and Measures (French: \"Conf\u00e9rence g\u00e9n\u00e9rale des poids et mesures\" \u2013 CGPM) in 1960. At that time, the metre was redefined in terms of the wavelength of a spectral line of the krypton-86 atom (krypton-86 being a stable isotope of an inert gas that occurs in undetectable or trace amounts naturally), and the standard metre artefact from 1889 was retired.\nToday, the International system of units consists of 7 base units and innumerable coherent derived units including 22 with special names. The last new derived unit, the \"katal\" for catalytic activity, was added in 1999. All the base units except the second are now defined in terms of exact and invariant constants of physics or mathematics, barring those parts of their definitions which are dependent on the second itself. As a consequence, the speed of light has now become an exactly defined constant, and defines the metre as &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044299,792,458 of the distance light travels in a second. The kilogram was defined by a cylinder of platinum-iridium alloy until a new definition in terms of natural physical constants was adopted in 2019. As of 2022, the range of decimal prefixes has been extended to those for 1030 (\"quetta\u2013\") and 10\u221230 (\"quecto\u2013\").\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44144", "revid": "51208", "url": "https://en.wikipedia.org/wiki?curid=44144", "title": "Colour-blindness", "text": ""}
{"id": "44145", "revid": "45963401", "url": "https://en.wikipedia.org/wiki?curid=44145", "title": "Interquartile mean", "text": "The interquartile mean (IQM) (or midmean) is a statistical measure of central tendency based on the truncated mean of the interquartile range. The IQM is very similar to the scoring method used in sports that are evaluated by a panel of judges: \"discard the lowest and the highest scores; calculate the mean value of the remaining scores\".\nCalculation.\nIn calculation of the IQM, only the data between the first and third quartiles is used, and the lowest 25% and the highest 25% of the data are discarded.\nformula_1\nassuming the values have been ordered.\nExamples.\nDataset size divisible by four.\nThe method is best explained with an example. Consider the following dataset:\n5, 8, 4, 38, 8, 6, 9, 7, 7, 3, 1, 6\nFirst sort the list from lowest-to-highest:\n1, 3, 4, 5, 6, 6, 7, 7, 8, 8, 9, 38\nThere are 12 observations (datapoints) in the dataset, thus we have 4 quartiles of 3 numbers. Discard the lowest and the highest 3 values:\n1, 3, 4, 5, 6, 6, 7, 7, 8, 8, 9, 38\nWe now have 6 of the 12 observations remaining; next, we calculate the arithmetic mean of these numbers:\n\"x\"IQM = (5 + 6 + 6 + 7 + 7 + 8) / 6 = 6.5\nThis is the interquartile mean.\nFor comparison, the arithmetic mean of the original dataset is\n(5 + 8 + 4 + 38 + 8 + 6 + 9 + 7 + 7 + 3 + 1 + 6) / 12 = 8.5\ndue to the strong influence of the outlier, 38.\nDataset size not divisible by four.\nThe above example consisted of 12 observations in the dataset, which made the determination of the quartiles very easy. Of course, not all datasets have a number of observations that is divisible by 4. We can adjust the method of calculating the IQM to accommodate this. So ideally we want to have the IQM equal to the mean for symmetric distributions, e.g.:\n1, 2, 3, 4, 5\nhas a mean value \"x\"mean = 3, and since it is a symmetric distribution, \"x\"IQM = 3 would be desired.\nWe can solve this by using a weighted average of the quartiles and the interquartile dataset:\nConsider the following dataset of 9 observations:\n1, 3, 5, 7, 9, 11, 13, 15, 17\nThere are 9/4 = 2.25 observations in each quartile, and 4.5 observations in the interquartile range. Truncate the fractional quartile size, and remove this number from the 1st and 4th quartiles (2.25 observations in each quartile, thus the lowest 2 and the highest 2 are removed).\n1, 3, (5), 7, 9, 11, (13), 15, 17\nThus, there are 3 \"full\" observations in the interquartile range with a weight of 1 for each full observation, and 2 fractional observations with each observation having a weight of 0.75 (1-0.25 = 0.75). Thus we have a total of 4.5 observations in the interquartile range, (3\u00d71 + 2\u00d70.75 = 4.5 observations).\nThe IQM is now calculated as follows:\n\"x\"IQM = {(7 + 9 + 11) + 0.75 \u00d7 (5 + 13)} / 4.5 = 9\nIn the above example, the mean has a value xmean = 9. The same as the IQM, as was expected. The method of calculating the IQM for any number of observations is analogous; the fractional contributions to the IQM can be either 0, 0.25, 0.50, or 0.75.\nComparison with mean and median.\nThe interquartile mean shares some properties of both the mean and the median:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44146", "revid": "36336112", "url": "https://en.wikipedia.org/wiki?curid=44146", "title": "1100s BC (decade)", "text": "Decade\nThe 1100s BC is a decade that lasted from 1109 BC to 1100 BC.\nDecade\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44147", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=44147", "title": "1110s BC", "text": "Decade\nThe 1110s BC is a decade that lasted from 1119 BC to 1110 BC.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44149", "revid": "38788892", "url": "https://en.wikipedia.org/wiki?curid=44149", "title": "1560s BC", "text": "Decade\nThe 1560s BC was a decade lasting from January 1, 1569 BC to December 31, 1560 BC.\nDecade\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44150", "revid": "32010320", "url": "https://en.wikipedia.org/wiki?curid=44150", "title": "1710s BC", "text": "Decade\nDecade\nThe 1710s BC was a decade lasting from January 1, 1719 BC to December 31, 1710 BC.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44151", "revid": "11487766", "url": "https://en.wikipedia.org/wiki?curid=44151", "title": "1550s BC", "text": "The 1550s BC was a decade lasting from January 1, 1559 BC to December 31, 1550 BC.\nDecade\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44152", "revid": "27199084", "url": "https://en.wikipedia.org/wiki?curid=44152", "title": "1720s BC", "text": "Decade\nDecade\nThe 1720s BC was a decade lasting from January 1, 1729 BC to December 31, 1720 BC.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44153", "revid": "2012100", "url": "https://en.wikipedia.org/wiki?curid=44153", "title": "Kill Doctor Lucky", "text": "Board game\nKill Doctor Lucky is a humorous board game designed by James Ernest and released in 1996 by Cheapass Games. In 1998, \"Kill Doctor Lucky\" won the Origins Award for \"Best Abstract Board Game of 1997\".\n\"Kill Doctor Lucky\" is, in concept, a sort of inversion and perhaps a parody of \"Cluedo\" (\"Clue\" in North America). Both games are set in a sprawling mansion full of colorfully named rooms, feature a variety of dangerous weapons, and deal with the murder of the mansion's owner. \"Cluedo\" begins after the murder has been committed, and players compete to solve it; \"Kill Doctor Lucky\" ends with the murder, and players compete to commit it.\nIn October 2015 a \"Deluxe 19.5th Anniversary Edition\" with new art and updated game mechanics was launched on Kickstarter.\nGameplay.\nThe gameboard is a floor plan of Doctor Lucky's mansion, and it is accompanied by a deck of cards representing the objects and opportunities that can be found there. Players take turns moving through the rooms of the mansion and accumulating cards, while Doctor Lucky moves through the mansion following a predetermined path. A player may attempt to kill Doctor Lucky by playing a weapon card (such as a runcible spoon, a monkey hand, a letter opener, a trowel, a chainsaw or pinking shears) while the player's token is in the same room as Doctor Lucky and out of sight of all other players. Each weapon card has a certain point value, and certain weapons are worth more points when used in certain rooms (for example, the trowel is worth extra points when used in the wine cellar, an allusion to Poe's \"The Cask of Amontillado\").\nAt this point, the player making the murder attempt succeeds, and thereby wins the game, unless the opponents play Failure cards of combined value equal to the value of the weapon used. The situation is complicated by the requirement that players play Failure cards in clockwise order, with each player having only one opportunity to play cards. Since it is to any player's advantage to eliminate failure cards from his opponents' hands, a large part of the strategy of the game consists in bluffing: when one player attacks Doctor Lucky, it is in your interest to persuade your other opponents that you have no failure cards in your hand, to attempt to force them to save the game by spending the required cards.\nWhen played, failure cards are set aside and not returned to the deck. Thus, as the game goes on, fewer and fewer failure cards are in play. This not only builds tension but also forces the game to end in a reasonable amount of time, because once all the failure cards are gone, the next murder attempt cannot fail.\nThe new Titanic Games version of \"Kill Doctor Lucky\" makes two changes to the original rules. First, a minor change was made to game play that now allows everyone to take at least one turn before the Doctor Lucky pawn determines turn order. In the original, it was possible for players to position themselves in such a way as to keep some players from ever getting a turn. This is no longer possible.\nThe second change was the addition of a new game piece called the \"spite token\" (a variant in the prior edition). Spite tokens are awarded when a murder attempt fails and adds a bonus point to all future murder attempts. A player also has the option to spend a spite token as a failure point to aid in thwarting an opponent's murder attempt. When spite tokens are spent in this manner they are given to the player they're spent against. This speeds the game up and adds a great deal of strategy to the late game when all of the failure cards have been removed from the deck.\nAs of the 19.5th Anniversary Edition, several rules have changed. If the deck runs out, the lights go out and line of sight is in-the-same-room only; a player may not draw a card if anyone, including Doctor Lucky, can see them; and hallways and stairs do not count against movement, making the board much smaller."}
{"id": "44154", "revid": "47725437", "url": "https://en.wikipedia.org/wiki?curid=44154", "title": "Catherine de' Medici", "text": "Queen of France from 1547 to 1559\nCatherine de' Medici (, ; , ; 13 April 1519\u00a0\u2013 5 January 1589) was an Italian Florentine noblewoman of the Medici family and Queen of France from 1547 to 1559 by marriage to King Henry\u00a0II. She was the mother of French kings Francis\u00a0II, Charles\u00a0IX, and Henry\u00a0III, and a cousin to Pope Clement\u00a0VII. The years during which her sons reigned have been called \"the age of Catherine de' Medici\" since she had extensive, albeit at times varying, influence on the political life of France.\nCatherine was born in Florence to Lorenzo de' Medici, Duke of Urbino, and his wife, Madeleine de La Tour d'Auvergne. In 1533, at the age of 14, Catherine married Henry, the second son of King Francis\u00a0I and Queen Claude of France, who would become Dauphin of France (heir to the throne) upon the death of his elder brother Francis in 1536. Catherine's marriage was arranged by Clement\u00a0VII. Henry largely excluded Catherine from state affairs during his reign, instead showering favours on his chief mistress, Diane de Poitiers, who wielded significant influence in the court. Henry's sudden accidental death in 1559 thrust Catherine into the political arena as mother of the frail 15-year-old Francis\u00a0II. When Francis\u00a0II died the next year, she became regent on behalf of her 10-year-old son Charles\u00a0IX and thus gained sweeping powers. After Charles died in 1574, Catherine played a key role in the reign of her third son, Henry\u00a0III. He dispensed with her advice only in the last months of her life but outlived her by just seven months.\nCatherine's three sons reigned in an age of almost constant civil and religious war in France. The Catholic Guise faction tried to displace her sons in its attempt to establish its very own dynasty to rule over France. Catherine also tried to think of ways to weaken the Guise Faction, which treated her scornfully. The problems facing the monarchy were complex and daunting. However, Catherine kept the monarchy and state institutions functioning, if at a minimal level. At first, Catherine compromised and made concessions to the rebelling Calvinist Protestants known as Huguenots. However, she failed to fully grasp the theological issues that drove their movement. Later, she resorted in frustration and anger to hardline policies against them. In return, she was blamed for the persecutions carried out under her sons' rules, in particular the St. Bartholomew's Day massacre of 1572, during which thousands of Huguenots were killed in France.\nSome historians have excused Catherine from blame for the worst decisions of the crown, but evidence for her ruthlessness can be found in her letters. In practice, her authority was limited by the effects of the civil wars, and she suffers in comparison to what might have been had her husband the king lived to take responsibility or stabilise the country. Therefore, her policies may be seen as desperate measures to keep the House of Valois on the throne at all costs and her patronage of the arts as an attempt to glorify a monarchy whose prestige was in steep decline. Without Catherine, it is unlikely that her sons would have remained in power. Catherine has been called \"the most important woman in Europe\" in the 16th century.\nBirth and upbringing.\nCatherine de' Medici was born Caterina Maria Romula de' Medici on 13 April 1519 in Florence, Republic of Florence, the only child of Lorenzo de' Medici, Duke of Urbino, and his wife, Madeleine de la Tour d'Auvergne, the countess of Boulogne. The young couple had been married the year before at Amboise as part of the alliance between King Francis\u00a0I of France and Lorenzo's uncle Pope Leo\u00a0X against the Holy Roman Emperor Maximilian\u00a0I. According to a contemporary chronicler, when Catherine was born, her parents were \"as pleased as if it had been a boy\".\nWithin a month of Catherine's birth, both her parents were dead: Madeleine died on 28 April of puerperal fever, and Lorenzo died on 4 May. King Francis wanted Catherine to be raised at the French court, but Pope Leo refused, claiming he wanted her to marry Ippolito de' Medici. Leo made Catherine Duchess of Urbino but annexed most of the Duchy of Urbino to the Papal States, permitting Florence to keep only the Fortress of San Leo. It was only after Leo's death in 1521, that his successor, Adrian\u00a0VI, restored the duchy to its rightful owner, Francesco Maria I della Rovere.\nCatherine was first cared for by her paternal grandmother, Alfonsina Orsini. After Alfonsina's death in 1520, Catherine joined her cousins and was raised by her aunt, Clarice de' Medici. The death of Pope Leo in 1521 briefly interrupted Medici power until Cardinal Giulio de' Medici was elected Pope Clement\u00a0VII in 1523. Clement housed Catherine in the Palazzo Medici Riccardi in Florence, where she lived in state. The Florentine people called her \"duchessina\" (\"the little duchess\"), in deference to her unrecognised claim to the Duchy of Urbino.\nIn 1527, the Medici were overthrown in Florence by a faction opposed to the regime of Clement's representative, Cardinal Silvio Passerini, and Catherine was taken hostage and placed in a series of convents. The final one, the was her home for three years. Mark Strage described these years as \"the happiest of her entire life\". Clement had no choice but to crown Charles of Austria as Holy Roman Emperor in return for his help in retaking the city. In October 1529, Charles's troops laid siege to Florence. As the siege dragged on, voices called for the 10-year-old Catherine to be killed, stripped naked and chained to the city walls. Some even suggested that she be handed over to the troops to be raped. The city finally surrendered on 12 August 1530. Clement summoned Catherine from her beloved convent to join him in Rome where he greeted her with open arms and tears in his eyes. Then he set about the business of finding her a husband.\nMarriage.\nOn her visit to Rome, the Venetian envoy described Catherine as \"small of stature, and thin, and without delicate features, but having the protruding eyes peculiar to the Medici family\". Suitors, however, lined up for her hand, including James\u00a0V of Scotland who sent the Duke of Albany to Clement to conclude a marriage in April and November 1530. When Francis I of France proposed his second son, Henry, Duke of Orl\u00e9ans, in early 1533, Clement jumped at the offer. Although distantly related in several ways, Henry was a prize catch for Catherine, whose Medici family, despite its wealth, was of common origin.\nThe wedding, a grand affair marked by extravagant display and gift-giving, took place in the \u00c9glise Saint-Ferr\u00e9ol les Augustins in Marseille on 28 October 1533. Prince Henry danced and jousted for Catherine. The fourteen-year-old couple left their wedding ball at midnight to perform their nuptial duties. Henry arrived in the bedroom with King Francis, who is said to have stayed until the marriage was consummated. He noted that \"each had shown valour in the joust\". Clement visited the newlyweds in bed the next morning and added his blessings to the night's proceedings.\nCatherine saw little of her husband in their first year of marriage, but the ladies of the court, impressed with her intelligence and keenness to please, treated her well. However, the death of her papal cousin on 25 September 1534 undermined Catherine's standing in the French court. The next pope, Alessandro Farnese, was elected on 13 October and took the title Paul\u00a0III. As a Farnese he felt no obligation to keep Clement's promises, broke the alliance with Francis and refused to continue paying her huge dowry. King Francis lamented, \"The girl has come to me stark naked.\"\nPrince Henry showed no interest in Catherine as a wife; instead, he openly took mistresses. For the first ten years of the marriage, the royal couple failed to produce any children together. In 1537, he had a brief affair with Philippa Duci, who gave birth to a daughter, whom he publicly acknowledged. This proved that Henry was fertile and added to the pressure on Catherine to produce a child.\nDauphine.\n In 1536, Henry's older brother, Francis, caught a chill after a game of tennis, contracted a fever and died shortly after, leaving Henry the heir. Suspicions of poison abounded, from Catherine to Emperor Charles V. Sebastiano de Montecuccoli confessed under torture to poisoning the Dauphin.\nAs dauphine, Catherine was expected to provide a future heir to the throne. According to the court chronicler Brant\u00f4me, \"many people advised the king and the Dauphin to repudiate her, since it was necessary to continue the line of France\". Divorce was discussed. In desperation, Catherine tried every known trick for getting pregnant, such as placing cow dung and ground stags' antlers on her \"source of life\", and drinking mule's urine. On 19 January 1544 she at last gave birth to a son, named after King Francis.\nAfter becoming pregnant once, Catherine had no trouble doing so again. She may have owed her change of fortune to the physician Jean Fernel, who allegedly noticed slight abnormalities in the couple's sexual organs and advised them how to solve the problem. However, he denied ever providing such advice. Catherine quickly conceived again and on 2 April 1545 she bore a daughter, Elisabeth. She went on to bear Henry a further eight children, five of whom survived infancy: the future Charles IX (born 27 June 1550); the future Henry\u00a0III (born 19 September 1551); and Francis, Duke of Anjou (born 18 March 1555), Claude (born 12 November 1547) and Margaret (born 14 May 1553). The long-term future of the Valois dynasty, which had ruled France since the 14th century, seemed assured.\nHowever, Catherine's ability to bear children failed to improve her marriage. About 1538, at the age of 19, Henry had taken as his mistress the 38-year-old Diane de Poitiers, whom he adored for the rest of his life. Even so, he respected Catherine's status as his consort. When King Francis\u00a0I died on 31 March 1547, Catherine became queen consort of France. She was crowned in the Basilica of Saint-Denis on 10 June 1549.\nQueen of France.\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nHenry allowed Catherine almost no political influence as queen. Although she sometimes acted as regent during his absences from France, her formal powers were strictly nominal. Henry even gave the Ch\u00e2teau of Chenonceau, which Catherine had wanted for herself, to his mistress Diane de Poitiers instead, who took her place at the center of power, dispensing patronage and accepting favors. The imperial ambassador reported that in the presence of guests, Henry would sit on Diane's lap and play the guitar, chat about politics, or fondle her breasts. Diane never regarded Catherine as a threat. She even encouraged the king to spend more time with Catherine and sire more children.\nIn 1556, Catherine nearly died giving birth to twin daughters, Jeanne and Victoire. Surgeons saved her life by breaking the legs of Jeanne, who died in her womb. The surviving daughter, Victoire, died seven weeks later. Because their birth very nearly cost Catherine her life, the king's physician advised the king that there should be no more children; therefore, Henry\u00a0II stopped visiting his wife's bedroom and spent all his time with his longtime mistress, Diane de Poitiers. Catherine had no more children.\nHenry's reign enabled the rise of the Guise brothers, Charles, who became a cardinal, and Henry's boyhood friend Francis, both of whom became Duke of Guise. Their sister Mary of Guise had married James V of Scotland in 1538 and was the mother of Mary, Queen of Scots. At the age of five and a half, Mary was brought to the French court, where she was promised to the Dauphin, Francis. Catherine brought her up with her own children at the French court, while Mary of Guise governed Scotland as her daughter's regent.\nOn 3\u20134 April 1559 Henry signed the Peace of Cateau-Cambr\u00e9sis with the Holy Roman Empire and England, ending a long period of Italian Wars. The treaty was sealed by the betrothal of Catherine's teenage daughter Elisabeth, aged 13, to Philip II of Spain. Their proxy wedding was celebrated in Paris on 22 June 1559. As part of the celebrations, a jousting tournament was held on 30 June 1559.\nKing Henry took part in the jousting, sporting Diane's black-and-white colours. He defeated the dukes of Guise and Nemours, but the young Gabriel, comte de Montgomery, knocked him half out of the saddle. Henry insisted on riding against Montgomery again, and this time, Montgomery's lance shattered in the king's face. Henry reeled out of the clash, his face pouring blood, with splinters \"of a good bigness\" sticking out of his eye and head. Catherine, Diane, and Prince Francis all fainted. Henry was carried to the Ch\u00e2teau de Tournelles, where five splinters of wood were extracted from his head, one of which had pierced his eye and brain. Catherine stayed by his bedside, but Diane kept away, \"for fear\", in the words of a chronicler, \"of being expelled by the Queen\". For the next ten days, Henry's state fluctuated. At times he even felt well enough to dictate letters and listen to music. Slowly, however, he lost his sight, speech, and reason, and on 10 July 1559, he died, aged 40. From that day, Catherine took a broken lance as her emblem, inscribed with the words \"lacrymae hinc, hinc dolor\" (\"from this come my tears and my pain\"), and wore black mourning in memory of Henry.\nQueen mother.\nReign of Francis II.\nFrancis\u00a0II became king at the age of fifteen. In what has been called a coup d'\u00e9tat, the Cardinal of Lorraine and the Duke of Guise\u2014whose niece, Mary, Queen of Scots, had married Francis II the year before\u2014seized power the day after Henry\u00a0II's death and quickly moved themselves into the Louvre Palace with the young couple. The English ambassador reported a few days later that \"the house of Guise ruleth and doth all about the French king\". For the moment, Catherine worked with the Guises out of necessity. She was not strictly entitled to a role in Francis's government, because he was deemed old enough to rule for himself. Nevertheless, all his official acts began with the words: \"This being the good pleasure of the Queen, my lady-mother, and I also approving of every opinion that she holdeth, am content and command that\u00a0...\". Catherine did not hesitate to exploit her new authority. One of her first acts was to force Diane de Poitiers to hand over the crown jewels and return the Ch\u00e2teau de Chenonceau to the crown. She later did her best to efface or outdo Diane's building work there.\nThe Guise brothers set about persecuting the Protestants with zeal. Catherine adopted a moderate stance and spoke against the Guise persecutions, though she had no particular sympathy for the Huguenots, whose beliefs she never shared. The Protestants looked for leadership first to Antoine de Bourbon, King of Navarre, the First Prince of the Blood, and then, with more success, to his brother, Louis de Bourbon, Prince of Cond\u00e9, who backed a plot to overthrow the Guises by force. When the Guises heard of the plot, they moved the court to the fortified Ch\u00e2teau of Amboise. The Duke of Guise launched an attack into the woods around the ch\u00e2teau. His troops surprised the rebels and killed many of them on the spot, including the commander, La Renaudie. Others they drowned in the river or strung up around the battlements while Catherine and the court watched.\nIn June 1560, Michel de l'H\u00f4pital was appointed Chancellor of France. He sought the support of France's constitutional bodies and worked closely with Catherine to defend the law in the face of the growing anarchy. Neither saw the need to punish Protestants who worshipped in private and did not take up arms. On 20 August 1560, Catherine and the chancellor advocated this policy to an Assembly of Notables at Fontainebleau. Historians regard the occasion as an early example of Catherine's statesmanship. Meanwhile, Cond\u00e9 raised an army and in autumn 1560 began attacking towns in the south. Catherine ordered him to court and had him imprisoned as soon as he arrived. He was tried in November, found guilty of offences against the crown, and sentenced to death. His life was saved by the illness and death of the king, as a result of an infection or an abscess in his ear.\nWhen Catherine realized Francis was going to die, she made a pact with Antoine de Bourbon by which he would renounce his right to the regency of the future king, Charles IX, in return for the release of his brother Cond\u00e9. As a result, when Francis died on 5 December 1560, the Privy Council appointed Catherine as governor of France (\"gouvernante de France\"), with sweeping powers. She wrote to her daughter Elisabeth: \"My principal aim is to have the honour of God before my eyes in all things and to preserve my authority, not for myself, but for the conservation of this kingdom and for the good of all your brothers\".\nReign of Charles IX.\nCharles IX was ten years old at the time of his royal consecration, during which he cried. At first Catherine kept him very close to her, and even slept in his chamber. She presided over his council, decided policy, and controlled state business and patronage. However, she was never in a position to control the country as a whole, which was on the brink of civil war. In many parts of France the rule of nobles held sway rather than that of the crown. The challenges Catherine faced were complex and, in some ways, difficult for her to comprehend as a foreigner.\nShe summoned church leaders from both sides to attempt to solve their doctrinal differences. Despite her optimism, the resulting Colloquy of Poissy ended in failure on 13 October 1561, dissolving itself without her permission. Catherine failed because she saw the religious divide only in political terms. In the words of historian R. J. Knecht, \"she underestimated the strength of religious conviction, imagining that all would be well if only she could get the party leaders to agree\". In January 1562, Catherine issued the tolerant Edict of Saint-Germain in a further attempt to build bridges with the Protestants. On 1 March 1562, however, in an incident known as the Massacre of Vassy, the Duke of Guise and his men attacked worshipping Huguenots in a barn at Vassy, killing 74 and wounding 104. Guise, who called the massacre \"a regrettable accident\", was cheered as a hero in the streets of Paris while the Huguenots called for revenge. The massacre lit the fuse that sparked the French Wars of Religion. For the next thirty years, France found itself in a state of either civil war or armed truce.\nWithin a month Louis de Bourbon, Prince of Cond\u00e9, and Admiral Gaspard de Coligny had raised an army of 1,800. They formed an alliance with England and seized town after town in France. Catherine met Coligny, but he refused to back down. She therefore told him: \"Since you rely on your forces, we will show you ours\". The royal army struck back quickly and laid siege to Huguenot-held Rouen. Catherine visited the deathbed of Antoine de Bourbon, King of Navarre, after he was fatally wounded by an arquebus shot. Catherine insisted on visiting the field herself and when warned of the dangers laughed, \"My courage is as great as yours\". The Catholics took Rouen, but their triumph was short-lived. On 18 February 1563, a spy called Poltrot de M\u00e9r\u00e9 fired an arquebus into the back of the Duke of Guise, at the siege of Orl\u00e9ans. The murder triggered an aristocratic blood feud that complicated the French civil wars for years to come. Catherine, however, was delighted with the death of her ally. \"If Monsieur de Guise had perished sooner\", she told the Venetian ambassador, \"peace would have been achieved more quickly\". On 19 March 1563, the Edict of Amboise, also known as the Edict of Pacification, ended the war. Catherine now rallied both Huguenot and Catholic forces to retake Le Havre from the English.\nHuguenots.\nOn 17 August 1563, Charles IX was declared of age at the Parlement of Rouen, but he was never able to rule on his own and showed little interest in government. Catherine decided to launch a drive to enforce the Edict of Amboise and revive loyalty to the crown. To this end, she set out with Charles and the court on a progress around France that lasted from January 1564 until May 1565. Catherine held talks with Jeanne d'Albret, the Protestant queen regnant of Navarre (and the wife of Antoine de Bourbon) at M\u00e2con and N\u00e9rac. She also met her daughter Elisabeth at Bayonne near the Spanish border, amidst lavish court festivities. Philip\u00a0II excused himself from the occasion. He sent the Duke of Alba to tell Catherine to scrap the Edict of Amboise and to find punitive solutions to the problem of heresy.\nIn 1566, through the ambassador to the Ottoman Empire, Guillaume de Grandchamp de Grantrie, and because of a long-standing Franco-Ottoman alliance, Charles and Catherine proposed to the Ottoman Court a plan to resettle French Huguenots and French and German Lutherans in Ottoman-controlled Moldavia, in order to create a military colony and a buffer against the Habsburg. This plan also had the added advantage of removing the Huguenots from France, but it failed to interest the Ottomans.\nOn 27 September 1567, in a swoop known as the Surprise of Meaux, Huguenot forces attempted to ambush the king, triggering renewed civil war. Taken unawares, the court fled to Paris in disarray. The war was ended by the Peace of Longjumeau of 22\u201323 March 1568, but civil unrest and bloodshed continued. The Surprise of Meaux marked a turning point in Catherine's policy towards the Huguenots. From that moment, she abandoned compromise for a policy of repression. She told the Venetian ambassador in June 1568 that all one could expect from Huguenots was deceit, and she praised the Duke of Alba's reign of terror in the Netherlands, where Calvinists and rebels were put to death in the thousands.\nThe Huguenots retreated to the fortified stronghold of La Rochelle on the west coast, where Jeanne d'Albret and her fifteen-year-old son, Henry of Bourbon, joined them. \"We have come to the determination to die, all of us\", Jeanne wrote to Catherine, \"rather than abandon our God, and our religion.\" Catherine called Jeanne, whose decision to rebel posed a dynastic threat to the Valois, \"the most shameless woman in the world\". Nevertheless, the Peace of Saint-Germain-en-Laye, signed on 8 August 1570 because the royal army ran out of cash, conceded wider toleration to the Huguenots than ever before.\nCatherine looked to further Valois interests by grand dynastic marriages. In 1570, Charles IX married Elisabeth of Austria, daughter of Maximilian II, Holy Roman Emperor. Catherine was also eager for a match between one of her two youngest sons and Elizabeth I of England. After Catherine's daughter Elisabeth died in childbirth in 1568, she had touted her youngest daughter Margaret as a bride for Philip II of Spain. Now she sought a marriage between Margaret and Henry III of Navarre, Jeanne's son, with the aim of uniting Valois and Bourbon interests. Margaret, however, was secretly involved with Henry of Guise, the son of the late Duke of Guise. When Catherine found this out, she had her daughter brought from her bed. Catherine and the king then beat her, ripping her nightclothes and pulling out handfuls of her hair.\nCatherine pressed Jeanne d'Albret to attend court. Writing that she wanted to see Jeanne's children, she promised not to harm them. Jeanne replied: \"Pardon me if, reading that, I want to laugh, because you want to relieve me of a fear that I've never had. I've never thought that, as they say, you eat little children.\" When Jeanne did come to court, Catherine pressured her hard, playing on Jeanne's hopes for her beloved son. Jeanne finally agreed to the marriage between her son and Margaret, so long as Henry could remain a Huguenot. When Jeanne arrived in Paris to buy clothes for the wedding, she was taken ill and died on 9 June 1572, aged forty-three. Huguenot writers later accused Catherine of murdering her with poisoned gloves. The wedding took place on 18 August 1572 at Notre-Dame, Paris.\nSt. Bartholomew's Day massacre.\nThree days later, Admiral Coligny was walking back to his rooms from the Louvre when a shot rang out from a house and wounded him in the hand and arm. A smoking arquebus was discovered in a window, but the culprit had made his escape from the rear of the building on a waiting horse. Coligny was carried to his lodgings at the H\u00f4tel de B\u00e9thisy, where the surgeon Ambroise Par\u00e9 removed a bullet from his elbow and amputated a damaged finger with a pair of scissors. Catherine, who was said to have received the news without emotion, made a tearful visit to Coligny and promised to punish his attacker. Many historians have blamed Catherine for the attack on Coligny. Others point to the Guise family or a Spanish-papal plot to end Coligny's influence on the king. Whatever the truth, the bloodbath that followed was soon beyond the control of Catherine or any other leader.\nThe St. Bartholomew's Day massacre, which began two days later, has stained Catherine's reputation ever since. There is reason to believe she was party to the decision when on 23 August Charles IX is said to have ordered, \"Then kill them all! Kill them all!\" Historians have suggested that Catherine and her advisers expected a Huguenot uprising to avenge the attack on Coligny. They chose therefore to strike first and wipe out the Huguenot leaders while they were still in Paris after the wedding.\nThe slaughter in Paris lasted for almost a week. It spread to many parts of France, where it persisted into the autumn. In the words of historian Jules Michelet, \"St Bartholomew was not a day, but a season\". On 29 September, when Navarre knelt before the altar as a Roman Catholic, having converted to avoid being killed, Catherine turned to the ambassadors and laughed. From this time dates the legend of the wicked Italian queen. Huguenot writers branded Catherine a scheming Italian, who had acted on Machiavelli's principles to kill all enemies in one blow.\nReign of Henry III.\nTwo years later, Catherine faced a new crisis with the death of Charles IX at the age of twenty-three. His dying words were \"oh, my mother\u00a0...\" The day before he died, he named Catherine regent, since his brother and heir, Henry the Duke of Anjou, was in the Polish\u2013Lithuanian Commonwealth, where he had been elected king the year before. However, three months after his coronation at Wawel Cathedral, Henry abandoned that throne and returned to France in order to become King of France. Catherine wrote to Henry of Charles IX's death: \"I am grief-stricken to have witnessed such a scene and the love which he showed me at the end\u00a0... My only consolation is to see you here soon, as your kingdom requires, and in good health, for if I were to lose you, I would have myself buried alive with you.\"\nHenry was Catherine's favourite son. Unlike his brothers, he came to the throne as a grown man. He was also healthier, though he suffered from weak lungs and constant fatigue. His interest in the tasks of government, however, proved fitful. He depended on Catherine and her team of secretaries until the last few weeks of her life. He often hid from state affairs, immersing himself in acts of piety, such as pilgrimages and flagellation.\nHenry married Louise de Lorraine-Vaud\u00e9mont in February 1575, two days after his coronation. His choice thwarted Catherine's plans for a political marriage to a foreign princess. Rumours of Henry's inability to produce children were by that time in wide circulation. The papal nuncio Salviati observed, \"it is only with difficulty that we can imagine there will be offspring\u00a0... physicians and those who know him well say that he has an extremely weak constitution and will not live long.\" As time passed and the likelihood of children from the marriage receded, Catherine's youngest son, Francis, Duke of Alen\u00e7on, known as \"Monsieur\", played upon his role as heir to the throne, repeatedly exploiting the anarchy of the civil wars, which were by now as much about noble power struggles as religion. Catherine did all in her power to bring Francis back into the fold. On one occasion, in March 1578, she lectured him for six hours about his dangerously subversive behaviour.\nIn 1576, in a move that endangered Henry's throne, Francis allied with the Protestant princes against the crown. On 6 May 1576, Catherine gave in to almost all Huguenot demands in the Edict of Beaulieu. The treaty became known as the \"Peace of Monsieur\" because it was thought that Francis had forced it on the crown. Francis died of consumption in June 1584, after a disastrous intervention in the Low Countries during which his army had been massacred. Catherine wrote, the next day: \"I am so wretched to live long enough to see so many people die before me, although I realize that God's will must be obeyed, that He owns everything, and that He lends us only for as long as He likes the children whom He gives us.\" The death of her youngest son was a calamity for Catherine's dynastic dreams. Under Salic law, by which only males could ascend the throne, the Huguenot Henry of Navarre now became heir presumptive to the French crown.\nCatherine had at least taken the precaution of marrying Margaret, her youngest daughter, to Navarre. Margaret, however, became almost as much of a thorn in Catherine's side as Francis, and in 1582, she returned to the French court without her husband. Catherine was heard yelling at her for taking lovers. Catherine sent Pomponne de Belli\u00e8vre to Navarre to arrange Margaret's return. In 1585, Margaret fled Navarre again. She retreated to her property at Agen and begged her mother for money. Catherine sent her only enough \"to put food on her table\". Moving on to the fortress of Carlat, Margaret took a lover called d'Aubiac. Catherine asked Henry to act before Margaret brought shame on them again. In October 1586, therefore, he had Margaret locked up in the Ch\u00e2teau d'Usson. D'Aubiac was executed, though not, despite Catherine's wish, in front of Margaret. Catherine cut Margaret out of her will and never saw her again.\nCatherine was unable to control Henry in the way she had Francis and Charles. Her role in his government became that of chief executive and roving diplomat. She travelled widely across the kingdom, enforcing his authority and trying to head off war. In 1578, she took on the task of pacifying the south. At the age of fifty-nine, she embarked on an eighteen-month journey around the south of France to meet Huguenot leaders face to face. Her efforts won Catherine new respect from the French people. On her return to Paris in 1579, she was greeted outside the city by the Parlement and crowds. The Venetian ambassador, Gerolamo Lipomanno, wrote: \"She is an indefatigable princess, born to tame and govern a people as unruly as the French: they now recognize her merits, her concern for unity and are sorry not to have appreciated her sooner.\" She was under no illusions, however. On 25 November 1579, she wrote to the king, \"You are on the eve of a general revolt. Anyone who tells you differently is a liar.\"\nCatholic League.\nMany leading Roman Catholics were appalled by Catherine's attempts to appease the Huguenots. After the Edict of Beaulieu, they had started forming local leagues to protect their religion. The death of the heir to the throne in 1584 prompted the Duke of Guise to assume the leadership of the Catholic League. He planned to block Henry of Navarre's succession and place Henry's Catholic uncle Cardinal Charles de Bourbon on the throne instead. In this cause, he recruited the great Catholic princes, nobles and prelates, signed the treaty of Joinville with Spain, and prepared to make war on the \"heretics\". By 1585, Henry\u00a0III had no choice but to go to war against the League. As Catherine put it, \"peace is carried on a stick\" (\"b\u00e2ton porte paix\"). \"Take care\", she wrote to the king, \"especially about your person. There is so much treachery about that I die of fear.\"\nHenry was unable to fight the Catholics and the Protestants at once, both of whom had stronger armies than his own. In the Treaty of Nemours, signed on 7 July 1585, he was forced to give in to all the League's demands, even that he pay its troops. He went into hiding to fast and pray, surrounded by a bodyguard known as \"the Forty-five\", and left Catherine to sort out the mess. The monarchy had lost control of the country, and was in no position to assist England in the face of the coming Spanish attack. The Spanish ambassador told Philip\u00a0II that the abscess was about to burst.\nBy 1587, the Catholic backlash against the Protestants had become a campaign across Europe. Elizabeth I of England's execution of Mary, Queen of Scots, on 8 February 1587 outraged the Catholic world. Philip II of Spain prepared for an invasion of England. The League took control of much of northern France to secure French ports for his armada.\nLast months and death.\nHenry hired Swiss troops to help him defend himself in Paris. The Parisians, however, claimed the right to defend the city themselves. On 12 May 1588, they set up barricades in the streets and refused to take orders from anyone except the Duke of Guise. When Catherine tried to go to Mass, she found her way barred, though she was allowed through the barricades. The chronicler L'Estoile reported that she cried all through her lunch that day. She wrote to Belli\u00e8vre, \"Never have I seen myself in such trouble or with so little light by which to escape.\" As usual, Catherine advised the king, who had fled the city in the nick of time, to compromise and live to fight another day. On 15 June 1588, Henry duly signed the Act of Union, which gave in to all the League's latest demands.\nOn 8 September 1588 at Blois, where the court had assembled for a meeting of the Estates, Henry dismissed all his ministers without warning. Catherine, in bed with a lung infection, had been kept in the dark. The king's actions effectively ended her days of power.\nAt the meeting of the Estates, Henry thanked Catherine for all she had done. He called her not only the mother of the king but the mother of the state. Henry did not tell Catherine of his plan for a solution to his problems. On 23 December 1588, he asked the Duke of Guise to call on him at the Ch\u00e2teau de Blois. As Guise entered the king's chamber, the Forty-five plunged their blades into his body, and he died at the foot of the king's bed. At the same moment, eight members of the Guise family were rounded up, including the Duke of Guise's brother, Louis II, Cardinal of Guise, whom Henry's men hacked to death the next day in the palace dungeons. Immediately after the murder of Guise, Henry entered Catherine's bedroom on the floor below and announced, \"Please forgive me. Monsieur de Guise is dead. He will not be spoken of again. I have had him killed. I have done to him what he was going to do to me.\" Catherine's immediate reaction is not known; but on Christmas Day, she told a friar, \"Oh, wretched man! What has he done?\u00a0... Pray for him\u00a0... I see him rushing towards his ruin.\" She visited her old friend Cardinal de Bourbon on 1 January 1589 to tell him she was sure he would soon be freed. He shouted at her, \"Your words, Madam, have led us all to this butchery.\" She left in tears.\nOn 5 January 1589, Catherine died at the age of sixty-nine, probably from pleurisy. L'Estoile wrote: \"those close to her believed that her life had been shortened by displeasure over her son's deed.\" He added that she had no sooner died than she was treated with as much consideration as a dead goat. Because Paris was held by enemies of the crown, Catherine had to be buried provisionally at Blois. Eight months later, Jacques Cl\u00e9ment stabbed Henry\u00a0III to death. At the time, Henry was besieging Paris with the King of Navarre, who would succeed him as Henry IV of France. Henry\u00a0III's assassination ended nearly three centuries of Valois rule and brought the Bourbon dynasty into power. Years later, Diane, daughter of Henry\u00a0II and Philippa Duci, had Catherine's remains reinterred in the Saint-Denis basilica in Paris. In 1793, a revolutionary mob tossed her bones into a mass grave with those of the other kings and queens.\nHenry IV was later reported to have said of Catherine:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I ask you, what could a woman do, left by the death of her husband with five little children on her arms, and two families of France who were thinking of grasping the crown\u2014our own [the Bourbons] and the Guises? Was she not compelled to play strange parts to deceive first one and then the other, in order to guard, as she did, her sons, who successively reigned through the wise conduct of that shrewd woman? I am surprised that she never did worse.\nPatron of the arts.\nCatherine believed in the humanist ideal of the learned Renaissance prince whose authority depended on letters as well as arms. She was inspired by the example of her father-in-law, King Francis I of France, who had hosted the leading artists of Europe at his court, and by her Medici ancestors. In an age of civil war and declining respect for the monarchy, she sought to bolster royal prestige through lavish cultural display. Once in control of the royal purse, she launched a programme of artistic patronage that lasted for three decades. During this time, she presided over a distinctive late French Renaissance culture in all branches of the arts.\nAn inventory drawn up at the H\u00f4tel de la Reine after Catherine's death shows her to have been a keen collector. Listed works of art included tapestries, hand-drawn maps, sculptures, rich fabrics, ebony furniture inlaid with ivory, sets of china, and Limoges pottery. There were also hundreds of portraits, for which a vogue had developed during Catherine's lifetime. Many portraits in her collection were by Jean Clouet (1480\u20131541) and his son Fran\u00e7ois Clouet (c.\u20091510\u20131572). Fran\u00e7ois Clouet drew and painted portraits of all Catherine's family and of many members of the court. After Catherine's death, a decline in the quality of French portraiture set in. By 1610, the school patronised by the late Valois court and brought to its pinnacle by Fran\u00e7ois Clouet had all but died out.\nBeyond portraiture, little is known about the painting at Catherine de' Medici's court. In the last two decades of her life, only two painters stand out as recognisable personalities: Jean Cousin the Younger (c.\u20091522\u00a0\u2013 c.\u20091594), few of whose works survive, and Antoine Caron (c.\u20091521\u20131599), who became Catherine's official painter after working at Fontainebleau under Primaticcio. Caron's vivid Mannerism, with its love of the ceremonial and its preoccupation with massacres, reflects the neurotic atmosphere of the French court during the Wars of Religion.\nMany of Caron's paintings, such as those of the \"Triumphs of the Seasons\", are of allegorical subjects that echo the festivities for which Catherine's court was famous. His designs for the Valois Tapestries celebrate the \"f\u00eates\", picnics, and mock battles of the \"magnificent\" entertainments hosted by Catherine. They depict events held at Fontainebleau in 1564; at Bayonne in 1565 for the summit meeting with the Spanish court; and at the Tuileries in 1573 for the visit of the Polish ambassadors who presented the Polish crown to Catherine's son Henry of Anjou.\nThe musical shows in particular allowed Catherine to express her creative gifts. They were usually dedicated to the ideal of peace in the realm and based on mythological themes. To create the necessary dramas, music, and scenic effects for these events, Catherine employed the leading artists and architects of the day. Historian Frances Yates has called her \"a great creative artist in festivals.\" Catherine gradually introduced changes to the traditional entertainments: for example, she increased the prominence of dance in the shows that climaxed each series of entertainments. A distinctive new art form, the \"ballet de cour\", emerged from these creative advances. Owing to its synthesis of dance, music, verse, and setting, the production of the \"Ballet Comique de la Reine\" in 1581 is regarded by scholars as the first authentic ballet.\nCatherine de' Medici's great love among the arts was architecture. \"As the daughter of the Medici,\" suggests French art historian Jean-Pierre Babelon, \"she was driven by a passion to build and a desire to leave great achievements behind her when she died.\" After Henry\u00a0II's death, Catherine set out to immortalise her husband's memory and to enhance the grandeur of the Valois monarchy through a series of costly building projects. These included work on the Ch\u00e2teau de Montceaux, Ch\u00e2teau de Saint-Maur, and Chenonceau. Catherine built two new palaces in Paris: the Tuileries and the H\u00f4tel de la Reine. She was closely involved in the planning and supervising of all her architectural schemes.\nCatherine had emblems of her love and grief carved into the stonework of her buildings. Poets lauded her as the new Artemisia, after Artemisia II of Caria, who built the Mausoleum at Halicarnassus as a tomb for her dead husband. As the centrepiece of an ambitious new chapel, she commissioned a magnificent tomb for Henry at the basilica of Saint Denis. It was designed by Francesco Primaticcio (1504\u20131570), with sculpture by Germain Pilon (1528\u20131590). Art historian Henri Zerner has called this monument \"the last and most brilliant of the royal tombs of the Renaissance.\" Catherine also commissioned Germain Pilon to carve the marble sculpture that contains Henry\u00a0II's heart. A poem by Ronsard, engraved on its base, tells the reader not to wonder that so small a vase can hold so large a heart, since Henry's real heart resides in Catherine's breast.\nAlthough Catherine spent ruinous sums on the arts, most of her patronage left no permanent legacy. The end of the Valois dynasty so soon after her death brought a change in priorities.\nCulinary legend.\nThe legend that de' Medici introduced a long list of foods, techniques and utensils from Italy to France is discredited by food historians. Barbara Ketcham Wheaton and Stephen Mennell provided the definitive arguments against these claims. They point out that Catherine's father-in-law, King Francis\u00a0I, and the flower of the French aristocracy had dined at some of Italy's most \u00e9lite tables during the king's Italian campaigns (and that an earlier generation had done so during King Charles VIII's invasion of 1494); that a vast Italian entourage had visited France for the wedding of Catherine de' Medici's father to her French-born mother; and that she had little influence at court until her husband's death because he was so besotted by his mistress, Diane de Poitiers. In fact, a large population of Italians\u2014bankers, silk-weavers, philosophers, musicians, and artists, including Leonardo da Vinci\u2014had emigrated to France to promote the burgeoning Renaissance. Nevertheless, popular culture frequently attributes Italian culinary influence and forks in France to Catherine.\nThe earliest known reference to Catherine as the popularizer of Italian culinary innovation is the entry for \"cuisine\" in Diderot and d'Alembert's \"Encyclop\u00e9die\" published in 1754, which describes haute cuisine as decadent and effeminate and explains that fussy sauces and fancy fricassees arrived in France via \"that crowd of corrupt Italians who served at the court of Catherine de' Medici.\"\nLinks to the occult.\nCatherine de' Medici has been labelled by Wiccan Gerald Gardner a \"sinister Queen ... noted for her interest in the occult arts\". Catherine and Henry's inability to produce an heir for the first ten years of their marriage gave rise to suspicion of witchcraft. Labouvie suggested that women's power was believed to be the ability to create and sustain life, whilst witches were believed to have the opposite power; that of attacking health, life and fertility. An infertile woman, and in particular an infertile queen, was therefore regarded as 'unnatural' and a small step from supernatural. Essentially, however, there exists no concrete proof that she took part in the occult, and it is now believed that Catherine's trouble in providing an heir was in fact due to Henry\u00a0II's penile deformity.\nCatherine herself had been educated by Cosimo Ruggeri in astrology and astronomy, which were closely linked in her day and were an academic rather than a Satanic activity, although his general background and favourite status suggests there was more to it than that. It has been suggested that Catherine educated her son, Henry\u00a0III, in the dark arts, and that \"the two devoted themselves to sorceries that were scandals of the age\". As a result, some (more extreme) authors believe Catherine to be the creator of the Black Mass, a Satanic inversion of the traditional Catholic Mass, although there is little to prove this aside from Jean Bodin's account in his book \"De la d\u00e9monomanie des sorciers\". Nevertheless, Catherine was never formally accused or prosecuted despite the fact that her reign experienced the greatest number of prosecutions for witchcraft in Italy. This lends some weight to the suggestion that people were labelled 'witches' simply because they did not act the way a woman would have been expected to act, or simply to suit personal or political agendas. This may be particularly true for Catherine as an Italian woman ruling in France; several historians argue that she was disliked by her French subjects, who labelled her \"the Italian woman\". In any event, the rumours have made a mark on Catherine's reputation over time, and there are now many dramaticised works about her involvement in the occult.\nIssue.\nCatherine de' Medici married Henry, Duke of Orl\u00e9ans, the future Henry II of France, in Marseille on 28 October 1533. She gave birth to ten children, of whom four sons and three daughters survived to marriageable age. Three of her sons became kings of France, while two of her daughters married kings and one married a duke. Catherine outlived all her children except Henry\u00a0III, who died seven months after her, and Margaret, who inherited her robust health. Victoire and Jeanne were twin daughters born in 1556; Jeanne was stillborn due to surgeons breaking her legs to save her mother's life; Victoire survived, dying less than two months later. According to the diplomat Simon Renard, the birth nearly killed Catherine, and the royal couple were advised by the King's physician to have no further children.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n'"}
{"id": "44156", "revid": "45417033", "url": "https://en.wikipedia.org/wiki?curid=44156", "title": "Clue", "text": "Clue may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44158", "revid": "4754841", "url": "https://en.wikipedia.org/wiki?curid=44158", "title": "Conservative force", "text": "Force in which the work done in moving an object depends only on its displacement\n&lt;templatestyles src=\"Hlist/styles.css\"/&gt;\n}}In physics, a conservative force is a force with the property that the total work done by the force in moving a particle between two points is independent of the path taken. Equivalently, if a particle travels in a closed loop, the total work done (the sum of the force acting along the path multiplied by the displacement) by a conservative force is zero.\nA conservative force depends only on the position of the object. If a force is conservative, it is possible to assign a numerical value for the potential at any point and conversely, when an object moves from one location to another, the force changes the potential energy of the object by an amount that does not depend on the path taken, contributing to the mechanical energy and the overall conservation of energy. If the force is not conservative, then defining a scalar potential is not possible, because taking different paths would lead to conflicting potential differences between the start and end points.\nGravitational force is an example of a conservative force, while frictional force is an example of a non-conservative force.\nOther examples of conservative forces are: force in elastic spring, force due to liquid pressure acting on a surface, electrostatic force between two electric charges, and magnetic force between two magnetic poles. The last two forces are called central forces as they act along the line joining the centres of two charged/magnetized bodies. A central force is conservative if and only if it is spherically symmetric.\nFor conservative forces,\nformula_1\nwhere formula_2 is the conservative force, formula_3 is the potential energy, and formula_4 is the position.\nInformal definition.\nInformally, a conservative force can be thought of as a force that \"conserves\" mechanical energy. Suppose a particle starts at point A, and there is a force \"F\" acting on it. Then the particle is moved around by other forces, and eventually ends up at A again. Though the particle may still be moving, at that instant when it passes point A again, it has traveled a closed path. If the net work done by \"F\" at this point is 0, then \"F\" passes the closed path test. Any force that passes the closed path test for all possible closed paths is classified as a conservative force.\nThe gravitational force, spring force, magnetic force (according to some definitions, see below) and electric force (at least in a time-independent magnetic field, see Faraday's law of induction for details) are examples of conservative forces, while friction and air drag are classical examples of non-conservative forces.\nFor non-conservative forces, the mechanical energy that is lost (not conserved) has to go somewhere else, by conservation of energy. Usually the energy is turned into heat, for example the heat generated by friction. In addition to heat, friction also often produces some sound energy. The water drag on a moving boat converts the boat's mechanical energy into not only heat and sound energy, but also wave energy at the edges of its wake. These and other energy losses are irreversible because of the second law of thermodynamics.\nPath independence.\nA direct consequence of the closed path test is that the work done by a conservative force on a particle moving between any two points does not depend on the path taken by the particle.\nThis is illustrated in the figure to the right: The work done by the gravitational force on an object depends only on its change in height because the gravitational force is conservative. The work done by a conservative force is equal to the negative of change in potential energy during that process. For a proof, imagine two paths 1 and 2, both going from point A to point B. The variation of energy for the particle, taking path 1 from A to B and then path 2 backwards from B to A, is 0; thus, the work is the same in path 1 and 2, i.e., the work is independent of the path followed, as long as it goes from A to B.\nFor example, if a child slides down a frictionless slide, the work done by the gravitational force on the child from the start of the slide to the end is independent of the shape of the slide; it only depends on the vertical displacement of the child.\nMathematical description.\nA force field \"F\", defined everywhere in space (or within a simply-connected volume of space), is called a \"conservative force\" or \"conservative vector field\" if it meets any of these three \"equivalent\" conditions:\n&lt;templatestyles src=\"Math_proof/styles.css\" /&gt; Proof that these three conditions are equivalent when \"F\" is a force field\n&lt;templatestyles src=\"Glossary/styles.css\" /&gt;\nThe term \"conservative force\" comes from the fact that when a conservative force exists, it conserves mechanical energy. The most familiar conservative forces are gravity, the electric force (in a time-independent magnetic field, see Faraday's law), and spring force.\nMany forces (particularly those that depend on velocity) are not force \"fields\". In these cases, the above three conditions are not mathematically equivalent. For example, the magnetic force satisfies condition 2 (since the work done by a magnetic field on a charged particle is always zero), but does not satisfy condition 3, and condition 1 is not even defined (the force is not a vector field, so one cannot evaluate its curl). Accordingly, some authors classify the magnetic force as conservative, while others do not. The magnetic force is an unusual case; most velocity-dependent forces, such as friction, do not satisfy any of the three conditions, and therefore are unambiguously nonconservative.\nNon-conservative force.\nDespite conservation of total energy, non-conservative forces can arise in classical physics due to neglected degrees of freedom or from time-dependent potentials. Many non-conservative forces may be perceived as macroscopic effects of small-scale conservative forces. For instance, friction may be treated without violating conservation of energy by considering the motion of individual molecules; however, that means every molecule's motion must be considered rather than handling it through statistical methods. For macroscopic systems the non-conservative approximation is far easier to deal with than millions of degrees of freedom.\nExamples of non-conservative forces are friction and non-elastic material stress. Friction has the effect of transferring some of the energy from the large-scale motion of the bodies to small-scale movements in their interior, and therefore appear non-conservative on a large scale. General relativity is non-conservative, as seen in the anomalous precession of Mercury's orbit. However, general relativity does conserve a stress\u2013energy\u2013momentum pseudotensor.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44159", "revid": "9203640", "url": "https://en.wikipedia.org/wiki?curid=44159", "title": "Coda", "text": "Coda or CODA may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44160", "revid": "1318934393", "url": "https://en.wikipedia.org/wiki?curid=44160", "title": "Sauk people", "text": "Indigenous people from the Northeastern Woodlands, U.S.\nThe Sauk or Sac (Sauk: \"Th\u00e2k\u00eewaki\") are Native Americans and Indigenous peoples of the Northeastern Woodlands. Their historical territory was near Green Bay, Wisconsin. Today they have three tribes based in Iowa, Kansas, Nebraska, and Oklahoma. Their federally recognized tribes are:\nThey are closely allied with the Meskwaki people. Their Sauk language is part of the Algonquian language family.\nName.\nThe Sauk or Sac called themselves Th\u00e2k\u00eewaki, translating as \"people coming forth [from the outlet]\" or \"[from the water]\". Their autonym is written o\u03b8aakiiwaki in the current orthography. Ojibwe people called them Ozaagii(-wag). The latter name was transliterated into French and English by European colonists.\nThe neighboring Anishanabeg Ojibwe (Sauk name: Ochipw\u00eawa) and Odawa peoples referred to them by the exonym \"Ozaagii(-wag)\", meaning \"those at the outlet\". French colonists transliterated that as \"Sac\" and the English as \"Sauk\". The Sauk/Sac called themselves the \nautonym of Oth\u00e2k\u00eewa, Th\u00e2k\u00eewa, Th\u00e2k\u00eewaki or \"Asaki-waki/O\u03b8aakiiwaki\" people of the yellow earth [(\"people coming forth [from the outlet],\" i.e., \"from the water\")], which is often interpreted to mean \"yellow-earth people\" or \"the Yellow-Earths\", due to the yellow-clay soils found around Saginaw Bay. This interpretation possibly derived from the Sauk words \"Ath\u00e2wethiwa\" or (\"yellow\") and \"Neniwaki\" (\"men, people\"). This was later shortened to \"Asaki-waki\". In addition, the Fox (Meskwaki) were generally known among neighboring tribes as the \"people of the red earth\". The Sauk and Fox also used this term: \"\u00cashkw\u00eeha\" or \"Meshkwahk\u00eeha\" (\"people of the red earth\").\nHistory.\nPrecontact to 17th century.\nThe Sauk, an Algonquian languages people, are believed to have developed as a people along the St. Lawrence River, which is now northern New York. The precise time is unknown, but around the time of the year 1600, they were driven from the area of the St. Lawrence River. Some historians believe that the Sauk migrated to what is now eastern Michigan, where they settled around Saginaw Bay (Ojibwe: \"Zaagiinaad-wiikwed\" \u2013 \"Of the Outlet Bay\"). For many years, the Sauk are believed to have prospered in the fertile valley of Saginaw thereafter. They had been driven west by pressure from other tribes, especially the powerful Haudenosaunee, which sought control over hunting grounds in the area.\nSome Ojibwe oral histories also place the Sauk in the Saginaw Valley some time before the arrival of Europeans. Sauk traditions state that the tribe occupied the vicinity of Saginaw river. (In this tradition, the name 'Saginaw' comes from the Ojibwe \"O-Sauk-e-non,\" meaning \"land of the Sauks\" or \"where the Sauks were.\") Approximately from the years 1638 to 1640, it is believed that a fierce battle ensued, nearly annihilating the entire Sauk Tribe. According to the legend, the Ojibwe inhabited the lands north of the Saginaw Bay, and the harsher northern climate caused more difficulty in prosperity compared to that of the Sauk occupying the area of Saginaw Valley. The Ojibwe allied with the Odawa, who resided south of the Sauk, and sprung a series of attacks on the Sauk, which practically decimated their people. One such attack, the Battle of Skull Island, occurred on a peninsula in the Saginaw River, which then was called Skull Island. (Its name came from the many skulls and bones supposedly found in mounds on that island over the years.) In this battle, it is said that the Sauk had used their boats to cross part of the river, escape to the island, and were temporarily free from their attackers. But when morning came, ice had solidified the river enough for the Ojibwe to cross. They killed every member of the Sauk tribe who had fled to that island besides 12 women whom they later sent west of the Mississippi River.\nBut later Europeans may have mistakenly recorded the Sauk as once dwelling at this location near Lake Huron. There is little archaeological evidence that the Sauk lived in the Saginaw area. In the early 17th century, when natives told French explorer Samuel de Champlain that the Sauk nation was located on the west shore of Lake Michigan, Champlain mistakenly placed them on the western shore of Lake Huron. This mistake was copied on subsequent maps, and future references identified this as the place of the Sauk. Champlain never visited what is now Michigan.\nAnishinaabe expansion and the Huron attempting to gain regional stability drove the Sac out of their territory. The Huron were armed with guns supplied by their French trading partners. The Sac moved south to territory in parts of what are now northern Illinois and Wisconsin. In the 17th century the Sauk also maintained close relations with the Potawatomi (Pehk\u00eenen\u00eeha or Sh\u00eesh\u00eep\u00eahinen\u00eeha). This relation has been found by borrowings of Sauk vocabulary that appear in the Potawatomi language.\nIn a loose coalition of tribes \u2013 including Dakota (Ash\u00e2ha), Ho-Chunk, Ojibwe, Odawa, Potawatomi, Kickapoo (K\u00eek\u00e2p\u00f4wa), Meskwaki (Fox), and Sauk, along with the Shawnee (Sh\u00e2wan\u00f4wa), Cherokee (Shanahk\u00eeha), and Choctaw (Ch\u00e2kit\u00e2ha) from the Southeast \u2013 they attacked the tribes of the Illinois Confederation (Mashkot\u00eawa) and tried to invade their tribal areas. The Illinois (Inoca) became their worst common enemies. The coalition warred for years until they destroyed the Illinois Confederation.\nLater they moved out on the prairie (Mashkot\u00eawi) along the Mississippi and adopted the semi-sedentary lifestyle of Plains Indians (Mashkot\u00eawineniwa). In addition to hunting buffalo, they lived in villages, raised crops, and actively traded with other tribes. The Sauk and allied eastern tribes had to compete with tribes who already occupied this territory. Disputes and clashes arose with the Dakota, Pawnee (P\u00e2n\u00eeha) and, most of all, the powerful Osage (Wash\u00e2sha).\n18th century.\nThe Sauk had good relations with the English (Th\u00e2kan\u00e2sha) through trading. At first, the Sauk had good relations with New France too, until their alliance with the Meskwaki (Fox) made them short-term enemies of the French (M\u00eamehtek\u00f4sh\u00eeha, W\u00eamehtek\u00f4sh\u00eeha).\nA closely allied tribe, the Meskwaki (Fox), were noted for resisting French encroachment, having fought two wars against them in the early 18th century. After a devastating battle of September 9, 1730, in Illinois, in which hundreds of warriors were killed and many women and children taken captive by French allies, Fox refugees took shelter with the Sac. This made the Sauk subject to French attack in turn. The Sauk continued moving west to Iowa and Kansas. Keokuk and Black Hawk were two important leaders who arose among the Sauk. At first, Keokuk accepted the loss of land as inevitable in the face of the vast numbers of white soldiers and settlers coming west. He tried to preserve tribal land and his people, and to keep the peace.\n19th century.\nHaving failed to receive expected supplies from the Americans on credit, Black Hawk wanted to fight, saying his people were \"forced into war by being deceived\". Led by Black Hawk in 1832, the mainly Sac band resisted the continued loss of lands (in western Illinois, this time.) Their warfare with United States forces resulted in defeat at the hands of General Edmund P. Gaines in the Black Hawk War.\nFrom 1832 to 1837, debt and poverty were tools used to coerce the Sauk and Meskwaki to relocate three times following successive cessions of territory. The population of the two tribes living in Iowa was halved in the twelve years from 1833 to 1845.\nOklahoma history.\nAbout this time, one group of Sac moved into Missouri, and later to Kansas and Nebraska. In 1869, after the Civil War, the United States forced the larger group of Sac to move into a reservation in Indian Territory (now the state of Oklahoma). They formed the federally recognized Sac and Fox Nation, which is misnamed and is primarily Sauk. The United States had been making treaties with the two tribes together since their residency in the Midwest. A number of Meskwaki returned to the Midwest from Oklahoma (or resisted leaving.) They joined the Meskwaki at the Meskwaki Settlement in Tama County, Iowa.\nThe land currently occupied by the Sauk is only a section of what used to be the Sac and Fox Reservation from 1867 to 1891. This reservation was established by the U.S. and spanned 480,000 acres. In 1887, however, the Dawes Act purposely broke collective tribal lands into small allotments designated for individual households. The remainder of land not allotted to the Sac and Fox was then sold to non-Native settlers in an attempt to gain Oklahoma statehood and the full assimilation of its Native American population.\nBy 1889, 519 of the tribe were located in Indian Territory, what is now central Oklahoma. On June 10, 1890, they ceded these Indian Territory lands to the federal government.\nTreaties with U.S..\nMany of the latter treaties listed have little to no information regarding their details, besides the date. The Sauk signed a total of 22 treaties from 1789 to 1891.\nClan system.\nThe Sauk and Fox peoples were divided into two moieties or \"divisions\", which in turn were subdivided into Patri-lineages and Clans as local subgroups (segments).\nThe moieties were known as the Kishko/Ki-sko-ha/K\u00eeshk\u00f4ha (male: \"K\u00eeshk\u00f4ha\", female: \"K\u00eeshk\u00f4hkw\u00eaha\") (\"the long-haired\") and as the Oskush/Askasa/Shkasha (male: \"Shkasha/Oshkash\u00eewiwa\", female: \"Shkash\u00eehkw\u00eawa/Oshkash\u00eehkw\u00eawiwa\") (\"the brave\"). The two moieties were each symbolized by two colors: The \"Askasa/Shkasha\" painted their faces and partly their bodies with charcoal in \"mahkat\u00eaw\u00e2wi\" (black) and the \"Ki-sko-ha/K\u00eeshk\u00f4ha\" painted their bodies with white clay in \"w\u00e2peshky\u00e2wi\" (white). This duality was also celebrated by the two moieties in Lacrosse, which was often played extremely brutally to toughen young warriors for combat, for recreation, as part of festivals, and used as preparation for imminent wars or raids.\nThis division has survived to the present day, but is now more related to the political system of the United States: the supporters of the Democratic Party are associated with the \"K\u00eeshk\u00f4ha/K\u00eeshk\u00f4hkw\u00eaha\", while the supporters of the Republican Party are associated with the \"Shkasha/Shkash\u00eehkw\u00eawa\".\nOriginally, the Sauk had a patrilineal and exogamous clan system, in which descent and inheritance was traced through the father. Clans or \"M\u00eethon\u00ee\" distinguished and named on the basis of totem animals, which are: \"Mahkwithowa\" (Bear Clan), \"Amehkwithowa\" (Beaver Clan), \"Peshekethiwithowa\" (Deer Clan), \"Ketiwithowa / Mekethiwithowa\" (Eagle Clan), \"Nem\u00eathithowa\" (Fish Clan), \"W\u00e2kosh\u00eahithowa\" (Fox Clan), \"Kehchikam\u00eewithowa\" (Ocean/Sea/Great Lake Clan), \"Kesh\u00eahokim\u00e2withowa\" (Peace Clan), \"Ahpen\u00eethowa\" (Potato Clan), \"Ak\u00f4nithowa\" (Snow Clan), \"Nenemehkiwithowa\" (Thunder Clan), \"Manethen\u00f4kim\u00e2withowa\" (Warrior Clan), and \"Mahw\u00eawithowa\" (Wolf Clan).\nSaukenuk or Saukietown (today: Black Hawk State Historic Site) near the mouth of the Rock River (\"Sinnissippi\" \u2013 \"rocky waters\") into the Mississippi (\"M\u00e4se'sibowi\" \u2013 \"great river\"), the most important Sauk settlement in the 18th and 19th centuries with about 4,000 inhabitants, was divided into 12 districts, which were assigned to the respective clans.\nThe tribe was governed by a council of sacred clan chiefs, a war chief, the head of families, and the warriors. Chiefs were recognized in three categories: civil, war, and ceremonial. Only the civil chiefs were hereditary. The other two chiefs were recognized by bands after they demonstrated their ability or spiritual power.\nThis traditional manner of selecting historic clan chiefs and governance was replaced in the 19th century by the United States appointing leaders through their agents at the Sac and Fox Agency, or reservation in Indian Territory (now Oklahoma). In the 20th century, the tribe adopted a constitutional government patterned after the United States form. They elect their chiefs.\nFederally recognized tribes.\nToday, the federally recognized Sac and Fox tribes include:\nGeographical names.\nLake Osakis in west-central Minnesota, the Sauk River, which flows from Lake Osakis, and the towns of Osakis, Sauk Centre, and Sauk Rapids all were named for association historically with a small party of Sac who made camp on the shores of Lake Osakis. They had been banished from their tribe for murder. According to Anishinaabe oral tradition, these five Sac were killed by local Dakota in the late 18th century.\nPlace names with \"Sauk\" references include: \n1.&lt;templatestyles src=\"Citation/styles.css\"/&gt;^ The name of the Sauk River in Washington State, however, comes from the \"Sah-kee-ma-hu\" (Sauk-Suiattle tribe), a group related to the Skagit tribes, not from the Sac tribe of the Midwestern U.S.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44161", "revid": "449239", "url": "https://en.wikipedia.org/wiki?curid=44161", "title": "Measure of central tendency", "text": ""}
{"id": "44162", "revid": "449239", "url": "https://en.wikipedia.org/wiki?curid=44162", "title": "Measures of central tendency", "text": ""}
{"id": "44164", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=44164", "title": "Handel", "text": ""}
{"id": "44165", "revid": "32990417", "url": "https://en.wikipedia.org/wiki?curid=44165", "title": "Cluedo", "text": "Board game \nCluedo (), known as Clue in North America, is a murder mystery game for three to six players (depending on editions) that was devised in 1943 by British board game designer Anthony E. Pratt. The game was first manufactured by Waddingtons in the United Kingdom in 1949. Since then, it has been relaunched and updated several times, and it is currently owned and published by the American game and toy company Hasbro.\nThe object of the game is to determine who murdered the game's victim, where the crime took place, and which weapon was used. Each player assumes the role of one of the six suspects and attempts to deduce the correct answer by strategically moving around a game board representing the rooms of a mansion and collecting clues about the circumstances of the murder from the other players.\nNumerous games, books, a film, television series, and theatre adaptations have been released as part of the \"Cluedo\" franchise. Several spinoffs have been released, featuring various extra characters, weapons, rooms, or different gameplay. The original game is marketed as the \"Classic Detective Game\", and the various spinoffs are all distinguished by different slogans.\nIn 2008, \"\" was created (with changes to the board, gameplay, and characters) as a modern spin-off, but was criticised in the media and by fans of the original game. \"Cluedo: The Classic Mystery Game\" was then introduced in 2012, returning to Pratt's classic formula but also adding several variations.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nHoled up in his home in Birmingham, England, during air raids on the city during World War II, Anthony E. Pratt, an English musician and factory worker, recalled the murder mystery games played by some of his clients at private music gigs as well as the detective fiction popular at the time, most notably Agatha Christie. In 1944, Pratt applied for a patent of his invention of a murder/mystery-themed game, originally named \"Murder.\"\u200a Shortly thereafter, Pratt and his wife, Elva Pratt (1913\u20131990), who had helped design the game, presented it to Waddingtons' executive Norman Watson, who immediately purchased it and provided its trademark name of \"Cluedo\" (a play on \"clue\" and \"ludo\", the Latin word for \"I play\", as used for the name of \"Ludo\", a popular board game based on Pachisi). The design of the house in the game is reputed to be based on what was the Tudor Close Hotel in Rottingdean, Brighton and Hove, with early editions of the game being titled \"Murder at Tudor Close\".\nAlthough the patent was granted in 1947, postwar shortages postponed the game's official United Kingdom launch until 1949. It was simultaneously licensed to Parker Brothers in the United States for publication, where it was renamed \"Clue\", as the name \"Ludo\" was not widely known there, Pachisi-style games having been published under other names and brands, so the play on words would not have been generally understood.\nPratt sold the international rights for ) in 1953, but continued to receive UK royalties until the patent expired in 1967.\nThere were several differences between the original game concept and the one initially published in 1949. In particular, Pratt's original design calls for 10 characters, one of whom was to be designated the victim by random drawing prior to the start of the game. These 10 included the eliminated Mr. Brown, Mr. Gold, Miss Grey, and Mrs. Silver. The characters of Nurse White and Colonel Yellow were renamed Mrs. White and Colonel Mustard for the actual release. The game allowed for play of up to eight remaining characters, providing for nine suspects in total. Originally there were 11 rooms, including the eliminated gun room and cellar. In addition, there were nine weapons, including the unused bomb, syringe, shillelagh (walking stick/cudgel), fireplace poker, and the later used axe and poison. Some of these unused weapons and characters appeared later in spin-off versions of the game.\nSome gameplay aspects were different as well. Notably, the remaining playing cards were distributed into the rooms to be retrieved, rather than dealt directly to the players. Players also had to land on another player in order to make suggestions about that player's character through the use of special tokens, and once exhausted, a player could no longer make suggestions. There were other minor differences, all of which were later updated by the game's initial release and remain essentially unchanged in the standard Classic Detective Game editions of the game.\nThe methodology used in the early versions of \"Cluedo\" is remarkably similar to a traditional, if little known, American card game: \"the king of hearts has five sons\". However, Pratt himself said his inspiration was a murder mystery parlour game he used to play with friends in which youngsters \"would congregate in each other's homes for parties at weekends. We'd play a stupid game called Murder, where guests crept up on each other in corridors and the victim would shriek and fall on the floor\". The country house mystery was a popular subgenre of \"cosy\" English detective fiction in the 1920s and 1930s; stories were set in a residence of the gentry isolated by circumstances such as a snowstorm with the suspects gathered for a weekend house party.\nMarketing.\n\"Cluedo\" was originally marketed as \"The Great New Detective Game\" upon its launch in 1949 in North America. A deal was quickly struck to license \"The Great New Sherlock Holmes Game\" from the Sir Arthur Conan Doyle estate. Advertising at the time suggested players would take on the guise of \"Sherlock Holmes following the path of the criminal\", but no depictions of Holmes appear in the advertising or on the box. From 1950 until the 1960s, the game was marketed as \"The Great Detective Game\", at which time it became the \"Parker Brothers Detective Game\". \nWith the launch of the US 1972 edition, a television commercial showed Holmes and Watson engaged in a particularly competitive game. Adjusting with the times, in 1979 US television commercials a detective, resembling a bumbling Inspector Clouseau from the popular \"Pink Panther\" film franchise, looks for clues. In 1986, the marketing slogan added \"Classic Detective Game\" which persists through the last 2002/2003 edition.\nIn the UK, \"Cluedo\" was marketed as \"The Great Detective Game\" from the mid-1950s until 2000, when it was rebranded as the \"Classic Detective Game\". However, in the mid-1950s Waddingtons also adopted a Sherlock Holmes-type detective to adorn their box covers for a brief time, though unlike in the US editions, there was no acknowledgement that the character was actually the famous detective. In the 1980s, as in the US, Sherlock Holmes also appeared in TV advertising of the time, along with other classic detectives such as Sam Spade.\nGame.\nEquipment.\nThe game consists of a board which shows the rooms, corridors, and secret passages of an English country house called \"Tudor Mansion\" (named \"Tudor Close\", \"Tudor Hall\", \"Arlington Grange\", \"Boddy Manor\" or \"Boddy Mansion\" in some editions) in Hampshire, England, in 1926. The game box also includes several coloured playing pieces to represent characters, miniature murder weapon props, two six-sided dice, three sets of cards (describing the aforementioned rooms, characters, or weapons), \"Solution Cards\" and an envelope (or a mirror in some editions) to contain one card from each set of cards and a \"Detective's Notes\" pad on which are printed lists of rooms, weapons, and characters, so players can keep detailed notes during the game.\nCharacters.\nThe murder victim in the game was known as Dr. Black in the UK edition and Mr. Boddy in North American versions. Updated editions of the game, released by Hasbro in 2023, refer to him as Boden \"Boddy\" Black Jr.\nPlayer tokens are typically plastic pawns or figurines; the standard edition of the game has six suspects:\nWeapons.\nThe weapon icons are typically made of unfinished pewter (except the rope, which may be plastic or string); special editions have included gold-plated, brass-finished, and sterling silver versions. Early versions of the game included lead piping that was made from actual lead, and which was replaced with a steel version in later editions.\nRooms.\nThere are nine rooms in the mansion where the murder can take place, laid out around the edge of the square game board. They consist of the hall, the lounge, the dining room, the kitchen, the ballroom, the conservatory, the billiard room, the library, and the study. Each of the four corner rooms contains a secret passage that leads to the room on the opposite diagonal corner of the map. The centre room (often referred to as the cellar or the stairs) is inaccessible to the players, but contains the solution envelope and is not otherwise used during gameplay. Each suspect token has a designated start space marked at the edge of the board. The corridor areas are marked out as square spaces through which the tokens move horizontally or vertically.\nRules.\nAt the beginning of the play, three cards\u2014one suspect, one room, and one weapon\u2014are chosen at random and put into a special envelope, so that no one can see them. These cards represent the solution. The remainder of the cards are distributed among the players.\nPlayers are instructed to assume the token/suspect nearest them. In older versions, the play begins with Miss Scarlett and proceeds clockwise. In modern versions, all players roll the die/dice and the highest total starts the game, with play again proceeding clockwise. Players roll the die/dice and move along the board's corridor spaces, or into the rooms accordingly.\nThe objective of the game is to deduce the details of the murder, i.e. the cards in the envelope. There are six characters, six murder weapons, and nine rooms, leaving the players with 324 possibilities. As soon as a player enters a room, they may make a suggestion as to the details, naming a suspect, the room they are in, and the weapon. For example: \"I suspect Professor Plum, in the Dining Room, with the candlestick\". The player's suggestions must include the room they are currently in and may not be made in the corridors. The tokens for the suggested suspect and weapon are immediately moved into that room if they are not both already present. Players may include themselves and weapons/rooms for which they hold cards in their suggestions.\nOnce a player makes a suggestion, the others are called upon to disprove it. If the player to their left holds any of the three named cards, that player must privately show one (and only one) of the cards to the current player. Otherwise, the process continues clockwise around the table until either one player disproves the accusation, or no one can do so. A player's turn normally ends once their suggestion is completed.\nA player who believes they have determined the correct elements may make an accusation on their turn. The accusation can include any room, not necessarily the one occupied by the player (if any), and may be made immediately following a suggestion. The accusing player privately checks the three cards in the envelope. If they match the accusation, the player shows them to everyone and wins; if not, the player returns them to the envelope and may not move nor make suggestions/accusations for the remainder of the game. However, the other players can move their tokens into rooms when making suggestions and they must continue to privately show cards to disprove the suggestions. A player who makes a false accusation while blocking the door to a room must move into that room so others can enter and leave. If all players except one make false accusations, the remaining player wins by default.\nIf a player's suggestion has brought another player's token into a room, the second player may make their own suggestion in the room when their turn comes up, if desired. If not, they may move out of the room, and if able to reach another room, make a suggestion therein, as usual. In the American version, players are not allowed to make suggestions repeatedly by remaining in one room; if they wish to make a second suggestion, they must first spend a turn out of the room.\nStrategy.\nThe choice of playing piece can make a difference. Mrs. Peacock has an immediate advantage of starting one space closer to the first room than any of the other players. Professor Plum can move to the study, and then take the secret passage to the Kitchen, the hardest room to reach. Traditionally, Miss Scarlett had the advantage of moving first, although this has been eliminated with the implementation of the high-roll rule in modern versions.\nMaking as many suggestions as possible maximises how much information a player can gain, which is advantageous. Therefore, moving into a new room as frequently as possible is one way to meet this goal. Mrs. Peacock has an advantage in that she is closest to the Conservatory, a corner room with a secret passage, enabling a player on their turn to move immediately to another room and make a suggestion after rolling the dice. Miss Scarlett has a similar advantage to the Lounge. Players should make good use of the secret passages. Following the shortest path between rooms then is a good choice, even if a player already holds the card representing that room in their hand. Blocking the passage of another player prevents them from attaining rooms from which to make suggestions.\nEach player begins the game with three to six cards in their hand, depending on the number of players. Keeping track of which cards are shown to each player is important in deducing the solution. Detective Notes are supplied with the game to make this task easier. The pads can keep not only a history of which cards are in a player's hand but also which cards have been shown by another player. It can also be useful in deducing which cards the other players have shown one another. For example, if Miss Scarlett disproves Rev. Green's accusation that Mrs. Peacock did the crime in the Ballroom with the Candlestick, a player with both the Ballroom and Mrs. Peacock cards in their hand can then deduce that Miss Scarlett has the Candlestick.\nA player makes a suggestion to learn which cards may be eliminated from suspicion, but in some cases, it may be advantageous for a player to include one of their own cards in a suggestion. This technique can be used for both forcing a player to reveal a different card as well as misleading other players into believing a specific card is suspect. Therefore, moving into a room already held in the player's hand may work to their advantage. Suggestions may also be used to thwart a player's opponent. Since every suggestion results in a suspect token being re-located to the suggested room, a suggestion may be used to prevent another player from achieving their intended destination, preventing them from suggesting a particular room, especially if that player appears to be getting close to a solution.\nEditions.\nParker Brothers and Waddingtons each produced their own unique editions between 1949 and 1992. Hasbro purchased both companies in the early 1990s and continued to produce unique editions for each market until 2002/2003 when the current edition of Cluedo/Clue was first released. At this time, Hasbro produced a unified product across markets. The game was then localised with regional differences in spelling and naming conventions.\nDuring Cluedo's long history, eight unique Clue editions were published in North America (1949, 1956/1960, 1960/1963, 1972, 1986, 1992, 1996, and 2002), including miniaturised \"travel\" editions. However, only three distinct editions of Cluedo were released in the UK \u2013 the longest of which lasted 47 years from its introduction in 1949 until its first successor in 1996. The eighth North American and fourth UK editions constitute the current shared game design. International versions occasionally developed their own unique designs for specific editions, although most drew on the designs and art from either the US or UK editions, and in some cases mixing elements from both, while localising others \u2013 specifically suspect portraits.\nIn July 2008, Hasbro released a revamped look for Clue in a reinvention called \"Clue: Discover the Secrets\". This new version of the game offered major changes to the gameplay and to the characters and their backstories.\nIn July 2016, Hasbro replaced Mrs. White with a new character, Dr. Orchid, represented by an orchid pink piece. In this current standard edition, Mrs. Peacock has a new game-opening opportunity as her starting square is one step closer to the billiard room (with 9steps instead of 10). The squared-off door to the Conservatory makes the room harder for Rev. Green to reach as an opening move and increases the distance between the Ballroom and the Conservatory (from 4steps to 5).\nIn January 2023, Hasbro released a new edition of the game that included new miniatures and updated art and character backstories.\nWhile the suspects' appearance and interior design of Dr. Black's/Mr. Boddy's mansion changed with each edition, the weapons underwent relatively minor changes, with the only major redesign occurring in the fourth 1972 US edition, which was adopted by the second 1996 UK edition and remains the standard configuration across all Classic Detective Game versions since. The artwork for the previous US editions tended to reflect the current popular style at the time they were released. The earlier UK editions were more artistically stylised themes. From 1972 on, the US editions presented lush box cover art depicting the six suspects in various candid poses within a room of the mansion. The UK would finally adopt this style only in its third release in 2000, prior to which Cluedo boxes depicted basic representations of the contents. Such lavish box art illustrations have become a hallmark of the game since copied for the numerous licensed variants which pay homage to Clue.\n\"Cluedo: Discover the Secrets\".\nIn August 2008, Hasbro redesigned and updated the board, characters, weapons, and rooms. Changes to the rules of gameplay were made, some to accommodate the new features.\nThe suspects have new names and backgrounds, as well as differing abilities that may be used during the game. The revolver is now a pistol, the lead pipe and spanner/wrench have been removed, and a baseball bat, axe, dumbbell, trophy, and poison have been added. The nine rooms have changed to (in clockwise order): Hall, Guest House, Dining Room, Kitchen, Patio, Spa, Theatre, Living Room, and Observatory.\nThere is also a second deck of cards\u2014the Intrigue cards. In this deck, there are two types of cards, Keepers and Clocks. Keepers are special abilities; for example, \"You can see the card\". There are eight clocks\u2014the first seven drawn do nothing\u2014whoever draws the eighth is killed by the murderer and is out of the game.\nThe player must move to the indoor swimming pool in the centre of the board to make an accusation. This adds some challenge versus the ability to make accusations from anywhere in the original game.\nThe most significant change to gameplay is that once the suspect cards have been taken, the remaining cards are dealt so that all players have an even number of cards (rather than dealt out so that \"one player may have a slight advantage\"). This means that depending on the number of players a number of cards are left over. These cards are placed face down in the middle and are not seen unless a player takes a turn in the pool room to look at them.\nThe changes to the game have been criticised in the media, and by fans of the game, for unnecessarily altering classic cultural icons.\nAs of 2014[ [update]], Hasbro no longer sells the game via its website, but they do continue to sell a version of it as part of their \"Grab &amp; Go\" travel series. Notably, it plays identically to standard classic rules but visually continues to use the new Discover the Secrets room layout, and two of the new weapons, as well as other design artwork. However, the Intrigue cards are no longer a part of the game.\n\"Clue/Cluedo: The Classic Mystery Game\".\n\"Clue/Cluedo\" is a digital adaptation based on the official Hasbro board game developed by Marmalade Game Studio for iOS and Android mobile devices as well as Steam and Nintendo Switch. The object of the game is to determine who killed the game's victim Dr. Black (or Mr. Boddy). The player, as one of the six suspects, will ask questions and take notes. The overall goal is to solve the crime first.\nFranchise.\nThe board game franchise spans a 1985 feature film, a 1997 musical, a 2011 mini-series, numerous books, and several video games. First staged in 2017, a began a U.S. national tour in 2024.\nWorldwide differences.\nBesides some rule differences listed above, some versions label the characters, weapons, rooms and in some instances the game itself differently.\nIn Canada and the U.S., the game is known as \"Clue\". It was retitled because the traditional British board game \"Ludo\", on which the name is based, was less well known there than its American variant \"Parcheesi\".\nThe North American versions of \"Clue\" also replace the character \"Reverend Green\" from the original \"Cluedo\" with \"Mr. Green\". This is the only region to continue to make such a change. Minor changes include \"Miss Scarlett\" with her name spelled with one 't', the spanner being called a wrench, and the dagger being renamed a knife. In the 2016 U.S. edition, the knife was changed to a dagger. Until 2003, the lead piping was known as the lead pipe only in the North American edition.\nIn some international versions of the game (mostly the Spanish-language ones), the colours of some pieces are different, so as to correspond with the changes to each suspect's unique foreign name variations. In some cases, rooms and weapons are changed in addition to other regional variances.\nIn South America, it is licensed and sold under several names. In Brazil, it is marketed under the Portuguese name .\nRecognition.\n\"Games\" magazine included \"Clue\" in their Top 100 Games of 1980, praising it as \"still \"the\" classic detective game\", Top 100 Games of 1981 (\"[the] characters in this 32-year-old game have become household words\"), and Top 100 Games of 1982 (\"Millions love this classic mystery game\").\nIn November 2025, Anthony Pratt's daughter, Marcia Lewis, donated her father's correspondence and memorabilia relating to the game, including an original version, to Birmingham Archives at the Library of Birmingham and to Birmingham Museum and Art Gallery.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44168", "revid": "784330", "url": "https://en.wikipedia.org/wiki?curid=44168", "title": "Brahms (disambiguation)", "text": "Johannes Brahms (1833\u20131897) was a German composer and pianist.\nBrahms may also refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44169", "revid": "23368140", "url": "https://en.wikipedia.org/wiki?curid=44169", "title": "Purcell (disambiguation)", "text": "Henry Purcell (1659\u20131695) was an English composer.\nPurcell may also refer to:\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44170", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=44170", "title": "Haydn", "text": ""}
{"id": "44172", "revid": "13541347", "url": "https://en.wikipedia.org/wiki?curid=44172", "title": "Vivaldi", "text": ""}
{"id": "44173", "revid": "275202", "url": "https://en.wikipedia.org/wiki?curid=44173", "title": "Wagner (disambiguation)", "text": "Richard Wagner (1813\u20131883) was a German composer.\nWagner may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44174", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=44174", "title": "Florentine Republic", "text": ""}
{"id": "44175", "revid": "11096", "url": "https://en.wikipedia.org/wiki?curid=44175", "title": "House of Medici", "text": "Italian banking family and political dynasty\nThe House of Medici ( , ; ) was an Italian banking family and political dynasty that first consolidated power in the Republic of Florence under Cosimo de' Medici and his grandson Lorenzo \"the Magnificent\" during the first half of the 15th century. The family originated in the Mugello region of Tuscany, and prospered gradually in trade until it was able to fund the Medici Bank. This bank was the largest in Europe in the 15th century and facilitated the Medicis' rise to political power in Florence, although they officially remained citizens rather than monarchs until the 16th century.\nIn 1532, the family acquired the hereditary title Duke of Florence. In 1569, the duchy was elevated to the Grand Duchy of Tuscany after territorial expansion. The Medici ruled the Grand Duchy from its inception under the builder Cosimo I until 1737, with the death of Gian Gastone de' Medici. The Medici produced four popes of the Catholic Church\u2014Pope Leo X (1513\u20131521), Pope Clement VII (1523\u20131534), Pope Pius IV (1559\u20131565) and Pope Leo XI (1605)\u2014and two queens of France\u2014Catherine de' Medici (1547\u20131559) and Marie de' Medici (1600\u20131610). The Medici's grand duchy witnessed degrees of economic growth under the early grand dukes, but was bankrupt by the time of Cosimo III de' Medici (r. 1670\u20131723).\nThe Medicis' wealth and influence was initially derived from the textile trade guided by the wool guild of Florence, the \"Arte della Lana\". Like other families ruling in Italian , the Medici dominated their city's government, were able to bring Florence under their family's power, and created an environment in which art and humanism flourished. The Italian Renaissance was inspired by the Medici along with other families of Italy, such as the Visconti and Sforza in Milan, the Este in Ferrara, the Borgia and Della Rovere in Rome, and the Gonzaga in Mantua.\nThe Medici Bank, from when it was created in 1397 to its fall in 1494, was one of the most prosperous and respected institutions in Europe, and the Medici family was considered the wealthiest in Europe for a time. From this base, they acquired political power initially in Florence and later in wider Italy and Europe. They were among the earliest businesses to use the general ledger system of accounting through the development of the double-entry bookkeeping system for tracking credits and debits.\nThe Medici family financed the construction of Saint Peter's Basilica and Florence Cathedral, and were patrons of Donatello, Brunelleschi, Botticelli, Leonardo da Vinci, Michelangelo, Raphael, Machiavelli, Galileo, and Francesco Redi, among many others in the arts and sciences. They funded the invention of the piano, and arguably that of opera. They were also protagonists of the Counter-Reformation, from the beginning of the Reformation through the Council of Trent and the French Wars of Religion.\nHistory.\n13th century (1200s).\nThe Medici family came from the agricultural Mugello region north of Florence, and they are first mentioned in a document of 1230. The origin of the name is uncertain. \"Medici\" is the plural of \"medico\", meaning \"medical doctor\".\nIn 1293, the Ordinances of Justice were enacted; effectively, they became the constitution of the Republic of Florence throughout the Italian Renaissance.\nFor most of the 13th century, the leading banking centre in Italy was Siena. In 1298, however, as the century came to a close, one of the leading banking families of Europe, the Bonsignoris, went bankrupt, and the city of Siena lost its status as the banking centre of Italy to Florence.\n14th century (1300s).\n&lt;templatestyles src=\"Stack/styles.css\"/&gt;\nMembers of the Medici family rose to some prominence in the early 14th century in the wool trade, especially with France and Spain. Despite the presence of some Medici in the city's government institutions, they were still far less notable than other outstanding families such as the Albizzi or the Strozzi. One Salvestro de' Medici was speaker of the woolmakers' guild during the Ciompi revolt of 1378\u20131382, and one Antonio de' Medici was exiled from Florence in 1396. Involvement in another plot in 1400 caused all branches of the family to be banned from Florentine politics for twenty years, with the exception of two.\nIn 1397, late in the 14th century, the Medici dynasty properly began with the founding of the Medici Bank in Florence.\nGiovanni di Bicci de' Medici (c. 1360\u20131429), son of Averardo de' Medici (1320\u20131363), increased the wealth of the family through the creation of the Medici Bank, and he became one of the richest men in the city of Florence. Although he never held any political office, he gained popular support for the family through his backing for the introduction of a proportional system of taxation. Giovanni's son Cosimo the Elder, (father of the country), took over in 1434 as gran maestro (the unofficial head of the Florentine Republic).\nRise to power.\nUntil the late 14th century, the leading family of Florence was the House of Albizzi.\nThe city's numerous luxurious palazzi were becoming surrounded by townhouses built by the prospering merchant class.\nThe main challengers to the Albizzi family were the Medici, first under Giovanni di Bicci de' Medici, later under his son Cosimo di Giovanni de' Medici and great-grandson, Lorenzo de' Medici. The Medici controlled the Medici Bank\u2014then Europe's largest bank\u2014and an array of other enterprises in Florence and elsewhere.\nThe Medici family was connected to most other elite families of the time through marriages of convenience, partnerships, or employment, so the family had a central position in the social network: several families had systematic access to the rest of the elite families only through the Medici, perhaps similar to banking relationships. Some examples of these families include the Bardi, Altoviti, Ridolfi, Zurla, Cavalcanti and the Tornabuoni. This has been suggested as a reason for the rise of the Medici family.\n15th century (1400s).\nIn 1433, the Medici additionally benefited from the discovery of vast deposits of alum in Tolfa. Alum is essential as a mordant in the dyeing of certain cloths and was used extensively in Florence, where the main industry was textile manufacturing. Before the Medici, the Turks were the only exporters of alum, so Europe was forced to buy from them until the discovery in Tolfa. Pius II granted the Medici family a monopoly on the mining there, making them the primary producers of alum in Europe.\nIn 1433, the Albizzi managed to have Cosimo exiled.\nThe next year, however, in 1434, a pro-Medici Signoria (civic government), led by Tommaso Soderini, Oddo Altoviti, and Lucca Pitti, was elected and Cosimo returned. The Medici became the city's leading family, a position they would hold for the next three centuries. Florence remained a republic until 1537, traditionally marking the end of the High Renaissance in Florence, but the instruments of republican government were firmly under the control of the Medici and their allies, save during intervals after 1494 and 1527. Cosimo and Lorenzo rarely held official posts but were the unquestioned leaders.\nThree successive generations of the Medici\u2014Cosimo, Piero, and Lorenzo\u2014ruled over Florence through the greater part of the 15th century. They clearly dominated Florentine representative government without abolishing it altogether. These three members of the Medici family had great skills in the management of so \"restive and independent a city\" as Florence. When Lorenzo died in 1492, however, his son Piero proved quite incapable of responding successfully to challenges caused by the French invasion of Italy in 1492, and within two years, he and his supporters were forced into exile and replaced with a republican government.\nPiero de' Medici (1416\u20131469), Cosimo's son, was in power for five years (1464\u201369). He was called \"Piero the Gouty\" because of the gout that pained his foot and led to his death. Unlike his father, Piero had little interest in the arts. Due to his illness, he often stayed at home bedridden.\nLorenzo de' Medici (1449\u20131492), called \"the Magnificent\", was more capable of leading and ruling a city, but he neglected the family banking business, which led to its ultimate ruin. To ensure the continuance of his family's success, Lorenzo planned his children's future careers for them. He groomed the headstrong Piero II to follow as his successor in civil leadership; Giovanni (future Pope Leo X) was placed in the church at an early age; and his daughter Maddalena was provided with a sumptuous dowry to make a politically advantageous marriage to a son of Pope Innocent VIII that cemented the alliance between the Medici and the Roman branches of the Cybo and Altoviti families.\nThe Pazzi conspiracy of 1478 was an attempt to depose the Medici family by killing Lorenzo with his younger brother Giuliano during Easter services; the assassination attempt ended with the death of Giuliano and an injured Lorenzo. The conspiracy involved the Pazzi and Salviati families, both rival banking families seeking to end the influence of the Medici, as well as the priest presiding over the church services, the Archbishop of Pisa, and even Pope Sixtus IV to a degree. The conspirators approached Sixtus IV in the hopes of gaining his approval, as he and the Medici had a long rivalry themselves, but the pope gave no official sanction to the plan. Despite his refusal of official approval, the pope nonetheless allowed the plot to proceed without interfering, and, after the failed assassination of Lorenzo, also gave dispensation for crimes done in the service of the church. After this, Lorenzo adopted his brother's illegitimate son Giulio de' Medici (1478\u20131535), the future Pope Clement VII. Lorenzo's son Piero II took over as the head of Florence after Lorenzo's death. The Medici were expelled from Florence from 1494 to 1512 after Piero acceded to all of the demands of invader Charles VIII of France.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nIn the dangerous circumstances in which our city is placed, the time for deliberation is past. Action must be taken... I have decided, with your approval, to sail for Naples immediately, believing that as I am the person against whom the activities of our enemies are chiefly directed, I may, perhaps, by delivering myself into their hands, be the means of restoring peace to our fellow-citizens. As I have had more honour and responsibility among you than any private citizen has had in our day, I am more bound than any other person to serve our country, even at the risk of my life. With this intention I now go. Perhaps God wills that this war, which began in the blood of my brother and of myself, should be ended by any means. My desire is that by my life or my death, my misfortune or my prosperity, I may contribute to the welfare of our city... I go full of hope, praying to God to give me grace to perform what every citizen should at all times be ready to perform for his country.\n\u2014 \"Lorenzo de' Medici, 1479\"\n16th century.\nThe exile of the Medici lasted until 1512, after which the \"senior\" branch of the family\u2014those descended from Cosimo the Elder\u2014were able to rule until the assassination of Alessandro de' Medici, first Duke of Florence, in 1537. This century-long rule was interrupted only on two occasions (between 1494\u20131512 and 1527\u20131530), when anti-Medici factions took control of Florence. Following the assassination of Duke Alessandro, power passed to the \"junior\" Medici branch\u2014those descended from Lorenzo the Elder, the youngest son of Giovanni di Bicci, starting with his great-great-grandson Cosimo I \"the Great\".\nCosimo the Elder and his father started the Medici foundations in banking and manufacturing\u2014including a form of franchises. The family's influence grew with its patronage of wealth, art, and culture. Ultimately, it reached its zenith in the papacy and continued to flourish for centuries afterward as Dukes of Florence and Tuscany. At least half, probably more, of Florence's people were employed by the Medici and their foundational branches in business.\nMedici popes.\nThe Medici briefly became leaders of Western Christendom through their two famous 16th century popes, Leo X and Clement VII. Both also served as \"de facto\" political rulers of Rome, Florence, and large swaths of Italy known as the Papal States. They were generous patrons of the arts who commissioned masterpieces such as Raphael's \"Transfiguration\" and Michelangelo's \"The Last Judgment\"; however, their reigns coincided with troubles for the Vatican, including Martin Luther's Protestant Reformation and the infamous sack of Rome in 1527.\nLeo X's fun-loving pontificate bankrupted Vatican coffers and accrued massive debts. From Leo's election as pope in 1513 to his death in 1521, Florence was overseen, in turn, by Giuliano de' Medici, Duke of Nemours, Lorenzo de' Medici, Duke of Urbino, and Giulio de' Medici, the latter of whom became Pope Clement VII.\nClement VII's tumultuous pontificate was dominated by a rapid succession of political crises\u2014many long in the making\u2014that resulted in the sack of Rome by the armies of Holy Roman Emperor Charles V in 1527 and rise of the Salviati, Altoviti and Strozzi as the leading bankers of the Roman Curia. From the time of Clement's election as pope in 1523 until the sack of Rome, Florence was governed by the young Ippolito de' Medici (future cardinal and vice-chancellor of the Holy Roman Church), Alessandro de' Medici (future duke of Florence), and their guardians. In 1530, after allying himself with Charles V, Pope Clement VII succeeded in securing the engagement of Charles V's daughter Margeret of Austria to his illegitimate nephew (reputedly his son) Alessandro de' Medici. Clement also convinced Charles V to name Alessandro as Duke of Florence. Thus began the reign of Medici monarchs in Florence, which lasted two centuries.\nAfter securing Alessandro de' Medici's dukedom, Pope Clement VII married off his first cousin, twice removed, Catherine de' Medici, to the son of Emperor Charles V's arch-enemy, King Francis I of France\u2014the future King Henry II. This led to the transfer of Medici blood, through Catherine's daughters, to the royal family of Spain through Elisabeth of Valois, and the House of Lorraine through Claude of Valois.\nIn 1534, following a lengthy illness, Pope Clement VII died\u2014and with him the stability of the Medici's \"senior\" branch. In 1535, Ippolito Cardinal de' Medici died under mysterious circumstances. In 1536, Alessandro de' Medici married Charles V's daughter, Margaret of Austria; however, the following year he was assassinated by a resentful cousin, Lorenzino de' Medici. The deaths of Alessandro and Ippolito enabled the Medici's \"junior\" branch to lead Florence.\nMedici Dukes.\nAnother outstanding figure of the 16th-century Medici family was Cosimo I, who rose from relatively modest beginnings in the Mugello to attain supremacy over the whole of Tuscany. Against the opposition of Catherine de' Medici, Pope Paul III and their allies, he prevailed in various battles to conquer Florence's hated rival Siena and found the Grand Duchy of Tuscany. Cosimo purchased a portion of the island of Elba from the Republic of Genoa and based the Tuscan navy there. He died in 1574, succeeded by his eldest surviving son Francesco, whose death without male heirs led to the succession of his younger brother, Ferdinando, in 1587. Francesco married Johanna of Austria, and with his consort produced Eleonora de' Medici, Duchess of Mantua, and Marie de' Medici, Queen of France and Navarre. Through Marie, all succeeding French monarchs (bar the Napoleons) were descended from Francesco.\nFerdinando eagerly assumed the government of Tuscany. He commanded the draining of the Tuscan marshlands, built a road network in southern Tuscany and cultivated trade in Livorno. To augment the Tuscan silk industry, he oversaw the planting of mulberry trees along the major roads (silk worms feed on mulberry leaves). In foreign affairs, he shifted Tuscany away from Habsburg hegemony by marrying the first non-Habsburg marriage candidate since Alessandro, Christina of Lorraine, a granddaughter of Catherine de' Medici. The Spanish reaction was to construct a citadel on their portion of the island of Elba. To strengthen the new Franco-Tuscan alliance, he married his niece, Marie, to Henry IV of France. Henry explicitly stated that he would defend Tuscany from Spanish aggression, but later reneged, after which Ferdinando was forced to marry his heir, Cosimo, to Maria Maddalena of Austria to assuage Spain (where Maria Maddalena's sister Margaret was the incumbent Queen consort). Ferdinando also sponsored a Tuscan expedition to the New World with the intention of establishing a Tuscan colony, an enterprise that brought no result for permanent colonial acquisitions.\nDespite all of these incentives for economic growth and prosperity, the population of Florence at the dawn of the 17th century was a mere 75,000, far smaller than the other capitals of Italy (i.e., Rome, Milan, Venice, Palermo, and Naples). Francesco and Ferdinando, due to lax distinction between Medici and Tuscan state property, are thought to have been wealthier than their ancestor, Cosimo de' Medici, the founder of the dynasty. The Grand Duke alone had the prerogative to exploit the state's mineral and salt resources, and the fortunes of the Medici were directly tied to the Tuscan economy.\n17th century.\nFerdinando, although no longer a cardinal, exercised much influence at successive conclaves. In 1605, Ferdinando succeeded in getting his candidate, Alessandro de' Medici, elected Pope Leo XI. He died the same month, but his successor, Pope Paul V, was also pro-Medici. Ferdinando's pro-papal foreign policy, however, had drawbacks. Tuscany was overrun with religious orders, not all of whom were obliged to pay taxes. Ferdinando died in 1609, leaving an affluent realm; his inaction in international affairs, however, would have long-reaching consequences down the line.\nIn France, Marie de' Medici was acting as regent for her son, Louis XIII. Louis repudiated her pro-Habsburg policy in 1617. She lived the rest of her life deprived of any political influence.\nFerdinando's successor, Cosimo II, reigned for less than 12 years. He married Maria Maddalena of Austria, with whom he had his eight children, including Margherita de' Medici, Ferdinando II de' Medici, and an Anna de' Medici.\nHe is most remembered as the patron of astronomer Galileo Galilei, whose 1610 treatise, Sidereus Nuncius, was dedicated to him. Cosimo died of consumption (tuberculosis) in 1621.\nCosimo's elder son, Ferdinando, was not yet of legal maturity to succeed him, thus Maria Maddalena and his grandmother, Christina of Lorraine, acted as regents. Their collective regency is known as the \"Turtici\". Maria Maddelana's temperament was analogous to Christina's, and together they aligned Tuscany with the papacy, re-doubled the Tuscan clergy, and allowed the heresy trial of Galileo Galilei to occur. Upon the death of the last Duke of Urbino (Francesco Maria II), instead of claiming the duchy for Ferdinando, who was married to the Duke of Urbino's granddaughter and heiress, Vittoria della Rovere, they permitted it to be annexed by Pope Urban VIII. In 1626, they banned any Tuscan subject from being educated outside the Grand Duchy, a law later overturned, but resurrected by Maria Maddalena's grandson, Cosimo III. Harold Acton, an Anglo-Italian historian, ascribed the decline of Tuscany to the \"Turtici\" regency.\nGrand Duke Ferdinado was obsessed with new technology, and had a variety of hygrometers, barometers, thermometers, and telescopes installed in the Palazzo Pitti. In 1657, Leopoldo de' Medici, the Grand Duke's youngest brother, established the Accademia del Cimento, organized to attract scientists to Florence from all over Tuscany for mutual study.\nTuscany participated in the Wars of Castro (the last time Medicean Tuscany proper was involved in a conflict) and inflicted a defeat on the forces of Pope Urban VIII in 1643. The war effort was costly and the treasury so empty because of it that when the Castro mercenaries were paid for, the state could no longer afford to pay interest on government bonds, with the result that the interest rate was lowered by 0.75%. At that time, the economy was so decrepit that barter trade became prevalent in rural market places.\nFerdinando died on 23 May 1670 afflicted by apoplexy and dropsy. He was interred in the Basilica of San Lorenzo, the Medici's necropolis. At the time of his death, the population of the grand duchy was 730,594; the streets were lined with grass and the buildings on the verge of collapse in Pisa.\nFerdinando's marriage to Vittoria della Rovere produced two children: Cosimo III de' Medici, Grand Duke of Tuscany, and Francesco Maria de' Medici, Duke of Rovere and Montefeltro. Upon Vittoria's death in 1694, her allodial possessions, the Duchies of Rovere and Montefeltro, passed to her younger son.\n18th century: the fall of the dynasty.\nCosimo III married Marguerite Louise d'Orl\u00e9ans, a granddaughter of Henry IV of France and Marie de' Medici. An exceedingly discontented pairing, this union produced three children, notably Anna Maria Luisa de' Medici, Electress Palatine, and the last Medicean Grand Duke of Tuscany, Gian Gastone de' Medici.\nJohann Wilhelm, Elector Palatine, Anna Maria Luisa's spouse, successfully requisitioned the dignity \"Royal Highness\" for the Grand Duke and his family in 1691, despite the fact that they had no claim to any kingdom. Cosimo frequently paid the Holy Roman Emperor, his nominal feudal overlord, exorbitant dues, and he sent munitions to the emperor during the Battle of Vienna.\nThe Medici lacked male heirs, and by 1705, the grand ducal treasury was virtually bankrupt. In comparison to the 17th century, the population of Florence declined by 50%, and the population of the grand duchy as a whole declined by an estimated 40%. Cosimo desperately tried to reach a settlement with the European powers, but Tuscany's legal status was very complicated: the area of the grand duchy formerly comprising the Republic of Siena was technically a Spanish fief, while the territory of the old Republic of Florence was thought to be under imperial suzerainty. Upon the death of his first son, Cosimo contemplated restoring the Florentine republic, either upon Anna Maria Luisa's death, or on his own, if he predeceased her. The restoration of the republic would entail resigning Siena to the Holy Roman Empire, but, regardless, it was vehemently endorsed by his government. Europe largely ignored Cosimo's plan. Only Great Britain and the Dutch Republic gave any credence to it, and the plan ultimately died with Cosimo III in 1723.\nOn 4 April 1718, Great Britain, France and the Dutch Republic (also later, Austria) selected Don Carlos of Spain, the elder child of Elisabeth Farnese and Philip V of Spain, as the Tuscan heir. By 1722, the electress was not even acknowledged as heiress, and Cosimo was reduced to spectator at the conferences for Tuscany's future. On 25 October 1723, six days before his death, Grand Duke Cosimo disseminated a final proclamation commanding that Tuscany stay independent: Anna Maria Luisa would succeed uninhibited to Tuscany after Gian Gastone, and the grand duke reserved the right to choose his successor. However, these portions of his proclamation were completely ignored, and he died a few days later.\nGian Gastone despised the electress for engineering his catastrophic marriage to Anna Maria Franziska of Saxe-Lauenburg; while she abhorred her brother's liberal policies, he repealed all of his father's anti-Semitic statutes. Gian Gastone revelled in upsetting her. On 25 October 1731, a Spanish detachment occupied Florence on behalf of Don Carlos, who disembarked in Tuscany in December of the same year.\nThe \"Ruspanti\", Gian Gastone's decrepit entourage, loathed the electress, and she them. Duchess Violante of Bavaria, Gian Gastone's sister-in-law, tried to withdraw the grand duke from the sphere of influence of the \"Ruspanti\" by organising banquets. His conduct at the banquets was less than regal; he often vomited repeatedly into his napkin, belched, and regaled those present with socially inappropriate jokes. Following a sprained ankle in 1731, he remained confined to his bed for the rest of his life. The bed, often smelling of faeces, was occasionally cleaned by Violante.\nIn 1736, following the War of the Polish Succession, Don Carlos was disbarred from Tuscany, and Francis III of Lorraine was made heir in his stead. In January 1737, the Spanish troops withdrew from Tuscany, and were replaced by Austrians.\nGian Gastone died on 9 July 1737, surrounded by prelates and his sister. Anna Maria Luisa was offered a nominal regency by the Prince de Craon until the new grand duke could peregrinate to Tuscany, but declined. Upon her brother's death, she received all the House of Medici's allodial possessions.\nAnna Maria Luisa signed the \"Patto di Famiglia\" (\"family pact\") on 31 October 1737. In collaboration with the Holy Roman Emperor and Grand Duke Francis of Lorraine, she willed all the personal property of the Medici to the Tuscan state, provided that nothing was ever removed from Florence.\nThe \"Lorrainers\", as the occupying forces were called, were popularly loathed, but the regent, the Prince de Craon, allowed the electress to live unperturbed in the Palazzo Pitti. She occupied herself with financing and overseeing the construction of the Basilica of San Lorenzo, started in 1604 by Ferdinando I, at a cost to the state of 1,000 crowns per week.\nThe electress donated much of her fortune to charity: \u00a34,000 a month. On 19 February 1743, she died, and the grand ducal line of the House of Medici died with her. The Florentines grieved her, and she was interred in the crypt that she helped to complete, San Lorenzo.\nThe extinction of the main Medici dynasty and the accession in 1737 of Francis Stephen, Duke of Lorraine and husband of Maria Theresa of Austria, led to Tuscany's temporary inclusion in the territories of the Austrian crown. The line of the Princes of Ottajano, an extant branch of the House of Medici who were eligible to inherit the grand duchy of Tuscany when the last male of the senior branch died in 1737, could have carried on as Medici sovereigns but for the intervention of Europe's major powers, which allocated the sovereignty of Florence elsewhere.\nAs a consequence, the grand duchy expired and the territory became a secundogeniture of the Habsburg-Lorraine dynasty. The first grand duke of the new dynasty, Francis I, was a great-great-great-grandson of Francesco I de' Medici, thus he continued the Medicean Dynasty on the throne of Tuscany through the female line. The Habsburgs were deposed in favor of the House of Bourbon-Parma in 1801 (themselves deposed in 1807), but were later restored at the Congress of Vienna. Tuscany became a province of the United Kingdom of Italy in 1861. However, several extant branches of the House of Medici survive, including the Princes of Ottajano, the Medici Tornaquinci, and the Verona Medici Counts of Caprara and Gavardo. (see Medici family tree)\nLegacy.\nAs a preeminent and influential family, the Medici's attracted both considerable controversy and praise from their contemporaries and modern sources. Ferdinand Schevill described there being \"an uncommonly wide range\" of opinion about the Medici's, with more critical sources from native contemporaries giving way to a more favorable view of the family by nationalists in other countries in later centuries. Machiavelli's \"The Prince\" was mixed with its opinions of the Medici, and ultimately came to be dedicated to Lorenzo di Piero de' Medici. Common criticisms of the family have described it as corrupt and with questionable virtues.\nArtistic patronage.\nThe greatest accomplishments of the Medici were in the sponsorship of art and architecture, mainly early and High Renaissance art and architecture. The Medici were responsible for a high proportion of the major Florentine works of art created during their period of rule. Their support was critical, since artists generally began work on their projects only after they had received commissions. Giovanni di Bicci de' Medici, the first patron of the arts in the family, aided Masaccio and commissioned Filippo Brunelleschi for the reconstruction of the Basilica of San Lorenzo, Florence in 1419. Cosimo the Elder's notable artistic associates were Donatello and Fra Angelico. In later years the most significant prot\u00e9g\u00e9 of the Medici family was Michelangelo Buonarroti (1475\u20131564), who produced work for a number of family members, beginning with Lorenzo the Magnificent, who was said to be extremely fond of the young Michelangelo and invited him to study the family collection of antique sculpture. Lorenzo also served as patron to Leonardo da Vinci (1452\u20131519) for seven years. Indeed, Lorenzo was an artist in his own right and an author of poetry and song; his support of the arts and letters is seen as a high point in Medici patronage.\nAfter Lorenzo's death the puritanical Dominican friar Girolamo Savonarola rose to prominence, warning Florentines against excessive luxury. Under Savonarola's fanatical leadership many great works were \"voluntarily\" destroyed in the Bonfire of the Vanities (February 7, 1497). The following year, on 23 May 1498, Savonarola and two young supporters were burned at the stake in the Piazza della Signoria, the same location as his bonfire. In addition to commissions for art and architecture, the Medici were prolific collectors and today their acquisitions form the core of the Uffizi museum in Florence. In architecture, the Medici were responsible for some notable features of Florence, including the Uffizi Gallery, the Boboli Gardens, the Belvedere, the Medici Chapel and the Palazzo Medici.\nLater, in Rome, the Medici popes continued in the family tradition of patronizing artists in Rome. Pope Leo X would chiefly commission works from Raphael, whereas Pope Clement VII commissioned Michelangelo to paint the altar wall of the Sistine Chapel just before the pontiff's death in 1534. Eleanor of Toledo, a princess of Spain and wife of Cosimo I the Great, purchased the Pitti Palace from Buonaccorso Pitti in 1550. Cosimo in turn patronized Vasari, who erected the Uffizi Gallery in 1560 and founded the Accademia delle Arti del Disegno \u2013 (\"Academy of the Arts of Drawing\") in 1563. Marie de' Medici, widow of Henry IV of France and mother of Louis XIII, is the subject of a commissioned cycle of paintings known as the Marie de' Medici cycle, painted for the Luxembourg Palace by court painter Peter Paul Rubens in 1622\u201323.\nAlthough none of the Medici themselves were scientists, the family is well known to have been the patrons of the famous Galileo Galilei, who tutored multiple generations of Medici children and was an important figurehead for his patron's quest for power. Galileo's patronage was eventually abandoned by Ferdinando II, when the Inquisition accused Galileo of heresy. However, the Medici family did afford the scientist a safe haven for many years. Galileo named the four largest moons of Jupiter after four Medici children he tutored, although the names Galileo used are not the names currently used.\nMain genealogical table.\nThe table below shows the origins of the Medici:\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;Medici Family Tree: Origins\nThis extract shows the branch that gave rise to the celebrated branch of the Medici descending from Giovanni \"di Bicci\", who founded the Medici fortunes:\nThis is the branch of Cosimo's brother, Lorenzo, called the \"Popolano\" Branch, which gave rise to the Grand-Dukes of Tuscany:\nCoats of arms.\nThe origin of the Medici coat of arms is not recorded. One unproven story traces their ancestry to a knight of Charlemagne's, Averardo, who defeated a giant, Mugello. In reward, Charlemagne is said to have rewarded Averardo with the shield mauled by the giant, with the dents in the shape of balls, and the giant's lands in Mugello.\nThe simplest, though also unproven, theory suggests that the balls represented coins copied from the coat of arms of the Guild of Moneychangers () to which the Medici belonged. That shield was red strewn with Byzantine coins (bezants). The number of balls also varied with time, as shown below. It has also been argued that these coins referenced the three coins or golden balls associated with St. Nicholas, particularly as the saint was invoked by Italian bankers as they took oaths.\nAs an Italian vocabulary word, \"medici\" means \"medical doctors\" and identifications with the family members as physicians may be found among their names as early as the eleventh century. Fanciful stories depict the images as pills or cupping glasses, a late-medieval medical instrument used to draw blood. Pills did not exist until much later and bloodletting was not a common practice at the time of the first Medici coat of arms. Art historian Rocky Ruggiero suggests plausibly however, that the images may represent whole ripe blood oranges that typically are grown in Italy. Although knowledge of vitamins did not exist at the time, the benefit of oranges for certain diseases was recognized and their association with recommendations by medical doctors suggests to Ruggiero that this likely is the imagery intended in the coats of arms for the Medici family.\nAlternatively, it has been suggested that the Medici coat of arms was initially inspired by symbols drawn from Etruscan votive sculpture, examples of which feature an oval dome with balls (echoing the forms of the Medici shield), as well as six balls within a triangle (as found in the alternative, triangular version of the Medici emblem). This particular influence offers an explanation for the red hue of the Medici balls, the colour of the terracotta sculpture. It would also have reflected the family's interest in Etruscan art and culture. In addition, the notion of Etruscan votive sculpture would have chimed with the participation of the Medici in the religious custom of offering up votive statues, a practice that recalled the ancient Etruscan convention of donating sculptures in the hope of, or gratitude for, divine favour. Such favours would have included the wish for a strong and healthy family, both for the supplicant and their descendants.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "44176", "revid": "11292982", "url": "https://en.wikipedia.org/wiki?curid=44176", "title": "Giovanni de Medici", "text": ""}
{"id": "44177", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=44177", "title": "Giulio de Medici", "text": ""}
{"id": "44178", "revid": "388090", "url": "https://en.wikipedia.org/wiki?curid=44178", "title": "Hanlon's razor", "text": "Adage to assume stupidity over malice\nHanlon's razor is an adage, or rule of thumb, that states: \"Never attribute to malice that which is adequately explained by stupidity.\" It is a philosophical razor that suggests a way of eliminating unlikely explanations for human behavior. It is purportedly named after one Robert J. Hanlon, who submitted the statement to \"Murphy's Law Book Two: More Reasons Why Things Go Wrong!\" (1980). Similar statements have been recorded since at least the 18th century.\nOrigin.\nThe adage was a submission credited in print to Robert J. Hanlon of Scranton, Pennsylvania, in a compilation of various jokes related to Murphy's law published in Arthur Bloch's \"Murphy's Law Book Two: More Reasons Why Things Go Wrong!\" (1980).\nA similar quotation appears in Robert A. Heinlein's 1941 novella \"Logic of Empire\". The character Doc in the story describes the \"devil theory\" fallacy, explaining, \"You have attributed conditions to villainy that simply result from stupidity.\"\nHanlon's razor became well known after its inclusion in the \"Jargon File\", a glossary of computer programmer slang, in 1990. Later that year, the \"Jargon File\" editors noted lack of knowledge of the term's derivation and the existence of a similar epigram by William James, although this was possibly intended as a reference to William James Laidlay. In 1996, the \"Jargon File\" entry on Hanlon's Razor noted the existence of the phrase in Heinlein's novella, with speculation that Hanlon's Razor might be a corruption of \"Heinlein's Razor\". The link to Murphy's law was described in a pair of 2001 blog entries by Quentin Stafford-Fraser, citing emails from Joseph E. Bigler. In 2002, the \"Jargon File\" entry noted the same. The \"Jargon File\" now calls it a \"Murphyism\".\nThe name was inspired by Occam's razor.\nVariations.\nGrey's law (a humorous parallel to Arthur C. Clarke's 3rd law): &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Any sufficiently advanced incompetence is indistinguishable from malice.\nDouglas W. Hubbard quoted Hanlon's razor and added \"a clumsier but more accurate corollary\": &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Never attribute to malice or stupidity that which can be explained by moderately rational individuals following incentives in a complex system. \nA variation appears in \"The Wheels of Chance\" (1896) by H. G. Wells:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There is very little deliberate wickedness in the world. The stupidity of our selfishness gives much the same results indeed, but in the ethical laboratory it shows a different nature.\nA similar quote is also misattributed to Napoleon. Andrew Roberts, in , quotes from Churchill's correspondence with King George VI in February 1943 regarding disagreements with Charles de Gaulle: \"His insolence\u00a0... may be founded on stupidity rather than malice.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44179", "revid": "20957809", "url": "https://en.wikipedia.org/wiki?curid=44179", "title": "Satellite temperature measurement", "text": "Type of Earth observation from space\nSatellite temperature measurements are inferences of the temperature of the atmosphere at various altitudes as well as sea and land surface temperatures obtained from radiometric measurements by satellites. These measurements can be used to locate weather fronts, monitor the El Ni\u00f1o-Southern Oscillation, determine the strength of tropical cyclones, study urban heat islands and monitor the global climate. Wildfires, volcanos, and industrial hot spots can also be found via thermal imaging from weather satellites.\nWeather satellites do not measure temperature directly. They measure radiances in various wavelength bands. Since 1978 microwave sounding units (MSUs) on National Oceanic and Atmospheric Administration polar orbiting satellites have measured the intensity of upwelling microwave radiation from atmospheric oxygen, which is related to the temperature of broad vertical layers of the atmosphere. Measurements of infrared radiation pertaining to sea surface temperature have been collected since 1967.\nSatellite datasets show that over the past four decades the troposphere has warmed and the stratosphere has cooled. Both of these trends are consistent with the influence of increasing atmospheric concentrations of greenhouse gases.\nPrinciples.\nSatellites measure radiances in various wavelength bands, which must then be mathematically inverted to obtain indirect inferences of temperature. The resulting temperature profiles depend on details of the methods that are used to obtain temperatures from radiances. As a result, different groups that have analyzed the satellite data have produced differing temperature datasets.\nThe satellite time series is not homogeneous. It is constructed from a series of satellites with similar but not identical sensors. The sensors also deteriorate over time, and corrections are necessary for orbital drift and decay. Particularly large differences between reconstructed temperature series occur at the few times when there is little temporal overlap between successive satellites, making intercalibration difficult.\nInfrared measurements.\nSurface measurements.\nInfrared radiation can be used to measure both the temperature of the surface (using \"window\" wavelengths to which the atmosphere is transparent), and the temperature of the atmosphere (using wavelengths for which the atmosphere is not transparent, or measuring cloud top temperatures in infrared windows).\nSatellites used to retrieve surface temperatures via measurement of thermal infrared in general require cloud-free conditions. Some of the instruments include the Advanced Very High Resolution Radiometer (AVHRR), Along Track Scanning Radiometers (AASTR), Visible Infrared Imaging Radiometer Suite (VIIRS), the Atmospheric Infrared Sounder (AIRS), and the ACE Fourier Transform Spectrometer (ACE\u2010FTS) on the Canadian SCISAT-1 satellite.\nWeather satellites have been available to infer sea surface temperature (SST) information since 1967, with the first global composites occurring during 1970. Since 1982, satellites have been increasingly utilized to measure SST and have allowed its spatial and temporal variation to be viewed more fully. For example, changes in SST monitored via satellite have been used to document the progression of the El Ni\u00f1o-Southern Oscillation since the 1970s.\nOver land the retrieval of temperature from radiances is harder, because of inhomogeneities in the surface. Studies have been conducted on the urban heat island effect via satellite imagery. By using the fractal technique, Weng, Q. et al. characterized the spatial pattern of urban heat island. Use of advanced very high resolution infrared satellite imagery can be used, in the absence of cloudiness, to detect density discontinuities (weather fronts) such as cold fronts at ground level. Using the Dvorak technique, infrared satellite imagery can be used to determine the temperature difference between the eye and the cloud top temperature of the central dense overcast of mature tropical cyclones to estimate their maximum sustained winds and their minimum central pressures.\nAlong Track Scanning Radiometers aboard weather satellites are able to detect wildfires, which show up at night as pixels with a greater temperature than . The Moderate-Resolution Imaging Spectroradiometer aboard the Terra satellite can detect thermal hot spots associated with wildfires, volcanoes, and industrial hot spots.\nThe Atmospheric Infrared Sounder on the Aqua satellite, launched in 2002, uses infrared detection to measure near-surface temperature.\nStratosphere measurements.\nStratospheric temperature measurements are made from the Stratospheric Sounding Unit (SSU) instruments, which are three-channel infrared (IR) radiometers. Since this measures infrared emission from carbon dioxide, the atmospheric opacity is higher and hence the temperature is measured at a higher altitude (stratosphere) than microwave measurements.\nSince 1979 the Stratospheric sounding units (SSUs) on the NOAA operational satellites have provided near global stratospheric temperature data above the lower stratosphere.\nThe SSU is a far-infrared spectrometer employing a pressure modulation technique to make measurement in three channels in the 15 \u03bcm carbon dioxide absorption band. The three channels use the same frequency but different carbon dioxide cell pressure, the corresponding weighting functions peaks at 29\u00a0km for channel 1, 37\u00a0km for channel 2 and 45\u00a0km for channel 3.\nThe process of deriving trends from SSUs measurement has proved particularly difficult because of satellite drift, inter-calibration between different satellites with scant overlap and gas leaks in the instrument carbon dioxide pressure cells. Furthermore since the radiances measured by SSUs are due to emission by carbon dioxide the weighting functions move to higher altitudes as the carbon dioxide concentration in the stratosphere increase.\nMid to upper stratosphere temperatures shows a strong negative trend interspersed by transient volcanic warming after the explosive volcanic eruptions of El Chich\u00f3n and Mount Pinatubo, little temperature trend has been observed since 1995.\nThe greatest cooling occurred in the tropical stratosphere consistent with enhanced Brewer-Dobson circulation under greenhouse gas concentrations increase.\nLower stratospheric cooling is mainly caused by the effects of ozone depletion with a possible contribution from increased stratospheric water vapor and greenhouse gases increase. There has been a decline in stratospheric temperatures, interspersed by warmings related to volcanic eruptions. Global Warming theory suggests that the stratosphere should cool while the troposphere warms.\n The long term cooling in the lower stratosphere occurred in two downward steps in temperature both after the transient warming related to explosive volcanic eruptions of El Chich\u00f3n and Mount Pinatubo, this behavior of the global stratospheric temperature has been attributed to global ozone concentration variation in the two years following volcanic eruptions.\nSince 1996 the trend is slightly positive due to ozone recovery juxtaposed to a cooling trend of 0.1K/decade that is consistent with the predicted impact of increased greenhouse gases.\nThe table below shows the stratospheric temperature trend from the SSU measurements in the three different bands, where negative trend indicated cooling.\nMicrowave (tropospheric and stratospheric) measurements.\nMicrowave Sounding Unit (MSU) measurements.\nFrom 1979 to 2005 the microwave sounding units (MSUs) and since 1998 the Advanced Microwave Sounding Units on NOAA polar orbiting weather satellites have measured the intensity of upwelling microwave radiation from atmospheric oxygen. The intensity is proportional to the temperature of broad vertical layers of the atmosphere. Upwelling radiance is measured at different frequencies; these different frequency bands sample a different weighted range of the atmosphere.\nFigure 3 (right) shows the atmospheric levels sampled by different wavelength reconstructions from the satellite measurements, where TLS, TTS, and TTT represent three different wavelengths.\nOther microwave measurements.\nA different technique is used by the Aura spacecraft, the Microwave Limb Sounder, which measure microwave emission horizontally, rather than aiming at the nadir.\nTemperature measurements are also made by GPS radio occultation. This technique measures the refraction of the radio waves transmitted by GPS satellites as they propagate in the Earth's atmosphere, thus allowing vertical temperature and moisture profiles to be measured.\nTemperature measurements on other planets.\nPlanetary science missions also make temperature measurements on other planets and moons of the Solar System, using both infrared techniques (typical of orbiter and flyby missions of planets with solid surfaces) and microwave techniques (more often used for planets with atmospheres). Infrared temperature measurement instruments used in planetary missions include surface temperature measurements taken by the Thermal Emission Spectrometer (TES) instrument on Mars Global Surveyor and the Diviner instrument on the Lunar Reconnaissance Orbiter; and atmospheric temperature measurements taken by the composite infrared spectrometer instrument on the NASA \"Cassini\" spacecraft.\nMicrowave atmospheric temperature measurement instruments include the Microwave Radiometer on the \"Juno\" mission to Jupiter.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44183", "revid": "5846", "url": "https://en.wikipedia.org/wiki?curid=44183", "title": "Ozone depletion", "text": "Atmospheric phenomenon\nOzone depletion consists of two related events observed since the late 1970s: a lowered total amount of ozone in Earth's upper atmosphere, and a much larger springtime decrease in stratospheric ozone (the ozone layer) around Earth's polar regions. The latter phenomenon is referred to as the ozone hole. There are also springtime polar tropospheric ozone depletion events in addition to these stratospheric events.\nThe main causes of ozone depletion and the ozone hole are manufactured chemicals, especially manufactured halocarbon refrigerants, solvents, propellants, and foam-blowing agents (chlorofluorocarbons (CFCs), HCFCs, halons), referred to as \"ozone-depleting substances\" (ODS). These compounds are transported into the stratosphere by turbulent mixing after being emitted from the surface, mixing much faster than the molecules can settle. Once in the stratosphere, they release atoms from the halogen group through photodissociation, which catalyze the breakdown of ozone (O3) into oxygen (O2). Both types of ozone depletion were observed to increase as emissions of halocarbons increased.\nOzone depletion and the ozone hole have generated worldwide concern over increased cancer risks and other negative effects. The ozone layer prevents harmful wavelengths of ultraviolet (UVB) light from passing through the Earth's atmosphere. These wavelengths cause skin cancer, sunburn, permanent blindness, and cataracts, which were projected to increase dramatically as a result of thinning ozone, as well as harming plants and animals. These concerns led to the adoption of the Montreal Protocol in 1987, which bans the production of CFCs, halons, and other ozone-depleting chemicals. Over time, scientists have developed new refrigerants with lower global warming potential (GWP) to replace older ones. For example, in new automobiles, R-1234yf systems are now common, being chosen over refrigerants with much higher GWP such as R-134a and R-12.\nThe ban came into effect in 1989. Ozone levels stabilized by the mid-1990s and began to recover in the 2000s, as the shifting of the jet stream in the Southern Hemisphere towards the South Pole has stopped and might even be reversing. Recovery was projected to continue over the next century, with the ozone hole expected to reach pre-1980 levels by around 2075. In 2019, NASA reported that the ozone hole was the smallest ever since it was first discovered in 1982. The UN now projects that under the current regulations the ozone layer will completely regenerate by 2045. The Montreal Protocol is considered the most successful international environmental agreement to date.\nOzone cycle overview.\nThree forms (or allotropes) of oxygen are involved in the ozone-oxygen cycle: oxygen atoms (O or atomic oxygen), oxygen gas (O2 or diatomic oxygen), and ozone gas (O3 or triatomic oxygen). Ozone is formed in the stratosphere when oxygen gas molecules photodissociate after absorbing UVC photons. This converts a single O2 into two atomic oxygen radicals. The atomic oxygen radicals then combine with separate O2 molecules to create two O3 molecules. These ozone molecules absorb UVB light, following which ozone splits into a molecule of O2 and an oxygen atom. The oxygen atom then joins up with an oxygen molecule to regenerate ozone. This is a continuing process that terminates when an oxygen atom recombines with an ozone molecule to make two O2 molecules. It is worth noting that ozone is the only atmospheric gas that absorbs UVB light.\nO + O3 \u2192 2 O2\nThe total amount of ozone in the stratosphere is determined by a balance between photochemical production and recombination.\nOzone can be destroyed by a number of free radical catalysts; the most important are the hydroxyl radical (OH\u00b7), nitric oxide radical (NO\u00b7), chlorine radical (Cl\u00b7) and bromine radical (Br\u00b7). The dot is a notation to indicate that each species has an unpaired electron and is thus extremely reactive. The effectiveness of different halogens and pseudohalogens as catalysts for ozone destruction varies, in part due to differing routes to regenerate the original radical after reacting with ozone or dioxygen.\nWhile all of the relevant radicals have both natural and man-made sources, human activity has impacted some more than others. As of 2020, most of the OH\u00b7 and NO\u00b7 in the stratosphere is naturally occurring, but human activity has drastically increased the levels of chlorine and bromine. These elements are found in stable organic compounds, especially chlorofluorocarbons, which can travel to the stratosphere without being destroyed in the troposphere due to their low reactivity. Once in the stratosphere, the Cl and Br atoms are released from the parent compounds by the action of ultraviolet light, e.g.\nCFCl3 + electromagnetic radiation \u2192 Cl\u00b7 + \u00b7CFCl2\nOzone is a highly reactive molecule that easily reduces to the more stable oxygen form with the assistance of a catalyst. Cl and Br atoms destroy ozone molecules through a variety of catalytic cycles. In the simplest example of such a cycle, a chlorine atom reacts with an ozone molecule (O3), taking an oxygen atom to form chlorine monoxide (ClO) and leaving an oxygen molecule (O2). The ClO can react with a second molecule of ozone, releasing the chlorine atom and yielding two molecules of oxygen. The chemical shorthand for these gas-phase reactions is:\nThe overall effect is a decrease in the amount of ozone, though the rate of these processes can be decreased by the effects of null cycles. More complicated mechanisms have also been discovered that lead to ozone destruction in the lower stratosphere.\nA single chlorine atom would continuously destroy ozone (thus a catalyst) for up to two years (the time scale for transport back down to the troposphere) except for reactions that remove it from this cycle by forming reservoir species such as hydrogen chloride (HCl) and chlorine nitrate (ClONO2). Bromine is even more efficient than chlorine at destroying ozone on a per-atom basis, but there is much less bromine in the atmosphere at present. Both chlorine and bromine contribute significantly to overall ozone depletion. Laboratory studies have also shown that fluorine and iodine atoms participate in analogous catalytic cycles. However, fluorine atoms react rapidly with water vapour, methane and hydrogen to form strongly bound hydrogen fluoride (HF) in the Earth's stratosphere, while organic molecules containing iodine react so rapidly in the lower atmosphere that they do not reach the stratosphere in significant quantities.\nA single chlorine atom is able to react with an average of 100,000 ozone molecules before it is removed from the catalytic cycle. This fact plus the amount of chlorine released into the atmosphere yearly by chlorofluorocarbons (CFCs) and hydrochlorofluorocarbons (HCFCs) demonstrates the danger of CFCs and HCFCs to the environment.\nObservations on ozone layer depletion.\nThe ozone hole is usually measured by reduction in the total \"column ozone\" above a point on the Earth's surface. This is normally expressed in Dobson units; abbreviated as \"DU\". The most prominent decrease in ozone has been in the lower stratosphere. Marked decreases in column ozone in the Antarctic spring and early summer compared to the early 1970s and before have been observed using instruments such as the Total Ozone Mapping Spectrometer (TOMS).\nReductions of up to 70 percent in the ozone column observed in the austral (Southern Hemispheric) spring over Antarctica and first reported in 1985 (Farman et al.) are continuing. As of 2010[ [update]], Antarctic total column ozone in September and October continued to be 40\u201350 percent lower than pre-ozone-hole values since the 1990s. A gradual trend toward \"healing\" was reported in 2016. In 2017, NASA announced that the ozone hole was the weakest since 1988 because of warm stratospheric conditions. It is expected to recover around 2070.\nThe amount lost is more variable year-to-year in the Arctic than in the Antarctic. The greatest Arctic declines are in the winter and spring, reaching up to 30 percent when the stratosphere is coldest.\nReactions that take place on polar stratospheric clouds (PSCs) play an important role in enhancing ozone depletion. PSCs form more readily in the extreme cold of the Arctic and Antarctic stratosphere. This is why ozone holes first formed, and are deeper, over Antarctica. Early models failed to take PSCs into account and predicted a gradual global depletion, which is why the sudden Antarctic ozone hole was such a surprise to many scientists.\nIt is more accurate to speak of ozone depletion in middle latitudes rather than holes. Total column ozone declined below pre-1980 values between 1980 and 1996 for mid-latitudes. In the northern mid-latitudes, it then increased from the minimum value by about two percent from 1996 to 2009 as regulations took effect and the amount of chlorine in the stratosphere decreased. In the Southern Hemisphere's mid-latitudes, total ozone remained constant over that time period. There are no significant trends in the tropics, largely because halogen-containing compounds have not had time to break down and release chlorine and bromine atoms at tropical latitudes.\nLarge volcanic eruptions have been shown to have substantial albeit uneven ozone-depleting effects, as observed with the 1991 eruption of Mt. Pinatubo in the Philippines.\nOzone depletion also explains much of the observed reduction in stratospheric and upper tropospheric temperatures. The source of the warmth of the stratosphere is the absorption of UV radiation by ozone, hence reduced ozone leads to cooling. Some stratospheric cooling is also predicted from increases in greenhouse gases such as CO2 and CFCs themselves; however, the ozone-induced cooling appears to be dominant.\nPredictions of ozone levels remain difficult, but the precision of models' predictions of observed values and the agreement among different modeling techniques have increased steadily. The World Meteorological Organization Global Ozone Research and Monitoring Project\u2014Report No. 44 is strongly in favor of the Montreal Protocol, but notes that a UNEP 1994 Assessment overestimated ozone loss for the 1994\u20131997 period.\nCompounds in the atmosphere.\nCFCs and related compounds.\nChlorofluorocarbons (CFCs) and other halogenated ozone-depleting substances (ODS) are mainly responsible for man-made chemical ozone depletion. The total amount of effective halogens (chlorine and bromine) in the stratosphere can be calculated and are known as the equivalent effective stratospheric chlorine (EESC).\nCFCs as refrigerants were invented by Thomas Midgley Jr. in the 1930s. They were used in air conditioning and cooling units, as aerosol spray propellants prior to the 1970s, and in the cleaning processes of delicate electronic equipment. They also occur as by-products of some chemical processes. No significant natural sources have ever been identified for these compounds\u2014their presence in the atmosphere is due almost entirely to human manufacture. As mentioned above, when such ozone-depleting chemicals reach the stratosphere, they are dissociated by ultraviolet light to release chlorine atoms. The chlorine atoms act as a catalyst, and each can break down tens of thousands of ozone molecules before being removed from the stratosphere. Given the longevity of CFC molecules, recovery times are measured in decades. It is calculated that a CFC molecule takes an average of about five to seven years to go from the ground level up to the upper atmosphere, and it can stay there for about a century, destroying up to one hundred thousand ozone molecules during that time.\n1,1,1-Trichloro-2,2,2-trifluoroethane, also known as CFC-113a, is one of four man-made chemicals newly discovered in the atmosphere by a team at the University of East Anglia. CFC-113a is the only known CFC whose abundance in the atmosphere is still growing. Its source remains a mystery, but illegal manufacturing is suspected by some. CFC-113a seems to have been accumulating unabated since 1960. Between 2012 and 2017, concentrations of the gas jumped by 40 percent.\nA study by an international team of researchers published in \"Nature\" found that since 2013 emissions that are predominately from north-eastern China have released large quantities of the banned chemical Chlorofluorocarbon-11 (CFC-11) into the atmosphere. Scientists estimate that without action, these CFC-11 emissions will delay the recovery of the planet's ozone hole by a decade.\nAluminum oxide.\nSatellites burning up upon re-entry into Earth's atmosphere produce aluminum oxide (Al2O3) nanoparticles that endure in the atmosphere for decades. Estimates for 2022 alone were ~17 metric tons (~30kg of nanoparticles per ~250kg satellite). Increasing populations of satellite constellations can eventually lead to significant ozone depletion.\nVery short-lived substances (VSLS).\n\"Very short-lived substances\" are a class of ozone-depleting chemicals, allowed by the Montreal Protocol, that degrade in under 6 months. 90% are naturally produced, for example bromine-based chemicals generated by seaweed and phytoplankton, but 10% are manmade, for example dichloromethane.\nComputer modeling.\nScientists have attributed ozone depletion to the increase of man-made (anthropogenic) halogen compounds from CFCs by combining observational data with computer models. These complex chemistry transport models (e.g. SLIMCAT, CLaMS\u2014Chemical Lagrangian Model of the Stratosphere) work by combining measurements of chemicals and meteorological fields with chemical reaction rate constants. They identify key chemical reactions and transport processes that bring CFC photolysis products into contact with ozone.\nOzone hole and its causes.\nThe Antarctic ozone hole is an area of the Antarctic stratosphere in which the recent ozone levels have dropped to as low as 33 percent of their pre-1975 values. The ozone hole occurs during the Antarctic spring, from September to early December, as strong westerly winds start to circulate around the continent and create an atmospheric container. Within this polar vortex, over 50 percent of the lower stratospheric ozone is destroyed during the Antarctic spring.\nAs explained above, the primary cause of ozone depletion is the presence of chlorine-containing source gases (primarily CFCs and related halocarbons). In the presence of UV light, these gases dissociate, releasing chlorine atoms, which then go on to catalyze ozone destruction. The Cl-catalyzed ozone depletion can take place in the gas phase, but it is substantially enhanced in the presence of polar stratospheric clouds (PSCs).\nThese polar stratospheric clouds form during winter, in the extreme cold. Polar winters are dark, consisting of three months without solar radiation (sunlight). The lack of sunlight contributes to a decrease in temperature and the polar vortex traps and chills the air. Temperatures are around or below \u221280\u00a0\u00b0C. These low temperatures form cloud particles. There are three types of PSC clouds\u2014nitric acid trihydrate clouds, slowly cooling water-ice clouds, and rapid cooling water-ice (nacreous) clouds\u2014that provide surfaces for chemical reactions whose products will, in the spring lead to ozone destruction.\nThe photochemical processes involved are complex but well understood. The key observation is that, ordinarily, most of the chlorine in the stratosphere resides in \"reservoir\" compounds, primarily chlorine nitrate (ClONO2) as well as stable end products such as HCl. The formation of end products essentially removes Cl from the ozone depletion process. Reservoir compounds sequester Cl, which can later be made available via absorption of light at wavelengths shorter than 400\u00a0nm. During the Antarctic winter and spring, reactions on the surface of the polar stratospheric cloud particles convert these \"reservoir\" compounds into reactive free radicals (Cl and ClO). Denitrification is the process by which the clouds remove NO2 from the stratosphere by converting it to nitric acid in PSC particles, which then are lost by sedimentation. This prevents newly formed ClO from being converted back into ClONO2.\nThe role of sunlight in ozone depletion is the reason why the Antarctic ozone depletion is greatest during spring. During winter, even though PSCs are at their most abundant, there is no light over the pole to drive chemical reactions. During the spring, however, sunlight returns and provides energy to drive photochemical reactions and melt the polar stratospheric clouds, releasing considerable ClO, which drives the hole mechanism. Further warming temperatures near the end of spring break up the vortex around mid-December. As warm, ozone and NO2-rich air flows in from lower latitudes, the PSCs are destroyed, the enhanced ozone depletion process shuts down, and the ozone hole closes.\nMost of the ozone that is destroyed is in the lower stratosphere, in contrast to the much smaller ozone depletion through homogeneous gas-phase reactions, which occurs primarily in the upper stratosphere.\nEffects.\nSince the ozone layer absorbs UVB ultraviolet light from the sun, ozone layer depletion increases surface UVB levels (all else equal), which could lead to damage, including an increase in skin cancer. This was the reason for the Montreal Protocol. Although decreases in stratospheric ozone are well-tied to CFCs and increases in surface UVB, there is no direct observational evidence linking ozone depletion to higher incidence of skin cancer and eye damage in human beings. This is partly because UVA, which has also been implicated in some forms of skin cancer, is not absorbed by ozone, and because it is nearly impossible to control statistics for lifestyle changes over time. Ozone depletion may also influence wind patterns.\nIncreased UV.\nOzone, while a minority constituent in Earth's atmosphere, is responsible for most of the absorption of UVB radiation. The amount of UVB radiation that penetrates through the ozone layer decreases exponentially with the slant-path thickness and density of the layer. When stratospheric ozone levels decrease, higher levels of UVB reach the Earth's surface. UV-driven phenolic formation in tree rings has dated the start of ozone depletion in northern latitudes to the late 1700s.\nIn October 2008, the Ecuadorian Space Agency published a report called HIPERION. The study used ground instruments in Ecuador and the last 28 years' data from 12 satellites of several countries, and found that the UV radiation reaching equatorial latitudes was far greater than expected, with the ultraviolet index climbing as high as 24 in Quito; the WHO considers 11 as an extreme index and a great risk to health. The report concluded that depleted ozone levels around the mid-latitudes of the planet are already endangering large populations in these areas. Later, the CONIDA, the Peruvian Space Agency, published its own study, which yielded almost the same findings as the Ecuadorian study.\nBiological effects.\nThe main public concern regarding the ozone hole has been the effects of increased surface UV radiation on human health. So far, ozone depletion in most locations has been typically a few percent and, as noted above, no direct evidence of health damage is available in most latitudes. If the high levels of depletion seen in the ozone hole were to be common across the globe, the effects could be substantially more dramatic. As the ozone hole over Antarctica has in some instances grown so large as to affect parts of Australia, New Zealand, Chile, Argentina, and South Africa, environmentalists have been concerned that the increase in surface UV could be significant. Excessive ultraviolet radiation (UVR) has reducing effects on the rates of photosynthesis and growth of benthic diatom communities (microalgae species that increase water quality and are pollution resistant) that are present in shallow freshwater. Ozone depletion not only affects human health but also has a profound impact on biodiversity. It damages plants and trees at the cellular level, affecting their growth, vitality, photosynthesis, water balance, and defense mechanisms against pests and diseases. This sets off a cascade of ecological impacts, harming soil microbes, insects, wildlife, and entire ecosystems.\nOzone depletion would magnify all of the effects of UV on human health, both positive (including production of vitamin D) and negative (including sunburn, skin cancer, and cataracts). In addition, increased surface UV leads to increased tropospheric ozone, which is a health risk to humans.\nBasal and squamous cell carcinomas.\nThe most common forms of skin cancer in humans, basal and squamous cell carcinomas, have been strongly linked to UV-B exposure. The mechanism by which UVB induces these cancers is well understood\u2014absorption of UV-B radiation causes the pyrimidine bases in the DNA molecule to form dimers, resulting in transcription errors when the DNA replicates. These cancers are relatively mild and rarely fatal, although the treatment of squamous cell carcinoma sometimes requires extensive reconstructive surgery. By combining epidemiological data with results of animal studies, scientists have estimated that every one percent decrease in long-term stratospheric ozone would increase the incidence of these cancers by 2%.\nMelanoma.\nAnother form of skin cancer, melanoma, is much less common but far more dangerous, being lethal in about 15\u201320 percent of the cases diagnosed. The relationship between melanoma and ultraviolet exposure is not yet fully understood, but it appears that both UV-B and UV-A are involved. Because of this uncertainty, it is difficult to estimate the effect of ozone depletion on melanoma incidence. One study showed that a 10 percent increase in UV-B radiation was associated with a 19 percent increase in melanomas for men and 16 percent for women. A study of people in Punta Arenas, at the southern tip of Chile, showed a 56 percent increase in melanoma and a 46 percent increase in non-melanoma skin cancer over a period of seven years, along with decreased ozone and increased UVB levels.\nCortical cataracts.\nEpidemiological studies suggest an association between ocular cortical cataracts and UV-B exposure, using crude approximations of exposure and various cataract assessment techniques. A detailed assessment of ocular exposure to UV-B was carried out in a study on Chesapeake Bay Watermen, where increases in average annual ocular exposure were associated with increasing risk of cortical opacity. In this highly exposed group of predominantly white males, the evidence linking cortical opacities to sunlight exposure was the strongest to date. Based on these results, ozone depletion is predicted to cause hundreds of thousands of additional cataracts by 2050.\nIncreased tropospheric ozone.\nIncreased surface UV leads to increased tropospheric ozone. Ground-level ozone is generally recognized to be a health risk, as ozone is toxic due to its strong oxidant properties. The risks are particularly high for young children, the elderly, and those with asthma or other respiratory difficulties. At this time, ozone at ground level is produced mainly by the action of UV radiation on combustion gases from vehicle exhausts.\nIncreased production of vitamin D.\nVitamin D is produced in the skin by ultraviolet light. Thus, higher UVB exposure raises human vitamin D in those deficient in it. Recent research (primarily since the Montreal Protocol) shows that many humans have less than optimal vitamin D levels. In particular, in the U.S. population, the lowest quarter of vitamin D (&lt;17.8\u00a0ng/ml) were found using information from the National Health and Nutrition Examination Survey to be associated with an increase in all-cause mortality in the general population. While blood level of vitamin D in excess of 100\u00a0ng/ml appear to raise blood calcium excessively and to be associated with higher mortality, the body has mechanisms that prevent sunlight from producing vitamin D in excess of the body's requirements.\nEffects on animals.\nA November 2011 report by scientists at the Institute of Zoology in London, England found that whales off the coast of California have shown a sharp rise in sun damage, and these scientists \"fear that the thinning ozone layer is to blame\". The study photographed and took skin biopsies from over 150 whales in the Gulf of California and found \"widespread evidence of epidermal damage commonly associated with acute and severe sunburn\", having cells that form when the DNA is damaged by UV radiation. The findings suggest \"rising UV levels as a result of ozone depletion are to blame for the observed skin damage, in the same way that human skin cancer rates have been on the increase in recent decades.\" Apart from whales many other animals such as dogs, cats, sheep and terrestrial ecosystems also suffer the negative effects of increased UV-B radiations.\nEffects on crops.\nAn increase of UV radiation would be expected to affect crops. A number of economically important species of plants, such as rice, depend on cyanobacteria residing on their roots for the retention of nitrogen. Cyanobacteria are sensitive to UV radiation and would be affected by its increase. \"Despite mechanisms to reduce or repair the effects of increased ultraviolet radiation, plants have a limited ability to adapt to increased levels of UVB, therefore plant growth can be directly affected by UVB radiation.\"\nEffects on plant life.\nOver the years, the Arctic ozone layer has depleted severely. As a consequence species that live above the snow cover or in areas where snow has melted abundantly, due to hot temperatures, are negatively impacted due to UV radiation that reaches the ground. Depletion of the ozone layer and allowing excess UVB radiation would initially be assumed to increase damage to plant DNA. Reports have found that when plants are exposed to UVB radiation similar to stratospheric ozone depletion, there was no significant change in plant height or leaf mass, but showed a response in shoot biomass and leaf area with a small decrease. However, UVB radiation has been shown to decrease quantum yield of photosystem II. UVB damage only occurs under extreme exposure, and most plants also have UVB absorbing flavonoids which allow them to acclimatize to the radiation present. Plants experience different levels of UV radiation throughout the day. It is known that they are able to shift the levels and types of UV sunscreens (i.e. flavonoids), that they contain, throughout the day. This allows them to increase their protection against UV radiation. Plants that have been affected by radiation throughout development are more affected by the inability to intercept light with a larger leaf area than having photosynthetic systems compromised. Damage from UVB radiation is more likely to be significant on species interactions than on plants themselves.\nAnother significant impact of ozone depletion on plant life is the stress experienced by plants when exposed to UV radiation. This can cause a decrease in plant growth and an increase in oxidative stress, due to the production of nitric oxide and hydrogen peroxide. In areas where substantial ozone depletion has occurred, increased UV-B radiation reduces terrestrial plant productivity (and likewise carbon sequestration) by about 6%.\nMoreover, if plants are exposed to high levels of UV radiation, it can elicit the production of harmful volatile organic compounds, like isoprenes. The emission of isoprenes into the air, by plants, can severely impact the environment by adding to air pollution and increasing the amount of carbon in the atmosphere, ultimately contributing to climate change.\nPublic policy.\nThe full extent of the damage that CFCs have caused to the ozone layer is not known and will not be known for decades; however, marked decreases in column ozone have already been observed. The Montreal and Vienna conventions were installed long before a scientific consensus was established or important uncertainties in the science field were being resolved. The ozone case was understood comparably well by lay persons as e.g. \"Ozone shield\" or \"ozone hole\" were useful \"easy-to-understand bridging metaphors\". Americans voluntarily switched away from aerosol sprays, resulting in a 50 percent sales loss even before legislation was enforced.\nAfter a 1976 report by the United States National Academy of Sciences concluded that credible scientific evidence supported the ozone depletion hypothesis a few countries, including the United States, Canada, Sweden, Denmark, and Norway, moved to eliminate the use of CFCs in aerosol spray cans. At the time this was widely regarded as a first step towards a more comprehensive regulation policy, but progress in this direction slowed in subsequent years, due to a combination of political factors (continued resistance from the halocarbon industry and a general change in attitude towards environmental regulation during the first two years of the Reagan administration) and scientific developments (subsequent National Academy assessments that indicated that the first estimates of the magnitude of ozone depletion had been overly large).\nA critical DuPont manufacturing patent for Freon was set to expire in 1979. The United States banned the use of CFCs in aerosol cans in 1978. The European Community rejected proposals to ban CFCs in aerosol sprays, and in the U.S., CFCs continued to be used as refrigerants and for cleaning circuit boards. Worldwide CFC production fell sharply after the U.S. aerosol ban, but by 1986 had returned nearly to its 1976 level. In 1993, DuPont Canada closed its CFC facility.\nThe U.S. government's attitude began to change again in 1983, when William Ruckelshaus replaced Anne M. Burford as Administrator of the United States Environmental Protection Agency (EPA). Under Ruckelshaus and his successor, Lee Thomas, the EPA pushed for an international approach to halocarbon regulations. In 1985 twenty nations, including most of the major CFC producers, signed the Vienna Convention for the Protection of the Ozone Layer, which established a framework for negotiating international regulations on ozone-depleting substances. That same year, the discovery of the Antarctic ozone hole was announced, causing a revival in public attention to the issue.\nIn 1987, representatives from 43 nations signed the Montreal Protocol. Meanwhile, the halocarbon industry shifted its position and started supporting a protocol to limit CFC production. However, this shift was uneven with DuPont acting more quickly than its European counterparts. DuPont may have feared court action related to increased skin cancer, especially as the EPA had published a study in 1986 claiming that an additional 40\u00a0million cases and 800,000 cancer deaths were to be expected in the U.S. in the next 88 years. The EU shifted its position as well after Germany gave up its defence of the CFC industry and started supporting moves towards regulation. Government and industry in France and the UK tried to defend their CFC producing industries even after the Montreal Protocol had been signed.\nAt Montreal, the participants agreed to freeze production of CFCs at 1986 levels and to reduce production by 50 percent by 1999. After a series of scientific expeditions to the Antarctic produced convincing evidence that the ozone hole was indeed caused by chlorine and bromine from manmade organohalogens, the Montreal Protocol was strengthened at a 1990 meeting in London. The participants agreed to phase out CFCs and halons entirely (aside from a very small amount marked for certain \"essential\" uses, such as asthma inhalers) by 2000 in non-Article 5 countries and by 2010 in Article 5 (less developed) signatories. At a 1992 meeting in Copenhagen, Denmark, the phase-out date was moved up to 1996. At the same meeting, methyl bromide (MeBr), a fumigant used primarily in agricultural production, was added to the list of controlled substances. For all substances controlled under the protocol, phaseout schedules were delayed for less developed ('Article 5(1)') countries, and phaseout in these countries was supported by transfers of expertise, technology, and money from non-Article 5(1) Parties to the Protocol. Additionally, exemptions from the agreed schedules could be applied for under the Essential Use Exemption (EUE) process for substances other than methyl bromide and under the Critical Use Exemption (CUE) process for methyl bromide.\nCivil society, including especially non-governmental organizations (NGOs), played critical roles at all stages of policy development leading to the Vienna Conference, the Montreal Protocol, and in assessing compliance afterwards. The major companies claimed that no alternatives to HFC existed. An ozone-safe hydrocarbon refrigerant was developed at a technological institute in Hamburg, Germany, consisting of a mixture of the hydrocarbon gases propane and butane, and in 1992 came to the attention of the NGO Greenpeace. Greenpeace called it \"Greenfreeze\". The NGO then worked successfully first with a small and struggling company to market an appliance beginning in Europe, then Asia and later Latin America, receiving a 1997 UNEP award. By 1995, Germany had made CFC refrigerators illegal. Since 2004, corporations like Coca-Cola, Carlsberg, and IKEA formed a coalition to promote the ozone-safe Greenfreeze units. Production spread to companies like Electrolux, Bosch, and LG, with sales reaching some 300\u00a0million refrigerators by 2008. In Latin America, a domestic Argentinian company began Greenfreeze production in 2003, while the giant Bosch in Brazil began a year later. By 2013 it was being used by some 700\u00a0million refrigerators, making up about 40 percent of the market.\nIn the U.S., however, change has been much slower. To some extent, CFCs were being replaced by the less damaging hydrochlorofluorocarbons (HCFCs), although concerns remain regarding HCFCs also. In some applications, hydrofluorocarbons (HFCs) were being used to replace CFCs. HFCs, which contain no chlorine or bromine, do not contribute to ozone depletion although they are potent greenhouse gases. The best known of these compounds is probably HFC-134a (R-134a), which in the United States has largely replaced CFC-12 (R-12) in automobile air conditioners. In laboratory analytics (a former \"essential\" use) the ozone depleting substances can be replaced with other solvents. Chemical companies like Du Pont, whose representatives disparaged Greenfreeze as \"that German technology,\" maneuvered the EPA to block the technology in the U.S. until 2011. Ben &amp; Jerry's of Unilever and General Electric, spurred by Greenpeace, had expressed formal interest in 2008 which figured in the EPA's final approval.\nThe EU recast its Ozone Regulation in 2009. The law bans ozone-depleting substances with the goal of protecting the ozone layer. The list of ODS that are subject to the regulation is the same as those under the Montreal Protocol, with some additions.\nMore recently, policy experts have advocated for efforts to link ozone protection efforts to climate protection efforts. Many ODS are also greenhouse gases, some thousands of times more powerful agents of radiative forcing than carbon dioxide over the short and medium term. Thus policies protecting the ozone layer have had benefits in mitigating climate change. The reduction of the radiative forcing due to ODS probably masked the true level of climate change effects of other greenhouse gases, and was responsible for the \"slow down\" of global warming from the mid-90s. Policy decisions in one arena affect the costs and effectiveness of environmental improvements in the other.\nODS requirements in the marine industry.\nThe IMO has amended MARPOL Annex VI Regulation 12 regarding ozone depleting substances. As from 1 July 2010, all vessels where MARPOL Annex VI is applicable should have a list of equipment using ozone depleting substances. The list should include the name of ODS, type and location of equipment, quantity in kilograms and date. All changes since that date should be recorded in an ODS Record book on board recording all intended or unintended releases to the atmosphere. Furthermore, new ODS supply or landing to shore facilities should be recorded as well.\nProspects of ozone depletion.\nSince the adoption and strengthening of the Montreal Protocol has led to reductions in the emissions of CFCs, atmospheric concentrations of the most-significant compounds have been declining. These substances are being gradually removed from the atmosphere; since peaking in 1994, the Effective Equivalent Chlorine (EECl) level in the atmosphere had dropped about 10 percent by 2008. The decrease in ozone-depleting chemicals has also been significantly affected by a decrease in bromine-containing chemicals. The data suggest that substantial natural sources exist for atmospheric methyl bromide (CH3Br). The phase-out of CFCs means that nitrous oxide (N2O), which is not covered by the Montreal Protocol, has become the most highly emitted ozone-depleting substance and is expected to remain so throughout the 21st century.\nAccording to the IPCC Sixth Assessment Report, global stratospheric ozone levels experienced rapid decline in the 1970s and 1980s and have since been increasing, but have not reached preindustrial levels. Although considerable variability is expected from year to year, including in polar regions where depletion is largest, the ozone layer is expected to continue recovering in coming decades due to declining ozone-depleting substance concentrations, assuming full compliance with the Montreal Protocol.\nThe Antarctic ozone hole is expected to continue for decades. Ozone concentrations in the lower stratosphere over Antarctica increased by 5\u201310 percent by 2020 and will return to pre-1980 levels by about 2060\u20132075. This is 10\u201325 years later than predicted in earlier assessments, because of revised estimates of atmospheric concentrations of ozone-depleting substances, including a larger predicted future usage in developing countries. Another factor that may prolong ozone depletion is the drawdown of nitrogen oxides from above the stratosphere due to changing wind patterns. A gradual trend toward \"healing\" was reported in 2016. In 2019, the ozone hole was at its smallest in the previous thirty years, due to the warmer polar stratosphere weakening the polar vortex. In September 2023, the Antarctic ozone hole was one of the largest on record, at 26 million square kilometers. The anomalously large ozone loss may have been a result of the 2022 Tonga volcanic eruption.\nAccording to a 2023 United Nations assessment, the ozone layer is on track to recover to 1980 levels by around 2066 over Antarctica, by 2045 over the Arctic, and by 2040 for the rest of the world, assuming current regulations remain in place.\nResearch history.\nThe basic physical and chemical processes that lead to the formation of an ozone layer in the Earth's stratosphere were discovered by Sydney Chapman in 1930. Short-wavelength UV radiation splits an oxygen (O2) molecule into two oxygen (O) atoms, which then combine with other oxygen molecules to form ozone. Ozone is removed when an oxygen atom and an ozone molecule \"recombine\" to form two oxygen molecules, i.e. O + O3 \u2192 2O2. In the 1950s, David Bates and Marcel Nicolet presented evidence that various free radicals, in particular hydroxyl (OH) and nitric oxide (NO), could catalyze this recombination reaction, reducing the overall amount of ozone. These free radicals were known to be present in the stratosphere, and so were regarded as part of the natural balance\u2014it was estimated that in their absence, the ozone layer would be about twice as thick as it currently is.\nIn 1970 Paul Crutzen pointed out that emissions of nitrous oxide (N2O), a stable, long-lived gas produced by soil bacteria, from the Earth's surface could affect the amount of nitric oxide (NO) in the stratosphere. Crutzen showed that nitrous oxide lives long enough to reach the stratosphere, where it is converted into NO. Crutzen then noted that increasing use of fertilizers might have led to an increase in nitrous oxide emissions over the natural background, which would in turn result in an increase in the amount of NO in the stratosphere. Thus human activity could affect the stratospheric ozone layer. In the following year, Crutzen and (independently) Harold Johnston suggested that NO emissions from supersonic passenger aircraft, which would fly in the lower stratosphere, could also deplete the ozone layer. However, more recent analysis in 1995 by David W. Fahey, an atmospheric scientist at the National Oceanic and Atmospheric Administration, found that the drop in ozone would be from 1\u20132 percent if a fleet of 500 supersonic passenger aircraft were operated. This, Fahey expressed, would not be a showstopper for advanced supersonic passenger aircraft development.\nRowland\u2013Molina hypothesis.\nIn 1974 Frank Sherwood Rowland, Chemistry Professor at the University of California at Irvine, and his postdoctoral associate Mario J. Molina suggested that long-lived organic halogen compounds, such as CFCs, might behave in a similar fashion as Crutzen had proposed for nitrous oxide. James Lovelock had recently discovered, during a cruise in the South Atlantic in 1971, that almost all of the CFC compounds manufactured since their invention in 1930 were still present in the atmosphere. Molina and Rowland concluded that, like N2O, the CFCs would reach the stratosphere where they would be dissociated by UV light, releasing chlorine atoms. A year earlier, Richard Stolarski and Ralph Cicerone at the University of Michigan had shown that Cl is even more efficient than NO at catalyzing the destruction of ozone. Similar conclusions were reached by Michael McElroy and Steven Wofsy at Harvard University. Neither group, however, had realized that CFCs were a potentially large source of stratospheric chlorine\u2014instead, they had been investigating the possible effects of HCl emissions from the Space Shuttle, which are very much smaller.\nThe Rowland\u2013Molina hypothesis was strongly disputed by representatives of the aerosol and halocarbon industries. The Chair of the Board of DuPont was quoted as saying that ozone depletion theory is \"a science fiction tale ... a load of rubbish ... utter nonsense\". Robert Abplanalp, the President of Precision Valve Corporation (and inventor of the first practical aerosol spray can valve), wrote to the Chancellor of UC Irvine to complain about Rowland's public statements. Nevertheless, within three years most of the basic assumptions made by Rowland and Molina were confirmed by laboratory measurements and by direct observation in the stratosphere. The concentrations of the source gases (CFCs and related compounds) and the chlorine reservoir species (HCl and ClONO2) were measured throughout the stratosphere, and demonstrated that CFCs were indeed the major source of stratospheric chlorine, and that nearly all of the CFCs emitted would eventually reach the stratosphere. Even more convincing was the measurement, by James G. Anderson and collaborators, of chlorine monoxide (ClO) in the stratosphere. ClO is produced by the reaction of Cl with ozone\u2014its observation thus demonstrated that Cl radicals not only were present in the stratosphere but also were actually involved in destroying ozone. McElroy and Wofsy extended the work of Rowland and Molina by showing that bromine atoms were even more effective catalysts for ozone loss than chlorine atoms and argued that the brominated organic compounds known as halons, widely used in fire extinguishers, were a potentially large source of stratospheric bromine. In 1976 the United States National Academy of Sciences released a report concluding that the ozone depletion hypothesis was strongly supported by the scientific evidence. In response the United States, Canada and Norway banned the use of CFCs in aerosol spray cans in 1978. Early estimates were that, if CFC production continued at 1977 levels, the total atmospheric ozone would after a century or so reach a steady state, 15 to 18 percent below normal levels. By 1984, when better evidence on the speed of critical reactions was available, this estimate was changed to 5 to 9 percent steady-state depletion.\nCrutzen, Molina, and Rowland were awarded the 1995 Nobel Prize in Chemistry for their work on stratospheric ozone.\nAntarctic ozone hole.\nThe discovery of the Antarctic \"ozone hole\" by British Antarctic Survey scientists Farman, Gardiner and Shanklin (first reported in a paper in \"Nature\" in May 1985) came as a shock to the scientific community, because the observed decline in polar ozone was far larger than had been anticipated. Satellite measurements (TOMS onboard Nimbus 7) showing massive depletion of ozone around the south pole were becoming available at the same time. However, these were initially rejected as unreasonable by data quality control algorithms (they were filtered out as errors since the values were unexpectedly low); the ozone hole was detected only in satellite data when the raw data was reprocessed following evidence of ozone depletion in \"in situ\" observations. When the software was rerun without the flags, the ozone hole was seen as far back as 1976.\nSusan Solomon, an atmospheric chemist at the National Oceanic and Atmospheric Administration (NOAA), proposed that chemical reactions on polar stratospheric clouds (PSCs) in the cold Antarctic stratosphere caused a massive, though localized and seasonal, increase in the amount of chlorine present in active, ozone-destroying forms. The polar stratospheric clouds in Antarctica are only formed at very low temperatures, as low as \u221280\u00a0\u00b0C, and early spring conditions. In such conditions the ice crystals of the cloud provide a suitable surface for conversion of unreactive chlorine compounds into reactive chlorine compounds, which can easily deplete ozone.\nMoreover, the polar vortex formed over Antarctica is very tight and the reaction occurring on the surface of the cloud crystals is far different from when it occurs in atmosphere. These conditions have led to ozone hole formation in Antarctica. This hypothesis was decisively confirmed, first by laboratory measurements and subsequently by direct measurements, from the ground and from high-altitude airplanes, of very high concentrations of chlorine monoxide (ClO) in the Antarctic stratosphere.\nAlternative hypotheses, which had attributed the ozone hole to variations in solar UV radiation or to changes in atmospheric circulation patterns, were also tested and shown to be untenable.\nMeanwhile, analysis of ozone measurements from the worldwide network of ground-based Dobson spectrophotometers led an international panel to conclude that the ozone layer was in fact being depleted, at all latitudes outside of the tropics. These trends were confirmed by satellite measurements. As a consequence, the major halocarbon-producing nations agreed to phase out production of CFCs, halons, and related compounds, a process that was completed in 1996.\nSince 1981 the United Nations Environment Programme, under the auspices of the World Meteorological Organization, has sponsored a series of technical reports on the Scientific Assessment of Ozone Depletion, based on satellite measurements. The 2007 report showed that the hole in the ozone layer was recovering and the smallest it had been for about a decade.\nA 2010 report found, \"Over the past decade, global ozone and ozone in the Arctic and Antarctic regions is no longer decreasing but is not yet increasing. The ozone layer outside the Polar regions is projected to recover to its pre-1980 levels some time before the middle of this century. In contrast, the springtime ozone hole over the Antarctic is expected to recover much later.\"\nIn 2012, NOAA and NASA reported \"Warmer air temperatures high above the Antarctic led to the second smallest season ozone hole in 20 years averaging 17.9 million square kilometres. The hole reached its maximum size for the season on Sept 22, stretching to 21.2 million square kilometres.\" A gradual trend toward \"healing\" was reported in 2016 and then in 2017. It is reported that the recovery signal is evident even in the ozone loss saturation altitudes.\nThe hole in the Earth's ozone layer over the South Pole has affected atmospheric circulation in the Southern Hemisphere all the way to the equator. The ozone hole has influenced atmospheric circulation all the way to the tropics and increased rainfall at low, subtropical latitudes in the Southern Hemisphere.\nArctic ozone \"mini-hole\".\nOn March 3, 2005, the journal \"Nature\" published an article linking 2004's unusually large Arctic ozone hole to solar wind activity.\nOn March 15, 2011, a record ozone layer loss was observed, with about half of the ozone present over the Arctic having been destroyed. The change was attributed to increasingly cold winters in the Arctic stratosphere at an altitude of approximately , a change associated with global warming in a relationship that is still under investigation. By March 25, the ozone loss had become the largest compared to that observed in all previous winters with the possibility that it would become an ozone hole. This would require that the quantities of ozone to fall below 200 Dobson units, from the 250 recorded over central Siberia. It is predicted that the thinning layer would affect parts of Scandinavia and Eastern Europe on March 30\u201331.\nOn October 2, 2011, a study was published in the journal \"Nature\", which said that between December 2010 and March 2011 up to 80 percent of the ozone in the atmosphere at about above the surface was destroyed. The level of ozone depletion was severe enough that scientists said it could be compared to the ozone hole that forms over Antarctica every winter. According to the study, \"for the first time, sufficient loss occurred to reasonably be described as an Arctic ozone hole.\" The study analyzed data from the Aura and CALIPSO satellites, and determined that the larger-than-normal ozone loss was due to an unusually long period of cold weather in the Arctic, some 30 days more than typical, which allowed for more ozone-destroying chlorine compounds to be created. According to Lamont Poole, a co-author of the study, cloud and aerosol particles on which the chlorine compounds are found \"were abundant in the Arctic until mid March 2011\u2014much later than usual\u2014with average amounts at some altitudes similar to those observed in the Antarctic, and dramatically larger than the near-zero values seen in March in most Arctic winters\".\nIn 2013, researchers analyzed the data and found the 2010\u20132011 Arctic event did not reach the ozone depletion levels to classify as a true hole. A hole in the ozone is generally classified as 220 Dobson units or lower; the Arctic hole did not approach that low level. It has since been classified as a \"mini-hole.\"\nFollowing the ozone depletion in 1997 and 2011, a 90% drop in ozone was measured by weather balloons over the Arctic in March 2020, as they normally recorded 3.5 parts per million of ozone, compared to only around 0.3 parts per million lastly, due to the coldest temperatures ever recorded since 1979, and a strong polar vortex which allowed chemicals, including chlorine and bromine, to reduce ozone.\nA rare hole, the result of unusually low temperatures in the atmosphere above the North Pole, was studied in 2020.\nTibet ozone hole.\nAs winters that are colder are more affected, at times there is an ozone hole over Tibet. In 2006, a 2.5\u00a0million square kilometer ozone hole was detected over Tibet. Again in 2011, an ozone hole appeared over mountainous regions of Tibet, Xinjiang, Qinghai and the Hindu Kush, along with an unprecedented hole over the Arctic, though the Tibet one was far less intense than the ones over the Arctic or Antarctic.\nPotential depletion by storm clouds.\nResearch in 2012 showed that the same process that produces the ozone hole over Antarctica, occurs over summer storm clouds in the United States, and thus may be destroying ozone there as well.\nOzone hole over tropics.\nPhysicist Qing-Bin Lu, of the University of Waterloo, claimed to have discovered a large, all-season ozone hole in the lower stratosphere over the tropics in July 2022. However, other researchers in the field refuted this claim, stating that the research was riddled with \"serious errors and unsubstantiated assertions.\" According to Dr Paul Young, a lead author of the 2022 WMO/UNEP Scientific Assessment of Ozone Depletion, \"The author's identification of a 'tropical ozone hole' is down to him looking at percentage changes in ozone, rather than absolute changes, with the latter being much more relevant for damaging UV reaching the surface.\" Specifically, Lu's work defines \"ozone hole\" as \"an area with O3 loss in percent larger than 25%, with respect to the undisturbed O3 value when there were no significant CFCs in the stratosphere (~ in the 1960s)\" instead of the general definition of 220 Dobson units or lower. Dr Marta Abalos Alvarez has added \"Ozone depletion in the tropics is nothing new and is mainly due to the acceleration of the Brewer-Dobson circulation.\"\nDepletion caused by wildfire smoke.\nAnalyzing the atmospheric impacts of the 2019\u20132020 Australian bushfire season, scientists led by MIT researcher Susan Solomon found the smoke destroyed 3\u20135% of ozone in affected areas of the Southern Hemisphere. Smoke particles absorb hydrogen chloride and act as a catalyst to create chlorine radicals that destroy ozone.\nOzone depletion and global warming.\nAmong others, Robert Watson had a role in the science assessment and in the regulation efforts of ozone depletion and global warming. Prior to the 1980s, the EU, NASA, NAS, UNEP, WMO and the British government had dissenting scientific reports and Watson played a role in the process of unified assessments. Based on the experience with the ozone case, the IPCC started to work on a unified reporting and science assessment to reach a consensus to provide the IPCC Summary for Policymakers.\nThere are various areas of linkage between ozone depletion and global warming science:\nMisconceptions.\nCFC weight.\nSince CFC molecules are heavier than air (nitrogen or oxygen), it is commonly believed that the CFC molecules cannot reach the stratosphere in significant amounts. However, atmospheric gases are not sorted by weight at these altitudes; the forces of wind can fully mix the gases in the atmosphere. Some of the heavier CFCs are not evenly distributed.\nPercentage of human-made chlorine.\nAnother misconception is that natural sources of chlorine are several times larger than human-made ones. While this statement is true for tropospheric chlorine, that is irrelevant to ozone depletion, which is only affected by stratospheric chlorine. Chlorine from ocean spray is soluble and thus is washed by rainfall before it reaches the stratosphere. CFCs, in contrast, are insoluble and long-lived, allowing them to reach the stratosphere. In the lower atmosphere, there is much more chlorine from CFCs and related haloalkanes than there is in HCl from salt spray, and in the stratosphere halocarbons are dominant. Only methyl chloride, which is one of these halocarbons, has a mainly natural source, and it is responsible for about 20 percent of the chlorine in the stratosphere; the remaining 80 percent comes from human-made sources.\nVery violent volcanic eruptions can inject HCl into the stratosphere, but researchers have shown that the contribution is not significant compared to that from CFCs. A similar erroneous assertion is that soluble halogen compounds from the volcanic plume of Mount Erebus on Ross Island, Antarctica are a major contributor to the Antarctic ozone hole.\nNevertheless, a 2015 study showed that the role of Mount Erebus volcano in the Antarctic ozone depletion was probably underestimated. Based on the NCEP/NCAR reanalysis data over the last 35 years and by using the NOAA HYSPLIT trajectory model, researchers showed that gas emissions from the volcano (including hydrogen chloride (HCl)) can reach the Antarctic stratosphere via high-latitude cyclones and then the polar vortex. Depending on the level of its volcanic activity, the additional annual HCl mass entering the stratosphere from Erebus varies from 1.0 to 14.3 kt.\nFirst observation.\nG.M.B. Dobson mentioned that when springtime ozone levels in the Antarctic over Halley Bay were first measured in 1956, he was surprised to find that they were only about 320 DU, about 150 DU below typical spring Arctic levels of around 450 DU. What Dobson observed was not an ozone hole but in fact a typical annual maximum Antarctic ozone concentration: actual ozone hole values are in the 150\u2013100 DU range. While Arctic ozone concentrations vary on a smooth annual cycle from around 300 to 450 DU, peaking in the northern hemisphere spring, Antarctic concentrations drop sharply in the southern hemisphere spring from highs of around 300 DU to much lower values. Peak values are not reached again until December.\nLocation of hole.\nSome people thought that the ozone hole should be above the sources of CFCs. However, CFCs are well mixed globally in the troposphere and stratosphere. The reason for occurrence of the ozone hole above Antarctica is not because there are more CFCs concentrated but because the low temperatures help form polar stratospheric clouds. In fact, there are findings of significant and localized \"ozone holes\" above other parts of the Earth, such as above Central Asia.\nAwareness campaigns.\nPublic misconceptions and misunderstandings of complex issues like ozone depletion are common. The limited scientific knowledge of the public led to confusion about global warming or the perception of global warming as a subset of the \"ozone hole\". In the beginning, classical green NGOs refrained from using CFC depletion for campaigning, as they assumed the topic was too complicated. They became active much later, e.g. in Greenpeace's support for a CFC-free refrigerator produced by the former East German company VEB dkk Scharfenstein.\nThe metaphors used in the CFC discussion (ozone shield, ozone hole) are not \"exact\" in the scientific sense. The \"ozone hole\" is more of a \"depression\", less \"a hole in the windshield\". The ozone does not disappear through the layer, nor is there a uniform \"thinning\" of the ozone layer. However, they resonated better with non-scientists and their concerns. The ozone hole was seen as a \"hot issue\" and imminent risk as laypeople feared severe personal consequences such as skin cancer, cataracts, damage to plants, and reduction of plankton populations in the ocean's photic zone. Not only on the policy level, ozone regulation compared to climate change fared much better in public opinion. Americans voluntarily switched away from aerosol sprays before legislation was enforced, while climate change failed to achieve comparable concern and public action. The sudden identification in 1985 that there was a substantial \"hole\" was widely reported in the press. The especially rapid ozone depletion in Antarctica had previously been dismissed as a measurement error. Scientific consensus was established after regulation.\nWhile the Antarctic ozone hole has a relatively small effect on global ozone, the hole has generated a great deal of public interest because:\nWorld Ozone Day.\nIn 1994, the United Nations General Assembly voted to designate 16 September as the International Day for the Preservation of the Ozone Layer, or \"World Ozone Day\". The designation commemorates the signing of the Montreal Protocol on that date in 1987.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44187", "revid": "38415975", "url": "https://en.wikipedia.org/wiki?curid=44187", "title": "Sound effect", "text": "Artificially created or enhanced sound\nA sound effect (or audio effect) is an artificially created or enhanced sound, or sound process used to emphasize artistic or other content of films, television shows, live performance, animation, video games, music, or other media.\nIn motion picture and television production, a sound effect is a sound recorded and presented to make a specific storytelling or creative point \"without\" the use of dialogue or music. Traditionally, in the twentieth century, they were created with Foley. The term often refers to a process applied to a recording, without necessarily referring to the recording itself. In professional motion picture and television production, dialogue, music, and sound effects recordings are treated as separate elements. Dialogue and music recordings are never referred to as sound effects, even though the processes applied to such as reverberation or flanging effects, often are called \"sound effects\".\nThis area and sound design have been slowly merged since the late-twentieth century.\nHistory.\nThe term \"sound effect\" dates back to the early days of radio. In its \"Year Book 1931\" the BBC published a major article about \"The Use of Sound Effects\". It considers sound effects deeply linked with broadcasting and states: \"It would be a great mistake to think of them as analogous to punctuation marks and accents in print. They should never be \"inserted\" into a program already existing. The author of a broadcast play or broadcast construction ought to have used Sound Effects as bricks with which to build, treating them as of equal value with speech and music.\" It lists six \"totally different primary genres of Sound Effect\":\n* \"Realistic, confirmatory effect\"\n* \"Realistic, evocative effect\"\n* \"Symbolic, evocative effect\"\n* \"Conventionalised effect\"\n* \"Impressionistic effect\"\n* \"Music as an effect\"\nAccording to the author, \"It is axiomatic that every Sound Effect, to whatever category it belongs, \"must\" register in the listener's mind instantaneously. If it fails to do so its presence could not be justified.\"\nFilm.\nIn the context of motion pictures and television, \"sound effects\" refers to an entire hierarchy of sound elements, whose production encompasses many different disciplines, including:\nEach of these sound effect categories is specialized, with sound editors known as specialists in an area of sound effects (e.g. a \"Car cutter\" or \"Guns cutter\").\nFoley is another method of adding sound effects. Foley is more of a technique for creating sound effects than a type of sound effect, but it is often used for creating the incidental real-world sounds that are very specific to what is going on onscreen, such as footsteps. With this technique, the action onscreen is essentially recreated to try to match it as closely as possible. If done correctly it is very hard for audiences to tell what sounds were added and what sounds were originally recorded (location sound).\nIn the early days of film and radio, Foley artists would add sounds in real time or pre-recorded sound effects would be played back from analog discs in real time (while watching the picture). Today, with effects held in digital format, it is easy to create any required sequence to be played in any desired timeline.\nIn the days of silent film, sound effects were added by the operator of a theater organ or photoplayer, both of which also supplied the soundtrack of the film. Theater organ sound effects are usually electric or electro-pneumatic, and activated by a button pressed with the hand or foot.\nPhotoplayer operators activate sound effects either by flipping switches on the machine or pulling \"cow-tail\" pull-strings, which hang above. Sounds like bells and drums are made mechanically, sirens and horns electronically. Due to its smaller size, a photoplayer usually has fewer special effects than a theater organ or less complex ones.\nVideo games.\nThe principles involved with modern video game sound effects (since the introduction of sample playback) are essentially the same as those of motion pictures. Typically a game project requires two jobs to be completed: sounds must be recorded or selected from a library and a sound engine must be programmed so that those sounds can be incorporated into the game's interactive environment.\nIn earlier computers and video game systems, sound effects were typically produced using sound synthesis. In modern systems, the increases in storage capacity and playback quality has allowed sampled sound to be used. The modern systems also frequently utilize positional audio, often with hardware acceleration, and real-time audio post-processing, which can also be tied to the 3D graphics development. Based on the internal state of the game, multiple different calculations can be made. This will allow for, for example, realistic sound dampening, echoes and Doppler effect.\nHistorically the simplicity of game environments reduced the required number of sounds needed, and thus only one or two people were directly responsible for the sound recording and design. As the video game business has grown and computer sound reproduction quality has increased, however, the team of sound designers dedicated to game projects has likewise grown and the demands placed on them may now approach those of mid-budget motion pictures.\nSome pieces of music use sound effects that are made by a musical instrument or by other means. An early example is the 18th century Toy Symphony. Richard Wagner in the opera \"Das Rheingold\" (1869) lets a choir of anvils introduce the scene of the dwarfs who have to work in the mines, similar to the introduction of the dwarfs in the 1937 Disney movie \"Snow White\". Klaus Doldingers soundtrack for the 1981 movie \"Das Boot\" includes a title score with a sonar sound to reflect the U-boat setting. John Barry integrated into the title song of \"Moonraker\" (1979) a sound representing the beep of a Sputnik like satellite.\nSound effects in theater.\nGao, Jianliang, Zhao, Yuezhe, and Pan, Lili explained how sound absorption in the stage area influences the acoustics within an opera house auditorium. Their research, using computer models and scale experiments, revealed that sound absorption significantly affects sound clarity and the time it takes for sound to fade, but not the volume. More absorption led to clearer sounds but quicker fades, showing the intricate dance between stage and auditorium acoustics.\nIn his book \"Sound: A Reader in Theatre Practice,\" Brown effectively connects the dots between theory and practice in the world of theater sound. He presents an engaging look into how sound design in theater has evolved, blending historical insights with current philosophical thoughts. Brown argues that the immersive nature of theater sound goes beyond traditional analysis, providing fresh perspectives on how sound interacts with societal contexts.\nBrown offers a fresh look at Ovadija's exploration of sound in theater, questioning the traditional focus on visuals over audio. He points out the significant, yet often overlooked, role of sound in shaping theater's impact and experience. Brown pushes for a broader appreciation of sound's essence in theater, beyond just supporting visuals, to acknowledge its deep influence on storytelling and audience immersion.\nRost explores the criteria for 'good sound' in theater through handbooks and prioritization as guiding principles. These criteria not only dictate the creation and selection of sounds to complement the narrative and mood but also aim to maintain audience focus. Rost's analysis reveals underlying hierarchies in sound selection and emphasizes the need for further research into the historical and practical aspects of theater sound.\nRecording.\nThe most realistic sound effects may originate from original sources; the closest sound to machine-gun fire could be an original recording of actual machine guns.\nDespite this, real life and actual practice do not always coincide with theory. When recordings of real life do not sound realistic on playback, Foley and effects are used to create more convincing sounds. For example, the realistic sound of bacon frying can be the crumpling of cellophane, while rain may be recorded as salt falling on a piece of tin foil.\nLess realistic sound effects are digitally synthesized or sampled and sequenced (the same recording played repeatedly using a sequencer). When the producer or content creator demands high-fidelity sound effects, the sound editor usually must augment his available library with new sound effects recorded in the field.\nWhen the required sound effect is of a small subject, such as scissors cutting, cloth ripping, or footsteps, the sound effect is best recorded in a studio, under controlled conditions in a process known as Foley. Many sound effects cannot be recorded in a studio, such as explosions, gunfire, and automobile or aircraft maneuvers. These effects must be recorded by a professional audio engineer.\nWhen such \"big\" sounds are required, the recordist will begin contacting professionals or technicians in the same way a producer may arrange a crew; if the recordist needs an explosion, they may contact a demolition company to see if any buildings are scheduled to be destroyed with explosives in the near future. If the recordist requires a volley of cannon fire, they may contact historical re-enactors or gun enthusiasts.\nDepending on the effect, recordists may use several DAT, hard disk, or Nagra recorders and a large number of microphones. During a cannon- and musket-fire recording session for the 2003 film \"The Alamo\", conducted by Jon Johnson and Charles Maynes, two to three DAT machines were used. One machine was stationed near the cannon itself, so it could record the actual firing. Another was stationed several hundred yards away, below the trajectory of the ball, to record the sound of the cannonball passing by. When the crew recorded musket fire, a set of microphones were arrayed close to the target (in this case a swine carcass) to record the musket-ball impacts.\nA counter-example is the common technique for recording an automobile. For recording \"onboard\" car sounds (which include the car interiors), a three-microphone technique is common. Two microphones record the engine directly: one is taped to the underside of the hood, near the engine block. The second microphone is covered in a wind screen and tightly attached to the rear bumper, within an inch or so of the tailpipe. The third microphone, which is often a stereo microphone, is stationed inside the car to get the car interior.\nHaving all of these tracks at once gives a sound designer or audio engineer a great deal of control over how they want the car to sound. In order to make the car more ominous or low, they can mix in more of the tailpipe recording; if they want the car to sound like it is running full throttle, they can mix in more of the engine recording and reduce the interior perspective. In cartoons, a pencil being dragged down a washboard may be used to simulate the sound of a sputtering engine.\nWhat is considered today to be the first recorded sound effect was of Big Ben striking 10:30, 10:45, and 11:00. It was recorded on a brown wax cylinder by technicians at Edison House in London on July 16, 1890. This recording is currently in the public domain.\nProcessing effects.\nAs the car example demonstrates, the ability to make multiple simultaneous recordings of the same subject\u2014through the use of several DAT or multitrack recorders\u2014has made sound recording into a sophisticated craft. The sound effect can be shaped by the sound editor or sound designer, not just for realism, but for emotional effect.\nOnce the sound effects are recorded or captured, they are usually loaded into a computer integrated with an audio non-linear editing system. This allows a sound editor or sound designer to heavily manipulate a sound to meet their needs.\nThe most common sound design tool is the use of layering to create a new, interesting sound out of two or three old, average sounds. For example, the sound of a bullet impact into a pig carcass may be mixed with the sound of a melon being gouged to add to the \"stickiness\" or \"gore\" of the effect. If the effect is featured in a close-up, the designer may also add an \"impact sweetener\" from their library. The sweetener may simply be the sound of a hammer pounding hardwood, equalized so that only the low-end can be heard. The low end gives the three sounds together added weight so that the audience actually \"feels\" the weight of the bullet hit the victim.\nIf the victim is the villain, and his death is climactic, the sound designer may add reverb to the impact, in order to enhance the dramatic beat. And then, as the victim falls over in slow motion, the sound editor may add the sound of a broom whooshing by a microphone, pitch-shifted down and time-expanded to further emphasize the death. If the film is science-fiction, the designer may phaser the \"whoosh\" to give it a more sci-fi feel. (For a list of many sound effects processes available to a sound designer, see the bottom of this article.)\nAesthetics.\nWhen creating sound effects for films, sound recordists and editors do not generally concern themselves with the verisimilitude or accuracy of the sounds they present. The sound of a bullet entering a person from a close distance may sound nothing like the sound designed in the above example, but since very few people are aware of how such a thing actually sounds, the job of designing the effect is mainly an issue of creating a conjectural sound which feeds the audience's expectations while still suspending disbelief.\nSci-fi and fantasy genres can be more forgiving in terms of audience expectations; the listener will not be caught off guard as much by unusual sound effects. In contrast, when creating sound effects for historical accuracy and realism, the listener likely had a lifetime of exposure to some of these sounds and so there are expectations of what they should sound like.\nIn the previous example, the phased 'whoosh' of the victim's fall has no analog in real-life experience, but it is emotionally immediate. If a sound editor uses such sounds in the context of emotional climax or a character's subjective experience, they can add to the drama of a situation in a way visuals simply cannot. If a visual effects artist were to do something similar to the 'whooshing fall' example, it would probably look ridiculous or at least excessively melodramatic.\nThe \"conjectural sound\" principle applies even to happenstance sounds, such as tires squealing, doorknobs turning or people walking. If the sound editor wants to communicate that a driver is in a hurry to leave, they will cut the sound of tires squealing when the car accelerates from a stop; even if the car is on a dirt road, the effect will work if the audience is dramatically engaged. If a character is afraid of someone on the other side of a door, the turning of the doorknob can take a second or more, and the mechanism of the knob can possess dozens of clicking parts. A skillful Foley artist can make someone walking calmly across the screen seem terrified simply by giving the actor a different gait.\nTechniques.\nIn music and film/television production, some typical effects used in recording and amplified performances are: \nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44189", "revid": "20483999", "url": "https://en.wikipedia.org/wiki?curid=44189", "title": "Reciprocal altruism", "text": "Form of behaviour between organisms\nIn evolutionary biology, reciprocal altruism is a behaviour whereby an organism acts in a manner that temporarily reduces its fitness while increasing another organism's fitness, with the expectation that the other organism will act in a similar manner at a later time.\nThe concept was initially developed by Robert Trivers to explain the evolution of cooperation as instances of mutually altruistic acts. The concept is close to the strategy of \"tit for tat\" used in game theory. In 1987, Trivers presented at a symposium on reciprocity, noting that he initially titled his article \"The Evolution of Delayed Return Altruism,\" but reviewer W. D. Hamilton suggested renaming it \"The Evolution of Reciprocal Altruism.\" While Trivers adopted the new title, he retained the original examples, causing confusion about reciprocal altruism for decades. Rothstein and Pierotti (1988) addressed this issue at the symposium, proposing new definitions that clarified the concepts. They argued that Delayed Return Altruism was a superior term and introduced \"pseudo-reciprocity\" to replace it.\nTheory.\nThe concept of \"reciprocal altruism\", as introduced by Trivers, suggests that altruism, defined as an act of helping another individual while incurring some cost for this act, could have evolved since it might be beneficial to incur this cost if there is a chance of being in a reverse situation where the individual who was helped before may perform an altruistic act towards the individual who helped them initially. This concept finds its roots in the work of W.D. Hamilton, who developed mathematical models for predicting the likelihood of an altruistic act to be performed on behalf of one's kin.\nPutting this into the form of a strategy in a repeated prisoner's dilemma would mean to cooperate unconditionally in the first period and behave cooperatively (altruistically) as long as the other agent does as well. If chances of meeting another reciprocal altruist are high enough, or if the game is repeated for a long enough amount of time, this form of altruism can evolve within a population.\nThis is close to the notion of \"tit for tat\" introduced by Anatol Rapoport, although there still seems a slight distinction in that \"tit for tat\" cooperates in the first period and from thereon always replicates an opponent's previous action, whereas \"reciprocal altruists\" stop cooperation in the first instance of non-cooperation by an opponent and stay non-cooperative from thereon. This distinction leads to the fact that in contrast to reciprocal altruism, tit for tat may be able to restore cooperation under certain conditions despite cooperation having broken down.\nChristopher Stephens shows a set of necessary and jointly sufficient conditions \"... for an instance of reciprocal altruism:\nThere are two additional conditions necessary \"...for reciprocal altruism to evolve:\"\nThe first two conditions are necessary for altruism as such, while the third is distinguishing reciprocal altruism from simple mutualism and the fourth makes the interaction reciprocal.\nCondition number five is required as otherwise non-altruists may always exploit altruistic behaviour without any consequences and therefore evolution of reciprocal altruism would not be possible. However, it is pointed out that this \"conditioning device\" does not need to be conscious. Condition number six is required to avoid cooperation breakdown through forward induction\u2014a possibility suggested by game theoretical models.\nIn 1987, Trivers told a symposium on reciprocity that he had originally submitted his article under the title \"The Evolution of Delayed Return Altruism\", but reviewer W. D. Hamilton suggested that he change the title to \"The Evolution of Reciprocal Altruism\". Trivers changed the title, but not the examples in the manuscript, which has led to confusion about what were appropriate examples of reciprocal altruism for the last 50 years. In their contribution to that symposium, Rothstein and Pierotti (1988) addressed this issue and proposed new definitions concerning the topic of altruism, that clarified the issue created by Trivers and Hamilton. They proposed that Delayed Return Altruism was a superior concept and used the term pseudo-reciprocity in place of DRA.\nExamples.\nThe following examples could be understood as altruism. However, showing reciprocal altruism in an unambiguous way requires more evidence as will be shown later.\nCleaner fish.\nAn example of reciprocal altruism is cleaning symbiosis, such as between cleaner fish and their hosts, though cleaners include shrimps and birds, and clients include fish, turtles, octopuses and mammals. Aside from the apparent symbiosis of the cleaner and the host during actual cleaning, which cannot be interpreted as altruism, the host displays additional behaviour that meets the criteria for delayed return altruism:\nThe host fish allows the cleaner fish free entrance and exit and does not eat the cleaner, even after the cleaning is done. The host signals the cleaner it is about to depart the cleaner's locality, even when the cleaner is not in its body. The host sometimes chases off possible dangers to the cleaner.\nThe following evidence supports the hypothesis:\nThe cleaning by cleaners is essential for the host. In the absence of cleaners the hosts leave the locality or suffer from injuries inflicted by ectoparasites. There is difficulty and danger in finding a cleaner. Hosts leave their element to get cleaned. Others wait no longer than 30 seconds before searching for cleaners elsewhere.\nA key requirement for the establishment of reciprocal altruism is that the same two individuals must interact repeatedly, as otherwise the best strategy for the host would be to eat the cleaner as soon as cleaning was complete. This constraint imposes both a spatial and a temporal condition on the cleaner and on its host. Both individuals must remain in the same physical location, and both must have a long enough lifespan, to enable multiple interactions. There is reliable evidence that individual cleaners and hosts do indeed interact repeatedly.\nThis example meets some, but not all, of the criteria described in Trivers's model. In the cleaner-host system the benefit to the cleaner is always immediate. However, the evolution of reciprocal altruism is contingent on opportunities for future rewards through repeated interactions. In one study, nearby host fish observed \"cheater\" cleaners and subsequently avoided them. In these examples, true reciprocity is difficult to demonstrate since failure means the death of the cleaner. However, if Randall's claim that hosts sometimes chase off possible dangers to the cleaner is correct, an experiment might be constructed in which reciprocity could be demonstrated. In actuality this is one of Trivers' examples of Delayed Return Altruism as discussed by Rothstein and Pierotti 1988.\nWarning calls in birds.\nWarning calls, although exposing a bird and putting it in danger, are frequently given by birds. An explanation in terms of altruistic behaviors given by Trivers:\nIt has been shown that predators learn specific localities and specialize individually on prey types and hunting techniques.\nIt is therefore disadvantageous for a bird to have a predator eat a conspecific, because the experienced predator may then be more likely to eat them. Alarming another bird by giving a warning call tends to prevent predators from specializing on the caller's species and locality. In this way, birds in areas in which warning calls are given will be at a selective advantage relative to birds in areas free from warning calls.\nNevertheless, this presentation lacks important elements of reciprocity. It is very hard to detect and ostracize cheaters. There is no evidence that a bird refrains from giving calls when another bird is not reciprocating, nor evidence that individuals interact repeatedly. Given the aforementioned characteristics of bird calling, a continuous bird emigration and immigration environment (true of many avian species) is most likely to be partial to cheaters, since selection against the selfish gene is unlikely.\nAnother explanation for warning calls is that these are not warning calls at all:\nA bird, once it has detected a bird of prey, calls to signal to the bird of prey that it was detected, and that there is no use trying to attack the calling bird. Two facts support this hypothesis:\nNest protecting.\nRed-winged blackbird males help defend neighbor's nests. There are many theories as to why males behave this way. One is that males only defend other nests which contain their extra-pair offspring. Extra-pair offspring are juveniles which may contain some of the male bird's DNA. Another is the tit-for-tat strategy of reciprocal altruism. A third theory is, males help only other closely related males. A study done by The Department of Fisheries and Wildlife provided evidence that males used a tit-for-tat strategy. The Department of Fisheries and Wildlife tested many different nests by placing stuffed crows by nests, and then observing behavior of neighboring males. The behaviors they looked for included the number of calls, dives, and strikes. After analyzing the results, there was not significance evidence for kin selection; the presence of extra-pair offspring did not affect the probability of help in nest defense. However, males reduced the amount of defense given to neighbors when neighbor males reduced defense for their nests. This demonstrates a tit-for-tat strategy, where animals help those who previously helped them. This strategy is one type of reciprocal altruism.\nVampire bats.\nVampire bats also display reciprocal altruism, as described by Wilkinson.\nThe bats feed each other by regurgitating blood. Since bats only feed on blood and will die after just 70 hours of not eating, this food sharing is a great benefit to the receiver and a great cost to the giver.\nTo qualify for reciprocal altruism, the benefit to the receiver would have to be larger than the cost to the donor. This seems to hold as these bats usually die if they do not find a blood meal two nights in a row. Also, the requirement that individuals who have behaved altruistically in the past are helped by others in the future is confirmed by the data. However, the consistency of the reciprocal behaviour, namely that a previously non-altruistic bat is refused help when it requires it, has not been demonstrated. Therefore, the bats do not seem to qualify yet as an unequivocal example of reciprocal altruism.\nPrimates.\nGrooming in primates meets the conditions for reciprocal altruism according to some studies. One of the studies in vervet monkeys shows that among unrelated individuals, grooming induce higher chance of attending to each other's calls for aid. However, vervet monkeys also display grooming behaviors within group members, displaying alliances. This would demonstrate vervet monkey's grooming behavior as a part of kin selection since the activity is done between siblings in this study. Moreover, following the criteria by Stephen, if the study is to be an example of reciprocal altruism, it must prove the mechanism for detecting cheaters.\nBacteria.\nNumerous species of bacteria engage in reciprocal altruistic behaviors with other species. Typically, this takes the form of bacteria providing essential nutrients for another species, while the other species provides an environment for the bacteria to live in. Reciprocal altruism is exhibited between nitrogen-fixing bacteria and plants in which they reside. Additionally, it can be observed between bacteria and some species of flies such as \"Bactrocera tryoni\". These flies consume nutrient-producing bacteria found on the leaves of plants; in exchange, they reside within the flies' digestive system. This reciprocal altruistic behavior has been exploited by techniques designed to eliminate \"B. tryoni\", which are fruit fly pests native to Australia.\nHumans.\nExamples of reciprocal altruism in humans include helping injured individuals, sharing food, tools, or knowledge, and providing assistance in crises with the expectation of future aid. In social interactions, individuals often engage in direct reciprocity, such as returning favors or lending resources with an implicit understanding of future repayment. Indirect reciprocity is also observed, where individuals help others based on reputation, encouraging mutual cooperation within a community. Economic and political systems rely on reciprocal altruism through trade agreements, diplomatic alliances, and social contracts, where long-term benefits outweigh short-term costs. Additionally, studies in game theory, such as the Prisoner's Dilemma, illustrate how cooperative behaviors emerge and stabilize when individuals recognize the advantages of mutual support.\nExceptions.\nSome animals seem to be unable to develop reciprocal altruism. For example, pigeons defect instead of a random response or a tit-for-tat in a prisoner's dilemma game against a computer. This may be due to favoring short-term thinking over long-term thinking.\nRegulation by emotional disposition.\nIn comparison to that of other animals, the human altruistic system is a sensitive and unstable one. Therefore, the tendency to give, to cheat, and the response to other's acts of giving and cheating must be regulated by a complex psychology in each individual, social structures, and cultural traditions. Individuals differ in the degree of these tendencies and responses.\nAccording to Trivers, the following emotional dispositions and their evolution can be understood in terms of regulation of altruism.\nIt is not known how individuals pick partners as there has been little research on choice. Modeling indicates that altruism about partner choices is unlikely to evolve, as costs and benefits between multiple individuals are variable. Therefore, the time or frequency of reciprocal actions contributes more to an individual's choice of partner than the reciprocal act itself.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44190", "revid": "7979758", "url": "https://en.wikipedia.org/wiki?curid=44190", "title": "The Selfish Gene", "text": "1976 book by Richard Dawkins\nThe Selfish Gene is a 1976 book by ethologist Richard Dawkins that promotes the gene-centered view of evolution, as opposed to views focused on the organism and the group. The book builds upon the thesis of George C. Williams's \"Adaptation and Natural Selection\" (1966); it also popularized ideas developed during the 1960s by W. D. Hamilton and others. From the gene-centred view, it follows that the more two individuals are genetically related, the more sense (at the level of the genes) it makes for them to behave cooperatively with each other.\nA lineage is expected to evolve to maximise its inclusive fitness\u2014the number of copies of its genes passed on globally (rather than by a particular individual). As a result, populations will tend towards an evolutionarily stable strategy. The book also introduces the term \"meme\" for a unit of human cultural evolution analogous to the gene, suggesting that such \"selfish\" replication may also model human culture, in a different sense. Memetics has become the subject of many studies since the publication of the book. In raising awareness of Hamilton's ideas, as well as making its own valuable contributions to the field, the book has also stimulated research on human inclusive fitness.\nDawkins uses the term \"selfish gene\" as a way of expressing the gene-centered view of evolution. As such, the book is not about a particular gene that causes selfish behaviour; in fact, much of the book's content is devoted to explaining the evolution of altruism. In the foreword to the book's 30th-anniversary edition, Dawkins said he \"can readily see that [the book's title] might give an inadequate impression of its contents\" and in retrospect wishes he had taken Tom Maschler's advice and titled it \"The Immortal Gene\". He laments that \u201cToo many people read it by title only.\u201d In response, he expanded on the evolution of altruism in the BBC documentary \"Nice Guys Finish First\".\nBackground.\nDawkins builds upon George C. Williams's book \"Adaptation and Natural Selection\" (1966), which argued that altruism is not based upon group benefit \"per se\", but results from selection that occurs \"at the level of the gene mediated by the phenotype\" and that any selection at the group level occurred only under rare circumstances. W. D. Hamilton and others developed this approach further during the 1960s; they opposed the concepts of group selection and of selection aimed directly at benefit to the individual organism:\nDespite the principle of 'survival of the fittest' the ultimate criterion which determines whether [a gene] \"G\" will spread is not whether the behavior is to the benefit of the behaver, but whether it is to the benefit of the gene \"G\" ...With altruism this will happen only if the affected individual is a relative of the altruist, therefore having an increased chance of carrying the gene.\n\u2014 W. D. Hamilton, \"The Evolution of Altruistic Behavior\" (1963), pp. 354\u2013355.\nWilkins and Hull (2014) provide an extended discussion of Dawkins's views and of his book \"The Selfish Gene\".\nBook.\nContents.\nDawkins begins by discussing the altruism that people display, indicating that he will argue it is explained by gene selfishness, and attacking group selection as an explanation. He considers the origin of life with the arrival of molecules able to replicate themselves. From there, he looks at DNA's role in evolution, and its organisation into chromosomes and genes, which in his view behave selfishly. He describes organisms as apparently purposive but fundamentally simple survival machines, which use negative feedback to achieve control. This extends, he argues, to the brain's ability to simulate the world with subjective consciousness, and signalling between species. He then introduces the idea of the evolutionarily stable strategy, and uses it to explain why alternative competitive strategies like bullying and retaliating exist. This allows him to consider what selfishness in a gene might actually mean, describing W. D. Hamilton's argument for kin selection, that genes for behaviour that improves the survival chances of close relatives can spread in a population, because those relatives carry the same genes.\nDawkins examines childbearing and raising children as evolutionary strategies. He attacks the idea of group selection for the good of the species as proposed by V. C. Wynne-Edwards, arguing instead that each parent necessarily behaves selfishly. A question is whether parents should invest in their offspring equally or should favour some of them and explains that what is best for the survival of the parents' genes is not always best for individual children. Similarly, Dawkins argues, there are conflicts of interest between males and females, but he notes that R. A. Fisher showed that the optimal sex ratio is 50:50. He explains that this is true even in an extreme case like the harem-keeping elephant seal, where 4% of the males get 88% of copulations. In that case, the strategy of having a female offspring is safe, as she'll have a pup, but the strategy of having a male can bring a large return (dozens of pups), even though many males live out their lives as bachelors. Amotz Zahavi's theory of honest signalling explains stotting as a selfish act, he argues, improving the springbok's chances of escaping from a predator by indicating how difficult the chase would be.\nDawkins discusses why many species live in groups, achieving mutual benefits through mechanisms such as Hamilton's selfish herd model: each individual behaves selfishly but the result is herd behaviour. Altruism too can evolve, as in the social insects such as ants and bees, where workers give up the right to reproduce in favour of a sister, the queen; in their case, the unusual (haplodiploid) system of sex determination may have helped to bring this about, as females in a nest are exceptionally closely related.\nThe final chapter of the first edition introduced the idea of the meme, a culturally-transmitted entity such as a hummable tune, by analogy to genetic transmission. Dawkins describes God as an old idea which probably arose many times, and which has sufficient psychological appeal to survive effectively in the meme pool. The second edition (1989) added two more chapters.\nThemes.\n\"Selfish\" genes.\nIn describing genes as being \"selfish\", Dawkins states unequivocally that he does not intend to imply that they are driven by any motives or will, but merely that their effects can be metaphorically and pedagogically described \"as if\" they were. His contention is that the genes that are passed on are the ones whose evolutionary consequences serve their own implicit interest (to continue the anthropomorphism) in being replicated, not necessarily those of the organism. In later work, Dawkins brings evolutionary \"selfishness\" down to creation of a widely proliferated \"extended phenotype\".\nFor some, the metaphor of \"selfishness\" is entirely clear, while to others it is confusing, misleading, or simply silly to ascribe mental attributes to something that is mindless. For example, Andrew Brown has written:\n\"Selfish\", when applied to genes, doesn't mean \"selfish\" at all. It means, instead, an extremely important quality for which there is no good word in the English language: \"the quality of being copied by a Darwinian selection process.\" This is a complicated mouthful. There ought to be a better, shorter word\u2014but \"selfish\" isn't it.\nDonald Symons also finds it inappropriate to use anthropomorphism in conveying scientific meaning in general, and particularly in this instance. He writes in \"The Evolution of Human Sexuality\" (1979):\nIn summary, the rhetoric of \"The Selfish Gene\" exactly reverses the real situation: through [the use of] metaphor genes are endowed with properties only sentient beings can possess, such as selfishness, while sentient beings are stripped of these properties and called machines...The anthropomorphism of genes...obscures the deepest mystery in the life sciences: the origin and nature of mind.\n\"Replicators\".\nDawkins proposes the idea of the \"replicator\":\nIt is finally time to return to the problem with which we started, to the tension between individual organism and gene as rival candidates for the central role in natural selection...One way of sorting this whole matter out is to use the terms 'replicator' and 'vehicle'. The fundamental units of natural selection, the basic things that survive or fail to survive, that form lineages of identical copies with occasional random mutations, are called replicators. DNA molecules are replicators. They generally, for reasons that we shall come to, gang together into large communal survival machines or 'vehicles'.\n\u2014 Richard Dawkins, \"The Selfish Gene\", https:// (Anniversary Edition)\nThe original replicator (Dawkins \"Replicator\") was the initial molecule which first managed to reproduce itself and thus gained an advantage over other molecules within the primordial soup. As replicating molecules became more complex, Dawkins postulates, the replicators became the genes within organisms, with each organism's body serving the purpose of a 'survival machine' for its genes.\nDawkins writes that gene combinations which help an organism to survive and reproduce tend to also improve the gene's own chances of being replicated, and, as a result, \"successful\" genes frequently provide a benefit to the organism. An example of this might be a gene that protects the organism against a disease. This helps the gene spread, and also helps the organism.\nGenes vs organisms.\nThere are other times when the implicit interests of the vehicle and replicator are in conflict, such as the genes behind certain male spiders' instinctive mating behaviour, which increase the organism's inclusive fitness by allowing it to reproduce but shorten its life by exposing it to the risk of being eaten by the cannibalistic female. Another example is the existence of segregation distorter genes that are detrimental to their host, but nonetheless propagate themselves at its expense. Likewise, the persistence of junk DNA that [Dawkins believed at that time] provides no benefit to its host can be explained on the basis that it is not subject to selection. These unselected for but transmitted DNA variations connect the individual genetically to its parents but confer no survival benefit.\nThese examples might suggest that there is a power struggle between genes and their interactor. In fact, the claim is that there isn't much of a struggle because the genes usually win without a fight. However, the claim is made that if the organism becomes intelligent enough to understand its own interests, as distinct from those of its genes, there can be true conflict.\nAn example of such a conflict might be a person using birth control to prevent fertilisation, thereby inhibiting the replication of his or her genes. But this action might not be a conflict of the 'self-interest' of the organism with his or her genes, since a person using birth control might also be enhancing the survival chances of their genes by limiting family size to conform with available resources, thus avoiding extinction as predicted under the Malthusian model of population growth.\nAltruism.\nDawkins says that his \"purpose\" in writing \"The Selfish Gene\" is \"to examine the biology of selfishness and altruism.\" He does this by supporting the claim that \"gene selfishness will usually give rise to selfishness in individual behaviour. However, as we shall see, there are special circumstances in which a gene can achieve its own selfish goals best by fostering a limited form of altruism at the level of individual animals.\" Gene selection provides one explanation for kin selection and eusociality, where organisms act altruistically, against their individual interests (in the sense of health, safety or personal reproduction), namely the argument that by helping related organisms reproduce, a gene succeeds in \"helping\" copies of themselves (or sequences with the same phenotypic effect) in other bodies to replicate. The claim is made that these \"selfish\" actions of genes lead to unselfish actions by organisms. A requirement upon this claim, supported by Dawkins in Chapter 10: \"You scratch my back, I'll ride on yours\" by examples from nature, is the need to explain how genes achieve kin recognition, or manage to orchestrate mutualism and coevolution. Although Dawkins (and biologists in general) recognize these phenomena result in more copies of a gene, evidence is inconclusive whether this success is selected for at a group or individual level. In fact, Dawkins has proposed that it is at the level of the \"extended phenotype\": \nWe agree [referring to Wilson and Sober's book \"Unto others: The evolution and psychology of unselfish behavior\"] that genes are replicators, organisms and groups are not. We agree that the group selection controversy ought to be a controversy about groups as vehicles, and we could easily agree to differ on the answer...I coined the [term] vehicle not to praise it but to bury it...Darwinism can work on replicators whose phenotypic effects (interactors) are too diffuse, too multi-levelled, too incoherent to deserve the accolade of vehicle...Extended phenotypes can include inanimate artifacts like beaver dams...But the vehicle is not something fundamental...Ask rather \"Is there a vehicle in this situation and, if so, why?\"\n\u2014Richard Dawkins, \"Burying the Vehicle\"\nAlthough Dawkins agrees that groups can assist survival, they rank as a \"vehicle\" for survival only if the group activity is replicated in descendants, recorded in the gene, the gene being the only true replicator. An improvement in the survival lottery for the group must improve that for the gene for sufficient replication to occur. Dawkins argues qualitatively that the lottery for the gene is based upon a very long and broad record of events, and group advantages are usually too specific, too brief, and too fortuitous to change the gene lottery:\nWe can now see that the organism and the group of organisms are true rivals for the vehicle role in the story, but neither of them is even a \"candidate\" for the replicator role. The controversy between 'individual selection' and 'group selection' is a real controversy between alternative vehicles...As it happens the outcome, in my view, is a decisive victory for the individual organism. The group is too wishy-washy an entity. \n\u2014Richard Dawkins, \"The Selfish Gene\", https://\nPrior to the 1960s, it was common for altruism to be explained in terms of group selection, where the benefits to the organism or even population were supposed to account for the popularity of the genes responsible for the tendency towards that behaviour. Modern versions of \"multilevel selection\" claim to have overcome the original objections, namely, that at that time no known form of group selection led to an evolutionarily stable strategy. The claim still is made by some that it would take only a single individual with a tendency towards more selfish behaviour to undermine a population otherwise filled only with the gene for altruism towards non-kin.\nReception.\n\"The Selfish Gene\" was extremely popular when first published, causing \"a silent and almost immediate revolution in biology\", and it continues to be widely read. It has sold over a million copies and has been translated into more than 25 languages. Proponents argue that the central point, that replicating the gene is the object of selection, usefully completes and extends the explanation of evolution given by Charles Darwin before the basic mechanisms of genetics were understood.\nAccording to the ethologist Alan Grafen, acceptance of adaptionist theories is hampered by a lack of a mathematical unifying theory and a belief that anything in words alone must be suspect. According to Grafen, these difficulties along with an initial conflict with population genetics models at the time of its introduction \"explains why within biology the considerable scientific contributions it [\"The Selfish Gene\"] makes are seriously underestimated, and why it is viewed mainly as a work of exposition.\" According to comparative psychologist Nicky Hayes, \"Dawkins presented a version of sociobiology that rested heavily on metaphors drawn from animal behavior, and extrapolated these...One of the weaknesses of the sociological approach is that it tends only to seek confirmatory examples from among the huge diversity of animal behavior. Dawkins did not deviate from this tradition.\" More generally, critics argue that \"The Selfish Gene\" oversimplifies the relationship between genes and the organism. (As an example, see Thompson.)\n\"The Selfish Gene\" further popularised sociobiology in Japan after its translation in 1980. With the addition of Dawkins's book to the country's consciousness, the term \"meme\" entered popular culture. Yuzuru Tanaka of Hokkaido University wrote a book, \"Meme Media and Meme Market Architectures\", while the psychologist Susan Blackmore wrote \"The Meme Machine\" (2000), with a foreword by Dawkins. The information scientist Osamu Sakura has published a book in Japanese and several papers in English on the topic. Nippon Animation produced an educational television program titled \"The Many Journeys of Meme\".\nIn 1976, the ecologist Arthur Cain, one of Dawkins's tutors at Oxford in the 1960s, called it a \"young man's book\" (which Dawkins points out was a deliberate quote of a commentator on the New College, Oxford philosopher A. J. Ayer's \"Language, Truth, and Logic\" (1936)). Dawkins noted that he had been \"flattered by the comparison, [but] knew that Ayer had recanted much of his first book and [he] could hardly miss Cain's pointed implication that [he] should, in the fullness of time, do the same.\" This point also was made by the philosopher Mary Midgley: \"The same thing happened to AJ Ayer, she says, but he spent the rest of his career taking back what he'd written in \"Language, Truth and Logic\". \"This hasn't occurred to Dawkins\", she says. \"He goes on saying the same thing.\"\" However, according to Wilkins and Hull, Dawkins's thinking has developed, although perhaps not defusing this criticism:\nIn Dawkins's early writings, replicators and vehicles played different but complementary and equally important roles in selection, but as Dawkins honed his view of the evolutionary process, vehicles became less and less fundamental...In later writings Dawkins goes even further and argues that phenotypic traits are what really matter in selection and that they can be treated independently of their being organized into vehicles...Thus, it comes as no surprise when Dawkins proclaims that he \"coined the term 'vehicle' not to praise it but to bury it.\" As prevalent as organisms might be, as determinate as the causal roles that they play in selection are, reference to them can and must be omitted from any perspicuous characterization of selection in the evolutionary process. Dawkins is far from a genetic \"determinist\", but he is certainly a genetic \"reductionist\".\n\u2014 John S Wilkins, David Hull, \"Dawkins on Replicators and Vehicles\", The \"Stanford Encyclopedia of Philosophy\"\nIn \"\", Rutger Bregman contrasts the portrayal of human nature in \"The Selfish Gene\" with a more cooperative view.\nUnits of selection.\nAs to the unit of selection: \"One internally consistent logical picture is that the unit of replication is the gene...and the organism is one kind of ...entity on which selection acts directly.\" Dawkins proposed the matter without a distinction between 'unit of replication' and 'unit of selection' that he made elsewhere: \"the fundamental unit of selection, and therefore of self-interest, is not the species, nor the group, nor even strictly the individual. It is the gene, the unit of heredity.\" However, he continues in a later chapter:\nOn any sensible view of the matter Darwinian selection does not work on genes directly. ...The important differences between genes emerge only in their \"effects\". The technical word \"phenotype\" is used for the bodily manifestation of a gene, the effect that a gene has on the body...Natural selection favours some genes rather than others not because of the nature of the genes themselves, but because of their consequences\u2014their phenotypic effects...But we shall now see that the phenotypic effects of a gene need to be thought of as \"all the effects that it has on the world\". ...The phenotypic effects of a gene are the tools by which it levers itself into the next generation. All I am going to add is that the tools may reach outside the individual body wall...Examples that spring to mind are artefacts like beaver dams, bird nests, and caddis houses.\n\u2014 Richard Dawkins, \"The Selfish Gene\", Chapter 13, pp. 234, 235, 238\nDawkins's later formulation is in his book \"The Extended Phenotype\" (1982), where the process of selection is taken to involve every possible phenotypical effect of a gene.\nStephen Jay Gould finds Dawkins's position tries to have it both ways:\nDawkins claims to prefer genes and to find greater insight in this formulation. But he allows that you or I might prefer organisms\u2014and it really doesn't matter.\n\u2014 Stephen Jay Gould, \"The Structure of Evolutionary Theory\", pp. 640-641\nThe view of \"The Selfish Gene\" is that selection based upon groups and populations is rare compared to selection on individuals. Although supported by Dawkins and by many others, this claim continues to be disputed. While na\u00efve versions of group selectionism have been disproved, more sophisticated formulations make accurate predictions in some cases while positing selection at higher levels. Both sides agree that very favourable genes are likely to prosper and replicate if they arise and both sides agree that living in groups can be an advantage to the group members. The conflict arises in part over defining concepts: \nCultural evolutionary theory, however, has suffered from an overemphasis on the experiences and behaviors of individuals at the expense of acknowledging complex group organization...Many important behaviors related to the success and function of human societies are only properly defined at the level of groups.\nIn \"The Social Conquest of Earth\" (2012), the entomologist E. O. Wilson contends that although the selfish-gene approach was accepted \"until 2010 [when] Martin Nowak, Corina Tarnita, and I demonstrated that inclusive fitness theory, often called kin selection theory, is both mathematically and biologically incorrect.\" Chapter 18 of \"The Social Conquest of Earth\" describes the deficiencies of kin selection and outlines group selection, which Wilson argues is a more realistic model of social evolution. He criticises earlier approaches to social evolution, saying: \"unwarranted faith in the central role of kinship in social evolution has led to the reversal of the usual order in which biological research is conducted. The proven best way in evolutionary biology, as in most of science, is to define a problem arising during empirical research, then select or devise the theory that is needed to solve it. Almost all research in inclusive-fitness theory has been the opposite: hypothesize the key roles of kinship and kin selection, then look for evidence to test that hypothesis.\" According to Wilson: \"People must have a tribe...Experiments conducted over many years by social psychologists have revealed how swiftly and decisively people divide into groups, and then discriminate in favor of the one to which they belong.\" (pp.\u00a057, 59) According to Wilson: \"Different parts of the brain have evolved by group selection to create groupishness.\" (p.\u00a061)\nSome authors consider facets of this debate between Dawkins and his critics about the level of selection to be blather:\n\"The particularly frustrating aspects of these constantly renewed debates is that, even though they seemed to be sparked by rival theories about how evolution works, in fact they often involve only rival metaphors for the very same evolutionary logic and [the debates over these aspects] are thus empirically empty.\"\n\u2014 Laurent Keller, \"Levels of Selection in Evolution\", p.4\nOther authors say Dawkins has failed to make some critical distinctions, in particular, the difference between group selection for group advantage and group selection conveying individual advantage.\nChoice of words.\nA good deal of objection to \"The Selfish Gene\" stemmed from its failure to be always clear about \"selection\" and \"replication\". Dawkins says the gene is the fundamental unit of selection, and then points out that selection does not act directly upon the gene, but upon \"vehicles\" or '\"extended phenotypes\". Stephen Jay Gould took exception to calling the gene a 'unit of selection' because selection acted only upon phenotypes. Summarizing the Dawkins-Gould difference of view, Sterelny says:\nGould thinks gene differences do not cause evolutionary changes in populations, they register those changes.\n\u2014Kim Sterelny: \"Dawkins \"vs.\" Gould\", p. 83\nThe word \"cause\" here is somewhat tricky: does a change in lottery rules (for example, inheriting a defective gene \"responsible\" for a disorder) \"cause\" differences in outcome that might or might not occur? It certainly alters the likelihood of events, but a concatenation of contingencies decides what actually occurs. Dawkins thinks the use of \"cause\" as a statistical weighting is acceptable in common usage. Like Gould, Gabriel Dover in criticizing \"The Selfish Gene\" says:\nIt is illegitimate to give 'powers' to genes, as Dawkins would have it, to control the outcome of selection...There are no genes for interactions, as such: rather, each unique set of inherited genes contributes interactively to one unique phenotype...the true determinants of selection.\n\u2014 Gabriel Dover: \"Dear Mr. Darwin\", p. 56\nHowever, from a comparison with Dawkins's discussion of this very same point, it would seem both Gould's and Dover's comments are more a critique of his sloppy usage than a difference of views. Hull suggested a resolution based upon a distinction between replicators and interactors. The term \"replicator\" includes genes as the most fundamental replicators but possibly other agents, and \"interactor\" includes organisms but maybe other agents, much as do Dawkins's 'vehicles'. The distinction is as follows:\n\"replicator\": an entity that passes on its structure largely intact in successive replications.\n\"interactor\": an entity that interacts as a cohesive whole with its environment in such a way that this interaction \"causes\" replication to be differential.\n\"selection\": a process in which the differential extinction or proliferation of interactors causes the differential perpetuation of the replicators that produced them.\nHull suggests that, despite some similarities, Dawkins takes too narrow a view of these terms, engendering some of the objections to his views. According to Godfrey-Smith, this more careful vocabulary has cleared up \"misunderstandings in the \"units of selection\" debates.\"\nEnactive arguments.\nBehavioural genetics entertains the view:\nthat genes are dynamic contributors to behavioral organization and are sensitive to feedback systems from the internal and external environments. Technically behavior is not inherited; only DNA molecules are inherited. From that point on behavioral formation is a problem of constant interplay between genetic potential and environmental shaping.\n\u2014D.D. Thiessen, \"Mechanism specific approaches in behavior genetics\", p. 91\nThis view from 1970 is still espoused today, and conflicts with Dawkins's view of \"the gene as a form of \"information [that] passes through bodies and affects them, but is not affected by them on its way through\"\". The philosophical/biological field of enactivism stresses the interaction of the living agent with its environment and the relation of probing the environment to cognition and adaptation. Gene activation depends upon the cellular milieu. An extended discussion of the contrasts between enactivism and Dawkins's views, and with their support by Dennett, is provided by Thompson.\nIn \"Mind in Life\", the philosopher Evan Thompson has assembled a multi-sourced objection to the \"selfish gene\" idea. Thompson takes issue with Dawkin's reduction of \"life\" to \"genes\" and \"information\":\nLife is just bytes and bytes and bytes of digital information.\n\u2014 Richard Dawkins: \"River out of Eden: A Darwinian View of Life\", p. 19\nOn the bank of the Oxford canal...is a large willow tree, and it is pumping downy seeds into the air...It is raining instructions out there; it's raining programs; it's raining tree-growing, fluff-spreading algorithms. That is not a metaphor, it is the plain truth.\n\u2014 Richard Dawkins: \"The Blind Watchmaker\", p. 111\nThompson objects that the gene cannot operate by itself, since it requires an environment such as a cell, and life is \"the creative outcome of highly structured contingencies\". Thompson quotes Sarkar:\nthere is no clear technical notion of \"information\" in molecular biology. It is little more than a metaphor that masquerades as a theoretical concept and ...leads to a misleading picture of the nature of possible explanations in molecular biology.\n\u2014 Sahotra Sarkar \"Biological information: a skeptical look at some central dogmas of molecular biology\", p. 187\nThompson follows with a detailed examination of the concept of DNA as a look-up-table and the role of the cell in orchestrating the DNA-to-RNA transcription, indicating that by anyone's account the DNA is hardly the whole story. Thompson goes on to suggest that the cell-environment interrelationship has much to do with reproduction and inheritance, and a focus on the gene as a form of \"information [that] passes through bodies and affects them but is not affected by them on its way through\" is tantamount to adoption of a form of material-informational dualism that has no explanatory value and no scientific basis. (Thomson, p.\u00a0187) The enactivist view, however, is that information results from the probing and experimentation of the agent with the agent's environment subject to the limitations of the agent's abilities to probe and process the result of probing, and DNA is simply one mechanism the agent brings to bear upon its activity.\nMoral arguments.\nAnother criticism of the book is its treatment of morality, and more particularly altruism, as existing only as a form of selfishness:\nIt is important to realize that the above definitions of altruism and selfishness are \"behavioural\", not subjective. I am not concerned here with the psychology of motives...My definition is concerned only with whether the effect of an act is to lower or raise the survival prospects of the presumed altruist and the survival prospects of the presumed beneficiary. \n\u2014 Richard Dawkins, \"The Selfish Gene\", p. 12 \nWe can even discuss ways of cultivating and nurturing pure, disinterested altruism, something that has no place in nature, something that has never existed before in the whole history of the world.\n\u2014 Richard Dawkins, \"The Selfish Gene\", p. 179\nThe philosopher Mary Midgley has suggested this position is a variant of Hobbes's explanation of altruism as enlightened self-interest, and that Dawkins goes a step further to suggest that our genetic programming can be overcome by what amounts to an extreme version of free will. Part of Mary Midgley's concern is that Richard Dawkins's account of \"The Selfish Gene\" serves as a moral and ideological justification for selfishness to be adopted by modern human societies as simply following \"nature\", providing an excuse for behavior with bad consequences for future human society.\nDawkins's major concluding theme, that humanity is finally gaining power over the \"selfish replicators\" by virtue of their intelligence, is criticized also by primatologist Frans de Waal, who refers to it as an example of a \"veneer theory\" (the idea that morality is not fundamental, but is laid over a brutal foundation). Dawkins claims he merely describes how things are under evolution, and makes no moral arguments. On BBC-2 TV, Dawkins pointed to evidence for a \"Tit-for-Tat\" strategy (shown to be successful in game theory) as the most common, simple, and profitable choice.\nMore generally, the objection has been made that \"The Selfish Gene\" discusses philosophical and moral questions that go beyond biological arguments, relying upon anthropomorphisms and careless analogies.\nPublication.\n\"The Selfish Gene\" was first published by Oxford University Press in 1976 in eleven chapters with a preface by the author and a foreword by Robert Trivers. A second edition was published in 1989. This edition added two extra chapters, and substantial endnotes to the preceding chapters, reflecting new findings and thoughts. It also added a second preface by the author, but the original foreword by Trivers was dropped. The book contains no illustrations.\nThe book has been translated into at least 23 languages including Arabic, Thai and Turkish.\nIn 2006, a 30th-anniversary edition was published with the Trivers foreword and a new introduction by the author in which he states, \"This edition does, however---and it is a source of particular joy to me---restore the original Foreword by Robert Trivers.\" This edition was accompanied by a \"festschrift\" entitled \"\" (2006). In March 2006, a special event entitled \"The Selfish Gene\": Thirty Years On was held at the London School of Economics. In March 2011, Audible Inc published an audiobook edition narrated by Richard Dawkins and Lalla Ward.\nIn 2016, Oxford University Press published \"The Extended Selfish Gene\", a 40th anniversary edition with a new epilogue, in which Dawkins describes the continued relevance of the gene's eye view of evolution and states that it, along with coalescence analysis \"illuminates the deep past in ways of which I had no inkling when I first wrote \"The Selfish Gene\"...\" It contains two chapters from his later \"The Extended Phenotype\". He credits Yan Wong, \"my co-author of \"The Ancestor's Tale\", from whom I learned everything I know about coalescence theory and much else besides.\" \nAwards and recognition.\nIn April 2016, \"The Selfish Gene\" was listed in \"The Guardian\"'s list of the 100 best nonfiction books, by Robert McCrum.\nIan McEwan writes that There has never been a science book quite like it. Drawing on the work of a handful of scientists, it bound together genetics and Darwinian natural selection in a creative synthesis that amazed even those few who were already familiar with the concepts. It hastened a sea change in evolutionary theory, it affected profoundly the teaching of biology, it enticed an enthusiastic younger generation into the subject, and spawned a huge literature, and eventually a new discipline - memetics. At the same time, and this is the measure of its achievement, it addressed itself without condescension to the layman. It did so provocatively, and with style.\nIn the years since then, Dawkins' work might be seen as one extended invitation addressed to us non-scientists to enjoy science, to indulge ourselves at a feast of human ingenuity. Just as we can sit around the kitchen table and discuss operas, movies or novels without being composers, directors or novelists, so we can engage with this subject, one more sublime achievement of accumulated creativity. We can make it \"ours\" just as we might the music of Bach or Bill Evans.\n\"The Selfish Gene\" stood at the beginning of a golden age of science writing. With a fine sense of literary tradition, the physicist Steven Weinberg, in his book \"Dreams of a Final Theory\", revisited Huxley's lecture on chalk in order to make the case for reductionism. Steven Pinker's application of Darwinian thought to Chomskyan linguistics in \"The Language Instinct\" is one of the finest celebrations of language I know. Among many other indispensable 'classics', I would propose EO Wilson's \"The Diversity of Life\" on the ecological wonders of the Amazon rain forest, and on the teeming micro-organisms in a handful of soil; David Deutsch's masterly account of the Many Worlds theory in \"The Fabric of Reality\"; Jared Diamond's melding of history with biological thought in \"Guns, Germs and Steel\"...\nWeinberg included it on his list of the 13 best science books for the general reader.\nIn July 2017, a poll to celebrate the 30th anniversary of the Royal Society Science Book Prize listed \"The Selfish Gene\" as the most influential science book of all time.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44191", "revid": "32589484", "url": "https://en.wikipedia.org/wiki?curid=44191", "title": "D. H. Lawrence", "text": "English writer and poet (1885\u20131930)\nDavid Herbert Lawrence (11 September 1885\u00a0\u2013 2 March 1930) was an English novelist, short story writer, poet, playwright, literary critic, travel writer, essayist, and painter. His modernist works reflect on modernity, social alienation and industrialisation, while championing sexuality, vitality and instinct. Four of his most famous novels\u00a0\u2013 \"Sons and Lovers\"\n(1913), \"The Rainbow\" (1915), \"Women in Love\" (1920), and \"Lady Chatterley's Lover\" (1928)\u00a0\u2013 were the subject of censorship trials for their radical portrayals of romance, sexuality and use of explicit language.\nLawrence's opinions and artistic preferences earned him a controversial reputation; he endured persecution and the misrepresentation of his creative work throughout his life, much of which he spent in a voluntary exile that he described as a \"savage enough pilgrimage\". At the time of his death, he had been variously scorned as tasteless, avant-garde, and a pornographer who had only garnered success for erotica; however, the English novelist and critic E. M. Forster, in an obituary notice, challenged this widely held view, describing him as \"the greatest imaginative novelist of our generation\". Later, the English literary critic F. R. Leavis also championed both his artistic integrity and his moral seriousness.\nEarly life and education.\nLawrence was born on 11 September 1885, in Eastwood, Nottinghamshire, England, the fourth child of Arthur John Lawrence, a barely literate miner at Brinsley Colliery, and Lydia Lawrence (n\u00e9e Beardsall), a former pupil-teacher who had been obliged to perform manual work in a lace factory due to her family's financial difficulties. He spent his formative years in the coal mining town of Eastwood, Nottinghamshire. The house in which he was born, 8a Victoria Street, is now the D. H. Lawrence Birthplace Museum. His working class background and the tensions between his parents provided the raw material for some of his early works. Lawrence roamed out from an early age in the patches of open, hilly country and remaining fragments of Sherwood Forest in Felley woods to the north of Eastwood, beginning a lifelong appreciation of the natural world, and he often wrote about \"the country of my heart\" as a setting for much of his fiction. Although Lawrence's father wanted his sons \"to go down pit\", his mother \"was bitterly determined that her children should not earn their livings by any kind of manual labor\", and she prevailed, at least as to Lawrence.\nLawrence attended Beauvale Board School (now renamed Greasley Beauvale D. H. Lawrence Primary School in his honour) from 1891 until 1898, becoming the first local pupil to win a county council scholarship to Nottingham High School in nearby Nottingham. He left in 1901, working for three months as a junior clerk at Haywood's surgical appliances factory, but a severe bout of pneumonia ended this career. During his convalescence he often visited Hagg's Farm, the home of the Chambers family, and began a friendship with one of the daughters, Jessie Chambers, who would inspire characters he created in his writing. An important aspect of his relationship with Chambers and other adolescent acquaintances was a shared love of books, an interest that lasted throughout Lawrence's life.\nIn a private letter written in 1908, Lawrence voiced support for eugenics by the method of a \"lethal chamber\" to dispose of \"all the sick, the halt, the maimed\".\nCareer.\nFrom 1902 until 1906, Lawrence was a pupil-teacher at the British School in Eastwood. He went on to become a full-time student and received a teaching certificate from University College, Nottingham (then an external college of University of London), in 1908. During these early years he was working on his first poems, some short stories, and a draft of a novel, \"Laetitia\", which was eventually to become \"The White Peacock.\" At the end of 1907, he won a short story competition in the \"Nottinghamshire Guardian\", the first time that he had gained any wider recognition for his literary talents.\nEarly career.\nIn the autumn of 1908, the newly qualified Lawrence left his childhood home for London. While teaching in Davidson Road School, Croydon, he continued writing. Jessie Chambers submitted some of Lawrence's early poetry to Ford Madox Ford (then known as Ford Hermann Hueffer), editor of the influential \"The English Review\". Hueffer then commissioned the story \"Odour of Chrysanthemums\" which, when published in that magazine, encouraged Heinemann, a London publisher, to ask Lawrence for more work. His career as a professional author now began in earnest, although he taught for another year.\nShortly after the final proofs of his first published novel, \"The White Peacock\", appeared in 1910, Lawrence's mother died of cancer, leaving him devastated. He described the following few months as his \"sick year\". Due to Lawrence's close relationship with his mother, his grief became a major turning point in his life, just as the death of his character, Mrs. Morel, is a major turning point in his autobiographical novel \"Sons and Lovers\", a work that draws upon the writer's provincial upbringing. Essentially concerned with the emotional battle for Lawrence's love between his mother and \"Miriam\" (in reality Jessie Chambers), the novel also documents Lawrence's (through his protagonist, Paul) brief intimate relationship with Chambers that Lawrence had finally initiated in the Christmas of 1909, ending it in August 1910. The hurt this caused Chambers and, finally, her portrayal in the novel, ended their friendship; after it was published, they never spoke again.\nIn 1911, Lawrence was introduced to Edward Garnett, a publisher's reader, who acted as a mentor and became a valued friend, as did his son David. Throughout these months, the young author revised \"Paul Morel\", the first draft of what became \"Sons and Lovers\". In addition, a teaching colleague, Helen Corke, gave him access to her intimate diaries about an unhappy love affair, which formed the basis of \"The Trespasser\", his second novel. In November 1911, Lawrence came down with a pneumonia again; once recovered, he abandoned teaching in order to become a full-time writer. In February 1912, he broke off an engagement to Louie Burrows, an old friend from his days in Nottingham and Eastwood.\nIn March 1912, Lawrence met Frieda Weekley (n\u00e9e von Richthofen), with whom he was to share the rest of his life. Six years his senior, she was married to Ernest Weekley, his former modern languages professor at University College, Nottingham, and had three young children. However, she and Lawrence eloped and left England for Frieda's parents' home in Metz, a garrison town (then in Germany) near the disputed border with France. Lawrence experienced his first encounter with tensions between Germany and France when he was arrested and accused of being a British spy, before being released following an intervention from Frieda's father. After this incident, Lawrence left for a small hamlet to the south of Munich where he was joined by Frieda for their \"honeymoon\", later memorialised in the series of love poems titled \"Look! We Have Come Through\" (1917).\nIn 1912, Lawrence wrote the first of his so-called \"mining plays\", \"The Daughter-in-Law\", written in Nottingham dialect. The play was not performed or even published in Lawrence's lifetime.\nFrom Germany, they walked southwards across the Alps to Italy, a journey that was recorded in the first of his travel books, a collection of linked essays titled \"Twilight in Italy\" and the unfinished novel, \"Mr Noon\".\nDuring his stay in Italy, Lawrence completed the final version of \"Sons and Lovers\". Having become tired of the manuscript, he allowed Edward Garnett to cut roughly 100 pages from the text. The novel was published in 1913 and hailed as a vivid portrait of the realities of working class provincial life.\nLawrence and Frieda returned to Britain in 1913 for a short visit, during which they encountered and befriended critic John Middleton Murry and New Zealand-born short story writer Katherine Mansfield.\nAlso during that year, on 28 July, Lawrence met the Welsh tramp poet W. H. Davies, whose nature poetry he initially admired. Davies collected autographs, and was keen to have Lawrence's. Georgian poetry publisher Edward Marsh secured this for Davies, probably as part of a signed poem, and also arranged a meeting between the poet and Lawrence and his wife. Despite his early enthusiasm for Davies' work, Lawrence's view cooled after reading \"Foliage\"; whilst in Italy, he also disparaged \"Nature Poems\", calling them \"so thin, one can hardly feel them\".\nAfter the couple returned to Italy, staying in a cottage in Fiascherino on the Gulf of Spezia Lawrence wrote the first draft of what would later be transformed into two of his best-known novels, \"The Rainbow\" and \"Women in Love\", in which unconventional female characters take centre stage. Both novels were highly controversial and were banned on publication in the UK for obscenity, although \"Women in Love\" was banned only temporarily. \n\"The Rainbow\" follows three generations of a Nottinghamshire farming family from the pre-industrial to the industrial age, focusing particularly on a daughter, Ursula, and her aspiration for a more fulfilling life than that of becoming a housebound wife. \"Women in Love\" delves into the complex relationships between four major characters, including Ursula of \"The Rainbow\" and her sister Gudrun. Both novels explore grand themes and ideas that challenged conventional thought on the arts, politics, economic growth, gender, sexual experience, friendship, and marriage. Lawrence's views as expressed in the novels are now thought to be far ahead of his time. The frank and relatively straightforward manner in which he wrote about sexual attraction was ostensibly why the books were initially banned, in particular the mention of same-sex attraction; Ursula has an affair with a woman in \"The Rainbow\", and there is an undercurrent of attraction between the two principal male characters in \"Women in Love\".\nWhile working on \"Women in Love\" in Cornwall during 1916\u201317, Lawrence developed a strong relationship with a Cornish farmer named William Henry Hocking, which some scholars believe was possibly romantic, especially considering Lawrence's fascination with the theme of homosexuality in \"Women in Love\". Although Lawrence never made it clear whether their relationship was sexual, Frieda believed it was. In a 1913 letter, he writes, \"I should like to know why nearly every man that approaches greatness tends to homosexuality, whether he admits it or not...\" He is also quoted as saying, \"I believe the nearest I've come to perfect love was with a young coal-miner when I was about 16.\" However, given his enduring and robust relationship with Frieda, it is likely that he was primarily what might be termed today bi-curious, and whether he actually ever had homosexual relations remains an open question.\nFrieda obtained her divorce from Ernest Weekley. Lawrence and Frieda returned to Britain shortly before the outbreak of World War I and were married on 13 July 1914. During this time, Lawrence worked with London intellectuals and writers such as Dora Marsden, T. S. Eliot, Ezra Pound, and others connected with \"The Egoist\", an important Modernist literary magazine that published some of his work. Lawrence also worked on adapting Filippo Tommaso Marinetti's \"Manifesto of Futurism\" into English. He also met the young Jewish artist Mark Gertler, with whom he became good friends for a time; Lawrence would later express his admiration for Gertler's 1916 anti-war painting, \"Merry-Go-Round\" as \"the best \"modern\" picture I have seen ... it is great and true.\" Gertler would inspire the character Loerke (a sculptor) in \"Women in Love\".\nFrieda's German parentage and Lawrence's open contempt for militarism caused them to be viewed with suspicion and live in near-destitution in wartime Britain; this may have contributed to \"The Rainbow\" being suppressed and investigated in 1915 for its alleged obscenity. Later, the couple were accused of spying and signalling to German submarines off the coast of Cornwall, where they lived at Zennor. During this period, Lawrence finished his final draft of \"Women in Love\". Not published until 1920, it is now widely recognised as a novel of great dramatic force and intellectual subtlety.\nIn late 1917, after constant harassment by the armed forces and other authorities, Lawrence was forced to leave Cornwall on three days' notice under the terms of the Defence of the Realm Act. He described this persecution in an autobiographical chapter of his novel \"Kangaroo\" (1923). Lawrence spent a few months of early 1918 in the small, rural village of Hermitage near Newbury, Berkshire. Subsequently, he lived for just under a year (mid-1918 to early 1919) at Mountain Cottage, Middleton-by-Wirksworth, Derbyshire, where he wrote one of his most poetic short stories, \"Wintry Peacock\". Until 1919, poverty compelled him to shift from address to address.\nDuring the 1918 influenza pandemic, he barely survived a severe attack of influenza.\nExile.\nAfter the wartime years, Lawrence began what he termed his \"savage pilgrimage\", a time of voluntary exile from his native country. He escaped from Britain at the earliest practical opportunity and returned only twice for brief visits, spending the remainder of his life travelling with Frieda. This wanderlust took him to Australia, Italy, Ceylon (Sri Lanka), the United States, Mexico and the south of France. Abandoning Britain in November 1919, they headed south, first to the Abruzzo region in central Italy and then onwards to Capri and the Fontana Vecchia in Taormina, Sicily. From Sicily they made brief excursions to Sardinia, Monte Cassino, Malta, Northern Italy, Austria and Southern Germany.\nMany of these places appear in Lawrence's writings, including \"The Lost Girl\" (for which he won the James Tait Black Memorial Prize for fiction), \"Aaron's Rod\" and the fragment titled \"Mr Noon\" (the first part of which was published in the Phoenix anthology of his works, and the entirety in 1984). He wrote novellas such as \"The Captain's Doll\", \"The Fox\" and \"The Ladybird\". In addition, some of his short stories were issued in the collection \"England, My England and Other Stories\". During these years Lawrence also wrote poems about the natural world in \"Birds, Beasts and Flowers\".\nLawrence is often considered one of the finest travel writers in English. His travel books include \"Twilight in Italy\", \"Etruscan Places\", \"Mornings in Mexico\", and \"Sea and Sardinia\", which describes a brief journey he undertook in January 1921 and focuses on the life of Sardinia's people. Less well known is his eighty-four page introduction to Maurice Magnus's 1924 \"Memoirs of the Foreign Legion\", in which Lawrence recalls his visit to the monastery of Monte Cassino. Lawrence told his friend Catherine Carswell that his introduction to Magnus's \"Memoirs\" was \"the best single piece of writing, as \"writing\", that he had ever done\".\nHis other nonfiction books include two responses to Freudian psychoanalysis, \"Psychoanalysis and the Unconscious\" and \"Fantasia of the Unconscious\"; \"Apocalypse and Other Writings on Revelation\"; and \"Movements in European History\", a school textbook published under a pseudonym, because of Lawrence's blighted reputation in Britain.\nLater life and career.\nIn late February 1922, the Lawrences left Europe intending to migrate to the United States. They sailed in an easterly direction, however, first to Ceylon and then on to Australia. During a short residence in Darlington, Western Australia, Lawrence met local writer Mollie Skinner, with whom he coauthored the novel \"The Boy in the Bush\". This stay was followed by a brief stop in the small coastal town of Thirroul, New South Wales, during which Lawrence completed \"Kangaroo\", a novel about local fringe politics that also explored his wartime experiences in Cornwall.\nThe Lawrences finally arrived in the United States in September 1922. Lawrence had several times discussed the idea of setting up a utopian community with several of his friends, having written in 1915 to Willie Hopkin, his old socialist friend from Eastwood: I want to gather together about twenty souls and sail away from this world of war and squalor and found a little colony where there shall be no money but a sort of communism as far as necessaries of life go, and some real decency \u2026 a place where one can live simply, apart from this civilisation \u2026 [with] a few other people who are also at peace and happy and live, and understand and be free.\u2026It was with this in mind that they made for Taos, New Mexico, a Pueblo town where many white \"bohemians\" had settled, including Mabel Dodge Luhan, a prominent socialite. Here they eventually acquired the 160-acre (0.65\u00a0km2) Kiowa Ranch, now called the D. H. Lawrence Ranch, in 1924 from Dodge Luhan in exchange for the manuscript of \"Sons and Lovers\". The couple stayed in New Mexico for two years, with extended visits to Lake Chapala and Oaxaca in Mexico. While Lawrence was in New Mexico, he was visited by Aldous Huxley.\nEditor and book designer Merle Armitage wrote a book about D. H. Lawrence in New Mexico. \"Taos Quartet in Three Movements\" was originally to appear in Flair Magazine, but the magazine folded before its publication. This short work describes the tumultuous relationship of D. H. Lawrence, his wife Frieda, artist Dorothy Brett, and Mabel Dodge Sterne Luhan. Armitage took it upon himself to print 16 hardcover copies of this work for his friends. Richard Pousette-Dart executed the drawings for \"Taos Quartet\", published in 1950.\nWhile in the US, Lawrence rewrote and published \"Studies in Classic American Literature\", a set of critical essays begun in 1917 and described by Edmund Wilson as \"one of the few first-rate books that have ever been written on the subject\". These interpretations, with their insights into symbolism, New England Transcendentalism and the Puritan sensibility, were a significant factor in the revival of the reputation of Herman Melville during the early 1920s. In addition, Lawrence completed new fictional works, including \"The Boy in the Bush\", \"The Plumed Serpent\", \"St Mawr\", \"The Woman who Rode Away\", \"The Princess\" and other short stories. He also produced the collection of linked travel essays that became \"Mornings in Mexico\".\nA brief voyage to England at the end of 1923 was a failure and Lawrence soon returned to Taos, convinced his life as an author now lay in the United States. However, in March 1925 he suffered a near fatal attack of malaria and tuberculosis while on a third visit to Mexico. Although he eventually recovered, the diagnosis of his condition obliged him to return once again to Europe. He was dangerously ill and poor health limited his ability to travel for the remainder of his life. The Lawrences made their home in a villa in Northern Italy near Florence, where he wrote \"The Virgin and the Gipsy\" and the various versions of \"Lady Chatterley's Lover\" (1928). The latter book, his last major novel, was initially published in private editions in Florence and Paris and reinforced his notoriety. A story set once more in Nottinghamshire about a cross-class relationship between a Lady and her gamekeeper, it broke new ground in describing their sexual relationship in explicit yet literary language. Lawrence hoped to challenge the British taboos around sex: to enable men and women \"to think sex, fully, completely, honestly, and cleanly.\" Lawrence responded robustly to those who took offence, even publishing satirical poems (\"Pansies\" and \"Nettles\") as well as a tract on \"Pornography and Obscenity\".\nThe return to Italy allowed him to renew old friendships; during these years he was particularly close to Aldous Huxley, who was to edit the first collection of Lawrence's letters after his death, along with a memoir. After Lawrence visited local archaeological sites (particularly old tombs) with artist Earl Brewster in April 1927, his collected essays inspired by the excursions were published as \"Sketches of Etruscan Places\", a book that contrasts the lively past with Benito Mussolini's fascism.\nLawrence continued to produce short stories and other works of fiction such as \"The Escaped Cock\" (also published as \"The Man Who Died\"), an unorthodox reworking of the story of Jesus Christ's Resurrection.\nDuring his final years, Lawrence renewed his serious interest in oil painting. Official harassment persisted; an exhibition of his paintings at the Warren Gallery in London was raided by the police in mid-1929, and several works were confiscated.\nDeath.\nLawrence continued to write despite his failing health. In his last months he wrote numerous poems, reviews, and essays, as well as a robust defence of his last novel against those who sought to suppress it. His last significant works were \"Apocalypse\", a reflection on the Book of Revelation, and \"Are Men of Today a Success?\", a posthumous contribution on the feminisation of modern society. \nAfter being discharged from a sanatorium, Lawrence died on 2 March 1930 at the Villa Robermond in Vence, France, from complications of tuberculosis. Frieda commissioned an elaborate headstone for his grave bearing a mosaic of his adopted emblem of the phoenix. After Lawrence's death, Frieda lived with the couple's friend Angelo Ravagli on a ranch in the mountains of Taos, New Mexico and eventually married him in 1950. In 1935, Ravagli arranged, on Frieda's behalf, to have Lawrence's body exhumed and cremated. However, upon boarding the ship he learned he would have to pay taxes on the ashes, so he instead scattered them in the Mediterranean, a preferable resting place, in his opinion, to a concrete block in a chapel. Dust and earth were interred in a small chapel on the Taos ranch, where they remain.\nWritten works.\nNovels.\nLawrence is best known for his novels \"Sons and Lovers\", \"The Rainbow\", \"Women in Love\" and \"Lady Chatterley's Lover\". In these books, Lawrence explores the possibilities for life within an industrial setting, particularly the nature of relationships that can be had within such a setting. Though often classed as a realist, Lawrence in fact uses his characters to give form to his personal philosophy. His depiction of sexuality, seen as shocking when his work was first published in the early 20th century, has its roots in this highly personal way of thinking and being.\nLawrence was very interested in the sense of touch, and his focus on physical intimacy has its roots in a desire to restore an emphasis on the body and rebalance it with what he perceived to be Western civilisation's overemphasis on the mind; in a 1929 essay, \"Men Must Work and Women As Well\", he wrote:\"Now then we see the trend of our civilization, in terms of human feeling and human relation. It is, and there is no denying it, towards a greater and greater abstraction from the physical, towards a further and further physical separateness between men and women, and between individual and individual... It only remains for some men and women, individuals, to try to get back their bodies and preserve the other flow of warmth, affection and physical unison. There is nothing else to do.\" \"Phoenix II: Uncollected, Unpublished, and Other Prose Works by D.H. Lawrence\", ed. Warren Roberts and Harry T. Moore (New York: The Viking Press, 1968), pp. 589, 591.In his later years, Lawrence developed the potentialities of the short novel form in \"St Mawr\", \"The Virgin and the Gypsy\" and \"The Escaped Cock\".\nShort stories.\nLawrence's best-known short stories include \"The Captain's Doll\", \"The Fox\", \"The Ladybird\", \"Odour of Chrysanthemums\", \"The Princess\", \"The Rocking-Horse Winner\", \"St Mawr\", \"The Virgin and the Gypsy\" and \"The Woman who Rode Away\". (\"The Virgin and the Gypsy\" was published as a novella after he died.) Among his most praised collections is \"The Prussian Officer and Other Stories\", published in 1914. His collection \"The Woman Who Rode Away and Other Stories\", published in 1928, develops the theme of leadership that Lawrence also explored in novels such as \"Kangaroo\" and \"The Plumed Serpent\" and the story \"Fanny and Annie\".\nPoetry.\nLawrence wrote almost 800 poems, most of them relatively short. His first poems were written in 1904 and two of his poems, \"Dreams Old\" and \"Dreams Nascent\", were among his earliest published works in \"The English Review\". It has been claimed that his early works clearly place him in the school of Georgian poets, and indeed some of his poems appear in the \"Georgian Poetry\" anthologies. However, James Reeves in his book on Georgian Poetry, notes that Lawrence was never really a Georgian poet. Indeed, later critics contrast Lawrence's energy and dynamism with the complacency of Georgian poetry.\nJust as the First World War dramatically changed the work of many of the poets who saw service in the trenches, Lawrence's own work dramatically changed, during his years in Cornwall. During this time, he wrote free verse influenced by Walt Whitman. He set forth his manifesto for much of his later verse in the introduction to \"New Poems\". \"We can get rid of the stereotyped movements and the old hackneyed associations of sound or sense. We can break down those artificial conduits and canals through which we do so love to force our utterance. We can break the stiff neck of habit [\u2026] But we cannot positively prescribe any motion, any rhythm.\"\nLawrence rewrote some of his early poems when they were collected in 1928. This was in part to fictionalise them, but also to remove some of the artifice of his first works. As he put it himself: \"A young man is afraid of his demon and puts his hand over the demon's mouth sometimes and speaks for him.\" His best-known poems are probably those dealing with nature such as those in the collection \"Birds, Beasts and Flowers\", including the Tortoise poems, and \"Snake\", one of his most frequently anthologised, displays some of his most frequent concerns: those of man's modern distance from nature and subtle hints at religious themes.&lt;poem&gt;\nIn the deep, strange-scented shade of the great dark carob tree\nI came down the steps with my pitcher\nAnd must wait, must stand and wait, for there he was at the trough before me.\n&lt;/poem&gt;\n\"Look! We have come through!\" is his other work from the period of the end of the war and it reveals another important element common to much of his writings; his inclination to lay himself bare in his writings. Ezra Pound in his \"Literary Essays\" complained of Lawrence's interest in his own \"disagreeable sensations\" but praised him for his \"low-life narrative\". This is a reference to Lawrence's dialect poems akin to the Scots poems of Robert Burns, in which he reproduced the language and concerns of the people of Nottinghamshire from his youth.\n&lt;poem&gt;\nTha thought tha wanted ter be rid o' me.\n'Appen tha did, an' a'.\nTha thought tha wanted ter marry an' se\nIf ter couldna be master an' th' woman's boss,\nTha'd need a woman different from me,\nAn' tha knowed it; ay, yet tha comes across\nTer say goodbye! an' a'.\n&lt;/poem&gt;\nAlthough Lawrence's works after his Georgian period are clearly in the modernist tradition, they were often very different from those of many other modernist writers, such as Pound. Pound's poems were often austere, with every word carefully worked on. Lawrence felt all poems had to be personal sentiments, and that a sense of spontaneity was vital. He called one collection of poems \"Pansies\", partly for the simple ephemeral nature of the verse, but also as a pun on the French word \"panser\", to dress or bandage a wound. \"Pansies\", as he made explicit in the introduction to \"New Poems\", is also a pun on Blaise Pascal's \"Pens\u00e9es\". \"The Noble Englishman\" and \"Don't Look at Me\" were removed from the official edition of \"Pansies\" on the grounds of obscenity, which wounded him. Even though he lived most of the last ten years of his life abroad, his thoughts were often still on England. Published in 1930, just eleven days after his death, his last work \"Nettles\" was a series of bitter, nettling but often wry attacks on the moral climate of England.\n&lt;poem&gt;\nO the stale old dogs who pretend to guard\nthe morals of the masses,\nhow smelly they make the great back-yard\nwetting after everyone that passes.\n&lt;/poem&gt;\nTwo notebooks of Lawrence's unprinted verse were posthumously published as \"Last Poems\" and \"More Pansies\". These contain two of Lawrence's most famous poems about death, \"Bavarian Gentians\" and \"The Ship of Death\".\nLiterary criticism.\nLawrence's criticism of other authors often provides insight into his own thinking and writing. Of particular note is his \"Study of Thomas Hardy and Other Essays\". In \"Studies in Classic American Literature\" Lawrence's responses to writers like Walt Whitman, Herman Melville and Edgar Allan Poe also shed light on his craft.\nPlays.\nLawrence wrote \"A Collier's Friday Night\" about 1906\u20131909, though it was not published until 1939 and not performed until 1965. He wrote \"The Daughter-in-Law\" in 1913, though it was not staged until 1967, when it was well received. In 1911 he wrote \"The Widowing of Mrs. Holroyd\", which he revised in 1914; it was staged in the US in 1916 and in the UK in 1920, in an amateur production. It was filmed in 1976; an adaptation was shown on television (BBC 2) in 1995. He also wrote \"Touch and Go\" towards the end of World War I, and his last play, \"David\", in 1925.\nPainting.\nD. H. Lawrence had a lifelong interest in painting, which became one of his main forms of expression in his last years. His paintings were exhibited at the Warren Gallery in London's Mayfair in 1929. The exhibition was extremely controversial, with many of the 13,000 people visiting mainly to gawk. The \"Daily Express\" claimed, \"\"Fight with an Amazon\" represents a hideous, bearded man holding a fair-haired woman in his lascivious grip while wolves with dripping jaws look on expectantly, [this] is frankly indecent\". However, several artists and art experts praised the paintings. Gwen John, reviewing the exhibition in \"Everyman\", spoke of Lawrence's \"stupendous gift of self-expression\" and singled out \"The Finding of Moses\", \"Red Willow Trees\" and \"Boccaccio Story\" as \"pictures of real beauty and great vitality\". Others singled out \"Contadini\" for special praise. After a complaint, the police seized thirteen of the twenty-five paintings, including \"Boccaccio Story\" and \"Contadini\". Despite declarations of support from many writers, artists, and members of Parliament, Lawrence was able to recover his paintings only by agreeing never to exhibit them in England again. Years after his death, his widow Frieda asked artist and friend Joseph Glasco to arrange an exhibition of Lawrence's paintings, which he discussed with his gallerist Catherine Viviano. The largest collection of the paintings is now at La Fonda de Taos hotel in Taos, New Mexico. Several others, including \"Boccaccio Story\" and \"Resurrection\", are at the Humanities Research Centre of the University of Texas at Austin.\n\"Lady Chatterley\" trial.\nA heavily censored abridgement of \"Lady Chatterley's Lover\" was published in the United States by Alfred A. Knopf in 1928. This edition was posthumously reissued in paperback in the United States by both Signet Books and Penguin Books in 1946. The first unexpurgated edition of \"Lady Chatterley's Lover\" was printed in July 1928 in Florence by a small publisher, Giuseppe Orioli: 1000 copies in a very good print, according D. H. Lawrence, who wrote a thank-you poem to Orioli. When the unexpurgated edition of \"Lady Chatterley's Lover\" was published by Penguin Books in Britain in 1960, the trial of Penguin under the Obscene Publications Act of 1959 became a major public event and a test of the new obscenity law. The 1959 act (introduced by Roy Jenkins) had made it possible for publishers to escape conviction if they could show that a work was of literary merit. One of the objections was to the frequent use of the word \"fuck\" and its derivatives and the word \"cunt\".\nVarious academic critics and experts of diverse kinds, including E. M. Forster, Helen Gardner, Richard Hoggart, Raymond Williams and Norman St John-Stevas, were called as witnesses, and the verdict, delivered on 2 November 1960, was \"not guilty\". This resulted in a far greater degree of freedom for publishing explicit material in the UK. The prosecution was ridiculed for being out of touch with changing social norms when the chief prosecutor, Mervyn Griffith-Jones, asked if it were the kind of book \"you would wish your wife or servants to read\".\nThe Penguin second edition, published in 1961, contains a publisher's dedication, which reads: \"For having published this book, Penguin Books were prosecuted under the Obscene Publications Act, 1959 at the Old Bailey in London from 20 October to 2 November 1960. This edition is therefore dedicated to the twelve jurors, three women and nine men, who returned a verdict of 'Not Guilty' and thus made D. H. Lawrence's last novel available for the first time to the public in the United Kingdom.\"\nPhilosophy and politics.\nDespite often writing about political, spiritual and philosophical matters, Lawrence was essentially contrary by nature and hated to be pigeonholed. Critics such as Terry Eagleton have argued that Lawrence was right-wing due to his lukewarm attitude to democracy, which he intimated would tend towards the levelling down of society and the subordination of the individual to the sensibilities of the \"average\" man. In his letters to Bertrand Russell around 1915, Lawrence voiced his opposition to enfranchising the working class and his hostility to the burgeoning labour movements, and disparaged the French Revolution, referring to \"Liberty, Equality, and Fraternity\" as the \"three-fanged serpent\". Rather than a republic, Lawrence called for an absolute dictator and equivalent dictatrix to lord over the lower peoples. In 1953, recalling his relationship with Lawrence in the First World War, Russell characterised Lawrence as a \"proto-German Fascist\", saying \"I was a firm believer in democracy, whereas he had developed the whole philosophy of Fascism before the politicians had thought of it.\" Russell felt Lawrence to be a \"positive force for evil\". However, in 1924 Lawrence wrote an epilogue to \"Movements in European History\" (a textbook he wrote, originally published in 1921) in which he denounced fascism and Soviet-style socialism as bullying and \"a mere worship of Force\". Further, he declared \"I believe a good form of socialism, if it could be brought about, would be the best form of government.\" In the late 1920s, he told his sister he would vote Labour if he was living back in England. In general, though, Lawrence disliked any organised groupings, and in his essay \"Democracy\", written in the late twenties, he argued for a new kind of democracy in which\neach man shall be spontaneously himself\u00a0\u2013 each man himself, each woman herself, without any question of equality or inequality entering in at all; and that no man shall try to determine the being of any other man, or of any other woman.\nLawrence held seemingly contradictory views on feminism. The evidence of his written works, particularly his earlier novels, indicates a commitment to representing women as strong, independent, and complex; he produced major works in which young, self-directing female characters were central. In his youth he supported extending the vote to women, and he once wrote, \"All women in their natures are like giantesses. They will break through everything and go on with their own lives.\" However, some feminist critics, notably Kate Millett, have criticised, indeed ridiculed, Lawrence's sexual politics, Millett claiming that he uses his female characters as mouthpieces to promote his creed of male supremacy and that his story \"The Woman Who Rode Away\" showed Lawrence as a pornographic sadist with its portrayal of \"human sacrifice performed upon the woman to the greater glory and potency of the male.\" Brenda Maddox further highlights this story and two others written around the same time, \"St. Mawr\" and \"The Princess\", as \"masterworks of misogyny\".\nDespite the inconsistency and at times inscrutability of his philosophical writings, Lawrence continues to find an audience, and the publication of a new scholarly edition of his letters and writings has demonstrated the range of his achievement. Philosophers like Gilles Deleuze and F\u00e9lix Guattari found in Lawrence's critique of Sigmund Freud an important precursor of anti-Oedipal accounts of the unconscious that has been much influential.\nPosthumous reputation.\nThe obituaries shortly after Lawrence's death were, with the exception of the one by E. M. Forster, unsympathetic or hostile. However, there were those who articulated a more favourable recognition of the significance of this author's life and works. For example, his long-time friend Catherine Carswell summed up his life in a letter to the periodical \"Time and Tide\" published on 16 March 1930. In response to his critics, she wrote:In the face of formidable initial disadvantages and lifelong delicacy, poverty that lasted for three quarters of his life and hostility that survives his death, he did nothing that he did not really want to do, and all that he most wanted to do he did. He went all over the world, he owned a ranch, he lived in the most beautiful corners of Europe, and met whom he wanted to meet and told them that they were wrong and he was right. He painted and made things, and sang, and rode. He wrote something like three dozen books, of which even the worst page dances with life that could be mistaken for no other man's, while the best are admitted, even by those who hate him, to be unsurpassed. Without vices, with most human virtues, the husband of one wife, scrupulously honest, this estimable citizen yet managed to keep free from the shackles of civilisation and the cant of literary cliques. He would have laughed lightly and cursed venomously in passing at the solemn owls\u2014each one secretly chained by the leg\u2014who now conduct his inquest. To do his work and lead his life in spite of them took some doing, but he did it, and long after they are forgotten, sensitive and innocent people\u2014if any are left\u2014will turn Lawrence's pages and will know from them what sort of a rare man Lawrence was.Aldous Huxley also defended Lawrence in his introduction to a collection of letters published in 1932. However, the most influential advocate of Lawrence's literary reputation was Cambridge literary critic F. R. Leavis, who asserted that the author had made an important contribution to the tradition of English fiction. Leavis stressed that \"The Rainbow\", \"Women in Love\", and the short stories and tales were major works of art. Later, the obscenity trials over the unexpurgated edition of \"Lady Chatterley's Lover\" in America in 1959, and in Britain in 1960, and subsequent publication of the full text, ensured Lawrence's popularity (and notoriety) with a wider public.\nSince 2008, an annual D. H. Lawrence Festival has been organised in Eastwood to celebrate Lawrence's life and works; in September 2016, events were held in Cornwall to celebrate the centenary of Lawrence's connection with Zennor.\nWorks.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "44193", "revid": "50518437", "url": "https://en.wikipedia.org/wiki?curid=44193", "title": "DocBook", "text": "Markup language for documentation\nDocBook is a semantic markup language for technical documentation. It was originally intended for writing technical documents related to computer hardware and software, but it can be used for any other sort of documentation.\nAs a semantic language, DocBook enables its users to create document content in a presentation-neutral form that captures the logical structure of the content; that content can then be published in a variety of formats, including HTML, XHTML, EPUB, PDF, man pages, WebHelp and HTML Help, without requiring users to make any changes to the source. In other words, when a document is written in DocBook format it becomes easily portable into other formats, rather than needing to be rewritten.\nDesign.\nDocBook is an XML language. In its current version (5.x), DocBook's language is formally defined by a RELAX NG schema with integrated Schematron rules. (There are also W3C XML Schema+Schematron and Document Type Definition (DTD) versions of the schema available, but these are considered non-standard.)\nAs a semantic language, DocBook documents do not describe what their contents \"look like\", but rather the meaning of those contents. For example, rather than explaining how the abstract for an article might be visually formatted, DocBook simply says that a particular section \"is\" an abstract. It is up to an external processing tool or application to decide where on a page the abstract should go and what it should look like or whether or not it should be included in the final output at all.\nDocBook provides a vast number of semantic element tags. They are divided into three broad categories, namely structural, block-level, and inline.\n\"Structural\" tags specify broad characteristics of their contents. The codice_1 element, for example, specifies that its child elements represent the parts of a book. This includes a title, chapters, glossaries, appendices, and so on. DocBook's structural tags include, but are not limited to:\nStructural elements can contain other structural elements. Structural elements are the only permitted top-level elements in a DocBook document.\n\"Block-level\" tags are elements like paragraph, lists, etc. Not all these elements can directly contain text. Sequential block-level elements render one \"after\" another. After, in this case, can differ depending on the language. In most Western languages, \"after\" means below: text paragraphs are printed down the page. Other languages' writing systems can have different directionality; for example, in Japanese, paragraphs are often printed in downward columns, with the columns running from right to left, so \"after\" in that case would be to the left. DocBook semantics are entirely neutral to these kinds of language-based concepts.\n\"Inline-level\" tags are elements like emphasis, hyperlinks, etc. They wrap text within a block-level element. These elements do not cause the text to break when rendered in a paragraph format, but typically they cause the document processor to apply some kind of distinct typographical treatment to the enclosed text, by changing the font, size, or similar attributes. (The DocBook specification \"does\" say that it expects different typographical treatment, but it does not offer specific requirements as to what this treatment may be.) That is, a DocBook processor doesn't have to transform an codice_15 tag into \"italics\". A reader-based DocBook processor could increase the size of the words, or, a text-based processor could use bold instead of italics.\nSample document.\n &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n &lt;book xml:id=\"simple_book\" xmlns=\"http://docbook.org/ns/docbook\" version=\"5.0\"&gt;\n &lt;title&gt;Very simple book&lt;/title&gt;\n &lt;chapter xml:id=\"chapter_1\"&gt;\n &lt;title&gt;Chapter 1&lt;/title&gt;\n &lt;para&gt;Hello world!&lt;/para&gt;\n &lt;para&gt;I hope that your day is proceeding &lt;emphasis&gt;splendidly&lt;/emphasis&gt;!&lt;/para&gt;\n &lt;/chapter&gt;\n &lt;chapter xml:id=\"chapter_2\"&gt;\n &lt;title&gt;Chapter 2&lt;/title&gt;\n &lt;para&gt;Hello again, world!&lt;/para&gt;\n &lt;/chapter&gt;\n &lt;/book&gt;\nSemantically, this document is a \"book\", with a \"title\", that contains two \"chapters\" each with their own \"titles\". Those \"chapters\" contain \"paragraphs\" that have text in them. The markup is fairly readable in English.\nIn more detail, the root element of the document is codice_1. All DocBook elements are in an XML Namespace, so the root element has an \"xmlns\" attribute to set the current namespace. Also, the root element of a DocBook document must have a \"version\" that specifies the version of the format that the document is built on.\nA codice_1 element must contain a codice_19, or an codice_20 element containing a codice_19. This must be before any child structural elements. Following the title are the structural children, in this case, two codice_6 elements. Each of these must have a codice_19. They contain codice_24 block elements, which can contain free text and other inline elements like the codice_15 in the second paragraph of the first chapter.\nSchemas and validation.\nRules are formally defined in the DocBook XML schema. Appropriate programming tools can validate an XML document (DocBook or otherwise), against its corresponding schema, to determine if (and where) the document fails to conform to that schema. XML editing tools can also use schema information to avoid creating non-conforming documents in the first place.\nAuthoring and processing.\nBecause DocBook is XML, documents can be created and edited with any text editor. A dedicated XML editor is likewise a functional DocBook editor. DocBook provides schema files for popular XML schema languages, so any XML editor that can provide content completion based on a schema can do so for DocBook. Many graphical or WYSIWYG XML editors come with the ability to edit DocBook like a word processor. \nTables, list items, and other stylized content can be copied and pasted into the DocBook editor and will be preserved in the DocBook XML output. Because DocBook conforms to a well-defined XML schema, documents can be validated and processed using any tool or programming language that includes XML support.\nHistory.\nDocBook began in 1991 in discussion groups on Usenet and eventually became a joint project of HAL Computer Systems and O'Reilly &amp; Associates and eventually spawned its own maintenance organization (the Davenport Group) before moving in 1998 to the \"SGML Open\" consortium, which subsequently became OASIS. DocBook is currently maintained by the \"DocBook Technical Committee\" at OASIS.\nDocBook is available in both SGML and XML forms, as a DTD. RELAX NG and W3C XML Schema forms of the XML version are available. Starting with DocBook 5, the RELAX NG version is the \"normative\" form from which the other formats are generated.\nDocBook originally started out as an SGML application, but an equivalent XML application was developed and has now replaced the SGML one for most uses. (Starting with version 4 of the SGML DTD, the XML DTD continued with this version numbering scheme.) Initially, a key group of software companies used DocBook since their representatives were involved in its initial design. Eventually, however, DocBook was adopted by the open source community where it has become a standard for creating documentation for many projects, including FreeBSD, KDE, GNOME desktop documentation, the GTK+ API references, the Linux kernel documentation (which, as of July 2016, is transitioning to Sphinx/reStructuredText), and the work of the Linux Documentation Project.\nPre DocBook v5.0.\nUntil DocBook 5, DocBook was defined normatively by a Document Type Definition (DTD). Because DocBook was built originally as an application of SGML, the DTD was the only available schema language. DocBook 4.x formats can be SGML or XML, but the XML version does not have its own namespace.\nDocBook 4.x formats had to live within the restrictions of being defined by a DTD. The most significant restriction was that an element name uniquely defines its possible contents. That is, an element named codice_20 must contain the same information no matter where it is in the DocBook file. As such, there are many kinds of info elements in DocBook 4.x: codice_27, codice_28, etc. Each has a slightly different content model, but they do share some of their content model. Additionally, they repeat context information. The book's codice_20 element is that, because it is a direct child of the book; it does not need to be named specially for a human reader. However, because the format was defined by a DTD, it did have to be named as such. The root element does not have or need a \"version\", as the version is built into the DTD declaration at the top of a pre-DocBook 5 document.\nDocBook 4.x documents are not compatible with DocBook 5, but can be converted into DocBook 5 documents via an XSLT stylesheet. One (codice_30) is provided as part of the distribution of the DocBook 5 schema and specification package.\nOutput formats.\nDocBook files are used to prepare output files in a wide variety of formats. Nearly always, this is accomplished using DocBook XSL stylesheets. These are XSLT stylesheets that transform DocBook documents into a number of formats (HTML, XSL-FO for later conversion into PDF, etc.). These stylesheets can be sophisticated enough to generate tables of contents, glossaries, and indexes. They can oversee the selection of particular designated portions of a master document to produce different versions of the same document (such as a \"tutorial\" or a \"quick-reference guide\", where each of these consist of a subset of the material). Users can write their own customized stylesheets or even a full-fledged program to process the DocBook into an appropriate output format as their needs dictate.\nNorman Walsh and the DocBook Project development team maintain the key application for producing output from DocBook source documents: A set of XSLT stylesheets (as well as a legacy set of DSSSL stylesheets) that can generate high-quality HTML and print (FO/PDF) output, as well as output in other formats, including RTF, man pages and HTML Help.\nWeb help is a chunked HTML output format in the DocBook XSL stylesheets that was introduced in version 1.76.1. The documentation for web help also provides an example of web help and is part of the DocBook XSL distribution.\nThe major features are its fully CSS-based page layout, search of the help content, and a table of contents in collapsible-tree form. Search has stemming, match highlighting, explicit page-scoring, and the standard multilingual tokenizer. The search and TOC are in a pane that appears as a frameset, but is actually implemented with div tags and cookies (so that it is progressive).\nSimplified DocBook.\nDocBook offers a large number of features that may be overwhelming to a new user. For those who want the convenience of DocBook without a steep learning curve, \"Simplified DocBook\" was designed. It is a small subset of DocBook designed for single documents such as articles or white papers (i.e., \"books\" are not supported). The Simplified DocBook DTD is currently at version 1.1.\nCriticism.\nIngo Schwarze, the author of OpenBSD's mandoc, considers DocBook inferior to the semantic \"mdoc\" macro for man pages. In an attempt to write a DocBook-to-mdoc converter (previous converters like docbook-to-man do not cover semantic elements), he finds the semantic parts \"bloated, redundant, and incomplete at the same time\" compared to elements covered in mdoc. Moreover, Schwarze finds the DocBook specification not specific enough about the use of tags, the language non-portable across versions, rough in details and overall inconsistent.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\nNorman Walsh is the principal author of the book http://, the official documentation of DocBook. This book is available online under the GFDL, and also as a print publication.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "44195", "revid": "313186", "url": "https://en.wikipedia.org/wiki?curid=44195", "title": "Friends of the Earth", "text": "International network of environmental organizations\nFriends of the Earth International (FoEI) is an international network of grassroots environmental organizations in 73 countries. About half of the member groups call themselves \"Friends of the Earth\" in their own languages; the others use other names. The organization was founded in 1969 in San Francisco by David Brower, Donald Aitken, and Gary Soucie after Brower's split with the Sierra Club because of the latter's positive approach to nuclear energy. It became an international network of organizations in 1971 with a meeting of representatives from four countries: U.S., Sweden, the UK and France.\nFoEI currently has a secretariat (based in Amsterdam, Netherlands) which provides support for the network and its agreed major campaigns. The executive committee of elected representatives from national groups sets policy and oversees the work of the secretariat. In 2016, Uruguayan activist Karin Nansen was elected to serve as chair of the organization. Sri Lankan activist Hemantha Withanage has served as chair of FoEI since 2021.\nCampaign issues.\nFriends of the Earth International is an international membership organisation, with members spread across the world. Its advocacy programs focus on environmental, economic and social issues, highlighting their political and human rights contexts.\nAs per its website, the current campaign priorities of Friends of the Earth International are: economic justice and resisting neoliberalism; forests and biodiversity; food sovereignty; and climate justice and energy. The campaign priorities of FOEI are set at its bi-annual general meeting. Additionally, FOEI also plans campaigns in other fields, such as waste and overconsumption, international financial institutions, ecological debt, mining and extractive industries, and opposition to nuclear power. FOEI has campaigned for the closure of the Diablo Canyon nuclear plant in California. FOEI also supports campaigns from the regions or member groups, such as the one on the consumption and intensive production of meat (\"Meat Atlas\") by Friends of the Earth Europe.\nFOEI claims that it has been successful as it has eliminated billions in taxpayer subsidies to corporate polluters, reformed the World Bank to address environmental and human rights concerns, pushed the debate on global warming to pressure the U.S. and U.K. to attempt the best legislation possible, stopped more than 150 destructive dams and water projects worldwide, pressed and won landmark regulations of strip mines and oil tankers and banned international whaling. Its critics claim that the organization tries only to obtain media attention (as by releasing the song \"Love Song to the Earth\"), but does not stay with locals to actually solve complicated problems, and that it prevents development in developing countries. They have also been critical of its policy to accept high levels of funding from companies and charities related to oil and gas.\nOne of Friends of the Earth's most recent campaigns and legal battles was the \"Shell Case\", led by Milieudefensie (Friends of the Earth Netherlands). In 2021, a court in the Netherlands ruled in a landmark case that the oil giant Shell must reduce its emissions in 2030 by 45% compared to 2019 levels. This was the first time that a company had been legally obliged to align its policies with the Paris Agreement. This was later overturned in November 2024.\nIn January 2025 when UK Prime Minister Keir Starmer announced plans to take on NIMBYs who block major infrastructure projects, such as nuclear power, roads, railway and wind farms, Friends of the Earth criticized Starmer, saying he was scapegoating people with \"valid concerns about a project's impact\".\nStructure of the network.\nThe member organization in a particular country may name itself Friends of the Earth or an equivalent translated phrase in the national language, e.g., Friends of the Earth (US), Friends of the Earth (EWNI) (England Wales and Northern Ireland), Amigos de la Tierra (Spain and Argentina). However, roughly half of the member groups work under their own names, sometimes reflecting an independent origin and subsequent accession to the network, such as Pro Natura (Switzerland), the Korean Federation for Environmental Movement, Environmental Rights Action (FOE Nigeria) and WALHI (FOE Indonesia).\nFriends of the Earth International (FoEI) is supported by a secretariat based in Amsterdam, and an executive committee known as ExCom. The ExCom is elected by all member groups at a general meeting held every two years, and it is the ExCom which employs the secretariat. At the same general meeting, overall policies and priority activities are agreed.\nIn addition to work which is coordinated at the FoEI level, national member groups are free to carry out their own campaigns and to work bi- or multi-laterally as they see fit, as long as this does not go against agreed policy at the international level.\nPublications.\nThe \"Meat Atlas\" is an annual report on the methods and impact of industrial animal agriculture. The publication consists of 27 short essays and, with the help of graphs, visualises facts about the production and consumption of meat. The \"Meat Atlas\" is jointly published by Friends of the Earth and Heinrich B\u00f6ll Foundation.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44196", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=44196", "title": "DSSSL", "text": ""}
{"id": "44198", "revid": "6326132", "url": "https://en.wikipedia.org/wiki?curid=44198", "title": "Mexican War", "text": "Mexican War may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "44199", "revid": "15881234", "url": "https://en.wikipedia.org/wiki?curid=44199", "title": "Document Style Semantics and Specification Language", "text": "Computer language for specifying stylesheets for SGML documents\nThe Document Style Semantics and Specification Language (DSSSL) is an international standard developed to provide stylesheets for SGML documents.\nDSSSL consists of two parts: a tree transformation process that can be used to manipulate the tree structure of documents prior to presentation, and a formatting process that associates the elements in the source document with specific nodes in the target representation\u2014the flow object tree. DSSSL specifications are device-independent pieces of information that can be interchanged between different platforms. DSSSL does not standardize the back-end formatters that generate the language's output. Such formatters may render the output for on-screen display, or write it to a computer file in a specific format (such as PostScript or Rich Text Format).\nBased on a subset of the Scheme programming language, it is specified by the standard ISO/IEC 10179:1996. It was developed by ISO/IEC JTC 1/SC 34 (ISO/IEC Joint Technical Committee 1, Subcommittee 34 - Document description and processing languages).\nSGML contains information in a machine-readable but not very human-readable format. A \"stylesheet\" is used to present the information stored in SGML in a more pleasing or accessible way. DSSSL can convert to a wide range of formats, including RTF, HTML, and LaTeX.\nDSSSL is compatible with any SGML-based document type, but it has been used most often with DocBook. In 1997, software engineer Geir Ove Gr\u00f8nmo published a syntax highlighting language definition for KEDIT.\nWith the appearance of XML as an alternative to SGML, XML's associated stylesheet language XSL was also widely and rapidly adopted, from around 1999. Although DSSSL continued to be in use within the shrinking SGML field, XSL was very soon in use more extensively, and by more coders, than DSSSL had ever achieved. This was emphasised when previous SGML strongholds such as DocBook converted from SGML to XML, and also converted their favoured stylesheet language from DSSSL to XSL.\nSometime in or before 1994, James Clark (programmer) began drafting a \"DSSSL Lite\" specification for the consideration of the World Wide Web Consortium, since DSSSL was thought to be too complex for the World Wide Web.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "44202", "revid": "39540191", "url": "https://en.wikipedia.org/wiki?curid=44202", "title": "Statecraft", "text": "Art of conducting public affairs\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n&lt;br&gt;Statecraft (st\"\u0113\"itkr\u0251ft). [f. \"state\" \"sb\". + \"craft\".]&lt;br&gt;The art of conducting state affairs; statesmanship. Sometimes with sinister implication: Crafty or overreaching statesmanship.\n Oxford English Dictionary, first edition\nStatecraft (also statesmanship) is the art of conducting public affairs. A statesman, stateswoman, or statesperson is someone who practices statecraft. As a contested concept, statecraft is difficult to define.\nDefinition and conceptions.\nThe word \"statecraft\", dating from the 1640s, refers to the art of conducting public affairs, which entails leading a state or country. Statecraft is thus said to be the practice of a statesman (derived from the Dutch ), stateswoman, or statesperson. \"Statecraft\" is a synonym of \"statesmanship\", but in a narrow form may also be synonymous with public diplomacy. Beyond a superficial level, however, finding an exact definition of statecraft is difficult and it is a contested concept which political scientist Wilfred M. McClay calls \"always a tricky, elusive matter\u2014hard to come by, hard to measure, and hard even to define or describe.\"\nThe Ancient Greek philosopher Plato conceived of the \"statesman\" in his dialogue of the same name as one who oversees and guides the work of many others:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There is an art which controls all these arts. It is concerned with the laws and with all that belongs to the life of the community. It weaves all into its unified fabric with perfect skill. It is a universal art and so we call it by a name of universal scope. That name is one which I believe to belong to this art and to this alone, the name of 'Statesmanship'.\nPlato's conception of statesmanship greatly influenced Sir Thomas More, who considered the \"statesman\" to be a virtuous leader armed with both the science and art of ruling. He sought to personally cultivate what he conceived as the three elements of statesmanship, those being its science, art, and presupposed personal virtue.\nAccording to Andrew Brady Spalding, the word \"statecraft\" may allow a narrow and a broad understanding. The narrow conception can be defined as \"managing relations between states to the advantage of one's own country\", a traditional usage dating back to Niccol\u00f2 Machiavelli. Otherwise, the term can be used broadly, as Colin Talbot puts it, \"for the study of states and governments and how to successfully build, run and adapt them, internally and externally.\"\nElder statesman.\nAn \"elder statesman\" is a retired politician whose influence extends beyond their official term, permitting them to unofficially advise incumbent politicians. The term originally referred to a member of the \"genr\u014d\", retired statesmen who were consulted by the emperor of Japan.\nAlthough the activities and influence of elder statesmen remains understudied, Taro Tsuda argues that the modern increase of life expectancy has likewise increased the influence of elder statesmen, which alongside the significance of informal politics and institutions compels the study of such statesmen.\nFor example, Nelson Mandela was often described as the \"world's elder statesman\" due to his political influence beyond his presidency of South Africa as a key figure of the globalized anti-Apartheid movement. Mandela founded The Elders in 2009, an association of elder statespeople with the goal of combining their collective experience to address problems of peace and human rights, who counted among their members Jimmy Carter and Kofi Annan.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
