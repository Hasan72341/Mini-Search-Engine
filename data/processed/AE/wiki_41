{"id": "49304", "revid": "50532483", "url": "https://en.wikipedia.org/wiki?curid=49304", "title": "Henry Morton Stanley", "text": "Welsh journalist and explorer (1841\u20131904)\nSir Henry Morton Stanley (born John Rowlands; 28 January 1841 \u2013 10 May 1904) was a Welsh-American explorer, journalist, soldier, colonial administrator, author, and politician famous for his exploration of Central Africa and search for missionary and explorer David Livingstone. Besides his discovery of Livingstone, he is mainly known for his search for the sources of the Nile and Congo rivers, the work he undertook as an agent of King Leopold II of the Belgians that enabled the occupation of the Congo Basin region, and his command of the Emin Pasha Relief Expedition. He was knighted in 1897, and served in Parliament as a Liberal Unionist member for Lambeth North from 1895 to 1900.\nMore than a century after his death, Stanley's legacy remains the subject of enduring controversy. Although he personally had high regard for many of the native African people who accompanied him on his expeditions, the exaggerated accounts of corporal punishment and brutality in his books fostered a public reputation as a hard-driving, cruel leader, in contrast to the supposedly more humanitarian Livingstone. His contemporary image in Britain also suffered from the perception that he was American. In the 20th century, his reputation was also seriously damaged by his role in establishing the Congo Free State for King Leopold II. Nevertheless, he is recognised for his important contributions to Western knowledge of the geography of Central Africa and for his resolute opposition to the slave trade in East Africa.\nEarly life.\nHenry Stanley was born as John Rowlands in Denbigh, Denbighshire, Wales. His mother Elizabeth Parry was 18 years old at the time of his birth. She abandoned him as a very young baby and cut off all communication. Stanley never knew his father, who died within a few weeks of his birth. There is some doubt as to his true parentage. As his parents were unmarried, his birth certificate describes him as a bastard; he was baptised in the parish of Denbigh on 19 February 1841, the register recording that he had been born on 28 January of that year. The entry states that he was the bastard son of John Rowland of Llys Llanrhaidr and Elizabeth Parry of Castle. The stigma associated with illegitimacy weighed heavily upon him all his life.\nThe boy was given his father's surname of Rowlands and brought up by his grandfather Moses Parry, a once-prosperous butcher who was living in reduced circumstances. He died when John was five. Rowlands then stayed with families of cousins and nieces for a short time, but he was eventually sent to the St Asaph Union Workhouse for the Poor. The overcrowding and lack of supervision resulted in his being frequently abused by older boys. Historian Robert Aldrich has alleged that the headmaster of the workhouse raped or sexually assaulted Rowlands, and that the older Rowlands was \"incontrovertibly bisexual\". When Rowlands was 10 years old, his mother and two half-siblings stayed for a short while in this workhouse, but he did not recognise them until the headmaster told him who they were.\nLife in the United States.\nRowlands emigrated to the United States in 1859 at age 18. He disembarked at New Orleans and, according to his own declarations, became friends by accident with Henry Hope Stanley, a wealthy trader. He saw Stanley sitting on a chair outside his store and asked him if he had any job openings. He did so in the British style: \"Do you need a boy, sir?\" The childless man had indeed been wishing he had a son, and the inquiry led to a job and a close relationship between them. Out of admiration, John took Stanley's name. Later, he wrote that his adoptive parent died two years after their meeting, but in fact the elder Stanley did not die until 1878. This and other discrepancies led John Bierman to argue that no adoption took place. Tim Jeal goes further, and, in his biography, subjects Stanley's account in his posthumously published \"Autobiography\" to detailed analysis. Because Stanley got so many basic facts wrong about his purported adoptive family, Jeal concludes that it is very unlikely that he ever met rich Henry Hope Stanley, and that an ordinary grocer, James Speake, was Rowlands' true benefactor until his (Speake's) sudden death in October 1859.\nStanley reluctantly joined in the American Civil War, first enrolling in the Confederate States Army's 6th Arkansas Infantry Regiment and fighting in the Battle of Shiloh in 1862. After being taken prisoner there, he was recruited at Camp Douglas, Illinois, by its commander Colonel James A. Mulligan as a \"Galvanized Yankee.\" He joined the Union Army on 4 June 1862 but was discharged 18 days later because of severe illness. After recovering, he served on several merchant ships before joining the US Navy in July 1864. He became a record keeper on board the , and participated in the First Battle of Fort Fisher and the Second Battle of Fort Fisher, which led him into freelance journalism. Stanley and a junior colleague jumped ship on 10 February 1865 in Portsmouth, New Hampshire, in search of greater adventures. Stanley was possibly the only man to serve in the Confederate Army, the Union Army, and the Union Navy.\nBritish expedition to Abyssinia (1867\u20131868).\nFollowing the American Civil War, Stanley became a journalist in the days of frontier expansion in the American West. He then organised an expedition to the Ottoman Empire that ended catastrophically when he was imprisoned. He eventually talked his way out of jail and received restitution for damaged expedition equipment.\nIn 1867, the emperor of Ethiopia, Tewodros II, held a British envoy and others hostage, and a force was sent to effect the release of the hostages. Stanley accompanied that force as a special correspondent of the \"New York Herald\". His report on the Battle of Magdala in 1868 was the first to be published, as he had bribed a telegraph operator to send his story first, even before the official army report. After his message was sent, the cable broke; British government officials were greatly irritated to learn of the battle from an American newspaper. Subsequently, he was assigned to report on Spain's Glorious Revolution in 1868. In 1870, Stanley undertook several assignments for the \"Herald\" in the Middle East and the Black Sea region, visiting Egypt, Jerusalem, Constantinople, the Crimea, the Caucasus, Persia and India, during which time he apparently carved his name into a stone of the ancient palace at Persepolis in Persia.\nFinding David Livingstone expedition (1871\u20131872).\nStanley travelled to Zanzibar in March 1871, later claiming that he outfitted an expedition with 192 porters. In his first dispatch to the \"New York Herald\", however, he stated that his expedition numbered only 111. This was in line with figures in his diaries. James Gordon Bennett Jr., publisher of the \"New York Herald\" and funder of the expedition, had delayed sending to Stanley the money he had promised, so Stanley borrowed money from the United States Consul.\nDuring the expedition through the tropical forest, his thoroughbred stallion died within a few days after a bite from a tsetse fly, many of his porters deserted, and the rest were decimated by tropical diseases.\nStanley found David Livingstone on 10 November 1871 in Ujiji, near Lake Tanganyika in present-day Tanzania. He later claimed to have greeted him with the now-famous line, \"Dr. Livingstone, I presume?\" However, this line does not appear in his journal from the time\u2014the two pages directly following the recording of his initial spotting of Livingstone were torn out of the journal at some point\u2014and it is likely that Stanley simply invented the pithy line sometime afterwards. Neither man mentioned it in any of the letters they wrote at this time, and Livingstone tended to instead recount the reaction of his servant, Susi, who cried out: \"An Englishman coming! I see him!\" The phrase is first quoted in a summary of Stanley's letters published by \"The New York Times\" on 2 July 1872. Stanley biographer Tim Jeal argued that the explorer invented it afterwards to help raise his standing because of \"insecurity about his background\", though ironically the phrase was mocked in the press for being absurdly formal for the situation.\nThe \"Herald\"'s own first account of the meeting, published 1 July 1872, reports:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Preserving a calmness of exterior before the Arabs which was hard to simulate as he reached the group, Mr. Stanley said: \u2013 \"Doctor Livingstone, I presume?\" A smile lit up the features of the pale white man as he answered: \"Yes, and I feel thankful that I am here to welcome you.\"\nStanley joined Livingstone in exploring the region, finding that there was no waterway from Lake Tanganyika to the Nile. On his return, he wrote a book about his experiences: \"How I Found Livingstone; travels, adventures, and discoveries in Central Africa\" (1872).\nFirst trans-Africa expedition (1874\u20131877).\nIn 1874, the \"New York Herald\" and the \"Daily Telegraph\" financed Stanley on another expedition to Africa. His ambitious objective was to complete the exploration and mapping of the Central African Great Lakes and rivers, in the process circumnavigating Lakes Victoria and Tanganyika and locating the source of the Nile. Between 1875 and 1876 Stanley succeeded in the first part of his objective, establishing that Lake Victoria had only a single outlet, the one discovered by John Hanning Speke on 21 July 1862 and named Ripon Falls. If this was not the Nile's source, then the separate massive northward flowing river called by Livingstone, the Lualaba, and mapped by him in its upper reaches, might flow on north to connect with the Nile via Lake Albert and thus be the river's primary source.\nIt was therefore essential that Stanley should trace the course of the Lualaba downstream (northward) from Nyangwe, the point where Livingstone had left it in July 1871. Between November 1876 and August 1877, Stanley and his men navigated the Lualaba up to and beyond the point where it turned sharply westward, away from the Nile, identifying itself as the Congo River. Having succeeded with this second objective, they then traced the river to the sea. During this expedition, Stanley used sectional boats and dug-out canoes to pass the large cataracts that separated the Congo into distinct tracts. These boats were transported around the rapids before being reassembled to travel on the next section of river. In passing the rapids many of his men were drowned, including his last white colleague, Frank Pocock. The expedition was repeatedly attacked by natives in canoes, likely by \"Ngobila\", a lord in the Tio Kingdom. Stanley and his men reached the Portuguese outpost of Boma, around from the mouth of the Congo River on the Atlantic Ocean, after 999\u00a0days on 9 August 1877. Muster lists and Stanley's diary (12 November 1874) show that he started with 228 people and reached Boma with 114 survivors, with him the only European left alive out of four. In Stanley's \"Through the Dark Continent\" (1878) (in which he coined the term \"Dark Continent\" for Africa), Stanley said that his expedition had numbered 356, the exaggeration detracting from his achievement.\nStanley attributed his success to his leading African porters, saying that his success was \"all due to the pluck and intrinsic goodness of 20 men\u00a0... take the 20 out and I could not have proceeded beyond a few days' journey\". Professor James Newman has written that \"establishing the connection between the Lualaba and Congo Rivers and locating the source of the Victoria Nile\" justified him (Newman) in stating that: \"In terms of exploration and discovery as defined in nineteenth-century Europe, he (Stanley) clearly stands at the top.\"\nInternational Upper Congo Expedition (1879\u20131884).\nOn 15 April 1877, King Leopold II of the Belgians sent his first expedition to Central Africa, then still under the flag of the International African Association. The members of the expedition, four Belgians, departing from Zanzibar, had the goal of establishing a scientific post in Karema, in today's Tanzania, but even before the group entered Central Africa, two of them had already died, one from a sun stroke, the other from a severe fever, upon which the other members of the expedition resigned. Because of these difficulties, Leopold realised how important it was to find experienced men to lead his expeditions. He first tried to persuade Pierre Savorgnan de Brazza, but he had already entered French service; his eye now fell on Stanley. Stanley had first hoped to continue his pioneering work in Africa under the British flag. But neither the Foreign Office nor Edward, the Prince of Wales, felt called to receive Stanley after the many rumours of his looting and killing in the interior of the African continent. Leopold eagerly received a disenchanted Stanley at his palace in June 1878, and signed a five-year contract with him in November.\nStanley persuaded Leopold that the first step should be the construction of a wagon trail around the Congo rapids and a chain of trading stations on the river. To avoid discovery, materials and workers were shipped in by various roundabout routes, and communications between Stanley and Leopold were entrusted to Colonel Maximilien Strauch.\nStanley as Leopold's agent.\nIn 1879, Stanley left for Africa for his first mission, ostensibly working for the Comit\u00e9 d'\u00e9tudes du Haut-Congo, under Leopold's orders. King Leopold gave Stanley clear instructions: \"It is not about Belgian colonies. It is about establishing a new state that is as large as possible and about its governance. It should be clear that in this project there can be no question of granting the Negroes the slightest form of political power. That would be ridiculous. The whites, who lead the posts, have all the power.\"\nStanley described in writings his dismay with the terrible scenes taking place in Congo. At the same time, his \"findings\" conveyed an idea that the Dark Continent must submit, willingly or otherwise. Stanley's writings show that he, too, held this view. \"Only by proving that we are superior to the savages, not only through our power to kill them but through our entire way of life, can we control them as they are now, in their present stage; it is necessary for their own well-being, even more than ours.\"\nUnexpectedly, France had sent its own expedition to the Congo Basin. Pierre Savorgnan de Brazza had undermined Stanley's mission by concluding contracts himself with native heads of state. The creation of a station that would later be called Brazzaville could not be prevented. Leopold was furious, writing angrily to Strauch: \"The terms of the treaties Stanley has made with native chiefs do not satisfy me. There must at least be an added article to the effect that they delegate to us their sovereign rights\u00a0... the treaties must be as brief as possible and in a couple of articles must grant us everything.\"\nSince everything in Central Africa was about the balance of power between the Great Powers, Leopold considered his next moves and sent an envoy to Berlin to press for a conference. Leopold wanted the International Association of the Congo boundaries drawn by Stanley to be officially confirmed, thus giving the Association an official status.\nOn 26 February 1885, the Berlin Act was signed. The Act regulated an immense free trade zone in the Congo Basin and made it a neutral territory. Furthermore, the Act declared war on slavery. The act contained only one article that Leopold disliked: Article 17 gave the superpowers the right to establish an international commission to supervise the freedom of trade and navigation in Congo. As a result, Leopold would not be able to collect customs duties on the Congo River \nIn 1890, on the 25th anniversary of Leopold's reign as Belgian monarch, Stanley was taken from one banquet hall to another, proclaimed a hero. Leopold honoured him with the Order of Leopold. Together they examined the entire Congolese situation. The key question was how the Free State could become profitable. Stanley pointed out to the monarch, among other things, the potential of rubber production. Stanley wrote: \"You can find it on almost any tree. As we made our way through the forest, it was literally raining rubber juice. Our clothes were full of it. The Congo has so many tributaries that a well-organized company can easily extract a few tons of rubber per year here. You only have to sail up such a river and the branches with rubber hang almost up to your ship.\"\nIn 1891, rubber extraction was divided among concessionaires. This soon led to abuses, when the switch was made to \"forced labour\".\nFounding of Leopoldville (Kinshasa).\nStanley, who had left from a post at Vivi near Matadi on 21 February 1880, arrived at Stanley pool on 3 December 1882. Building a road from Vivi to Isangila, Stanley took almost two years to traverse the rapids towing with him 50 tonnes of equipment, including two dismantled steamboats and a barge. After he arrived at Stanley pool, a local king, Makoko of the Anziku Kingdom, gave him a site near Kintambo to build a city. Despite hostilities from another nearby king, Ngaliema, he decided to start the construction of L\u00e9opoldville on the hillside of Khonzo Izulu. Today Kinshasa's population is 17,000,000, and it is one of the world's fastest growing megacities.\nDealings with Zanzibari slave traders.\nTippu Tip, the most powerful of Zanzibar's slave traders of the 19th century, was well known to Stanley, as was the social chaos and devastation brought by slave-hunting. It had only been through Tippu Tip's help that Stanley had found Livingstone, who had survived years on the Lualaba under Tippu Tip's friendship. Now, Stanley discovered that Tippu Tip's men had reached still further west in search of fresh populations to enslave.\nFour years earlier, the Zanzibaris had thought the Congo deadly and impassable and warned Stanley not to attempt to go there, but when Tippu Tip learned that Stanley had survived, he was quick to act. Villages throughout the region were burned and depopulated. Tippu Tip had raided 118 villages, killed 4,000 Africans, and, when Stanley reached his camp, had 2,300 slaves, mostly young women and children, in chains ready to transport halfway across the continent to the markets of Zanzibar. \nHaving found the new ruler of the Upper Congo, Stanley had no choice but to negotiate an agreement with him, to stop Tip coming further downstream and attacking Leopoldville and other stations. To achieve this, he had to allow Tip to build his final river station just below Stanley Falls, which prevented vessels from sailing further upstream. At the end of his physical resources, Stanley returned home, to be replaced by Lieutenant Colonel Francis de Winton, a former British Army officer.\nEmin Pasha Relief Expedition (1887\u20131890).\nIn 1886, Stanley led the Emin Pasha Relief Expedition to \"rescue\" Emin Pasha, the governor of Equatoria in the southern Sudan, who was threatened by Mahdist forces. King Leopold II demanded that Stanley take the longer route via the Congo River, hoping to acquire more territory and perhaps even Equatoria. After immense hardships and great loss of life, Stanley met Emin in 1888, mapped the Ruwenzori Range and Lake Edward, and emerged from the interior with Emin and his surviving followers at the end of 1890. Despite its success, this expedition tarnished Stanley's name because of the conduct of the other Europeans on the expedition. Army Major Edmund Musgrave Barttelot was killed by an African porter after behaving with extreme cruelty. James Sligo Jameson, heir to Irish whiskey manufacturer Jameson's, allegedly bought a 10-year-old girl and offered her to cannibals to document and sketch how she was cooked and eaten. Stanley found out only when Jameson had died of fever.\nThe spread of sleeping sickness across areas of central and eastern Africa that were previously free of the disease has been attributed to this expedition, but this hypothesis has been disputed. Sleeping sickness had been endemic in these regions for generations and then flared into epidemics as colonial trade increased trade throughout Africa during the ensuing decades.\nIn a number of publications made after the expedition, Stanley asserts that the purpose of the effort was singular; to offer relief to Emin Pasha. For example, he writes the following while explaining the final route decision.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The advantages of the Congo route were about five hundred miles shorter land journey, and less opportunities for deserting. It also quieted the fears of the French and Germans that, behind this professedly humanitarian quest, we might have annexation projects.\nHowever, Stanley's other writings point to a secondary goal which was precisely territorial annexation. He writes in his book on the expedition about his meeting with the Sultan of Zanzibar, when he arrived there at the start of the expedition, and a certain matter that was discussed at that meeting. At first, he is not explicit on the agenda but it is clear enough:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We then entered heartily into our business; how absolutely necessary it was that he should promptly enter into an agreement with the English within the limits assigned by Anglo-German treaty. It would take too long to describe the details of the conversation, but I obtained from him the answer needed.\nA few pages further in the same book, Stanley explains what the matter was about and this time, he makes it clear that indeed, it had to do with annexation.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I have settled several little commissions at Zanzibar satisfactorily. One was to get the Sultan to sign the concessions which Mackinnon tried to obtain a long time ago. As the Germans have magnificent territory east of Zanzibar, it was but fair that England should have some portion for the protection she has accorded to Zanzibar since 1841\u00a0... The concession that we wished to obtain embraced a portion of East African coast, of which Mombasa and Malindi were the principal towns. For eight years, to my knowledge, the matter had been placed before His Highness, but the Sultan's signature was difficult to obtain.\nThe records at the National Archives at Kew, London, offer an even deeper insight and show that annexation was a purpose he had been aware of for the expedition. This is because there are a number of treaties curated there (and gathered by Stanley himself from what is present-day Uganda during the Emin Pasha Expedition), ostensibly gaining British protection for a number of African chiefs. Amongst these were a number that have long been identified as possible frauds. A good example is treaty number 56, supposedly agreed upon between Stanley and the people of \"Mazamboni, Katto, and Kalenge\". These people had signed over to Stanley, \"the Sovereign Right and Right of Government over our country for ever in consideration of value received and for the protection he has accorded us and our Neighbours against KabbaRega and his Warasura.\"\nLater years.\nOn his return to Europe, Stanley married English artist Dorothy Tennant. They adopted a child named Denzil, who was the son of one of Stanley's first cousins, though Stanley concealed this fact from the public and possibly even from Dorothy. Denzil later donated around 300 items to the Stanley archives at the Royal Museum of Central Africa in Tervuren, Belgium in 1954. He died in 1959.\nMainly at his wife's behest, Stanley took up British citizenship and entered Parliament as a Liberal Unionist member for Lambeth North, serving from 1895 to 1900. He disliked politics and made little impression on Parliament. He became Sir Henry Morton Stanley when he was made a Knight Grand Cross of the Order of the Bath in the 1899 Birthday Honours, in recognition of his service to the British Empire in Africa. In 1890, he was given the Grand Cordon of the Order of Leopold by King Leopold II.\nStanley died at his home at 2 Richmond Terrace, Whitehall, London on 10 May 1904. At his funeral, he was eulogised by Daniel P. Virmar. His grave is in the churchyard of St Michael and All Angels' Church in Pirbright, Surrey, marked by a large piece of granite inscribed with the words \"Henry Morton Stanley, Bula Matari, 1841\u20131904, Africa\". Bula Matari translates as \"Breaker of Rocks\" or \"Breakstones\" in Kongo and was Stanley's name among locals in Congo. It can be translated as a term of endearment for, as the leader of Leopold's expedition, he commonly worked with the labourers breaking rocks with which they built the first modern road along the Congo River. Author Adam Hochschild suggested that Stanley understood it as a heroic epithet, but there is evidence that Nsakala, the man who coined it, had meant it humorously.\nControversies.\nOverview.\nHaving survived for ten years of his childhood in the workhouse at St Asaph, it is postulated that he needed as a young man to be thought of as harder and more formidable than other explorers. This made him exaggerate punishments and hostile encounters. It was a serious error of judgement for which his reputation continues to pay a heavy price. In the conclusion to his account of a fight with a fellow boy while in the workhouse, Stanley remarked, \"Often since have I learned how necessary is the application of force for the establishment of order. There comes a time when pleading is of no avail.\" He was accused of indiscriminate cruelty against Africans by contemporaries, which included men who served under him or otherwise had first-hand information. Stanley himself acknowledged, \"Many people have called me hard, but they are always those whose presence a field of work could best dispense with, and whose nobility is too nice to be stained with toil.\"\nAbout society women, Stanley wrote that they were \"toys to while slow time\" and \"trifling human beings.\" When he met the American journalist and traveller May Sheldon, he was attracted because she was a modern woman who insisted on serious conversation and not social chit-chat. \"She soon lets you know that chaff won't do,\" he wrote. The authors of the book \"The Congo: Plunder and Resistance\" tried to argue that Stanley had \"a pathological fear of women, an inability to work with talented co-workers, and an obsequious love of the aristocratic rich,\" This is not only at odds with his opinions about society women, but Stanley's intimate correspondence in the Royal Museum of Central Africa, between him and his two fianc\u00e9es, Katie Gough Roberts and Alice Pike, as well as between him and the American journalist May Sheldon, and between him and his wife Dorothy Tennant, shows that he enjoyed close relationships with those women, but both Roberts and Pike ultimately rejected him when he refused to abandon his protracted travels.\nWhen Stanley married Dorothy, he invited his friend, Arthur Mounteney Jephson, to visit while they were on their honeymoon. Dr. Thomas Parke also came because Stanley was seriously ill at the time. Stanley's good relations with these two colleagues from the Emin Pasha Expedition could possibly be seen as demonstrating that he could get along with colleagues.\nGeneral opinion about African people.\nIn \"Through the Dark Continent\", Stanley observed the peoples of the region, and wrote that \"the savage only respects force, power, boldness, and decision\". Stanley further wrote: \"If Europeans will only\u00a0... study human nature in the vicinity of Stanley Pool (Kinshasa), they will go home thoughtful men, and may return again to this land to put to good use the wisdom they should have gained\u00a0... during their peaceful sojourn.\"\nIn \"How I Found Livingstone\" (1872), he wrote that he was \"prepared to admit any black man possessing the attributes of true manhood, or any good qualities\u00a0... to a brotherhood with myself.\"\nStanley insulted and shouted at William Grant Stairs and Arthur Jephson for mistreating the Wangwana. He described the history of Boma as \"two centuries of pitiless persecution of black men by sordid whites\". He also wrote about what he thought was the superior beauty of black people in comparison with whites. According to Jeal, Stanley was not a racist, unlike his contemporaries Sir Richard Burton and Sir Samuel Baker.\nOpinion about mixed African-Arab peoples.\nThe Wangwana of Zanzibar were of mixed Arabian and African ancestry: \"Africanized Arabs\", in Stanley's words. They became the backbone of all his major expeditions and were referred to as \"his dear pets\" by sceptical young officers on the Emin Pasha Expedition, who resented their leader for favouring the Wangwana above themselves. \"All are dear to me\", Stanley told William Grant Stairs and Arthur Jephson, \"who do their duty and the Zanzibaris have quite satisfied me on this and on previous expeditions.\" Stanley came to think of an individual Wangwana as \"superior in proportion to his wages to ten Europeans\". When Stanley first met a group of his Wangwana assistants, he was surprised: \"They were an exceedingly fine looking body of men, far more intelligent in appearance than I could ever have believed African barbarians could be\".\nOn the other hand, in one of his books, Stanley said about mixed Afro-Arab people: \"For the half-castes I have great contempt. They are neither black nor white, neither good nor bad, neither to be admired nor hated. They are all things, at all times\u00a0... If I saw a miserable, half-starved negro, I was always sure to be told, he belonged to a half-caste. Cringing and hypocritical, cowardly and debased, treacherous and mean\u00a0... this syphilitic, blear-eyed, pallid-skinned, abortion of an Africanized Arab.\"\nAccounts of cruel treatment toward African people.\nThe British House of Commons appointed a committee to investigate missionary reports of Stanley's mistreatment of native populations in 1871, which was likely secured by Horace Waller, a member on the committee of the Anti-slavery Society and fellow of the Royal Geographical Society. The British vice consul in Zanzibar, John Kirk (Waller's brother-in-law) conducted the investigation. Stanley was charged with excessive violence, wanton destruction, the selling of labourers into slavery, the sexual exploitation of native women and the plundering of villages for ivory and canoes. Kirk's report to the British Foreign Office was never published, but in it, he claimed: \"If the story of this expedition were known it would stand in the annals of African discovery unequalled for the reckless use of power that modern weapons placed in his hands over natives who never before heard a gun fired.\" When Kirk was appointed to investigate reports of brutality against Stanley, he was delighted because he had hated Stanley for almost a decade. Firstly, for having publicly exposed him (Kirk) for having failed to send provisions to Livingstone from Zanzibar during the late 1860s; secondly, because Stanley had revealed in the press that Kirk had sent slaves to David Livingstone as porters, rather than the free men Livingstone had made very plain he wanted. Kirk was related to Horace Waller by marriage; and so Waller also hated Stanley on Kirk's behalf. He used his membership of the executive committee of the Universities Mission to Central Africa to persuade J. P. Farler (a missionary in East Africa) to name Stanley's assistants who might provide evidence against the explorer and be prepared to be interviewed by Kirk in Zanzibar. An American merchant in Zanzibar, Augustus Sparhawk, wrote that several of Stanley's African assistants, including Manwa Sera, \"a big rascal and too fond of money\", had been bribed to tell Kirk what he wanted to hear. Stanley was accused, in Kirk's report, of cruelty to his Wangwana carriers and guards whom he idolised and who re-enlisted with him again and again. He wrote to the owner of the \"Daily Telegraph\", insisting that he (Lawson) force the British government to send a warship to take the Wangwana home to Zanzibar and to pay all their back wages. If a ship was not sent, they would die on their overland journey home. The ship was sent. Stanley's hatred of the promiscuity that had caused his illegitimacy and his legendary shyness with women, made the Kirk report's claim that he had accepted an African mistress offered to him by Kabaka Mutesa exceedingly implausible. Both Stanley and his colleague, Frank Pocock, loathed slavery and the slave trade and wrote about this loathing in letters and diaries at this time, which speaks against the likelihood that they sold their own men. The report was never shown to Stanley, so he had been unable to defend himself.\nIn a letter to the Secretary of the Royal Geographical Society in the 1870s, Conservative MP and treasurer of the Aborigines' Protection Society, Sir Robert Fowler, who believed Kirk's report and refused to \"whitewash Stanley\", insisted that his \"heartless butchery of unfortunate natives has brought dishonour on the British flag and must have rendered the course of future travellers more perilous and difficult.\"\nGeneral Charles George Gordon remarked in a letter to Richard Francis Burton that Stanley shared Samuel Baker's tendency to write openly about deploying firearms against Africans in self-defense: \"These things may be done, but not advertised\", Burton himself wrote that Stanley \"shoots negros as if they were monkeys\" in an October 1876 letter to Kirk. He also loathed Stanley for disproving his long-held theory that Lake Tanganyika, which he was the first European to discover, was the true source of the Nile, which may have influenced Burton to misrepresent Stanley's activities in Africa.\nIn 1877, not long after one of Stanley's expeditions, Farler met with African porters who had been part of the expedition and wrote, \"Stanley's followers give dreadful accounts to their friends of the killing of inoffensive natives, stealing their ivory and goods, selling their captives, and so on. I do think a commission ought to inquire into these charges, because if they are true, it will do untold harm to the great cause of emancipating Africa\u00a0... I cannot understand all the killing that Stanley has found necessary\". Stanley, when reporting the American Indian Wars as a young reporter, had been encouraged by his editors to exaggerate the number of Indians killed by the US Army. The legacy for Stanley, of being a helpless illegitimate boy, deserted by both parents, was a deep sense of inferiority that could only be kept at bay by claims of being much more powerful and feared than he was. Tim Jeal, in his biography of Stanley, has shown by a study of Stanley's diary and his colleague Frank Pocock's diary that on almost every occasion when there was conflict with Africans on the Congo in 1875\u201376, Stanley exaggerated the scale of the conflict and the deaths on both sides. On 14 February 1877, according to his colleague, Frank Pocock's diary, Stanley's nine canoes, and his sectional boat the \"Lady Alice\", were attacked and followed by eight canoes, crewed by Africans with firearms. In Stanley's book, \"Through the Dark Continent\", Stanley inflated this incident into a major battle, by increasing the number of hostile canoes to 60 and adjusting the casualties accordingly.\nStanley wrote with some measure of satisfaction when describing how Captain John Hanning Speke, the first European to visit Uganda, had been punched in the teeth for disobedience to Sidi Mubarak Bombay, a caravan leader also employed by Stanley, which made Stanley claim that he would never allow Bombay to have the audacity to stand up for a boxing match with him. In the same paragraph, Stanley described how he several months later administered punishment to the African.\nWilliam Grant Stairs found Stanley during the Emin Pasha expedition to be cruel, secretive and selfish. John Rose Troup, in his book about the Emin Pasha expedition, said that he saw Stanley's self-serving and vindictive side: \"In the forgoing letter he brings forward disgraceful charges, that really do not refer to me at all, although he blames me for what happened. The injustice of his accusations, made as they are without documentary or, as far as I can learn, any evidence, can hardly be made clear to the public, but they must be aware, when they read what has preceded this correspondence, that he has acted as no one in his position should have acted\".\nBy way of counterpoint, it may be noted that, in later in life, Stanley rebuked subordinates for inflicting needless corporal punishment. For beating one of his most trusted African servants, he told Lieutenant Carlos Branconnier \"that cruelty was not permissible\" and that he would dismiss him for a future offence, and he did. Stanley was admired by Arthur Jephson, whom William Bonny, the acerbic medical assistant, described as the \"most honourable\" officer on the expedition. Jephson wrote, \"Stanley never fights where there is the smallest chance of making friends with the natives and he is wonderfully patient &amp; long suffering with them\". Writer Tim Jeal has argued that during Stanley's 1871 expedition, he treated his indigenous porters well under \"contemporary standards.\"\nPossible inspiration for \"Heart of Darkness\".\nThe legacy of death and destruction in the Congo region during the Free State period and the fact that Stanley had worked for Leopold are considered by author Norman Sherry to have made him an inspiration for Joseph Conrad's \"Heart of Darkness.\"\nConrad, however, had spent six months of 1890 as a steamship captain on the Congo, years after Stanley had been there (1879\u20131884) and five years after Stanley had been recalled to Europe and ceased to be Leopold's chief agent in Africa.\nWorks by Stanley.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nHonours and legacy.\nA former hospital in St Asaph, north Wales, was named after Stanley in honour of his birth in the area. It was formerly the workhouse in which he spent much of his early life. Memorials to Stanley were erected in St Asaph and in Denbigh (a statue of Stanley with an outstretched hand) in 2011. A working party was set up in 2020 to consider new wording for a plaque on the St Asaph obelisk, and a public consultation and vote was held in 2021 over a proposal to remove the Denbigh statue, which resulted in an 80 per cent majority for retaining the statue.\nTaxa named in honour of Stanley include:\nThe mineral stanleyite is named in his honour, as the describer of the mineral was surnamed Livingstone but a mineral named livingstonite (named for David Livingstone) already existed.\nStanley Electric, a major Japanese supplier of automotive lighting, was named by founder Takaharu Kitano after Stanley in admiration of his \"perseverance and pioneering spirit\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49307", "revid": "980331", "url": "https://en.wikipedia.org/wiki?curid=49307", "title": "Standing (law)", "text": "Legal concept\nIn law, standing or locus standi is a condition that a party seeking a legal remedy must show they have, by demonstrating to the court, sufficient connection to and harm from the law or action challenged to support that party's participation in the case. A party has standing in the following situations:\nIn the United States, a person may not bring a suit challenging the constitutionality of a law unless they can demonstrate that they are or will \"imminently\" be harmed by the law. Otherwise, the court will rule that the plaintiff lacks standing to bring the suit and will dismiss it without considering the merits of the claim of unconstitutionality.\nInternational courts.\nThe Council of Europe created the first international court before which individuals have automatic \"locus standi\".\nAustralia.\nAustralia has a common law understanding of \"locus standi\" or standing which is expressed in statutes such as the Administrative Decisions (Judicial Review) Act 1977 and common law decisions of the High Court of Australia especially the case \"Australian Conservation Foundation v Commonwealth\" (1980). At common law, the test for standing is whether the plaintiff has a \"special interest in the subject matter of the action\". Under the \"Administrative Decisions (Judicial Review) Act\" 1977 to have standing the applicant must be \"a person who is aggrieved\", defined as \"a person whose interests are adversely affected\" by the decision or conduct complained of. This has generally been interpreted in accordance with the common law test.\nThere is no open standing, unless statute allows it, or represents needs of a specified class of people. The issue is one of remoteness.\nStanding may apply to class of aggrieved people, where essentially the closeness of the plaintiff to the subject matter is the test. Furthermore, a plaintiff must show that he or she has been specially affected in comparison with the public at large.\nAlso, while there is no open standing per se, prerogative writs like certiorari, writ of prohibition, quo warranto and habeas corpus have a low burden in establishing standing. Australian courts also recognise amicus curiae (friend of the court), and the various Attorneys General have a presumed standing in administrative law cases.\nCanada.\nIn Canadian administrative law, whether an individual has standing to bring an application for judicial review, or an appeal from the decision of a tribunal, is governed by the language of the particular statute under which the application or the appeal is brought. Some statutes provide for a narrow right of standing while others provide for a broader right of standing.\nFrequently a litigant wishes to bring a civil action for a declaratory judgment against a public body or official. This is considered an aspect of administrative law, sometimes with a constitutional dimension, as when the litigant seeks to have legislation declared unconstitutional.\nPublic interest standing.\nThe Supreme Court of Canada developed the concept of public interest standing in three constitutional cases commonly called \"the Standing trilogy\": \"Thorson v. Attorney General of Canada\", \"Nova Scotia Board of Censors v. McNeil\", and \"Minister of Justice v. Borowski\". The trilogy was summarized as follows in \"Canadian Council of Churches v. Canada (Minister of Employment and Immigration)\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It has been seen that when public interest standing is sought, consideration must be given to three aspects. First, is there a serious issue raised as to the invalidity of legislation in question? Second, has it been established that the plaintiff is directly affected by the legislation or if not does the plaintiff have a genuine interest in its validity? Third, is there another reasonable and effective way to bring the issue before the court?\nPublic-interest standing is also available in non-constitutional cases, as the Court found in \"Finlay v. Canada (Minister of Finance)\".\nNigeria.\nLike in other jurisdictions, the right to approach a court is contained in the Constitution. The right to approach a court has been interpreted in several cases, this has led to the right to be view differently in different cases. In recent times, there have been different approaches to locus standi. They are:\nUnited Kingdom.\nIn British administrative law, an applicant for judicial review needs to have a sufficient interest in the matter to which the application relates. This sufficient interest requirement has been construed liberally by the courts. As Lord Diplock put it:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[i]t would\u00a0... be a grave danger to escape lacuna in our system of public law if a pressure group\u00a0... or even a single public spirited taxpayer, were prevented by outdated technical rules of from bringing the matter to the attention of the court to vindicate the rule of law and get the unlawful conduct stopped.\nIn the law of contract, the doctrine of privity means that only those who are party to a contract can sue or be sued upon it. This doctrine was substantially amended by the Contracts (Rights of Third Parties) Act 1999, which allows third parties specified in a contract to enforce it provided the contract expressly grants them the right to do so.\nAlmost all criminal prosecutions are brought by the state via the Crown Prosecution Service, so private prosecutions are rare. A famous exception was the case of \"Whitehouse v Lemon\" where Mrs Mary Whitehouse, a self-appointed guardian of suburban morality, was permitted to bring a private prosecution for blasphemous libel (an offence still in existence until 2008) against the publisher of \"Gay News\", Denis Lemon. Victims of crime have standing to sue the perpetrator and they may claim criminal injuries compensation from the state. If the state fails properly to bring a case, the victim or his family may have standing to bring a private prosecution, as in the case of Stephen Lawrence.\nUnited States.\nIn United States law, the Supreme Court has stated, \"In essence the question of standing is whether the litigant is entitled to have the court decide the merits of the dispute or of particular issues.\" John Rutledge, the second chief justice of the United States, was largely responsible at the Constitutional Convention for denying the Supreme Court the right to give advisory opinions. Being a judge himself, he strongly believed that a judge's sole purpose was to resolve legal conflicts; he held that judges should hand down an opinion only when they rule on an actual case.\nThere are a number of requirements that a plaintiff must establish to have standing before a federal court. Some are based on the case or controversy requirement of the judicial power of Article Three of the United States Constitution, \u00a7 2, cl. 1, which provides, \"The judicial Power shall extend to all Cases\u00a0... [and] Controversies\". The requirement that a plaintiff have standing to sue is a limit on the role of the judiciary; Article III standing is an aspect of separation of powers.\nFederal courts may exercise power only \"in the last resort, and as a necessity\". The Supreme Court has determined that the case or controversy requirement found in Article Three prohibits United States federal courts from issuing advisory opinions. Accordingly, before the court will hear a case, it must find that the parties have a tangible interest at stake in the matter, the issue presented must be \"mature for judicial resolution\" or ripe, and a justiciable issue must remain before the court throughout the course of the lawsuit.\nThe American doctrine of standing is assumed as having begun with the case of \"Frothingham v. Mellon\" (1923). However, legal standing truly rests its first prudential origins in \"Fairchild v. Hughes\" (1922) which was authored by Justice Louis Brandeis. In \"Fairchild\", a citizen sued the Secretary of State and the Attorney General to challenge the procedures by which the Nineteenth Amendment was ratified. Prior to it, the doctrine was that all persons had a right to pursue a private prosecution of a public right. Since then the doctrine has been embedded in judicial rules and some statutes.\nIn 2011, in \"Bond v. United States\", the U.S. Supreme Court held that a criminal defendant charged with violating a federal statute has standing to challenge the constitutionality of that statute under the Tenth Amendment.\nStanding requirements.\nThere are three standing requirements:\nPrudential limitations.\nAdditionally, there are three major prudential (judicially created) standing principles (prudential standing). Congress can override these principles via statute:\nRecent development of the doctrine.\nIn 1984, the Supreme Court reviewed and further outlined the standing requirements in a major ruling concerning the meaning of the three standing requirements of injury, causation, and redressability. In the suit, parents of black public school children alleged that the Internal Revenue Service was not enforcing standards and procedures that would deny tax-exempt status to racially discriminatory private schools. The Court found that the plaintiffs did not have the standing necessary to bring suit. Although the Court established a significant injury for one of the claims, it found the causation of the injury (the nexus between the defendant's actions and the plaintiff's injuries) to be too attenuated. \"The injury alleged was not fairly traceable to the Government conduct respondents challenge as unlawful\".\nIn another major standing case, \"Lujan v. Defenders of Wildlife\", 504 U.S. 555 (1992), the Supreme Court elaborated on the redressability requirement for standing. The case involved a challenge to a rule promulgated by the Secretary of the Interior interpreting \u00a77 of the Endangered Species Act of 1973 (ESA). The rule rendered \u00a77 of the ESA applicable only to actions within the United States or on the high seas. The Court found that the plaintiffs did not have the standing necessary to bring suit, because no injury had been established. The injury claimed by the plaintiffs was that damage would be caused to certain species of animals and that this in turn injures the plaintiffs by the reduced likelihood that the plaintiffs would see the species in the future. The court insisted though that the plaintiffs had to show how damage to the species would produce imminent injury to the plaintiffs. The Court found that the plaintiffs did not sustain this burden of proof. \"The 'injury in fact' test requires more than an injury to a cognizable interest. It requires that the party seeking review be himself among the injured\". The injury must be imminent and not hypothetical.\nBeyond failing to show injury, the Court found that the plaintiffs failed to demonstrate the standing requirement of redressability. The Court pointed out that the respondents chose to challenge a more generalized level of government action, \"the invalidation of which would affect all overseas projects\". This programmatic approach has \"obvious difficulties insofar as proof of causation or redressability is concerned\".\nIn a 2000 case, \"Vermont Agency of Natural Resources v. United States ex rel. Stevens\", 529 U.S. 765 (2000), the United States Supreme Court endorsed the \"partial assignment\" approach to \"qui tam\" relator standing to sue under the False Claims Act \u2013 allowing private individuals to sue on behalf of the U.S. government for injuries suffered solely by the government.\nIn a 2009 case, \"Summers v. Earth Island Institute,\" 555 U.S. 488 (2009), the Supreme Court held the petitioner environmental organizations' claim that it was \"statistically likely\" that some of their members would visit the affected lands was insufficient to support Article III standing. The majority opinion stated the \"deprivation of a procedural right without some concrete interest that is affected by the deprivation\u00a0... is insufficient to create Article III standing.\"\nTaxpayer standing.\nThe initial case that established the doctrine of standing, \"Frothingham v. Mellon\", was a taxpayer standing case.\nTaxpayer standing is the concept that any person who pays taxes should have standing to file a lawsuit against the taxing body if that body allocates funds in a way that the taxpayer feels is improper. The United States Supreme Court has held that taxpayer standing is not by itself a sufficient basis for standing against the United States government. The Court has consistently found that the conduct of the federal government is too far removed from individual taxpayer returns for any injury to the taxpayer to be traced to the use of tax revenues, e.g., \"United States v. Richardson.\"\nIn \"DaimlerChrysler Corp. v. Cuno\", the Court extended this analysis to state governments as well. However, the Supreme Court has also held that taxpayer standing is constitutionally sufficient to sue a municipal government in a federal court.\nStates are also protected against lawsuits by their sovereign immunity. Even where states waive their sovereign immunity, they may nonetheless have their own rules limiting standing against simple taxpayer standing against the state. Furthermore, states have the power to determine what will constitute standing for a litigant to be heard in a state court, and may deny access to the courts premised on taxpayer standing alone.\nIn California, taxpayers have standing to sue for any \"illegal expenditure of, waste of, or injury to the estate, funds, or other property of a local agency\". In Florida, a taxpayer has standing to sue if the state government is acting unconstitutionally with respect to public funds, or if government action is causing some special injury to the taxpayer that is not shared by taxpayers in general. In Virginia, the Supreme Court of Virginia has more or less adopted a similar rule. An individual taxpayer generally has standing to challenge an act of a city or county where they live, but does not have general standing to challenge state expenditures.\nStanding to challenge statutes.\nWith limited exceptions, a party cannot have standing to challenge the constitutionality of a statute unless they will be subjected to the provisions of that statute. There are some exceptions, however; for example, courts will accept First Amendment challenges to a statute on overbreadth grounds, where a person who is only partially affected by a statute can challenge the parts that do not affect him on the grounds that laws that restrict speech have a chilling effect on other people's right to free speech.\nThe only other way someone can have standing to challenge the constitutionality of a statute is if the existence of the statute would otherwise deprive him of a right or a privilege even if the statute itself would not apply to him. The Virginia Supreme Court made this point clear in the case of \"Martin v. Ziherl\" 607 S.E.2d 367 (Va. 2005). Martin and Ziherl were girlfriend and boyfriend and engaged in unprotected sexual intercourse when Martin discovered that Ziherl had infected her with herpes, even though he knew he was infected and did not inform her of this. She sued him for damages, but because it was illegal (at the time the case was filed) to commit \"fornication\" (sexual intercourse between a man and a woman who are not married), Ziherl argued that Martin could not sue him because joint tortfeasors \u2013 those involved in committing a crime \u2013 cannot sue each other over acts occurring as a result of a criminal act (\"Zysk v. Zysk\", 404 S.E.2d 721 (Va. 1990)). Martin argued in rebuttal that because of the U.S. Supreme Court decision in \"Lawrence v. Texas\" (finding that state's sodomy law unconstitutional), Virginia's anti-fornication law was also unconstitutional for the reasons cited in Lawrence. Martin argued, therefore, she could, in fact, sue Ziherl for damages.\nLower courts decided that because the Commonwealth's Attorney does not prosecute fornication cases and no one had been prosecuted for fornication anywhere in Virginia in over 100\u00a0years, Martin had no risk of prosecution and thus lacked standing to challenge the statute. Martin appealed. Since Martin had something to lose \u2013 the ability to sue Ziherl for damages \u2013 if the statute was upheld, she had standing to challenge the constitutionality of the statute even though the possibility of her being prosecuted for violating it was zero. Since the U.S. Supreme Court in \"Lawrence\" had found that there is a privacy right in one's private, noncommercial sexual practices, the Virginia Supreme Court decided that the statute against fornication was unconstitutional. The finding gave Martin standing to sue Ziherl since the decision in \"Zysk\" was no longer applicable. However, the only reason Martin had standing to challenge the statute was that she had something to lose if it stayed on the books.\nStanding to challenge a contract award.\nOnly an \"interested party\" has standing to challenge a federal contract award. In this context, an \"interested party\" is a company or person who bid for a contract, or a prospective bidder, whose \"direct economic interest would be affected by the award of the contract\" to another business.\nBallot measures.\nIn \"Hollingsworth v. Perry\", the Supreme Court ruled that being the proponents of a ballot measure is not by itself enough to confer legal standing. In that case, Proposition 8 had banned same-sex marriage in California, a ban that was ruled unconstitutional. The Supreme Court ruled that the proponents of Proposition 8 has no standing in court since they failed to show that they were harmed by the decision.\nState law.\nState law on standing differs substantially from federal law and varies considerably from state to state.\nCalifornia.\nCalifornians may bring \"taxpayer actions\" against public officials for wasting public funds through mismanagement of a government agency, where the relief sought is an order compelling the official not to waste money and fulfill his duty to protect the public fisc.\nOn December 29, 2009, the California Court of Appeal for the Sixth District ruled that California Code of Civil Procedure Section 367 cannot be read as imposing a federal-style standing doctrine on California's code pleading system of civil procedure. In California, the fundamental inquiry is \"always\" whether the plaintiff has sufficiently pleaded a cause of action, not whether the plaintiff has some entitlement to judicial action separate from proof of the substantive merits of the claim advanced. The court acknowledged that the word \"standing\" is often sloppily used to refer to what is really , and held that in state law is not the same thing as the federal standing doctrine.\nDistrict of Columbia.\nThe District of Columbia's regulations concerning contract award appeals provide for the jurisdiction of the District's Contracts Appeals Board. Standing to appeal a bid is limited to unsuccessful bidders who are in line to be awarded a contract should their protest be successful: the Board has regularly held that a protestor who would not be in line for the contract lacks standing.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49308", "revid": "38455", "url": "https://en.wikipedia.org/wiki?curid=49308", "title": "Ripeness", "text": "Readiness of a case for litigation in US law\nIn United States law, ripeness refers to the readiness of a case for litigation; \"a claim is not ripe for adjudication if it rests upon contingent future events that may not occur as anticipated, or indeed may not occur at all.\" For example, if a law of ambiguous quality has been enacted but never applied, a case challenging that law lacks the ripeness necessary for a decision.\nThe goal is to prevent premature adjudication; if a dispute is insufficiently developed, any potential injury or stake is too speculative to warrant judicial action. Ripeness issues most usually arise when a plaintiff seeks anticipatory relief, such as an injunction. \nOriginally stated in \"Liverpool, New York &amp; Philadelphia Steamship Co. v. Commissioners of Emigration\" (1885), ripeness is one the seven rules of the constitutional avoidance doctrine established in \"Ashwander v. Tennessee Valley Authority\" (1936) that requires that the Supreme Court of the United States to \"not 'anticipate a question of constitutional law in advance of the necessity of deciding it.'\" The Court fashioned a two-part test for assessing ripeness challenges to federal regulations. The case is often applied to constitutional challenges to federal and state statutes as well. The Court said in \"Abbott Laboratories v. Gardner\", 387 U.S. https:// (1967):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Without undertaking to survey the intricacies of the ripeness doctrine it is fair to say that its basic rationale is to prevent the courts, through avoidance of premature adjudication, from entangling themselves in abstract disagreements over administrative policies, and also to protect the agencies from judicial interference until an administrative decision has been formalized and its effects felt in a concrete way by the challenging parties. The problem is best seen in a twofold aspect, requiring us to evaluate both the fitness of the issues for judicial decision and the hardship to the parties of withholding court consideration.\nIn both \"Abbott Laboratories\" and its first companion case, \"Toilet Goods Association v. Gardner\", 387 U.S. https:// (1967), the Court upheld pre-enforcement review of an administrative regulation. However, the Court denied such review in the second companion case because any harm from noncompliance with the FDA regulation at issue was too speculative in the Court's opinion to justify judicial review. Justice Harlan wrote for the Court in all three cases.\nThe ripeness doctrine should not be confused with the advisory opinion doctrine, another justiciability concept in American law.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49309", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=49309", "title": "Mootness", "text": "Legal term on the status of a matter\nThe terms moot, mootness and moot point are used both in English and in American law, although with significantly different meanings.\nIn the legal system of the United States, a matter is \"moot\" if further legal proceedings with regard to it can have no effect or events have placed it beyond the reach of the law, thereby depriving the matter of practical significance or rendering it purely academic.\nThe U.S. development of this word stems from the practice of moot courts, in which hypothetical or fictional cases were argued as a part of legal education. These purely academic settings led the U.S. courts to describe cases where developing circumstances made any judgment ineffective as \"moot\".\nThe mootness doctrine can be compared to the ripeness doctrine, another court rule (rather than law) that holds that judges should not rule on cases based entirely on anticipated disputes or hypothetical facts. These rules and similar doctrines, taken together, prevent the federal courts of the United States from issuing advisory opinions, as required by the Case or Controversy Clause of the United States Constitution.\nThe usage in the British legal system, on the other hand, is that the term \"moot\" has the meaning of \"remains open to debate\" or \"remains unresolved\". The divergence in usage was first observed in the United States, and the extent to which the U.S. definition is used in U.S. jurisprudence and public discourse has ensured it is rarely used in a British courtroom. This is partially to avoid ambiguity, but also because the British definition is rarely relevant in practical cases.\nU.S. federal courts.\nIn the U.S. federal judicial system, a moot case must be dismissed, there being a constitutional limitation on the jurisdiction of the federal courts. The reason for this is that Article Three of the United States Constitution limits the jurisdiction of all federal courts to \"cases and controversies\". Thus, a civil action or appeal in which the court's decision will not affect the rights of the parties is ordinarily beyond the power of the court to decide, provided it does not fall within one of the recognized exceptions.\nA textbook example of such a case is the United States Supreme Court case \"DeFunis v. Odegaard\", 416 U.S. https:// (1974). The plaintiff was a student who had been denied admission to law school, and had then been provisionally admitted during the pendency of the case. Because the student was slated to graduate within a few months at the time the decision was rendered, and there was no action the law school could take to prevent that, the Court determined that a decision on its part would have no effect on the student's rights. Therefore, the case was dismissed as moot.\nHowever, there is disagreement as to both the source of the standards, and their application in the courts. Some courts and observers opine that cases \"must\" be dismissed because this is a constitutional bar, and there is no \"case or controversy\"; others have rejected the pure constitutional approach and adopted a so-called \"prudential\" view, where dismissal \"may\" depend upon a host of factors, whether the particular person has lost a viable interest in the case, or whether the issue itself survives outside the interests of the particular person, whether the circumstance are likely to recur, etc. In actual practice, the U.S. federal courts have been uneven in their decisions, which has led to the accusation that determinations are \"ad hoc\" and 'result-oriented.'\nThere are four major exceptions to this mootness rule. These are cases of \"voluntary cessation\" on the part of the defendant; questions that involve secondary or collateral legal consequences; questions that are \"capable of \"repetition,\" yet evading review\"; and questions involving class actions where the named party ceases to represent the class.\nVoluntary cessation.\nWhere a defendant is acting wrongfully, but as a tactic to avoid an adverse decision, ceases to engage in such conduct once a litigation has been threatened or commenced, the court will still not deem this correction to moot the case. Obviously, a party could stop acting improperly just long enough for the case to be dismissed and then resume the improper conduct. For example, in \"Friends of the Earth, Inc. v. Laidlaw Environmental Services, Inc.\", 528 U.S. https:// (2000), the Supreme Court held that an industrial polluter, against whom various deterrent civil penalties were being pursued, could not claim that the case was moot, even though the polluter had ceased polluting and had closed the factory responsible for the pollution. The court noted that so long as the polluter still retained its license to operate such a factory, it could open similar operations elsewhere if not deterred by the penalties sought.\nA separate situation occurs when a court dismisses as \"moot\" a legal challenge to an existing law, in the case where the law being challenged is either amended or repealed through legislation before the court case could be settled. Since the remedy available for a bad law is simply the removal or changing of the law, the court's decision cannot create an outcome different than that which has already occurred. As such, any decision would merely be advisory, in violation of the Case or Controversy clause. While this sometimes casually referred to as voluntary cessation, the differences in available remedy make it distinct from technical voluntary cessation.\nA recent instance of this occurred in \"Moore v. Madigan,\" where Illinois Attorney General Lisa Madigan declined to appeal a ruling from the Seventh Circuit striking down Illinois' handgun carry ban to the United States Supreme Court. As Illinois subsequently passed a law legalizing concealed carry with a state-issued license, the appeal would have been moot since the original case or controversy was no longer relevant.\nSecondary or collateral legal consequences.\n\"The obvious fact of life is that most criminal convictions do in fact entail adverse collateral legal consequences. The mere possibility that this will be the case is enough to preserve a criminal case from ending ignominiously in the limbo of mootness.\" Sibron v. New York.\nCapable of repetition, yet evading review.\nA court will allow a case to go forward if it is the type for which persons will frequently be faced with a particular situation, but will likely cease to be in a position where the court can provide a remedy for them in the time that it takes for the justice system to address their situation. The most frequently cited example is the 1973 United States Supreme Court case of \"Roe v. Wade\", 410 U.S. https:// (1973), which challenged a Texas law forbidding abortion in most circumstances. The state argued that the case was moot because plaintiff Roe was no longer pregnant by the time the case was heard. As Justice Blackmun wrote in the majority opinion:\nThe normal 266-day human gestation period is so short that the pregnancy will come to term before the usual appellate process is complete. If that termination makes a case moot, pregnancy litigation seldom will survive much beyond the trial stage, and appellate review will be effectively denied. Our law should not be that rigid.\nBy contrast, in \"McCorvey v. Hill\", 2004, the case failed to proceed based on being moot, without standing and out of time.\nThe Court cited \"Southern Pacific Terminal Co. v. ICC\", 219 U.S. https:// (1911), which had held that a case was not moot when it presented an issue that was \"capable of repetition, yet evading review\". Perhaps in response to increasing workloads at all levels of the judiciary, the recent trend in the Supreme Court and other U.S. courts has been to construe this exception rather narrowly.\nMany cases fall under the \"capable of repetition\" doctrine; however, because there is a review process available under most circumstances, the exception to declaring mootness did not apply to such cases. In \"Memphis Light, Gas &amp; Water Div. v. Craft\", 436 U. S. 1, 8\u20139 (1978), the court noted that claims for damages save cases from mootness.\nClass action representatives.\nWhere a class action lawsuit is brought, with one named plaintiff actually representing the interests of many others, the case will not become moot even if the named plaintiff ceases to belong to the class that is seeking a remedy. In \"Sosna v. Iowa\", 419 U.S. https:// (1975), the plaintiff represented a class that was challenging an Iowa law that required persons to reside there for a year before seeking a divorce in Iowa's courts. The Supreme Court held that, although the plaintiff successfully divorced in another state, her attorneys could continue to competently advance the interests of other members of the class.\nAbuse of mootness doctrine not acceptable.\nConstruction of a project without regulatory compliance cannot be used to moot a court case challenge simply because construction has been completed. Since remedies remain available even long after the project has been completed, the case remains non-moot.\nFor example, where an Environmental Impact Statement (EIS) was challenged, completion of the project construction could not be used to evade regulatory compliance with the National Environmental Policy Act (NEPA), as the 9th Circuit Court explained:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nU.S. state courts.\nThe U.S. state courts are not subject to the Article III limitations on their jurisdiction, and some state courts are permitted by their local constitutions and laws to render opinions in moot cases where the establishment of a legal precedent is desirable. They may also establish exceptions to the doctrine. For instance, in some state courts the prosecution can lodge an appeal after a defendant is acquitted: although the appellate court cannot set aside a not-guilty verdict due to double jeopardy, it can issue a ruling as to whether a trial court's ruling on a particular issue during the trial was erroneous. This opinion will then be binding on future cases heard by the courts of that state.\nSome U.S. states also accept certified questions from the federal courts or the courts of other states. Under these procedures, state courts can issue opinions, usually for the purpose of clarifying or updating state law, in cases not actually pending in those courts.\nOutside the U.S..\nAlthough free from the U.S. Constitutional limitation, Canada has recognized that considerations of judicial economy and comity with the legislative and executive branch may justify a decision to dismiss an allegedly moot case, as deciding hypothetical controversies is tantamount to legislating. Considerations of the effectiveness of advocacy involved in the adversarial system and the possibility of recurrence of an alleged constitutional violation may sway the court. Additionally, the federal and provincial governments can ask for advisory opinions in hypothetical scenarios, termed reference questions, from their respective highest courts.\nMoot point.\nThe phrase 'moot point' refers (in American English) to an issue that is irrelevant to a subject being discussed or (in British English) to one that is debatable. Due to the relatively uncommon usage of the word moot, and because \"moot\" and \"mute\" are homophones in some pronunciations, this is sometimes erroneously rendered as \"mute point\". A point which was raised by lawyers but \"moot\" in the opinion of the judge arose in the case of \"The HALO Trust v Secretary of State for International Development\" in 2011, where the court is required to balance the seriousness of the issues to be resolved against the balance of convenience in allowing matters to stand. The point which was moot concerned what should happen if \"the seriousness of the issues\" and \"the balance of convenience\" were exactly equal.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49313", "revid": "300", "url": "https://en.wikipedia.org/wiki?curid=49313", "title": "Time constraint (law)", "text": ""}
{"id": "49315", "revid": "5662528", "url": "https://en.wikipedia.org/wiki?curid=49315", "title": "Default (law)", "text": "Failure to do something required by law (e.g. pay a debt or appear in legal proceedings)\nIn law, a default is the failure to do something required by law or to comply with a contractual obligation. Legal obligations can arise when a response or appearance is required in legal proceedings, after taking out a loan, or as agreed in a contract; failure to carry them out puts one in defaults of the obligations.\nThe concept of a \"deliberate default\" was considered in a UK legal case determined in 2010, \"De Beers UK Ltd. v Atos Origin It Services UK Ltd.\", where a contract had referred to this term. Edwards-Stuart J described \"deliberate default\" as meaning, in his view, &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;a default that is deliberate, in the sense that the person committing the relevant act knew that it was a default (i.e. in this case a breach of contract). I consider that it does not extend to recklessness and is therefore narrower than wilful misconduct (although the latter will embrace deliberate default). Before the De Beers case there was little judicial guidance on the meaning of \"deliberate default\".\nThe same term (\"deliberate defaulters\") has been used by Her Majesty's Revenue and Customs (HMRC) in the UK to describe \"people who deliberately get their tax affairs wrong\".\nDefault in legal proceedings.\nFailure to appear at a required time in legal proceedings can constitute a default.\nIn the United States, for example, when a party has failed to file meaningful response to pleadings within the time allowed, with the result that only one side of a controversy has been presented to the court, the party who has pleaded a claim for relief and received no response may request entry of default. In some jurisdictions the court may proceed to enter judgment immediately: others require that the plaintiff file a notice of intent to take the default judgment and serve it on the unresponsive party. If this notice is not opposed, or no adequate justification for the delay or lack of response is presented, then the plaintiff is entitled to judgment in his favor. Such a judgment is referred to as a \"default judgment\" and, unless otherwise ordered, has the same effect as a judgment entered in a contested case.\nIt is possible to vacate or remove the default judgment, depending on the particular state's law.\nEntry of default in the United States district courts is governed by Rule 55 of the Federal Rules of Civil Procedure.\nFinancial default.\nA common type of default is failure to meet the financial obligations of a loan. This can occur due to inability to pay, or voluntarily in a strategic default. When the debtor is a government, it is called a sovereign default.\nA notice of default is a notification given to a borrower stating that a payment has not been made by the predetermined deadline, or is otherwise in default on the mortgage contract. Other ways a borrower may be in default include not providing proper insurance coverage for the property, or not paying due property taxes as agreed. It dictates that if the money owed (plus an additional legal fee), or other breach(es) are not paid/remedied in a given time, the lender may choose to foreclose the borrower's property. Any other people who may be affected by the foreclosure may also receive a copy of the notification.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49316", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=49316", "title": "Clare Martin", "text": "Australian politician\nClare Majella Martin (born 15 June 1952) is a former Australian journalist and politician. She was elected to the Northern Territory Legislative Assembly in a shock by-election win in 1995. She was appointed Opposition Leader in 1999, and won a surprise victory at the 2001 territory election, becoming the first Labor Party (ALP) and first female Chief Minister of the Northern Territory. At the 2005 election, she led Territory Labor to the second-largest majority government in the history of the Territory, before resigning as Chief Minister on 26 November 2007.\nEarly life.\nMartin was one of ten children. Her parents were Catholics and Democratic Labor Party supporters. Her uncle, Kevin Cairns, was a Liberal minister and MP in the McMahon government, but the family was not inclined towards his conservative politics. Martin's ancestry includes the Coughlin family, which also had NSW's first female statistician and the noted test cricketer Victor Trumper. The family was originally from County Offaly, Ireland, until the Cromwell invasion, then left County Cork in the 1850s just after the Great Famine. After attending Loreto Normanhurst, Martin graduated from the University of Sydney in 1975 with a Bachelor of Arts degree, in which her major study was Music.\nPre-political career.\nHaving spent time in London and other overseas cities, she began working as a typist for the Australian Broadcasting Corporation in Sydney in 1978. In 1979, she became a trainee reporter. After several years, she began to take an interest in presenting, but was told that she would not be given a position in Sydney unless she had experience elsewhere . In February 1983, Martin was then offered a six-month position presenting a morning radio show in Darwin for the ABC Radio station 5DR.\nShe had little intention of staying there, and briefly returned to Canberra in May 1983, before being offered a job in Sydney. However, at the same time, Martin's partner was offered a partner's position at the law firm he had worked in Darwin. He liked living in Darwin and was keen to take up the position, so Martin agreed to decline the Sydney job and return to Darwin in May 1985 where she gained another position on an ABC Radio morning show.\nIn 1986, Martin made the move to television, as the presenter of \"The 7.30 Report\" until 1988. After returning from long service leave where she cared for her two young children, Martin returned to work in 1990 to work on ABC Radio's morning program.\nPolitical career.\nMartin had been interested in political journalism for some years, although she was not a member of any party, believing that party affiliation compromises journalistic integrity. In 1994, she was approached to contest the Darwin Legislative Assembly seat of Casuarina for the Labor Party at the 1994 election. However, she was defeated by Country Liberal Party candidate Peter Adamson. She soon resigned from the party and returned to journalism, but when former CLP Chief Minister Marshall Perron resigned from his Darwin seat of Fannie Bay, Martin opted to contest the ensuing by-election as the Labor candidate. Fannie Bay, like most Darwin electorates, had been a CLP stronghold; Perron held it with a majority of 8 percent. However, in a considerable upset, Martin went on to win the seat by 69 votes, becoming one of only two ALP MLAs in Darwin.\nMartin worked hard to retain her seat at the 1997 election, and was successful, holding Fannie Bay despite a heavy defeat for the ALP. She subsequently served as Shadow Minister for Lands under then leader Maggie Hickey. When Hickey unexpectedly resigned in February 1999, Martin was in a position to succeed her, and was soon elected party leader, and hence Opposition Leader. She soon emerged as a vocal critic of the Burke government's policy of mandatory sentencing, and began preparing the ALP for the next election, which was then two years away.\nTerm as Chief Minister.\nMartin faced her first electoral test as leader at the 2001 election. At the time, the Country Liberal Party had held office for 27 years, and Labor had never come particularly close to government. Indeed, it had never managed to win more than nine seats at any election. However, the ALP was coming off a particularly successful eighteen months, and Martin ran a skilled campaign. She was also able to take advantage of a number of gaffes made by then-Chief Minister Denis Burke, such as the decision to preference One Nation over the ALP \u2013 which lost the CLP a number of votes in crucial Darwin seats. The election also came during a bad time for the federal Coalition government, which was under fire for introducing a GST after previously vowing not to do so.\nDespite this, most commentators were predicting the CLP would be returned for a ninth term in government, albeit with a reduced majority. However, in a shock result, Labor scored an eight-seat swing, achieving majority government by one seat. It did so on the strength of an unexpected Labor wave in Darwin. Labor had gone into the election holding only two seats in the capital\u2014those of Martin and Paul Henderson\u2014and had never held more than two seats in Darwin at any time. In the 2001 election, however, Labor took all but one seat in Darwin, including all seven seats in the northern part of the city. Darwin's northern suburbs are somewhat more diverse than the rest of the city. In the process, they ousted four sitting MLAs; Labor had not unseated a CLP incumbent since 1980. Although the CLP won a bare majority of the two-party vote, Labor's gains in Darwin were enough to make Martin the first ALP and first female Chief Minister in the history of the Northern Territory. Martin herself was reelected with a healthy swing of 9.2 percent in Fannie Bay, turning it into a safe Labor seat in one stroke.\nAs Chief Minister, Martin immediately set about making changes, repealing the territory's controversial mandatory sentencing laws, and introducing freedom of information legislation, which had been neglected during the CLP's 27-year rule. \nAboriginal issues.\nAlthough Martin appointed Aboriginal Territorians to her cabinet, she has been criticised for not improving the lot of her Aboriginal constituents, who on average have a life expectancy well below that of white Australians. A respected commentator in \"The Bulletin\" suggested that she had gone slow on Aboriginal issues because she feared a white backlash that could have resulted in her government being toppled.\nThe life expectancy of the Northern Territory's Aboriginal citizens did not increase markedly during Martin's administration. Alcohol abuse continued to be a major issue in Aboriginal communities and third-world diseases like trachoma could be seen in remote Aboriginal townships. However, in 2006, Martin rejected accusations by John Howard and Federal Indigenous Affairs Minister, Mal Brough, that her government had been underfunding Aboriginal communities. A summit between the federal and territory governments was proposed by Mal Brough in May 2006, but this was snubbed by Martin.\nMartin was critical of the Federal Government's intervention in Aboriginal communities as announced in 2007. She opposed certain aspects of the intervention such as removal of the permit system. In response, the Federal Government rejected the Territory's argument, saying it was essential to remove artificial barriers to Aboriginal townships that prevent the measures needed to improve living conditions for Indigenous children\nAchievements.\nIn the longer term, she oversaw the completion of the Adelaide-Darwin railway, which had begun under the Burke government, and vowed to resurrect the stalled statehood movement. She also managed to markedly boost the ALP's standing among the electorate, as seen in the 2003 Katherine by-election, which saw a major swing to the party.\nBy 2005, the Northern Territory, under Martin's leadership, had achieved the following:\nAs Chief Minister, Martin led the ALP to the 2005 election, which was their first as an incumbent government in the Territory. Martin campaigned largely on law and order issues. It was predicted that the ALP would win a relatively narrow victory. However, in a result that had not been predicted by any commentators or even the most optimistic Labor observers, Martin led the ALP to a smashing victory. The final result gave 19 seats to the ALP, 4 to the opposition CLP and 2 to independents. The ALP won six seats from the CLP, four of which they had never won before in any election. Two of them were in Palmerston, an area where Labor had never previously come close to winning. In the most unexpected victory of all, the ALP even managed to unseat the Opposition Leader and former Chief Minister, Burke, in his own Palmerston-area electorate. Labor won the second-largest majority government in the history of the Territory, bettered only by the CLP's near-sweep of the Legislative Assembly at the first elections, in 1974.\nOn 10 September 2007, Queensland Premier Peter Beattie announced he would leave politics that week. This left Martin as Labor's longest-serving current state or territory leader, and as the longest-serving state or territory head of government in Australia, until she herself announced her resignation on 26 November 2007.\nResignation.\nOn 26 November 2007, Clare Martin and her deputy Syd Stirling announced their resignations at a media conference in Darwin. Education Minister Paul Henderson was elected as the new leader and Chief Minister by the ALP caucus.\nPost-political career.\nIn 2008, Martin became chief executive officer of the Australian Council of Social Service, based in Sydney. In August 2010 she returned to the Northern Territory to become a Professorial Fellow in the Public and Social Policy Research Institute at Charles Darwin University.\nIn June 2019, she was appointed as an Officer of the Order of Australia for distinguished service to the people and Legislative Assembly of the Northern Territory, and as a community advocate.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49322", "revid": "6196463", "url": "https://en.wikipedia.org/wiki?curid=49322", "title": "Rajmund Kanelba", "text": "Polish painter\nRaymond Kanelba (1897\u20131960), also known as \"Rajmund Kanelba\", was a 20th-century Polish painter.\nHe was born in Warsaw and educated there as well as in Vienna and Paris. He was strongly influenced by the \u00e9cole de Paris but with rather realistic and anti-impressionist style. In 1926 his works were on display in Salon des Ind\u00e9pendants and Salon d'Automne and in 1952 he had a large exposition of his paintings in New York City.\nHe was especially admired for his portraits of women, as well as his paintings of flowers, interiors, and street scenes. Some of his works were signed as Raymond Kanelba.\nIn 1955, the painter traveled to England, where he undertook the most important commission of his career \u2013 at the request of the Grenadier Regiment, he painted a portrait of Queen Elizabeth II.\nRajmund Kanelba lived most of his life in France but died in London in July 1960.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49324", "revid": "39151475", "url": "https://en.wikipedia.org/wiki?curid=49324", "title": "Unit interval", "text": "Closed interval [0,1] on the real number line\nIn mathematics, the unit interval is the closed interval [0,1], that is, the set of all real numbers that are greater than or equal to 0 and less than or equal to 1. It is often denoted \"I\" (capital letter I). In addition to its role in real analysis, the unit interval is used to study homotopy theory in the field of topology.\nIn the literature, the term \"unit interval\" is sometimes applied to the other shapes that an interval from 0 to 1 could take: (0,1], [0,1), and (0,1). However, the notation \"I\" is most commonly reserved for the closed interval [0,1].\nProperties.\nThe unit interval is a complete metric space, homeomorphic to the extended real number line. As a topological space, it is compact, contractible, path connected and locally path connected. The Hilbert cube is obtained by taking a topological product of countably many copies of the unit interval.\nIn mathematical analysis, the unit interval is a one-dimensional analytical manifold whose boundary consists of the two points 0 and 1. Its standard orientation goes from 0 to 1.\nThe unit interval is a totally ordered set and a complete lattice (every subset of the unit interval has a supremum and an infimum).\nCardinality.\nThe \"size\" or \"cardinality\" of a set is the number of elements it contains.\nThe unit interval is a subset of the real numbers formula_1. However, it has the same size as the whole set: the cardinality of the continuum. Since the real numbers can be used to represent points along an infinitely long line, this implies that a line segment of length 1, which is a part of that line, has the same number of points as the whole line. Moreover, it has the same number of points as a square of area 1, as a cube of volume 1, and even as an unbounded \"n\"-dimensional Euclidean space formula_2 (see Space filling curve).\nThe number of elements (either real numbers or points) in all the above-mentioned sets is uncountable, as it is strictly greater than the number of natural numbers.\nOrientation.\nThe unit interval is a curve. The open interval (0,1) is a subset of the positive real numbers and inherits an orientation from them. The orientation is reversed when the interval is entered from 1, such as in the integral formula_3 used to define natural logarithm for \"x\" in the interval, thus yielding negative values for logarithm of such \"x\". In fact, this integral is evaluated as a signed area yielding \"negative area\" over the unit interval due to reversed orientation there.\nGeneralizations.\nThe interval [-1,1], with length two, demarcated by the positive and negative units, occurs frequently, such as in the range of the trigonometric functions sine and cosine and the hyperbolic function tanh. This interval may be used for the domain of inverse functions. For instance, when \ud835\udf03 is restricted to [\u2212\u03c0/2, \u03c0/2] then formula_4 is in this interval and arcsine is defined there.\nSometimes, the term \"unit interval\" is used to refer to objects that play a role in various branches of mathematics analogous to the role that [0,1] plays in homotopy theory. For example, in the theory of quivers, the (analogue of the) unit interval is the graph whose vertex set is formula_5 and which contains a single edge \"e\" whose source is 0 and whose target is 1. One can then define a notion of homotopy between quiver homomorphisms analogous to the notion of homotopy between continuous maps.\nFuzzy logic.\nIn logic, the unit interval [0,1] can be interpreted as a generalization of the Boolean domain {0,1}, in which case rather than only taking values 0 or 1, any value between and including 0 and 1 can be assumed. Algebraically, negation (NOT) is replaced with 1 \u2212 \"x\"; conjunction (AND) is replaced with multiplication (\"xy\"); and disjunction (OR) is defined, per De Morgan's laws, as 1 \u2212 (1 \u2212 \"x\")(1 \u2212 \"y\").\nInterpreting these values as logical truth values yields a multi-valued logic, which forms the basis for fuzzy logic and probabilistic logic. In these interpretations, a value is interpreted as the \"degree\" of truth \u2013 to what extent a proposition is true, or the probability that the proposition is true."}
{"id": "49325", "revid": "1308210293", "url": "https://en.wikipedia.org/wiki?curid=49325", "title": "Houyhnhnm", "text": "Fictional race of horses\nHouyhnhnms are a fictional race of intelligent horses described in the last part of Jonathan Swift's satirical 1726 novel \"Gulliver's Travels\". The name is pronounced either or . Swift apparently intended all words of the Houyhnhnm language to echo the neighing of horses.\nDescription.\nGulliver's visit to the Land of the Houyhnhnms is described in Part IV of his \"Travels\", and its location illustrated on the map at the start of Part IV.\nThe map shows Houyhnhnms Land to be south of Australia; it indicates Edels Land and Lewins Land to the north, and Nuyts Land to the north-east, on the mainland with the islands of St Francis and St Pieter further east, and Sweers, Maatsuyker and De Wit islands to the east. The map is somewhat careless with the scale, however; Edels Land to Lewins Land are shown adjacent, while in reality they are some 1000\u00a0km apart, while the sweep of the Great Australian Bight, from Cape Leeuwin, Australia's south-westerly point to the Maatsuyker Islands, off the southern tip of Tasmania, is over 3000\u00a0km.\nGulliver describes the land as \"divided by long rows of trees, not regularly planted but naturally growing\", with a \"great plenty of grass, and several fields of oats\".\nThe Houyhnhnms are rational equine beings and are masters of the land, contrasting strongly with the Yahoos, savage humanoid creatures who are no better than beasts of burden, or livestock. Whereas the Yahoos represent all that is bad about humans, Houyhnhnms have a settled, calm, reliable and rational society. Gulliver much prefers the Houyhnhnms' company to the Yahoos', even though the latter are biologically closer to him.\nInterpretation.\nIt is possible to interpret the Houyhnhnms in a number of different ways. One interpretation could be a sign of Swift's liberal views on race, or one could regard Gulliver's preference (and his immediate division of Houyhnhnms into color-based hierarchies) as absurd and the sign of his self-deception. It is now generally accepted that the story involving the Houyhnhnms embody a wholly pessimistic view of the place of man and the meaning of his existence in the universe. In a modern context the story might be seen as presenting an early example of animal rights concerns, especially in Gulliver's account of how horses are cruelly treated in his society and the reversal of roles. The story is a possible inspiration for Pierre Boulle's novel \"Planet of the Apes\".\nBook IV of \"Gulliver's Travels\" is the keystone, in some ways, of the entire work, and critics have traditionally answered the question whether Gulliver is insane (and thus just another victim of Swift's satire) by questioning whether or not the Houyhnhnms are truly admirable. Gulliver loves the land and is obedient to a race that is not like his own. The Houyhnhnm society is based upon reason, and only upon reason, and therefore the horses practice eugenics based on their analyses of benefit and cost. They have no religion and their sole morality is the defence of reason, and so they are not particularly moved by pity or a belief in the intrinsic value of life. Gulliver himself, in their company, builds the sails of his skiff from \"Yahoo skins\". The Houyhnhnms' lack of passion surfaces during the scheduled visit of \"a friend and his family\" to the home of Gulliver's master \"upon some affair of importance\". On the day of the visit, the mistress of his friend and her children arrive very late. She made no excuses \"first for her husband\" who had died just that morning and she had to remain to make the proper arrangements for a \"convenient place where his body should be laid\". Gulliver remarked that \"she behaved herself at our house as cheerfully as the rest\". A further example of the lack of humanity and emotion in the Houyhnhnms is that their laws reason that each couple produce two children, one male and one female. In the event that a marriage produced two offspring of the same sex, the parents would take their children to the annual meeting and trade one with a couple who produced two children of the opposite sex. This was viewed as his spoofing and or criticising the notion that the \"ideal\" family produces children of both sexes. George Orwell viewed the Houyhnhnm society as one whose members try to be as close to dead as possible while alive and matter as little as possible in life and death.\nOn one hand, the Houyhnhnms have a seemingly orderly and peaceful society. They have a philosophy and a language that is entirely absent of political and ethical basis (or a supposed lack need for such). They have no word for a \"lie\" (and must substitute a circumlocution: \"to say a thing which is not\", thus technically being able to have terms for lies). They also have a form of art that is derived from nature. Outside \"Gulliver's Travels\", Swift had expressed longstanding concern over the corruption of the English language, and he had proposed language reform. He had also, in \"Battle of the Books\" and in general in \"A Tale of a Tub\", expressed a preference for the Ancients (Classical authors) because their art was based directly upon nature, and not upon other art.\nOn the other hand, Swift was profoundly mistrustful of attempts at reason that resulted in either hubris (for example, the Projectors satirised in \"A Tale of a Tub\" or in Book III of \"Gulliver's Travels\") or immorality (such as the speaker of \"A Modest Proposal\", who offers an seemingly entirely logical and wholly immoral and amoral proposal for cannibalism). The Houyhnhnms embody both the apparently good and the bad side of reason, for they have the \"pure\" language Swift wished for and the amorally rationalist approach to solving the problems of humanity (Yahoos); the extirpation of the Yahoo population by the horses is very like the aspirations of the speaker of \"A Modest Proposal\".\nIn the shipping lanes he is rescued by a Portuguese sea captain, a level-headed individual albeit full of concern for others, whose temperament at one level appears intermediate between the calm, rational Houyhnhnms of Houyhnhnmland and the norm of corrupt, European humanity, which Gulliver no longer distinguishes from Houyhnhnmland's wild Yahoos. Gulliver can speak with him, and though now disaffected from all humanity, he began to tolerate his company. Gulliver is returned to his home and family, finds their smell and look intolerable and all his countrymen no better than \"Yahoos\", purchases and converses with two stabled horses, tolerating the stable boy, and assures the reader of his account's utter veracity.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49326", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=49326", "title": "KStars", "text": "Graphical desktop planetarium\nKStars is a free and open-source planetarium program built using the KDE Frameworks. It is available for Linux, BSD, macOS, and Microsoft Windows. A light version of KStars is available for Android devices. It provides an accurate graphical representation of the night sky, from any location on Earth, at any date and time. The display includes up to 100 million stars (with additional addons), 13,000 deep sky objects, constellations from different cultures, all 8 planets, the Sun and Moon, and thousands of comets, asteroids, satellites, and supernovae. It has features to appeal to users of all levels, from informative hypertext articles about astronomy, to robust control of telescopes and CCD cameras, and logging of observations of specific objects.\nKStars supports adjustable simulation speeds in order to view phenomena that happen over long timescales. For astronomical calculations, Astrocalculator can be used to predict conjunctions, lunar eclipses, and perform many common astronomical calculations. The following tools are included:\nKStars has been packaged by many Linux/BSD distributions, including Red Hat Linux, OpenSUSE, Arch Linux, and Debian. Some distributions package KStars as a separate application, some just provide a kdeedu package, which includes KStars. KStars is distributed with the KDE Software Compilation as part of the kdeedu \"Edutainment\" module.\nKStars participated in Google Summer of Code in 2008, 2009, 2010, 2011 2012, 2015 and 2016. It has also participated in the first run of ESA's Summer of Code in Space in 2011.\nIt has been identified as one of the three best \"Linux stargazing apps\" in a Linux.com review.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49329", "revid": "20612", "url": "https://en.wikipedia.org/wiki?curid=49329", "title": "Strait of Juan de Fuca", "text": "Strait between Vancouver Island and the Olympic Peninsula\nThe Strait of Juan de Fuca (officially named Juan de Fuca Strait in Canada) is a body of water about long that is the Salish Sea's main outlet to the Pacific Ocean. The international boundary between Canada and the United States runs down the centre of the Strait.\nIt was named in 1787 by the maritime fur trader Charles William Barkley, captain of \"Imperial Eagle\", for Juan de Fuca, the Greek navigator who sailed in a Spanish expedition in 1592 to seek the fabled Strait of Ani\u00e1n. Barkley was the first recorded person to find the strait, unless Juan de Fuca's story was true.\nSamuel Bawlf posited in 2003 that \"Fuca's story was nothing more than a fabrication designed to extract money from the English government\". The story would have been based on Francis Drake's 1579 exploration of the Northwest Passage at its western end. Drake would therefore have been more than likely the first European to ever sight the entry of the strait but would also have been the first to sail through when heading west after rounding Vancouver Island, sailing through the Strait of Georgia and sighting the entry to Puget Sound. Much of Bawlf's provocative thesis was based on the geographical information that started to leak following Drake's return to England and to show up in subsequent maps such as Ortelius's. Fuca's story would have originated from a pilot named Morera, part of Drake's expedition, who miraculously returned on his own before getting arrested by the Spanish.\nWhile the U.S. Geological Survey Geographic Names Information System says John Meares named the strait in 1788, most sources say it was Barkley in 1787, for example: the BC Geographical Names Information System; Washington Secretary of State; \"Historical Atlas of the Pacific Northwest: Maps of exploration and Discovery\"; and \"The Nootka Connection: Europe and the Northwest Coast 1790\u20131795\". It is well established that Meares tried to take credit for much of Barkley's work. The strait was explored in detail between 1789 and 1791 by Manuel Quimper, Jos\u00e9 Mar\u00eda Narv\u00e1ez, Juan Carrasco, Gonzalo L\u00f3pez de Haro, and Francisco de Eliza.\nDefinition.\nThe United States Geological Survey defines the Strait of Juan de Fuca as a channel. It extends east from the Pacific Ocean between Vancouver Island, British Columbia, and the Olympic Peninsula, Washington, to Haro Strait, San Juan Channel, Rosario Strait, and Puget Sound. The Pacific Ocean boundary is formed by a line between Cape Flattery and Tatoosh Island, Washington, and Carmanah Point (Vancouver Island), British Columbia. Its northern boundary follows the shoreline of Vancouver Island from Carmanah Point to Gonzales Point, then follows a continuous line east to Seabird Point (Discovery Island), British Columbia, Cattle Point (San Juan Island), Washington, Iceberg Point (Lopez Island), Point Colville (Lopez Island), and then to Rosario Head (Fidalgo Island). The eastern boundary runs south from Rosario Head across Deception Pass to Whidbey Island, then along the western coast of Whidbey Island to Point Partridge, then across Admiralty Inlet to Point Wilson (Quimper Peninsula). The northern coast of the Olympic Peninsula forms the southern boundary of the strait. In the eastern entrance to the Strait, the Race Rocks Archipelago is in the high current zone halfway between Port Angeles, Washington, and Victoria, BC.\nClimate.\nLike the rest of the Salish Sea and surrounding regions, the climate of the Strait is disputed, with the K\u00f6ppen system classifying it as Mediterranean, but most regional climatologists preferring oceanic. While the climate is mostly oceanic in nature, the dry summers result in the Mediterranean classification in the K\u00f6ppen system. Rainfall ranges from over (temperate rainforest) conditions at the west end to as little as at the east end, near Sequim.\nBecause it is exposed to the generally westerly winds and waves of the Pacific, seas and weather in Juan de Fuca Strait are, on average, rougher than in the more protected waters inland, thereby resulting in a number of small-craft advisories, gale warnings, and storm warnings.\nFerries.\nAn international vehicle ferry, the , crosses the Strait from Port Angeles, Washington, to Victoria, British Columbia, several times each day. It began operating in 1959, replacing an earlier ferry, and remains privately owned; the \"Coho\" carried 475,000 passengers and 130,000 vehicles in 2018. A passenger-only ferry on the same route, named the Victoria Express, operated from 1990 to 2011. Victoria is also the terminus of the Victoria Clipper, a passenger-only ferry from Seattle. Sidney, located northeast of Victoria, is served by a seasonal extension the Washington State Ferries system's route serving the San Juan Islands and Anacortes, Washington.\nBoundary dispute.\nThis strait remains the subject of a maritime boundary dispute between Canada and the United States. The dispute is only over the seaward boundary extending west from the mouth of the strait. The maritime boundary within the strait is not in dispute. Both governments have proposed a boundary based on the principle of equidistance, but with different basepoint selections, resulting in small differences in the line. Resolution of the issue should be simple, but has been hindered because it might influence maritime boundary issues between Canada and the United States. In addition, the government of British Columbia has rejected both equidistant proposals, instead arguing that the Juan de Fuca submarine canyon is the appropriate \"geomorphic and physiogeographic boundary\". The proposed equidistant boundary currently marks the northern boundary of the Olympic Coast National Marine Sanctuary. British Columbia's position is based on the principle of natural prolongation which developed in international law. It poses a dilemma for the federal government of Canada. If Canada holds that the principle of natural prolongation applies to the Juan de Fuca Canyon on its Pacific Ocean coast, the assertion could undermine Canada's argument in the \"Gulf of Maine\" boundary dispute. In this Atlantic Ocean context, Canada favours an outcome based on the principle of equidistance.\nSalish Sea.\nIn March 2008, the Chemainus First Nation proposed renaming the strait the \"Salish Sea\", an idea that reportedly met with approval by British Columbia's Aboriginal Relations Minister Mike de Jong, who pledged to put it before the BC cabinet for discussion. Making \"Salish Sea\" official required a formal application to the Geographical Names Board of Canada. A parallel American movement promoting the name had a different definition, combining of the Strait of Juan de Fuca and Puget Sound as well as the Strait of Georgia and related waters under the more general name \"Salish Sea\". This latter definition was made official in 2009 by geographic boards of Canada and the United States.\nIn October 2009, the Washington State Board of Geographic Names approved the Salish Sea toponym, not to replace the names of the Strait of Georgia, Puget Sound, and Strait of Juan de Fuca, but instead as a collective term for all three. The British Columbia Geographical Names Office passed a resolution only recommending that the name be adopted by the Geographical Names Board of Canada, should its US counterpart approve the name-change. The United States Board on Geographic Names approved the name on November 12, 2009.\nCounties and regional districts.\nCounties along the Strait of Juan de Fuca:\nRegional districts along the Strait of Juan de Fuca:\nFauna.\nCertain groups of seabirds called common murre migrate north by swimming. Some Pacific Coast murres paddle north to the sheltered bays of the Strait of Juan de Fuca to feed on herring and other small fish.\nHumpback whales can be observed near the western end of the Strait of Juan de Fuca, mostly from June to November, especially in areas near Neah Bay and La Push. There is a resident (non-nomadic) population of killer whale in the Strait and surrounding waters, where they feed on spawning Chinook salmon. The migrating, so-called \"transient\" populations of killer whale often prey on the California sea lion and Steller's sea lion, in addition to the gray whale, another cetacean with both residential and nomadic groups in the Strait. As migratory gray whales swim between Baja California and Alaska each year, they will often be seen in the Strait of Juan de Fuca, feeding on small marine creatures on the seafloor, and using the Strait as a sort of \"rest stop\" on their long journeys. They also socialize with and encounter the local, non-migratory gray whales in the Strait; from a conservation standpoint, this is vital for the whales' ability to meet potential new mates, form new bonds, and ultimately create new bloodlines. This yearly influx of whales is best observed between March and May, at the peak of the migration times.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49330", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=49330", "title": "Spellevator", "text": "1988 educational video game\nSpellevator is an educational video game for the Apple II published by MECC in 1988.\nGameplay.\nThe player controls a dust bunny, which is chased by several vacuum cleaners with different movement patterns. The objective of the level is to grab all the letters and exit through the upper left corner. The player can pass through an unoccupied elevator (some vacuum cleaners use elevators also) by correctly answering a spelling or vocabulary question. Once one completes a level, the player can receive a bonus by correctly unscrambling the letters one grabbed into a word.\n\"Spellevator\" has a utility on the disk's flipside that let a user create a word list and save it to any ProDOS formatted floppy disk. This way, teachers could customize the game to fit their own particular vocabulary lists.\nLegacy.\n\"Spellevator\" was followed by \"Spellevator Plus\" for additional platforms.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49331", "revid": "18953656", "url": "https://en.wikipedia.org/wiki?curid=49331", "title": "Patchwork", "text": "Needlework with fabric pieces sewn together\nPatchwork or \"pieced work\" is a form of needlework that involves sewing together pieces of fabric into a larger design. The larger design is usually based on repeating patterns built up with different fabric shapes (which can be different colors). These shapes are carefully measured and cut, basic geometric shapes making them easy to piece together. \nUses.\nPatchwork is often used to make quilts, but it can also be used to make rugs, bags, wall-hangings, warm jackets, cushion covers, skirts, waistcoats and other items of clothing. Some textile artists work with patchwork, often combining it with embroidery and other forms of stitchery.\nWhen used to make a quilt, this larger patchwork or pieced design becomes the \"top\" of a three-layered quilt, the middle layer being the batting and the bottom layer the backing. To keep the batting from shifting, a patchwork or pieced quilt is often quilted by hand or machine using a running stitch in order to outline the individual shapes that make up the pieced top, or the quilting stitches may be random or highly ordered overall patterns that contrast with the patchwork composition.\nHistory.\nEvidence of patchwork\u2014piecing small pieces of fabric together to create a larger piece and quilting layers of textile fabrics together\u2014has been found throughout history. Patchwork was used by ancient Egyptians for their clothes, wall decorations, draperies and furniture, with oldest depictions from 5,500 years ago (3,400 BCE). Chinese patchwork is storied to have begun by emperor Liu Yu of the Liu Song dynasty. Earliest preserved pieces have been dated from the early Middle Ages, where among other uses layers of quilted fabric were used in the construction of armor\u2014this kept the soldiers warm and protected. Japanese armor was made in a similar fashion.\nUsing this technique, quilts began to appear in households of the 11th to 13th centuries. As the European climate became colder around this time, the incidence of the use of bed quilts rose, and so developed the practice of embellishing a simple cloth through the creation of pattern and design, alongside the development of decorative quilting. The tradition of making quilts in this fashion was taken to America by the Pilgrims.\nThe United States.\nPatchwork enjoyed a widespread revival during the Great Depression as a way to recycle worn clothing into warm quilts. Even very small and worn pieces of material are suitable for use in patchwork, although crafters today more often use new 100% cotton fabrics as the basis for their designs. In the US, patchwork declined after World War II but was again revived during the American bicentennial. In the past, hand quilting was often done in a group around a frame. Instead of quilting, the layers are sometimes tied together at regular intervals with pieces of yarn, a practice known as tying or knotting, and which produces a \"comforter\".\nPopularity.\nThe 2003 Quilting in America survey estimated that the total value of the American quilting industry was $2.7 billion. International quilting exhibitions attract thousands of visitors, while countless smaller exhibitions are held every weekend in local regions. Active cyber-quilting communities abound on the web; books and magazines on the subject are published in the hundreds every year; and there are many active local quilting guilds and shops in different countries. \"Quilt Art\" is established as a legitimate artistic medium, with quilted works of art selling for thousands of dollars to corporate buyers and galleries. Quilt historians and quilt appraisers are re-evaluating the heritage of traditional quilting and antique quilts, while superb examples of antique quilts are purchased for large sums by collectors and museums. The American Quilt Study Group is active in promotion of research on the history of quilting.\nAsia.\nIn India Kantha originated from the Sanskrit word \"kontha\", which means rags, as the blankets are made out of rags using different scrap pieces of cloth. Nakshi kantha consisting of a running (embroidery) stitch, similar to the Japanese Sashiko is used for decorating and reinforcing the cloth and sewing patterns. Katab work called in Kutch. It is popularly known as Koudhi in Karnataka. Such blankets are given as gifts to newborn babies in many parts of India. Lambani tribes wear skirts with such art.\nPatchwork is also done in various parts of Pakistan, especially in the Sindh region, where they call it \"ralli\". Pakistani \"ralli\" quilts are famous all over the subcontinent even in the west. These quilts are a part of their tradition and are made by women. Now these are gaining international recognition even though they have been making them for thousands of years.\nPatchwork is also common in Azerbaijan, where it is called \"qurama\".\nEgypt.\nThe history of patchwork is not all recent. Patchwork was used by Ancient Egyptians on their clothing and walls. An Egyptian queen, Esi-Mem-Kev, who lived around 980 BCE possessed a patchwork funeral canopy which was found in her tomb.\nStructure.\nThere are three traditional structures used to construct a patchwork or pieced composition: 1) the block, 2) overall, and 3) strip piecing. Traditional patchwork has identifying names based on the arrangement of colors and shapes.\nBlocks.\nPatchwork blocks are pieced squares made up of colored shapes that repeat specific shapes to create patterns within the square or block of, say, light and dark or contrasting colors (motif). The blocks can all repeat the same pattern, or blocks can have several different patterns. The patchwork blocks are typically around . They are sewn together in stacked rows to make a larger composition. Often strips of contrasting fabric forming a lattice separate the patchwork blocks from each other. Some common patchwork block names are Log Cabin, Drunkard's Path, Bear's Paw, Tulip, and Nine Patch.\nA unique form of patchwork quilt is the crazy quilt. Crazy quilting was popular during the Victorian era (mid\u2013late 19th century). The crazy quilt is made up of random shapes of luxurious fabric such as velvets, silks, and brocades and buttons, lace, and other embellishments left over from the gowns they had made for themselves. The patchwork pieces are stitched together forming \"crazy\" or non-repeat, asymmetric compositions. Fancy embroidery embellishes the seam lines between the individual, pieced shapes. The crazy quilt was a status symbol, as only well-to-do women had a staff to do all the household work, and had the time to sew their crazy quilt. Traditionally, the top was left without lining or batting. Many surviving crazy quilts still have the newspaper and other foundation papers used for piecing.\nOverall.\nOverall patchwork designs are incrementally pieced geometric shapes stitched together to form a larger random or composed design. The colored shapes can be randomly pieced or follow a strict order to create a specific effect, e.g. value (light to dark) progressions, or checkerboard effects. Names such as Hit or Miss, Clamshell, back-stitch, needle weave, criss-cross, and Starburst identify some overall patchwork structures.\nRound pieces formed by cutting a circle of fabric, gathering the edges with a running stitch and pulling them tightly shut are known as Suffolk puffs in the United Kingdom due to the Suffolk wool used to pad them. In the United States, the pieces are called yo-yos. The origin date of this type of piecework is unknown, but it was popular in the United States during the Great Depression and in the United Kingdom after World War II. These round pieces can be joined with several stitches on the sides to connect other puffs together and form a coverlet or other items. Scrap pieces may be used, or colors may be coordinated into patterns.\nStrip piecing.\nStrip piecing involves stitching together pieces of fabric in repeat patterns into long strips and then stitching the strips together lengthwise. The patchwork strips can be alternated with strips of contrasting colors. A typical strip patchwork quilt is the http:// pattern.\nForms.\nSpecialised forms of patchwork include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49332", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=49332", "title": "Needlework", "text": "Craft of creating or decorating objects using needle\nNeedlework refers to decorative sewing and other textile handicrafts that involve the use of a needle. Needlework may also include related textile crafts like crochet (which uses a hook), or tatting, (which uses a shuttle).\nSimilar abilities often transfer well between different varieties of needlework, such as fine motor skill and knowledge of textile fibers. Some of the same tools may be used in several different varieties of needlework.\nBackground.\nDuring the Italian Renaissance, needlework products were used to demonstrate the feminine ideal within the domestic sphere. Young women of all classes learned various types of embroidery, sewing, weaving, and quilting for their respective purposes. Those of lower economic status focused on practical sewing skills for housework, while those of higher economic status could afford to invest in more decorative needlework, such as tapestries. Common themes presented in these works included religious narratives, especially Biblical tales, although scenes from classical Greek and Roman mythos were depicted as well. Outside of narrative needlework, motifs and patterns that reflected the natural world were popular, alongside the introduction of the Islamic arabesque design to Renaissance Italy. Its interlocking knots and foliage shapes were used among needleworkers and painters alike, spread across countries through pattern books. Islamic arabesques particularly found a market through its implementation into Venetian lace. The production of needlework during the Renaissance was mostly done by women. Notable of this era was the use of band samplers, on which long rows of stitches were practiced. The two most common type of stitches were tent and cross stitches.\nNeedlework was an important fact of women's identity during the Victorian age, including embroidery, netting, knitting, crochet, and Berlin wool work. A growing middle class had more leisure time than ever before; printed materials offered homemakers thousands of patterns. Women were still limited to roles in the household, and under the standards of the time a woman working on needle work while entertaining the parlor was considered beautiful. According to one publication from 1843: \"Never is beauty and feminine grace so attractive as, when engaged in the honorable discharge of household duties, and domestic cares.\"\nFancy work was distinguished from plain sewing and it was a mark of a prosperous and well-managed home to display handmade needlework. While plain sewing was often handed over to servants, even in middle class households, fancy work would often be done while entertaining guests, in the afternoons, evenings, or on Sundays. The types of goods that could be decorated with needlework techniques was limited only by the imagination: knitted boots, embroidered book covers, footstools, lampshades, sofa cushions, fans and on and on.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49333", "revid": "50688320", "url": "https://en.wikipedia.org/wiki?curid=49333", "title": "Cultural bias", "text": "Interpretation and judgement of phenomena by the standards of one's culture\nCultural bias is the tendency of individuals to interpret and judge others' experiences through the lens of one's own cultural background. It is sometimes considered a problem central to social and human sciences, such as economics, psychology, anthropology, and sociology. Some practitioners of these fields have attempted to develop methods and theories to compensate for or eliminate cultural bias. Cultural bias can be intentional or unintentional, due to the nature of these sciences, drawing conclusions about human behavior can result in unintentional cultural bias. \nCultural bias occurs when people of a culture make assumptions about conventions, including conventions of language, notation, proof and evidence. They are then accused of mistaking these assumptions for laws of logic or nature. Numerous such biases exist, concerning cultural norms for color, mate selection, concepts of justice, linguistic and logical validity, the acceptability of evidence, and taboos.\nPsychology.\nCultural bias is a phenomenon that gets its presence from differential performance of socioracial (e.g., Blacks, Whites), ethnic (e.g., Latinos/Latinas, Anglos), or national groups (e.g., Americans, Japanese) on measures of psychological constructs such as cognitive abilities, knowledge or skills (CAKS), or symptoms of psychopathology (e.g., depression). Historically, the term grew out of efforts to explain score differences on CAKS tests primarily of African American and Latino/Latina American test takers relative to their White American counterparts and concerns that test scores should not be interpreted in the same manner across those groups in the name of fairness and equality (see also Cognitive dissonance). Although the concept of cultural bias in testing and assessment also pertains to score differences and potential misdiagnoses with respect to a broader range of psychological concepts, particularly in applied psychology and other social and behavioral sciences, this aspect of cultural bias has received less attention in the relevant literature.\nCultural bias in psychological testing refers to the standardized psychological tests that are conducted to determine the level of intelligence among the test-takers. Limitations of such verbal or non-verbal intelligence tests have been observed since their introduction. Many tests have been objected to, as they produced poor results for the ethnic or racial minorities (students), as compared to the racial majorities. There is minimal evidence supporting claims of cultural bias and cross-cultural examination is both possible and done frequently. As discussed above, the learning environment, the questions posed or situations given in the test may be familiar and strange at the same time to students from different backgrounds- the type of ambiguity in which intellectual differences become apparent in individual capacities to resolve the strange-yet-familiar entity.\nEconomics.\nCultural bias in economic exchange is often overlooked.The perception of culture can directly effect commerce. A study done at the Northwestern University suggests that the cultural perception that two countries have of each other plays a large factor in the economic activity between them. The study suggests that low bilateral trust between two countries results in less trade, less portfolio investment, and less direct investment. The effect is amplified for goods, as they are more trust-intensive. A similar study published by the London School of Economics and Political Science found that investors underestimate vastly foreign markets and overestimate companies with culturally similar CEO's. Inversely, banks with culturally diverse management result in less biased views of investment and ultimately more diverse and profitable portfolios.\nSociology.\nIt is thought that societies with conflicting beliefs will more likely have cultural bias, as it is dependent on the group's standing in society in which the social constructions affect how a problem is produced. One example of cultural bias within the context of sociology can be seen in a study done at the University of California by Jane R. Mercer of how test \"validity\", \"bias\", and \"fairness\" in different cultural belief systems affect one's future in a pluralistic society. A definition of the cultural bias was given as \"the extent that the test contains cultural content that is generally peculiar to the members of one group but not to the members of another group\", which leads to a belief that \"the internal structure of the test will differ for different cultural groups\". In addition, the different types of errors made on culture-biased tests are dependent on different cultural groups. The idea progressed to the conclusion that a non-cultural-test represents the ability of a population as intended and not the abilities of a group that is not represented.\nHistory.\nHistory, as a narrative, indicates cultural bias. Stories that are passed down from generation to generation and are notoriously different based on the narrator is a prime example of cultural bias at play. Historical accounts are shaped by the surroundings, biases, and perceptions of those who interpret and retell them. For example, the narrative of Settlers \"discovering\" new land in the Americas and creating \"civilization\" were only ever historically accurate to those who learned from European ancestors. Native American descendants had different accounts, but up until recent years the former was utterly true to most of the general population. Cultural bias may also arise in historical scholarship when the standards, assumptions and conventions of the historian's own era are anachronistically used to report and to assess events of the past. The tendency is sometimes known as presentism, and is regarded by many historians as a fault to be avoided. Arthur Marwick has argued that \"a grasp of the fact that past societies are very different from our own, and... very difficult to get to know\" is an essential and fundamental skill of the professional historian; and that \"anachronism is still one of the most obvious faults when the unqualified (those expert in other disciplines, perhaps) attempt to do history.\" \nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49334", "revid": "1517", "url": "https://en.wikipedia.org/wiki?curid=49334", "title": "Culture bias", "text": ""}
{"id": "49338", "revid": "3125232", "url": "https://en.wikipedia.org/wiki?curid=49338", "title": "Bipolar junction transistor", "text": "Transistor that uses both electrons and holes as charge carriers\nA bipolar junction transistor (BJT) is a type of transistor that uses both electrons and electron holes as charge carriers. In contrast, a unipolar transistor, such as a field-effect transistor (FET), uses only one kind of charge carrier. A bipolar transistor allows a small current injected at one of its terminals to control a much larger current between the remaining two terminals, making the device capable of amplification or switching.\nBJTs use two p\u2013n junctions between two semiconductor types, n-type and p-type, which are regions in a single crystal of material. The junctions can be made in several different ways, such as changing the doping of the semiconductor material as it is grown, by depositing metal pellets to form alloy junctions, or by such methods as diffusion of n-type and p-type doping substances into the crystal. The superior predictability and performance of junction transistors quickly displaced the original point-contact transistor. Diffused transistors, along with other components, are elements of integrated circuits for analog and digital functions. Hundreds of bipolar junction transistors can be made in one circuit at a very low cost.\nBipolar transistor integrated circuits were the main active devices of a generation of mainframe and minicomputers, but computer systems now use complementary metal\u2013oxide\u2013semiconductor (CMOS) integrated circuits relying on the field-effect transistor (FET). Bipolar transistors are still used for amplification of signals, switching, and in mixed-signal integrated circuits using BiCMOS. Specialized types are used for high voltage and high current switches, or for radio-frequency (RF) amplifiers.\nHistory.\nThe bipolar point-contact transistor was invented in December 1947 at the Bell Telephone Laboratories by John Bardeen and Walter Brattain under the direction of William Shockley. The junction version known as the bipolar junction transistor (BJT), invented by Shockley in 1948, was for three decades the device of choice in the design of discrete and integrated circuits. Nowadays, the use of the BJT has declined in favor of CMOS technology in the design of digital integrated circuits. The incidental low performance BJTs inherent in CMOS ICs, however, are often utilized as bandgap voltage reference, silicon bandgap temperature sensor and to handle electrostatic discharge.\nGermanium transistors.\nThe germanium transistor was more common in the 1950s and 1960s but has a greater tendency to exhibit thermal runaway. Since germanium p-n junctions have a lower forward bias than silicon, germanium transistors turn on at lower voltage.\nEarly manufacturing techniques.\nVarious methods of manufacturing bipolar transistors were developed.\nFunction.\nBJTs exist as PNP and NPN types, based on the doping types of the three main terminal regions. An NPN transistor comprises two semiconductor junctions that share a thin p-doped region, and a PNP transistor comprises two semiconductor junctions that share a thin n-doped region. N-type means doped with impurities (such as phosphorus or arsenic) that provide mobile electrons, while p-type means doped with impurities (such as boron) that provide holes that readily accept electrons.\nCharge flow in a BJT is due to diffusion of charge carriers (electrons and holes) across a junction between two regions of different charge carrier concentration. The regions of a BJT are called \"emitter\", \"base\", and \"collector\". A discrete transistor has three leads for connection to these regions. Typically, the emitter region is heavily doped compared to the other two layers, and the collector is doped more lightly (typically ten times lighter) than the base. By design, most of the BJT collector current is due to the flow of charge carriers injected from a heavily doped emitter into the base where they are minority carriers (electrons in NPNs, holes in PNPs) that diffuse toward the collector, so BJTs are classified as \"minority-carrier devices\".\nIn typical operation, the base\u2013emitter junction is forward biased, which means that the p-doped side of the junction is at a more positive potential than the n-doped side, and the base\u2013collector junction is reverse biased. When forward bias is applied to the base\u2013emitter junction, the equilibrium between the thermally generated carriers and the repelling electric field of the emitter depletion region is disturbed. This allows thermally excited carriers (electrons in NPNs, holes in PNPs) to inject from the emitter into the base region. These carriers create a diffusion current through the base from the region of high concentration near the emitter toward the region of low concentration near the collector.\nTo minimize the fraction of carriers that recombine before reaching the collector\u2013base junction, the transistor's base region must be thin enough that carriers can diffuse across it in much less time than the semiconductor's minority-carrier lifetime. Having a lightly doped base ensures recombination rates are low. In particular, the thickness of the base must be much less than the of the carriers. The collector\u2013base junction is reverse-biased, and so negligible carrier injection occurs from the collector to the base, but carriers that are injected into the base from the emitter, and diffuse to reach the collector\u2013base depletion region, are swept into the collector by the electric field in the depletion region. The thin \"shared\" base and asymmetric collector\u2013emitter doping are what differentiates a bipolar transistor from two \"separate\" diodes connected in series.\nVoltage, current, and charge control.\nThe collector\u2013emitter current can be viewed as being controlled by the base\u2013emitter current (current control), or by the base\u2013emitter voltage (voltage control). These views are related by the current\u2013voltage relation of the base\u2013emitter junction, which is the usual exponential current\u2013voltage curve of a p\u2013n junction (diode).\nThe explanation for collector current is the concentration gradient of minority carriers in the base region. Due to low-level injection (in which there are many fewer excess carriers than normal majority carriers) the ambipolar transport rates (in which the excess majority and minority carriers flow at the same rate) is in effect determined by the excess minority carriers.\nDetailed transistor models of transistor action, such as the Gummel\u2013Poon model, account for the distribution of this charge explicitly to explain transistor behavior more exactly. The charge-control view easily handles phototransistors, where minority carriers in the base region are created by the absorption of photons, and handles the dynamics of turn-off, or recovery time, which depends on charge in the base region recombining. However, because base charge is not a signal that is visible at the terminals, the current- and voltage-control views are generally used in circuit design and analysis.\nIn analog circuit design, the current-control view is sometimes used because it is approximately linear. That is, the collector current is approximately formula_1 times the base current. Some basic circuits can be designed by assuming that the base\u2013emitter voltage is approximately constant and that collector current is \u03b2 times the base current. However, to accurately and reliably design production BJT circuits, the voltage-control model (e.g. the Ebers\u2013Moll model) is required. The voltage-control model requires an exponential function to be taken into account, but when it is linearized such that the transistor can be modeled as a transconductance, as in the Ebers\u2013Moll model, design for circuits such as differential amplifiers again becomes a mostly linear problem, so the voltage-control view is often preferred. For translinear circuits, in which the exponential I\u2013V curve is key to the operation, the transistors are usually modeled as voltage-controlled current sources whose transconductance is proportional to their collector current. In general, transistor-level circuit analysis is performed using SPICE or a comparable analog-circuit simulator, so mathematical model complexity is usually not of much concern to the designer, but a simplified view of the characteristics allows designs to be created following a logical process.\nTurn-on, turn-off, and storage delay.\nBipolar transistors, and particularly power transistors, have long base-storage times when they are driven into saturation; the base storage limits turn-off time in switching applications. A Baker clamp can prevent the transistor from heavily saturating, which reduces the amount of charge stored in the base and thus improves switching time.\nTransistor characteristics: alpha (\"\u03b1\") and beta (\"\u03b2\").\nThe proportion of carriers able to cross the base and reach the collector is a measure of the BJT efficiency. The heavy doping of the emitter region and light doping of the base region causes many more electrons to be injected from the emitter into the base than holes to be injected from the base into the emitter. A thin and lightly doped base region means that most of the minority carriers that are injected into the base will diffuse to the collector and not recombine.\nCommon-emitter current gain.\nThe \"common-emitter current gain\" is represented by \u03b2F or the h-parameter hFE; it is approximately the ratio of the collector's direct current to the base's direct current in forward-active region. (The F subscript is used to indicate the forward-active mode of operation.) It is typically greater than 50 for small-signal transistors, but can be smaller in transistors designed for high-power applications. Both injection efficiency and recombination in the base reduce the BJT gain.\nCommon-base current gain.\nAnother useful characteristic is the \"common-base current gain\", \u03b1F. The common-base current gain is approximately the gain of current from emitter to collector in the forward-active region. This ratio usually has a value close to unity; between 0.980 and 0.998. It is less than unity due to recombination of charge carriers as they cross the base region.\nAlpha and beta are related by the following identities:\n formula_2\nBeta is a convenient figure of merit to describe the performance of a bipolar transistor, but is not a fundamental physical property of the device. Bipolar transistors can be considered voltage-controlled devices (fundamentally the collector current is controlled by the base\u2013emitter voltage; the base current could be considered a defect and is controlled by the characteristics of the base\u2013emitter junction and recombination in the base). In many designs beta is assumed high enough so that base current has a negligible effect on the circuit. In some circuits (generally switching circuits), sufficient base current is supplied so that even the lowest beta value a particular device may have will still allow the required collector current to flow.\nStructure.\nBJTs consists of three differently doped semiconductor regions: the \"emitter\" region, the \"base\" region and the \"collector\" region. These regions are, respectively, \"p\" type, \"n\" type and \"p\" type in a PNP transistor, and \"n\" type, \"p\" type and \"n\" type in an NPN transistor. Each semiconductor region is connected to a terminal, appropriately labeled: \"emitter\" (E), \"base\" (B) and \"collector\" (C).\nThe \"base\" is physically located between the \"emitter\" and the \"collector\" and is made from lightly doped, high-resistivity material. The collector surrounds the emitter region, making it almost impossible for the electrons injected into the base region to escape without being collected, thus making the resulting value of \u03b1 very close to unity, and so, giving the transistor a large \u03b2. A cross-section view of a BJT indicates that the collector\u2013base junction has a much larger area than the emitter\u2013base junction.\nThe bipolar junction transistor, unlike other transistors, is usually not a symmetrical device. This means that interchanging the collector and the emitter makes the transistor leave the forward active mode and start to operate in reverse mode. Because the transistor's internal structure is usually optimized for forward-mode operation, interchanging the collector and the emitter makes the values of \u03b1 and \u03b2 in reverse operation much smaller than those in forward operation; often the \u03b1 of the reverse mode is lower than 0.5. The lack of symmetry is primarily due to the doping ratios of the emitter and the collector. The emitter is heavily doped, while the collector is lightly doped, allowing a large reverse bias voltage to be applied before the collector\u2013base junction breaks down. The collector\u2013base junction is reverse biased in normal operation. The reason the emitter is heavily doped is to increase the emitter injection efficiency: the ratio of carriers injected by the emitter to those injected by the base. For high current gain, most of the carriers injected into the emitter\u2013base junction must come from the emitter.\nThe low-performance \"lateral\" bipolar transistors sometimes used in bipolar and MOS integrated circuits are sometimes designed symmetrically, that is, with no difference between forward and backward operation.\nSmall changes in the voltage applied across the base\u2013emitter terminals cause the current between the \"emitter\" and the \"collector\" to change significantly. This effect can be used to amplify the input voltage or current. BJTs can be thought of as voltage-controlled current sources, but are more simply characterized as current-controlled current sources, or current amplifiers, due to the low impedance at the base.\nEarly transistors were made from germanium but most modern BJTs are made from silicon. A significant minority are also now made from gallium arsenide, especially for very high speed applications (see HBT, below).\nThe heterojunction bipolar transistor (HBT) is an improvement of the BJT that can handle signals of very high frequencies up to several hundred GHz. It is common in modern ultrafast circuits, mostly RF systems.\nTwo commonly used HBTs are silicon\u2013germanium and aluminum gallium arsenide, though a wide variety of semiconductors may be used for the HBT structure. HBT structures are usually grown by epitaxy techniques like MOCVD and MBE.\nRegions of operation.\nBipolar transistors have four distinct regions of operation, defined by BJT junction biases:\nAlthough these regions are well defined for sufficiently large applied voltage, they overlap somewhat for small (less than a few hundred millivolts) biases. For example, in the typical grounded-emitter configuration of an NPN BJT used as a pulldown switch in digital logic, the \"off\" state never involves a reverse-biased junction because the base voltage never goes below ground; nevertheless the forward bias is close enough to zero that essentially no current flows, so this end of the forward active region can be regarded as the cutoff region.\nActive-mode transistors in circuits.\nThe diagram shows a schematic representation of an NPN transistor connected to two voltage sources. (The same description applies to a PNP transistor with reversed directions of current flow and applied voltage.) This applied voltage causes the lower p\u2013n junction to become forward biased, allowing a flow of electrons from the emitter into the base. In active mode, the electric field existing between base and collector (caused by \"V\"CE) will cause the majority of these electrons to cross the upper p\u2013n junction into the collector to form the collector current \"I\"C. The remainder of the electrons recombine with holes, the majority carriers in the base, making a current through the base connection to form the base current, \"I\"B. As shown in the diagram, the emitter current, \"I\"E, is the total transistor current, which is the sum of the other terminal currents, (i.e. \"I\"E\u00a0=\u00a0\"I\"B\u00a0+\u00a0\"I\"C).\nIn the diagram, the arrows representing current point in the direction of conventional current\u00a0\u2013 the flow of electrons is in the opposite direction of the arrows because electrons carry negative electric charge. In active mode, the ratio of the collector current to the base current is called the \"DC current gain\". This gain is usually 100 or more, but robust circuit designs do not depend on the exact value (for example see op-amp). The value of this gain for DC signals is referred to as formula_3, and the value of this gain for small signals is referred to as formula_4. That is, when a small change in the currents occurs, and sufficient time has passed for the new condition to reach a steady state formula_4 is the ratio of the change in collector current to the change in base current. The symbol formula_6 is used for both formula_3 and formula_4.\nThe emitter current is related to formula_9 exponentially. At room temperature, an increase in formula_9 by approximately 60\u00a0mV increases the emitter current by a factor of 10. Because the base current is approximately proportional to the collector and emitter currents, they vary in the same way.\nTheory and modeling.\nBJTs can be thought of as two diodes (p\u2013n junctions) sharing a common region that minority carriers can move through. A PNP BJT will function like two diodes that share an N-type cathode region, and the NPN like two diodes sharing a P-type anode region. Connecting two diodes with wires will not make a BJT, since minority carriers will not be able to get from one p\u2013n junction to the other through the wire.\nBoth types of BJT function by letting a small current input to the base control an amplified output from the collector. The result is that the BJT makes a good switch that is controlled by its base input. The BJT also makes a good amplifier, since it can multiply a weak input signal to about 100 times its original strength. Networks of BJTs are used to make powerful amplifiers with many different applications.\nIn the discussion below, focus is on the NPN BJT. In what is called active mode, the base\u2013emitter voltage formula_9 and collector\u2013base voltage formula_12 are positive, forward biasing the emitter\u2013base junction and reverse-biasing the collector\u2013base junction. In this mode, electrons are injected from the forward biased n-type emitter region into the p-type base where they diffuse as minority carriers to the reverse-biased n-type collector and are swept away by the electric field in the reverse-biased collector\u2013base junction.\nFor an illustration of forward and reverse bias, see semiconductor diodes.\nLarge-signal models.\nIn 1954, Jewell James Ebers and John L. Moll introduced their mathematical model of transistor currents:\nEbers\u2013Moll model.\nThe DC emitter and collector currents in active mode are well modeled by an approximation to the Ebers\u2013Moll model:\n formula_13\nThe base internal current is mainly by diffusion (see Fick's law) and\n formula_14\nwhere\nThe formula_23 and forward formula_6 parameters are as described previously. A reverse formula_6 is sometimes included in the model.\nThe unapproximated Ebers\u2013Moll equations used to describe the three currents in any operating region are given below. These equations are based on the transport model for a bipolar junction transistor.\n formula_26\nwhere\nBase-width modulation.\nAs the collector\u2013base voltage (formula_36) varies, the collector\u2013base depletion region varies in size. An increase in the collector\u2013base voltage, for example, causes a greater reverse bias across the collector\u2013base junction, increasing the collector\u2013base depletion region width, and decreasing the width of the base. This variation in base width often is called the \"Early effect\" after its discoverer James M. Early.\nNarrowing of the base width has two consequences:\nBoth factors increase the collector or \"output\" current of the transistor in response to an increase in the collector\u2013base voltage.\nPunchthrough.\nWhen the base\u2013collector voltage reaches a certain (device-specific) value, the base\u2013collector depletion region boundary meets the base\u2013emitter depletion region boundary. When in this state the transistor effectively has no base. The device thus loses all gain when in this state.\nGummel\u2013Poon charge-control model.\nThe Gummel\u2013Poon model is a detailed charge-controlled model of BJT dynamics, which has been adopted and elaborated by others to explain transistor dynamics in greater detail than the terminal-based models typically do. This model also includes the dependence of transistor formula_6-values upon the direct current levels in the transistor, which are assumed current-independent in the Ebers\u2013Moll model.\nSmall-signal models.\nHybrid-pi model.\nThe hybrid-pi model is a popular circuit model used for analyzing the small signal and AC behavior of bipolar junction and field effect transistors. Sometimes it is also called \"Giacoletto model\" because it was introduced by L.J. Giacoletto in 1969. The model can be quite accurate for low-frequency circuits and can easily be adapted for higher-frequency circuits with the addition of appropriate inter-electrode capacitances and other parasitic elements.\nh-parameter model.\nAnother model commonly used to analyze BJT circuits is the \"h-parameter\" model, also known as the hybrid equivalent model, closely related to the hybrid-pi model and the y-parameter two-port, but using input current and output voltage as independent variables, rather than input and output voltages. This two-port network is particularly suited to BJTs as it lends itself easily to the analysis of circuit behavior, and may be used to develop further accurate models. As shown, the term \"x\" in the model represents a different BJT lead depending on the topology used. For common-emitter mode the various symbols take on the specific values as:\nand the h-parameters are given by:\nAs shown, the h-parameters have lower-case subscripts and hence signify AC conditions or analyses. For DC conditions they are specified in upper-case. For the CE topology, an approximate h-parameter model is commonly used which further simplifies the circuit analysis. For this the \"h\"oe and \"h\"re parameters are neglected (that is, they are set to infinity and zero, respectively). The h-parameter model as shown is suited to low-frequency, small-signal analysis. For high-frequency analyses the inter-electrode capacitances that are important at high frequencies must be added.\nEtymology of \"h\"FE.\nThe \"h\" refers to its being an h-parameter, a set of parameters named for their origin in a \"hybrid equivalent circuit\" model (see above). As with all h parameters, the choice of lower case or capitals for the letters that follow the \"h\" is significant; lower-case signifies \"small signal\" parameters, that is, the slope the particular relationship; upper-case letters imply \"large signal\" or DC values, the ratio of the voltages or currents. In the case of the very often used \"h\"FE:\nSo hFE (or hFE) refers to the (total; DC) collector current divided by the base current, and is dimensionless. It is a parameter that varies somewhat with collector current, but is often approximated as a constant; it is normally specified at a typical collector current and voltage, or graphed as a function of collector current.\nHad capital letters not been used for used in the subscript, i.e. if it were written \"hfe\" the parameter indicate small signal (AC) current gain, i.e. the slope of the Collector current versus Base current graph at a given point, which is often close to the hFE value unless the test frequency is high.\nIndustry models.\nThe Gummel\u2013Poon SPICE model is often used, but it suffers from several limitations. For instance, reverse breakdown of the base\u2013emitter diode is not captured by the SGP (SPICE Gummel\u2013Poon) model, neither are thermal effects (self-heating) or quasi-saturation. These have been addressed in various more advanced models which either focus on specific cases of application (Mextram, HICUM, Modella) or are designed for universal usage (VBIC).\nCurrent direction conventions.\nBy convention, the direction of current on diagrams is shown as the direction in which a positive charge would move. This is called \"conventional current\". However, in actuality, current in metal conductors is due to the flow of electrons. Because electrons carry a negative charge, they move in the direction opposite to conventional current. On the other hand, inside a bipolar transistor, currents can be composed of both positively charged holes and negatively charged electrons. In this article, current arrows are shown in the conventional direction, but labels for the movement of holes and electrons show their actual direction inside the transistor.\nArrow direction.\nThe arrow on the symbol for bipolar transistors indicates the p\u2013n junction between base and emitter and points in the direction in which conventional current travels.\nApplications.\nThe BJT remains a device that excels in some applications, such as discrete circuit design, due to the very wide selection of BJT types available, and because of its high transconductance and output resistance compared to MOSFETs.\nThe BJT is also the choice for demanding analog circuits, especially for very-high-frequency applications, such as radio-frequency circuits for wireless systems.\nHigh-speed digital logic.\nEmitter-coupled logic (ECL) use BJTs.\nBipolar transistors can be combined with MOSFETs in an integrated circuit by using a BiCMOS process of wafer fabrication to create circuits that take advantage of the application strengths of both types of transistor.\nAmplifiers.\nOne of the most prominent early uses of the transistor was in consumer products such as the transistor radio which began production in 1954. The use of transistors in handheld radios and would also jumpstart a small Japanese company named Tokyo Tsushin Kogyo K.K. to prominence with its TR-55 transistor radio bearing the name the company would soon change to match: Sony. The follow-on pocket-sized Sony TR-63 and several larger models by other manufacturers cemented the transistor and miniaturized electronics as critical to the new, portable consumer device market for decades to come.\nThe \u03b1 and \u03b2 characterize the current gain of the BJT. It is this gain that allows BJTs to be used as the building blocks of electronic amplifiers. The three main BJT amplifier topologies are:\nTemperature sensors.\nBecause of the known temperature and current dependence of the forward-biased base\u2013emitter junction voltage, the BJT can be used to measure temperature by subtracting two voltages at two different bias currents in a known ratio.\nLogarithmic converters.\nBecause base\u2013emitter voltage varies as the logarithm of the base\u2013emitter and collector\u2013emitter currents, a BJT can also be used to compute logarithms and anti-logarithms. A diode can also perform these nonlinear functions but the transistor provides more circuit flexibility.\nAvalanche pulse generators.\nTransistors may be deliberately made with a lower collector to emitter breakdown voltage than the collector to base breakdown voltage. If the emitter\u2013base junction is reverse biased the collector emitter voltage may be maintained at a voltage just below breakdown. As soon as the base voltage is allowed to rise, and current flows avalanche occurs and impact ionization in the collector base depletion region rapidly floods the base with carriers and turns the transistor fully on. So long as the pulses are short enough and infrequent enough that the device is not damaged, this effect can be used to create very sharp falling edges.\nSpecial avalanche transistor devices are made for this application.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49339", "revid": "244109", "url": "https://en.wikipedia.org/wiki?curid=49339", "title": "Doped", "text": ""}
{"id": "49340", "revid": "1171570780", "url": "https://en.wikipedia.org/wiki?curid=49340", "title": "Doping", "text": "Doping may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "49348", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=49348", "title": "Element mercury", "text": ""}
{"id": "49350", "revid": "901231", "url": "https://en.wikipedia.org/wiki?curid=49350", "title": "Emergency operation", "text": ""}
{"id": "49351", "revid": "31439127", "url": "https://en.wikipedia.org/wiki?curid=49351", "title": "Trucker's hitch", "text": "Type of knot\nThe trucker's hitch is a compound knot commonly used for securing loads on trucks or trailers. The general arrangement, using loops and turns in the rope itself to form a crude block and tackle, has long been used to tension lines and is known by multiple names. Knot author Geoffrey Budworth claims the knot can be traced back to the days when carters and hawkers used horse-drawn conveyances to move their wares from place to place.\nVariations.\nThe portion of the trucker's hitch which differs in the following variations is the method used to form the loop which the working end slides through to produce the mechanical advantage. The different methods of forming the loop affect the ease and speed of tying and releasing, and the stability of the final product.\nThe variations are presented in order of increasing stability.\nSheepshank style loop.\nThis version of the knot uses a sheepshank, in this kind of application also known as a \"bell ringer's knot\", to form the loop. It is quicker to make than a fixed loop, but is less dependable. It is avoided in critical applications (such as securing a load on a truck) as it can fall apart under too little load or too much load, and can capsize if not dressed properly. However, this knot may be made secure by adding a Half Hitch to the top bight of the Sheepshank. This form of the trucker's hitch is least likely to jam, coming apart easily once tension is released. Different sources show slight variations in the way the sheepshank portion is formed and dressed.\nVersions popular in East Asia use variations of sheep shank using either a simple half hitch or a double turn self crossing half hitch or a triple turn self crossing half hitch. A sheep shank with two consecutive half hitches i.e. a clove hitch to secure the upper eye and to form the lower eye is more popular in the west.\nSlipped overhand loop.\nThe loop formed in one version is a simple Slipped Overhand Loop. This version is good for light to moderate loads\nSimple friction loop.\nAnother version uses a multiply twisted bight to pass a bight of the working end to form the eye of the loop. This version tolerates higher load. \nFixed loop.\nThe most reliable common variation uses a fixed loop, such as an alpine butterfly loop, artillery loop, figure-eight loop or directional figure-eight loop, or another of many suitable loop knots. If a fixed loop is used repeatedly for tying the trucker's hitch in the same portion of rope, excessive wear or other damage may be suffered by the portion of the loop which working end slides against.\nIf extra loops are used to form the eye it tends to ease untying. In order to prevent the closing of the loop under load, the loop must be formed by the working end of the rope (which will later pass through the loop). If the standing end goes through the loop, it will close under load.\nFinishing the hitch.\nIn tightening the trucker's hitch, tension can be effectively increased by repeatedly pulling sideways while preventing the tail end from slipping through the loop, and then cinching the knot tighter as the sideways force is released. This is called \"sweating a line\".\nIf the tail end is wrapped through the last loop twice, the resulting friction may render manual cinching unnecessary.\nOnce tight, the trucker's hitch is often secured with a half hitch, usually slipped for easy releasing and to avoid the necessity of access to the end of the rope, though a more secure finish, such as two half-hitches, may be called for. Under large loads, the finishing half hitch can jam, especially if it is not slipped; the difficulty of releasing it can be compounded by the fact that the knot is typically still under tension when it is to be untied.\nFinishing with a taut-line hitch or a Farrimond friction hitch to the standing part allows the finishing knot to be tied and untied with no tension. This eliminates any jamming problems and also allows the line to be re-tensioned if necessary.\nA mechanical advantage of ideally 3:1 can nearly be achieved when using an equivalent setup with pulleys, but is reduced substantially by friction when using knots.\nCultural references.\nThe trucker's hitch knot is portrayed by comedy duo Ylvis in their 2014 song with the same name. The lyrics and the video pretend (in a humorous way) to demonstrate how to tie the knot.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49352", "revid": "28992838", "url": "https://en.wikipedia.org/wiki?curid=49352", "title": "Hangman's knot", "text": "Type of knot\nThe hangman's knot or hangman's noose (also known as a collar during the Elizabethan era) is a knot most often associated with its use in hanging a person.\nFunction.\nThis knot was typically used as a method of capital punishment. The pull on the knot at end of the gallows often resulted in a cervical fracture. Another method intended to result in the mass of the knot crushing closed (occluding) neck arteries, causing cessation of brain circulation, which was not always rapid. The knot is non-jamming but tends to resist attempts to loosen it.\nIn culture.\nSurviving nooses in the United Kingdom show simple slipknots that were superseded in the late 19th century with a metal eye spliced into one end of the rope, the noose being formed by passing the other end through it. The classic hangman's knot was largely developed in the United States. Filmed hangings of war criminals in Europe after World War II, conducted under US jurisdiction, show such knots placed in various locations.\nEach additional coil adds friction to the knot, which makes the noose harder to pull closed or open. When Grover Cleveland was the sheriff of Erie County, he performed two hangings. Cleveland was advised by a more experienced Sheriff to grease the rope with tallow and run it through the knot a few times to ensure rapid closure with the drop. The number of coils should therefore be adjusted depending on the intended use, the type and thickness of rope, and environmental conditions such as wet or greasy rope. One coil makes it equivalent to the simple running knot.\nWoody Guthrie sings of the hangman using thirteen coils:\n&lt;poem&gt;Did you ever see a hangman tie a hangknot?\nI've seen it many a time and he winds, he winds,\nAfter thirteen times he's got a hangknot.&lt;/poem&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49355", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=49355", "title": "Thief knot", "text": "Type of knot\nThe thief knot resembles the reef knot (square knot) except that the free, or bitter ends are on opposite sides. It is said that sailors would secure their belongings in a using the thief knot, often with the ends hidden. If another sailor went through the bag, the odds were high the thief would tie the bag back using the more common reef knot, revealing the tampering, hence the name. It is difficult to tie by mistake, unlike the granny knot, unless one attempts to tie a square knot in a similar manner to a sheet bend (which is the correct way to tie a thief knot), then it is possible to tie accidentally.\nThe thief knot is much less secure than the already insecure reef knot. It unties itself if the lines are pulled when the same action would seize a reef knot.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The thief or bag knot is also called bread bag knot. It appears very like the reef knot, but there is one real and scarcely evident difference. It does not consist of two half knots. There is a legend that sailors tie clothesbags, and bread bags with this knot and that thieves always retie them with reef knots and so are inevitably detected. It is a pleasing story that should encourage honesty. However, if I have ever met this knot in practical use, I have neither recognized it nor paid penalty for my failure to do so.\u2014\u200a\nSources.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49364", "revid": "50736557", "url": "https://en.wikipedia.org/wiki?curid=49364", "title": "Turner syndrome", "text": "X chromosome monosomy\nMedical condition&lt;templatestyles src=\"Template:Infobox/styles-images.css\" /&gt;\nTurner syndrome (TS), commonly known as 45,X, or 45,X0, is a chromosomal disorder in which cells of females have only one X chromosome instead of two, or are partially missing an X chromosome (sex chromosome monosomy) leading to the complete or partial deletion of the pseudoautosomal regions (PAR1, PAR2) in the affected X chromosome. Humans typically have two sex chromosomes, XX for females or XY for males. The chromosomal abnormality is often present in just some cells, in which case it is known as Turner syndrome with mosaicism. 45,X0 with mosaicism can occur in males or females, but Turner syndrome without mosaicism only occurs in females. Signs and symptoms vary among those affected but often include additional skin folds on the neck, arched palate, low-set ears, low hairline at the nape of the neck, short stature, and lymphedema of the hands and feet. Those affected do not normally develop menstrual periods or mammary glands without hormone treatment and are unable to reproduce without assistive reproductive technology. Small chin (micrognathia), loose folds of skin on the neck, slanted eyelids and prominent ears are found in Turner syndrome, though not all will show it. Heart defects, Type II diabetes, and hypothyroidism occur in the disorder more frequently than average. Most people with Turner syndrome have normal intelligence; however, some have problems with spatial visualization that can hinder learning mathematics. Ptosis (droopy eyelids) and conductive hearing loss also occur more often than average.\nTurner syndrome is caused by one X chromosome (45,X), a ring X chromosome, 45,X/46,XX mosaicism, or a small piece of the Y chromosome in what should be an X chromosome. They may have a total of 45 chromosomes or will not develop menstrual periods due to loss of ovarian function genes. Their karyotype often lacks Barr bodies due to lack of a second X or may have Xp deletions. It occurs during formation of the reproductive cells in a parent or in early cell division during development. No environmental risks are known, and the mother's age does play a role. While most people have 46 chromosomes, people with Turner syndrome usually have 45 in some or all cells. In cases of mosaicism, the symptoms are usually fewer, and possibly none occur at all. Diagnosis is based on physical signs and genetic testing.\nNo cure for Turner syndrome is known. Treatment may help with symptoms. Human growth hormone injections during childhood may increase adult height. Estrogen replacement therapy can promote development of the breasts and hips. Medical care is often required to manage other health problems with which Turner syndrome is associated.\nTurner syndrome occurs in between one in 2,000 and one in 5,000 females at birth. All regions of the world and cultures are affected about equally. Generally people with Turner syndrome have a shorter life expectancy, mostly due to heart problems and diabetes. American endocrinologist Henry Turner first described the condition in 1938. In 1964, it was determined to be due to a chromosomal abnormality.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nPresentation.\nTurner syndrome is associated with a number of physical features, including short stature, heart defects, webbed neck, micrognathia, amenorrhoea, and infertility. The phenotype of Turner syndrome is affected by mosaicism, where cell lines with a single sex chromosome are combined with those with multiple. Individuals with mosaicism of 45,X0/46,XY may be phenotypically male, female, or ambiguous, while those with 45,X0/46,XX will be phenotypically female. Patients with 45,X0/46,XY do not receive the diagnosis of Turner syndrome if phenotypically male. Around 40%\u201350% of cases of Turner syndrome are true \"monosomy X\" with a 45,X0 karyotype, while the remainder are mosaic for another cell line, most commonly 46,XX, or have other structural abnormalities of the X chromosome. The classic features of Turner syndrome, while distinctive, may be rarer than previously thought; incidental diagnosis, such as in biobank samples or prenatal testing for older mothers, finds many girls and women with few traditional signs of Turner syndrome.\nPhysiological.\nStature.\nTurner syndrome is associated with short stature. The mean adult height of women with Turner syndrome without growth hormone therapy is around shorter than the mean of women in the general population. Mosaicism affects height in Turner syndrome; a large population sample drawn from the UK Biobank found women with 45,X0 karyotypes to have an average height of , while those with 45,X0/46,XX karyotypes averaged . The strength of the association between Turner syndrome and short stature is such that idiopathic short stature alone is a major diagnostic indication.\nGrowth delay in Turner syndrome does not begin at birth; most neonates with the condition have a birth weight in the lower end of the normal range. Height begins to lag in toddlerhood, with a delayed growth velocity becoming apparent as early as 18 months. Marked short stature becomes obvious in mid-childhood. In undiagnosed preadolescents and adolescents, growth delay may be mistaken for a side effect of delayed puberty and improperly treated. \nShort stature in Turner syndrome and its counterpoint, tall stature in sex chromosome polysomy conditions such as Klinefelter syndrome, XYY syndrome, and trisomy X, is caused by the short-stature homeobox gene on the X and Y chromosomes. The absence of a copy of the \"SHOX\" gene in Turner's inhibits skeletal growth, resulting both in overall short stature and in a distinctive pattern of skeletal malformations including micrognathia (small chin), cubitus valgus (abnormal forearm angles), and shorter fingers.\nWhen Turner syndrome is diagnosed in early life, growth hormone therapy can decrease the degree of short stature. Treatment with human growth hormone appears to increase expected adult height by approximately from an otherwise expected norm of \u2013. The effects of growth hormone therapy are at their strongest during the first year of treatment and taper off over time. In some cases oxandrolone, a steroid with a relatively mild masculinizing effect, may be used alongside growth hormone. The addition of oxandrolone to a Turner syndrome treatment regimen adds around to the final height. Oxandrolone is used particularly often in girls diagnosed later in their growth period, due to the reduced impact of growth hormone alone in this population. However, oxandrolone use runs the risk of delayed breast development, voice deepening, increased body hair, or clitoromegaly.\nPhysical features.\nIn addition to short stature, Turner syndrome is associated with a number of characteristic physical features. These include a short, webbed neck, low hairline, small chin and lower jaw, arched palate, and broad chest with widely-spaced nipples. Lymphedema (swelling) of the hands and feet is common at birth and sometimes persistent throughout the lifespan.\nA number of the external manifestations of Turner syndrome are focused on the limbs, hands, and feet. Lymphedema at birth is one of the classic features of the syndrome; though it often resolves during toddlerhood, recurrence in later life is frequent, often without apparent cause. Cases where the retained X chromosome was inherited from the mother more often experience lymphedema than those where it was from the father. As a consequence of lymphedema's effects on nail anatomy, females with Turner syndrome frequently have small, hypoplastic, upturned nails. Their fingers are shorter and the hands are broad. Their feet are puffy, thicker, and swollen. Shortened metacarpal bones, particularly the fourth metacarpal, are a frequent finding. The body shape of individuals with Turner syndrome is frequently quite broad and stocky, as the growth deficiency is more pronounced in the length of bones than in their width. Scoliosis is common in Turner syndrome, and is seen in 40% of girls without growth hormone treatment.\nFacial features associated with Turner syndrome include \nbroad, prominent ears, a low hairline at the nape of the neck, a webbed neck, a small chin with dental malocclusion, and downslanting palpebral fissures (the opening between the eyelids). These are thought to be related to lymphedema during the fetal period, specifically to the presence and resorption of excess fluids in the head and neck region. Neck webbing is a particularly distinctive trait of Turner syndrome, leading to many neonatal diagnoses. The underlying etiology of neck webbing is related to prenatal blood flow issues, and even in populations without Turner's has broad health consequences; the rate of congenital heart disease in webbed neck is 150-fold higher than in the general population, while the feature is also associated with reduced height and minor developmental impairments. Some women with Turner syndrome have premature facial wrinkling. Acne is less common in teenage girls and women with Turner syndrome, though the reasons why are unclear.\nOther physical features connected to the condition include long eyelashes, sometimes including an additional set of eyelashes, and unusual dermatoglyphics (fingerprints). Some women with Turner's report being unable to create fingerprint passwords due to hypoplastic dermatoglyphics. Unusual dermatoglyphics are common to chromosome anomalies like Down Syndrome. and in the case of Turner's may be a consequence of fetal lymphedema. Keloid scars, or raised hypertrophic scars growing beyond the boundaries of the original wound, are potentially associated with Turner syndrome; however, the association is underresearched. Though traditional medical counselling on the topic urges conservatism about elective procedures such as ear piercing due to the risk of severe scarring, the actual consequences are unclear. Keloids in Turner syndrome are particularly frequent following surgical procedures to reduce neck webbing. Turner syndrome has been associated with unusual patterns of hair growth, such as patches of short and long hair. Armpit and pubic hair is often sparse, while arm and leg hair is often thick. Though armpit hair is reduced in amount and thickness, the pattern in which it is implanted in the skin is as in men, rather than as in women.\nCardiac.\nApproximately half of individuals with Turner syndrome have congenital heart defects. CHDs associated with Turner syndrome include bicuspid aortic valves (30%), coarctation of the aorta (15%), and abnormalities of the arteries in the head and neck. A rare but potentially fatal complication of heart defects in Turner syndrome is aortic dissection, where the inner layer of the aorta tears open. Aortic dissection is six times as common in females with Turner syndrome as the general population and accounts for 8% of all deaths in the syndrome. The risk is substantially increased for individuals with bicuspid aortic valves, who make up 95% of patients with aortic dissection compared to 30% of all Turner's patients, and coarctation of the aorta, who make up 90% and 15% respectively.\nCoronary artery disease onsets earlier in life in women with Turner syndrome compared to controls, and mortality from cardiac events is increased. This is thought to be in part a function of the relationship between Turner syndrome and obesity; women with Turner syndrome have a higher percentage of body fat for their weight than control women, and their short stature makes weight control more difficult. Though coronary artery disease is frequently thought a disease of older adults, young women with Turner syndrome are more likely to develop the disease than their 46,XX peers. Treatment recommendations for women with Turner syndrome and coronary artery disease are as in the general population, but as Turner's increases the risk of type 2 diabetes, women with insulin resistance must weigh up the benefits of prophylactic or early statin treatment with the risk of Type II diabetes.\nInternal medicine.\nTurner syndrome is associated with a broad variety of health considerations, such as liver and kidney issues, obesity, diabetes, and hypertension. Liver dysfunction is common in women with Turner syndrome, with 50\u201380% having elevated liver enzymes. Non-alcoholic fatty liver disease is increased in prevalence in Turner syndrome, likely related in part to both conditions' associations with obesity. Hepatic vascular diseases are also seen in the syndrome as an aspect of Turner syndrome's broader vascular, aortic and cardiac impacts. Primary biliary cholangitis is more common in 45,X0 than 46,XX women. An unclear association exists between estrogen replacement therapy and liver dysfunction in Turner syndrome; some studies imply estrogen therapy worsens such conditions, while others imply improvement.\nKidney issues, such as horseshoe kidney, are sometimes observed in Turner syndrome. Horseshoe kidney, where the kidneys are fused together in a U-shape, occurs in around 10% of Turner's cases compared to less than 0.5% of the general population. A missing kidney is observed in as many as 5% of individuals with Turner syndrome, compared to around 0.1% of the population. A duplicated ureter, where two ureters drain a single kidney, occurs in as much as 20\u201330% of the Turner syndrome population. Kidney malformations ( horseshoe kidney, etc.) in Turner syndrome may be more common in mosaicism than in the full 45,X karyotype. Serious complications of the kidney anomalies associated with Turner syndrome are rare, although there is some risk of issues such as obstructive uropathy, where the flow of urine from the kidneys is blocked.\nWomen with Turner syndrome are more likely than average to have high blood pressure; as many as 60% of women with the condition are hypertensive. Isolated diastolic hypertension often precedes systolic hypertension in the condition and may develop at a young age. Treatments for hypertension in Turner syndrome are as in the general population.\nApproximately 25\u201380% of women with Turner syndrome have some level of insulin resistance, and a minority develop type 2 diabetes. The risk of diabetes in Turner syndrome varies by karyotype and appears to be raised by specific deletions of the short arm of the X chromosome (Xp). One study found that while a relatively low 9% of women with Xq (long arm) deletions had type 2 diabetes, 18% of those with full 45,X0 karyotypes did, as well as 23% with Xp deletions. 43% of women with isochromosome Xq, who both lacked the short arm and had an additional copy of the long arm, developed type 2 diabetes. Though part of the diabetes risk in Turner syndrome is a function of weight control, some is independent; age- and weight-matched women with non-Turner's ovarian failure have a lower diabetes risk than in Turner syndrome. Growth hormone treatment plays an unclear role in diabetes risk, as does estrogen supplementation.\nThe association between Turner syndrome and other diseases, such as cancer, is unclear. Overall, women with Turner syndrome do not appear more likely to develop cancer than women with 46,XX karyotypes, but the specific pattern of what cancers are highest risk seems to differ. The risk of breast cancer appears lower in Turner's than in control women, perhaps due to decreased levels of estrogen. Neuroblastoma, a cancer of infancy and early childhood, has been reported in girls with Turner syndrome. Tumours of the nervous system, both the central nervous system and the peripheral nervous system, are overrepresented amongst cancers in Turner syndrome. Furthermore, about 5.5% of Turner syndrome individuals have an extra, abnormal small supernumerary marker chromosome (sSMC) which consists of part of a Y chromosome. This partial Y chromosome-bearing sSMC may include the \"SRY\" gene located on the p arm of the Y chromosome at band 11.2 (notated as Yp11.2). This gene encodes the testis-determining factor protein (also known as sex-determining region Y protein). Turner syndrome individuals with this \"SRY\" gene-containing sSMC have increased risk of developing gonadal tissue neoplasms such as gonadoblastomas and in situ seminomas (also termed dysgerminomas to indicate that this tumor has the pathology of the testicular tumor, seminoma, but develops in ovaries). In one study, 34 Turner syndrome girls without overt evidence of these tumors were found at preventative surgery to have a gonadoblastoma (7 cases), dysgerminoma (1 case), or non-specific \"in situ\" gonadal neoplasm (1 case). Turner syndrome girls with this sSMC otherwise have typical features of the Turner syndrome except for a minority who also have hirsutism and/or clitoral enlargement. Surgical removal of the gonads has been recommended to remove the threat of developing these sSMC-associated neoplasms. Turner syndrome individuals with an sSMC that lacks the \"SRY\" gene are not at an increased risk of developing these cancers.\nSensory.\nHearing loss is common in Turner syndrome. Though at birth hearing is normal with good hearing abilities, chronic middle ear infections are frequent throughout childhood, which can cause permanent conductive hearing loss or deafness. In adulthood, sensorineural hearing loss occurs more often than in 46,XX women and at younger ages; though differing thresholds of hearing loss make it difficult to compare between studies, younger adult women with Turner syndrome are routinely found to have disproportionate rates of hearing issues, with sometimes up to half of women in their 20s and 30s having difficulty hearing well. This hearing loss is progressive; at the age of 40, women with Turner syndrome have equivalent hearing loss to 46,XX women aged 60, on average. Cohort studies imply hearing loss may be more common in women who also have metabolic syndrome. The high prevalence of sensorineural hearing loss in Turner syndrome appears to be related to \"SHOX\" deficiency.\nOcular and visual disorders are also increased in prevalence in Turner syndrome. More than half of individuals with Turner syndrome have some form of eye disorder. This may be a consequence of shared genes on the X chromosome in both visual and ovarian development. Nearly half of cases have hyperopia or myopia, usually mild. Strabismus, or misalignment of the eye, occurs in around one-fifth to one-third of girls with Turner syndrome. As with strabismus outside the Turner's context, it may be treated with glasses, patching, or surgical correction. Esotropia, where the eye turns inwards, is more common than exotropia, where it turns outwards. Ptosis, or a drooping eyelid, is a common facial manifestation of Turner syndrome; it usually has no appreciable impact on vision, but severe cases may limit visual range and require surgical correction. The rate of red-green colourblindness in Turner syndrome is 8%, the same as in XY males. This is due to red-green colourblindness being an X-linked recessive condition; in people with a single X chromosome, whether normal males or Turner females, only a single mutated X is necessary for symptoms. Red-green colourblindness may be underdiagnosed in the Turner context, as the rarity of the condition in females reduces the likelihood of screening, and practitioners may not connect that the karyotype of Turner syndrome increases the risk from the female baseline.\nAutoimmune.\nWomen with Turner syndrome are two to three times as likely to develop autoimmune disorders as the general population. Specific autoimmune disorders linked to Turner syndrome include Hashimoto's disease, vitiligo, psoriasis and psoriatic arthritis, alopecia,Type I diabetes, and celiac disease Type I diabetes, when the immune system attacks the beta cells in the pancreas, is a major autoimmune disorder and is much more common in Turner females than 46,XX and 47,XXX females in most cases. Inflammatory bowel disease is also common, while the prevalence of type 1 diabetes is unclear, though appears increased.\nThyroid disease is common in Turner syndrome. Hypothyroidism is prevalent; 30%\u201350% of women with Turner syndrome have Hashimoto's disease, where the thyroid gland is slowly destroyed by an autoimmune reaction from the immune system. By age 50, half of women with Turner syndrome have subclinical or clinical hypothyroidism. Hyperthyroidism and Graves' disease are also increased in prevalence, though more modestly. The Turner's presentation of hyperthyroidism is as in the general population, while the presentation of hypothyroidism is often atypical, with a mild early presentation yet a more severe progression. Women with isochromosome Xq are more likely to develop autoimmune thyroid disease than women with other forms of Turner syndrome.\nThe risk of irritable bowel syndrome is increased around fivefold in Turner syndrome, and that of ulcerative colitis around fourfold. Celiac disease is also increased in prevalence, with around 4\u20138% of Turner's patients having comorbid celiac disease compared to 0.5\u20131% of the general population. Diagnosis of such conditions is difficult due to their nonspecific early symptoms. In the Turner's context, diagnosis may in particular be missed due to growth delay; such conditions cause growth delay and failure to thrive when they onset in childhood, but as girls with Turner syndrome already have such delay, symptoms may be overlooked and ascribed to the original condition.\nAlopecia areata, or recurrent patchy hair loss, is three times as common in Turner syndrome as the general population. Alopecia in the Turner syndrome context is frequently treatment-resistant, also seen in other chromosome aneuploidies such as Down syndrome. Psoriasis is common in Turner syndrome, although the precise prevalence is unclear. Turner's psoriasis may be related to growth hormone treatment, as psoriasis as a side effect of such therapies has been reported in patients without the karyotype. Psoriasis may progress to psoriatic arthritis, and this progression may be more common in Turner syndrome. Vitiligo has been reported in conjunction with Turner syndrome, but the risk is unclear and may be a side effect of increased clinical attention to autoimmune disease in this population.\nPuberty.\nPuberty is delayed or absent in Turner syndrome. A 2019 literature review found that 13% of women with a 45,X0 karyotype could expect to experience spontaneous thelarche (breast development), while 9% would undergo spontaneous menarche (beginning of menstruation). These numbers were higher in women with mosaic Turner's; 63% with 45,X0/46,XX karyotypes experienced spontaneous thelarche and 39% spontaneous menarche, while 88% with 45,X0/47,XXX (the presence of a trisomy X cell line) experienced spontaneous thelarche and 66% spontaneous menarche. Unexpectedly, women with Y-chromosome cells also had increased rates of thelarche and menarche compared to the 45,X baseline, at 41% and 19%. However, few women with trisomy X or Y-chromosome cell lines were covered in the review, impeding extrapolation from these results. 6% of women with Turner syndrome have regular menstrual cycles; the rest experience primary or secondary amenorrhea or other menstrual dysfunction.\nIn girls with Turner syndrome who do not experience spontaneous puberty, exogenous estrogen is used to induce and maintain feminization. Estrogen replacement is recommended to begin at around age 11\u201312, although some parents prefer to delay the induction of puberty in girls with lower social and emotional preparedness. The dose of estrogen in induced puberty begins at 10% of adult estrogen levels and is steadily increased at six-month intervals, with a full adult dose attained two to three years after the beginning of treatment. Estrogen replacement may interfere with growth hormone therapy, due to the closing effects of estrogen on growth plates; individuals must weigh up their preferences for taller height versus greater feminization.\nFertility.\nWomen with Turner syndrome are at extremely high risk for primary ovarian insufficiency (POI) and infertility. Although about 70\u201380% have no spontaneous pubertal development and 90% experience primary amenorrhea, the remainder may possess a small residual of ovarian follicles at birth or early childhood.\nEarly in gestation, fetuses with Turner syndrome have a normal number of gametes in their developing ovaries, but this starts decreasing rapidly as early as 18 weeks of pregnancy; by birth, girls with the condition have markedly reduced follicular counts. Women with Turner syndrome who wish to raise families but are incapable of conception with their own oocytes have the options of adoption or of pregnancy with donor eggs; the latter has a comparable success rate to donor pregnancy in women with 46,XX karyotypes.\nPregnancy in Turner syndrome is inherently high-risk; the maternal death rate is 2%.\nUsually, estrogen replacement therapy is used to spur the growth of secondary sexual characteristics at the time when puberty should onset. While very few women with Turner syndrome menstruate spontaneously, estrogen therapy requires a regular shedding of the uterine lining (\"withdrawal bleeding\") to prevent its overgrowth. Withdrawal bleeding can be induced monthly, like menstruation, or less often, usually every three months, if the patient desires. Estrogen therapy does not make a woman with nonfunctional ovaries fertile, but it plays an important role in assisted reproduction; the health of the uterus must be maintained with estrogen if an eligible woman with Turner Syndrome wishes to use IVF (using donated oocytes).\nEspecially in mosaic cases of Turner syndrome that contains Y-chromosome (e.g., 45,X/46,XY) due to the risk of development of ovarian malignancy (most common is gonadoblastoma) gonadectomy is recommended. Turner syndrome is characterized by primary amenorrhoea, premature ovarian failure (hypergonadotropic hypogonadism), streak gonads and infertility (however, technology (especially oocyte donation) provides the opportunity of pregnancy in these patients). Failure to develop secondary sex characteristics (sexual infantilism) is typical.\nCognition.\nNeurodevelopmental.\nIndividuals with Turner syndrome have normal intelligence. Verbal IQ is usually higher than performance IQ; one review of thirteen studies found an average verbal IQ of 101 compared to an average performance IQ of 89.\nPeople with Turner syndrome demonstrate relative strengths in verbal skills, but may exhibit weaker nonverbal skills \u2013 particularly in arithmetic, select visuospatial skills, and processing speed. They have difficulties with directional sense, visualization of three-dimensional shapes, properties of shapes, and symmetry and may have dyscalculia. Turner syndrome does not typically cause intellectual disability or impair cognition. However, learning difficulties are common among women with Turner syndrome, particularly a specific difficulty in perceiving spatial relationships, such as nonverbal learning disorder. This may also manifest itself as a difficulty with motor control or with mathematics. While it is not correctable, in most cases it does not cause difficulty in daily living. Most Turner syndrome patients are employed as adults and lead productive lives.\nAlso, a rare variety of Turner syndrome, known as \"Ring-X Turner syndrome\", has about a 60% association with intellectual disability. This variety accounts for around 2\u20134% of all Turner syndrome cases.\nPsychological.\nSocial difficulties appear to be an area of vulnerability for girls with Turner Syndrome. Counseling affected individuals and their families about the need to carefully develop social skills and relationships may prove useful in advancing social adaptation. Women with Turner syndrome may experience adverse psychosocial outcomes which can be improved through early intervention and the provision of appropriate psychological and psychiatric care. Although Turner syndrome constitutes a chronic medical condition, with possible physical, social, and psychological complications in a woman's life, hormonal and estrogen replacement therapy, and assisted reproduction, are treatments that can be helpful for Turner syndrome patients and improve their quality of life. Research shows a possible association between age at diagnosis and increased substance use and depressive symptoms. \nPrenatal.\nDespite the excellent postnatal prognosis, 99% of Turner syndrome conceptions are thought to end in miscarriage or stillbirth, and as many as 15% of all spontaneous abortions have the 45,X karyotype. Among cases that are detected by routine amniocentesis or chorionic villus sampling, one study found that the prevalence of Turner syndrome among tested pregnancies was 5.58 and 13.3 times higher, respectively, than among live neonates in a similar population.\nCause.\nTurner syndrome is caused by the complete or partial absence of one copy of the X chromosome in some or all the cells. The abnormal cells may have only one X (monosomy) (45,X) or they may be affected by one of several types of partial monosomy like a deletion of the short p arm of one X chromosome (46,X,del(Xp)) or the presence of an isochromosome with two q arms (46,X,i(Xq)) Turner syndrome has distinct features due to the lack of pseudoautosomal regions, which are typically spared from X-inactivation. In mosaic individuals, cells with X monosomy (45,X) may occur along with cells that are normal (46,XX), cells that have partial monosomies, or cells that have a Y chromosome (46,XY). The presence of mosaicism is estimated to be relatively common in affected individuals (67\u201390%).\nThe (46,X,i(Xq) isochromosome in the Turner syndrome is classified as a small supernumerary marker chromosome (sSMC). Two of the types of sSMCs in this syndrome contain parts of the genetic material from either an X or, much less frequently, Y chromosome and may or may not contain an \"XIST\" gene. Turner syndrome females with (46,X,i(Xq) sSMC consisting of a partial X chromosome that does not contain the \"XIST\" gene express at least some of this sSMC's genetic material and therefore contain excesses of this material. In consequence, they have a more serious form of the Turner syndrome that ranges form moderately severe to extremely severe. The extremely severe cases have anencephaly (absence of a major portion of the brain, skull, and scalp), agenesis of the corpus callosum (lack of the thick tract of nerve fibers that connect the left and right cerebral hemispheres), and complex heart deformities. Individuals with Turner syndrome that have partial X chromosome containing(46,X,i(Xq) sSMCs that have the \"XIST\" gene do not express this sSMC's genetic material and do not have the more severe manifestations of the syndrome.\nInheritance.\nIn the majority of cases where monosomy occurs, the X chromosome comes from the mother. This may be due to a nondisjunction in the Mother. Meiotic errors that lead to the production of X with p arm deletions. Isochromosome X or ring chromosome X on the other hand are formed equally often by both parents. Overall, the functional X chromosome usually comes from the mother.\nIn most cases, Turner syndrome is a sporadic event, and for the parents of an individual with Turner syndrome the risk of recurrence is not increased for subsequent pregnancies. Rare exceptions may include the presence of a balanced translocation of the X chromosome in a parent, or where the mother has 45,X mosaicism restricted to her germ cells.\nDiagnosis.\nPrenatal.\nTurner syndrome may be diagnosed by amniocentesis or chorionic villus sampling during pregnancy.\nUsually, fetuses with Turner syndrome can be identified by abnormal ultrasound findings (\"i.e.\", heart defect, kidney abnormality, cystic hygroma, ascites). In a study of 19 European registries, 67.2% of prenatally diagnosed cases of Turner syndrome were detected by abnormalities on ultrasound. 69.1% of cases had one anomaly present, and 30.9% had two or more anomalies.\nAn increased risk of Turner syndrome may also be indicated by abnormal triple or quadruple maternal serum screen. The fetuses diagnosed through positive maternal serum screening are more often found to have a mosaic karyotype than those diagnosed based on ultrasonographic abnormalities, and conversely, those with mosaic karyotypes are less likely to have associated ultrasound abnormalities.\nPostnatal.\nTurner syndrome can be diagnosed postnatally at any age. Often, it is diagnosed at birth due to heart problems, an unusually wide neck or swelling of the hands and feet. However, it is also common for it to go undiagnosed for several years, often until the girl reaches the age of puberty and fails to develop typically (the changes associated with puberty do not occur). In childhood, a short stature can be indicative of Turner syndrome.\nA test called a karyotype, also known as a chromosome analysis, analyzes the chromosomal composition of the individual. This is the test of choice to diagnose Turner syndrome.\nTreatment.\nAs a chromosomal condition, there is no cure for Turner syndrome. However, much can be done to minimize the symptoms. While most of the physical findings are harmless, significant medical problems can be associated with the syndrome. Most of these significant conditions are treatable with surgery and other therapies including hormonal therapy.\nEpidemiology.\nTurner syndrome occurs in between one in 2,000 and one in 5,000 females at birth.\nApproximately 99 percent of fetuses with Turner syndrome spontaneously terminate during the first trimester. Turner syndrome accounts for about 10 percent of the total number of spontaneous abortions in the United States.\nHistory.\nThe syndrome is named after Henry Turner, an American endocrinologist, who described it in 1938. In Europe, it is often called \"Ullrich\u2013Turner syndrome\" and was sometimes called \"Bonnevie\u2013Ullrich syndrome\" although the latter term is rarely used today. Both syndrome names acknowledge(d) that earlier cases had also been described 1930 by European doctors Kristine Bonnevie and Otto Ullrich. In Russian and Soviet literature, it is called \"Shereshevsky\u2013Turner syndrome\" to acknowledge that the condition was first described as hereditary in 1925 by the Soviet endocrinologist Nikolai Shereshevsky, who believed that it was due to the underdevelopment of the gonads and the anterior pituitary gland and was combined with congenital malformations of internal development.\nThe first published report of a female with a 45,X karyotype was in 1959 by Charles Ford and colleagues in Harwell near Oxford, and Guy's Hospital in London. It was found in a 14-year-old girl with signs of Turner syndrome.\nCultural and social impacts.\nIn many cultures, the symptoms of Turner Syndrome\u2014particularly short stature and infertility\u2014carried social stigmas. Historically, women who were unable to conceive were often marginalized or considered undesirable as wives. For example, Turner syndrome has been identified with the ayelonit gender category in Rabbinic Judaism, where it connoted an apparent woman with \"masculine\" characteristics. While the understanding of TS has improved, the psychological impact of infertility remains a significant concern for many women with the condition. In modern times, advocacy groups have played a vital role in raising awareness and offering support to those with TS, helping to reduce stigma and provide better access to medical care.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49365", "revid": "39598171", "url": "https://en.wikipedia.org/wiki?curid=49365", "title": "LGM-30 Minuteman", "text": "American ICBM\nThe LGM-30 Minuteman is an American land-based intercontinental ballistic missile (ICBM) in service with the Air Force Global Strike Command. As of 2024[ [update]], the LGM-30G (Version 3) is the only land-based ICBM in service in the United States and represents the land leg of the U.S. nuclear triad, along with the Trident II submarine-launched ballistic missile (SLBM) and nuclear weapons carried by long-range strategic bombers.\nDevelopment of the Minuteman began in the mid-1950s when basic research indicated that a solid-fuel rocket motor could stand ready to launch for long periods of time, in contrast to liquid-fueled rockets that required fueling before launch and so might be destroyed in a surprise attack. The missile was named for the colonial minutemen of the American Revolutionary War, who could be ready to fight on short notice.\nThe Minuteman entered service in 1962 as a deterrence weapon that could hit Soviet cities with a second strike and countervalue counterattack if the U.S. was attacked. However, the development of the United States Navy (USN) UGM-27 Polaris, which addressed the same role, allowed the Air Force to modify the Minuteman, boosting its accuracy enough to attack hardened military targets, including Soviet missile silos. The Minuteman II entered service in 1965 with a host of upgrades to improve its accuracy and survivability in the face of an anti-ballistic missile (ABM) system the Soviets were known to be developing. In 1970, the Minuteman III became the first deployed ICBM with multiple independently targetable reentry vehicles (MIRV): three smaller warheads that improved the missile's ability to strike targets defended by ABMs. However, the Minutemen III missiles were later \"de-MIRVed\"; since 2016 they have had only a single warhead per missile, either a W78 (335 kT) or W87 (300 kT).\nBy the 1970s, 1,000 Minuteman missiles were deployed. This force has shrunk to 400 Minuteman III missiles as of \u00a02017[ [update]], deployed in missile silos around Malmstrom AFB, Montana; Minot AFB, North Dakota; and Francis E. Warren AFB, Wyoming. The Minuteman III will be progressively replaced by the new LGM-35 Sentinel ICBM, to be built by Northrop Grumman, beginning in 2030.\nHistory.\nEdward Hall and solid fuels.\nMinuteman owes its existence largely to Air Force Colonel Edward N. Hall, who in 1956 was given charge of the solid-fuel-propulsion division of General Bernard Schriever's Western Development Division, created to lead development of the SM-65 Atlas and HGM-25A Titan I ICBMs. Solid fuels were already commonly used in short-range rockets. Hall's superiors were interested in short- and medium-range missiles with solids, especially for use in Europe where the fast reaction time was an advantage for weapons that might be attacked by Soviet aircraft. But Hall was convinced that they could be used for a true ICBM with a range.152\nTo achieve the required energy, that year Hall began funding research at Boeing and Thiokol into the use of ammonium perchlorate composite propellant. Adapting a concept developed in the UK, they cast the fuel into large cylinders with a star-shaped hole running along the inner axis. This allowed the fuel to burn along the entire length of the cylinder, rather than just the end as in earlier designs. The increased burn rate meant increased thrust. This also meant the heat was spread across the entire motor, instead of the end, and because it burned from the inside out it did not reach the wall of the missile fuselage until the fuel was finished burning. In comparison, older designs burned primarily from one end to the other, meaning that at any instant one small section of the fuselage was being subjected to extreme loads and temperatures.\nGuidance of an ICBM is based not only on the direction the missile is traveling but the precise instant that thrust is cut off. Too much thrust and the warhead will overshoot its target, too little and it will fall short. Solids are normally very hard to predict in terms of burn time and their instantaneous thrust during the burn, which made them questionable for the sort of accuracy required to hit a target at intercontinental range. While this initially appeared to be an insurmountable problem, it ended up being solved in an almost trivial fashion. A series of ports were added inside the rocket nozzle that were opened when the guidance systems called for engine cut-off. The reduction in pressure was so abrupt that the remaining fuel broke up and blew out the nozzle without contributing to the thrust.\nThe first to use these developments was the US Navy. It had been involved in a joint program with the US Army to develop the liquid-fueled PGM-19 Jupiter, but had always been skeptical of the system. The Navy felt that liquid fuels were too dangerous to use onboard ships, especially submarines. Rapid success in the solids development program, combined with Edward Teller's promise of much lighter nuclear warheads during Project Nobska, led the Navy to abandon Jupiter and begin development of their own solid-fuel missile. Aerojet's work with Hall was adapted for their UGM-27 Polaris starting in December 1956.\nMissile farm concept.\nThe US Air Force saw no pressing need for a solid fuel ICBM. Development of the SM-65 Atlas and SM-68 Titan ICBMs was progressing, and \"storable\" (hypergolic) liquid propellants were being developed that would allow missiles to be left in a ready-to-shoot form for extended periods. These could be placed in missile silos for added protection, and launch in minutes. This met their need for a weapon that would be safe from sneak attacks; hitting all of the silos within a limited time window before they could launch simply did not seem possible.153\nBut Hall saw solid fuels not only as a way to improve launch times or survivability, but part of a radical plan to greatly reduce the cost of ICBMs so that thousands could be built. He envisioned a future where ICBMs were the primary weapon of the US, not in the supporting role of \"last ditch backup\" as the Air Force saw them at the time. This would require huge deployments, which would not be possible with existing weapons due to their high cost and operational manpower requirements. A solid fuel design would be simpler to build, and easier to maintain.153\nHall's ultimate plan was to build a number of integrated missile \"farms\" that included factories, missile silos, transport and recycling. He was aware that new computerized assembly lines would allow continual production, and that similar equipment would allow a small team to oversee operations for dozens or hundreds of missiles, radically reducing the manpower requirements. Each farm would support between 1,000 and 1,500 missiles being produced in a continuous low rate cycle. Systems in a missile would detect failures, at which point it would be removed and recycled, while a newly built missile would take its place.153 The missile design was based purely on lowest possible cost, reducing its size and complexity because \"the basis of the weapon's merit was its low cost per completed mission; all other factors \u2013 accuracy, vulnerability, and reliability \u2013 were secondary.\"154\nHall's plan did not go unopposed, especially by the more established names in the ICBM field. Ramo-Wooldridge pressed for a system with higher accuracy, but Hall countered that the missile's role was to attack Soviet cities, and that \"a force which provides numerical superiority over the enemy will provide a much stronger deterrent than a numerically inferior force of greater accuracy.\"154 Hall was known for his \"friction with others\" and in 1958 Schriever removed him from the Minuteman project, sending him to the UK to oversee deployment of the Thor IRBM.152 On his return to the US in 1959, Hall retired from the Air Force. He received his second Legion of Merit in 1960 for his work on solid fuels.\nAlthough he was removed from the Minuteman project, Hall's work on cost reduction had already produced a new design of diameter, much smaller than the Atlas and Titan at , which meant smaller and cheaper silos. Hall's goal of dramatic cost reduction was a success, although many of the other concepts of his missile farm were abandoned.154\nGuidance system.\nPrevious long-range missiles used liquid fuels that could be loaded only just prior to firing. The loading process took from 30 to 60 minutes in typical designs. Although lengthy, this was not considered to be a problem at the time, because it took about the same amount of time to spin up the inertial guidance system, set the initial position, and program in the target coordinates.156\nMinuteman was designed from the outset to be launched in minutes. While solid fuel eliminated the fueling delays, the delays in starting and aligning the guidance system remained. For the desired quick launch, the guidance system would have to be kept running and aligned at all times. This was a serious problem for the mechanical systems, especially the gyroscopes which used ball bearings.157\nAutonetics had an experimental design using air bearings that they claimed had been running continually from 1952 to 1957.157 Autonetics further advanced the state of the art by building the platform in the form of a ball which could rotate in two directions. Conventional solutions used a shaft with ball bearings at either end that allowed it to rotate around a single axis only. Autonetics' design meant that only two gyros would be needed for the inertial platform, instead of the typical three.159\nThe last major advance was to use a general-purpose digital computer in place of the analog or custom designed digital computers. Previous missile designs normally used two single-purpose and very simple electromechanical computers; one ran the autopilot that kept the missile flying along a programmed course, and the second compared the information from the inertial platform to the target coordinates and sent any needed corrections to the autopilot. To reduce the total number of parts used in Minuteman, a single faster computer was used, running separate subroutines for these functions.160\nSince the guidance program would not be running while the missile sat in the silo, the same computer was also used to run a program that monitored the various sensors and test equipment. With older designs this had been handled by external systems, requiring miles of extra wiring and many connectors to locations where test instruments could be connected during servicing. Now these could all be accomplished by communicating with the computer through a single connection. In order to store multiple programs, the computer, the D-17B, was built in the form of a drum machine but used a hard disk in place of the drum.160\nBuilding a computer with the required performance, size and weight demanded the use of transistors, which were at that time very expensive and not very reliable. Earlier efforts to use computers for guidance, BINAC and the system on the SM-64 Navaho, had failed and were abandoned. The Air Force and Autonetics spent millions on a program to improve transistor and component reliability 100 times, leading to the \"Minuteman high-rel parts\" specifications. The techniques developed during this program were equally useful for improving all transistor construction, and greatly reduced the failure rate of transistor production lines in general. This improved yield, which had the effect of greatly lowering production costs, had enormous spin-off effects in the electronics industry.\nUsing a general-purpose computer also had long-lasting effects on the Minuteman program and the US's nuclear stance in general. With Minuteman, the targeting could be easily changed by loading new trajectory information into the computer's hard drive, a task that could be completed in a few hours. Earlier ICBMs' custom wired computers, on the other hand, could have attacked only a single target, whose precise trajectory information was hard-coded directly in the system's logic.156\nMissile gap.\nIn 1957, a series of intelligence reports suggested the Soviet Union was far ahead in the missile race and would be able to overwhelm the US by the early 1960s. If the Soviets were building missiles in the numbers being predicted by the CIA and others within the defense establishment, by as early as 1961 they would have enough to attack all SAC and ICBM bases in the US in a single first strike. It was later demonstrated that this \"missile gap\" was just as fictional as the \"bomber gap\" of a few years earlier, but through the late 1950s, it was a serious concern.\nThe Air Force responded by beginning research into survivable strategic missiles, starting the WS-199 program. Initially, this focused on air-launched ballistic missiles, which would be carried aboard aircraft flying far from the Soviet Union, and thus impossible to attack by either ICBM, because they were moving, or long-range interceptor aircraft, because they were too far away. In the shorter term, looking to rapidly increase the number of missiles in its force, Minuteman was given crash development status starting in September 1958. Advanced surveying of the potential silo sites had already begun in late 1957.46\nAdding to their concerns was a Soviet anti-ballistic missile system which was known to be under development at Sary Shagan. WS-199 was expanded to develop a maneuvering reentry vehicle (MARV), which greatly complicated the problem of shooting down a warhead. Two designs were tested in 1957, Alpha Draco and the Boost Glide Reentry Vehicle. These used long and skinny arrow-like shapes that provided aerodynamic lift in the high atmosphere, and could be fitted to existing missiles like Minuteman.\nThe shape of these reentry vehicles required more room on the front of the missile than a traditional reentry vehicle design. To allow for this future expansion, the Minuteman silos were revised to be built deeper. Although Minuteman would not deploy a boost-glide warhead, the extra space proved invaluable in the future, as it allowed the missile to be extended and carry more fuel and payload.46\nPolaris.\nDuring Minuteman's early development, the Air Force maintained the policy that the manned strategic bomber was the primary weapon of nuclear war. Blind bombing accuracy on the order of was expected, and the weapons were sized to ensure even the hardest targets would be destroyed as long as the weapon fell within this range. The USAF had enough bombers to attack every military and industrial target in the USSR and was confident that its bombers would survive in sufficient numbers that such a strike would utterly destroy the country.202\nSoviet ICBMs upset this equation to a degree. Their accuracy was known to be low, on the order of , but they carried large warheads that would be useful against Strategic Air Command's bombers, which parked in the open. Since there was no system to detect the ICBMs being launched, the possibility was raised that the Soviets could launch a sneak attack with a few dozen missiles that would take out a significant portion of SAC's bomber fleet.202\nIn this environment, the Air Force saw their own ICBMs not as a primary weapon of war, but as a way to ensure that the Soviets would not risk a sneak attack. ICBMs, especially newer models that were housed in silos, could be expected to survive an attack by a single Soviet missile. In any conceivable scenario where both sides had similar numbers of ICBMs, the US forces would survive a sneak attack in sufficient numbers to ensure the destruction of all major Soviet cities in return. The Soviets would not risk an attack under these conditions.202\nConsidering this \"countervalue\" attack concept, strategic planners calculated that an attack of \"400 equivalent megatons\" aimed at the largest Soviet cities would promptly kill 30% of their population and destroy 50% of their industry. Larger attacks raised these numbers only slightly, as all of the larger targets would already have been hit. This suggested that there was a \"finite deterrent\" level around 400 megatons that would be enough to prevent a Soviet attack no matter how many missiles they had of their own. All that had to be ensured was that the US missiles survived, which seemed likely given the low accuracy of the Soviet weapons.199 Reversing the problem, the addition of ICBMs to the US Air Force's arsenal did not eliminate the need, or desire, to attack Soviet military targets, and the Air Force maintained that bombers were the only suitable platform in that role.199\nInto this argument came the Navy's UGM-27 Polaris. Launched from submarines, Polaris was effectively invulnerable and had enough accuracy to attack Soviet cities. If the Soviets improved the accuracy of their missiles this would present a serious threat to the Air Force's bombers and missiles, but none at all to the Navy's submarines. Based on the same 400 equivalent megatons calculation, they set about building a fleet of 41 submarines carrying 16 missiles each, giving the Navy a finite deterrent that was unassailable.197\nThis presented a serious problem for the Air Force. They were still pressing for the development of newer bombers, like the supersonic B-70, for attacks against military targets, but this role seemed increasingly unlikely in a nuclear war scenario. A February 1960 memo by RAND, entitled \"The Puzzle of Polaris\", was passed around among high-ranking Air Force officials. It suggested that Polaris negated any need for Air Force ICBMs if they were also being aimed at Soviet cities. If the role of the missile was to present an unassailable threat to the Soviet population, Polaris was a far better solution than Minuteman. The document had long-lasting effects on the future of the Minuteman program, which, by 1961, was firmly evolving towards a counterforce capability.197\nKennedy.\nMinuteman's final tests coincided with the start of John F. Kennedy's presidency. His new Secretary of Defense, Robert McNamara, was tasked with continuing the expansion and modernisation of the US nuclear deterrent while limiting spending. McNamara began to apply cost/benefit analysis, and Minuteman's low production cost ensured its selection. Atlas and Titan were soon scrapped, and the storable liquid fueled Titan II deployment was severely curtailed.154 McNamara also cancelled the XB-70 bomber project.203\nMinuteman's low cost had spin-off effects on non-ICBM programs. The Army's LIM-49 Nike Zeus, an interceptor missile capable of shooting down Soviet warheads, provided another way to prevent a sneak attack. This had initially been proposed as a way to defend the SAC bomber fleet. The Army argued that upgraded Soviet missiles might be able to attack US missiles in their silos, and Zeus would be able to blunt such an attack. Zeus was expensive and the Air Force said it was more cost-effective to build another Minuteman missile. Given the large size and complexity of the Soviet liquid-fueled missiles, an ICBM building race was one the Soviets could not afford. Zeus was canceled in 1963.\nCounterforce.\nMinuteman's selection as the primary Air Force ICBM was initially based on the same \"second strike\" logic as their earlier missiles: that the weapon was primarily one designed to survive any potential Soviet attack and ensure they would be hit in return. But Minuteman had a combination of features that led to its rapid evolution into the US's primary weapon of nuclear war.\nChief among these qualities was its digital computer, the D-17B. This could be updated in the field with new targets and better information about the flight paths with relative ease, gaining accuracy for little cost. One of the unavoidable effects on the warhead's trajectory was the mass of the Earth, which contains many mass concentrations that pull on the warhead as it passes over them. Through the 1960s, the Defense Mapping Agency (now part of National Geospatial-Intelligence Agency) mapped these with increasing accuracy, feeding that information back into the Minuteman fleet. The Minuteman was initially deployed with a circular error probable (CEP) of about , but this had improved to about by 1965.166 This was accomplished without any mechanical changes to the missile or its navigation system.156\nAt those levels, the ICBM begins to approach the manned bomber in terms of accuracy; a small upgrade, roughly doubling the accuracy of the INS, would give it the same CEP as the manned bomber. Autonetics began such development even before the original Minuteman entered fleet service, and the Minuteman II had a CEP of . Additionally, the computers were upgraded with more memory, allowing them to store information for eight targets, which the missile crews could select among almost instantly, greatly increasing their flexibility.152 From that point, Minuteman became the US's primary deterrent weapon, until its performance was matched by the Navy's Trident missile of the 1980s.\nQuestions about the need for the manned bomber were quickly raised. The Air Force began to offer a number of reasons why the bomber offered value, in spite of costing more money to buy and being much more expensive to operate and maintain. Newer bombers with better survivability, like the B-70, cost many times more than the Minuteman, and, in spite of great efforts through the 1960s, became increasingly vulnerable to surface-to-air missiles. The B-1 of the early 1970s eventually emerged with a price tag around $200 million (equivalent to $ million in 2024) while the Minuteman IIIs built during the 1970s cost only $7 million ($ million in 2024).\nThe Air Force countered that having a variety of platforms complicated the defense; if the Soviets built an effective anti-ballistic missile system of some sort, the ICBM and SLBM fleet might be rendered useless, while the bombers would remain. This became the nuclear triad concept, which survives into the present. Although this argument was successful, the number of manned bombers has been repeatedly cut and the deterrent role increasingly passed to missiles.\n\"See also W56 Warhead\"\nMinuteman I (LGM-30A/B or SM-80/HSM-80A).\nDeployment.\nThe LGM-30A Minuteman I was first test-fired on 1 February 1961 at Cape Canaveral, entering into the Strategic Air Command's arsenal in 1962. After the first batch of Minuteman I's were fully developed and ready for stationing, the United States Air Force (USAF) had originally decided to put the missiles at Vandenberg AFB in California, but before the missiles were set to officially be moved there it was discovered that this first set of Minuteman missiles had defective boosters which limited their range from their initial to . This defect would cause the missiles to fall short of their targets if launched over the North Pole as planned. The decision was made to station the missiles at Malmstrom AFB in Montana instead. These changes would allow the missiles, even with their defective boosters, to reach their intended targets in the case of a launch.\nThe \"improved\" LGM-30B Minuteman I became operational at Ellsworth AFB, South Dakota, Minot AFB, North Dakota, F.E. Warren AFB, Wyoming, and Whiteman AFB, Missouri, in 1963 and 1964. All 800 Minuteman I missiles were delivered by June 1965. Each of the bases had 150 missiles emplaced; F.E. Warren had 200 of the Minuteman IB missiles. Malmstrom had 150 of the Minuteman I, and about five years later added 50 of the Minuteman II similar to those installed at Grand Forks AFB, ND.\nSpecifications.\nThe Minuteman I's length varied by variant. The Minuteman I/A had a length of and the Minuteman I/B had a length of . The Minuteman I weighed roughly , had an operational range of with an accuracy of about .\nGuidance.\nThe Minuteman I Autonetics D-17 flight computer used a rotating air bearing magnetic disk holding 2,560 \"cold-stored\" words in 20 tracks (write heads disabled after program fill) of 24 bits each and one alterable track of 128 words. The time for a D-17 disk revolution was 10 ms. The D-17 also used a number of short loops for faster access to intermediate results storage. The D-17 computational minor cycle was three disk revolutions or 30 ms. During that time all recurring computations were performed. For ground operations, the inertial platform was aligned and gyro correction rates updated.\nDuring a flight, filtered command outputs were sent by each minor cycle to the engine nozzles. Unlike modern computers, which use descendants of that technology for secondary storage on hard disk, the disk was the active computer memory. The disk storage was considered hardened to radiation from nearby nuclear explosions, making it an ideal storage medium. To improve computational speed, the D-17 borrowed an instruction look-ahead feature from the Autonetics-built Field Artillery Data Computer (M18 FADAC) that permitted simple instruction execution every word time.\nWarhead.\nAt its introduction into service in 1962, Minuteman I was fitted with the W59 warhead with a yield of 1 Mt. Production for the W56 warhead with a 1.2 Mt yield began in March 1963 and W59 production was ended in July 1963 with a production run of only 150 warheads before being retired in June 1969. The W56 would continue production until May 1969 with a production run of 1000 warheads. Mods 0 to 3 were retired by September 1966 and the Mod 4 version would remain in service until the 1990s.\nIt's not clear exactly why the W59 was replaced by the W56 after deployment but issues with \"... one-point safety\" and \"performance under aged conditions\" were cited in a 1987 congressional report regarding the warhead. Chuck Hansen alleged that all weapons sharing the \"Tsetse\" nuclear primary design including the W59 suffered from a critical one-point safety issue and suffered premature tritium aging issues that needed to be corrected after entry into service.\n\"See also W56 warhead\"\nMinuteman II (LGM-30F).\nThe LGM-30F Minuteman II was an improved version of the Minuteman I missile. Its first test launch took place on September 24, 1964. Development on the Minuteman II began in 1962 as the Minuteman I entered the Strategic Air Command's nuclear force. Minuteman II production and deployment began in 1965 and completed in 1967. It had an increased range, greater throw weight and guidance system with better azimuthal coverage, providing military planners with better accuracy and a wider range of targets. Some missiles also carried penetration aids, allowing the higher probability of kill against Moscow's anti-ballistic missile system. The payload consisted of a single Mk-11C reentry vehicle containing a W56 nuclear warhead with a yield of 1.2 megatons of TNT (5 PJ).\nSpecifications.\nThe Minuteman II had a length of , weighed roughly , had an operational range of with an accuracy of about .\nThe major new features provided by Minuteman II were:\nSystem modernization was concentrated on launch facilities and command and control facilities. This provided decreased reaction time and increased survivability when under nuclear attack. Final changes to the system were performed to increase compatibility with the expected LGM-118A Peacekeeper. These newer missiles were later deployed into modified Minuteman silos.\nThe Minuteman II program was the first mass-produced system to use a computer constructed from integrated circuits (the Autonetics D-37C). The Minuteman II integrated circuits were diode\u2013transistor logic and diode logic made by Texas Instruments. The other major customer of early integrated circuits was the Apollo Guidance Computer, which had similar weight and ruggedness constraints. The Apollo integrated circuits were resistor\u2013transistor logic made by Fairchild Semiconductor. The Minuteman II flight computer continued to use rotating magnetic disks for primary storage. The Minuteman II included diodes by Microsemi Corporation.\n\"See also W62 warhead\"\nMinuteman III (LGM-30G).\nThe LGM-30G Minuteman III program started in 1966 and included several improvements over the previous versions. Its first test launch took place on August 16, 1968. It was first deployed in 1970. Most modifications related to the final stage and reentry system (RS). The final (third) stage was improved with a new fluid-injected motor, giving finer control than the previous four-nozzle system.\nPerformance improvements realized in Minuteman III include increased flexibility in reentry vehicle (RV) and penetration aids deployment, increased survivability after a nuclear attack, and increased payload capacity. The missile retains a gimballed inertial navigation system.\nMinuteman III originally contained the following distinguishing features:\nThe Minuteman III missiles use D-37D computers and complete the 1,000 missile deployment of this system. The initial cost of these computers range from about $139,000 (D-37C) to $250,000 (D-17B).\nThe existing Minuteman III missiles have been further improved over the decades in service, with more than $7 billion spent in the 2010s to upgrade the 450 missiles.\nSpecifications.\nThe Minuteman III has a length of , weighs , an operational range of , and an accuracy of about .\nW78 warhead.\nIn December 1979 the higher-yield W78 warhead (335\u2013350 kilotons) began replacing a number of the W62s deployed on the Minuteman IIIs. These were delivered in the Mark 12A reentry vehicle. A small, unknown number of the previous Mark 12 RVs were retained operationally, however, to maintain a capability to attack more-distant targets in the south-central Asian republics of the USSR (the Mark 12 RV weighed slightly less than the Mark 12A).\nGuidance Replacement Program.\nThe Guidance Replacement Program replaces the NS20A Missile Guidance Set with the NS50 Missile Guidance Set. The newer system extends the service life of the Minuteman missile beyond the year 2030 by replacing aging parts and assemblies with current, high reliability technology while maintaining the current accuracy performance. The replacement program was completed 25 February 2008.\nPropulsion Replacement Program.\nBeginning in 1998 and continuing through 2009, the Propulsion Replacement Program extends the life and maintains the performance by replacing the old solid propellant boosters (downstages).\nSingle Reentry Vehicle.\nThe Single Reentry Vehicle modification enabled the United States ICBM force to abide by the now-voided START II treaty requirements by reconfiguring Minuteman III missiles from three reentry vehicles down to one. Though it was eventually ratified by both parties, START II never entered into force and was essentially superseded by follow-on agreements such as SORT and New START, which do not limit MIRV capability. Minuteman III remains fitted with a single warhead due to the warhead limitations in New START.\nSafety Enhanced Reentry Vehicle.\nBeginning in 2005, Mk-21/W87 RVs from the deactivated Peacekeeper missile were replaced on the Minuteman III force under the Safety Enhanced Reentry Vehicle (SERV) program. The older W78 did not have many of the safety features of the newer W87, such as insensitive high explosives, as well as more advanced safety devices. In addition to implementing these safety features in at least a portion of the future Minuteman III force, the decision to transfer W87s onto the missile was based on two features that improved the targeting capabilities of the weapon: more fuzing options which allowed for greater targeting flexibility, and the most accurate reentry vehicle available, which provided a greater probability of damage to the designated targets.\nDeployment.\nThe Minuteman III missile entered service in 1970, with weapon systems upgrades included during the production run from 1970 to 1978 to increase accuracy and payload capacity. As of \u00a02024[ [update]], the USAF plans to operate it until the mid-2030s.\nThe LGM-118A Peacekeeper (MX) ICBM, which was to have replaced the Minuteman, was retired in 2005 as part of START II.\nA total of 450 LGM-30G missiles are emplaced at F.E. Warren Air Force Base, Wyoming (90th Missile Wing), Minot Air Force Base, North Dakota (91st Missile Wing), and Malmstrom Air Force Base, Montana (341st Missile Wing). All Minuteman I and Minuteman II missiles have been retired. The United States prefers to keep its MIRV deterrents on submarine-launched Trident Nuclear Missiles In 2014, the Air Force decided to put fifty Minuteman III silos into \"warm\" unarmed status, taking up half of the 100 slots in America's allowable nuclear reserve. These can be reloaded in the future if necessary.\nTesting.\nMinuteman III missiles are regularly tested with launches from Vandenberg Space Force Base in order to validate the effectiveness, readiness, and accuracy of the weapon system, as well as to support the system's primary purpose, nuclear deterrence. The safety features installed on the Minuteman III for each test launch allow the flight controllers to terminate the flight at any time if the systems indicate that its course may take it unsafely over inhabited areas. Since these flights are for test purposes only, even terminated flights can send back valuable information to correct a potential problem with the system.\nThe test of an unarmed Minuteman III failed on November 1, 2023, from Vandenberg Space Force Base, California. The U.S. Air Force said it had blown up the missile over the Pacific Ocean after an anomaly was detected following its launch.\nThe 576th Flight Test Squadron is responsible for planning, preparing, conducting, and assessing all ICBM ground and flight tests.\nAirborne Launch Control System (ALCS).\nThe Airborne Launch Control System (ALCS) is an integral part of the Minuteman ICBM command and control system and provides a survivable launch capability for the Minuteman ICBM force if ground-based launch control centers (LCCs) are destroyed.\nWhen the Minuteman ICBM was first placed on alert, the Soviet Union did not have the number of weapons, accuracy, nor significant nuclear yield to completely destroy the Minuteman ICBM force during an attack. However, starting in the mid-1960s, the Soviets began to gain parity with the US and potentially had the capability to target and successfully attack the Minuteman force with an increased number of ICBMs that had greater yields and accuracy than were previously available.\nStudying the problem, SAC realized that in order to prevent the US from launching all 1,000 Minuteman ICBMs, the Soviets did not have to target all 1,000 Minuteman missile silos. The Soviets needed to launch only a disarming decapitation strike against the 100 Minuteman LCCs \u2013 the command and control sites \u2013 in order to prevent the launch of all Minuteman ICBMs. Even though the Minuteman ICBMs would have been left unscathed in their missile silos following an LCC decapitation strike, the Minuteman missiles could not be launched without a command and control capability.\nIn other words, the Soviets needed only 100 warheads to eliminate command and control of the Minuteman ICBMs. Even if the Soviets chose to expend two to three warheads per LCC for assured damage expectancy, the Soviets would have had to expend only up to 300 warheads to disable the Minuteman ICBM force \u2013 far less than the total number of Minuteman silos. The Soviets could have then used the remaining warheads to strike other targets they chose.\nFaced with only a few Minuteman LCC targets, the Soviets could have concluded that the odds of being successful in a Minuteman LCC decapitation strike were higher with less risk than it would have been having to face the almost insurmountable task of successfully attacking and destroying 1000 Minuteman silos and 100 Minuteman LCCs to ensure Minuteman was disabled. This theory motivated SAC to design a survivable means to launch Minuteman, even if all the ground-based command and control sites were destroyed.\nAfter thorough testing and modification of EC-135 command post aircraft, the ALCS demonstrated its capability on 17 April 1967 by launching an ERCS configured Minuteman II out of Vandenberg AFB, CA. Afterward, ALCS achieved Initial Operational Capability on 31 May 1967. From that point on, airborne missileers stood alert with ALCS-capable EC-135 aircraft for several decades. All Minuteman ICBM Launch Facilities were modified and built to have the capability to receive commands from ALCS. With ALCS standing alert around-the-clock, the Soviets could no longer successfully launch a Minuteman LCC decapitation strike. Even if the Soviets attempted to do so, EC-135s equipped with the ALCS could fly overhead and launch the remaining Minuteman ICBMs in retaliation.\nWith the ALCS on alert, the Soviet war planning was complicated by forcing them to target not only the 100 LCCs, but also the 1,000 silos with more than one warhead in order to guarantee destruction. This would have required upwards of 3,000 warheads to complete such an attack. The odds of being successful in such an attack on the Minuteman ICBM force would have been extremely low.\nThe ALCS is operated by airborne missileers from the Air Force Global Strike Command's (AFGSC) 625th Strategic Operations Squadron (STOS) and United States Strategic Command (USSTRATCOM). The weapon system is also located on board the United States Navy's E-6B Mercury. The ALCS crews are integrated into the battle staff of the USSTRATCOM \"Looking Glass\" Airborne Command Post (ABNCP) and are on alert around-the-clock. Although the Minuteman ICBM force has been reduced since the end of the Cold War, the ALCS continues to act as a force multiplier by ensuring that some enemy cannot launch a successful Minuteman LCC decapitation strike.\nOther roles.\nMobile Minuteman.\nMobile Minuteman was a program for rail-based ICBMs to help increase survivability and for which the USAF released details on 12 October 1959. Minuteman Mobility Test Trains were first exercised from 20 June to 27 August 1960 at Hill Air Force Base, and the 4062nd Strategic Missile Wing (Mobile) was organized 1 December 1960. It was planned to include three missile train squadrons, each with 10 trains carrying 3 missiles per train. During the Kennedy/McNamara force reductions, the Department of Defense announced \"that it has abandoned the plan for a mobile Minuteman ICBM. The concept called for 600 to be placed in service\u00a0\u2013 450 in silos and 150 on special trains, each train carrying 5 missiles.\" Kennedy announced on 18 March 1961 that the 3 squadrons were to be replaced with \"fixed-base squadrons\", and Strategic Air Command discontinued the 4062nd Strategic Missile Wing on 20 February 1962.\nAir-Launched ICBM.\nAir-Launched ICBM was a STRAT-X proposal in which SAMSO (Space &amp; Missile Systems Organization) successfully conducted an Air Mobile Feasibility Test that airdropped a Minuteman 1b from a C-5A Galaxy aircraft from over the Pacific Ocean. The missile fired at , and the 10-second engine burn carried the missile to 20,000 feet again before it dropped into the ocean. Operational deployment was discarded due to engineering and security difficulties, and the capability was a negotiating point in the Strategic Arms Limitation Talks.\nEmergency Rocket Communications System (ERCS).\nFrom 1963 through 1991, the National Command Authority communication relay system included the Emergency Rocket Communication System (ERCS). Specially designed rockets called BLUE SCOUT carried radio-transmitting payloads high above the continental United States, to relay messages to units within line-of-sight. In the event of a nuclear attack, ERCS payloads would relay pre-programmed messages giving the \"go-order\" to SAC units.\nBLUE SCOUT launch sites were located at Wisner, West Point and Tekamah, Nebraska. These locations were vital for ERCS effectiveness due to their centralized position in the US, within range of all missile complexes. In 1968, ERCS configurations were placed on the top of modified Minuteman II ICBMs (LGM-30Fs) under the control of the 510th Strategic Missile Squadron located at Whiteman Air Force Base, Missouri.\nThe Minuteman ERCS may have been assigned the designation LEM-70A.\nSatellite launching role.\nThe U.S. Air Force has considered using some decommissioned Minuteman missiles in a satellite launching role. These missiles would be stored in silos, for launch upon short notice. The payload would be variable and would have the ability to be replaced quickly. This would allow a surge capability in times of emergency.\nDuring the 1980s, surplus Minuteman missiles were used to power the Conestoga rocket produced by Space Services Inc. of America. It was the first privately funded rocket, but saw only three flights and was discontinued due to a lack of business. More recently, converted Minuteman missiles have been used to power the Minotaur line of rockets produced by Orbital Sciences (nowadays Northrop Grumman Innovation Systems).\nGround and air launch targets.\nL-3 Communications is currently using SR-19 SRBs, Minuteman II Second Stage Solid Rocket Boosters, as delivery vehicles for a range of different re-entry vehicles as targets for the THAAD and ASIP interceptor missile programs as well as radar testing.\nOperators.\nThe United States Air Force has been the only operator of the Minuteman ICBM weapons system, currently with three operational wings and one test squadron operating the LGM-30G. The active inventory in FY 2025 is 400 missiles and 45 Missile Alert Facilities (MAF).\nOperational units.\nThe basic tactical unit of a Minuteman wing is the squadron, consisting of five flights. Each flight consists of ten unmanned launch facilities (LFs) which are remotely controlled by a manned launch control center (LCC). A two-officer crew is on duty in the LCC, typically for 24 hours. The five flights are interconnected and status from any LF may be monitored by any of the five LCCs. Each LF is located at least three nautical miles (5.6\u00a0km) from any LCC.\nControl does not extend outside the squadron (thus the 319th Missile Squadron's five LCCs cannot control the 320th Missile Squadron's 50 LFs even though they are part of the same Missile Wing). Each Minuteman wing is assisted logistically by a nearby Missile Support Base (MSB). If the ground-based LCCs are destroyed or incapacitated, the Minuteman ICBMs can be launched by airborne missileers utilizing the Airborne Launch Control System.\nHistorical.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReplacement.\nA request for proposal for development and maintenance of a Ground Based Strategic Deterrent (GBSD) next-generation nuclear ICBM, was made by the US Air Force Nuclear Weapons Center, ICBM Systems Directorate, GBSD Division on 29 July 2016. The GBSD would replace MMIII in the land-based portion of the US Nuclear Triad. The new missile to be phased in over a decade from the late 2020s are estimated over a fifty-year life cycle to cost around $86 billion. Boeing, Lockheed Martin, and Northrop Grumman were competing for the contract.\nOn 21 August 2017, the US Air Force awarded 3-year development contracts to Boeing and Northrop Grumman, for $349 million and $329 million, respectively. One of these companies will be selected to produce this ground-based nuclear ICBM in 2020. In 2027, the GBSD program is expected to enter service and remain active until 2075.\nOn 14 December 2019, it was announced that Northrop Grumman had won the competition to build the future ICBM. Northrop won by default, as their bid was at the time the only bid left to be considered for the GBSD program (Boeing had dropped out of the bidding contest earlier in 2019). The US Air Force stated that it would \"proceed with an aggressive and effective sole-source negotiation.\"\nPreservation.\nThe Minuteman Missile National Historic Site in South Dakota preserves a Launch Control Facility (D-01) and a launch facility (D-09) under the control of the National Park Service. The North Dakota State Historical Society maintains the Ronald Reagan Minuteman Missile Site, preserving a Missile Alert Facility, Launch Control Center and Launch Facility in the WS-133B \"Deuce\" configuration, near Cooperstown, North Dakota.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49367", "revid": "49286892", "url": "https://en.wikipedia.org/wiki?curid=49367", "title": "Laurent-D\u00e9sir\u00e9 Kabila", "text": "President of the Democratic Republic of the Congo, 1997\u20132001\nLaurent-D\u00e9sir\u00e9 Kabila (; 27 November 1939 \u2013 16 January 2001) usually known as Laurent Kabila or Kabila the Father ( ), was a Congolese rebel and politician who served as the third president of the Democratic Republic of the Congo from 1997 until his assassination in 2001.\nKabila initially gained prominence as an opponent of Mobutu Sese Seko during the Congo Crisis (1960\u20131965). He took part in the Simba rebellion and led the Communist-aligned Fizi rebel breakaway state in eastern Congo from 1967 to 1988 before disappearing from public. In the 1990s, Kabila re-emerged as leader of the Alliance of Democratic Forces for the Liberation of Congo (ADFL), a Rwandan and Ugandan-sponsored rebel group that invaded Zaire and overthrew Mobutu during the First Congo War from 1996 to 1997. Following the war, Kabila became the new president of the country, whose name was changed back to the Democratic Republic of the Congo.\nThe following year, he ordered all foreign troops to leave the country following the Kasika massacre to prevent a potential coup, leading to the Second Congo War (1998\u20132003), in which his former Rwandan and Ugandan allies supported several rebel groups to overthrow him. In 2001, he was assassinated by one of his bodyguards, and was succeeded by his 29-year-old son Joseph.\nEarly life.\nKabila was born to the Luba people in Baudouinville, Katanga Province, (now Moba, Tanganyika Province), or Jadotville, Katanga Province, (now Likasi, Haut-Katanga Province) in the Belgian Congo. His father was a Luba and his mother was a Lunda; his father's ethnicity was defining in the patriarchal kinship system. It is claimed that he studied abroad (political philosophy in Paris, got a PhD in Tashkent, in Belgrade and at last in Dar es Salaam), but no proof has been found or provided.\nPolitical activities.\n1960s Congo Crisis.\nShortly after the Congo achieved independence in 1960, Katanga seceded under the leadership of Mo\u00efse Tshombe. Kabila organised the Baluba in an anti-secessionist rebellion in Manono. In September 1962 a new province, North Katanga, was established. He became a member of the provincial assembly and served as chief of cabinet for Minister of Information Ferdinand Tumba. In September 1963 he and other young members of the assembly were forced to resign, facing allegations of communist sympathies.\nKabila established himself as a supporter of hard-line Lumumbist Prosper Mwamba Ilunga. When the Lumumbists formed the Conseil National de Lib\u00e9ration, he was sent to eastern Congo to help organize a revolution, in particular in the Kivu and North Katanga provinces. This revolution was part of the larger Simba rebellions happening in the provinces at the time. In 1965, Kabila set up a cross-border rebel operation from Kigoma, Tanzania, across Lake Tanganyika.\nAssociation with Che Guevara.\nKabila met Che Guevara for the first time in late April 1965 where Guevara had appeared in the Congo with approximately 100 Cuban men who envisaged to bring about a Cuban-style revolution to overthrow the Congolese government. Guevara assisted Kabila and his rebel forces for a few months before Guevara judged Kabila (then age 26) as \"not the man of the hour\" he had alluded to, being too distracted and his men poorly trained and disciplined. This, in Guevara's opinion, accounted for Kabila showing up days late at times to provide supplies, aid, or backup to Guevara's men. Kabila preferred to spend most of his time at local bars or brothels instead of training his men or fighting the Congolese government forces. The lack of cooperation between Kabila and Guevara contributed to the suppression of the revolt in November that same year.\nIn Guevara's view, of all of the people he met during his campaign in Congo, only Kabila had \"genuine qualities of a mass leader\"; but Guevara castigated Kabila for a lack of \"revolutionary seriousness\". After the failure of the rebellion, Kabila turned to smuggling gold and timber on Lake Tanganyika. He also ran a bar and brothel in Kigoma, Tanzania.\nMarxist mini-state (1967\u20131988).\nIn 1967, Kabila and his remnant of supporters moved their operation into the mountainous Fizi \u2013 Baraka area of South Kivu in the Congo, and founded the People's Revolutionary Party (PRP). With the support of the People's Republic of China, the PRP created a secessionist Marxist state in South Kivu province, west of Lake Tanganyika.\nThe PRP state came to an end in 1988 and Kabila disappeared and was widely believed to be dead. While in Kampala, Kabila reportedly met Yoweri Museveni, the future president of Uganda. Museveni and former Tanzanian President Julius Nyerere later introduced Kabila to Paul Kagame, who would become president of Rwanda. These personal contacts became vital in mid-1990s, when Uganda and Rwanda sought a Congolese face for their intervention in Zaire.\nFirst Congo War.\nAs Rwandan Hutu refugees fled to Congo (then Zaire) after the 1994 genocide in Rwanda, refugee camps along the Zaire-Rwanda border became militarized with Hutu militia vowing to retake power in Rwanda. The Kigali regime considered these militias as a security threat and was seeking a way to dismantle those refugee camps. After Kigali had expressed its security concerns to Kinshasa, requesting that refugee camps get moved further inside the country, and Kinshasa ignored these concerns, Kigali believed that only military option could solve the issue. However, a military operation inside Zaire was likely be seen by the international community as an . A plan was put in place to foment a Banyamulenge rebellion that would serve as a cover. The Alliance of Democratic Forces for the Liberation of Congo (AFDL) was then born when Rwanda brought together four Congolese political exiles, with Kabila as its spokesperson and one of the co-founders, on 18 October 1996. Furthermore, on 4 January 1997, an agreement to merge the four founding political parties into a single movement for \"assembling all the live forces of the Congolese nation,\" was adopted. Kabila was made the leader of the AFDL as the chairman of its executive committee.\nAs a native of Katanga, he was used to give the AFDL more of a national character instead of being a Tutsi movement. By mid-1997, the AFDL had almost completely overrun the country and the remains of Mobutu's army. Only the country's decrepit infrastructure slowed Kabila's forces down; in many areas, the only means of transit were irregularly used dirt paths. Following failed peace talks held on board of the South African ship SAS \"Outeniqua\", Mobutu fled into exile on 16 May.\nThe next day, from his base in Lubumbashi, Kabila declared victory and installed himself as president. Kabila suspended the Constitution and changed the name of the country from Zaire to the Democratic Republic of the Congo\u2014the country's official name from 1964 to 1971. He made his grand entrance into Kinshasa on 20 May and was sworn in on 29 May, officially commencing his tenure as president.\nPresidency (1997\u20132001).\nKabila had previously been a committed Marxist, but his policies at this point were social democratic. He declared that elections would not be held for two years, since it would take him at least that long to restore order. While some in the West hailed Kabila as representing a \"new breed\" of African leadership, critics charged that Kabila's policies differed little from his predecessor's, being characterised by authoritarianism, corruption, and human rights abuses. As early as late 1997, Kabila was being denounced as \"another Mobutu\". Kabila was also accused trying to set up a personality cult. Mobutu's former minister of information, Dominique Sakombi Inongo, was retained by Kabila; he branded Kabila as \"the Mzee,\" and created posters reading \"Here is the man we needed\" () appeared all over the country.\nBy 1998, Kabila's former allies in Uganda and Rwanda had turned against him and backed a new rebellion of the Rally for Congolese Democracy (RCD) and the Movement for the Liberation of the Congo (MLC). Kabila found new allies in Angola, Namibia and Zimbabwe, and managed to hold on in the south and west of the country and by July 1999, peace talks led to the withdrawal of most foreign forces.\nAssassination and trial.\nOn 16 January 2001, Kabila was shot in his office at the Palais de Marbre and subsequently transported to Zimbabwe for medical treatment. The DRC's authorities managed to keep power, despite Kabila's assassination. The exact circumstances are still contested. Kabila reportedly died on the spot, according to DRC's then-health minister Leonard Mashako Mamba, who was in the next door office when Kabila was shot and arrived immediately after the assassination. The government claimed that Kabila was still alive, however, and he was flown to a hospital in Zimbabwe after he was shot so that DRC authorities could organize the succession.\nThe Congolese government announced that he had died of his wounds on 18 January. One week later, his body was returned to Congo for a state funeral and his son, Joseph Kabila, became president ten days later. By doing so, DRC officials were accomplishing the \"verbal testimony\" of the deceased President. Then Justice Minister Mwenze Kongolo and Kabila's aide-de-camp Eddy Kapend reported that Kabila had told them that his son Joseph, then number two of the army, should take over, if he were to die in office.\nThe investigation into Kabila's assassination led to 135 people, including four children, being tried before a special military tribunal. The alleged ringleader, Colonel Eddy Kapend (one of Kabila's cousins), and 25 others were sentenced to death in January 2003, but not executed. Of the remaining defendants, 64 were incarcerated, with sentences from six months to life, and 45 were exonerated. Some individuals were also accused of being involved in a plot to overthrow his son. Among them was Kabila's special advisor Emmanuel Dungia, former ambassador to South Africa. Many people believe the trial was flawed and the convicted defendants innocent; doubts are summarized in an \"Al Jazeera\" investigative film, \"Murder in Kinshasa\".\nIn January 2021, DRC's President F\u00e9lix Tshisekedi pardoned all those convicted in the murder of Laurent-D\u00e9sir\u00e9 Kabila in 2001. Colonel Eddy Kapend and his co-defendants, who have been incarcerated for 15 years, were released.\nPersonal life.\nHe had at least nine children with his wife Sifa Mahanya: Josephine, C\u00e9cile, Fifi, Selemani, twins Jaynet and Joseph, Zo\u00e9, Anina and Tetia. He was also the alleged father of Aim\u00e9e Kabila Mulengela whose mother is Za\u00efna Kibangula.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49369", "revid": "50395928", "url": "https://en.wikipedia.org/wiki?curid=49369", "title": "1928 Winter Olympics", "text": "Multi-sport event in Sankt Moritz, Switzerland\nThe 1928 Winter Olympics, officially known as the II Olympic Winter Games (; ; ; ) and commonly known as St. Moritz 1928 (; ), were an international winter multi-sport event that was celebrated from 11 to 19\u00a0February 1928 in St. Moritz, Switzerland.\nThe 1928 Games were the first true Winter Olympics to be held as a stand-alone event, not in conjunction with a Summer Olympics. The preceding 1924 Winter Games were retroactively renamed the inaugural Winter Olympics, although they had in fact been organised alongside the 1924 Summer Olympics in France. Before 1924, the winter events were included in the schedule of the Summer Games and there were no separate Winter Games. The 1928 Winter Games also replaced the now redundant Nordic Games, which had been held at varying intervals since early in the 20th century.\nThe hosts were challenged by fluctuating weather conditions; the opening ceremony was held in a blizzard, while warm weather conditions plagued sporting events throughout the rest of the Games. The 10,000\u00a0metre speed-skating event was controversially abandoned and officially cancelled. Filmed footage of the games exists in a silent, feature-length documentary, \"The White Stadium\".\nEvents.\nMedals were awarded in 14 events contested in 4 sports (8 disciplines).\nParticipating nations.\nAthletes from 25 nations competed at these Games, up from 16 in 1924. Nations making their first appearance at the Winter Olympic Games were Argentina (first participation of a delegation coming from a country belonging to the Southern Hemisphere), Estonia, Germany, Japan, Lithuania, Luxembourg, Mexico, the Netherlands, and Romania.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49370", "revid": "753665", "url": "https://en.wikipedia.org/wiki?curid=49370", "title": "Anthony Burgess", "text": "English writer and composer (1917\u20131993)\nJohn Anthony Burgess Wilson (; 25 February 1917\u00a0\u2013 22 November 1993) was an English writer and composer.\nAlthough Burgess was primarily a comic writer, his dystopian satire \"A Clockwork Orange\" remains his best-known novel. In 1971, it was adapted into a controversial film by Stanley Kubrick, which Burgess said was chiefly responsible for the popularity of the book. Burgess produced a number of other novels, including the Enderby quartet, and \"Earthly Powers\". He wrote librettos and screenplays, including the 1977 television mini-series \"Jesus of Nazareth\". He worked as a literary critic for several publications, including \"The Observer\" and \"The Guardian\", and wrote studies of classic writers, notably James Joyce. A versatile linguist, Burgess lectured in phonetics, and translated \"Cyrano de Bergerac\", \"Oedipus Rex\", and the opera \"Carmen\", among others. Burgess was nominated and shortlisted for the Nobel Prize in Literature in 1973.\nBurgess also composed over 250 musical works; he considered himself as much a composer as an author, although he achieved considerably more success in writing.\nBiography.\nEarly life.\nIn 1917, Burgess was born at 91 Carisbrook Street in Harpurhey, a suburb of Manchester, England, to Catholic parents, Joseph and Elizabeth Wilson. He described his background as lower middle class; growing up during the Great Depression, his parents, who were shopkeepers, were fairly well off, as the demand for their tobacco and alcohol wares remained constant. He was known in childhood as Jack, Little Jack, and Johnny Eagle. At his confirmation, the name Anthony was added and he became John Anthony Burgess Wilson. He began using the pen name Anthony Burgess upon the publication of his 1956 novel \"Time for a Tiger\".\nHis mother Elizabeth (\"n\u00e9e\" Burgess) died at the age of 30 at home on 19 November 1918, during the 1918 flu pandemic. The causes listed on her death certificate were influenza, acute pneumonia, and cardiac failure. His sister Muriel had died four days earlier on 15 November from influenza, broncho-pneumonia, and cardiac failure, aged eight. Burgess believed he was resented by his father, Joseph Wilson, for having survived, when his mother and sister did not.\nAfter the death of his mother, Burgess was raised by his maternal aunt, Ann Bromley, in Crumpsall with her two daughters. During this time, Burgess's father worked as a bookkeeper for a beef market by day, and in the evening played piano at a public house in Miles Platting. After his father married the landlady of this pub, Margaret Dwyer, in 1922, Burgess was raised by his father and stepmother. By 1924 the couple had established a tobacconist and off-licence business with four properties. Burgess was briefly employed at the tobacconist shop as a child. On 18 April 1938, Joseph Wilson died from cardiac failure, pleurisy, and influenza at the age of 55, leaving no inheritance despite his apparent business success. Burgess's stepmother died of a heart attack in 1940.\nBurgess has said of his largely solitary childhood \"I was either distractedly persecuted or ignored. I was one despised.\u00a0... Ragged boys in gangs would pounce on the well-dressed like myself.\" Burgess attended St. Edmund's Elementary School, before moving on to Bishop Bilsborrow Memorial Elementary School, both Catholic schools, in Moss Side. He later reflected \"When I went to school I was able to read. At the Manchester elementary school I attended, most of the children could not read, so I was\u00a0... a little apart, rather different from the rest.\" Good grades resulted in a place at a grammar school, Xaverian College, which he attended from 1928 to 1936.\nMusic.\nBurgess was indifferent to music until he heard on his home-built radio \"a quite incredible flute solo\", which he characterised as \"sinuous, exotic, erotic\", and became spellbound. Eight minutes later the announcer told him he had been listening to \"Pr\u00e9lude \u00e0 l'apr\u00e8s-midi d'un faune\" by Claude Debussy. He referred to this as a \"psychedelic moment\u00a0... a recognition of verbally inexpressible spiritual realities\". When Burgess announced to his family that he wanted to be a composer, they objected as \"there was no money in it\". Music was not taught at his school, but at the age of about 14 he taught himself to play the piano.\nUniversity.\nBurgess had originally hoped to study music at university, but the music department at the Victoria University of Manchester turned down his application because of poor grades in physics. Instead, he studied English language and literature there between 1937 and 1940, graduating with a Bachelor of Arts degree. His thesis concerned Marlowe's \"Doctor Faustus\", and he graduated with upper second-class honours, which he found disappointing. When grading one of Burgess's term papers, the historian A. J. P. Taylor wrote: \"Bright ideas insufficient to conceal lack of knowledge.\"\nMarriage.\nBurgess met Llewela \"Lynne\" Isherwood Jones at the university where she was studying economics, politics and modern history, graduating in 1942 with an upper second-class. Burgess and Jones were married on 22 January 1942. She was the daughter of secondary school headmaster Edward Jones (1886\u20131963) and Florence (n\u00e9e Jones; 1867\u20131956), and reportedly claimed to be a distant relative of Christopher Isherwood, although the Lewis and Biswell biographies dispute this. According to Burgess's own account, it was not from his wife that the alleged connection to Christopher Isherwood originated: \"Her father was an English Jones, her mother a Welsh one. [...] Of Christopher Isherwood [...] neither the Jones father or daughter had heard. She was unliterary\u00a0...\" Biswell identifies Burgess as the origin of the alleged relationship with Christopher Isherwood\u2014\"if the rumour of an Isherwood affiliation signifies anything, it is that Burgess wanted people to believe that he was connected by marriage to another famous writer\"\u2014and notes that \"Llewela was not, as Burgess claims in his autobiography, a 'cousin' of the writer Christopher Isherwood\"; referring to a pedigree owned by the family, Biswell observes that \"Llewela's father was descended from a female Isherwood\"\u00a0... \"which means going back four generations\u00a0... before encountering any Isherwoods\", making any connection \"at best\" \"tenuous and distant\". He also establishes that per official records, \"Llewela's family name was Jones, not (as Burgess liked to suggest) 'Isherwood Jones' or 'Isherwood-Jones'.\"\nMilitary service.\nBurgess spent six weeks in 1940 as a British Army recruit in Eskbank before becoming a Nursing Orderly Class 3 in the Royal Army Medical Corps. During his service, he was unpopular and was involved in incidents such as knocking off a corporal's cap and polishing the floor of a corridor to make people slip. In 1941, Burgess was pursued by the Royal Military Police for desertion after overstaying his leave from Morpeth military base with his future bride Lynne. The following year he asked to be transferred to the Army Educational Corps and, despite his loathing of authority, he was promoted to sergeant. During the blackout, his pregnant wife Lynne was raped and assaulted by four American deserters; perhaps as a result, she lost the child. Burgess, stationed at the time in Gibraltar, was denied leave to see her.\nAt his stationing in Gibraltar, which he later wrote about in \"A Vision of Battlements\", he worked as a training college lecturer in speech and drama, teaching alongside Ann McGlinn in German, French and Spanish. McGlinn's communist ideology would have a major influence on his later novel \"A Clockwork Orange\". Burgess played a key role in \"The British Way and Purpose\" programme, designed to introduce members of the forces to the peacetime socialism of the post-war years in Britain. He was an instructor for the Central Advisory Council for Forces Education of the Ministry of Education. Burgess's flair for languages was noticed by army intelligence, and he took part in debriefings of Dutch expatriates and Free French who found refuge in Gibraltar during the war. In the neighbouring Spanish town of La L\u00ednea de la Concepci\u00f3n, he was arrested for insulting General Franco but released from custody shortly after the incident.\nEarly teaching career.\nBurgess left the army in 1946 with the rank of sergeant-major. For the next four years he was a lecturer in speech and drama at the Mid-West School of Education near Wolverhampton and at the Bamber Bridge Emergency Teacher Training College near Preston. Burgess taught in the extramural department of Birmingham University (1946\u201350).\nIn late 1950, he began working as a secondary school teacher at Banbury Grammar School (now Banbury School) teaching English literature. In addition to his teaching duties, he supervised sports and ran the school's drama society. He organised a number of amateur theatrical events in his spare time. These involved local people and students and included productions of T. S. Eliot's \"Sweeney Agonistes\". Reports from his former students and colleagues indicate that he cared deeply about teaching.\nWith financial assistance provided by Lynne's father, the couple was able to put a down payment on a cottage in the village of Adderbury, close to Banbury. He named the cottage \"Little Gidding\" after one of Eliot's \"Four Quartets\". Burgess cut his journalistic teeth in Adderbury, writing several articles for the local newspaper, the \"Banbury Guardian\".\nMalaya.\nIn 1954, Burgess joined the British Colonial Service as a teacher and education officer in Malaya, initially stationed at Kuala Kangsar in Perak. Here he taught at the \"Malay College\" (now Malay College Kuala Kangsar \u2013 MCKK), modelled on English public school lines. In addition to his teaching duties, he was a housemaster in charge of students of the preparatory school, who were housed at a Victorian mansion known as \"King's Pavilion\". A variety of the music he wrote there was influenced by the country, notably Sinfoni Melayu for orchestra and brass band, which included cries of Merdeka (independence) from the audience. No score, however, is extant.\nBurgess and his wife had occupied a noisy apartment where privacy was minimal, and this caused resentment. Following a dispute with the Malay College's principal about this, Burgess was reposted to the Malay Teachers' Training College at Kota Bharu, Kelantan. Burgess attained fluency in Malay, spoken and written, achieving distinction in the examinations in the language set by the Colonial Office. He was rewarded with a salary increase for his proficiency in the language.\nHe devoted some of his free time in Malaya to creative writing \"as a sort of gentlemanly hobby, because I knew there wasn't any money in it,\" and published his first novels: \"Time for a Tiger\", \"The Enemy in the Blanket\" and \"Beds in the East\". These became known as \"The Malayan Trilogy\" and were later published in one volume as \"The Long Day Wanes\".\nBrunei.\nAfter a brief period of leave in Britain during 1958, Burgess took up a further Eastern post, this time at the Sultan Omar Ali Saifuddien College in Bandar Seri Begawan, Brunei. Brunei had been a British protectorate since 1888, and was not to achieve independence until 1984. In the sultanate, Burgess sketched the novel that, when it was published in 1961, was to be entitled \"Devil of a State\" and, although it dealt with Brunei, to avoid libel the action had to be transposed to an imaginary East African territory similar to Zanzibar, named Dunia. In his autobiography \"Little Wilson and Big God\" (1987), Burgess wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThis novel was, is, about Brunei, which was renamed Naraka, Malay-Sanskrit for \"hell\". Little invention was needed to contrive a large cast of unbelievable characters and a number of interwoven plots. Though completed in 1958, the work was not published until 1961, for what it was worth it was made a choice of the book society. Heinemann, my publisher, was doubtful about publishing it: it might be libellous. I had to change the setting from Brunei to an East African one. Heinemann was right to be timorous. In early 1958, \"The Enemy in the Blanket\" appeared and at once provoked a libel suit.\nAbout this time, Burgess collapsed in a Brunei classroom while teaching history and was diagnosed as having an inoperable brain tumour. Burgess was given just a year to live, prompting him to write several novels to get money to provide for his widow. He gave a different account, however, to Jeremy Isaacs in a \"Face to Face\" interview on the BBC \"The Late Show\" (21 March 1989). He said \"Looking back now I see that I was driven out of the Colonial Service. I think possibly for political reasons that were disguised as clinical reasons\". He alluded to this in an interview with Don Swaim, explaining that his wife Lynne had said something \"obscene\" to the Duke of Edinburgh during an official visit, and the colonial authorities turned against him. He had already earned their displeasure, he told Swaim, by writing articles in the newspaper in support of the revolutionary opposition party the Parti Rakyat Brunei, and for his friendship with its leader Dr. Azahari. Burgess's biographers attribute the incident to the author's notorious mythomania. Geoffrey Grigson writes:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He was, however, suffering from the effects of prolonged heavy drinking (and associated poor nutrition), of the often oppressive south-east Asian climate, of chronic constipation, and of overwork and professional disappointment. As he put it, the scions of the sultans and of the \u00e9lite in Brunei \"did not wish to be taught\", because the free-flowing abundance of oil guaranteed their income and privileged status. He may also have wished for a pretext to abandon teaching and get going full-time as a writer, having made a late start.\nRepatriate years.\nBurgess was invalided home in 1959 and relieved of his position in Brunei. He spent some time in the neurological ward of a London hospital (see \"The Doctor is Sick\") where he underwent cerebral tests that found no illness. On discharge, benefiting from a sum of money which Lynne Burgess had inherited from her father, together with their savings built up over six years in the East, he decided to become a full-time writer. The couple lived first in an apartment in Hove, near Brighton. They later moved to a semi-detached house called \"Applegarth\" in Etchingham, about four miles from Bateman's where Rudyard Kipling had lived in Burwash, and one mile from the Robertsbridge home of Malcolm Muggeridge. Upon the death of Burgess's father-in-law, the couple used their inheritance to decamp to a terraced town house in Chiswick. This provided convenient access to the BBC Television Centre where he later became a frequent guest. During these years Burgess became a regular drinking partner of the novelist William S. Burroughs. Their meetings took place in London and Tangiers.\nA sea voyage the couple took with the Baltic Line from Tilbury to Leningrad in June 1961 resulted in the novel \"Honey for the Bears\". He wrote in his autobiographical \"You've Had Your Time\" (1990), that in re-learning Russian at this time, he found inspiration for the Russian-based slang Nadsat that he created for \"A Clockwork Orange\", going on to note, \"I would resist to the limit any publisher's demand that a glossary be provided.\"\nLiana Macellari, an Italian translator twelve years younger than Burgess, came across his novels \"Inside Mr. Enderby\" and \"A Clockwork Orange\", while writing about English fiction. The two first met in 1963 over lunch in Chiswick and began an affair. In 1964, Liana gave birth to Burgess's son, Paolo Andrea. The affair was hidden from Burgess's alcoholic wife, whom he refused to leave for fear of offending his cousin (by Burgess's stepmother, Margaret Dwyer Wilson), George Dwyer, the Roman Catholic Bishop of Leeds.\nLynne Burgess died from cirrhosis of the liver, on 20 March 1968. Six months later, in September 1968, Burgess married Liana, acknowledging her four-year-old boy as his own, although the birth certificate listed Roy Halliday, Liana's former partner, as the father. Paolo Andrea (also known as Andrew Burgess Wilson) died in London in 2002, aged 37. Liana died in 2007.\nTax exile.\nBurgess was a Conservative (though, as he clarified in an interview with \"The Paris Review\", his political views could be considered \"a kind of anarchism\", since his ideal of a \"Catholic Jacobite imperial monarch\" was not practicable) a (lapsed) Catholic and monarchist, harbouring a distaste for all republics. He believed socialism for the most part was \"ridiculous\" but did \"concede that socialised medicine is a priority in any civilised country today\". To avoid the 90% tax the family would have incurred because of their high income, they left Britain and toured Europe in a Bedford Dormobile motor-home. During their travels through France and across the Alps, Burgess wrote in the back of the van as Liana drove.\nIn this period, he wrote novels and produced film scripts for Lew Grade and Franco Zeffirelli. His first place of residence after leaving England was Lija, Malta (1968\u201370). The negative reaction from a lecture that Burgess delivered to an audience of Catholic priests in Malta precipitated a move by the couple to Italy after the Maltese government confiscated the property. (He would go on to fictionalise these events in \"Earthly Powers\" a decade later.) The Burgesses maintained a flat in Rome, a country house in Bracciano, and a property in Montalbuccio. On hearing rumours of a mafia plot to kidnap Paolo Andrea while the family was staying in Rome, Burgess decided to move to Monaco in 1975. Burgess was also motivated to move to the tax haven of Monaco, as the country did not levy income tax, and widows were exempt from death duties, a form of taxation on their husband's estates. The couple also had a villa in France, at Callian, Var, Provence.\nBurgess lived for a number of years in the United States, working as writer-in-residence at the University of North Carolina at Chapel Hill in 1969, as a visiting professor at Princeton University with the creative writing program in 1970, and as a distinguished professor at the City College of New York in 1972. At City College he was a close colleague and friend of Joseph Heller. He went on to teach creative writing at Columbia University, lectured on the novel at the University of Iowa in 1975, and was and at the University at Buffalo in 1976. Eventually he settled in Monaco in 1976, where he was active in the local community, becoming a co-founder of the Princess Grace Irish Library, a centre for Irish cultural studies, in 1984.\nIn May 1988, Burgess made an extended appearance with, among others, Andrea Dworkin on the episode \"What Is Sex For?\" of the discussion programme \"After Dark\". He spoke at one point about divorce:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Liking involves no discipline; love does\u00a0... A marriage, say that lasts twenty years or more, is a kind of civilisation, a kind of microcosm \u2013 it develops its own language, its own semiotics, its own slang, its own shorthand\u00a0... sex is part of it, part of the semiotics. To destroy, wantonly, such a relationship, is like destroying a whole civilisation.\nAlthough Burgess lived not far from Graham Greene, whose house was in Antibes, Greene became aggrieved shortly before his death by comments in newspaper articles by Burgess and broke off all contact. Gore Vidal revealed in his 2006 memoir \"Point to Point Navigation\" that Greene disapproved of Burgess's appearance on various European television stations to discuss his (Burgess's) books. Vidal recounts that Greene apparently regarded a willingness to appear on television as something that ought to be beneath a writer's dignity. \"He talks about his books,\" Vidal quotes an exasperated Greene as saying. During this time, Burgess spent much time at his chalet outside Lugano, Switzerland.\nDeath.\nAlthough Burgess wrote that he expected to \"die somewhere in the Mediterranean lands, with an inaccurate obituary in the \"Nice-Matin\", unmourned, soon forgotten\", he returned to die in Twickenham, an outer suburb of London, where he owned a house. Burgess died on 22\u00a0November 1993 from lung cancer, at the Hospital of St\u00a0John &amp; St\u00a0Elizabeth in London. His ashes were inurned at the Monaco Cemetery.\nThe epitaph on Burgess's marble memorial stone, reads: \"Abba Abba\", which means \"Father, father\" in Aramaic, Arabic, Hebrew, and other Semitic languages and is pronounced by Christ during his agony in Gethsemane () as he prays God to spare him. It is also the title of Burgess's 22nd novel, concerning the death of John Keats. Eulogies at his memorial service at St\u00a0Paul's, Covent Garden, London, in 1994 were delivered by the journalist Auberon Waugh and the novelist William Boyd. \"The Times\" obituary heralded the author as \"a great moralist\". His estate was worth US$3\u00a0million and included a large European property portfolio of houses and apartments.\nWriting.\nNovels.\nHis Malayan trilogy \"The Long Day Wanes\" was Burgess's first published fiction. Its three books are \"Time for a Tiger,\" \"The Enemy in the Blanket\" and \"Beds in the East.\" \"Devil of a State\" is a follow-on to the trilogy, set in a fictionalised version of Brunei. It was Burgess's ambition to become \"the true fictional expert on Malaya\". In these works, Burgess was working in the tradition established by Kipling for British India, and Conrad and Maugham for Southeast Asia. Burgess operated more in the mode of Orwell, who had a good command of Urdu and Burmese (necessary for Orwell's work as a police officer) and Kipling, who spoke Hindi (having learnt it as a child). Like many of his fellow English expatriates in Asia, Burgess had excellent spoken and written command of his operative language(s), both as a novelist and as a speaker, including Malay.\nBurgess's repatriate years (c.\u20091960\u20131969) produced \"Enderby\" and \"The Right to an Answer,\" which touches on the theme of death and dying, and \"One Hand Clapping,\" a satire on the vacuity of popular culture. \"The Worm and the Ring\" (1961) had to be withdrawn from circulation under the threat of libel action from one of Burgess's former colleagues, a school secretary.\nHis dystopian novel, \"A Clockwork Orange\", was published in 1962. It was inspired initially by an incident during the London Blitz of World War II in which his wife Lynne was robbed, assaulted, and violated by deserters from the US Army in London during the blackout. The event may have contributed to her subsequent miscarriage. The book was an examination of free will and morality. The young anti-hero, Alex, captured after a short career of violence and mayhem, undergoes a course of aversion therapy treatment to curb his violent tendencies. This results in making him defenceless against other people and unable to enjoy some of his favourite music that, besides violence, had been an intense pleasure for him. In the non-fiction book \"Flame into Being\" (1985), Burgess described \"A Clockwork Orange\" as \"a jeu d'esprit knocked off for money in three weeks. It became known as the raw material for a film which seemed to glorify sex and violence\". He added, \"the film made it easy for readers of the book to misunderstand what it was about, and the misunderstanding will pursue me till I die\". In a 1980 BBC interview, Burgess distanced himself from the novel and cinematic adaptations. Near the time of publication, the final chapter was cut from the American edition of the book.\nBurgess had written \"A Clockwork Orange\" with 21 chapters, meaning to match the age of majority. \"21 is the symbol of human maturity, or used to be, since at 21 you got to vote and assumed adult responsibility\", Burgess wrote in a foreword for a 1986 edition. Needing money and thinking that the publisher was \"being charitable in accepting the work at all,\" Burgess accepted the deal and allowed \"A Clockwork Orange\" to be published in the US with the twenty-first chapter omitted. Stanley Kubrick's film adaptation of \"A Clockwork Orange\" was based on the American edition, and thus helped to perpetuate the loss of the last chapter. In 2021, The International Anthony Burgess Foundation premiered a webpage cataloguing various stage productions of \"A Clockwork Orange\" from around the world.\nIn Martin Seymour-Smith's \"Novels and Novelists: A Guide to the World of Fiction,\" Burgess related that he would often prepare a synopsis with a name-list before beginning a project. Seymour-Smith wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Burgess believes overplanning is fatal to creativity and regards his unconscious mind and the act of writing itself as indispensable guides. He does not produce a draft of a whole novel but prefers to get one page finished before he goes on to the next, which involves a good deal of revision and correction.\n\"Nothing Like the Sun\" is a fictional recreation of Shakespeare's love-life and an examination of the supposedly partly syphilitic sources of the bard's imaginative vision. The novel, which drew on Edgar I.\u00a0Fripp's 1938 biography \"Shakespeare, Man and Artist\", won critical acclaim and placed Burgess among the first rank novelists of his generation. \"M/F\" (1971) was listed by the writer himself as one of the works of which he was most proud. \"Beard's Roman Women\" was revealing on a personal level, dealing with the death of his first wife, his bereavement, and the affair that led to his second marriage. In \"Napoleon Symphony\", Burgess brought Bonaparte to life by shaping the novel's structure to Beethoven's \"Eroica\" symphony. The novel contains a portrait of an Arab and Muslim society under occupation by a Christian western power (Egypt by Catholic France). In the 1980s, religious themes began to feature heavily (\"The Kingdom of the Wicked,\" \"Man of Nazareth,\" \"Earthly Powers\"). Though Burgess lapsed from Catholicism early in his youth, the influence of the Catholic \"training\" and worldview remained strong in his work all his life. This is notable in the discussion of free will in \"A Clockwork Orange\", and in the apocalyptic vision of devastating changes in the Catholic Church\u00a0\u2013 due to what can be understood as Satanic influence\u00a0\u2013 in \"Earthly Powers\" (1980).\nBurgess kept working through his final illness and was writing on his deathbed. The late novel \"Any Old Iron\" is a generational saga of two families, one Russian-Welsh, the other Jewish, encompassing the sinking of the Titanic, World War\u00a0I, the Russian Revolution, the Spanish Civil War, World War\u00a0II, the early years of the State of Israel, and the rediscovery of Excalibur. \"A Dead Man in Deptford\", about Christopher Marlowe, is a companion novel to \"Nothing Like the Sun\". The verse novel \"\" was published posthumously.\nBurgess announced in a 1972 interview that he was writing a novel about the Black Prince which incorporated John Dos Passos's narrative techniques, although he never finished writing it. After Burgess's death, English writer Adam Roberts completed the novel, and it was published in 2018 under the title \"The Black Prince\". In 2019, a previously unpublished analysis of \"A Clockwork Orange\" was discovered titled, \"The Clockwork Condition\". It is structured as Burgess's philosophical musings on the novel that won him so much acclaim.\nCritical studies.\nBurgess started his career as a critic. His \"English Literature, A Survey for Students\" was aimed at newcomers to the subject. He followed this with \"The Novel To-day\" (Longmans, 1963) and \"The Novel Now: A Student's Guide to Contemporary Fiction\" (New York: W.\u00a0W. Norton and Company, 1967). He wrote the Joyce studies \"Here Comes Everybody: An Introduction to James Joyce for the Ordinary Reader\" (also published as \"Re Joyce\") and \"Joysprick: An Introduction to the Language of James Joyce\". Also published was \"A Shorter \"Finnegans Wake\"\", Burgess's abridgement. His 1970 \"Encyclop\u00e6dia Britannica\" entry on the novel (under \"Novel, the\") is regarded as a classic of the genre. Burgess wrote full-length critical studies of William Shakespeare, Ernest Hemingway and D.\u00a0H. Lawrence, as well as \"Ninety-nine Novels: The Best in English since 1939\".\nScreenwriting.\nBurgess wrote the screenplays for \"Moses the Lawgiver\" (Gianfranco De Bosio 1974), \"Jesus of Nazareth\" (Franco Zeffirelli 1977), and \"A.D.\" (Stuart Cooper, 1985). Burgess was co-writer of the script for the TV series \"Sherlock Holmes and Doctor Watson\" (1980). The film treatments he produced include \"Amundsen\", \"Attila\", \"The Black Prince\", \"Cyrus the Great\", \"Dawn Chorus\", \"The Dirty Tricks of Bertoldo\", \"Eternal Life\", \"Onassis\", \"Puma\", \"Samson and Delilah\", \"Schreber\", \"The Sexual Habits of the English Middle Class\", \"Shah\", \"That Man Freud\" and \"Uncle Ludwig\". Burgess devised a Stone Age language for \"La Guerre du Feu\" (\"Quest for Fire\"; Jean-Jacques Annaud, 1981).\nBurgess wrote many unpublished scripts, including \"Will!\" or \"The Bawdy Bard\" about Shakespeare, based on the novel \"Nothing Like The Sun\". Encouraged by the success of \"Tremor of Intent\" (a parody of James Bond adventures), Burgess wrote a screenplay for \"The Spy Who Loved Me\" featuring characters from and a similar tone to the novel. It had Bond fighting the criminal organisation CHAOS in Singapore to try to stop an assassination of Queen Elizabeth\u00a0II using surgically implanted bombs at Sydney Opera House. It was described as \"an outrageous medley of sadism, hypnosis, acupuncture, and international terrorism\". His screenplay was rejected, although the huge submarine silo seen in the finished film was reportedly Burgess's inspiration.\nPlaywright.\nAnthony Burgess's involvement with theatre started while attending university in Manchester, where directed plays and wrote theatre reviews. In Oxfordshire he was an active member of the Adderbury Drama Group, where he directed multiple plays, including \"Juno and the Paycock\" by Sean O'Casey, \"A Phoenix Too Frequent\" by Christopher Fry, \"The Giaconda Smile\" by Aldous Huxley and \"The Adding Machine\" by Elmer Rice.\nHe wrote his first play in 1951, called \"The Eve of Saint Venus.\" There are no records of the play being performed, and in 1964 he turned the text into a novella. Throughout his life he wrote multiple adaptations and translations for theatre. His most famous work \"A Clockwork Orange\", he adapted for the stage under the title \"\". An expanded edition of this play, with a facsimile of the handwritten score, appeared in 1999; \"A Clockwork Orange 2004\", adapted from Burgess's novel by the director Ron Daniels and published by Arrow Books, was produced at the Barbican Theatre in London in 1990, with music by The Edge from U2. \u00a0\nHis other famous translations include the English version of \"Cyrano de Bergerac\" by Edmond Rostand. Recently two of his until now unpublished translations were published by Salamander Street, which the Foundation called a 'significant literary discovery'. One is \"Miser! Miser!\" A translation of Moli\u00e8re's \"The Miser.\" Although the original French play is written in prose, Burgess remakes it in a mixture of verse and prose, in the style of his famous adaptation of \"Cyrano de Bergerac\". The other work is \"Chatsky\", subtitled \"'The Importance of Being Stupid'\" based on \"Woe from Wit\" by Alexander Griboyedov. In \"Chatsky\", Burgess remakes a classic Russian play in the spirit of Oscar Wilde.\nMusic.\nAn accomplished musician, Burgess composed regularly throughout his life, and once said: \"I wish people would think of me as a musician who writes novels, instead of a novelist who writes music on the side.\" He wrote more than 250 compositions in a variety of forms, including symphonies, concertos, chamber music, piano music, and works for the theatre. His early introduction to music is lightly disguised as fiction in his novel \"The Pianoplayers\" (1986), and Burgess identified the piano as his main instrument. Many of his unpublished compositions are listed in \"This Man and Music\" (1982).\nOrchestral and chamber.\nBurgess began composing seriously while in the army during the war, and then while working as a teacher in Malaya, but could not earn a living from it. His early symphony, \"Sinfoni Melayu\" (now lost), was an attempt \"to combine the musical elements of the country [Malaya] into a synthetic language which called on native drums and xylophones\". A second symphony has also been lost. But his Symphony No 3 in C was commissioned by the University of Iowa Symphony Orchestra in 1974, resulting in the first public performance of an orchestral work by Burgess \u2013 a momentous occasion for the composer which spurred him on to renew his composing activities with other large scale works, including a violin concerto for Yehudi Menuhin which remained unperformed due to the violinist's death. More recently, the Symphony was broadcast on BBC Radio 3 as part of the Manchester International Festival in July 2017.\nHe wrote a good deal of chamber and instrumental music. A recently recovered work is a string quartet from 1980, influenced by Dmitri Shostakovich, which unexpectedly turned up in the archive of the International Anthony Burgess Foundation. For piano, Burgess composed his most substantial body of musical writing, including a set of 24 Preludes and Fugues, \"The Bad-Tempered Electronic Keyboard\" (1985), which has been recorded by Stephane Ginsburgh. The Prima Facia label has released two CDs of his piano music: Vol. 1 (2015) and Vol. 2 (2025).\nBurgess also wrote extensively for the recorder as his son played the instrument. Several works for recorder and piano, including the Sonata No.\u00a01, Sonatina and \"Tre Pezzetti\", have been recorded by John Turner with pianist Harvey Davies. His collected guitar quartets have also been recorded by the M\u0113la Guitar Quartet.\nMusicals and opera.\nBurgess composed the operetta \"Blooms of Dublin\" in 1982, adapting the libretto from James Joyce's \"Ulysses\". It is a free interpretation of Joyce's text, with changes and interpolations by Burgess himself, all set to original music that blends opera with Gilbert and Sullivan and music hall styles. The musical was televised by the BBC, to mixed reviews. He wrote the libretto for the 1973 Broadway musical \"Cyrano\" (music by Michael J. Lewis), using his own adaptation of the original Rostand play as his basis. Burgess also produced a translation of Meilhac and Hal\u00e9vy's libretto to Bizet's \"Carmen\", which was performed by the English National Opera in 1986, and wrote a new libretto for Weber's last opera \"Oberon\" (1826), reprinted alongside the original in \"Oberon Old and New\". It was performed by the Glasgow-based Scottish Opera in 1985, but hasn't been revived since.\nMusic and literature.\nNearly all the writings, fiction and non-fiction, reflect Burgess' musical experiences. Biographical elements concerning musicians, particularly failed composers, occur everywhere. His early novel \"A Vision of Battlements\" (1965) concerns Richard Ennis, a composer of symphonies and concertos who is serving in the British army in Gibraltar. His last, \"\" (1995), a novel set in verse form, is about a minor modern composer who enjoys greater success in bed than he does in the concert hall. Fictional works mentioned in the novels often parallel Burgess's own real compositions, and provide a commentary on them, such as the cantata \"St Celia's Day\", described in the 1976 novel \"Beard's Roman Women\", which surfaced two years after the novel was published as a real Burgess work.\nThe musical influences go far beyond the biographical. There are experiments combining musical forms and literature. \"Tremor of Intent\" (1966), the James Bond spoof thriller, is set in sonata form. \"Mozart and the Wolf Gang\" (1991) mirrors the sound and rhythm of Mozartian composition, among other things attempting a fictional representation of Symphony No.\u00a040. \"Napoleon Symphony: A Novel in Four Movements\" (1974) is a literary interpretation of Beethoven's \"Eroica\", while Beethoven's Symphony No. 9 features prominently in \"A Clockwork Orange\" (and in Stanley Kubrick's film version of the novel).\nHis use of language often highlights sound over meaning \u2013 in the made-up, Russian-influenced language \"Nadsat\" used by the narrator of \"A Clockwork Orange\", in the wordless film script \"Quest for Fire\" (1981), where he invents a tribal language that prehistoric man might have spoken, and in the non-fiction work on the sound of language, \"A Mouthful of Air\" (1992).\nMusical enthusiasms.\nOn the BBC's \"Desert Island Discs\" radio programme in 1966, Burgess chose as his favourite music Purcell's \"Rejoice in the Lord alway\"; Bach's \"Goldberg Variations\" No.\u00a013; Elgar's Symphony No.\u00a01 in A-flat major; Wagner's \"Walter's Trial Song\" from \"Die Meistersinger von N\u00fcrnberg\"; Debussy's \"F\u00eates\" from \"Nocturnes\"; Lambert's \"The Rio Grande\"; Walton's Symphony No.\u00a01 in B-flat minor; and Vaughan Williams' \"On Wenlock Edge\". A collection of essays on music by Burgess was published in 2024.\nLinguistics.\n\"Burgess's linguistic training\", wrote Raymond Chapman and Tom McArthur in \"The Oxford Companion to the English Language\": \"...is shown in dialogue enriched by distinctive pronunciations and the niceties of register\". During his years in Malaya, and after he had mastered Jawi, the Arabic script adapted for Malay, Burgess taught himself the Persian language, after which he produced a translation of Eliot's \"The Waste Land\" into Persian (unpublished). He worked on an anthology of the best of English literature translated into Malay, which failed to achieve publication. Burgess's published translations include two versions of \"Cyrano de Bergerac\", \"Oedipus the King\" and \"Carmen\".\nBurgess's interest in language was reflected in the invented, Anglo-Russian teen slang of \"A Clockwork Orange\" (Nadsat), and in the movie \"Quest for Fire\" (1981), for which he invented a prehistoric language (\"Ulam\") for the characters. His interest is reflected in his characters. In \"The Doctor is Sick\", Dr Edwin Spindrift is a lecturer in linguistics who escapes from a hospital ward which is peopled, as the critic Saul Maloff put it in a review, with \"brain cases who happily exemplify varieties of English speech\". Burgess, who had lectured on phonetics at the University of Birmingham in the late 1940s, investigates the field of linguistics in \"Language Made Plain\" and \"A Mouthful of Air\".\nThe depth of Burgess's multilingual proficiency came under discussion in Roger Lewis's . Lewis claimed that during production in Malaysia of the BBC documentary \"A Kind of Failure\" (1982), Burgess's supposedly fluent Malay was not understood by waitresses at a restaurant where they were filming. It was claimed that the documentary's director deliberately kept these moments intact in the film to expose Burgess's linguistic pretensions. A letter from David Wallace that appeared in the magazine of the London \"Independent on Sunday\" newspaper on 25\u00a0November 2002 shed light on the affair. Wallace's letter read, in part:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\n...\u00a0the tale was inaccurate. It tells of Burgess, the great linguist, \"bellowing Malay at a succession of Malayan waitresses\" but \"unable to make himself understood\". The source of this tale was a 20-year-old BBC documentary\u00a0... [The suggestion was] that the director left the scene in, in order to poke fun at the great author. Not so, and I can be sure, as I was that director\u00a0... The story as seen on television made it clear that Burgess knew that these waitresses were not Malay. It was a Chinese restaurant and Burgess's point was that the ethnic Chinese had little time for the government-enforced national language, Bahasa Malaysia [Malay]. Burgess may well have had an accent, but he did speak the language; it was the girls in question who did not.\nLewis may not have been fully aware of the fact that a quarter of Malaysia's population is made up of Hokkien- and Cantonese-speaking Chinese. However, Malay had been installed as the National Language with the passing of the Language Act of 1967. By 1982 all national primary and secondary schools in Malaysia would have been teaching with Bahasa Melayu as a base language (see Harold Crouch, \"Government and Society in Malaysia\", Ithaca and London: Cornell University Press, 1996).\nArchive.\nThe largest archive of Anthony Burgess's belongings is housed at the International Anthony Burgess Foundation in Manchester, UK. The holdings include: handwritten journals and diaries; over 8000 books from Burgess's personal library; manuscripts of novels, journalism and musical compositions; professional and private photographs dating from between 1918 and 1993; an extensive archive of sound recordings; Burgess's music collection; furniture; musical instruments including two of Burgess's pianos; and correspondence that includes letters from Angela Carter, Graham Greene, Thomas Pynchon and other notable writers and publishers. The International Anthony Burgess Foundation was established by Burgess's widow, Liana, in 2003.\nBeginning in 1995, Burgess's widow sold a large archive of his papers at the Harry Ransom Center at the University of Texas at Austin with several additions made in subsequent years. Comprising over 136 boxes, the archive includes typed and handwritten manuscripts, sheet music, correspondence, clippings, contracts and legal documents, appointment books, magazines, photographs, and personal effects. \nA substantial amount of unpublished and unproduced music compositions is included in the collection, along with a small number of audio recordings of Burgess's interviews and performances of his work. Over 90 books from Burgess's library can also be found in the Ransom Center's holdings. In 2014, the Ransom Center added the archive of Burgess's long-time agent Gabriele Pantucci, which also includes substantial manuscripts, sheet music, correspondence, and contracts. Burgess's archive at the Ransom Center is supplemented by significant archives of artists Burgess admired including James Joyce, Graham Greene and D. H. Lawrence.\nA small collection of papers, musical manuscripts and other items was deposited with the University of Angers in 1998. Its present whereabouts are unclear.\nSelected works.\nNovels.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49372", "revid": "1416331", "url": "https://en.wikipedia.org/wiki?curid=49372", "title": "Liege (province)", "text": ""}
{"id": "49373", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=49373", "title": "Grid computing", "text": "Use of widely distributed computer resources to reach a common goal\nGrid computing is the use of widely distributed computer resources to reach a common goal. A computing grid can be thought of as a distributed system with non-interactive workloads that involve many files. Grid computing is distinguished from conventional high-performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application. Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers. Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.\nGrids are a form of distributed computing composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, distributed or grid computing can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus. This technology has been applied to computationally intensive scientific, mathematical, and academic problems through volunteer computing, and it is used in commercial enterprises for such diverse applications as drug discovery, economic forecasting, seismic analysis, and back office data processing in support for e-commerce and Web services.\nGrid computing combines computers from multiple administrative domains to reach a common goal, to solve a single task, and may then disappear just as quickly. The size of a grid may vary from small\u2014confined to a network of computer workstations within a corporation, for example\u2014to large, public collaborations across many companies and networks. \"The notion of a confined grid may also be known as an intra-nodes cooperation whereas the notion of a larger, wider grid may thus refer to an inter-nodes cooperation\".\nCoordinating applications on Grids can be a complex task, especially when coordinating the flow of information across distributed computing resources. Grid workflow systems have been developed as a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in the grid context.\nComparison of grids and conventional supercomputers.\n\u201cDistributed\u201d or \u201cgrid\u201d computing in general is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing a small number of custom supercomputers. The primary performance disadvantage is that the various processors and local storage areas do not have high-speed connections. This arrangement is thus well-suited to applications in which multiple parallel computations can take place independently, without the need to communicate intermediate results between processors. The high-end scalability of geographically dispersed grids is generally favorable, due to the low need for connectivity between nodes relative to the capacity of the public Internet.\nThere are also some differences between programming for a supercomputer and programming for a grid computing system. It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues. If a problem can be adequately parallelized, a \u201cthin\u201d layer of \u201cgrid\u201d infrastructure can allow conventional, standalone programs, given a different part of the same problem, to run on multiple machines. This makes it possible to write and debug on a single conventional machine and eliminates complications due to multiple instances of the same program running in the same shared memory and storage space at the same time.\nDesign considerations and variations.\nOne feature of distributed grids is that they can be formed from computing resources belonging to one or multiple individuals or organizations (known as multiple administrative domains). This can facilitate commercial transactions, as in utility computing, or make it easier to assemble volunteer computing networks.\nOne disadvantage of this feature is that the computers which are actually performing the calculations might not be entirely trustworthy. The designers of the system must thus introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results, and from using the system as an attack vector. This often involves assigning work randomly to different nodes (presumably with different owners) and checking that at least two different nodes report the same answer for a given work unit. Discrepancies would identify malfunctioning and malicious nodes. However, due to the lack of central control over the hardware, there is no way to guarantee that nodes will not drop out of the network at random times. Some nodes (like laptops or dial-up Internet customers) may also be available for computation but not network communications for unpredictable periods. These variations can be accommodated by assigning large work units (thus reducing the need for continuous network connectivity) and reassigning work units when a given node fails to report its results in the expected time.\nAnother set of what could be termed social compatibility issues in the early days of grid computing related to the goals of grid developers to carry their innovation beyond the original field of high-performance computing and across disciplinary boundaries into new fields, like that of high-energy physics.\nThe impacts of trust and availability on performance and development difficulty can influence the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors. In many cases, the participating nodes must trust the central system not to abuse the access that is being granted, by interfering with the operation of other programs, mangling stored information, transmitting private data, or creating new security holes. Other systems employ measures to reduce the amount of trust \u201cclient\u201d nodes must place in the central system such as placing applications in virtual machines.\nPublic systems or those crossing administrative domains (including different departments in the same organization) often result in the need to run on heterogeneous systems, using different operating systems and hardware architectures. With many languages, there is a trade-off between investment in software development and the number of platforms that can be supported (and thus the size of the resulting network). Cross-platform languages can reduce the need to make this tradeoff, though potentially at the expense of high performance on any given node (due to run-time interpretation or lack of optimization for the particular platform). Various middleware projects have created generic infrastructure to allow diverse scientific and commercial projects to harness a particular associated grid or for the purpose of setting up new grids. BOINC is a common one for various academic projects seeking public volunteers; more are listed at the end of the article.\nIn fact, the middleware can be seen as a layer between the hardware and the software. On top of the middleware, a number of technical areas have to be considered, and these may or may not be middleware independent. Example areas include SLA management, Trust, and Security, Virtual organization management, License Management, Portals and Data Management. These technical areas may be taken care of in a commercial solution, though the cutting edge of each area is often found within specific research projects examining the field.\nMarket segmentation of the grid computing market.\nFor the segmentation of the grid computing market, two perspectives need to be considered: the provider side and the user side:\nThe provider side.\nThe overall grid market comprises several specific markets. These are the grid middleware market, the market for grid-enabled applications, the utility computing market, and the software-as-a-service (SaaS) market.\nGrid middleware is a specific software product, which enables the sharing of heterogeneous resources, and Virtual Organizations. It is installed and integrated into the existing infrastructure of the involved company or companies and provides a special layer placed among the heterogeneous infrastructure and the specific user applications. Major grid middlewares are Globus Toolkit, gLite, and UNICORE.\nUtility computing is referred to as the provision of grid computing and applications as service either as an open grid utility or as a hosting solution for one organization or a VO. Major players in the utility computing market are Sun Microsystems, IBM, and HP.\nGrid-enabled applications are specific software applications that can utilize grid infrastructure. This is made possible by the use of grid middleware, as pointed out above.\nSoftware as a service (SaaS) is \u201csoftware that is owned, delivered and managed remotely by one or more providers.\u201d (Gartner 2007) Additionally, SaaS applications are based on a single set of common code and data definitions. They are consumed in a one-to-many model, and SaaS uses a Pay As You Go (PAYG) model or a subscription model that is based on usage. Providers of SaaS do not necessarily own the computing resources themselves, which are required to run their SaaS. Therefore, SaaS providers may draw upon the utility computing market. The utility computing market provides computing resources for SaaS providers.\nThe user side.\nFor companies on the demand or user side of the grid computing market, the different segments have significant implications for their IT deployment strategy. The IT deployment strategy as well as the type of IT investments made are relevant aspects for potential grid users and play an important role for grid adoption.\nCPU scavenging.\nCPU-scavenging, cycle-scavenging, or shared computing creates a \u201cgrid\u201d from the idle resources in a network of participants (whether worldwide or internal to an organization). Typically, this technique exploits the 'spare' instruction cycles resulting from the intermittent inactivity that typically occurs at night, during lunch breaks, or even during the (comparatively minuscule, though numerous) moments of idle waiting that modern desktop CPU's experience throughout the day (when the computer is waiting on IO from the user, network, or storage). In practice, participating computers also donate some supporting amount of disk storage space, RAM, and network bandwidth, in addition to raw CPU power.\nMany volunteer computing projects, such as BOINC, use the CPU scavenging model. Since nodes are likely to go \"offline\" from time to time, as their owners use their resources for their primary purpose, this model must be designed to handle such contingencies.\nCreating an Opportunistic Environment is another implementation of CPU-scavenging where special workload management system harvests the idle desktop computers for compute-intensive jobs, it also refers as Enterprise Desktop Grid (EDG). For instance, HTCondor (the open-source high-throughput computing software framework for coarse-grained distributed rationalization of computationally intensive tasks) can be configured to only use desktop machines where the keyboard and mouse are idle to effectively harness wasted CPU power from otherwise idle desktop workstations. Like other full-featured batch systems, HTCondor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. It can be used to manage workload on a dedicated cluster of computers as well or it can seamlessly integrate both dedicated resources (rack-mounted clusters) and non-dedicated desktop machines (cycle scavenging) into one computing environment.\nHistory.\nThe term \"grid computing\" originated in the early 1990s as a metaphor for making computer power as easy to access as an electric power grid. The power grid metaphor for accessible computing quickly became canonical when Ian Foster and Carl Kesselman published their seminal work, \"The Grid: Blueprint for a new computing infrastructure\" (1999). This was preceded by decades by the metaphor of utility computing (1961): computing as a public utility, analogous to the phone system.\nCPU scavenging and volunteer computing were popularized beginning in 1997 by distributed.net and later in 1999 by SETI@home to harness the power of networked PCs worldwide, in order to solve CPU-intensive research problems.\nThe ideas of the grid (including those from distributed computing, object-oriented programming, and Web services) were brought together by Ian Foster and Steve Tuecke of the University of Chicago, and Carl Kesselman of the University of Southern California's Information Sciences Institute. The trio, who led the effort to create the Globus Toolkit, is widely regarded as the \"fathers of the grid\". The toolkit incorporates not just computation management but also storage management, security provisioning, data movement, monitoring, and a toolkit for developing additional services based on the same infrastructure, including agreement negotiation, notification mechanisms, trigger services, and information aggregation. While the Globus Toolkit remains the de facto standard for building grid solutions, a number of other tools have been built that answer some subset of services needed to create an enterprise or global grid.\nIn 2007 the term cloud computing came into popularity, which is conceptually similar to the canonical Foster definition of grid computing (in terms of computing resources being consumed as electricity is from the power grid) and earlier utility computing.\nProgress.\nIn November 2006, Edward Seidel received the Sidney Fernbach Award at the Supercomputing Conference in Tampa, Florida. \"For outstanding contributions to the development of software for HPC and Grid computing to enable the collaborative numerical investigation of complex problems in physics; in particular, modeling black hole collisions.\" This award, which is one of the highest honors in computing, was awarded for his achievements in numerical relativity.\nFastest virtual supercomputers.\nAlso, as of March 2019, the Bitcoin Network had a measured computing power equivalent to over 80,000 exaFLOPS (Floating-point Operations Per Second). This measurement reflects the number of FLOPS required to equal the hash output of the Bitcoin network rather than its capacity for general floating-point arithmetic operations, since the elements of the Bitcoin network (Bitcoin mining ASICs) perform only the specific cryptographic hash computation required by the Bitcoin protocol.\nProjects and applications.\nGrid computing offers a way to solve Grand Challenge problems such as protein folding, financial modeling, earthquake simulation, and climate/weather modeling, and was integral in enabling the Large Hadron Collider at CERN. Grids offer a way of using information technology resources optimally inside an organization. They also provide a means for offering information technology as a utility for commercial and noncommercial clients, with those clients paying only for what they use, as with electricity or water.\nAs of October 2016, over 4 million machines running the open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform are members of the World Community Grid. One of the projects using BOINC is SETI@home, which was using more than 400,000 computers to achieve 0.828 TFLOPS as of October 2016. As of October 2016 Folding@home, which is not part of BOINC, achieved more than 101 x86-equivalent petaflops on over 110,000 machines.\nThe European Union funded projects through the framework programmes of the European Commission. BEinGRID (Business Experiments in Grid) was a research project funded by the European Commission as an Integrated Project under the Sixth Framework Programme (FP6) sponsorship program. Started on June 1, 2006, the project ran 42 months, until November 2009. The project was coordinated by Atos Origin. According to the project fact sheet, their mission is \u201cto establish effective routes to foster the adoption of grid computing across the EU and to stimulate research into innovative business models using Grid technologies\u201d. To extract best practice and common themes from the experimental implementations, two groups of consultants are analyzing a series of pilots, one technical, one business. The project is significant not only for its long duration but also for its budget, which at 24.8 million Euros, is the largest of any FP6 integrated project. Of this, 15.7 million is provided by the European Commission and the remainder by its 98 contributing partner companies. Since the end of the project, the results of BEinGRID have been taken up and carried forward by IT-Tude.com.\nThe Enabling Grids for E-sciencE project, based in the European Union and included sites in Asia and the United States, was a follow-up project to the European DataGrid (EDG) and evolved into the European Grid Infrastructure. This, along with the Worldwide LHC Computing Grid (WLCG), was developed to support experiments using the CERN Large Hadron Collider. A list of active sites participating within WLCG can be found online as can real time monitoring of the EGEE infrastructure. The relevant software and documentation is also publicly accessible. There is speculation that dedicated fiber optic links, such as those installed by CERN to address the WLCG's data-intensive needs, may one day be available to home users thereby providing internet services at speeds up to 10,000 times faster than a traditional broadband connection. The European Grid Infrastructure has been also used for other research activities and experiments such as the simulation of oncological clinical trials.\nThe distributed.net project was started in 1997.\nThe NASA Advanced Supercomputing facility (NAS) ran genetic algorithms using the Condor cycle scavenger running on about 350 Sun Microsystems and SGI workstations.\nIn 2001, United Devices operated the United Devices Cancer Research Project based on its Grid MP product, which cycle-scavenges on volunteer PCs connected to the Internet. The project ran on about 3.1 million machines before its close in 2007.\nRecent innovations have explored the integration of blockchain technology with grid computing principles. For example, the VirtEngine system, detailed in granted Australian patent AU2024203136, proposes a decentralized model that combines a distributed computing network with a Proof-of-Stake blockchain-based framework for identification, authentication, and resource management. This approach aims to create an autonomous system for managing a decentralized cloud marketplace and a distributed supercomputer, utilizing consumer &amp; provider based computing resources to power a globally distributed grid computing network.\nDefinitions.\nToday there are many definitions of \"grid computing\":\nSee also.\nList of grid computing projects\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49374", "revid": "1319100340", "url": "https://en.wikipedia.org/wiki?curid=49374", "title": "978", "text": "Calendar year\nYear 978 (CMLXXVIII) was a common year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nBy topic.\nReligion.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49375", "revid": "34957066", "url": "https://en.wikipedia.org/wiki?curid=49375", "title": "Larynx", "text": "Voice box, an organ in the neck of amphibians, reptiles, and mammals\nThe larynx (pl.: larynges or larynxes), commonly called the voice box, is an organ in the top of the neck involved in respiration, producing sound and protecting the trachea against food aspiration. The opening of the larynx into the pharynx known as the laryngeal inlet is about 4\u20135 centimeters in diameter. The larynx houses the vocal cords, and manipulates pitch and volume, which is essential for phonation. It is situated just below where the tract of the pharynx splits into the trachea and the esophagus.\nStructure.\nThe triangle-shaped larynx consists largely of cartilages that are attached to one another, and to surrounding structures, by muscles or by fibrous and elastic tissue components. The larynx is lined by a ciliated columnar epithelium except for the vocal folds. The cavity of the larynx extends from its triangle-shaped inlet, to the epiglottis, and to the circular outlet at the lower border of the cricoid cartilage, where it is continuous with the lumen of the trachea. The mucous membrane lining the larynx forms two pairs of lateral folds that project inward into its cavity. The upper folds are called the vestibular folds. They are also sometimes called the false vocal cords for the rather obvious reason that they play no part in vocalization. The Kargyraa style of Tuvan throat singing makes use of these folds to sing an octave lower, and they are used in Umngqokolo, a type of Xhosa throat singing. The lower pair of folds are known as the vocal cords, which produce sounds needed for speech and other vocalizations. The slit-like space between the left and right vocal cords, called the rima glottidis, is the narrowest part of the larynx. The vocal cords and the rima glottidis are together designated as the glottis. The laryngeal cavity above the vestibular folds is called the vestibule. The very middle portion of the cavity between the vestibular folds and the vocal cords is the ventricle of the larynx, or laryngeal ventricle. The infraglottic cavity is the open space below the glottis.\nLocation.\nIn adult humans, the larynx is found in the anterior neck at the level of the cervical vertebrae C3\u2013C6. It connects the inferior part of the pharynx (hypopharynx) with the trachea. The laryngeal skeleton consists of nine cartilages: three single (epiglottic, thyroid and cricoid) and three paired (arytenoid, corniculate, and cuneiform). The hyoid bone is not part of the larynx, though the larynx is suspended from the hyoid. The larynx extends vertically from the tip of the epiglottis to the inferior border of the cricoid cartilage. Its interior can be divided in supraglottis, glottis and subglottis.\nCartilages.\nThere are nine cartilages, three unpaired and three paired (3 pairs=6), that support the mammalian larynx and form its skeleton.\nUnpaired cartilages:\nPaired cartilages:\nMuscles.\nThe muscles of the larynx are divided into \"intrinsic\" and \"extrinsic\" muscles. The extrinsic muscles act on the region and pass between the larynx and parts around it but have their origin elsewhere; the intrinsic muscles are confined entirely within the larynx and have their origin and insertion there.\nThe intrinsic muscles are divided into respiratory and the phonatory muscles (the muscles of phonation). The respiratory muscles move the vocal cords apart and serve breathing. The phonatory muscles move the vocal cords together and serve the production of voice. The main respiratory muscles are the posterior cricoarytenoid muscles. The phonatory muscles are divided into adductors (lateral cricoarytenoid muscles, arytenoid muscles) and tensors (cricothyroid muscles, thyroarytenoid muscles).\nIntrinsic.\nThe intrinsic laryngeal muscles are responsible for controlling sound production.\nNotably the only muscle capable of separating the vocal cords for normal breathing is the posterior cricoarytenoid. If this muscle is incapacitated on both sides, the inability to pull the vocal cords apart (abduct) will cause difficulty breathing. Bilateral injury to the recurrent laryngeal nerve would cause this condition. It is also worth noting that all muscles are innervated by the recurrent laryngeal branch of the vagus except the cricothyroid muscle, which is innervated by the external laryngeal branch of the superior laryngeal nerve (a branch of the vagus).\nAdditionally, intrinsic laryngeal muscles present a constitutive Ca2+-buffering profile that predicts their better ability to handle calcium changes in comparison to other muscles. This profile is in agreement with their function as very fast muscles with a well-developed capacity for prolonged work. Studies suggests that mechanisms involved in the prompt sequestering of Ca2+ (sarcoplasmic reticulum Ca2+-reuptake proteins, plasma membrane pumps, and cytosolic Ca2+-buffering proteins) are particularly elevated in laryngeal muscles, indicating their importance for the myofiber function and protection against disease, such as Duchenne muscular dystrophy. Furthermore, different levels of Orai1 in rat intrinsic laryngeal muscles and extraocular muscles over the limb muscle suggests a role for store operated calcium entry channels in those muscles' functional properties and signaling mechanisms.\nExtrinsic.\nThe extrinsic laryngeal muscles support and position the larynx within the mid-cervical cereal region. \nNerve supply.\nThe larynx is innervated by branches of the vagus nerve on each side. Sensory innervation to the glottis and laryngeal vestibule is by the internal branch of the superior laryngeal nerve. The external branch of the superior laryngeal nerve innervates the cricothyroid muscle. Motor innervation to all other muscles of the larynx and sensory innervation to the subglottis is by the recurrent laryngeal nerve. While the sensory input described above is (general) visceral sensation (diffuse, poorly localized), the vocal cords also receives general somatic sensory innervation (proprioceptive and touch) by the superior laryngeal nerve.\nInjury to the external branch of the superior laryngeal nerve causes weakened phonation because the vocal cords cannot be tightened. Injury to one of the recurrent laryngeal nerves produces hoarseness, if both are damaged the voice may or may not be preserved, but breathing becomes difficult.\nDevelopment.\nThe larynx is derived from the mesoderm of the fourth and sixth pharyngeal arches. In newborn infants, the larynx is initially at the level of the C2\u2013C3 vertebrae, and is further forward and higher relative to its position in the adult body. The larynx descends as the child grows.\nLaryngeal cavity.\nThe laryngeal cavity (cavity of the larynx) extends from the laryngeal inlet downwards to the lower border of the cricoid cartilage where it is continuous with that of the trachea.\nIt is divided into two parts by the projection of the vocal folds, between which is a narrow triangular opening, the rima glottidis.\nThe portion of the cavity of the larynx above the vestibular folds is called the laryngeal vestibule; it is wide and triangular in shape, its base or anterior wall presenting, however, about its center the backward projection of the tubercle of the epiglottis.\nIt contains the vestibular folds, and between these and the vocal folds are the laryngeal ventricles.\nThe portion below the vocal folds is called the infraglottic cavity. It is at first of an elliptical form, but lower down it widens out, assumes a circular form, and is continuous with the tube of the trachea.\nFunction.\nSound generation.\nSound is generated in the larynx, and that is where pitch and volume are manipulated. The strength of expiration from the lungs also contributes to loudness.\nManipulation of the larynx is used to generate a source sound with a particular fundamental frequency, or pitch. This source sound is altered as it travels through the vocal tract, configured differently based on the position of the tongue, lips, mouth, and pharynx. The process of altering a source sound as it passes through the filter of the vocal tract creates the many different vowel and consonant sounds of the world's languages as well as tone, certain realizations of stress and other types of linguistic prosody. The larynx also has a similar function to the lungs in creating pressure differences required for sound production; a constricted larynx can be raised or lowered affecting the volume of the oral cavity as necessary in glottalic consonants.\nThe vocal cords can be held close together (by adducting the arytenoid cartilages) so that they vibrate (see phonation). The muscles attached to the arytenoid cartilages control the degree of opening. Vocal cord length and tension can be controlled by rocking the thyroid cartilage forward and backward on the cricoid cartilage (either directly by contracting the cricothyroids or indirectly by changing the vertical position of the larynx), by manipulating the tension of the muscles within the vocal cords, and by moving the arytenoids forward or backward. This causes the pitch produced during phonation to rise or fall. In most males the vocal cords are longer and have a greater mass than most females' vocal cords, producing a lower pitch.\nThe vocal apparatus consists of two pairs of folds, the vestibular folds (false vocal cords) and the true vocal cords. The vestibular folds are covered by respiratory epithelium, while the vocal cords are covered by stratified squamous epithelium. The vestibular folds are not responsible for sound production, but rather for resonance. The exceptions to this are found in Tibetan chanting and Kargyraa, a style of Tuvan throat singing. Both make use of the vestibular folds to create an undertone. These false vocal cords do not contain muscle, while the true vocal cords do have skeletal muscle.\nOther.\nThe most important role of the larynx is its protective function, the prevention of foreign objects from entering the lungs by coughing and other reflexive actions. A cough is initiated by a deep inhalation through the vocal cords, followed by the elevation of the larynx and the tight adduction (closing) of the vocal cords. The forced expiration that follows, assisted by tissue recoil and the muscles of expiration, blows the vocal cords apart, and the high pressure expels the irritating object out of the throat. Throat clearing is less violent than coughing, but is a similar increased respiratory effort countered by the tightening of the laryngeal musculature. Both coughing and throat clearing are predictable and necessary actions because they clear the respiratory passageway, but both place the vocal cords under significant strain.\nAnother important role of the larynx is abdominal fixation, a kind of Valsalva maneuver in which the lungs are filled with air in order to stiffen the thorax so that forces applied for lifting can be translated down to the legs. This is achieved by a deep inhalation followed by the adduction of the vocal cords. Grunting while lifting heavy objects is the result of some air escaping through the adducted vocal cords ready for phonation.\nAbduction of the vocal cords is important during physical exertion. The vocal cords are separated by about during normal respiration, but this width is doubled during forced respiration.\nDuring swallowing, elevation of the posterior portion of the tongue levers (inverts) the epiglottis over the glottis' opening to prevent swallowed material from entering the larynx which leads to the lungs, and provides a path for a food or liquid bolus to \"slide\" into the esophagus; the hyo-laryngeal complex is also pulled upwards to assist this process. Stimulation of the larynx by aspirated food or liquid produces a strong cough reflex to protect the lungs.\nIn addition, intrinsic laryngeal muscles are spared from some muscle wasting disorders, such as Duchenne muscular dystrophy, may facilitate the development of novel strategies for the prevention and treatment of muscle wasting in a variety of clinical scenarios. ILM have a calcium regulation system profile suggestive of a better ability to handle calcium changes in comparison to other muscles, and this may provide a mechanistic insight for their unique pathophysiological properties\nClinical significance.\nDisorders.\nThere are several things that can cause a larynx to not function properly. Some symptoms are hoarseness, loss of voice, pain in the throat or ears, and breathing difficulties. \nTreatments.\nPatients who have lost the use of their larynx are typically prescribed the use of an electrolarynx device. Larynx transplants are a rare procedure. The world's first successful operation took place in 1998 at the Cleveland Clinic in Cleveland, Ohio, USA, and the second took place in October 2010 at the University of California Davis Medical Center in Sacramento, California, USA.\nOther animals.\nPioneering work on the structure and evolution of the larynx was carried out in the 1920s by the British comparative anatomist Victor Negus, culminating in his monumental work \"The Mechanism of the Larynx\" (1929). Negus, however, pointed out that the descent of the larynx reflected the reshaping and descent of the human tongue into the pharynx. This process is not complete until age six to eight years. Some researchers, such as Philip Lieberman, Dennis Klatt, Bart de Boer and Kenneth Stevens using computer-modeling techniques have suggested that the species-specific human tongue allows the vocal tract (the airway above the larynx) to assume the shapes necessary to produce speech sounds that enhance the robustness of human speech. Sounds such as the vowels of the words \u27e8see\u27e9 and \u27e8do\u27e9, [i] and [u] (in phonetic notation), have been shown to be less subject to confusion in classic studies such as the 1950 Peterson and Barney investigation of the possibilities for computerized speech recognition.\nIn contrast, though other species have low larynges, their tongues remain anchored in their mouths and their vocal tracts cannot produce the range of speech sounds of humans. The ability to lower the larynx transiently in some species extends the length of their vocal tract, which as Fitch showed creates the acoustic illusion that they are larger. Research at Haskins Laboratories in the 1960s showed that speech allows humans to achieve a vocal communication rate that exceeds the fusion frequency of the auditory system by fusing sounds together into syllables and words. The additional speech sounds that the human tongue enables us to produce, particularly [i], allow humans to unconsciously infer the length of the vocal tract of the person who is talking, a critical element in recovering the phonemes that make up a word.\nNon-mammals.\nMost tetrapod species possess a larynx, but its structure is typically simpler than that found in mammals. The cartilages surrounding the larynx are apparently a remnant of the original gill arches in fish, and are a common feature, but not all are always present. For example, the thyroid cartilage is found only in mammals. Similarly, only mammals possess a true epiglottis, although a flap of non-cartilagenous mucosa is found in a similar position in many other groups. In modern amphibians, the laryngeal skeleton is considerably reduced; frogs have only the cricoid and arytenoid cartilages, while salamanders possess only the arytenoids.\nAn example of a frog that possesses a larynx is the t\u00fangara frog. While the larynx is the main sound producing organ in t\u00fangara frogs, it serves a higher significance due to its contribution to mating call, which consist of two components: 'whine' and 'chuck'. While 'whine' induces female phonotaxis and allows species recognition, 'chuck' increases mating attractiveness. In particular, the t\u00fangara frog produces 'chuck' by vibrating the fibrous mass attached to the larynx.\nVocal folds are found only in mammals, and a few lizards. As a result, many reptiles and amphibians are essentially voiceless; frogs use ridges in the trachea to modulate sound, while birds have a separate sound-producing organ, the syrinx.\nHistory and etymology.\nThe ancient Greek physician Galen first described the larynx, describing it as the \"first and supremely most important instrument of the voice\".\nThe word \"larynx\" is borrowed from the Ancient Greek (\"l\u00e1runx\", lit.).\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49378", "revid": "25046916", "url": "https://en.wikipedia.org/wiki?curid=49378", "title": "924", "text": "Calendar year\nYear 924 (CMXXIV) was a leap year starting on Thursday of the Julian calendar.\nEvents.\nJanuary\u2014March.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49379", "revid": "16416757", "url": "https://en.wikipedia.org/wiki?curid=49379", "title": "925", "text": "Calendar year\nYear 925 (CMXXV) was a common year starting on Saturday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49380", "revid": "20957809", "url": "https://en.wikipedia.org/wiki?curid=49380", "title": "929", "text": "Calendar year\nYear 929 (CMXXIX) was a common year starting on Thursday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49382", "revid": "40571816", "url": "https://en.wikipedia.org/wiki?curid=49382", "title": "928", "text": "Calendar year\nYear 928 (CMXXVIII) was a leap year starting on Tuesday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nBy topic.\nReligion.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49383", "revid": "47691707", "url": "https://en.wikipedia.org/wiki?curid=49383", "title": "576", "text": "Calendar year\nYear 576 (DLXXVI) was a leap year starting on Wednesday of the Julian calendar. The denomination 576 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.\nEvents.\n&lt;onlyinclude&gt;\nBy place.\nAsia.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49386", "revid": "990344", "url": "https://en.wikipedia.org/wiki?curid=49386", "title": "927", "text": "Calendar year\nYear 927 (CMXXVII) was a common year starting on Monday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nBy topic.\nReligion.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49387", "revid": "43240415", "url": "https://en.wikipedia.org/wiki?curid=49387", "title": "Deep Blue (chess computer)", "text": "Chess-playing computer made by IBM\nDeep Blue was a customized IBM RS/6000 SP supercomputer for chess-playing designed by computer scientist Feng-hsiung Hsu. It was the first computer to win a game, and the first to win a match, against a reigning world champion under regular time controls. Development began in 1985 at Carnegie Mellon University under the name ChipTest. It then moved to IBM, where it was first renamed Deep Thought, then again in 1989 to Deep Blue. It first played world champion Garry Kasparov in a six-game match in 1996, where it won one, drew two, and lost three games. It was upgraded in 1997, and in a six-game re-match it defeated Kasparov by winning two games and drawing three. Deep Blue's victory is considered a milestone in the history of artificial intelligence and has been the subject of several books and films.\nHistory.\nWhile a doctoral student at Carnegie Mellon University, Feng-hsiung Hsu began development of a chess-playing supercomputer under the name ChipTest. The machine won the North American Computer Chess Championship in 1987 and Hsu and his team followed up with a successor, Deep Thought, in 1988. After receiving his doctorate in 1989, Hsu and Murray Campbell joined IBM Research to continue their project to build a machine that could defeat a world chess champion. Their colleague Thomas Anantharaman briefly joined them at IBM before leaving for the finance industry and being replaced by programmer Arthur Joseph Hoane. Jerry Brody, a long-time employee of IBM Research, subsequently joined the team in 1990.\nAfter Deep Thought's two-game 1989 loss to Kasparov, IBM held a contest to rename the chess machine: the winning name was \"Deep Blue\", submitted by Peter Fitzhugh Brown, which was a play on IBM's nickname, \"Big Blue\". After a scaled-down version of Deep Blue played Grandmaster Joel Benjamin, Hsu and Campbell decided that Benjamin was the expert they were looking for to help develop Deep Blue's opening book, so they hired him to assist with the preparations for Deep Blue's matches against Garry Kasparov. In 1995, a Deep Blue prototype played in the eighth World Computer Chess Championship, playing Wchess to a draw before ultimately losing to Fritz in round five, despite playing as White.\nToday, one of the two racks that made up Deep Blue is held by the National Museum of American History, having previously been displayed in an exhibit about the Information Age, while the other rack was acquired by the Computer History Museum in 1997, and is displayed in the Revolution exhibit's \"Artificial Intelligence and Robotics\" gallery. Several books were written about Deep Blue, among them \"Behind Deep Blue: Building the Computer that Defeated the World Chess Champion\" by Deep Blue developer Feng-hsiung Hsu.\nDeep Blue versus Kasparov.\nSubsequent to its predecessor Deep Thought's 1989 loss to Garry Kasparov, Deep Blue played Kasparov twice more. In the first game of the first match, which took place from 10 to 17 February 1996, Deep Blue became the first machine to win a chess game against a reigning world champion under regular time controls. However, Kasparov won three and drew two of the following five games, beating Deep Blue by 4\u20132 at the close of the match.\nDeep Blue's hardware was subsequently upgraded, doubling its speed before it faced Kasparov again in May 1997, when it won the six-game rematch 3\u00bd\u20132\u00bd. Deep Blue won the deciding game after Kasparov failed to secure his position in the opening, thereby becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls. The version of Deep Blue that defeated Kasparov in 1997 typically searched to a depth of six to eight moves, and twenty or more moves in some situations. David Levy and Monty Newborn estimate that each additional ply (half-move) of forward insight increases the playing strength between 50 and 70 Elo points.\nIn the 44th move of the first game of their second match, unknown to Kasparov, a bug in Deep Blue's code led it to enter an unintentional loop, which it exited by taking a randomly selected valid move. Kasparov did not take this possibility into account, and misattributed the seemingly pointless move to \"superior intelligence\". Subsequently, Kasparov experienced a decline in performance in the following game, though he denies this was due to anxiety in the wake of Deep Blue's inscrutable move.\nAfter his loss, Kasparov said that he sometimes saw unusual creativity in the machine's moves, suggesting that during the second game, human chess players had intervened on behalf of the machine. IBM denied this, saying the only human intervention occurred between games. Kasparov demanded a rematch, but IBM had dismantled Deep Blue after its victory and refused the rematch. The rules allowed the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play that were revealed during the course of the match. Kasparov requested printouts of the machine's log files, but IBM refused, although the company later published the logs on the Internet.\nThe 1997 tournament awarded a $700,000 first prize to the Deep Blue team and a $400,000 second prize to Kasparov. Carnegie Mellon University awarded an additional $100,000 to the Deep Blue team, a prize created by computer science professor Edward Fredkin in 1980 for the first computer program to beat a reigning world chess champion.\nAftermath.\nChess.\nKasparov initially called Deep Blue an \"alien opponent\", but later belittled it, stating that it was \"as intelligent as your alarm clock\". According to Martin Amis, two grandmasters who played Deep Blue agreed that it was \"like a wall coming at you\". Hsu had the rights to use the Deep Blue design independently of IBM, but also independently declined Kasparov's rematch offer. In 2003, the documentary film \"\" investigated Kasparov's claims that IBM had cheated. In the film, some interviewees describe IBM's investment in Deep Blue as an effort to boost its stock value.\nOther games.\nFollowing Deep Blue's victory, AI specialist Omar Syed designed a new game, Arimaa, which was intended to be very simple for humans but very difficult for computers to master; however, in 2015, computers proved capable of defeating strong Arimaa players. Since Deep Blue's victory, computer scientists have developed software for other complex board games with competitive communities. The AlphaGo series (AlphaGo, AlphaGo Zero, AlphaZero) defeated top Go players in 2016\u20132017.\nComputer science.\nComputer scientists such as Deep Blue developer Campbell believed that playing chess was a good measurement for the effectiveness of artificial intelligence, and by beating a world champion chess player, IBM showed that they had made significant progress. Deep Blue is also responsible for the popularity of using games as a display medium for artificial intelligence, as in the cases of IBM Watson or AlphaGo.\nWhile Deep Blue, with its capability of evaluating 200\u00a0million positions per second, was the first computer to face a world chess champion in a formal match, it was a then-state-of-the-art expert system, relying upon rules and variables defined and fine-tuned by chess masters and computer scientists. In contrast, current chess engines such as Leela Chess Zero typically use reinforcement machine learning systems that train a neural network to play, developing its own internal logic rather than relying upon rules defined by human experts.\nIn a November 2006 match between Deep Fritz and world chess champion Vladimir Kramnik, the program ran on a computer system containing a dual-core Intel Xeon 5160 CPU, capable of evaluating only 8\u00a0million positions per second, but searching to an average depth of 17 to 18 plies (half-moves) in the middlegame thanks to heuristics; it won 4\u20132.\nDesign.\nSoftware.\nDeep Blue ran under the AIX operating system, and its chess playing program was written in C. \nIts evaluation function was initially written in a generalized form, with many to-be-determined parameters (e.g., how important is a safe king position compared to a space advantage in the center, etc.). Values for these parameters were determined by analyzing thousands of master games. The evaluation function was then split into 8,000 parts, many of them designed for special positions. The opening book encapsulated more than 4,000 positions and 700,000 grandmaster games, while the endgame database contained many six-piece endgames and all five and fewer piece endgames. An additional database named the \"extended book\" summarizes entire games played by Grandmasters. The system combines its searching ability of 200 million chess positions per second with summary information in the extended book to select opening moves.\nBefore the second match, the program's rules were fine-tuned by grandmaster Joel Benjamin. The opening library was provided by grandmasters Miguel Illescas, John Fedorowicz, and Nick de Firmian. When Kasparov requested that he be allowed to study other games that Deep Blue had played so as to better understand his opponent, IBM refused, leading Kasparov to study many popular PC chess games to familiarize himself with computer gameplay.\nHardware.\nDeep Blue used custom VLSI chips to parallelize the alpha\u2013beta search algorithm, an example of symbolic AI. The system derived its playing strength mainly from computing power. It was an IBM RS/6000 SP, a supercomputer with a massively parallel architecture based on 30 PowerPC 604e processors and 480 custom 600\u00a0nm CMOS VLSI \"chess chips\" designed to execute the chess-playing expert system, as well as FPGAs intended to allow patching of the VLSIs (which ultimately went unused) all housed in two cabinets. The chess chip has four parts: the move generator, the smart-move stack, the evaluation function, and the search control. The move generator is a 8x8 combinational logic circuit, a chess board in miniature.\nIn 1997, Deep Blue was upgraded again to become the 259th most powerful supercomputer according to the TOP500 list, achieving 11.38 GFLOPS on the parallel high performance LINPACK benchmark. Deeper Blue was capable of evaluating 200\u00a0million positions per second, twice as many as the 1996 version.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49388", "revid": "29463730", "url": "https://en.wikipedia.org/wiki?curid=49388", "title": "932", "text": "Calendar year\nYear 932 (CMXXXII) was a leap year starting on Sunday of the Julian calendar.\nEvents.\n&lt;onlyinclude&gt;\nBy topic.\nReligion.\n&lt;/onlyinclude&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49390", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=49390", "title": "Cello (web browser)", "text": "Web browser\nCello is an early, discontinued graphical web browser for Windows 3.1; it was developed by Thomas R. Bruce of the Legal Information Institute at Cornell Law School. It was released as shareware in 1993. While other browsers ran on various Unix machines, Cello was the first web browser for Microsoft Windows, using the winsock system to access the Internet. In addition to the basic Windows, Cello worked on Windows NT 3.5 and with small modifications on OS/2.\nCello was created because of a demand for Web access by lawyers, who were more likely to use Microsoft Windows than the Unix operating systems supporting earlier Web browsers, including the first release of Mosaic. The lack of a Windows browser meant many legal experts were unable to access legal information made available in hypertext on the World Wide Web. Cello was popular during 1993/1994, but fell out of favor following the release of Mosaic for Windows and Netscape, after which Cello development was abandoned.\nCello was first publicly released on 8 June 1993. A version 2.0 was announced, but development was abandoned. Version 1.01a, 16 April 1994, was the last public release. Since then, the Legal Information Institute at Cornell Law School has licensed the Cello 2.0 source code, which has been used to develop commercial software.\nThe browser is no longer available from its original homepage. However, it can still be downloaded from mirror sites.\nDevelopment and history.\nThe development of Cello started in 1992, with beta versions planned for June 1993 and a release for July 1993. It was publicly announced on 12 April 1993.\nThe Legal Information Institute at Cornell Law School created the first law site on the\nInternet in 1992 and the first legal website in 1993. However, at the time, there were no web browsers for the Microsoft Windows operating system, which was used by most lawyers. Thus, to allow lawyers to use their website, the Legal Information Institute developed the first Windows-based Web browser. This was made possible by a grant from the National Center for Automated Information Research.\nAlthough other browsers at the time were based on CERN's WWW libraries called libwww, PCs of the time were not powerful enough to run the UNIX-oriented code. As a result, Thomas Bruce had to rewrite most of the WWW libraries to work on Microsoft Windows. Unlike most commercial browsers at that time, Cello used none of Mosaic's source code and thus had a different look and feel.\nSteven Sinofsky, president of the Windows division at Microsoft wrote in a June 1994 email: \"We do not currently plan on any other client software [in the upcoming release of Windows 95], especially something like Mosaic or Cello.\" Nevertheless, on 11 January 1995, Microsoft announced that it had licensed the Mosaic technology from Spyglass, which it would use to create Internet Explorer. On 15 August 1995, Microsoft debuted its own web browser Internet Explorer 1 for Windows 95. While it did not ship with the original release of Windows 95, it shipped with Microsoft Plus! for Windows 95.\nUsage.\nWhen released in 1993, Cello was the only browser for the Microsoft Windows platform. Shortly after launch, Cello was being downloaded at a rate of 500 copies per day. As such, it achieved a fair amount of use and recognition within the legal community, including a number of PC users with between 150,000 and 200,000 users. In 1994, most websites were visited using either the Cello browser or the Mosaic browser. Despite having fewer features than Mosaic, Cello continued to be used due to its simpler interface and lower system requirements. Cello was praised for being easy to install, because it wasn't necessary to install Win32s or a TCP/IP stack for Windows 3.1. Following the release of Windows 95, which offered a much better TCP/IP interface, Cello fell into disuse and was abandoned.\nBy 1995, Cello, like the Mosaic browser, was overshadowed by two newer browsers \u2014 Netscape and Internet Explorer \u2014 and fell into disuse. By 1999, Cello was considered to be a \"historical\" browser.\nCello is considered to be one of the early casualties of the first browser wars.\nFeatures.\nCello had the following features:\nUnlike Mosaic, \"Cello did not have toolbar buttons\", and instead commands were accessed through pull-down menus.\nCello supported the following protocols: HTTP 1.0, Gopher (but not Gopher+), read-only FTP, SMTP mailing, Telnet, Usenet, CSO/ph/qi directly and WAIS, HyTelnet, TechInfo, Archie, X.500, TN3270 and a number of others through public gateways.\nCello supported the following FTP servers: most Unix servers (including SunOS, System V, and Linux), IBM VM, VMS systems, Windows NT, QVTNet, NCSA/CUTCP/Rutgers PC servers, FTP Software PC server, HellSoft NLM for Novell.\nCello works best with a direct Ethernet connection, but it also supports SLIP and PPP dialup connections through the use of asynchronous sockets. Cello has an integrated TCP/IP runtime stack.\nRelease history.\nThe following versions were released:\nAlthough Cello 2.0 had been announced, development ceased before a public release.\nIBM released a fix for their TCP/IP V2.0 stack so that Cello would work with OS/2 WinOS/2 on 9 February 1994.\nBrowser comparison table.\nThe following table shows how Cello compared to browsers of its time.\nTechnical.\nWhile originally Cello required the Distinct Corporation's TCP/IP stack, with the release of Cello Beta Version .8, Cello dropped support for Distinct, and became exclusively Winsock-based.\nOriginally, although Cello could run on OS/2, OS/2's implementation of Winsock had bugs that prevented Cello from accessing the Internet. The bug, \"APAR #PN52335\", was later fixed allowing Cello to properly work on OS/2.\nThe user agent for Cello is codice_1.\nDDE support.\nCello featured Dynamic Data Exchange (DDE) support. OLE support and DDE client support were planned, but never released.\nAn example of how to invoke Cello from a Microsoft Word macro:\nSub MAIN\nChanNum = DDEInitiate(\"Cello\", \"URL\")\nDDEExecute(ChanNum, \"http://www.law.cornell.edu\")\nDDETerminate(ChanNum)\nEnd Sub\nSystem requirements.\nCello has the following system requirements:\nCriticism.\nCello was not very stable and its development halted early.\nCello did not render graphics well and required that the user reload the webpage when resizing the window. Like most browsers at the time, Cello also did not support any web security protocols. It was also said that Cello rendered html \"crudely\" and pages would appear jaggedly.\nCello also had sub-par performance in accessing the Internet and processing hypermedia documents.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49392", "revid": "10759180", "url": "https://en.wikipedia.org/wiki?curid=49392", "title": "Affirmative action", "text": "Policies aiming to increase inclusion of people from marginalized groups\nAffirmative action (also sometimes called reservations, alternative access, positive discrimination or positive action in various countries' laws and policies) refers to a set of policies and practices within a government or organization seeking to address systemic discrimination. Historically and internationally, support for affirmative action has been justified by the idea that it may help with bridging inequalities in employment and pay, increasing access to education, and promoting diversity, social equity, and social inclusion and redressing wrongs, harms, or hindrances, also called substantive equality.\nThe nature of affirmative-action policies varies from region to region and exists on a spectrum from a hard quota to merely targeting encouragement for increased participation. Some countries use a quota system, reserving a certain percentage of government jobs, political positions, and school vacancies for members of a certain group; an example of this is the reservation system in India. In some other jurisdictions where quotas are not used, minority-group members are given preference or special consideration in selection processes. In the United States, affirmative action by executive order originally meant selection without regard to race but preferential treatment was widely used in college admissions, as upheld in the 2003 Supreme Court case \"Grutter v. Bollinger\", until 2023, when this was overturned in \"Students for Fair Admissions v. Harvard.\"\nA variant of affirmative action more common in Europe is known as positive action, wherein equal opportunity is promoted by encouraging underrepresented groups into a field. This is often described as being \"color blind\", but some American sociologists have argued that this is insufficient to achieve substantive equality of outcomes based on race.\nIn the United States, affirmative action is controversial and public opinion on the subject is divided. Supporters of affirmative action argue that it promotes substantive equality for group outcomes and representation for groups, which are socio-economically disadvantaged or have faced historical discrimination or oppression. Opponents of affirmative action have argued that it is a form of reverse discrimination, that it tends to benefit the most privileged within minority groups at the expense of the least fortunate within majority groups, or that\u2014when applied to universities\u2014it can hinder minority students by placing them in courses for which they have not been adequately prepared.\nOrigins.\nThe term \"affirmative action\" was first used in the United States in Executive Order 10925, signed by President John F. Kennedy on 6 March 1961, which included a provision that government contractors \"take \"affirmative action\" to ensure that applicants are employed, and employees are treated [fairly] during employment, without regard to their race, creed, color, or national origin\". In 1965, President Lyndon B. Johnson issued Executive Order 11246 which required government employers to \"hire without regard to race, religion and national origin\" and \"take affirmative action to ensure that applicants are employed and that employees are treated during employment, without regard to their race, color, religion, sex or national origin.\" The Civil Rights Act of 1964 prohibited discrimination on the basis of race, color, religion, sex or national origin. Neither executive order nor The Civil Rights Act authorized group preferences. The Senate floor manager of the bill, Senator Hubert Humphrey, declared that the bill \"would prohibit preferential treatment for any particular group\" adding \"I will eat my hat if this leads to racial quotas.\"\n However affirmative action in practice would eventually become synonymous with preferences, goals and quotas as upheld or struck down by Supreme Court decisions even though no law had been passed explicitly permitting discrimination in favor of disadvantaged groups. \nAffirmative action is intended to promote the opportunities of defined minority groups within a society to give them equal access to that of the majority population. The philosophical basis of the policy has various rationales, including but not limited to compensation for past discrimination, correction of current discrimination, and the diversification of society. It is often implemented in governmental and educational settings to ensure that designated groups within a society can participate in all promotional, educational, and training opportunities.\nThe stated justification for affirmative action by its proponents is to help compensate for past discrimination, persecution or exploitation by the ruling class of a culture, and to address existing discrimination.\nWomen.\nSeveral different studies investigated the effect of affirmative action on women. Kurtulus (2012) in her review of affirmative action and the occupational advancement of minorities and women during 1973\u20132003 showed that the effect of affirmative action on advancing black, Hispanic, and white women into management, professional, and technical occupations occurred primarily during the 1970s and early 1980s. During this period, contractors grew their shares of these groups more rapidly than non-contractors because of the implementation of affirmative action. But the positive effect of affirmative action vanished entirely in the late 1980s, which Kurtulus says may be due to the slowdown into advanced occupation for women and minorities because of the political shift of affirmative action that started with President Reagan. Becoming a federal contractor increased white women's share of professional occupations by 0.183 percentage points, or 9.3 percent, on average during these three decades, and increased black women's share by 0.052 percentage points (or by 3.9 percent). Becoming a federal contractor also increased Hispanic women's and black men's share of technical occupations on average by 0.058 percent and 0.109 percentage points respectively (or by 7.7 and 4.2 percent). These represent a substantial contribution of affirmative action to overall trends in the occupational advancement of women and minorities over the three decades under the study. A reanalysis of multiple scholarly studies, especially in Asia, considered the impact of four primary factors on support for affirmative action programs for women: gender; political factors; psychological factors; and social structure. Kim and Kim (2014) found that, \"Affirmative action both corrects existing unfair treatment and gives women equal opportunity in the future.\"\nQuotas.\nLaw regarding quotas and affirmative action varies widely from nation to nation.\nCaste-based and other group-based quotas are used in the reservation system.\nIn 2012, the European Union Commission approved a plan for women to constitute 40% of non-executive board directorships in large listed companies in Europe by 2020. Directive (EU) 2022/2381 requires that EU member states adopt by 28 December 2024 laws to ensure that by 30 June 2026 members of the underrepresented sex hold at least 40% of non-executive director positions and at least 33% of all director positions, including both executive and non-executive directors, for listed companies. Directive (EU) 2022/2381 expires on 31 December 2038.\nIn Sweden, the Supreme Court has ruled that \"affirmative action\" ethnic quotas in universities are discrimination and hence unlawful. It said that the requirements for the intake should be the same for all. The justice minister said that the decision left no room for uncertainty.\nNational approaches.\nIn some countries that have laws on racial equality, affirmative action is rendered illegal because it does not treat all races equally. This approach of equal treatment is sometimes described as being \"color blind\", in hopes that it is effective against discrimination without engaging in reverse discrimination.\nIn such countries, the focus tends to be on ensuring equal opportunity and, for example, targeted advertising campaigns to encourage ethnic minority candidates to join the police force. This is sometimes called positive action.\nAfrica.\nSouth Africa.\nBetween 1948 and 1974, the apartheid government introduced statutes which enshrined racial discrimination in all areas of life. Individuals were classified in a racial hierarchy which placed whites at the top, followed by \"Coloureds\", \nthen Asians or Indians, with black Africans at the bottom. Benefits were afforded based on this hierarchy, and favoured white-owned, especially Afrikaner-owned companies, which marginalised and excluded black people and limited their employment opportunities. Legislation meant that skilled and highly paid jobs were reserved for white people, and black people were largely used as cheap, unskilled labour, creating and extending the \"colour bar\" in South African labour. The variation in skills and productivity between groups of people ultimately caused disparities in employment, occupation, and income within labour markets. Following the end of apartheid, affirmative action legislation aimed to address these disparities.\nThe African National Congress-led government chose to implement affirmative action legislation to correct previous imbalances (a policy known as employment equity), and fulfil the obligations of the Republic as a member of the International Labour Organisation. As a result of the Employment Equity Act and the Broad Based Black Economic Empowerment Act, companies were required to employ previously disenfranchised groups (blacks, Indians, and Coloureds), as well as women and disabled people. \nMany have embraced these acts; others have criticised them. Proponents have said that South African affirmative action legislation aims to promote economic growth rather than to redistribute wealth, address vast racial inequalities in wealth and income, and to restore equal access to the benefits of society. Critics suggest these laws limit the free market, raise costs, reduce economic growth, and advantage the black middle class over poorer blacks and other groups. The Supreme Court of Appeal of South Africa ruled that while blacks may be favoured in principle, in practice this should not lead to unfair discrimination against others.\nGhana.\nThe Parliament of Ghana passed the Affirmative Action Bill on July 30, 2024. The bill aims to increase the number of women in politics, with a target of 30% by 2026, 35% by 2028 and 50% by 2030. The 2024 Ghanaian general election in December was the first with affirmative action legislation in place. In the election campaign, both the ruling New Patriotic Party and the main opposition party NDC voiced their commitment to the targets, in an effort to appeal to women voters. After the election, the number of women remained the same as after the 2020 election (15%). In his inaugural speech on 7 January 2025, the newly elected President John Mahama said the NDC was committed to \"breaking the glass ceiling\" for women in politics.\nAsia.\nChina.\nThere is affirmative action in education for minority nationalities in China, this may equate to lowering minimum requirements for the National University Entrance Examination, which is mandatory for all students to enter university. Liangshaoyikuan refers a policy in China on affirmative action in criminal justice.\nIsrael.\nA class-based affirmative action policy was incorporated into the admission practices of the four most selective universities in Israel during the early to mid-2000s. In evaluating the eligibility of applicants, neither their financial status nor their national or ethnic origins are considered; the emphasis is on structural disadvantages, especially neighborhood socioeconomic status and high school rigor, although several individual hardships are also weighed. This policy made the four institutions, especially the echelons at the most selective departments, more diverse than they otherwise would have been. Proponents of this model have said that rising geographic, economic and demographic diversity of the student population suggests the focus on structural determinants of disadvantage yields broad diversity dividends.\nIn civil service employment, Israeli citizens who are women, Arabs, Blacks or people with disabilities are supported by affirmative action policies. Israeli citizens who are Arabs, Blacks or people with disabilities are also entitled to full university scholarships from the state.\nIndia.\nReservation in India is a form of affirmative action designed to improve the well-being of Scheduled Castes and Scheduled Tribes (SC/ST), and Other Backward Classes (OBC), defined primarily by their caste. Members of these categories comprise about two-thirds of the population of India. According to the Constitution of India, up to 50% of all government-run higher education admissions and government job vacancies may be reserved for members of the SC/ST/OBC-NCL categories, and 10% for those in Economically Weaker Sections (EWS), with the remaining unreserved. In 2014, the Indian National Sample Survey found that 12% of surveyed Indian households had received academic scholarships, with 94% being on account of SC/ST/OBC membership, 2% based on financial weakness and 0.7% based on merit.\nIndonesia.\nIndonesia has offered affirmative action for native Papuans in education, government civil worker selection, and police &amp; army selection. After the 2019 Papua protests, many Papuan students chose to abandon their scholarship and return to their respective provinces. The program has been subject to criticism, with complaints made towards a lack of sufficient quotas and alleged corruption. Prabowo Subianto, Indonesian defense minister, has expressed that he will direct more effort towards recruiting Papuans to the Indonesian National Armed Forces. Education scholarship by Ministry of Education and Culture, called ADik to the native Papuans and students from perhipery regions close to Indonesian border.\nMalaysia.\nThe Malaysian New Economic Policy (NEP) is a form of ethnicity-based affirmative action aimed at addressing socioeconomic disadvantages among those who are deemed \"Bumiputera\", which includes the Malay population, Orang Asli, and the indigenous people of Sabah and Sarawak, who together form a majority of the population. Within Malaysia, the Malays (representing 58% of the population) have lower incomes than Chinese Malaysians (22% of the population) and Indian Malaysians (6% of the population), who have traditionally been involved in businesses and industries, and who were also general migrant workers. The mean income for Malays, Chinese and Indians in 1957/58 were 134, 288 and 228 respectively. In 1967/68 it was 154, 329 and 245, and in 1970 it was 170, 390 and 300. Mean income disparity ratio for Chinese/Malays rose from 2.1 in 1957/58 to 2.3 in 1970, whereas for Indians/Malays the disparity ratio also rose from 1.7 to 1.8 in the same period.\nTo address these inequalities, following the sectarian violence of the 13 May incident in 1969, the NEP was introduced as a time-limited policy, which was supposed to expire after 20 years but remains policy to this day. Although the NEP has succeeded in creating a significant urban Malay and Native Bornean middle class, it has been less effective in eradicating poverty among rural communities. Critics say it has widened disparities between the wealthy and middle classes, and those who are poorest. It has also been described as racially discriminatory.\nTaiwan.\nA 2004 legislation requires that, for a firm with 100 employees or more wishing to compete for government contracts, at least 1 percent of its employees must be Taiwanese aborigines. Ministry of Education and Council of Aboriginal Affairs announced in 2002 that Taiwanese Aboriginal students would have their high-school or undergraduate entrance exams boosted by 33% for demonstrating some knowledge of their tribal language and culture. The percentage of boost have been revised several times, and the latest percentage is 35% in 2013.\nEurope.\nDenmark.\nGreenlanders have special advantages when applying for university, college or vocation university degrees in Denmark. With these specific rules, Greenlanders can get into degrees without the required grade averages by fulfilling certain criteria. They need to have a grade average of over 6,0 and have lived a certain number of years in Greenland. These rules have been in force since 1 January 2014.\nFinland.\nIn certain university education programs, including legal and medical education, there are quotas for persons who reach a certain standard of skills in the Swedish language; for students admitted in these quotas, the education is partially arranged in Swedish. The purpose of the quotas is to guarantee that a sufficient number of professionals with skills in Swedish are educated for nationwide needs.\nFrance.\nNo distinctions based on race, religion or sex are allowed under the 1958 French Constitution. Since the 1980s, a French version of affirmative action based on neighborhood is in place for primary and secondary education. Some schools, in neighborhoods labeled \"Priority Education Zones\", are granted more funds than the others. Students from these schools also benefit from special policies in certain institutions (such as Sciences Po).\nThe French Ministry of Defence tried in 1990 to make it easier for young French soldiers of North-African descent to be promoted in rank and obtain driving licenses. After a strong protest by a young French lieutenant in the Ministry of Defence newspaper (\"Arm\u00e9es d'aujourd'hui\"), the driving license and rank plan was cancelled. After the Sarkozy election, a new attempt in favour of Arab-French students was made, but Sarkozy did not gain enough political support to change the French constitution.\nAfter 27 January 2014, following the Norwegian example, women had to represent at least 20% of board members in all stock-exchange-listed or state-owned companies. After 27 January 2017, the proportion increased to 40%. All appointments of men as directors were deemed invalid as long as the quotas were not met, and monetary penalties may apply for other directors.\nGermany.\nArticle 3 of the German Basic Law provides for equal rights of all people regardless of sex, race or social background. There are programs stating that if men and women have equal qualifications, women have to be preferred for a job; moreover, the disabled should be preferred to non-disabled people. This is typical for all positions in state and university service as of 2007[ [update]], typically using the phrase \"We try to increase diversity in this line of work\". In recent years, there has been a long public debate about whether to issue programs that would grant women a privileged access to jobs in order to fight discrimination. Germany's \"Left Party\" brought up the discussion about affirmative action in Germany's school system. According to Stefan Zillich, quotas should be \"a possibility\" to help working class children who did not do well in school gain access to a \"Gymnasium\" (University-preparatory school). Headmasters of \"Gymnasien\" have objected, saying that this type of policy would \"be a disservice\" to poor children.\nNorway.\nThe boards of all public stock companies (ASA) in Norway should have at least 40% of either gender. This affects roughly 400 companies of over 300,000 in total. In their study of the effects of affirmative action on presence, prominence, and social capital of women directors in Norway, researchers Seierstad &amp; Opsahl found that, when the affirmative action policy was first implemented, only 7 prominent directors were women and 84 were men. By August 2009, this had risen to 107 women compared to 117 men. The proportion of boards led by a woman remained low overall, but increased from 3.4% to 4.3%. By applying more restrictive definitions of prominence, the proportion of directors who were women generally increased. If only considering directors with at least three directorships, 61.4% of them were women. When considering directors with seven or more directorships, all of them were women. A 2016 study found no effect of the ASA representation requirement on either valuation or profits of the affected companies, and also no correlation between the requirement and the restructuring of companies away from ASA.\nRomania.\nRomani people are allocated quotas for access to public schools and state universities.\nSoviet Union and Russia.\nSoon after the 1918 revolution, Inessa Armand, Lenin's secretary and lover, was instrumental in creating Zhenotdel, which functioned until the 1930s as part of the international egalitarian and affirmative action movements. Quota systems existed in the USSR for various social groups including ethnic minorities, women and factory workers. Before 1934 ethnic minorities were described as culturally backward, but in 1934 this term was found inappropriate. In 1920s and early 1930s Korenizatsiia applied affirmative action to ethnic minorities. Quotas for access to university education, offices in the Soviet system and the Communist Party existed: for example, the position of First Secretary of a Soviet Republic's (or Autonomous Republic's) Party Committee was always filled by a representative of this republic's \"titular ethnicity\". Russia retains this system partially. Quotas are abolished, but preferences for some ethnic minorities and inhabitants of certain territories remain.\nSerbia.\nThe Constitution of the Republic of Serbia from 2006 established the principles of equality and the prohibition of discrimination on any grounds. It also allows affirmative action as \"special measures\" for certain marginalized groups, such as national minorities, by specifically excluding it from the legal definition of discrimination. The Roma national minority is enabled to enroll in public schools under more favorable conditions.\nSlovakia.\nThe Constitutional Court declared in October 2005 that affirmative action\u2014i.e., \"providing advantages for people of an ethnic or racial minority group\"\u2014was against its Constitution.\nUnited Kingdom.\nIn the United Kingdom, hiring someone simply because of their protected-group status, without regard to their performance, is illegal. By default, so is any other form of discrimination, quota or favouritism based on such \"protected characteristics\" in education, in employment, during commercial transactions, at private clubs or associations, and while using public services. \nThe Equality Act 2010 does allow for membership in a protected and disadvantaged group to be considered in hiring and promotion when the group is under-represented in a given area and if the candidates are of equal merit (in which case membership in a disadvantaged group can become a \"tie-breaker\"). Under Section 159 of the Equality Act 2010, an employer must \"reasonably think that people with the protected characteristic suffer a disadvantage or are under-represented in that particular activity\" and any positive action must be \"a proportionate means of enabling or encouraging people to overcome the disadvantage or to take part in the activity\". Specific exemptions include:\nIn a 2019, an employment tribunal, found that \"while positive action can be used to boost diversity, it should only be applied to distinguish between candidates who were all equally well qualified for a role\".\nNorth America.\nCanada.\nThe equality section of the Canadian Charter of Rights and Freedoms explicitly permits affirmative action legislation, although the Charter does not \"require\" legislation that gives preferential treatment. Subsection 2 of Section 15 states that the equality provisions do \"not preclude any law, program or activity that has as its object the amelioration of conditions of disadvantaged individuals or groups including those that are disadvantaged because of race, national or ethnic origin, colour, religion, sex, age or mental or physical disability\".\nThe Canadian Employment Equity Act requires employers in federally-regulated industries to give preferential treatment to four designated groups: women, persons with disabilities, aboriginal peoples, and visible minorities. Less than one-third of Canadian Universities offer alternative admission requirements for students of aboriginal descent. Some provinces and territories also have affirmative action policies. For example, in the Northwest Territories in the Canadian north, aboriginal people are given preference for jobs and education and are considered to have P1 status. Non-aboriginal people who were born in the NWT or have resided half of their life there are considered a P2, as well as women and people with disabilities.\nUnited States.\nThe United States' policy of affirmative action dates to the Reconstruction Era in the United States. Current policy was introduced in the early 1960s in the United States, as a way to combat racial discrimination in the hiring process, with the concept later expanded to address gender discrimination. During this time, the legal and constitutional legitimacy of affirmative action has been the subject of several court cases.\nAffirmative action was first created from Executive Order 10925, which was signed by President John F. Kennedy on 6 March 1961 and required that government employers \"not discriminate against any employee or applicant for employment because of race, creed, color, or national origin\" and \"take affirmative action to ensure that applicants are employed and that employees are treated during employment, without regard to their race, creed, color, or national origin\" but did not require or permit group preferences.\nOn 24 September 1965, President Lyndon B. Johnson signed Executive Order 11246, thereby replacing Executive Order 10925, but continued to use the same terminology that did not require or permit group preferences. Affirmative action was extended to sex by Executive Order 11375 which amended Executive Order 11246 on 13 October 1967, by adding \"sex\" to the list of protected categories. In the U.S., affirmative action's original purpose was to pressure institutions into compliance with the nondiscrimination mandate of the Civil Rights Act of 1964. The Civil Rights Acts do not cover discrimination based on veteran status, disabilities, or age that is 40 years and older. These groups may be protected from discrimination under different laws. \nSome colleges use financial criteria to attract racial groups that have typically been under-represented and typically have lower living conditions. Some states such as California (California Civil Rights Initiative), Michigan (Michigan Civil Rights Initiative), and Washington (Initiative 200) have passed constitutional amendments banning public institutions, including public schools, from practicing affirmative action within their respective states. \nSince the 1990s, conservative groups have increasingly suggested that college quotas have been used to illegally discriminate against people on the basis of race and have launched numerous lawsuits to stop them. In 2003, a Supreme Court decision regarding affirmative action in higher education (\"Grutter v. Bollinger\", 539 US 244 \u2013 Supreme Court 2003) permitted educational institutions to consider race as a factor when admitting students. In 2014, the U.S. Supreme Court held that \"States may choose to prohibit the consideration of racial preferences in governmental decisions\". By that time, eight states (Oklahoma, New Hampshire, Arizona, Colorado, Nebraska, Michigan, Florida, Washington, and California) had already banned affirmative action. On 29 June 2023, the Supreme Court ruled 6\u20132 that the use of race in college admissions is unconstitutional under the Equal Protection Clause of the 14th Amendment in \"Students for Fair Admissions v. Harvard\".\nOceania.\nNew Zealand.\nIndividuals of M\u0101ori or other Polynesian descent are often afforded improved access to university courses, or have scholarships earmarked specifically for them. Such access to University courses have in the past faced criticism, particularly at the University of Auckland due to a phenomenon known as Mismatch theory, accusations of setting the kids up to fail have been made due to a lack of transparency as to the preferred groups graduation rates and the university informing the students of such historical statistics dating back to the 1970s. \nAffirmative action is provided for under section 73 of the Human Rights Act 1993 and section 19(2) of the New Zealand Bill of Rights Act 1990. Affirmative action in New Zealand is most often carried out indirectly by encouraging those in groups favored by affirmative action to get jobs in sectors they are underrepresented in. Diversity Awards NZ is an organization in New Zealand whose goal is to \" celebrate excellence in workplace diversity, equity and inclusion.\"\nUnder section 73 of the Human Rights Act 1993, affirmative action would be permissible if:\nSouth America.\nBrazil.\nSome Brazilian universities (state and federal) have created systems of preferred admissions (quotas) for racial minorities (blacks and Amerindians), the poor and people with disabilities. There are also quotas of up to 20% of vacancies reserved for people with disabilities in the civil public services. The Democrats party, accusing the board of directors of the University of Bras\u00edlia for \"resurrecting Nazist ideals\", appealed to the Supreme Federal Court against the constitutionality of the quotas the university reserves for minorities. The Supreme Court unanimously approved their constitutionality on 26 April 2012.\nInternational organizations.\nUnited Nations.\nThe International Convention on the Elimination of All Forms of Racial Discrimination stipulates (in Article 2.2) that affirmative action programs may be required of countries that ratified the convention, in order to rectify systematic discrimination. It states, however, that such programs \"shall in no case entail as a consequence the maintenance of unequal or separate rights for different racial groups after the objectives for which they were taken have been achieved\".\nThe United Nations Human Rights Committee states that \"the principle of equality sometimes requires States parties to take affirmative action in order to diminish or eliminate conditions which cause or help to perpetuate discrimination prohibited by the Covenant. For example, in a State where the general conditions of a certain part of the population prevent or impair their enjoyment of human rights, the State should take specific action to correct those conditions. Such action may involve granting for a time to the part of the population concerned certain preferential treatment in specific matters as compared with the rest of the population. However, as long as such action is needed to correct discrimination, in fact, it is a case of legitimate differentiation under the Covenant.\"\nResponses.\nProponents of affirmative action say it aims to promote societal equality through the preferential treatment of socioeconomically disadvantaged people. Often, these people are disadvantaged for historical reasons, such as oppression or slavery.\nHistorically and internationally, support for affirmative action has sought to achieve a range of goals: bridging inequalities in employment and pay; increasing access to education; enriching state, institutional, and professional leadership with the full spectrum of society; redressing apparent past wrongs, harms, or hindrances, in particular addressing the apparent social imbalance left in the wake of slavery and slave laws. Critics of affirmative action have suggested that affirmative action hinders reconciliation, replaces old wrongs with new wrongs, undermines the achievements of minorities, and encourages individuals to identify themselves as disadvantaged, even if they are not. It may increase racial tension and benefit the more privileged people within minority groups at the expense of the least fortunate within majority groups. \nA 2017 study of temporary federal affirmative action regulation in the United States estimated that the regulation \"increases the black share of employees over time: in 5 years after an establishment is first regulated, the black share of employees increases by an average of 0.8 percentage points. Strikingly, the black share continues to grow at a similar pace even after an establishment is deregulated. [The author] argue[s] that this persistence is driven in part by affirmative action inducing employers to improve their methods for screening potential hires.\"\nJournalist Vann R. Newkirk II says that critics of affirmative action often claim court cases such as \"Fisher v. University of Texas\", which held that colleges have some discretion to consider race when making admissions decisions, demonstrate that discrimination occurs in the name of affirmative action. He says this is one of several \"misconceptions\" often used to engender \"white resentment\" in opposition to affirmative action. \nAccording to scholar George Sher, some critics of affirmative action say that it devalues the accomplishments of individuals chosen only based on the social groups to which they belong rather than their qualifications. Legal scholar Tseming Yang and others have discussed the challenges of fraudulent self-identification when implementing affirmative action policies. Yang suggests that because some individuals from non-preferred groups may designate themselves as members of preferred groups to access the benefits of such programs, this requires the \"necessary evil\" of verifying individuals' race to prevent this. Critics of affirmative action also suggest that programs may benefit the members of the targeted group that least need the benefit\u2014that is, those who have the greatest social, economic and educational advantages within the targeted group\u2014or may lead the beneficiaries of affirmative action to conclude that it is unnecessary to work as hard, and those who do not benefit may perceive hard work as futile. Political scientist Charles Murray has said that beneficiaries are often wholly unqualified for the opportunity made available, citing his belief in the innate differences between races. He reaffirmed these views in his essay \"The Advantages of Social Apartheid\", in which he advocates separation of people based on race and intelligence. \nMismatching.\n\"Mismatching\" is the proposed negative effect affirmative action has when it places a student into a college that is too difficult for them based on meeting quotas, which may increase the chance they drop out or fail the course, thus hurting the intended beneficiaries of affirmative action. According to this theory, in the absence of affirmative action, a student may be admitted to a college that matches their academic ability and therefore has a better chance of graduating. \nIn 2017, researcher Andrew J. Hill found that affirmative action bans resulted in a reduction in minority students completing four-year STEM degrees, and suggests this indicates that the mismatch hypothesis is unfounded. He says this is evidence that affirmative action may be effective in \"some circumstances\", such as in encouraging greater minority engagement in STEM degrees. In 2020, researcher Zachary Bleemer found that an affirmative action ban in California (Prop 209) had resulted in average wage drops of 5% annually among underrepresented minorities aged 24\u201334 in STEM industries, especially effecting Hispanic people.\nIn 2007, Gail Heriot, a professor of law at the University of San Diego and a member of the U.S. Commission on Civil Rights, discussed the evidence in support of mismatching in law courses. She pointed to a study by Richard Sander which suggests there were 7.9% fewer Black attorneys than if there had been no affirmative action. Sander suggests that mismatching meant Black students were more likely to drop out of law school and fail bar exams. Sander's paper on mismatching has been criticized by several law professors, including Ian Ayres and Richard Brooks from Yale, who argue that eliminating affirmative action would actually reduce the number of\nBlack lawyers by 12.7%. Furthermore, they suggest that students attending higher ranking colleges do better than those who don't. A 2008 study by Jesse Rothstein and Albert H. Yoon said Sander's results were \"plausible\", but said that eliminating affirmative action would \"lead to a 63 percent decline in black matriculants at all law schools and a 90 percent decline at elite law schools\". They dismissed the mismatch theory, concluding that \"one cannot credibly invoke mismatch effects to argue that there are no benefits\" to affirmative action. In a 2016 review of previous studies by Peter Arcidiacono and Michael Lovenheim, they suggested that more African-American students attending less-selective schools would significantly improve first-attempt pass rates at the state bar, but cautioned that such improvements could be outweighed by decreases in law school attendance.\nA 2011 study of data held by Duke University said there was no evidence of mismatch, and proposed that mismatch could only occur if a selective school possessed private information about students' prospects at the college which it failed to share. Providing such information to prospective students would avoid mismatch because the students could choose another school that was a better match. A 2016 study on affirmative action in India said there was no evidence for the mismatching hypothesis.\nPolls.\nAccording to a poll taken by \"USA Today\" in 2005, the majority of Americans supported affirmative action for women, while views on minority groups were more split. Men are only slightly more likely to support affirmative action for women, though a majority of both do. However, a slight majority of Americans do believe that affirmative action goes beyond ensuring access and goes into the realm of preferential treatment. Also in 2005, a Gallup poll showed that 72% of black Americans and 44% of white Americans supported racial affirmative action (with 21% and 49% opposing), with support and opposition among Hispanic people falling between those of black people and white people. Support among black people, unlike among white people, had almost no correlation with political affiliation.\nA Quinnipiac poll from June 2009 found that 55% of Americans felt that affirmative action, in general, should be discontinued, though 55% supported it for people with disabilities. The Quinnipiac University Polling Institute survey found 65% of American voters opposed the application of affirmative action to homosexuals, with 27% indicating they supported it.\nA Leger poll taken in 2010 found 59% of Canadians opposed considering race, gender, or ethnicity when hiring for government jobs.\nA 2014 Pew Research Center poll found that 63% of Americans thought affirmative action programs aimed at increasing minority representation on college campuses were \"a good thing\", compared to 30% who thought they were \"a bad thing\". The following year, Gallup released a poll showing that 67% of Americans supported affirmative action programs aimed at increasing female representation, compared to 58% who supported such programs aimed at increasing the representation of racial minorities.\nA 2019 Pew Research Center poll found 73% of Americans believe race or ethnicity should not factor into college admissions decisions. A few years later in 2022, a Pew Research Center poll found that 74% of Americans believe race or ethnicity should not factor into college admissions decisions.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49393", "revid": "173030", "url": "https://en.wikipedia.org/wiki?curid=49393", "title": "Office of National Assessments", "text": "Australian former intelligence agency\nThe Office of National Assessments (ONA) was an Australian statutory intelligence agency established by the \"Office of National Assessments Act 1977\" as an independent statutory body directly accountable to the Prime Minister of Australia as a portfolio agency of the Department of the Prime Minister and Cabinet. ONA provided all-source assessments on international political, strategic and economic developments to the Prime Minister and the National Security Committee of Cabinet. ONA also played a coordination role in the Australian Intelligence Community through evaluating foreign intelligence products, convening the National Intelligence Coordination Committee, and developing relationships with intelligence agencies around the world.\nONA was regarded as the Australian equivalent of the United Kingdom Joint Intelligence Organisation, a combination of the United States Office of the Director of National Intelligence and United States Department of State Bureau of Intelligence and Research, and the New Zealand National Assessments Bureau.\nOn 18 July 2017, Prime Minister Malcolm Turnbull announced the creation of the Office of National Intelligence in line with recommendations from the 2017 Independent Review of the Australian Intelligence Community led by Michael L'Estrange and Stephen Merchant. ONI was formally stood up on 20 December 2018.\nHistory.\nThe origins of ONA stem from recommendations of the Royal Commission on Intelligence and Security (also known as the First Hope Commission) which was established on 21 August 1974 by Australia's Prime Minister Gough Whitlam and led by Justice Robert Hope, for the formation of an independent agency to provide intelligence assessments on political, strategic and economic issues directly to the Prime Minister. The Commission reported in 1977 to the Australian Government led by Malcolm Fraser, and four of its eight reports were tabled in Parliament.\nThe ONA was established under the \"Office of National Assessments Act 1977\", which ensured ONA's statutory independence from government. ONA began operations on 20 February 1978, assuming the Joint Intelligence Organisation's foreign intelligence assessment role. The Joint Intelligence Organisation retained its defence intelligence assessment role until it was restructured as the Defence Intelligence Organisation in 1990.\nThe formation of the Office of National Intelligence was announced by Prime Minister Malcolm Turnbull on 18 July 2017 in line with recommendations from the 2017 Independent Review of the Australian Intelligence Community led by Michael L'Estrange and Stephen Merchant. The Office of National Intelligence subsumes the Office of National Assessments with an expended role in the strategic development and enterprise management of the National Intelligence Community. On 1 December 2017, Prime Minister Malcolm Turnbull announced Nick Warner, then Director-General of the Australian Secret Intelligence Service and former Secretary of the Department of Defence, to serve as the Director-General of the Office of National Intelligence. ONI was formally stood up on 20 December 2018.\nMedia reporting.\nAlthough not a secret organisation, ONA usually attracts little attention. However, a striking exception occurred in 2001 when Prime Minister John Howard publicly relied upon an ONA assessment to support his claims about asylum seekers on the MV \"Tampa\", in an incident which became known as the \"Tampa affair\". The ONA assessment was later leaked to the public in its entirety, showing that the assessment was ultimately based on nothing more than press releases from various government ministers.\nIn 2003, in the lead-up to the 2003 invasion of Iraq, an ONA intelligence officer named Andrew Wilkie resigned from the agency, citing ethical concerns in relation to selective and exaggerated use of intelligence by the Australian Government on the matter of Iraq and weapons of mass destruction.\nFlood Report.\nONA has experienced substantial growth since the release of the report into intelligence agencies by Philip Flood which recommended a doubling of the agency's budget and staffing resources and formalisation of the agency's role as a coordinator and evaluator of the other Australian foreign intelligence agencies. The only ONA specific recommendation not implemented from the Flood report was the renaming of ONA to the Australian Foreign Intelligence Assessment Agency (AFIAA).\nRole and responsibilities.\nONA is an all-source intelligence assessment agency which reports directly to the Prime Minister. ONA provides assessments on international political, strategic and economic developments and analyses intelligence products of the Australian Intelligence Community. ONA plays a leadership role in the Australian Intelligence Community through coordination and evaluation of Australia's foreign intelligence activities. ONA also collects and analyses open-source intelligence.\nUltimately the ONA assessments are designed to assist the Australian Government in strategic decision making and ensure that government is fully briefed on international developments and emerging threats both in the Indo-Pacific region and around world.\nOrganisational structure.\nThe ONA is divided into analytic branches covering geographic or thematic areas including:\nEnterprise Management Group.\nThe Enterprise Management Group is led by a Deputy Director-General is responsible for interagency and intergovernmental intelligence coordination, integration and engagement functions and the governance and capability development of the Australian Intelligence Community.\nCoordination.\nThe Executive and Foreign Intelligence Coordination Branch (EFIC) supports the coordination of matters of common interest across the Australian Intelligence Community (AIC), in areas such as cross-agency policy, long-term planning and the setting of Australia's foreign intelligence requirements based on the National Intelligence Priorities. EFIC also runs an active foreign relationship program and manages ONA's Cabinet liaison functions. EFIC supports the Department of the Prime Minister and Cabinet and the National Intelligence Coordination Committee.\nWashington Liaison.\nThe ONA Liaison Officer Washington works within the Embassy of Australia, Washington, D.C. and liaises with and develops relationships with the United States Intelligence Community.\nLondon Liaison.\nThe ONA Liaison Officer London works within the High Commission of Australia, London and liaises with and develops relationships with the United Kingdom Intelligence Community.\nAssessments Group.\nThe Assessments Group is led by a Deputy Director-General and is made up of 2 assessments divisions.\nNorth Asia.\nThe North Asia Branch monitors and forecasts political, security, social, and economic developments, issues and trends in East Asia (including China, Japan, and the Korean peninsula) and analyses and assesses all-source intelligence products on North Asia from the Australian Intelligence Community.\nOceania.\nThe Oceania Branch monitors and forecasts political, security, social, and economic developments, issues and trends in Oceania (including Fiji, Papua New Guinea, and the Solomon Islands), and analyses and assesses all-source intelligence products on Oceania from the Australian Intelligence Community.\nOpen Source Centre.\nThe Open Source Centre collects, analyses and researches open-source intelligence to support Australian Government intelligence priorities and the work of the Australian Intelligence Community, with a focus on international developments that affect Australia's national interests.\nStrategic Analysis.\nThe Strategic Analysis Branch monitors and forecasts strategic, military and security developments around the world including military-political affairs, science and technology, cybersecurity, space, weapons of mass destruction, global health, and climate change. The Strategic Analysis Branch works also collaborates with the Defence Intelligence Organisation.\nInternational Economy.\nThe International Economy Branch monitors and forecasts international economic, financial, and trade developments, issues and trends around the world. The International Economy Branch works with the Department of the Treasury.\nTransnational Issues.\nThe Transnational Issues Branch monitors and forecasts transnational issues such as terrorism, illegal drug trade, people smuggling, transnational crime, and human security around the world as well as political, economic, social and strategic developments in Africa and the Americas.\nSouth East Asia.\nThe South East Asia Branch monitors and forecasts political, security, social, and economic developments, issues and trends in Southeast Asia (including Indonesia, Malaysia, Thailand, and the Philippines), and analyses and assesses all-source intelligence products on South East Asia from the Australian Intelligence Community.\nSouth Asia and Middle East.\nThe South Asia and Middle East Branch monitors and forecasts political, security, social, and economic developments, issues and trends in South Asia (including India, Pakistan, and Afghanistan) and the Middle East (including Iran, Iraq, Syria, Israel, and Saudi Arabia) and analyses and assesses all-source intelligence products on South Asia and Middle East from the Australian Intelligence Community.\nChief Operating Officer Division.\nThe Chief Operating Officer Division provides support for human resources, internal security, facilities management, business management, information technology, information management, report production, and administration.\nDirectors-General.\nThe Director-General of ONA is an independent statutory officer who is not subject to external direction on the content of ONA assessments. ONA has about 150 staff, including 100 analysts. The previous Director-General of ONA was Richard Maude, who was appointed to a five-year term at the agency in April\u00a02013. Maude took leave from ONA in 2017 to prepare the White Paper on Foreign Policy, with Bruce Miller acting in the role. In December 2017, former ASIS director-general Nick Warner was appointed.\nHeadquarters.\nIn October 2011, ONA moved into the Robert Marsden Hope Building, a refurbished building in the Parliamentary Triangle. The building is named for Justice Hope, who led two Royal Commissions into Australia's intelligence and security agencies and operations, the first of which led to the creation of ONA. Before its move, ONA had been a sub-tenant in the Central Office building of the Australian Security Intelligence Organisation (ASIO) in Russell, Canberra.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49396", "revid": "46784266", "url": "https://en.wikipedia.org/wiki?curid=49396", "title": "Volunteer (botany)", "text": "Plant growing on its own\nIn gardening and agronomic terminology, a volunteer is a plant that grows on its own, rather than being deliberately planted by a farmer or gardener. The action of such plants \u2014 to sprout or grow in this fashion \u2014 may also be described as volunteering.\nBackground.\nVolunteers often grow from seeds that float in on the wind, are dropped by birds, or are inadvertently mixed into compost. Some volunteers may be encouraged by gardeners once they appear, being watered, fertilized, or otherwise cared for, unlike weeds, which are unwanted volunteers. \nVolunteers that grow from the seeds of specific cultivars are not reliably identical or similar to their parent and often differ significantly from it. Such open pollinated plants, if they show desirable characteristics, may be selected to become new cultivars.\nAgriculture.\nIn agricultural rotations, self-set plants from the previous year's crop may become established as weeds in the current crop. For example, volunteer winter wheat will germinate to quite high levels in a following oilseed rape crop, usually requiring chemical control measures. \nIn agricultural research, the high purity of a harvested crop is often desirable. To achieve this, typically a group of temporary workers will walk the crop rows looking for volunteer plants, or \"rogue\" plants in an exercise typically referred to as \"roguing\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49397", "revid": "44555953", "url": "https://en.wikipedia.org/wiki?curid=49397", "title": "Battle of Chosin Reservoir", "text": "1950 Korean War battle\nThe Battle of Chosin Reservoir, also known as the Chosin Reservoir Campaign or the Battle of Lake Changjin (Korean:\u00a0; Hanja:\u00a0; RR:\u00a0; MR:\u00a0), was an important battle in the Korean War. The name \"Chosin\" is derived from the Japanese pronunciation \"\"Ch\u014dshin\"\", instead of the Korean pronunciation.\nThe battle took place about a month after the People's Republic of China entered the conflict and sent the People's Volunteer Army (PVA) 9th Corps to infiltrate the northeastern part of North Korea. On 27 November 1950, the Chinese force surprised the US X\u00a0Corps commanded by Major General Edward Almond in the Chosin Reservoir area. A brutal 17-day battle in freezing weather soon followed. Between 27 November and 13 December, 30,000 United Nations Command troops, later nicknamed \"The Chosin Few\", under the field command of Major General Oliver P. Smith were encircled and attacked by about 120,000 Chinese troops under the command of Song Shilun, who had been ordered by Mao Zedong to destroy the UN forces.\nThe UN forces were nevertheless able to break out of the encirclement and withdraw to the port of Hungnam in what U.S. historians described as the \"greatest evacuation movement by sea in U.S. military history.\" Both sides suffered heavy casualties, with both battle casualties and non-battle casualties caused by the frigid weather. The withdrawal of the US Eighth Army from northwest Korea and its recovery by Chinese forces in the aftermath of the Battle of the Ch'ongch'on River and the evacuation of the X\u00a0Corps from the port of Hungnam in northeast Korea marked the complete withdrawal of UN troops from North Korea.\nBackground.\nBy mid-October 1950, after the successful landing at Inchon by the US X\u00a0Corps, the Eighth Army breakout from the Pusan Perimeter and the subsequent pursuit and destruction of the Korean People's Army (KPA), the Korean War appeared to be all but over. United Nations (UN) forces advanced rapidly into North Korea with the intention of reuniting North and South Korea before the end of 1950. North Korea is divided through the center by the impassable Taebaek Mountains, which separated the UN forces into two groups. The US Eighth Army advanced north through the western coast of the Korean Peninsula, while the Republic of Korea (ROK) I Corps and the US X\u00a0Corps advanced north on the eastern coast.\nAt the same time the People's Republic of China entered the conflict after issuing several warnings to the United Nations. On 19 October 1950, large formations of Chinese troops, dubbed the People's Volunteer Army (PVA), secretly crossed the border and into North Korea. One of the first Chinese units to reach the Chosin Reservoir area was the PVA 42nd Army, which was tasked with stopping the eastern UN advances. On 25 October, the advancing ROK I Corps made contact with the Chinese and halted at Funchilin Pass (), south of the Chosin Reservoir. \nAfter the landing at Wonsan, the US 1st Marine Division of the X\u00a0Corps engaged the defending PVA 124th Division on 2 November; the ensuing battle caused heavy casualties among the Chinese. On 6 November, the PVA 42nd Corps ordered a retreat to the north with the intention of luring the UN forces into the Chosin Reservoir. By 24 November, the 1st Marine Division occupied both Sinhung-ni () on the eastern side of the reservoir and Yudami-ni () on the west side.\nFaced with the sudden attacks by Chinese forces in the Eighth Army sector, General Douglas MacArthur ordered the Eighth Army to launch the Home-by-Christmas Offensive. To support the offensive, MacArthur ordered the X\u00a0Corps to attack west from the Chosin Reservoir and to cut the vital Manpojin\u2014Kanggye\u2014Huichon supply line. In response, Major General Edward M. Almond, commander of the US X\u00a0Corps, formulated a plan on 21 November. It called for the US 1st Marine Division to advance west through Yudami-ni, while the US 7th Infantry Division would provide a regimental combat team to protect the right flank at Sinhung-ni. The US 3rd Infantry Division would also protect the left flank, while providing security in the rear area. By then the X\u00a0Corps was stretched thin along a 400-mile front.\nSurprised by the Marine landing at Wonsan, Chinese Communist Party chairman Mao Zedong called for the immediate destruction of the ROK Capital Division, ROK 3rd Infantry Division, US 1st Marine Division, and US 7th Infantry Division in a telegram to Commander Song Shilun of the PVA 9th Corps on 31 October. Under Mao's urgent orders, the 9th Corps was rushed into North Korea on 10 November. Undetected by UN intelligence, it entered the Chosin Reservoir area on 17 November, with the 20th Army of the 9th Corps relieving the 42nd Army near Yudami-ni.\nPrelude.\nLocation, terrain and weather.\nChosin Reservoir is a man-made lake located in the northeast of the Korean peninsula. The name Chosin is the Japanese pronunciation of the Korean place name Changjin, and the name stuck due to the outdated Japanese maps used by UN forces. The battle's main focus was around the road that connects Hungnam and Chosin Reservoir, which served as the only retreat route for the UN forces. Through these roads, Yudami-ni and Sinhung-ni, located at the west and east side of the reservoir respectively, are connected at Hagaru-ri (now Changjin-\u016dp) (). From there, the road passes through Koto-ri () and eventually leads to the port of Hungnam. The area around the Chosin Reservoir was sparsely populated.\nThe battle was fought over some of the roughest terrain during some of the harshest winter weather conditions of the Korean War. The road was created by cutting through the hilly terrain of Korea, with steep climbs and drops. Dominant peaks, such as the Funchilin Pass and the Toktong Pass (), overlook the entire length of the road. The road's quality was poor, and in some places it was reduced to a one-lane gravel trail. On 14 November 1950, a cold front from Siberia descended over the Chosin Reservoir, and the temperature plunged, according to estimates, to as low as . The cold weather created considerable danger of frostbite casualties and was accompanied by frozen ground, icy roads, and weapon malfunctions. Medical supplies froze; morphine syrettes had to be defrosted in a medic's mouth before they could be injected; blood plasma was frozen and useless on the battlefield. Even cutting off clothing to deal with a wound risked gangrene and frostbite. Batteries used for the Jeeps and radios did not function properly in the temperature and quickly ran down. The lubrication in the guns gelled and rendered them useless in battle. Likewise, the springs on the firing pins would not strike hard enough to fire the round, or would jam.\nForces and strategies.\nAlthough the 1st Marine Division landed at Wonsan as part of Almond's US X\u00a0Corps, Almond and Major General Oliver P. Smith of the 1st Marine Division shared a mutual loathing of each other that dated back to a meeting before the landing at Inchon, when Almond had spoken of how easy amphibious landings are even though he had never been involved in one. Smith believed there were large numbers of Chinese forces in North Korea despite the fact that higher headquarters in Tokyo said otherwise, but Almond felt Smith was overly cautious. The mutual distrust between the commanders caused Smith to slow the 1st Marine Division's advance towards the Chosin Reservoir in violation of Almond's instructions. Smith established supply points and airfields along the way at Hagaru-ri and Koto-ri.\nAs the US X\u00a0Corps was pushing towards the reservoir, the Chinese formulated their strategy, based on their experiences in the Chinese Civil War. Working from the assumption that only a light UN presence would be at the reservoir, the Chinese 9th Corps was first to destroy the UN garrisons at Yudami-ni and Sinhung-ni, then push towards Hagaru-ri. Believing the bulk of the US X\u00a0Corps would move to rescue the destroyed units, the 9th Corps would then block and trap the main UN forces on the road between Hagaru-ri and Hungnam. The 9th Corps initially committed eight divisions for the battle, with most of the forces concentrated at Yudami-ni and Sinhung-ni.\nThe flaw in the Chinese plan was a lack of accurate intelligence about the UN forces. Even though the US X\u00a0Corps was stretched thin over northeast Korea, the slow Marine advance allowed the bulk of the US 1st Marine Division, including the 5th, 7th and 11th Marines, to be concentrated at Yudami-ni. Furthermore, the strategically important Hagaru-ri, where a C-47-capable airfield was under construction and a supply dump, was not a priority for the Chinese despite being lightly defended by the 1st and 7th Marines. Only Regimental Combat Team 31 (RCT-31), an understrength and hastily formed regimental combat team of the US 7th Infantry Division, was thinly spread along the eastern bank of the reservoir. Those units later took the brunt of the Chinese assaults.\nAs for the UN forces, the 1st Marine Division had an effective strength of 25,473 men at the start of the battle, and it was further reinforced by the British Royal Marines unit 41 (Independent) Commando and the equivalent of two regiments from the 3rd and 7th Army Infantry Divisions. The UN forces had a combined strength of about 30,000 men during the course of the battle. The UN forces at Chosin were also supported by one of the greatest concentrations of air power during the Korean War, since the 1st Marine Aircraft Wing stationed at Yonpo Airfield and five aircraft carriers from the US Navy's Task Force 77 were able to launch 230 sorties daily to provide close air support during the battle, while the US Air Force Far East Combat Cargo Command in Japan reached the capacity of airdropping 250 tons of supplies per day to resupply the trapped UN forces.\nAlthough the 9th Corps was one of China's elite formations, composed of veterans and former POWs from the Huaihai Campaign, several deficiencies hampered its ability during the battle. Initially the 9th Corps was intended to be outfitted in Manchuria during November, but Mao suddenly ordered it into Korea before that could happen. As a result, the 9th Corps had almost no winter clothing for the harsh Korean winter. Similarly, poor logistics forced the 9th Corps to abandon heavy artillery, while working with little food and ammunition. The food shortage forced the 9th Corps to initially station a third of its strength away from the Chosin Reservoir in reserve, and starvation and exposure weakened the Chinese units, since foraging was not an option in the sparsely populated area. By the end of the battle, more Chinese troops had died from the cold than from combat and air raids.\nChinese strength is usually estimated at 120,000 troops for the battle. Before arriving in Korea, the 9th Corps was also reinforced. Each of its three corps had four divisions instead of the regular three; thus it had 12 divisions, with 10,000 men per division. Infantry from two formerly \"liberated\" (surrendered) Nationalist divisions were absorbed to bring each infantry company up to strength. Some companies had approximately 150 men, while others were reinforced with more than 200 men. However, attrition due to UN air raids, poor logistics and cold weather had also taken a toll on the way to the battlefield. On the day 9th Corps entered Korea, for example, frostbite inflicted 700 casualties, while most of its transport vehicles were destroyed by UN air raids. During the course of the battle, Chinese prisoners of war reported that most of the 9th Corps\u2019 divisions had become under strength, numbering about 6,500 to 7,000 men per division. These factors, plus uncertainties over the Chinese order of battle in Western sources, led some historians to revise Chinese numbers down to as low as 60,000 during the course of battle.\nEventually, all 12 Chinese divisions of the 9th Corps were deployed, although the 78th and the 88th Divisions of the PVA 26th Army did not make contact with UN forces during the course of the battle. Eight divisions of the PVA 20th and 27th Armies served as the main attacking force. Four divisions of the PVA 26th Army initially were held back in reserve, and deployed after 20th and 27th Armies had exhausted all their available strength.\nBattle.\nOn the night of 27 November, the PVA 20th and 27th Armies of the 9th Corps launched multiple attacks and ambushes along the road between the Chosin Reservoir and Kot'o-ri. At Yudam-ni, the 5th, 7th and 11th Marines were surrounded and attacked by the PVA 79th and 89th Divisions, with the 59th Division attacking the road between Yudam-ni and Hagaru-ri to cut off communication. Similarly, RCT-31 was isolated and ambushed at Sinhung-ni by the PVA 80th and 81st Divisions. At Hagaru-ri, the 1st Marine Division command headquarters was targeted by the PVA 58th Division. Finally, the PVA 60th Division surrounded elements of the 1st Marines at Kot'o-ri from the north. Caught by complete surprise, the UN forces were cut off at Yudam-ni, Sinhung-ni, Hagaru-ri and Kot'o-ri by 28 November.\nActions at Yudam-ni.\nActing on Almond's order, Smith ordered the 5th Marines to attack west toward Mupyong-ni on 27 November. The attack was soon stalled by the PVA 89th Division and forced the Marines to dig in on the ridges surrounding Yudam-ni. As night came, three Chinese regiments of the 79th Division attacked the ridges on the north and northwest of Yudam-ni, hoping to annihilate the garrison in one stroke. Close quarters fighting soon developed as the attackers infiltrated Marine positions, but the 5th and 7th Marines held the line while inflicting heavy casualties on the Chinese. As day broke on 28 November, the Chinese and Americans were locked in a stalemate around the Yudam-ni perimeter.\nWhile the battle was underway at Yudam-ni, the PVA 59th Division blocked the road between Yudam-ni and Hagaru-ri by attacking the defending Charlie and Fox Companies of the 7th Marines. The successful assault forced Charlie Company to retreat into Yudam-ni, which left Fox Company, commanded by Captain William E. Barber, isolated on a hill overlooking the Toktong Pass, a vital pass that controlled the road. On 29 November, several efforts by the 7th Marines failed to rescue Fox Company, despite inflicting heavy casualties on the Chinese. Aided by artillery from Hagaru-ri and Marine Corsair fighters, Fox Company managed to hold out for five days while enduring constant attacks by the PVA 59th Division.\nAfter the heavy losses suffered by the PVA 79th Division at Yudam-ni, 9th Corps headquarters realized that the bulk of the 1st Marine Division was stationed at Yudam-ni, with a garrison strength double the initial estimate. Believing that any further assaults would be futile, Song Shilun ordered the 9th Corps to switch their main attacks toward Sinhung-ni and Hagaru-ri, leaving Yudam-ni alone from 28 to 30 November. At the same time, the US Eighth Army on the Korean western front was forced into full retreat at the Battle of the Ch'ongch'on River, and MacArthur ordered Almond to withdraw the US X\u00a0Corps to the port of Hungnam. Acting on Almond and Smith's instructions, Lieutenant Colonel Raymond L. Murray and Colonel Homer L. Litzenberg, commanders of the 5th and 7th Marines, respectively, issued a joint order to break out from Yudam-ni to Hagaru-ri on 30 November. Faced with tough fighting between the blocking Chinese divisions and the withdrawing Marines, Smith remarked: \"Retreat, hell! We're not retreating, we're just advancing in a different direction.\"\nFor the breakout, the Marines formed into a convoy with a single M4A3 Sherman tank in the lead. The plan was to have 3rd Battalion, 5th Marines (3/5) as the vanguard of the convoy, with three battalions covering the rear. At the same time, 1st Battalion, 7th Marines (1/7) would attack towards Fox Company in order to open the road at Toktong Pass. To start the breakout, 3rd Battalion, 7th Marines (3/7) had to first attack south and capture Hills 1542 and 1419 in order to cover the road from Chinese attacks. The breakout was carried out under the air cover of the 1st Marine Air Wing.\nOn the morning of 1 December, 3/7 Marines engaged the PVA 175th Regiment of the 59th Division at Hills 1542 () and 1419 (). The Chinese defenders soon forced the Marines to dig in on the slopes between the road and the peaks when the convoy passed 3/7's position by the afternoon. With Hagaru-ri still not captured, the PVA High Command scrambled the 79th Division to resume attacks on Yudam-ni, while the 89th Division rushed south towards Kot'o-ri. The Chinese struck at night, and the ferocity of the fighting forced the rear covering forces to call in night fighters to suppress the attacks. The fighting lasted well into the morning of 2 December until all the Marines had managed to withdraw from Yudam-ni.\nAt the same time, 1/7 Marines also tried to break the Chinese blockade at Hill 1419 on 1 December. Despite being badly reduced by combat, hunger and frostbite, the PVA 59th Division sent in its last five platoons and refused to yield. As night approached, 1/7 finally captured the peak and started to march through the hills on the east side of the road. Relying on the element of surprise, they managed to destroy several Chinese positions along the road. On the morning of 2 December, a joint attack by Fox Company and 1/7 secured the Toktong Pass, thus opening the road between Yudam-ni and Hagaru-ri.\nAlthough the road had been opened between Yudam-ni and Hagaru-ri, the convoy still had to fight through the numerous Chinese positions on the hills overlooking the road. On the first night of the retreat, the Chinese struck the convoy in force and inflicted heavy casualties on 3/5 Marines. Although strong air cover suppressed most of the Chinese forces for the rest of the march, the cold weather, harassing fire, raiding parties, and roadblocks slowed the retreat to a crawl, while inflicting numerous casualties. Despite those difficulties, the convoy reached Hagaru-ri in an orderly fashion on the afternoon of 3 December, with the withdrawal completed on 4 December.\nEast of the reservoir.\nRCT-31, later known as \"Task Force Faith,\" was a hastily formed regimental combat team from the 7th Infantry Division that guarded the right flank of the Marine advance towards Mupyong-ni. Before the battle, RCT-31 was spread thin, with main elements separated on the hills north of Sinhung-ni, the Pyungnyuri Inlet west of Sinhung-ni, and the town of Hudong-ni () south of Sinhung-ni. Although the Chinese believed RCT-31 to be a reinforced regiment, the task force was actually understrength, with one battalion missing, due to the bulk of the 7th Infantry Division being scattered over northeast Korea.\nOn the night of 27 November, three regiments from the 80th Division attacked the northern hills () and the inlet, completely surprising the defenders. The 1st Battalion, 32nd Infantry, to the north of Sinhung-ni suffered heavy casualties, while the 57th Field Artillery Battalion and the 3rd Battalion, 31st Infantry, were almost overrun at Pyungnyuri Inlet. The Chinese also sent the 242nd Regiment of the 81st Division towards Hill 1221, (), an undefended hill that controlled the road between Sinhung-ni and Hudong-ni. As the night's fighting ended, RCT-31 was split into three elements.\nBelieving that the defenders had been completely destroyed at the inlet, the Chinese stopped their attacks and proceeded to loot the American positions for food and clothing. As morning came on 28 November, the 3/31st Infantry counterattacked the PVA 239th Regiment at the inlet, driving the surprised Chinese back in a complete rout. In the afternoon, Almond flew into the Sinhung-ni perimeter of RCT-31, convinced that RCT-31 was strong enough to begin its attack north and deal with whatever \"remnants\" of Chinese forces were in their way. Almond ordered Colonel Allan D. Maclean, the commander of RCT-31, to resume the offensive north, while presenting Silver Stars to three of Maclean's officers. In disgust, Lieutenant Colonel Don C. Faith Jr., the commander of the 1/32nd Infantry, threw his medal into the snow.\nOn the night of 28 November, the PVA 80th Division attacked again with four regiments. At the inlet, the Chinese assault became a disaster as communications broke down, while devastating fire from the M16 and M19 anti-aircraft (AA) guns attached to the 57th Field Artillery Battalion swept the Chinese ranks. In the aftermath, the PVA 238th and the 239th Regiment together had fewer than 600 soldiers. The attacks by PVA 240th Regiment, on the other hand, forced Maclean to order a retreat from the northern hills towards Sinhung-ni. On 29 November, the 1st Battalion managed to break through the Chinese blockade and reached the Sinhung-ni perimeter, but Maclean was lost when he mistook some Chinese soldiers for American. The Chinese finally stopped their attacks on the night of 29 November, while waiting for fresh reinforcements.\nWhile RCT-31 was under siege, Almond finally instructed the 1st Marine Division to rescue it by breaking out of Yudam-ni, an impossible order for Smith to implement. Only the 31st Tank Company tried to rescue RCT-31, by attacking Hill 1221 from Hudong-ni, but without infantry support, the two armored attacks on 28 and 29 November were stalled by slippery roads, rough terrain, and close infantry assaults. By 30 November, the US forces evacuated Hudong-ni in order to defend Hagaru-ri, leaving the rest of RCT-31 completely isolated.\nOn 30 November, Major General David G. Barr, the commander of the 7th Infantry Division, flew into Sinhung-ni and met with Faith, who by now had assumed command of RCT-31. Faith laid out the difficulties of a breakout, particularly the 500 wounded that RCT-31 had to bring along. On the same day, parts of the PVA 94th Division and the rest of the 81st Division arrived as reinforcements for the 80th Division. By midnight, six Chinese regiments renewed their attacks, and Zhan Danan, the commander of the 80th Division, ordered the complete destruction of RCT-31 before dawn. Again, the 57th Battalion's AA guns held the Chinese at bay, but supplies of shells were desperately low. On the day of 1 December, Faith finally ordered RCT-31 to break out from Sinhung-ni and withdraw to Hagaru-ri.\nThe breakout began as soon as the weather allowed the 1st Marine Aircraft Wing to provide air cover on 1 December. As the soldiers formed a convoy and tried to leave the Sudong-ni perimeter, the PVA 241st Regiment immediately swarmed over the American forces, with three other regiments closing in. Left with no choice, the covering aircraft dropped napalm right in front of RCT-31, inflicting casualties on both American and Chinese troops. The resulting firestorm wiped out the blocking Chinese company, allowing the convoy to advance. As the front of RCT-31 made its way forward, heavy small arms fire caused many members of the rear guard to seek shelter below the road instead of protecting the trucks. Chinese fire also killed or wounded those already in the trucks as well as the drivers, who viewed the job as a form of suicide. Slowly, the convoy approached a roadblock under Hill 1221 in the late afternoon. Several parties tried to clear Hill 1221, but after taking part of the hill, the leaderless soldiers continued out onto the frozen reservoir instead of returning to the column. As Faith led an assault on the roadblock, he was hit by a Chinese grenade and subsequently died of his wounds. The convoy managed to fight past the first roadblock, but as it reached the second at Hudong-ni, RCT-31 disintegrated under Chinese attacks. Out of the original 2,500 soldiers, about 1,050 managed to reach Hagaru-ri, and only 385 survivors were deemed able-bodied. The remnants of RCT-31 were formed into a provisional army battalion for the rest of the battle.\nActions at Hagaru-ri.\nTo support the Marine attack towards Mupyong-ni, Hagaru-ri became an important supply dump with an airfield under construction. Smith and 1st Marine Division headquarters were also located at Hagaru-ri. With the bulk of the 1st Marine Division gathered at Yudam-ni, Hagaru-ri was lightly defended by two battalions from the 1st and 7th Marines, the rest of the garrison being composed of engineers and rear support units from both the Army and the Marine Corps.\nThe original Chinese plan called for the 58th Division to attack Hagaru-ri on the night of 27 November, but the division became lost in the countryside due to the outdated Japanese maps it used. It was not until the dawn of 28 November that the 58th Division arrived at Hagaru-ri. Meanwhile, from the fighting and ambushes that had occurred the previous night, the garrison at Hagaru-ri noticed the Chinese forces around them. Lieutenant Colonel Thomas L. Ridge, commander of 3rd Battalion, 1st Marines (3/1), predicted the Chinese attack would come on the night of 28 November. Almost everyone, including rear support units with little combat training, was pressed into service on the front line due to the manpower shortage, and the entire perimeter was on full alert by 21:30.\nIt was not long before the PVA 173rd Regiment attacked the western and the southern perimeter, while the 172nd Regiment struck the hills on the northern perimeter. Despite the preparations, the understrength garrison was overwhelmed, with the Chinese opening several gaps in the defenses and reaching the rear areas. The resulting chaos, however, caused a breakdown in discipline among the Chinese soldiers, who began looting food and clothing instead of exploiting the situation. The defending Americans managed to destroy the Chinese forces in counterattacks, while a breakdown of communications between the Chinese regiments allowed the gaps to close. When the fighting stopped, the Chinese had only gained the East Hill () on the northern perimeter. Another attack was planned for the night of 29 November, but air raids by VMF-542 broke up the Chinese formations before it could be carried out.\nGiven the critical manpower shortage at Hagaru-ri, on 29 November, Smith ordered Colonel Lewis \"Chesty\" Puller of the First Marine Regiment to assemble a task force to be sent north from Kot'o-ri to open the road south of Hagaru-ri. A task force was formed with 921 troops from 41 (Royal Marine) Commando, G Company of the 1st Marines and B Company of the 31st Infantry. It was dubbed \"Task Force Drysdale\" after its commander, Lieutenant Colonel Douglas B. Drysdale, who also commanded 41 Commando.\nOn the afternoon of 29 November, Task Force Drysdale pushed north from Koto-ri, while under constant attack from the PVA 60th Division. The task force's harrowing experience later earned the road the nickname \"Hell Fire Valley\". As the Chinese attacks dragged on, the task force became disorganized, and a destroyed truck in the convoy later split the task force into two segments. Although the lead segment fought its way into Hagaru-ri on the night of 29 November, the rear segment was destroyed. Despite suffering 162 dead and missing and 159 wounded, the task force managed to bring in 300 badly needed infantrymen for the defense at Hagaru-ri.\nAs more reinforcements arrived from Hudong-ni on 30 November, the garrisons attempted to recapture the East Hill. All efforts failed, despite the destruction of a Chinese company. When darkness settled, the PVA 58th Division gathered its remaining 1,500 soldiers in a last-ditch attempt to capture Hagaru-ri. The reinforced defenders annihilated most of the attacking forces, with only the defences around the East Hill giving way. As the Chinese tried to advance from the East Hill, they were cut down by the 31st Tank Company.\nBy 1 December, the PVA 58th Division was virtually destroyed, with the remainder waiting for reinforcements from the 26th Army of the 9th Corps. Much to the frustration of Song Shilun, the 26th Corps did not arrive before the Marines broke out of Yudam-ni. The airfield was opened to traffic on 1 December, allowing UN forces to bring in reinforcements and to evacuate the dead and wounded. With the Marines at Yudam-ni completing their withdrawal on 4 December, the trapped UN forces could finally start their breakout towards the port of Hungnam.\nBreakout.\nAfter a short rest, the breakout began on 6 December, with the 7th Marines as the vanguard of the retreating column, while the 5th Marines covered the rear. At the same time, the much-delayed PVA 26th Army arrived at Hagaru-ri with its 76th and 77th Divisions to relieve the 58th and 60th Divisions. As the 7th Marines pushed aside the PVA 76th Division south of Hagaru-ri, the 5th Marines took over the Hagaru-ri perimeter and recaptured the East Hill from the 76th Division. In a last effort to stop the breakout, the customary Chinese night attack was launched, with the 76th and 77th Division striking the Hagaru-ri perimeter from all directions. The Marines repulsed the attacks, inflicting heavy casualties.\nMeanwhile, the 7th Marines opened the road between Hagaru-ri and Koto-ri by capturing the high ground surrounding the road. But as soon as the Marines pulled out, the 77th Division returned to the peaks and attacked the column. Chaotic fighting broke out and the retreat was slowed to a crawl. The Marine night fighters returned to subdue the Chinese forces, and most of the blocking troops were eliminated. On 7 December, the rest of the column managed to reach Kot'o-ri with little difficulty, with the last elements reaching Kot'o-ri that night.\nAfter the failure of the 26th Corps at Hagaru-ri, the PVA High Command ordered the 26th and 27th Corps to chase the escaping UN force, with the 20th Corps assigned to block the escape route. But with most of the 20th Corps destroyed at Yudam-ni and Hagaru-ri, the only forces between Kot'o-ri and Hungnam were the remnants of the 58th and 60th Divisions. In desperation, Song Shilun ordered these troops to dig in at Funchilin Pass, while blowing up the vital bridge (), hoping the terrain and obstacles would allow the 26th and 27th Corps to catch up with the retreating UN forces.\nThe PVA 180th Regiment that occupied Hill 1081 () blew up the original concrete bridge and two improvised replacements in succession, believing the bridge was rendered irreparable. In response, the 1st Battalion, 1st Marines (1/1) attacked Hill 1081 from the south, and the hill was captured on 9 December, though the defenders fought to the last man. At the same time, the 7th Marines and RCT-31 attacked the bridge from the north, only to encounter defenders who were already frozen in their foxholes.\nWith the path to Hungnam blocked at Funchilin Pass, eight C-119 Flying Boxcars flown by the US 314th Troop Carrier Wing were used to drop portable bridge sections by parachute. The bridge, consisting of eight separate long, sections, was dropped one section at a time, using a parachute on each section. Four of these sections, together with additional wooden extensions were successfully reassembled into a replacement bridge by Marine Corps combat engineers and the US Army 58th Engineer Treadway Bridge Company on 9 December, enabling UN forces to proceed. Outmaneuvered, the PVA 58th and 60th Divisions still tried to slow the UN advance with ambushes and raids, but after weeks of non-stop fighting, the two Chinese divisions combined had only 200 soldiers left. The last UN forces left Funchilin Pass by 11 December.\nOne of the last engagements during the withdrawal was an ambush at Sudong () by the pursuing PVA 89th Division, which Task Force Dog of the 3rd Infantry Division repulsed with little difficulty. The trapped UN forces finally reached the Hungnam perimeter by 21:00 on 11 December.\nEvacuation at Hungnam.\nBy the time the UN forces arrived at Hungnam, MacArthur had already ordered the evacuation of the US X\u00a0Corps on 8 December in order to reinforce the US Eighth Army, which by then was badly depleted and retreating rapidly towards the 38th parallel. Following his orders, the ROK I Corps, the ROK 1st Marine Regiment and the US 3rd and 7th Infantry Divisions had also set up defensive positions around the port. Some skirmishes broke out between the defending US 7th, 17th and 65th Infantry and the pursuing PVA 27th Corps, but against the strong naval gun fire support provided by US Navy Task Force 90, the badly mauled 9th Corps was in no shape to approach the Hungnam perimeter.\nIn what US historians called the \"greatest evacuation movement by sea in US military history\", a 193-ship armada assembled at the port and evacuated not only the UN troops, but also their heavy equipment and roughly a third of the Korean refugees. One Victory ship, the , evacuated 14,000 refugees by herself, despite being designed to carry only 12 passengers. The last UN unit left at 14:36 on 24 December, and the port was destroyed to deny its use to the Chinese. The PVA 27th Army entered Hungnam on the morning of 25 December.\nAftermath.\nCasualties.\nThe US X\u00a0Corps and the ROK I Corps reported a total of 10,495 battle casualties: 4,385 US Marines, 3,163 US Army personnel, 2,812 South Koreans attached to American formations and 78 British Royal Marines. The 1st Marine Division also reported 7,338 non-battle casualties due to the cold weather, adding up to a total of 17,833 casualties. Despite the losses, the US X\u00a0Corps preserved much of its strength. About 105,000 soldiers, 98,000 civilians, 17,500 vehicles, and 350,000 tons of supplies were shipped from Hungnam to Pusan, and they later rejoined the war effort in Korea. Commanding General Smith was credited for saving the US X\u00a0Corps from destruction, while the 1st Marine Division, 41 (Royal Marine) Commando and RCT-31 were awarded the Presidential Unit Citation for their tenacity during the battle. Fourteen Marines, two soldiers and one Navy pilot received the Medal of Honor, and all of the UN troops that served at Chosin were later nicknamed \"The Chosin Few\".\nAccording to official estimates by the People's Liberation Army General Logistics Department published in 1988, the PVA 9th Corps suffered 21,366 combat casualties, including 7,304 killed. In addition, 30,732 non-combat casualties were attributed to the harsh Korean winter and lack of food. Total casualties thus amounted to 52,098 - more than one third of its total strength. Outside of official channels, the estimation of Chinese casualties has been described as high as 60,000 by Patrick C. Roe, the chairman of Chosin Few Historical Committee, citing the number of replacements requested by 9th Corps in the aftermath of the battle. Regardless of the varying estimates, historian Yan Xue of PLA National Defence University noted that the 9th Corps was put out of action for three months. With the absence of 9th Corps the Chinese order of battle in Korea was reduced to 18 infantry divisions by 31 December 1950, as opposed to the 30 infantry divisions present on 16 November 1950.\nOperation Glory.\nDuring the battle, UN dead were buried at temporary grave sites along the road. Operation Glory took place from July to November 1954, during which the dead of each side were exchanged. The remains of 4,167 US soldiers were exchanged for 13,528 North Korean and Chinese dead. In addition, 546 civilians who died in UN prisoner-of-war camps were turned over to the South Korean government. After Operation Glory, 416 Korean War \"unknowns\" were buried in the National Memorial Cemetery of the Pacific, the \"Punchbowl Cemetery\" in Honolulu, Hawaii. According to a Defense Prisoner of War/Missing Personnel Office (DPMO) white paper, 1,394 names were also transmitted from the Chinese and North Koreans during the operation, of which 858 proved to be correct.\nThe 4,167 returned remains were found to be 4,219 individuals, of whom 2,944 were found to be Americans, with all but 416 identified by name. Of the 239 Korean War unaccounted for, 186 are not associated with the Punchbowl Cemetery unknowns. From 1990 to 1994, North Korea excavated and returned more than 208 sets of remains, which possibly include 200 to 400 US servicemen, but very few have been identified due to the co-mingling of remains. From 2001 to 2005, more remains were recovered from the Chosin Battle site, and around 220 were recovered near the Chinese border between 1996 and 2006.\nOutcome assessment.\nRoy E. Appleman, the author of US Army official history \"South to Naktong, North to Yalu\", writes that both sides could claim victory: the PVA 9th Corps ultimately held the battlefield, while X\u00a0Corps held off the PVA 9th Corps in a series of battles that enabled it to withdraw most of its forces as an effective tactical unit. Writing for National Public Radio, Anthony Kuhn said that both sides have remembered the battle in \"starkly differing\" ways: for the United States, it won because its forces broke out of their encirclement largely intact and \"inflicted heavy losses on the Chinese\" while for China, it won because it \"drove a vastly technologically superior foe from the battlefield, and eventually forced it to sign an armistice agreement some three years later.\" Allan R. Millett qualifies his assessment of the battle as a Chinese \"geographic victory\" in that they ejected X\u00a0Corps from North Korea with the PVA\u2019s tactical failure of achieving their stated objective of destroying the 1st Marine Division, adding that the campaign gave the UN confidence that it could withstand the superior numbers of the Chinese forces. The official Chinese history, published by PLA Academy of Military Science, states that despite the heavy casualties, the PVA 9th Corps had earned its victory by successfully protecting the eastern flank of Chinese forces in Korea, while inflicting over 10,000 casualties to the UN forces.\nEliot A. Cohen writes that the retreat from Chosin was a UN victory which inflicted such heavy losses on the PVA 9th Corps that it was put out of action until March 1951. Paul M. Edwards, founder of the Center for the Study of the Korean War, draws parallels between the battle at Chosin and the Dunkirk evacuation. He writes that the retreat from Chosin following a \"massive strategic victory\" by the Chinese has been represented as \"a moment of heroic history\" for the UN forces. Appleman, on the other hand, questioned the necessity of a sea-borne evacuation to preserve the UN forces, asserting that X\u00a0Corps had the strength to break out of the Chinese encirclement at Hungnam at the end of the battle. Chinese historian Li Xiaobing acknowledges X\u00a0Corps' successful withdrawal from North Korea, and writes that the Battle of Chosin \"has become a part of Marine lore, but it was still a retreat, not a victory.\" Bruce Cumings simply refers to the battle as a \"terrible defeat\" for the Americans.\nPatrick C. Roe, who served as an intelligence officer with the 7th Marine Regiment at Chosin, asserts that X\u00a0Corps directly allowed the Eighth Army to hold the south and quoted MacArthur in corroborating his view. Yu Bin, a historian and a former member of the Chinese People's Liberation Army, states that while the destruction of Task Force Faith was viewed as the single greatest Chinese victory of the war, ultimately the PVA 9th Corps had become \"a giant hospital\" while failing to destroy the numerically inferior UN forces at Chosin as planned. Zhang Renchu, whose 26th Army was blamed for allowing the X\u00a0Corps to escape, had threatened suicide over the outcome, while Song Shilun offered to resign his post.\nThe battle exacerbated inter-service hostility, the Marines blaming the US Army and its leadership for the failure. The collapse of the army units fighting on the east of the reservoir was regarded as shameful, and for many years afterwards their role in the battle was largely ignored. Later studies concluded that Task Force MacLean/Faith had held off for five days a significantly larger force than previously thought and that their stand was a significant factor in the Marines' survival. This was eventually recognized in September 1999 when, for its actions at Chosin, Task Force Faith was awarded the Presidential Unit Citation, an award that General Smith blocked when it was first proposed in 1952.\nThe Marines evacuated from North Korea and spent January and most of February 1951 rebuilding in the relatively secure South Korea, where they destroyed the well-respected but already weakened North Korean 10th Division in counter-guerrilla operations during the Second Battle of Wonju. The Marines returned to regular and heavy action on 21 February in Operation Killer.\nWider effect on the war.\nThe battle ended the UN force's expectation of total victory, including the capture of North Korea and the reunification of the peninsula. By the end of 1950, PVA/KPA forces had recaptured North Korea and pushed UN forces back south of the 38th parallel. Serious consideration was given to the evacuation of all US forces from the Korean peninsula and US military leaders made secret contingency plans to do so. The disregard by Far Eastern Command under MacArthur of the initial warnings and diplomatic hints by the PVA almost led the entire UN army to disaster at Ch'ongch'on River and Chosin Reservoir and only after the formation and stabilization of a coherent UN defensive line under Lieutenant General Matthew Ridgway did the \"period of headlong retreats from an attacking, unsuspected foe\" cease.\nOn the other hand, the battle affected the PVA in two ways, both of which had the result of helping the UN Command to secure its position in South Korea, while losing North Korea. First, according to historian Shu Guang Zhang, PVA commanders were persuaded by their victories at Chosin and Ch'ongch'on that they could \"defeat American armed forces\", and this led to \"unrealistic expectations that the CPV [PVA] would work miracles.\" Second, the heavy casualties caused by sub-zero temperatures and combat, plus poor logistical support weakened the PVA's eight elite divisions of the 20th and 27th Corps. Of those eight divisions, two were forced to disband. With the absence of 12 out of 30 of Chinese divisions in Korea in early 1951, Roe says that the heavy Chinese losses at Chosin enabled the UN forces to maintain a foothold in Korea.\nRequest for nuclear weapons.\nBy the end of the withdrawal, the Chinese troops had advanced and retaken almost all of North Korean territories. On 24 December 1950, MacArthur submitted a list of \"retardation targets\" in Korea, Manchuria and other parts of China, and requested 34 atomic bombs from Washington with the purpose of sowing a belt of radioactive cobalt to prevent further Chinese advances. His request was firmly declined and led to his later dismissal.\nLegacy.\nThe Battle of Chosin Reservoir is regarded by some historians as the most brutal in American history due to violence, casualty rate, weather conditions, and endurance. Over the course of fourteen days, 17 Medals of Honor (Army and Navy) and 78 Service Cross Medals (Army and Navy) were awarded by the United States, the second most as of 2020 after the Battle of the Bulge (20 MOHs and 83 SCMs).\nAmerican veterans of the battle are colloquially referred to as the \"Chosin Few\" and symbolized by the \"Star of Koto-ri\".\nNamesakes and memorials\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49398", "revid": "119644", "url": "https://en.wikipedia.org/wiki?curid=49398", "title": "Switching center", "text": ""}
{"id": "49399", "revid": "28481209", "url": "https://en.wikipedia.org/wiki?curid=49399", "title": "XY sex-determination system", "text": "Method of determining sex\nThe XY sex-determination system is a sex-determination system present in many mammals (including humans), some insects (\"Drosophila\"), some snakes, some fish (guppies), and some plants (\"Ginkgo\" tree). \nIn this system, the karyotypic sex of an individual is usually determined by a pair of sex chromosomes. Typically, karyotypic females have two of the same kind of sex chromosome (XX), and are called the homogametic sex. Karyotypic males typically have two different kinds of sex chromosomes (XY), and are called the heterogametic sex. In humans, the presence of the Y chromosome is responsible for triggering male phenotypic development; in the absence of the Y chromosome, the individual will usually develop phenotypicaly female. In most species with XY sex determination, an organism must have at least one X chromosome in order to survive.\nThe XY system contrasts in several ways with the ZW sex-determination system found in birds, some insects, many reptiles, and various other animals, in which the heterogametic sex is female. A temperature-dependent sex determination system is found in some reptiles and fish.\nMechanisms.\nAll animals have a genome made of DNA, which forms chromosomes during cell division. In humans, most mammals, and some other species, two of the chromosomes, called the X chromosome and Y chromosome, contain genes which code for sex. In these species, one or more genes are present on their Y chromosome that trigger development of the male phenotype . In this system, the X chromosome and the Y chromosome determine the karyotypic sex of offspring, while genes located on the Y chromosome trigger development of the male phenotype. Offspring usually have two sex chromosomes: an offspring with two X chromosomes (XX) will usually develop the female phenotype, and an offspring with an X and a Y chromosome (XY) will usually develop the male phenotype. Variations such as individuals with Swyer syndrome, that have an XY karyotype yet a female phenotype, and de la Chapelle Syndrome, that have XX chromosomes and a male phenotype, though less common, are exceptions. Additionally, there are several cases of phenotypic females with an XY karyotype (Swyer syndrome), who have successfully hosted a pregnancy.\nMammals.\nIn most mammals, karyotypic sex is determined by presence of the Y chromosome. This makes individuals with XXY and XYY karyotypic males, and individuals with X and XXX karyotypic females.\nIn the 1930s, Alfred Jost determined that the presence of testosterone was required for Wolffian duct development in the male rabbit.\nSRY is a sex-determining gene on the Y chromosome in the therians (placental mammals and marsupials). Non-human mammals use several genes on the Y chromosome.\nNot all male-specific genes are located on the Y chromosome. The platypus, a monotreme, use five pairs of different XY chromosomes with six groups of male-linked genes, AMH being the master switch.\nHumans.\nA single gene (\"SRY\") present on the Y chromosome acts as a signal to trigger male phenotypic developmental. Presence of this gene starts off the process of virilization. This and other factors result in the sex differences in humans. In individuals with two X chromosomes, cells undergo X-inactivation, in which one of the two X chromosomes is inactivated. The inactivated X chromosome remains within a cell as a Barr body.\nOther animals.\nSome species of turtles have convergently evolved XY sex determination systems, specifically those in Chelidae and Staurotypinae.\nOther species (including most \"Drosophila\" species) use the presence of two X chromosomes to determine femaleness: one X chromosome gives putative maleness, but the presence of Y chromosome genes is required for normal male development. In the fruit fly individuals with XY are male and individuals with XX are female; however, individuals with XXY or XXX can also be female, and individuals with X can be males.\nPlants.\nAngiosperms.\nWhile very few species of dioecious angiosperm have XY sex determination, making up less than 5% of all species, the sheer diversity of angiosperms means that the total number of species with XY sex determination is actually quite high, estimated to be at around 13,000 species. Molecular and evolutionary studies also show that XY sex determination has evolved independently many times in upwards of 175 unique families, with a recent study suggesting its evolution has independently occurred hundreds to thousands of times. \nMany economically important crops are known to have an XY system of sex determination, including kiwifruit, asparagus, grapes and date palms.\nGymnosperms.\nIn sharp contrast to angiosperms, approximately 65% of gymnosperms are dioecious. Some families which contain members that are known to have a XY system of sex determination include the cycad families Cycadaceae and Zamiaceae, Ginkgoaceae, Gnetaceae and Podocarpaceae.\nOther systems.\nWhilst XY sex determination is the most familiar, since it is the system that humans use, there are a range of alternative systems found in nature. The inverse of the XY system (called ZW to distinguish it) is used in birds and many insects, in which it is the females that are heterogametic (ZW), while males are homogametic (ZZ). \nMany insects of the order Hymenoptera instead have a haplo-diploid system, where the females are full diploids (with all chromosomes appearing in pairs) but males are haploid (having just one copy of all chromosomes). Some other insects have the X0 sex-determination system, where just the sex-determining chromosome varies in ploidy (XX in females but X in males), while all other chromosomes appear in pairs in both sexes.\nInfluences.\nGenetic.\nIn an interview for the \"Rediscovering Biology\" website, researcher Eric Vilain described how the paradigm changed since the discovery of the SRY gene:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;For a long time we thought that SRY would activate a cascade of male genes. It turns out that the sex determination pathway is probably more complicated and SRY may in fact inhibit some anti-male genes.\nThe idea is instead of having a simplistic mechanism by which you have pro-male genes going all the way to make a male, in fact there is a solid balance between pro-male genes and anti-male genes and if there is a little too much of anti-male genes, there may be a female born and if there is a little too much of pro-male genes then there will be a male born.\nWe [are] entering this new era in molecular biology of sex determination where it's a more subtle dosage of genes, some pro-males, some pro-females, some anti-males, some anti-females that all interplay with each other rather than a simple linear pathway of genes going one after the other, which makes it very fascinating but very complicated to study.\nIn an interview by \"Scientific American\" in 2007, Vilian was asked: \"It sounds as if you are describing a shift from the prevailing view that female development is a default molecular pathway to active pro-male and antimale pathways. Are there also pro-female and antifemale pathways?\" He replied:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Modern sex determination started at the end of the 1940s\u20141947\u2014when the French physiologist Alfred Jost said it's the testis that is determining sex. Having a testis determines maleness, not having a testis determines femaleness. The ovary is not sex-determining. It will not influence the development of the external genitalia. Now in 1959 when the karyotype of Klinefelter [a male who is XXY] and Turner [a female who has one X] syndromes was discovered, it became clear that in humans it was the presence or the absence of the Y chromosome that's sex determining. Because all Klinefelters that have a Y are male, whereas Turners, who have no Y, are females. So it's not a dosage or the number of X's, it's really the presence or absence of the Y. So if you combine those two paradigms, you end up having a molecular basis that's likely to be a factor, a gene, that's a testis-determining factor, and that's the sex-determining gene. So the field based on that is really oriented towards finding testis-determining factors. What we discovered, though, was not just pro-testis determining factors. There are a number of factors that are there, like WNT4, like DAX1, whose function is to counterbalance the male pathway.\nIn mammals, including humans, the SRY gene triggers the development of non-differentiated gonads into testes rather than ovaries. However, there are cases in which testes can develop in the absence of an SRY gene (see sex reversal). In these cases, the SOX9 gene, involved in the development of testes, can induce their development without the aid of SRY. In the absence of SRY and SOX9, no testes can develop and the path is clear for the development of ovaries. Even so, the absence of the SRY gene or the silencing of the SOX9 gene are not enough to trigger sexual differentiation of a fetus in the female direction. A recent finding suggests that ovary development and maintenance is an active process, regulated by the expression of a \"pro-female\" gene, FOXL2. In an interview for the \"TimesOnline\" edition, study co-author Robin Lovell-Badge explained the significance of the discovery:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We take it for granted that we maintain the sex we are born with, including whether we have testes or ovaries. But this work shows that the activity of a single gene, FOXL2, is all that prevents adult ovary cells turning into cells found in testes.\nImplications.\nLooking into the genetic determinants of human sex can have wide-ranging consequences. Scientists have been studying different sex determination systems in fruit flies and animal models to attempt an understanding of how the genetics of sexual differentiation can influence biological processes like reproduction, ageing and disease.\nMaternal.\nIn humans and many other species of animals, the father determines the sex of the child. In the XY sex-determination system, the female-provided ovum contributes an X chromosome and the male-provided sperm contributes either an X chromosome or a Y chromosome, resulting in female (XX) or male (XY) offspring, respectively.\nHormone levels in the male parent affect the sex ratio of sperm in humans. Maternal influences also impact which sperm are more likely to achieve conception.\nHuman ova, like those of other mammals, are covered with a thick translucent layer called the zona pellucida, which the sperm must penetrate to fertilize the egg. Once viewed simply as an impediment to fertilization, recent research indicates the zona pellucida may instead function as a sophisticated biological security system that chemically controls the entry of the sperm into the egg and protects the fertilized egg from additional sperm.\nRecent research indicates that human ova may produce a chemical which appears to attract sperm and influence their swimming motion. However, not all sperm are positively impacted; some appear to remain uninfluenced and some actually move away from the egg.\nMaternal influences may also be possible that affect sex determination in such a way as to produce fraternal twins equally weighted between one male and one female.\nThe time at which insemination occurs during the estrus cycle has been found to affect the sex ratio of the offspring of humans, cattle, hamsters, and other mammals. Hormonal and pH conditions within the female reproductive tract vary with time, and this affects the sex ratio of the sperm that reach the egg.\nSex-specific mortality of embryos also occurs.\nHistory.\nAncient ideas on sex determination.\nAristotle believed incorrectly that the sex of an infant is determined by how much heat a man's sperm had during insemination. He wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;... the semen of the male differs from the corresponding secretion of the female in that it contains a principle within itself of such a kind as to set up movements also in the embryo and to concoct thoroughly the ultimate nourishment, whereas the secretion of the female contains material alone. If, then, the male element prevails it draws the female element into itself, but if it is prevailed over it changes into the opposite or is destroyed.\nAristotle claimed in error that the male principle was the driver behind sex determination, such that if the male principle was insufficiently expressed during reproduction, the fetus would develop as a female.\n20th century genetics.\nNettie Stevens (working with beetles) and Edmund Beecher Wilson (working with hemiptera) are credited with independently discovering, in 1905, the chromosomal XY sex-determination system in insects: the fact that males have XY sex chromosomes and females have XX sex chromosomes. In the early 1920s, Theophilus Painter demonstrated that sex in humans (and other mammals) was also determined by the X and Y chromosomes, and the chromosomes that make this determination are carried by the spermatozoa.\nThe first clues to the existence of a factor that determines the development of testis in mammals came from experiments carried out by Alfred Jost, who castrated embryonic rabbits in utero and noticed that they all acquired a female phenotype.\nIn 1959, C. E. Ford and his team, in the wake of Jost's experiments, discovered that the Y chromosome was needed for a fetus to develop as male when they examined patients with Turner's syndrome, who grew up as phenotypic females, and found them to be X0 (hemizygous for X and no Y). At the same time, Jacob &amp; Strong described a case of a patient with Klinefelter syndrome (XXY), which implicated the presence of a Y chromosome in development of maleness.\nAll these observations led to a consensus that a dominant gene that determines testis development (TDF) must exist on the human Y chromosome. The search for this testis-determining factor (TDF) led to Peter Goodfellow's team of scientists in 1990 to discover a region of the Y chromosome that is necessary for the male sex determination, which was named SRY (sex-determining region of the Y chromosome).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49400", "revid": "47023628", "url": "https://en.wikipedia.org/wiki?curid=49400", "title": "Window", "text": "Opening to admit light, air, or objects\nA window is an opening in a wall, door, roof, or vehicle that allows the exchange of light and may also allow the passage of sound and sometimes air. Modern windows are usually glazed or covered in some other transparent or translucent material, a sash set in a frame in the opening; the sash and frame are also referred to as a window. Many glazed windows may be opened, to allow ventilation, or closed to exclude inclement weather. Windows may have a latch or similar mechanism to lock the window shut or to hold it open by various amounts.\nTypes include the eyebrow window, fixed windows, hexagonal windows, single-hung, and double-hung sash windows, horizontal sliding sash windows, casement windows, awning windows, hopper windows, tilt, and slide windows (often door-sized), tilt and turn windows, transom windows, sidelight windows, jalousie or louvered windows, clerestory windows, lancet windows, skylights, roof windows, roof lanterns, bay windows, oriel windows, thermal, or Diocletian, windows, picture windows, rose windows, emergency exit windows, stained glass windows, French windows, panel windows, double/triple-paned windows, and witch windows.\nEtymology.\nThe English language-word \"window\" originates from the Old Norse , from 'wind' and 'eye'. In Norwegian, Nynorsk, and Icelandic, the Old Norse form has survived to this day (in Icelandic only as a less used word for a type of small open \"window\", not strictly a synonym for , the Icelandic word for 'window'). In Swedish, the word remains as a term for a hole through the roof of a hut, and in the Danish language and Norwegian , the direct link to \"eye\" is lost, just as for \"window\". The Danish (but not the ) word is pronounced fairly similarly to \"window\".\n\"Window\" is first recorded in the early 13th century, and originally referred to an unglazed hole in a roof. \"Window\" replaced the Old English , which literally means 'eye-hole', and 'eye-door'. Many Germanic languages, however, adopted the Latin word to describe a window with glass, such as standard Swedish , or German . The use of \"window\" in English is probably because of the Scandinavian influence on the English language by means of loanwords during the Viking Age. In English, the word \"fenester\" was used as a parallel until the mid-18th century. \"Fenestration\" is still used to describe the arrangement of windows within a fa\u00e7ade, as well as \"defenestration\", meaning 'to throw out of a window'.\nHistory.\nThe Romans were the first known to use glass for windows, a technology likely first produced in Roman Egypt, in Alexandria c.\u2009100 AD. Presentations of windows can be seen in ancient Egyptian wall art and sculptures from Assyria. Paper windows were economical and widely used in ancient China, Korea, and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century whereas windows made up of panes of flattened animal horn were used as early as the 14th century. In the 19th century American west, greased paper windows came to be used by pioneering settlers. Modern-style floor-to-ceiling windows became possible only after the industrial plate glass making processes were fully perfected.\nTechnologies.\nIn the 13th century BC, the earliest windows were unglazed openings in a roof to admit light during the day. Later, windows were covered with animal hide, cloth, or wood. Shutters that could be opened and closed came next. Over time, windows were built that both protected the inhabitants from the elements and transmitted light, using multiple small pieces of translucent material, such as flattened pieces of translucent animal horn, paper sheets, thin slices of marble (such as fengite), or pieces of glass, set in frameworks of wood, iron or lead. In the Far East, paper was used to fill windows.\nThe Romans were the first known users of glass for windows, exploiting a technology likely first developed in Roman Egypt. Specifically, in Alexandria c. 100 CE, cast-glass windows, albeit with poor optical properties, began to appear, but these were small thick productions, little more than blown-glass jars (cylindrical shapes) flattened out into sheets with circular striation patterns throughout. (Compare traditional church windows made of stained glass.) It would be over a millennium before window glass became transparent enough to see through clearly, as we expect now. (However, ancient Roman windows were still very useful, as they presented \"an often-overlooked advance in heating technology (allowing solar heat to enter a home or building while preventing the warmed air from escaping).\") In 1154, Al-Idrisi described glass windows as a feature of the palace belonging to the king of the Ghana Empire.\nOver the centuries techniques were developed to shear through one side of a blown glass cylinder and produce thinner rectangular window panes from the same amount of glass material. This gave rise to tall narrow windows, usually separated by a vertical support called a mullion. Mullioned glass windows were the windows of choice among the European well-to-do, whereas paper windows were economical and widely used in ancient China, Korea, and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century, whereas windows made up of panes of flattened animal horn were used as early as the 14th century.\nModern-style floor-to-ceiling windows became possible only after the industrial plate glass-making processes were perfected in the late 19th century. Modern windows are usually filled using glass, although transparent plastic is also used.\nFashions and trends.\nThe introduction of lancet windows into Western European church architecture from the 12th century CE built on a tradition of arched windows inserted between columns, and led not only to tracery and elaborate stained-glass windows but also to a long-standing motif of pointed or rounded window-shapes in ecclesiastical buildings, still seen in many churches today.\nPeter Smith discusses overall trends in early-modern rural Welsh window architecture:\nUp to about 1680 windows tended to be horizontal in proportion, a shape suitable for lighting the low-ceilinged rooms that had resulted from the insertion of the upper floor into the hall-house. After that date vertically proportioned windows came into fashion, partly at least as a response to the Renaissance taste for the high ceiling. Since 1914 the wheel has come full circle and a horizontally proportioned window is again favoured.\nThe spread of plate-glass technology made possible the introduction of picture windows (in Levittown, Pennsylvania, founded 1951\u20131952).\nMany modern day windows may have a window screen or mesh, often made of aluminum or fibreglass, to keep bugs out when the window is opened. Windows are primarily designed to facilitate a vital connection with the outdoors, offering those within the confines of the building visual access to the everchanging events occurring outside. The provision of this connection serves as an integral safeguard for the health and well-being of those inhabiting buildings, lest they experience the detrimental effects of enclosed buildings devoid of windows. Among the myriad criteria for the design of windows, several pivotal criteria have emerged in daylight standards: location, time, weather, nature, and people. Of these criteria, windows that are designed to provide views of nature are considered to be the most important by people.\nTypes.\nCross.\nA cross-window is a rectangular window usually divided into four lights by a mullion and transom that form a Latin cross.\nEyebrow.\nThe term \"eyebrow window\" is used in two ways: a curved top window in a wall or an eyebrow dormer; and a row of small windows usually under the front eaves such as the James-Lorah House in Pennsylvania.\nFixed.\nA \"fixed window\" is a window that cannot be opened, whose function is limited to allowing light to enter (unlike an unfixed window, which can open and close). Clerestory windows in church architecture are often fixed. Transom windows may be fixed or operable. This type of window is used in situations where light or vision alone is needed as no ventilation is possible in such windows without the use of trickle vents or overglass vents.\nSingle-hung sash.\nA \"single-hung sash window\" is a window that has one sash that is movable (usually the bottom one) and the other fixed. This is the earlier form of sliding sash window and is also cheaper.\nDouble-hung sash.\nA sash window is the traditional style of window in the United Kingdom, and many other places that were formerly colonized by the UK, with two parts (sashes) that overlap slightly and slide up and down inside the frame. The two parts are not necessarily the same size; where the upper sash is smaller (shorter) it is termed a cottage window. Currently, most new double-hung sash windows use spring balances to support the sashes, but traditionally, counterweights held in boxes on either side of the window were used. These were and are attached to the sashes using pulleys of either braided cord or, later, purpose-made chain. Three types of spring balances are called a tape or clock spring balance; channel or block-and-tackle balance, and a spiral or tube balance.\nDouble-hung sash windows were traditionally often fitted with shutters. Sash windows can be fitted with simplex hinges that let the window be locked into hinges on one side, while the rope on the other side is detached\u2014so the window can be opened for fire escape or cleaning.\nFoldup.\nA \"foldup\" has two equal sashes similar to a standard double-hung but folds upward allowing air to pass through nearly the full-frame opening. The window is balanced using either springs or counterbalances, similar to a double-hung. The sashes can be either offset to simulate a double-hung, or in-line. The inline versions can be made to fold inward or outward. The inward swinging foldup windows can have fixed screens, while the outward swinging ones require movable screens. The windows are typically used for screen rooms, kitchen pass-throughs, or egress.\nHorizontal sliding sash.\nA \"horizontal sliding sash window\" has two or more sashes that overlap slightly but slide horizontally within the frame. In the UK, these are sometimes called Yorkshire sash windows, presumably because of their traditional use in that county.\nCasement.\nA casement window is a window with a hinged sash that swings in or out like a door comprising either a side-hung, top-hung (also called \"awning window\"; see below), or occasionally bottom-hung sash or a combination of these types, sometimes with fixed panels on one or more sides of the sash. In the US, these are usually opened using a crank, but in parts of Europe, they tend to use projection friction stays and espagnolette locking. Formerly, plain hinges were used with a casement stay. Handing applies to casement windows to determine direction of swing; a casement window may be left-handed, right-handed, or double. The casement window is the dominant type now found in modern buildings in the UK and many other parts of Europe.\nAwning.\nAn \"awning window\" is a casement window that is hung horizontally, hinged on top, so that it swings outward like an awning. In addition to being used independently, they can be stacked, several in one opening, or combined with fixed glass. They are particularly useful for ventilation.\nHopper.\nA \"hopper window\" is a bottom-pivoting casement window that opens by tilting vertically, typically to the inside, resembling a hopper chute.\nPivot.\nA \"pivot window\" is a window hung on one hinge on each of two opposite sides which allows the window to revolve when opened. The hinges may be mounted top and bottom (Vertically Pivoted) or at each jamb (Horizontally Pivoted). The window will usually open initially to a restricted position for ventilation and, once released, fully reverse and lock again for safe cleaning from inside. Modern pivot hinges incorporate a friction device to hold the window open against its weight and may have restriction and reversed locking built-in. In the UK, where this type of window is most common, they were extensively installed in high-rise social housing.\nTilt and slide.\nA \"tilt and slide window\" is a window (more usually a door-sized window) where the sash tilts inwards at the top similar to a hopper window and then slides horizontally behind the fixed pane.\nTilt and turn.\nA \"tilt and turn window\" can both tilt inwards at the top or open inwards from hinges at the side. This is the most common type of window in Germany, its country of origin. It is also widespread in many other European countries. In Europe, it is usual for these to be of the \"turn first\" type. i.e. when the handle is turned to 90 degrees the window opens in the side hung mode. With the handle turned to 180 degrees the window opens in bottom hung mode. Most usually in the UK the windows will be \"tilt first\" i.e. bottom hung at 90 degrees for ventilation and side hung at 180 degrees for cleaning the outer face of the glass from inside the building.\nTransom.\nA transom window is a window above a door. In an exterior door the transom window is often fixed, in an interior door, it can open either by hinges at top or bottom, or rotate on hinges. It provided ventilation before forced air heating and cooling. A fan-shaped transom is known as a fanlight, especially in the British Isles.\nSide light.\nWindows beside a door or window are called side-, wing-, margen-lights, and flanking windows.\nJalousie window.\nAlso known as a louvered window, the jalousie window consists of parallel slats of glass or acrylic that open and close like a Venetian blind, usually using a crank or a lever. They are used extensively in tropical architecture. A jalousie door is a door with a jalousie window.\nClerestory.\nA clerestory window is a window set in a roof structure or high in a wall, used for daylighting.\nSkylight.\nA skylight is a window built into a roof structure. This type of window allows for natural daylight and moonlight.\nRoof.\nA \"roof window\" is a sloped window used for daylighting, built into a roof structure. It is one of the few windows that could be used as an exit. Larger roof windows meet building codes for emergency evacuation.\nRoof lantern.\nA roof lantern is a multi-paned glass structure, resembling a small building, built on a roof for day or moon light. Sometimes includes an additional clerestory. May also be called a cupola.\nBay.\nA bay window is a multi-panel window, with at least three panels set at different angles to create a protrusion from the wall line.\nOriel.\nAn \"oriel window\" is a form of bay window. This form most often appears in Tudor-style houses and monasteries. It projects from the wall and does not extend to the ground. Originally a form of porch, they are often supported by brackets or corbels.\nThermal.\nThermal, or Diocletian, windows are large semicircular windows (or niches) which are usually divided into three lights (window compartments) by two mullions. The central compartment is often wider than the two side lights on either side of it.\nPicture.\nA \"picture window\" is a large fixed window in a wall, typically without glazing bars, or glazed with only perfunctory glazing bars (muntins) near the edge of the window. Picture windows provide an unimpeded view, as if framing a picture.\nMulti-lite.\nA \"multi-lite window\" is a window glazed with small panes of glass separated by wooden or lead \"glazing bars\", or \"muntins\", arranged in a decorative \"glazing pattern\" often dictated by the building's architectural style. Due to the historic unavailability of large panes of glass, the multi-lit (or \"lattice window\") was the most common window style until the beginning of the 20th century, and is still used in traditional architecture.\nEmergency exit/egress.\nAn \"emergency exit window\" is a window big enough and low enough so that occupants can escape through the opening in an emergency, such as a fire. In many countries, exact specifications for emergency windows in bedrooms are given in many building codes. Specifications for such windows may also allow for the entrance of emergency rescuers. Vehicles, such as buses, aircraft, and trains frequently have emergency exit windows as well.\nStained glass.\nA stained glass window is a window composed of pieces of colored glass, transparent, translucent or opaque, frequently portraying persons or scenes. Typically the glass in these windows is separated by lead glazing bars. Stained glass windows were popular in Victorian houses and some Wrightian houses, and are especially common in churches.\nFrench.\nA French door has two columns of upright rectangular glass panes (lights) extending its full length; and two of these doors on an exterior wall and without a mullion separating them, that open outward with opposing hinges to a terrace or porch, are referred to as a French window. Sometimes these are set in pairs or multiples thereof along the exterior wall of a very large room, but often, one French window is placed centrally in a typically sized room, perhaps among other fixed windows flanking the feature. French windows are known as \"porte-fen\u00eatre\" in France and \"portafinestra\" in Italy, and frequently are used in modern houses.\nDouble-paned.\n\"Double-paned windows\" have two parallel panes (slabs of glass) with a separation of typically about 1\u00a0cm; this space is permanently sealed and filled at the time of manufacture with dry air or other dry nonreactive gas. Such windows provide a marked improvement in thermal insulation (and usually in acoustic insulation as well) and are resistant to fogging and frosting caused by temperature differential. They are widely used for residential and commercial construction in intemperate climates. In the UK, double-paned and triple-paned are referred to as double-glazing and triple-glazing. Triple-paned windows are now a common type of glazing in central to northern Europe. Quadruple glazing is now being introduced in Scandinavia.\nHexagonal window.\nA hexagonal window is a hexagon-shaped window, resembling a bee cell or crystal lattice of graphite. The window can be vertically or horizontally oriented, openable or dead. It can also be regular or elongately-shaped and can have a separator (mullion). Typically, the cellular window is used for an attic or as a decorative feature, but it can also be a major architectural element to provide the natural lighting inside buildings.\nGuillotine window.\nA \"guillotine window\" is a window that opens vertically. Guillotine windows have more than one sliding frame, and open from bottom to top or top to bottom.\nTerms.\nEN 12519 is the European standard that describes windows terms officially used in EU Member States. The main terms are:\nLabeling.\nThe United States NFRC Window Label lists the following terms:\nThe European harmonised standard hEN 14351\u20131, which deals with doors and windows, defines 23 characteristics (divided into \"essential\" and \"non essential\"). Two other, preliminary European Norms that are under development deal with internal pedestrian doors (prEN 14351\u20132), smoke and fire resisting doors, and openable windows (prEN 16034).\nConstruction.\nWindows can be a significant source of heat transfer. Therefore, insulated glazing units consist of two or more panes to reduce the transfer of heat.\nGrids or muntins.\nThese are the pieces of framing that separate a larger window into smaller panes. In older windows, large panes of glass were quite expensive, so muntins let smaller panes fill a larger space. In modern windows, light-colored muntins still provide a useful function by reflecting some of the light going through the window, making the window itself a source of diffuse light (instead of just the surfaces and objects illuminated within the room). By increasing the indirect illumination of surfaces near the window, muntins tend to brighten the area immediately around a window and reduce the contrast of shadows within the room.\nFrame and sash construction.\nFrames and sashes can be made of the following materials:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nComposites (also known as Hybrid Windows) are start since early 1998 and combine materials like aluminium + pvc or wood to obtain aesthetics of one material with the functional benefits of another.\nA special class of PVC window frames, uPVC window frames, became widespread since the late 20th century, particularly in Europe: there were 83.5 million installed by 1998 with numbers still growing as of 2012.\nGlazing and filling.\nLow-emissivity coated panes reduce heat transfer by radiation, which, depending on which surface is coated, helps prevent heat loss (in cold climates) or heat gains (in warm climates).\nHigh thermal resistance can be obtained by evacuating or filling the insulated glazing units with gases such as argon or krypton, which reduces conductive heat transfer due to their low thermal conductivity. Performance of such units depends on good window seals and meticulous frame construction to prevent entry of air and loss of efficiency.\nModern double-pane and triple-pane windows often include one or more low-e coatings to reduce the window's U-factor (its insulation value, specifically its rate of heat loss). In general, soft-coat low-e coatings tend to result in a lower solar heat gain coefficient (SHGC) than hard-coat low-e coatings.\nModern windows are usually glazed with one large sheet of glass per sash, while windows in the past were glazed with multiple panes separated by \"glazing bars\", or \"muntins\", due to the unavailability of large sheets of glass. Today, glazing bars tend to be decorative, separating windows into small panes of glass even though larger panes of glass are available, generally in a pattern dictated by the architectural style at use. Glazing bars are typically wooden, but occasionally lead glazing bars soldered in place are used for more intricate glazing patterns.\nOther construction details.\nMany windows have movable window coverings such as blinds or curtains to keep out light, provide additional insulation, or ensure privacy.\nWindows allow natural light to enter, but too much can have negative effects such as glare and heat gain. Additionally, while windows let the user see outside, there must be a way to maintain privacy on in the inside. Window coverings are practical accommodations for these issues.\nImpact of the sun.\nSun incidence angle.\nHistorically, windows are designed with surfaces parallel to vertical building walls. Such a design allows considerable solar light and heat penetration due to the most commonly occurring incidence of sun angles. In passive solar building design, an extended eave is typically used to control the amount of solar light and heat entering the window(s).\nAn alternative method is to calculate an optimum window mounting angle that accounts for summer sun load minimization, with consideration of actual latitude of the building. This process has been implemented, for example, in the Dakin Building in Brisbane, California\u2014in which most of the fenestration is designed to reflect summer heat load and help prevent summer interior over-illumination and glare, by canting windows to nearly a 45 degree angle.\nSolar window.\nPhotovoltaic windows not only provide a clear view and illuminate rooms, but also convert sunlight to electricity for the building. In most cases, translucent photovoltaic cells are used.\nPassive solar.\n\"Passive solar windows\" allow light and solar energy into a building while minimizing air leakage and heat loss. Properly positioning these windows in relation to sun, wind, and landscape\u2014while properly shading them to limit excess heat gain in summer and shoulder seasons, and providing thermal mass to absorb energy during the day and release it when temperatures cool at night\u2014increases comfort and energy efficiency. Properly designed in climates with adequate solar gain, these can even be a building's primary heating system.\nCoverings.\nA window covering is a shade or screen that provides multiple functions. Some coverings, such as drapes and blinds provide occupants with privacy. Some window coverings control solar heat gain and glare. There are external shading devices and internal shading devices. Low-e window film is a low-cost alternative to window replacement to transform existing poorly-insulating windows into energy-efficient windows. For high-rise buildings, smart glass can provide an alternative.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49401", "revid": "24198", "url": "https://en.wikipedia.org/wiki?curid=49401", "title": "Hall", "text": "Large room used for meetings, social affairs or events\nIn architecture, a hall is a relatively large space enclosed by a roof and walls. In the Iron Age and the Early Middle Ages in northern Europe, a mead hall was where a lord and his retainers ate and also slept. Later in the Middle Ages, the great hall was the largest room in castles and large houses, and where the servants usually slept. As more complex house plans developed, the hall remained a large room for dancing and large feasts, often still with servants sleeping there. It was usually immediately inside the main door. In modern British houses, an entrance hall next to the front door remains an indispensable feature, even if it is essentially merely a corridor.\nToday, the (entrance) hall of a house is the space next to the front door or vestibule leading to the rooms directly and/or indirectly. Where the hall inside the front door of a house is elongated, it may be called a passage, corridor (from Spanish \"corredor\" used in El Escorial and 100 years later in Castle Howard), or hallway.\nHistory.\nIn warmer climates, the houses of the wealthy were often built around a courtyard, but in northern areas manors were built around a great hall. The hall was home to the hearth and was where all the residents of the house would eat, work, and sleep. One common example of this form is the longhouse. Only particularly messy tasks would be done in separate rooms on the periphery of the hall. Still today the term \"hall\" is often used to designate a country house such as a hall house, or specifically a Wealden hall house, and manor houses.\nIn later medieval Europe, the main room of a castle or manor house was the great hall. In a medieval building, the hall was where the fire was kept. As heating technology improved and a desire for privacy grew, tasks moved from the hall to other rooms. First, the master of the house withdrew to private bedrooms and eating areas. Over time servants and children also moved to their own areas, while work projects were also given their own chambers leaving the hall for special functions. With time, its functions as dormitory, kitchen, parlour, and so on were divided into separate rooms or, in the case of the kitchen, a separate building.\nUntil the early modern era that majority of the population lived in houses with a single room. In the 17th century, even lower classes began to have a second room, with the main chamber being the hall and the secondary room the parlor. The hall and parlor house was found in England and was a fundamental, historical floor plan in parts of the United States from 1620 to 1860.\nIn Europe, as the wealthy embraced multiple rooms initially the common form was the enfilade, with rooms directly connecting to each other. In 1597 John Thorpe is the first recorded architect to replace multiple connected rooms with rooms along a corridor each accessed by a separate door.\nOther uses.\nCollegiate halls.\nMany institutions and buildings at colleges and universities are formally titled \"_______ Hall\", typically being named after the person who endowed it, for example, King's Hall, Cambridge. Others, such as Lady Margaret Hall, Oxford, commemorate respected people. Between these in age, Nassau Hall at Princeton University began as the single building of the then college. In medieval origin, these were the halls in which the members of the university lived together during term time. In many cases, some aspect of this community remains.\nSome of these institutions are titled \"Hall\" instead of \"College\" because at the time of their foundation they were not recognised as colleges (in some cases because their foundation predated the existence of colleges) and did not have the appropriate Royal Charter. Examples at the University of Oxford are:\nIn colleges of the universities of Oxford and Cambridge, the term \"Hall\" is also used for the dining hall for students, with High Table at one end for fellows. Typically, at \"Formal Hall\", gowns are worn for dinner during the evening, whereas for \"informal Hall\" they are not. The medieval collegiate dining hall, with a dais for the high table at the upper end and a screen passage at the lower end, is a modified or assimilated form of the Great hall.\nMeeting hall.\nA hall is also a building consisting largely of a principal room, that is rented out for meetings and social affairs. It may be privately or government-owned, such as a function hall owned by one company used for weddings and cotillions (organized and run by the same company on a contractual basis) or a community hall available for rent to anyone, such as a British village hall.\nReligious halls.\nIn religious architecture, as in Islamic architecture, the prayer hall is a large room dedicated to the practice of worship. (example: the prayer hall of the Great Mosque of Kairouan in Tunisia). A hall church is a church with a nave and side aisles of approximately equal height. Many churches have an associated church hall used for meetings and other events.\nPublic buildings.\nFollowing a line of similar development, in office buildings and larger buildings (theatres, cinemas etc.), the entrance hall is generally known as the foyer (the French for fireplace). The atrium, a name sometimes used in public buildings for the entrance hall, was the central courtyard of a Roman house.\nTypes.\nIn architecture, the term \"double-loaded\" describes corridors that connect to rooms on both sides. Conversely, a single-loaded corridor only has rooms on one side (and possible windows on the other). A blind corridor does not lead anywhere.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49402", "revid": "16035602", "url": "https://en.wikipedia.org/wiki?curid=49402", "title": "Closet", "text": "Enclosed space used for storage, particularly that of clothes\nA closet (especially in North American English usage) is an enclosed space, with a door, used for storage, particularly that of clothes. \"Fitted closets\" are built into the walls of the house so that they take up no apparent space in the room. Closets are often built under stairs, thereby using awkward space that would otherwise go unused.\nA piece of furniture such as a cabinet or chest of drawers serves the same purpose of storage, but is not a closet, which is an architectural feature rather than a piece of furniture. A closet always has space for hanging, where a cupboard may consist only of shelves for folded garments. \"Wardrobe\" can refer to a free-standing piece of furniture (also known as an \"armoire\"), but according to the \"Oxford English Dictionary\", a wardrobe can also be a \"large cupboard or cabinet for storing clothes or other linen\", including \"built-in wardrobe, fitted wardrobe, walk-in wardrobe, etc.\"\nOther uses of the word.\nIn Elizabethan and Middle English, \"closet\" referred to a small private room, an inner sanctum within a far larger house, used for prayer, reading, or study. \nThe use of \"closet\" for \"toilet\" dates back to 1662. In Indian English, this use continues. Related forms include earth closet and water closet (flush toilet). \"Privy\" meaning an outhouse derives from \"private\", making the connection with the Middle English use of \"closet\", above.\nCloset tax question in colonial America.\nThough some sources claim that colonial American houses often lacked closets because of a \"closet tax\" imposed by the British crown, others argue that closets were absent in most houses simply because their residents had few possessions.\nCloset organizers.\nCloset organizers are integrated shelving systems. Different materials have advantages and disadvantages:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49404", "revid": "45093283", "url": "https://en.wikipedia.org/wiki?curid=49404", "title": "Kitchen", "text": "Space primarily used for preparation and storage of food\nA kitchen is a room or part of a room used for cooking and food preparation in a dwelling or in a commercial establishment. A modern middle-class residential kitchen is typically equipped with a stove, a sink with hot and cold running water, a refrigerator, and worktops and kitchen cabinets arranged according to a modular design. Many households have a microwave oven, a dishwasher, and other electric appliances. The main functions of a kitchen are to store, prepare and cook food (and to complete related tasks such as dishwashing). The room or area may also be used for dining (or small meals such as breakfast), entertaining and laundry. The design and construction of kitchens is a huge market all over the world.\nCommercial kitchens are found in restaurants, cafeterias, hotels, hospitals, educational and workplace facilities, army barracks, and similar establishments. These kitchens are generally larger and equipped with bigger and more heavy-duty equipment than a residential kitchen. For example, a large restaurant may have a walk-in refrigerator and a large commercial dishwashing machine. In some instances, commercial kitchen equipment such as commercial sinks are used in household settings as they offer ease of use for food preparation and high durability.\nIn developed countries, commercial kitchens are generally subject to public health laws. They are inspected periodically by public-health officials, and forced to close if they do not meet hygienic requirements mandated by law.\nHistory.\nMiddle Ages.\nEarly medieval European longhouses had an open fire under the highest point of the building. The kitchen area was between the entrance and the fireplace. In wealthy homes, there was typically more than one kitchen. In some homes, there were upwards of three kitchens. The kitchens were divided based on the types of food prepared in them.\nThe kitchen might be separate from the great hall due to the smoke from cooking fires and the chance the fires may get out of control. Few medieval kitchens survive as they were \"notoriously ephemeral structures\".\nColonial America.\nIn Connecticut, as in other colonies of New England during Colonial America, kitchens were often built as separate rooms and were located behind the parlor and keeping room or dining room. One early record of a kitchen is found in the 1648 inventory of the estate of a John Porter of Windsor, Connecticut. The inventory lists goods in the house \"over the kittchin\" and \"in the kittchin\". The items listed in the kitchen were: silver spoons, pewter, brass, iron, arms, ammunition, hemp, flax and \"other implements about the room\".\nTechnological developments such as the Rumford roaster and the kitchen range enabled more efficient use of space and fuel.\nRationalization.\nA stepping stone to the modern fitted kitchen was the Frankfurt Kitchen, designed by Margarete Sch\u00fctte-Lihotzky for social housing projects in 1926. This kitchen measured , and was built to optimize kitchen efficiency and lower building costs. The design was the result of detailed time-motion studies and interviews with future tenants to identify what they needed from their kitchens. Sch\u00fctte-Lihotzky's fitted kitchen was built in some 10,000 apartments in housing projects erected in Frankfurt, Germany in the 1930s.\nMaterials.\nThe Frankfurt Kitchen of 1926 was made of several materials depending on the application. The modern built-in kitchens of today use particle board or MDF, decorated with a variety of materials and finishes including wood veneers, lacquer, glass, melamine, laminate, ceramic and eco gloss. Very few manufacturers produce home built-in kitchens from stainless steel. Until the 1950s, steel kitchens were used by architects, but this material was displaced by the cheaper particle board panels sometimes decorated with a steel surface.\nDomestic kitchen planning.\nDomestic (or residential) kitchen design is a relatively recent discipline. The first ideas to optimize the work in the kitchen go back to Catharine Beecher's \"A Treatise on Domestic Economy\" (1843, revised and republished together with her sister Harriet Beecher Stowe as \"The American Woman's Home\" in 1869). Beecher's \"model kitchen\" propagated for the first time a systematic design based on early ergonomics. The design included regular shelves on the walls, ample workspace, and dedicated storage areas for various food items. Beecher even separated the functions of preparing food and cooking it altogether by moving the stove into a compartment adjacent to the kitchen.\nChristine Frederick published from 1913 a series of articles on \"New Household Management\" in which she analyzed the kitchen following Taylorist principles of efficiency, presented detailed time-motion studies, and derived a kitchen design from them. Her ideas were taken up in the 1920s by architects in Germany and Austria, most notably Bruno Taut, Erna Meyer, Margarete Sch\u00fctte-Lihotzky and Benita Otte, who designed the first fitted kitchen for the Haus am Horn, which was completed in 1923. Similar design principles were employed by Sch\u00fctte-Lihotzky for her famous Frankfurt kitchen, designed for Ernst May's \"R\u00f6merstadt\", a social housing project in Frankfurt, in 1927.\nWhile this \"work kitchen\" and variants derived from it were a great success for tenement buildings, homeowners had different demands and did not want to be constrained by a kitchen. Nevertheless, the kitchen design was mostly ad-hoc following the whims of the architect. In the U.S., the \"Small Homes Council\", since 1993 the \"Building Research Council\", of the School of Architecture of the University of Illinois at Urbana\u2013Champaign was founded in 1944 with the goal to improve the state of the art in home building, originally with an emphasis on standardization for cost reduction. It was there that the notion of the \"kitchen work triangle\" was formalized: the three main functions in a kitchen are storage, preparation, and cooking (which Catharine Beecher had already recognized), and the places for these functions should be arranged in the kitchen in such a way that work at one place does not interfere with work at another place, the distance between these places is not unnecessarily large, and no obstacles are in the way. A natural arrangement is a triangle, with the refrigerator, the sink, and the stove at a vertex each.\nThis observation led to a few common kitchen forms, commonly characterized by the arrangement of the kitchen cabinets and sink, stove, and refrigerator:\nIn the 1980s, there was a backlash against industrial kitchen planning and cabinets with people installing a mix of work surfaces and free standing furniture, led by kitchen designer Johnny Grey and his concept of the \"unfitted kitchen\". Modern kitchens often have enough informal space to allow for people to eat in it without having to use the formal dining room. Such areas are called \"breakfast areas\", \"breakfast nooks\" or \"breakfast bars\" if space is integrated into a kitchen counter. Kitchens with enough space to eat in are sometimes called \"eat-in kitchens\". During the 2000s, flat pack kitchens were popular for people doing DIY renovating on a budget. The flat pack kitchens industry makes it easy to put together and mix and matching doors, bench tops and cabinets. In flat pack systems, many components can be interchanged.\nIn larger homes, where the owners might have meals prepared by a household staff member, the home may have a \"chef's kitchen\". This typically differs from a normal domestic kitchen by having multiple ovens (possibly of different kinds for different kinds of cooking), multiple sinks, and warming drawers to keep food heated between cooking and service.\nOther types.\nRestaurant and canteen kitchens found in hotels, hospitals, educational and workplace facilities, army barracks, and similar institutions are generally (in developed countries) subject to public health laws. They are inspected periodically by public health officials and forced to close if they do not meet hygienic requirements mandated by law.\nCanteen kitchens (and castle kitchens) were often the places where new technology was used first. For instance, Benjamin Thompson's \"energy saving stove\", an early 19th-century fully closed iron stove using one fire to heat several pots, was designed for large kitchens; another thirty years passed before they were adapted for domestic use.\nAs of 2017, restaurant kitchens usually have tiled walls and floors and use stainless steel for other surfaces (workbench, but also door and drawer fronts) because these materials are durable and easy to clean. Professional kitchens are often equipped with gas stoves, as these allow cooks to regulate the heat more quickly and more finely than electrical stoves. Some special appliances are typical for professional kitchens, such as large installed deep fryers, steamers, or a bain-marie.\nThe fast food and convenience food trends have changed the manner in which restaurant kitchens operate. Some of these type restaurants may only \"finish\" convenience food that is delivered to them or just reheat completely prepared meals. At the most they may grill a hamburger or a steak. But in the early 21st century, c-stores (convenience stores) are attracting greater market share by performing more food preparation on-site and better customer service than some fast food outlets.\nThe kitchens in railway dining cars have presented special challenges: space is limited, and, personnel must be able to serve a great number of meals quickly. Especially in the early history of railways, this required flawless organization of processes; in modern times, the microwave oven and prepared meals have made this task much easier. Kitchens aboard ships, aircraft and sometimes railcars are often referred to as galleys. On yachts, galleys are often cramped, with one or two burners fueled by an LP gas bottle. Kitchens on cruise ships or large warships, by contrast, are comparable in every respect with restaurants or canteen kitchens.\nOn passenger airliners, the kitchen is reduced to a pantry. The crew's role is to heat and serve in-flight meals delivered by a catering company. An extreme form of the kitchen occurs in space, \"e.g.\", aboard a Space Shuttle (where it is also called the \"galley\") or the International Space Station. The astronauts' food is generally completely prepared, dehydrated, and sealed in plastic pouches before the flight. The kitchen is reduced to a rehydration and heating module.\nOutdoor areas where food is prepared are generally not considered kitchens, even though an outdoor area set up for regular food preparation, for instance when camping, might be referred to as an \"outdoor kitchen\". An outdoor kitchen at a campsite might be placed near a well, water pump, or water tap, and it might provide tables for food preparation and cooking (using portable camp stoves). Some campsite kitchen areas have a large tank of propane connected to burners so that campers can cook their meals. Military camps and similar temporary settlements of nomads may have dedicated kitchen tents, which have a vent to enable cooking smoke to escape.\nIn schools where home economics, food technology (previously known as \"domestic science\"), or culinary arts are taught, there are typically a series of kitchens with multiple equipment (similar in some respects to laboratories) solely for the purpose of teaching. These consist of multiple workstations, each with its own oven, sink, and kitchen utensils, where the teacher can show students how to prepare food and cook it.\nBy region.\nChina.\nKitchens in China are called . More than 3000 years ago, the ancient Chinese used the ding for cooking food. The ding was developed into the wok and pot used today. In Chinese spiritual tradition, a Kitchen God watches over the kitchen for the family and reports to the Jade Emperor annually about the family's behavior. On Chinese New Year's Eve, families would gather to pray for the kitchen god to give a good report to heaven and wish him to bring back good news on the fifth day of the New Year. \nThe most common cooking equipment in Chinese family kitchens and restaurant kitchens are woks, steamer baskets and pots. The fuel or heating resource was also an important technique to practice the cooking skills. Traditionally, Chinese were using wood or straw as the fuel to cook food. A Chinese chef had to master flaming and heat radiation to reliably prepare traditional recipes. Chinese cooking will use a pot or wok for pan-frying, stir-frying, deep frying or boiling.\nJapan.\nKitchens in Japan are called Daidokoro (\u53f0\u6240; lit. \"kitchen\"). Daidokoro is the place where food is prepared in a Japanese house. Until the Meiji era, a kitchen was also called \"kamado\" (\u304b\u307e\u3069; lit. stove) and there are many sayings in the Japanese language that involve kamado as it was considered the symbol of a house and the term could even be used to mean \"family\" or \"household\" (similar to the English word \"hearth\"). When separating a family, it was called \"Kamado wo wakeru\", which means \"divide the stove\". \"Kamado wo yaburu\" (lit. \"break the stove\") means that the family was bankrupt.\nIndia.\nIn India, a kitchen is called a \"Rasoi\" (in Hindi\\Sanskrit) or a \"Swayampak ghar\" in Marathi, and there exist many other names for it in the various regional languages. Many different methods of cooking exist across the country, and the structure and the materials used in constructing kitchens have varied depending on the region. For example, in the north and central India, cooking used to be carried out in clay ovens called \"chulha\" (also \"chullha\" or \"chullah\"), fired by wood, coal or dried cow dung. In households where members observed vegetarianism, separate kitchens were maintained to cook and store vegetarian and non-vegetarian food. Religious families often treat the kitchen as a sacred space. Indian kitchens are built on an Indian architectural science called vastushastra. The Indian kitchen vastu is of utmost importance while designing kitchens in India. Modern-day architects also follow the norms of vastushastra while designing Indian kitchens across the world.\nWhile many kitchens belonging to poor families continue to use clay stoves and the older forms of fuel, the urban middle and upper classes usually have gas stoves with cylinders or piped gas attached. Electric cooktops are rarer since they consume a great deal of electricity, but microwave ovens are gaining popularity in urban households and commercial enterprises. Indian kitchens are also supported by biogas and solar energy as fuel. World's largest solar energy kitchen is built in India. In association with government bodies, India is encouraging domestic biogas plants to support the kitchen system.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Sister-inline/styles.css\"/&gt; Media related to at Wikimedia Commons"}
{"id": "49407", "revid": "494861", "url": "https://en.wikipedia.org/wiki?curid=49407", "title": "Pliny the Younger", "text": "Roman lawyer, author and magistrate (61 \u2013 c. 113)\nGaius Plinius Caecilius Secundus (born Gaius Caecilius or Gaius Caecilius Cilo; 61 \u2013 c.\u2009113), better known in English as Pliny the Younger ( ), was a lawyer, author, and magistrate of Ancient Rome. Pliny's uncle, Pliny the Elder, helped raise and educate him.\nPliny the Younger wrote hundreds of letters, of which 247 survived, and which are of some historical value. These include 121 official memoranda addressed to Emperor Trajan (reigned 98-117). Some are addressed to reigning emperors or to notables such as the historian Tacitus. Pliny served as an imperial magistrate under Trajan, and his letters to Trajan provide one of the few surviving records of the relationship between the imperial office and provincial governors.\nPliny rose through a series of civil and military offices, the \"cursus honorum\". He was a friend of the historian Tacitus and might have employed the biographer Suetonius on his staff. Pliny also came into contact with other well-known men of the period, including the philosophers Artemidorus and Euphrates the Stoic, during his time in Syria.\nBackground.\nChildhood.\nPliny the Younger was born in \"Novum Comum\" (Como, Northern Italy) around 61 AD, the son of Lucius Caecilius Cilo, born there, and his wife Plinia Marcella, a sister of Pliny the Elder. He was the grandson of Senator and landowner Gaius Caecilius, revered his uncle, who at this time was extremely famous around the Roman Empire because of his intelligence, and provided sketches of how his uncle worked on the \"Naturalis Historia\".\nCilo died at an early age when Pliny was still young. As a result, the boy probably lived with his mother. His guardian and preceptor in charge of his education was Lucius Verginius Rufus, famed for quelling a revolt against Nero in 68 AD. After being first tutored at home, Pliny went to Rome for further education. There he was taught rhetoric by Quintilian, a great teacher and author, and Nicetes Sacerdos of Smyrna. It was at this time that Pliny became closer to his uncle Pliny the Elder. When Pliny the Younger was 17 or 18 in 79 AD, his uncle Pliny the Elder died attempting to rescue victims of the Vesuvius eruption, and the terms of the Elder Pliny's will passed his estate to his nephew. In the same document, the younger Pliny was adopted by his uncle. As a result, Pliny the Younger changed his name from \"Gaius Caecilius Cilo\" to \"Gaius Plinius Caecilius Secundus\" (his official title was \"Gaius Plinius Luci filius Caecilius Secundus\").\nThere is some evidence that Pliny had a sibling. A memorial erected in Como (now CIL http://) repeats the terms of a will by which the \"aedile\" Lucius Caecilius Cilo, son of Lucius, established a fund, the interest of which was to buy oil (used for soap) for the baths of the people of Como. The trustees are apparently named in the inscription: \"L. Caecilius Valens and P. Caecilius Secundus, sons of Lucius, and the \"contubernalis\" Lutulla.\" The word \"contubernalis\" describing Lutulla is the military term meaning \"tent-mate\", which can only mean that she was living with Lucius, not as his wife. The first man mentioned, L. Caecilius Valens, is probably the older son. Pliny the Younger confirms that he was a trustee for the largesse \"of my ancestors\". It seems unknown to Pliny the Elder, so Valens' mother was probably not his sister Plinia; perhaps Valens was Lutulla's son from an earlier relationship.\nMarriages.\nPliny the Younger married three times: first, when he was very young (about 18), to a stepdaughter of Veccius Proculus, who died at age 37; secondly, at an unknown date, to the daughter of Pompeia Celerina; and thirdly to Calpurnia who was 14 at the time and 26 years younger than Pliny, daughter of Calpurnius and granddaughter of Calpurnius Fabatus of Comum. Letters survive in which Pliny recorded this last marriage taking place, his attachment to Calpurnia, and his sadness when she miscarried their child at the age of 17.\nDeath.\nPliny is thought to have died suddenly during his convention in Bithynia-Pontus, around 113 AD, since no events referred to in his letters date later than that.\nCareer.\nPliny was by birth of equestrian rank, that is, a member of the aristocratic order of \"equites\" (knights), the lower (beneath the senatorial order) of the two Roman aristocratic orders that monopolised senior civil and military offices during the early Empire. His career began at the age of 18 and initially followed a normal equestrian route. But, unlike most equestrians, he achieved entry into the upper order by being elected Quaestor in his late twenties. (See Career summary below.)\nPliny was active in the Roman legal system, especially in the sphere of the Roman centumviral court, which dealt with inheritance cases. Later, he was a well-known prosecutor and defender at the trials of a series of provincial governors, including Baebius Massa, governor of Baetica; Marius Priscus, governor of Africa; Gaius Caecilius Classicus, governor of Baetica; and most ironically in light of his later appointment to this province, Gaius Julius Bassus and Varenus Rufus, both governors of Bithynia and Pontus.\nPliny's career is commonly considered as a summary of the main Roman public charges and is the best-documented example from this period, offering proof for many aspects of imperial culture. Effectively, Pliny crossed all the principal fields of the organization of the early Roman Empire. It is an achievement for a man to have not only survived the reigns of several disparate emperors, especially the much-detested Domitian, but also to have risen in rank throughout.\nWritings.\nPliny wrote his first work, a tragedy in Greek, at age 14. Additionally, in the course of his life, he wrote numerous poems, most of which are lost. He was also known as a notable orator; though he professed himself a follower of Cicero, Pliny's prose was more magniloquent and less direct than Cicero's.\nPliny's only oration that now survives is the \"Panegyricus Traiani\". This was delivered in the Senate in 100 and is a description of Trajan's figure and actions in an adulatory and emphatic form, especially contrasting him with the Emperor Domitian. It is, however, a relevant document that reveals many details about the Emperor's actions in several fields of his administrative power such as taxes, justice, military discipline, and commerce. Recalling the speech in one of his letters, Pliny shrewdly defines his own motives thus:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I hoped in the first place to encourage our Emperor in his virtues by a sincere tribute and, secondly, to show his successors what path to follow to win the same renown, not by offering instruction but by setting his example before them. To proffer advice on an Emperor's duties might be a noble enterprise, but it would be a heavy responsibility verging on insolence, whereas to praise an excellent ruler (\"optimum principem\") and thereby shine a beacon on the path posterity should follow would be equally effective without appearing presumptuous.\n\"Epistulae\".\nThe largest surviving body of Pliny's work is his 247 \"Epistulae\", letters to his friends and associates. These letters are a unique testimony of Roman administrative history and everyday life in the 1st century AD. Especially noteworthy among the letters are two in which he describes the eruption of Mount Vesuvius on August or October 24 in AD 79, during which his uncle Pliny the Elder died (\"Epistulae\" VI.16, VI.20), and one in which he asks the Emperor for instructions regarding official policy concerning Christians (\"Epistulae\" X.96).\nEpistles concerning the eruption of Mount Vesuvius.\nPliny wrote the two letters describing the eruption of Mount Vesuvius approximately 25 years after the event, and both were sent in response to the request of his friend, the historian Tacitus. The first letter outlines the events preceding the death of Pliny the Elder during the attempted rescue of his friend Rectina. The second letter details the Younger's movements across the same period of time. The two letters have great historical value due to their accurate description of the Vesuvius eruption; Pliny's attention to detail in the letters about Vesuvius is so keen that modern volcanologists describe those types of eruptions as \"Plinian eruptions\".\nEpistle concerning the Christian religion.\n As the Roman governor of Bithynia-Pontus (now in modern Turkey) Pliny wrote a letter to Emperor Trajan around 112 AD and asked for counsel on dealing with Christians. In the letter (\"Epistulae\" X.96), Pliny detailed an account of how he conducted trials of suspected Christians who appeared before him as a result of anonymous accusations and asked for the Emperor's guidance on how they should be treated. Pliny had never performed a legal investigation of Christians and thus consulted Trajan in order to be on solid ground regarding his actions. Pliny saved his letters and Trajan's replies and these are the earliest surviving Roman documents to refer to early Christians.\nEpistle concerning voting systems.\nVoting theorists and historians of social choice note Pliny's early mention of how the choice of voting procedure could influence the outcome of an election. On June 24, 105, Pliny wrote a letter to Titius Aristo, where he describes a criminal trial: under the traditional rules of the Senate, there would first be a vote on guilt and then (if the accused were found guilty) on punishment, for which execution and exile were proposed. Of the three distinct proposals, acquittal, exile, and execution, acquittal had the largest number of supporters but not a majority, although exile would have defeated either acquittal or execution in a direct two-way vote. Pliny supported acquittal but anticipated that first guilt and then execution would be chosen under the traditional rules, and so he argued for a novel three-way plurality vote, which would have resulted in acquittal. In response, those in favor of execution withdrew their proposal, the vote defaulted to a traditional majority vote between exile and acquittal, and exile carried.\nManuscripts.\nThe first edition of Pliny's \"Epistles\" was published in Italy in 1471. Sometime between 1495 and 1500 Giovanni Giocondo discovered a manuscript in Paris of Pliny's tenth book of letters, containing his correspondence with Trajan, and published it in Paris, dedicating the work to Louis XII. The first complete edition was produced by the press of Aldus Manutius in 1508. (See \"Editio princeps\" for details.)\nVillas, farms and estates.\nBeing wealthy, Pliny owned many villas and wrote in detail about his villa near Ostia, at Laurentum, Italy. Others included one near Lake Como named \"Tragedy\" because of its location high on a hill, and, another, \"Comedy,\" on the shores of the lake, so called because it was sited low down, referencing the practice of actors in comedy wearing flat shoes, while those in tragedy wore high-heeled buskins.\nPliny's main estate in Italy and the one he loved best was his Villa \"in Tuscis\" near San Giustino, Umbria, under the passes of Bocca Trabaria and Bocca Serriola, where wood was harvested for Roman ships and sent to Rome via the Tiber.\nAs a response to \"declining returns from his north Italian farms\", Pliny may have contemplated switching the administration of his estate to a sharecropping system called \"colonia partiaria\". Under the sharecropping system, Pliny's slaves would act as overseers.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49411", "revid": "86247", "url": "https://en.wikipedia.org/wiki?curid=49411", "title": "Victor von Doom", "text": ""}
{"id": "49414", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=49414", "title": "Sex-determination system", "text": "Biological system that determines the development of an organism's sex\nA sex-determination system is a biological system that determines the development of the organism's sex. Most organisms that create their offspring using sexual reproduction have two common sexes, males and females, and in other species there are hermaphrodites, organisms that can function reproductively as either female or male, or both.\nThere are also some species in which only one sex is present, temporarily or permanently. This can be due to parthenogenesis, the act of a female reproducing without fertilization, mostly seen in plant species. In some plants or algae the gametophyte stage may reproduce itself, thus producing more individuals of the same sex as the parent.\nIn some species, sex determination is genetic: males and females have different alleles or even different genes that specify their sexual morphology. In animals this is often accompanied by chromosomal differences, generally through combinations of XY, ZW, XO, ZO chromosomes, or haplodiploidy. The sexual differentiation is generally triggered by a main gene (a \"sex locus\"), with a multitude of other genes following in a domino effect.\nIn other cases, the sex of a fetus is determined by environmental variables (such as temperature). The details of some sex-determination systems are not yet fully understood.\nSome species such as various plants and fish do not have a fixed sex and instead go through life cycles and change sex based on genetic cues during corresponding life stages of their type. This could be due to environmental factors such as seasons and temperature. In some gonochoric species, a few individuals may have conditions that cause a mix of different sex characteristics.\nDiscovery.\nThe scientific understanding of sex determination has evolved significantly over the centuries through a series of landmark discoveries across plants, animals, and insects.\nIn 1694, German botanist Rudolf Jakob Camerarius conducted pioneering experiments on pollination, during which he identified the existence of distinct male and female reproductive structures in plants, including maize. His findings laid the groundwork for understanding plant reproduction and sexual differentiation .\nIn 1866, Gregor Mendel, an Austrian monk and scientist, published his seminal work on the inheritance of traits in pea plants. These findings\u2014now known as Mendelian inheritance\u2014introduced the concept of heritable units (genes) passed from two gametes. Mendel's principles eventually became the foundation of modern genetics.\nIn 1902, American zoologist C.E. McClung proposed that sex chromosomes played a central role in determining sex, based on cytological studies in insects. This was one of the earliest steps toward a chromosomal theory of sex determination.\nIn 1903, American geneticist Nettie Stevens made a groundbreaking discovery while studying the mealworm (Tenebrio molitor), demonstrating that sex is determined by specific chromosomes, now known as X and Y chromosomes. Her findings provided the first concrete evidence for chromosomal sex determination.\nIn 1917, botanist Charles Elmer Allen extended this understanding to the plant kingdom by discovering sex chromosomes in plants, confirming that the chromosomal mechanism is not exclusive to animals.\nIn 1922, geneticist Calvin B. Bridges introduced the Genic Balance Theory, based on experiments with Drosophila melanogaster (fruit flies). He proposed that the ratio of X chromosomes to sets of autosomes determines sexual development, adding complexity to the previously accepted XX/XY system.\nFinally, in 1928, Swiss biologist Fritz Baltzer was the first to describe environmental sex determination, showing that external environmental factors, such as temperature, could influence the development of sex in certain organisms. This discovery expanded the understanding of sex determination beyond purely genetic mechanisms.\nChromosomal systems.\nAmong animals, the most common chromosomal sex determination systems are XY, XO, ZW, ZO, but with numerous exceptions.\nAccording to the Tree of Sex database (as of 2023), the known sex determination systems are:\n1. complex sex chromosomes, homomorphic sex chromosomes, or others\nXX/XY sex chromosomes.\nThe XX/XY sex-determination system is perhaps the most familiar as it is found in humans and most other mammals, as well as in some insect species. In the XX/XY system, karyotypic females usually have two X chromosomes (XX), while karyotypic males usually have a single X and a single Y chromosome (XY). The X and Y sex chromosomes are different in shape and size from each other, unlike other chromosome pairs (autosomes), and are sometimes called allosomes. In some species, including humans, individuals remain phenotypically undifferentiated for some time during development (embryogenesis); in others, however, such as fruit flies, sexual differentiation occurs as soon fertilization occurs.\nY-centered sex determination.\nSome species (including humans) have a gene SRY on the Y chromosome that triggers development of the male phenotype. Members of SRY-reliant species can also have other chromosomal combinations such as XXY. In humans, karyotypic sex is generally determined by the presence or absence of a Y chromosome with a functional SRY gene. If the SRY gene is present and activated during fetal development, cells create testosterone and anti-m\u00fcllerian hormone which typically leads to male phenotypic development. In embryos that lack a functioning SRY gene, such as most XX individuals, the individual develops phenotypically female.\nIn Y-centered sex determination, the SRY gene is the gene that triggers male phenotype development, however multiple genes are required for this process to complete. In XY mice, lack of the gene DAX1 on the X chromosome results in sterility, but in humans it causes adrenal hypoplasia congenita. However, when an extra DAX1 gene is placed on the X chromosome, the result is phenotypically female, despite the existence of SRY, since it overrides the effects of SRY. Even when there are functional X chromosomes in XX females, duplication or expression of SOX9 causes testes to develop. Gradual sex reversal in developed mice can also occur when the gene FOXL2 is removed from females. Even though the gene DMRT1 is used by birds as their sex locus, species who have XY chromosomes also rely upon DMRT1, contained on chromosome 9, for sexual differentiation at some point in their formation.\nX-centered sex determination.\nSome species, such as fruit flies, use the presence of two X chromosomes to determine femaleness. Species that use the number of Xs to determine sex are nonviable with an extra X chromosome.\nOther variants of XX/XY sex determination.\nSome fish have variants of the XY sex-determination system, as well as the regular system. For example, while having an XY format, \"Xiphophorus nezahualcoyotl\" and \"X. milleri\" also have a second Y chromosome, known as Y', that creates XY' females and YY' males.\nAt least one monotreme, the platypus, presents a particular sex determination scheme that in some ways resembles that of the ZW sex chromosomes of birds and lacks the SRY gene. The platypus has sex chromosomes formula_1. The males have formula_2, while females have formula_3. During meiosis, 5 of X form one chain, and 5 of Y form another chain. Thus, they behave effectively as a typical XY chromosomal system, except each of X and Y is broken into 5 parts, with the effect that recombinations occur very frequently at 4 particular points. One of the X chromosomes is homologous to the human X chromosome, and another is homologous to the bird Z chromosome.\nAlthough it is an XY system, the platypus' sex chromosomes share no homologues with eutherian sex chromosomes. Instead, homologues with eutherian sex chromosomes lie on the platypus chromosome 6, which means that the eutherian sex chromosomes were autosomes at the time that the monotremes diverged from the therian mammals (marsupials and eutherian mammals). However, homologues to the avian DMRT1 gene on platypus sex chromosomes X3 and X5 suggest that it is possible the sex-determining gene for the platypus is the same one that is involved in bird sex-determination. More research must be conducted in order to determine the exact sex determining gene of the platypus.\nXX/X0 sex chromosomes.\nIn this variant of the XY system, females have two copies of the sex chromosome (XX) but males have only one (X0). The \"0\" denotes the absence of a second sex chromosome. Generally in this method, the sex is determined by amount of genes expressed across the two chromosomes. This system is observed in a number of insects, including the grasshoppers and crickets of order Orthoptera and in cockroaches (order Blattodea). A small number of mammals also lack a Y chromosome. These include the Amami spiny rat (\"Tokudaia osimensis\") and the Tokunoshima spiny rat (\"Tokudaia tokunoshimensis\") and \"Sorex araneus\", a shrew species. Transcaucasian mole voles (\"Ellobius lutescens\") also have a form of XO determination, in which both sexes lack a second sex chromosome. The mechanism of sex determination is not yet understood.\nThe nematode \"C. elegans\" is male with one sex chromosome (X0); with a pair of chromosomes (XX) it is a hermaphrodite. Its main sex gene is XOL, which encodes XOL-1 and also controls the expression of the genes TRA-2 and HER-1. These genes reduce male gene activation and increase it, respectively.\nZW/ZZ sex chromosomes.\nThe ZW sex-determination system is found in birds, some reptiles, and some insects and other organisms. The ZW sex-determination system is reversed compared to the XY system: females have two different kinds of chromosomes (ZW), and males have two of the same kind of chromosomes (ZZ). In the chicken, this was found to be dependent on the expression of DMRT1. In birds, the genes FET1 and ASW are found on the W chromosome for females, similar to how the Y chromosome contains SRY. However, not all species depend upon the W for their sex. For example, there are moths and butterflies that are ZW, but some have been found female with ZO, as well as female with ZZW. Also, while mammals deactivate one of their extra X chromosomes when female, it appears that in the case of Lepidoptera, the males produce double the normal amount of enzymes, due to having two Z's. Because the use of ZW sex determination is varied, it is still unknown how exactly most species determine their sex. However, reportedly, the silkworm \"Bombyx mori\" uses a single female-specific piRNA as the primary determiner of sex. Despite the similarities between the ZW and XY systems, these sex chromosomes evolved separately. In the case of the chicken, their Z chromosome is more similar to humans' autosome 9. The chicken's Z chromosome also seems to be related to the X chromosome of the platypus. When a ZW species, such as the Komodo dragon, reproduces parthenogenetically, usually only males are produced. This is due to the fact that the haploid eggs double their chromosomes, resulting in ZZ or WW. The ZZ become males, but the WW are not viable and are not brought to term.\nIn both XY and ZW sex determination systems, the sex chromosome carrying the critical factors is often significantly smaller, carrying little more than the genes necessary for triggering the development of a given sex.\nZZ/Z0 sex chromosomes.\nThe ZZ/Z0 sex-determination system is found in some moths. In these insects there is one sex chromosome, Z. Males have two Z chromosomes, whereas females have one Z. Males are ZZ, while females are Z0.\nUV sex chromosomes.\nIn some bryophyte and some algae species, the gametophyte stage of the life cycle, rather than being hermaphrodite, occurs as separate male or female individuals that produce male and female gametes respectively. When meiosis occurs in the sporophyte generation of the life cycle, the sex chromosomes known as U and V assort in spores that carry either the U chromosome and give rise to female gametophytes, or the V chromosome and give rise to male gametophytes.\nMating types.\nThe mating type in microorganisms is analogous to sex in multi-cellular organisms, and is sometimes described using those terms, though they are not necessarily correlated with physical body structures. Some species have more than two mating types. \"Tetrahymena,\" a type of ciliate, has 7 mating types\nMating types are extensively studied in fungi. Among fungi, mating type is determined by chromosomal regions called mating-type loci. Furthermore, it is not as simple as \"two different mating types can mate\", but rather, a matter of combinatorics. As a simple example, most \"basidiomycete\" have a \"tetrapolar heterothallism\" mating system: there are two loci, and mating between two individuals is possible if the alleles on \"both\" loci are different. For example, if there are 3 alleles per locus, then there would be 9 mating types, each of which can mate with 4 other mating types. By multiplicative combination, it generates a vast number of mating types. For example, \"Schizophyllum commune,\" a type of fungus, has formula_4 mating types.\nHaplodiploidy.\nHaplodiploidy is found in insects belonging to Hymenoptera, such as ants and bees. Sex determination is controlled by the zygosity of a complementary sex determiner (\"csd\") locus. Unfertilized eggs develop into haploid individuals which have a single, hemizygous copy of the \"csd\" locus and are therefore males. Fertilized eggs develop into diploid individuals which, due to high variability in the \"csd\" locus, are generally heterozygous females. In rare instances diploid individuals may be homozygous, these develop into sterile males.\nThe gene acting as a \"csd\" locus has been identified in the honeybee and several candidate genes have been proposed as a \"csd\" locus for other Hymenopterans.\nMost females in the Hymenoptera order can decide the sex of their offspring by holding received sperm in their spermatheca and either releasing it into their oviduct or not. This allows them to create more workers, depending on the status of the colony.\nPolygenic sex determination.\nPolygenic sex determination is when the sex is primarily determined by genes that occur on multiple non-homologous chromosomes. The environment may have a limited, minor influence on sex determination. Examples include African cichlid fish (\"Metriaclima spp.\"), lemmings (\"Myopus schisticolor\"), green swordtail, medaka, etc. In such systems, there is typically a dominance hierarchy, where one system is dominant over another if in conflict. For example, in some species of cichlid fish from Lake Malawi, if an individual has both the XY locus (on one chromosome pair) and the WZ locus (on another chromosome pair), then the W is dominant and the individual has a female phenotype.\nThe sex-determination system of zebrafish is polygenic. Juvenile zebrafishes (0\u201330 days after hatching) have both ovary-like tissue to testis tissue. They then develop into male or female adults, with the determination based on a complex interaction genes on multiple chromosomes, but not affected by environmental variations.\nOther chromosomal systems.\nIn systems with two sex chromosomes, they can be heteromorphic or homomorphic. Homomorphic sex chromosomes are almost identical in size and gene content. The two familiar kinds of sex chromosome pairs (XY and ZW) are heteromorphic. Homomorphic sex chromosomes exist among pufferfish, ratite birds, pythons, and European tree frogs. Some are quite old, meaning that there is some evolutionary force that resists their differentiation. For example, three species of European tree frogs have homologous, homomorphic sex chromosomes, and this homomorphism was maintained for at least 5.4 million years by occasional recombination.\nThe \"Nematocera\", particularly the \"Simuliids\" and \"Chironomus\", have sex determination regions that are labile, meaning that one species may have the sex determination region in one chromosome, but a closely related species might have the same region moved to a different non-homologous chromosome. Some species even have the sex determination region different among individuals \"within\" \"the same species\" (intraspecific variation). In some species, some populations have homomorphic sex chromosomes while other populations have heteromorphic sex chromosomes.\nThe New Zealand frog, \"Leiopelma hochstetteri\", uses a supernumerary sex chromosome. With zero of that chromosome, the frog develops into a male. With one or more, the frog develops into a female. One female had as many as 16 of that chromosome.\nDifferent populations of the Japanese frog \"Rana rugosa\" uses different systems. Two use homomorphic male heterogamety, one uses XX/XY, one uses ZZ/ZW. Remarkably, the X and Z chromosomes are homologous, and the Y and W as well. \"Dmrt1\" is on autosome 1 and not sex-linked. This means that an XX female individual is genetically similar to a ZZ male individual, and an XY male individual is to a ZW female individual. The mechanism behind this is yet unclear, but it is hypothesized that during its recent evolution, the XY-to-ZW transition occurred twice.\n\"Clarias gariepinus\" uses both XX/XY and ZW/ZZ system within the species, with some populations using homomorphic XX/XY while others using heteromorphic ZW/ZZ. A population in Thailand appears to use both systems simultaneously, possibly because \"C. gariepinus\" were not native to Thailand, and were introduced from different source populations which resulted in a mixture.\nMultiple sex chromosomes like those of platypus also occurs in bony fish. Some moths and butterflies have formula_5 or formula_6.\nThe Southern platyfish has a complex sex determination system involving 3 sex chromosomes and 4 autosomal alleles.\n\"Gastrotheca pseustes\" has formula_7 C-banding heteromorphism, meaning that both males and females have XY chromosomes, but their Y chromosomes are different on one or more C-bands. \"Eleutherodactylus maussi\" has a formula_8 system.\nEvolution.\nSee for a review.\nOrigin of sex chromosomes.\nSexual chromosome pairs can arise from an autosomal pair that, for various reasons, stopped recombination, allowing for their divergence. The rate at which recombination is suppressed, and therefore the rate of sex chromosome divergence, is very different across clades.\nIn analogy with geological strata, historical events in the evolution of sex chromosomes are called evolutionary strata. The human Y-chromosome has had about 5 strata since the origin of the X and Y chromosomes about 300 Mya from a pair of autosomes. Each stratum was formed when a pseudoautosomal region (PAR) of the Y chromosome is inverted, stopping it from recombination with the X chromosome. Over time, each inverted region decays, possibly due to Muller's ratchet. Primate Y-chromosome evolution was rapid, with multiple inversions and shifts of the boundary of PAR.\nAmong many species of the salamanders, the two chromosomes are only distinguished by a pericentric inversion, so that the banding pattern of the X chromosome is the same as that of Y, but with a region near the centromere reversed. (fig 7 ) In some species, the X is pericentrically inverted and the Y is ancestral. In other species it is the opposite. (p.\u00a015 )\nThe gene content of the X chromosome is almost identical among placental mammals. This is hypothesized to be because the X inactivation means any change would cause serious disruption, thus subjecting it to strong purifying selection. Similarly, birds have highly conserved Z chromosomes.\nNeo-sex chromosomes.\nNeo-sex chromosomes are currently existing sex chromosomes that formed when an autosome pair fused to the previously existing sex chromosome pair. Following this fusion, the autosomal portion undergoes recombination suppression, allowing them to differentiate. Such systems have been observed in insects, reptiles, birds, and mammals. They are useful to the study of the evolution of Y chromosome degeneration and dosage compensation.\nSex-chromosome turnover.\nThe sex-chromosome turnover is an evolutionary phenomenon where sex chromosomes disappear or become autosomal, and autosomal chromosomes become sexual, repeatedly over evolutionary time. Some lineages have extensive turnover, but others don't. Generally, in an XY system, if the Y chromosome is degenerate, mostly different from the X chromosome, and has X dosage compensation, then turnover is unlikely. In particular, this applies to humans.\nThe ZW and XY systems can evolve into to each other due to sexual conflict.\nHomomorphism and the fountain of youth.\nIt is an evolutionary puzzle why certain sex chromosomes remain homomorphic over millions of years, especially among lineages of fishes, amphibians, and nonavian reptiles. The fountain-of-youth model states that heteromorphy results from recombination suppression, and recombination suppression results from the male phenotype, not the sex chromosomes themselves. Therefore, if some XY sex-reversed females are fertile and adaptive under some circumstances, then the X and Y chromosomes would recombine in these individuals, preventing Y chromosome decay and maintaining long-term homomorphism.\nSex reversal denotes a situation where the phenotypic sex is different from the genotypic sex. While in humans, sex reversal (such as the XX male syndrome) are often infertile, sex-reversed individuals of some species are fertile under some conditions. For example, some XY-individuals in population of Chinook salmon in the Columbia River became fertile females, producing YY sons. Since Chinook salmons have homomorphic sex chromosomes, such YY sons are healthy. When YY males mate with XX females, all their progeny would be XY male if grown under normal conditions.\nSupport for the hypothesis is found in the common frog, for which XX males and XY males both suppresses sex chromosome recombination, but XX and XY females both recombine at the same rate.\nEnvironmental systems.\nTemperature-dependent.\nMany other sex-determination systems exist. In some species of reptiles, including alligators, some turtles, and the tuatara, sex is determined by the temperature at which the egg is incubated during a temperature-sensitive period. There are no examples of temperature-dependent sex determination (TSD) in birds. Megapodes had formerly been thought to exhibit this phenomenon, but were found to actually have different temperature-dependent embryo mortality rates for each sex. For some species with TSD, sex determination is achieved by exposure to hotter temperatures resulting in the offspring being one sex and cooler temperatures resulting in the other. This type of TSD is called \"Pattern\u00a0I\". For others species using TSD, it is exposure to temperatures on both extremes that results in offspring of one sex, and exposure to moderate temperatures that results in offspring of the opposite sex, called \"Pattern\u00a0II\" TSD. The specific temperatures required to produce each sex are known as the female-promoting temperature and the male-promoting temperature. When the temperature stays near the threshold during the temperature sensitive period, the sex ratio is varied between the two sexes. Some species' temperature standards are based on when a particular enzyme is created. These species that rely upon temperature for their sex determination do not have the SRY gene, but have other genes such as DAX1, DMRT1, and SOX9 that are expressed or not expressed depending on the temperature. The sex of some species, such as the Nile tilapia, Australian skink lizard, and Australian dragon lizard, has an initial bias, set by chromosomes, but can later be changed by the temperature of incubation.\nIt is unknown how exactly temperature-dependent sex determination evolved. It could have evolved through certain sexes being more suited to certain areas that fit the temperature requirements. For example, a warmer area could be more suitable for nesting, so more females are produced to increase the amount that nest next season. \nIn amniotes, environmental sex determination preceded the genetically determined systems of birds and mammals; it is thought that a temperature-dependent amniote was the common ancestor of amniotes with sex chromosomes.\nOther environmental systems.\nThere are other environmental sex determination systems including location-dependent determination systems as seen in the marine worm \"Bonellia viridis\" \u2013 larvae become males if they make physical contact with a female, and females if they end up on the bare sea floor. This is triggered by the presence of a chemical produced by the females, bonellin. Some species, such as some snails, practice sex change: adults start out male, then become female. In tropical clownfish, the dominant individual in a group becomes female while the other ones are male, and bluehead wrasses (\"Thalassoma bifasciatum\") are the reverse. \nClownfish live in colonies of several small undifferentiated fish and two large fish (male and female). The male and female are the only sexually mature fish to reproduce. Clownfish are protandrous hermaphrodites, which means after they mature into males, they eventually can transform into females. They develop undifferentiated until they are needed to fill a certain role in their environment, i.e., if they receive the social and environmental cues to do so.\nSome species, however, have no sex-determination system. Hermaphrodite species include the common earthworm and certain species of snails. A few species of fish, reptiles, and insects reproduce by parthenogenesis and are female altogether. There are some reptiles, such as the boa constrictor and Komodo dragon that can reproduce both sexually and asexually, depending on whether a mate is available.\nOthers.\nThere are exceptional sex-determination systems, neither genetic nor environmental.\nCytoplasmic sex determination.\nThe \"Wolbachia\" genus of parasitic bacteria lives inside the cytoplasm of its host, and is vertically transmitted from parents to children. They primarily infect arthropods and nematodes. Different \"Wolbachia\" can alter the reproductive abilities of its host by a variety of means, including cytoplasmic incompatibility, parthenogenesis, feminization and embryonic male killing.\n\"Mitochondrial male sterility\": In many flowering plants, the mitochondria can cause hermaphrodite individuals to be unable to father offspring, effectively turning them into exclusive females. This is a form of mother's curse. It is an evolutionarily adaptive strategy for mitochondria as mitochondria are inherited exclusively from mother to offspring. The first published case of mitochondrial male sterility among metazoans was reported in 2022 in the hermaphroditic snail \"Physa acuta\".\nPaternal genome elimination.\nIn some species of insects, springtails and mites, male offspring lose their paternal genome (in whole or in part) during development or in the germline. Males can either be diploid, diploid with missing sex chromosome, functionally haploid or truly haploid, depending on the mechanism of elimination.\nMonogeny.\nIn some species of Hymenoptera (ants, bees and wasps), flies and crustaceans, all offspring of a particular individual female are either exclusively male or exclusively female. The underlying mechanisms are diverse and include maternally controlled paternal genome elimination and Mendelian inherited maternal sex-determining factors.\nEvolution.\nSex determination systems may have evolved from mating type, which is a feature of microorganisms.\nChromosomal sex determination may have evolved early in the history of eukaryotes. But in plants it has been suggested to have evolved recently.\nThe accepted hypothesis of XY and ZW sex chromosome evolution in amniotes is that they evolved at the same time, in two different branches.\nNo genes are shared between the avian ZW and mammal XY chromosomes and the chicken Z chromosome is similar to the human autosomal chromosome 9, rather than X or Y. This suggests not that the ZW and XY sex-determination systems share an origin but that the sex chromosomes are derived from autosomal chromosomes of the common ancestor of birds and mammals. In the platypus, a monotreme, the X1 chromosome shares homology with therian mammals, while the X5 chromosome contains an avian sex-determination gene, further suggesting an evolutionary link.\nHowever, there is some evidence to suggest that there could have been transitions between ZW and XY, such as in \"Xiphophorus maculatus\", which have both ZW and XY systems in the same population, despite the fact that ZW and XY have different gene locations. A recent theoretical model raises the possibility of both transitions between the XY/XX and ZZ/ZW system and environmental sex determination The platypus' genes also back up the possible evolutionary link between XY and ZW, because they have the DMRT1 gene possessed by birds on their X chromosomes. Regardless, XY and ZW follow a similar route. All sex chromosomes started out as an original autosome of an original amniote that relied upon temperature to determine the sex of offspring. After the mammals separated, the reptile branch further split into Lepidosauria and Archosauromorpha. These two groups both evolved the ZW system separately, as evidenced by the existence of different sex chromosomal locations. In mammals, one of the autosome pair, now Y, mutated its SOX3 gene into the SRY gene, causing that chromosome to designate sex. After this mutation, the SRY-containing chromosome inverted and was no longer completely homologous with its partner. The regions of the X and Y chromosomes that are still homologous to one another are known as the pseudoautosomal region. Once it inverted, the Y chromosome became unable to remedy deleterious mutations, and thus degenerated. There is some concern that the Y chromosome will shrink further and stop functioning in ten million years: but the Y chromosome has been strictly conserved after its initial rapid gene loss.\nThere are some vertebrate species, such as the medaka fish, that evolved sex chromosomes separately; their Y chromosome never inverted and can still swap genes with the X. These species' sex chromosomes are relatively primitive and unspecialized. Because the Y does not have male-specific genes and can interact with the X, XY and YY females can be formed as well as XX males. Non-inverted Y chromosomes with long histories are found in pythons and emus, each system being more than 120 million years old, suggesting that inversions are not necessarily an eventuality. XO sex determination can evolve from XY sex determination with about 2 million years.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49415", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=49415", "title": "Wikipedia feature request", "text": ""}
{"id": "49416", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=49416", "title": "NMOS logic", "text": "Form of digital logic family in integrated circuits\nNMOS or nMOS logic (from N-type metal\u2013oxide\u2013semiconductor) uses n-type (-) MOSFETs (metal\u2013oxide\u2013semiconductor field-effect transistors) to implement logic gates and other digital circuits. \nNMOS transistors operate by creating an inversion layer in a p-type transistor body. This inversion layer, called the n-channel, can conduct electrons between n-type \"source\" and \"drain\" terminals. The n-channel is created by applying voltage to the third terminal, called the \"gate\". Like other MOSFETs, nMOS transistors have four modes of operation: cut-off (or subthreshold), triode, saturation (sometimes called active), and velocity saturation.\nNMOS AND-by-default logic can produce unusual glitches or buggy behavior in NMOS components, such as the 6502 \"illegal opcodes\" which are absent in CMOS 6502s. In some cases such as Commodore's VIC-II chip, the bugs present in the chip's logic were extensively exploited by programmers for graphics effects.\nFor many years, NMOS circuits were much faster than comparable PMOS and CMOS circuits, which had to use much slower p-channel transistors. It was also easier to manufacture NMOS than CMOS, as the latter has to implement p-channel transistors in special n-wells on the p-substrate, not prone to damage from bus conflicts, and not as vulnerable to electrostatic discharge damage. The major drawback with NMOS (and most other logic families) is that a direct current must flow through a logic gate even when the output is in a steady state (low in the case of NMOS). This means static power dissipation, i.e. power drain even when the circuit is not switching, leading to high power consumption.\nAnother disadvantage of NMOS circuits is their thermal output. Due to the need to keep constant current running through the circuit to hold the transistors' states, NMOS circuits can generate a considerable amount of heat in operation which can reduce the device's reliability. This was especially problematic with the early large gate process nodes in the 1970s. CMOS circuits, for contrast, generate almost no heat unless the transistor count approaches 1 million.\nCMOS components were relatively uncommon in the 1970s-early 1980s and would typically be indicated with a \"C\" in the part number. Throughout the 1980s, both NMOS and CMOS parts were widely used with CMOS becoming more widespread as the decade went along. NMOS was preferred for components that performed active processing such as CPUs or graphics processors due to its higher speed and cheaper manufacturing cost as these were expensive compared to a passive component such as a memory chip, and some chips such as the Motorola 68030 were hybrids with both NMOS and CMOS sections. CMOS has been near-universal in integrated circuits since the 1990s.\nAdditionally, just like in diode\u2013transistor logic, transistor\u2013transistor logic, emitter-coupled logic etc., the asymmetric input logic levels make NMOS and PMOS circuits more susceptible to noise than CMOS. These disadvantages are why CMOS logic has supplanted most of these types in most high-speed digital circuits such as microprocessors despite the fact that CMOS was originally very slow compared to logic gates built with bipolar transistors.\nOverview.\nMOS stands for \"metal-oxide-semiconductor\", reflecting the way MOS-transistors were originally constructed, predominantly before the 1970s, with gates of metal, typically aluminium. Since around 1970, however, most MOS circuits have used self-aligned gates made of polycrystalline silicon, a technology first developed by Federico Faggin at Fairchild Semiconductor. These silicon gates are still used in most types of MOSFET based integrated circuits, although metal gates (Al or Cu) started to reappear in the early 2000s for certain types of high speed circuits, such as high performance microprocessors.\nThe MOSFETs are n-type enhancement mode transistors, arranged in a so-called \"pull-down network\" (PDN) between the logic gate output and negative supply voltage (typically the ground). A pull up (i.e. a \"load\" that can be thought of as a resistor, see below) is placed between the positive supply voltage and each logic gate output. Any logic gate, including the logical inverter, can then be implemented by designing a network of parallel and/or series circuits, such that if the desired output for a certain combination of boolean input values is zero (or false), the PDN will be active, meaning that at least one transistor is allowing a current path between the negative supply and the output. This causes a voltage drop over the load, and thus a low voltage at the output, representing the \"zero.\"\nAs an example, here is a NOR gate implemented in schematic NMOS. If either input A or input B is high (logic 1, = True), the respective MOS transistor acts as a very low resistance between the output and the negative supply, forcing the output to be low (logic 0, = False). When both A and B are high, both transistors are conductive, creating an even lower resistance path to ground. The only case where the output is high is when both transistors are off, which occurs only when both A and B are low, thus satisfying the truth table of a NOR gate:\nA MOSFET can be made to operate as a resistor, so the whole circuit can be made with n-channel MOSFETs only. NMOS circuits are slow to transition from low to high. When transitioning from high to low, the transistors provide low resistance, and the capacitive charge at the output drains away very quickly (similar to discharging a capacitor through a very low resistor). But the resistance between the output and the positive supply rail is much greater, so the low to high transition takes longer (similar to charging a capacitor through a high value resistor). Using a resistor of lower value will speed up the process but also increases static power dissipation. However, a better (and the most common) way to make the gates faster is to use depletion-mode transistors instead of enhancement-mode transistors as loads. This is called depletion-load NMOS logic.\nHistory.\nThe MOSFET was invented by Egyptian engineer Mohamed M. Atalla and Korean engineer Dawon Kahng at Bell Labs in 1959, and demonstrated in 1960. They fabricated both PMOS and NMOS devices with a 20\u03bcm process. However, the NMOS devices were impractical, and only the PMOS type were practical devices.\nIn 1965, Chih-Tang Sah, Otto Leistiko and Andrew Grove at Fairchild Semiconductor fabricated several NMOS devices with channel lengths between 8\u03bcm and 65\u03bcm. Dale L. Critchlow and Robert H. Dennard at IBM also fabricated NMOS devices in the 1960s. The first IBM NMOS product was a memory chip with 1kb data and 50\u2013100 ns access time, which entered large-scale manufacturing in the early 1970s. This led to MOS semiconductor memory replacing earlier bipolar and ferrite-core memory technologies in the 1970s.\nThe earliest microprocessors in the early 1970s were PMOS processors, which initially dominated the early microprocessor industry. In 1973, NEC's \u03bcCOM-4 was an early NMOS microprocessor, fabricated by the NEC LSI team, consisting of five researchers led by Sohichi Suzuki. By the late 1970s, NMOS microprocessors had overtaken PMOS processors. CMOS microprocessors were introduced in 1975. However, CMOS processors did not become dominant until the 1980s.\nCMOS was initially slower than NMOS logic, thus NMOS was more widely used for computers in the 1970s. The Intel 5101 (1kb SRAM) CMOS memory chip (1974) had an access time of 800ns, whereas the fastest NMOS chip at the time, the Intel 2147 (4kb SRAM) HMOS memory chip (1976), had an access time of 55/70ns. In 1978, a Hitachi research team led by Toshiaki Masuhara introduced the twin-well Hi-CMOS process, with its HM6147 (4kb SRAM) memory chip, manufactured with a 3 \u03bcm process. The Hitachi HM6147 chip was able to match the performance (55/70ns access) of the Intel 2147 HMOS chip, while the HM6147 also consumed significantly less power (15mA) than the 2147 (110mA). With comparable performance and much less power consumption, the twin-well CMOS process eventually overtook NMOS as the most common semiconductor manufacturing process for computers in the 1980s.\nIn the 1980s, CMOS microprocessors overtook NMOS microprocessors.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49417", "revid": "1315151407", "url": "https://en.wikipedia.org/wiki?curid=49417", "title": "Extinction", "text": "Termination of an organism by the death of its last member\nExtinction is the termination of an organism by the death of its last member. A taxon may become functionally extinct before the death of its last member if it loses the capacity to reproduce and recover. As a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively. This difficulty leads to phenomena such as Lazarus taxa, where a species presumed extinct abruptly \"reappears\" (typically in the fossil record) after a period of apparent absence.\nOver five billion species are estimated to have died out. It is estimated that there are currently around 8.7\u00a0million species of eukaryotes globally, possibly many times more if microorganisms are included. Notable extinct animal species include non-avian dinosaurs, saber-toothed cats, and mammoths. Through evolution, species arise through the process of speciation. Species become extinct when they are no longer able to survive in changing conditions or against superior competition. The relationship between animals and their ecological niches has been firmly established. A typical species becomes extinct within 10\u00a0million years of its first appearance, although some species, called living fossils, survive with little to no morphological change for hundreds of millions of years, though this claim has been disputed.\nMass extinctions are relatively rare events; however, isolated extinctions of species and clades are quite common, and are a natural part of the evolutionary process. Only recently have extinctions begun to be recorded, and there is an ongoing mass extinction event caused by human activity. Most species that become extinct are never scientifically documented. Some scientists estimate that up to half of presently existing plant and animal species may become extinct by 2100. A 2018 report indicated that the phylogenetic diversity of 300 mammalian species erased during the human era since the Late Pleistocene would require 5 to 7\u00a0million years to recover.\nAccording to the 2019 \"Global Assessment Report on Biodiversity and Ecosystem Services\" by IPBES, the biomass of wild mammals has fallen by 82%, natural ecosystems have lost about half their area and a million species are at risk of extinction\u2014all largely as a result of human actions. Twenty-five percent of plant and animal species are threatened with extinction. In a subsequent report, IPBES listed unsustainable fishing, hunting and logging as being some of the primary drivers of the global extinction crisis. In June 2019, one million species of plants and animals were at risk of extinction. At least 571 plant species have been lost since 1750. The main cause of the extinctions is the destruction of natural habitats by human activities, such as cutting down forests and converting land into fields for farming.\nA dagger symbol (\u2020) placed next to the name of a species or other taxon normally indicates its status as extinct.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nDefinition.\nA species is extinct when the last existing member dies. Extinction therefore becomes a certainty when there are no surviving individuals that can reproduce and create a new generation. A species may become functionally extinct when only a handful of individuals survive, which cannot reproduce due to poor health, age, sparse distribution over a large range, a lack of individuals of both sexes (in sexually reproducing species), or other reasons.\nPinpointing the extinction (or pseudoextinction) of a species requires a clear definition of that species. If it is to be declared extinct, the species in question must be uniquely distinguishable from any ancestor or daughter species, and from any other closely related species. Extinction of a species (or replacement by a daughter species) plays a key role in the punctuated equilibrium hypothesis of Stephen Jay Gould and Niles Eldredge.\nIn ecology, \"extinction\" is sometimes used informally to refer to local extinction, in which a species ceases to exist in the chosen area of study, despite still existing elsewhere. Local extinctions may be made good by the reintroduction of individuals of that species taken from other locations; wolf reintroduction is an example of this. Species that are not globally extinct are termed extant. Those species that are extant, yet are threatened with extinction, are referred to as threatened or endangered species.\nCurrently, an important aspect of extinction is human attempts to preserve critically endangered species. These are reflected by the creation of the conservation status \"extinct in the wild\" (EW). Species listed under this status by the International Union for Conservation of Nature (IUCN) are not known to have any living specimens in the wild and are maintained only in zoos or other artificial environments. Some of these species are functionally extinct, as they are no longer part of their natural habitat and it is unlikely the species will ever be restored to the wild. When possible, modern zoological institutions try to maintain a viable population for species preservation and possible future reintroduction to the wild, through use of carefully planned breeding programs.\nThe extinction of one species' wild population can have knock-on effects, causing further extinctions. These are also called \"chains of extinction\". This is especially common with extinction of keystone species.\nA 2018 study indicated that the sixth mass extinction started in the Late Pleistocene could take up to 5 to 7\u00a0million years to restore mammal diversity to what it was before the human era.\nPseudoextinction.\nExtinction of a parent species where daughter species or subspecies are still extant is called pseudoextinction or phyletic extinction. Effectively, the old taxon vanishes, transformed (anagenesis) into a successor, or split into more than one (cladogenesis).\nPseudoextinction is difficult to demonstrate unless one has a strong chain of evidence linking a living species to members of a pre-existing species. For example, it is sometimes claimed that the extinct \"Hyracotherium\", which was an early horse that shares a common ancestor with the modern horse, is pseudoextinct, rather than extinct, because there are several extant species of \"Equus\", including zebra and donkey; however, as fossil species typically leave no genetic material behind, one cannot say whether \"Hyracotherium\" evolved into more modern horse species or merely evolved from a common ancestor with modern horses. Pseudoextinction is much easier to demonstrate for larger taxonomic groups.\nLazarus taxa.\nA Lazarus taxon or Lazarus species refers to instances where a species or taxon was thought to be extinct, but was later rediscovered. It can also refer to instances where large gaps in the fossil record of a taxon result in fossils reappearing much later, although the taxon may have ultimately become extinct at a later point.\nThe coelacanth, a fish related to lungfish and tetrapods, is an example of a Lazarus taxon that was known only from the fossil record and was considered to have been extinct since the end of the Cretaceous Period. In 1938, however, a living specimen was found off the Chalumna River (now Tyolomnqa) on the east coast of South Africa. \"Calliostoma bullatum\", a species of deepwater sea snail originally described from fossils in 1844 proved to be a Lazarus species when extant individuals were described in 2019.\nAttenborough's long-beaked echidna (\"Zaglossus attenboroughi\") is an example of a Lazarus species from Papua New Guinea that had last been sighted in 1962 and believed to be possibly extinct, until it was recorded again in November 2023.\nSome species thought to be extinct have had ongoing speculation that they may still exist, and in the event of rediscovery would be considered Lazarus species. Examples include the thylacine, or Tasmanian tiger (\"Thylacinus cynocephalus\"), the last known example of which died in Hobart Zoo in Tasmania in 1936; the Japanese wolf (\"Canis lupus hodophilax\"), last sighted over 100 years ago; the American ivory-billed woodpecker (\"Campephilus principalis\"), with the last universally accepted sighting in 1944; and the slender-billed curlew (\"Numenius tenuirostris\"), not seen since 2007.\nCauses.\nAs long as species have been evolving, species have been going extinct. It is estimated that over 99.9% of all species that ever lived are extinct. The average lifespan of a species is 1\u201310\u00a0million years, although this varies widely between taxa.\nA variety of causes can contribute directly or indirectly to the extinction of a species or group of species. \"Just as each species is unique\", write Beverly and Stephen C. Stearns, \"so is each extinction\u00a0... the causes for each are varied\u2014some subtle and complex, others obvious and simple\". Most simply, any species that cannot survive and reproduce in its environment and cannot move to a new environment where it can do so, dies out and becomes extinct. Extinction of a species may come suddenly when an otherwise healthy species is wiped out completely, as when toxic pollution renders its entire habitat unliveable; or may occur gradually over thousands or millions of years, such as when a species gradually loses out in competition for food to better adapted competitors. Extinction may occur a long time after the events that set it in motion, a phenomenon known as extinction debt.\nAssessing the relative importance of genetic factors compared to environmental ones as the causes of extinction has been compared to the debate on nature and nurture. The question of whether more extinctions in the fossil record have been caused by evolution or by competition or by predation or by disease or by catastrophe is a subject of discussion; Mark Newman, the author of \"Modeling Extinction\", argues for a mathematical model that falls in all positions. By contrast, conservation biology uses the extinction vortex model to classify extinctions by cause. When concerns about human extinction have been raised, for example in Sir Martin Rees' 2003 book \"Our Final Hour\", those concerns lie with the effects of climate change or technological disaster.\nHuman-driven extinction started as humans migrated out of Africa more than 60,000 years ago. Currently, environmental groups and some governments are concerned with the extinction of species caused by humanity, and they try to prevent further extinctions through a variety of conservation programs. Humans can cause extinction of a species through overharvesting, pollution, habitat destruction, introduction of invasive species (such as new predators and food competitors), overhunting, and other influences. Explosive, unsustainable human population growth and increasing per capita consumption are essential drivers of the extinction crisis. According to the International Union for Conservation of Nature (IUCN), 784 extinctions have been recorded since the year 1500, the arbitrary date selected to define \"recent\" extinctions, up to the year 2004; with many more likely to have gone unnoticed. Several species have also been listed as extinct since 2004.\nGenetics and demographic phenomena.\nIf adaptation increasing population fitness is slower than environmental degradation plus the accumulation of slightly deleterious mutations, then a population will go extinct. Smaller populations have fewer beneficial mutations entering the population each generation, slowing adaptation. It is also easier for slightly deleterious mutations to fix in small populations; the resulting positive feedback loop between small population size and low fitness can cause mutational meltdown.\nLimited geographic range is the most important determinant of genus extinction at background rates but becomes increasingly irrelevant as mass extinction arises. Limited geographic range is a cause both of small population size and of greater vulnerability to local environmental catastrophes.\nExtinction rates can be affected not just by population size, but by any factor that affects evolvability, including balancing selection, cryptic genetic variation, phenotypic plasticity, and robustness. A diverse or deep gene pool gives a population a higher chance in the short term of surviving an adverse change in conditions. Effects that cause or reward a loss in genetic diversity can increase the chances of extinction of a species. Population bottlenecks can dramatically reduce genetic diversity by severely limiting the number of reproducing individuals and make inbreeding more frequent.\nGenetic pollution.\nExtinction sometimes results for species evolved to specific ecologies that are subjected to genetic pollution\u2014i.e., uncontrolled hybridization, introgression and genetic swamping that lead to homogenization or out-competition from the introduced (or hybrid) species. Endemic populations can face such extinctions when new populations are imported or selectively bred by people, or when habitat modification brings previously isolated species into contact. Extinction is likeliest for rare species coming into contact with more abundant ones; interbreeding can swamp the rarer gene pool and create hybrids, depleting the purebred gene pool (for example, the endangered wild water buffalo is most threatened with extinction by genetic pollution from the abundant domestic water buffalo). Such extinctions are not always apparent from morphological (non-genetic) observations. Some degree of gene flow is a normal evolutionary process; nevertheless, hybridization (with or without introgression) threatens rare species' existence.\nThe gene pool of a species or a population is the variety of genetic information in its living members. A large gene pool (extensive genetic diversity) is associated with robust populations that can survive bouts of intense selection. Meanwhile, low genetic diversity (see inbreeding and population bottlenecks) reduces the range of adaptions possible. Replacing native with alien genes narrows genetic diversity within the original population, thereby increasing the chance of extinction.\nHabitat degradation.\nHabitat degradation is currently the main anthropogenic cause of species extinctions. The main cause of habitat degradation worldwide is agriculture, with urban sprawl, logging, mining, and some fishing practices close behind. The degradation of a species' habitat may alter the fitness landscape to such an extent that the species is no longer able to survive and becomes extinct. This may occur by direct effects, such as the environment becoming toxic, or indirectly, by limiting a species' ability to compete effectively for diminished resources or against new competitor species.\nHabitat destruction, particularly the removal of vegetation that stabilizes soil, enhances erosion and diminishes nutrient availability in terrestrial ecosystems. This degradation can lead to a reduction in agricultural productivity. Furthermore, increased erosion contributes to poorer water quality by elevating the levels of sediment and pollutants in rivers and streams.\nHabitat degradation through toxicity can kill off a species very rapidly, by killing all living members through contamination or sterilizing them. It can also occur over longer periods at lower toxicity levels by affecting life span, reproductive capacity, or competitiveness.\nHabitat degradation can also take the form of a physical destruction of niche habitats. The widespread destruction of tropical rainforests and replacement with open pastureland is widely cited as an example of this; elimination of the dense forest eliminated the infrastructure needed by many species to survive. For example, a fern that depends on dense shade for protection from direct sunlight can no longer survive without forest to shelter it. Another example is the destruction of ocean floors by bottom trawling.\nDiminished resources or introduction of new competitor species also often accompany habitat degradation. Global warming has allowed some species to expand their range, bringing competition to other species that previously occupied that area. Sometimes these new competitors are predators and directly affect prey species, while at other times they may merely outcompete vulnerable species for limited resources. Vital resources including water and food can also be limited during habitat degradation, leading to extinction.\nPredation, competition, and disease.\nIn the natural course of events, species become extinct for a number of reasons, including but not limited to: extinction of a necessary host, prey or pollinator, interspecific competition, inability to deal with evolving diseases and changing environmental conditions (particularly sudden changes) which can act to introduce novel predators, or to remove prey. Recently in geological time, humans have become an additional cause of extinction of some species, either as a new mega-predator or by transporting animals and plants from one part of the world to another. Such introductions have been occurring for thousands of years, sometimes intentionally (e.g. livestock released by sailors on islands as a future source of food) and sometimes accidentally (e.g. rats escaping from boats). In most cases, the introductions are unsuccessful, but when an invasive alien species does become established, the consequences can be catastrophic. Invasive alien species can affect native species directly by eating them, competing with them, and introducing pathogens or parasites that sicken or kill them; or indirectly by destroying or degrading their habitat. Human populations may themselves act as invasive predators. According to the \"overkill hypothesis\", the swift extinction of the megafauna in areas such as Australia (40,000 years before present), North and South America (12,000 years before present), Madagascar, Hawaii (AD 300\u20131000), and New Zealand (AD 1300\u20131500), resulted from the sudden introduction of human beings to environments full of animals that had never seen them before and were therefore completely unadapted to their predation techniques.\nCoextinction.\nCoextinction refers to the loss of a species due to the extinction of another; for example, the extinction of parasitic insects following the loss of their hosts. Coextinction can also occur when a species loses its pollinator, or to predators in a food chain who lose their prey. \"Species coextinction is a manifestation of one of the interconnectednesses of organisms in complex ecosystems\u00a0... While coextinction may not be the most important cause of species extinctions, it is certainly an insidious one.\" Coextinction is especially common when a keystone species goes extinct. Models suggest that coextinction is the most common form of biodiversity loss. There may be a cascade of coextinction across the trophic levels. Such effects are most severe in mutualistic and parasitic relationships. An example of coextinction is the Haast's eagle and the moa: the Haast's eagle was a predator that became extinct because its food source became extinct. The moa were several species of flightless birds that were a food source for the Haast's eagle.\nClimate change.\nExtinction as a result of climate change has been confirmed by fossil studies. Particularly, the extinction of amphibians during the Carboniferous Rainforest Collapse, 305\u00a0million years ago. A 2003 review across 14 biodiversity research centers predicted that, because of climate change, 15\u201337% of land species would be \"committed to extinction\" by 2050. The ecologically rich areas that would potentially suffer the heaviest losses include the Cape Floristic Region and the Caribbean Basin. These areas might see a doubling of present carbon dioxide levels and rising temperatures that could eliminate 56,000 plant and 3,700 animal species. Climate change has also been found to be a factor in habitat loss and desertification.\nSexual selection and male investment.\nStudies of fossils following species from the time they evolved to their extinction show that species with high sexual dimorphism, especially characteristics in males that are used to compete for mating, are at a higher risk of extinction and die out faster than less sexually dimorphic species, the least sexually dimorphic species surviving for millions of years while the most sexually dimorphic species die out within mere thousands of years. Earlier studies based on counting the number of currently living species in modern taxa have shown a higher number of species in more sexually dimorphic taxa which have been interpreted as higher survival in taxa with more sexual selection, but such studies of modern species only measure indirect effects of extinction and are subject to error sources such as dying and doomed taxa speciating more due to splitting of habitat ranges into more small isolated groups during the habitat retreat of taxa approaching extinction. Possible causes of the higher extinction risk in species with more sexual selection shown by the comprehensive fossil studies that rule out such error sources include expensive sexually selected ornaments having negative effects on the ability to survive natural selection, as well as sexual selection removing a diversity of genes that under current ecological conditions are neutral for natural selection but some of which may be important for surviving climate change.\nMass extinctions.\n &lt;imagemap&gt;\nImage:Extinction intensity.svg\nMarine extinction intensity during Phanerozoic\nMillions of years ago\nK\u2013Pg\nTr\u2013J\nP\u2013Tr\nCap\nLate D\nO\u2013S\n&lt;imagemap&gt;\nImage:Extinction intensity.svg\nThe blue graph shows the apparent \"percentage\" (not the absolute number) of marine animal genera becoming extinct during any given time interval. It does not represent all marine species, just those that are readily fossilized. The labels of the traditional \"Big Five\" extinction events and the more recently recognised Capitanian mass extinction event are clickable links; see Extinction event for more details. \"()\"\nThere have been at least five mass extinctions in the history of life on earth, and four in the last 350\u00a0million years in which many species have disappeared in a relatively short period of geological time. A massive eruptive event that released large quantities of tephra particles into the atmosphere is considered to be one likely cause of the \"Permian\u2013Triassic extinction event\" about 250\u00a0million years ago, which is estimated to have killed 90% of species then existing. There is also evidence to suggest that this event was preceded by another mass extinction, known as Olson's Extinction. The Cretaceous\u2013Paleogene extinction event (K\u2013Pg) occurred 66\u00a0million years ago, at the end of the Cretaceous period; it is best known for having wiped out non-avian dinosaurs, among many other species.\nModern extinctions.\nAccording to a 1998 survey of 400 biologists conducted by New York's American Museum of Natural History, nearly 70% believed that the Earth is currently in the early stages of a human-caused mass extinction, known as the Holocene extinction. In that survey, the same proportion of respondents agreed with the prediction that up to 20% of all living populations could become extinct within 30 years (by 2028). A 2014 special edition of \"Science\" declared there is widespread consensus on the issue of human-driven mass species extinctions. A 2020 study published in \"PNAS\" stated that the contemporary extinction crisis \"may be the most serious environmental threat to the persistence of civilization, because it is irreversible.\" A 2025 study found that human activities are to blame for biodiversity loss across all species and ecosystems.\nBiologist E. O. Wilson estimated in 2002 that if current rates of human destruction of the biosphere continue, one-half of all plant and animal species of life on earth will be extinct in 100 years. More significantly, the current rate of global species extinctions is estimated as 100 to 1,000 times \"background\" rates (the average extinction rates in the evolutionary time scale of planet Earth), faster than at any other time in human history, while future rates are likely 10,000 times higher. However, some groups are going extinct much faster. Biologists Paul R. Ehrlich and Stuart Pimm, among others, contend that human population growth and overconsumption are the main drivers of the modern extinction crisis.\nIn January 2020, the UN's Convention on Biological Diversity drafted a plan to mitigate the contemporary extinction crisis by establishing a deadline of 2030 to protect 30% of the Earth's land and oceans and reduce pollution by 50%, with the goal of allowing for the restoration of ecosystems by 2050. The 2020 United Nations' \"Global Biodiversity Outlook\" report stated that of the 20 biodiversity goals laid out by the Aichi Biodiversity Targets in 2010, only 6 were \"partially achieved\" by the deadline of 2020. The report warned that biodiversity will continue to decline if the status quo is not changed, in particular the \"currently unsustainable patterns of production and consumption, population growth and technological developments\". In a 2021 report published in the journal \"Frontiers in Conservation Science\", some top scientists asserted that even if the Aichi Biodiversity Targets set for 2020 had been achieved, it would not have resulted in a significant mitigation of biodiversity loss. They added that failure of the global community to reach these targets is hardly surprising given that biodiversity loss is \"nowhere close to the top of any country's priorities, trailing far behind other concerns such as employment, healthcare, economic growth, or currency stability.\"\nHistory of scientific understanding.\nFor much of history, the modern understanding of extinction as the end of a species was incompatible with the prevailing worldview. Prior to the 19th century, much of Western society adhered to the belief that the world was created by God and as such was complete and perfect. This concept reached its heyday in the 1700s with the peak popularity of a theological concept called the great chain of being, in which all life on earth, from the tiniest microorganism to God, is linked in a continuous chain. The extinction of a species was impossible under this model, as it would create gaps or missing links in the chain and destroy the natural order. Thomas Jefferson was a firm supporter of the great chain of being and an opponent of extinction, famously denying the extinction of the woolly mammoth on the grounds that nature never allows a race of animals to become extinct.\nA series of fossils were discovered in the late 17th century that appeared unlike any living species. As a result, the scientific community embarked on a voyage of creative rationalization, seeking to understand what had happened to these species within a framework that did not account for total extinction. In October 1686, Robert Hooke presented an impression of a nautilus to the Royal Society that was more than two feet in diameter, and morphologically distinct from any known living species. Hooke theorized that this was simply because the species lived in the deep ocean and no one had discovered them yet. While he contended that it was possible a species could be \"lost\", he thought this highly unlikely. Similarly, in 1695, Sir Thomas Molyneux published an account of enormous antlers found in Ireland that did not belong to any extant taxa in that area. Molyneux reasoned that they came from the North American moose and that the animal had once been common on the British Isles. Rather than suggest that this indicated the possibility of species going extinct, he argued that although organisms could become locally extinct, they could never be entirely lost and would continue to exist in some unknown region of the globe. The antlers were later confirmed to be from the extinct deer \"Megaloceros\". Hooke and Molyneux's line of thinking was difficult to disprove. When parts of the world had not been thoroughly examined and charted, scientists could not rule out that animals found only in the fossil record were not simply \"hiding\" in unexplored regions of the Earth.\nGeorges Cuvier is credited with establishing the modern conception of extinction in a 1796 lecture to the French Institute, though he would spend most of his career trying to convince the wider scientific community of his theory. Cuvier was a well-regarded geologist, lauded for his ability to reconstruct the anatomy of an unknown species from a few fragments of bone. His primary evidence for extinction came from mammoth skulls found near Paris. Cuvier recognized them as distinct from any known living species of elephant, and argued that it was highly unlikely such an enormous animal would go undiscovered. In 1798, he studied a fossil from the Paris Basin that was first observed by Robert de Lamanon in 1782, first hypothesizing that it belonged to a canine but then deciding that it instead belonged to an animal that was unlike living ones. His study paved the way to his naming of the extinct mammal genus \"Palaeotherium\" in 1804 based on the skull and additional fossil material along with another extinct contemporary mammal genus \"Anoplotherium\". In both genera, he noticed that their fossils shared some similarities with other mammals like ruminants and rhinoceroses but still had distinct differences. In 1812, Cuvier, along with Alexandre Brongniart and Geoffroy Saint-Hilaire, mapped the strata of the Paris basin. They saw alternating saltwater and freshwater deposits, as well as patterns of the appearance and disappearance of fossils throughout the record. From these patterns, Cuvier inferred historic cycles of catastrophic flooding, extinction, and repopulation of the earth with new species.\nCuvier's fossil evidence showed that very different life forms existed in the past than those that exist today, a fact that was accepted by most scientists. The primary debate focused on whether this turnover caused by extinction was gradual or abrupt in nature. Cuvier understood extinction to be the result of cataclysmic events that wipe out huge numbers of species, as opposed to the gradual decline of a species over time. His catastrophic view of the nature of extinction garnered him many opponents in the newly emerging school of uniformitarianism.\nJean-Baptiste Lamarck, a gradualist and colleague of Cuvier, saw the fossils of different life forms as evidence of the mutable character of species. While Lamarck did not deny the possibility of extinction, he believed that it was exceptional and rare and that most of the change in species over time was due to gradual change. Unlike Cuvier, Lamarck was skeptical that catastrophic events of a scale large enough to cause total extinction were possible. In his geological history of the earth titled Hydrogeologie, Lamarck instead argued that the surface of the earth was shaped by gradual erosion and deposition by water, and that species changed over time in response to the changing environment.\nCharles Lyell, a noted geologist and founder of uniformitarianism, believed that past processes should be understood using present day processes. Like Lamarck, Lyell acknowledged that extinction could occur, noting the total extinction of the dodo and the extirpation of indigenous horses to the British Isles. He similarly argued against mass extinctions, believing that any extinction must be a gradual process. Lyell also showed that Cuvier's original interpretation of the Parisian strata was incorrect. Instead of the catastrophic floods inferred by Cuvier, Lyell demonstrated that patterns of saltwater and freshwater deposits, like those seen in the Paris basin, could be formed by a slow rise and fall of sea levels.\nThe concept of extinction was integral to Charles Darwin's \"On the Origin of Species\", with less fit lineages disappearing over time. For Darwin, extinction was a constant side effect of competition. Because of the wide reach of \"On the Origin of Species\", it was widely accepted that extinction occurred gradually and evenly (a concept now referred to as background extinction). It was not until 1982, when David Raup and Jack Sepkoski published their seminal paper on mass extinctions, that Cuvier was vindicated and catastrophic extinction was accepted as an important mechanism. The current understanding of extinction is a synthesis of the cataclysmic extinction events proposed by Cuvier, and the background extinction events proposed by Lyell and Darwin.\nHuman attitudes and interests.\nExtinction is an important research topic in the field of zoology, and biology in general, and has also become an area of concern outside the scientific community. A number of organizations, such as the Worldwide Fund for Nature, have been created with the goal of preserving species from extinction. Governments have attempted, through enacting laws, to avoid habitat destruction, agricultural over-harvesting, and pollution. While many human-caused extinctions have been accidental, humans have also engaged in the deliberate destruction of some species, such as dangerous viruses, and the total destruction of other problematic species has been suggested. Other species were deliberately driven to extinction, or nearly so, due to poaching or because they were \"undesirable\", or to push for other human agendas. One example was the near extinction of the American bison, which was nearly wiped out by mass hunts sanctioned by the United States government, to force the removal of Native Americans, many of whom relied on the bison for food.\nBiologist Bruce Walsh states three reasons for scientific interest in the preservation of species: genetic resources, ecosystem stability, and ethics; and today the scientific community \"stress[es] the importance\" of maintaining biodiversity.\nIn modern times, commercial and industrial interests often have to contend with the effects of production on plant and animal life. However, some technologies with minimal, or no, proven harmful effects on \"Homo sapiens\" can be devastating to wildlife (for example, DDT). Biogeographer Jared Diamond notes that while big business may label environmental concerns as \"exaggerated\", and often cause \"devastating damage\", some corporations find it in their interest to adopt good conservation practices, and even engage in preservation efforts that surpass those taken by national parks.\nGovernments sometimes see the loss of native species as a loss to ecotourism, and can enact laws with severe punishment against the trade in native species in an effort to prevent extinction in the wild. Nature preserves are created by governments as a means to provide continuing habitats to species crowded by human expansion. The 1992 Convention on Biological Diversity has resulted in international Biodiversity Action Plan programmes, which attempt to provide comprehensive guidelines for government biodiversity conservation. Advocacy groups, such as The Wildlands Project and the Alliance for Zero Extinctions, work to educate the public and pressure governments into action.\nPeople who live close to nature can be dependent on the survival of all the species in their environment, leaving them highly exposed to extinction risks. However, people prioritize day-to-day survival over species conservation; with human overpopulation in tropical developing countries, there has been enormous pressure on forests due to subsistence agriculture, including slash-and-burn agricultural techniques that can reduce endangered species's habitats.\nAntinatalist philosopher David Benatar concludes that any popular concern about non-human species extinction usually arises out of concern about how the loss of a species will impact human wants and needs, that \"we shall live in a world impoverished by the loss of one aspect of faunal diversity, that we shall no longer be able to behold or use that species of animal.\" He notes that typical concerns about possible human extinction, such as the loss of individual members, are not considered in regards to non-human species extinction. Anthropologist Jason Hickel speculates that the reason humanity seems largely indifferent to anthropogenic mass species extinction is that we see ourselves as separate from the natural world and the organisms within it. He says that this is due in part to the logic of capitalism: \"that the world is not really alive, and it is certainly not our kin, but rather just stuff to be extracted and discarded \u2013 and that includes most of the human beings living here too.\"\nPlanned extinction.\nProposed.\nDisease agents.\nThe poliovirus is now confined to small parts of the world due to extermination efforts.\n\"Dracunculus medinensis\", or Guinea worm, a parasitic worm which causes the disease dracunculiasis, is now close to eradication thanks to efforts led by the Carter Center.\n\"Treponema pallidum pertenue\", a bacterium which causes the disease yaws, is in the process of being eradicated.\nDisease vectors.\nBiologist Olivia Judson has advocated the deliberate extinction of certain disease-carrying mosquito species. In an article in \"The New York Times\" on 25 September 2003, she advocated \"specicide\" of thirty mosquito species by introducing a genetic element that can insert itself into another crucial gene, to create recessive \"knockout genes\". She says that the \"Anopheles\" mosquitoes (which spread malaria) and \"Aedes\" mosquitoes (which spread dengue fever, yellow fever, elephantiasis, and other diseases) represent only 30 of around 3,500 mosquito species; eradicating these would save at least one million human lives per year, at a cost of reducing the genetic diversity of the family Culicidae by only 1%. She further argues that since species become extinct \"all the time\" the disappearance of a few more will not destroy the ecosystem: \"We're not left with a wasteland every time a species vanishes. Removing one species sometimes causes shifts in the populations of other species\u2014but different need not mean worse.\" In addition, anti-malarial and mosquito control programs offer little realistic hope to the 300\u00a0million people in developing nations who will be infected with acute illnesses this year. Although trials are ongoing, she writes that if they fail \"we should consider the ultimate swatting\".\nBiologist E. O. Wilson has advocated the eradication of several species of mosquito, including malaria vector \"Anopheles gambiae\". Wilson stated, \"I'm talking about a very small number of species that have co-evolved with us and are preying on humans, so it would certainly be acceptable to remove them. I believe it's just common sense.\"\nThere have been many campaigns \u2013 some successful \u2013 to locally eradicate tsetse flies and their trypanosomes in areas, countries, and islands of Africa (including Pr\u00edncipe). There are currently serious efforts to do away with them all across Africa, and this is generally viewed as beneficial and morally necessary, although not always.\nCloning.\nSome, such as Harvard geneticist George M. Church, believe that ongoing technological advances will let us \"bring back to life\" an extinct species by cloning, using DNA from the remains of that species. Proposed targets for cloning include the mammoth, the thylacine, and the Pyrenean ibex. For this to succeed, enough individuals would have to be cloned, from the DNA of different individuals (in the case of sexually reproducing organisms) to create a viable population. Though bioethical and philosophical objections have been raised, the cloning of extinct creatures seems theoretically possible.\nIn 2003, scientists tried to clone the extinct Pyrenean ibex (\"C. p. pyrenaica\"). This attempt failed: of the 285 embryos reconstructed, 54 were transferred to 12 Spanish ibexes and ibex\u2013domestic goat hybrids, but only two survived the initial two months of gestation before they, too, died. In 2009, a second attempt was made to clone the Pyrenean ibex: one clone was born alive, but died seven minutes later, due to physical defects in the lungs.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49418", "revid": "1107685757", "url": "https://en.wikipedia.org/wiki?curid=49418", "title": "MidasWWW", "text": " \nMidasWWW is one of the earliest (now discontinued) web browsers, developed at the Stanford Linear Accelerator Center (SLAC). It ran under Unix and OpenVMS. The last release was version 2.2. The 16 Nov 1992 sources were made available in June 2015 at GitHub.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49420", "revid": "50950899", "url": "https://en.wikipedia.org/wiki?curid=49420", "title": "CMOS", "text": "Technology for constructing integrated circuits\nComplementary metal\u2013oxide\u2013semiconductor (CMOS, pronounced \"sea-moss\", , ) is a type of metal\u2013oxide\u2013semiconductor field-effect transistor (MOSFET) fabrication process that uses complementary and symmetrical pairs of p-type and n-type MOSFETs for logic functions. CMOS technology is used for constructing integrated circuit (IC) chips, including microprocessors, microcontrollers, memory chips, and other digital logic circuits. CMOS overtook NMOS logic as the dominant MOSFET fabrication process for very large-scale integration (VLSI) chips in the 1980s, replacing earlier transistor\u2013transistor logic (TTL) technology at the same time. CMOS has since remained the standard fabrication process for MOSFET semiconductor devices. As of 2011[ [update]], 99% of IC chips, including most digital, analog and mixed-signal ICs, were fabricated using CMOS technology.\nIn 1948, Bardeen and Brattain patented an insulated-gate transistor (IGFET) with an inversion layer. Bardeen's concept forms the basis of CMOS technology today. The CMOS process was presented by Fairchild Semiconductor's Frank Wanlass and Chih-Tang Sah at the International Solid-State Circuits Conference in 1963. Wanlass later filed for CMOS circuitry and it was granted in 1967. &lt;templatestyles src=\"Template:Tooltip/styles.css\" /&gt; commercialized the technology with the trademark \"COS-MOS\" in the late 1960s, forcing other manufacturers to find another name, leading to \"CMOS\" becoming the standard name for the technology by the early 1970s. \nTwo important characteristics of CMOS devices are high noise immunity and low static power consumption. Since one transistor of the MOSFET pair is always off, the series combination draws significant power only momentarily during switching between on and off states. Consequently, CMOS devices do not produce as much waste heat as other forms of logic, like NMOS logic or transistor\u2013transistor logic (TTL), which normally have some standing current even when not changing state. These characteristics allow CMOS to integrate a high density of logic functions on a chip. It was primarily for this reason that CMOS became the most widely used technology to be implemented in VLSI chips.\nThe phrase \"metal\u2013oxide\u2013semiconductor\" is a reference to the physical structure of MOS field-effect transistors, having a metal gate electrode placed on top of an oxide insulator, which in turn is on top of a semiconductor material. Aluminium was once used but now the material is polysilicon. Other metal gates have made a comeback with the advent of high-\u03ba dielectric materials in the CMOS process, as announced by IBM and Intel for the 45 nanometer node and smaller sizes.\nCMOS technology is also used for analog circuits such as image sensors (CMOS sensors), data converters, RF circuits (RF CMOS), and highly integrated transceivers for many types of communication.\nHistory.\nThe principle of complementary symmetry was first introduced by George Sziklai in 1953 who then discussed several complementary bipolar circuits. Paul Weimer, also at RCA, invented in 1962 thin-film transistor (TFT) complementary circuits, a close relative of CMOS. He invented complementary flip-flop and inverter circuits, but did no work in a more complex complementary logic. He was the first person able to put p-channel and n-channel TFTs in a circuit on the same substrate. Three years earlier, John T. Wallmark and Sanford M. Marcus published a variety of complex logic functions implemented as integrated circuits using JFETs, including complementary memory circuits. Frank Wanlass was familiar with work done by Weimer at RCA.\nIn 1955, Carl Frosch and Lincoln Derick accidentally grew a layer of silicon dioxide over the silicon wafer, for which they observed surface passivation effects. By 1957 Frosch and Derick, using masking and predeposition, were able to manufacture silicon transistors. They showed that silicon dioxide protected silicon wafers from dopants diffusing into the wafer, and insulated the wafer from damage due to heat during the process. J.R. Ligenza and W.G. Spitzer studied the mechanism of thermally grown oxides and fabricated a high quality Si/SiO2 stack in 1960.\nFollowing this research, Mohamed Atalla and Dawon Kahng proposed a silicon MOS transistor in 1959 and successfully demonstrated a working MOS device with their Bell Labs team in 1960. Their team included E. E. LaBate and E. I. Povilonis who fabricated the device; M. O. Thurston, L. A. D'Asaro, and J. R. Ligenza who developed the diffusion processes, and H. K. Gummel and R. Lindner who characterized the device. There were originally two types of MOSFET logic, PMOS (p-type MOS) and NMOS (n-type MOS).\nIn 1948, Bardeen and Brattain patented the progenitor of MOSFET, an insulated-gate FET (IGFET) with an inversion layer. Bardeen's patent, and the concept of an inversion layer, forms the basis of CMOS technology today. A new type of MOSFET logic combining both the PMOS and NMOS processes was developed, called complementary MOS (CMOS), by Chih-Tang Sah and Frank Wanlass at Fairchild. In February 1963, they published the invention in a research paper. In both the research paper and the patent filed by Wanlass, the fabrication of CMOS devices was outlined, on the basis of thermal oxidation of a silicon substrate to yield a layer of silicon dioxide located between the drain contact and the source contact.\nCMOS was commercialised by RCA in the late 1960s. RCA adopted CMOS for the design of integrated circuits (ICs), developing CMOS circuits for an Air Force computer in 1965 and then a 288-bit CMOS SRAM memory chip in 1968. RCA also used CMOS for its 4000-series integrated circuits in 1968, starting with a 20\u03bcm semiconductor manufacturing process before gradually scaling to a 10\u00a0\u03bcm process over the next several years.\nCMOS technology was initially overlooked by the American semiconductor industry in favour of NMOS, which was more powerful at the time. However, CMOS was quickly adopted and further advanced by Japanese semiconductor manufacturers due to its low power consumption, leading to the rise of the Japanese semiconductor industry. Toshiba developed C2MOS (\"clocked CMOS\"), a circuit technology with lower power consumption and faster operating speed than ordinary CMOS, in 1969. Toshiba used its C2MOS technology to develop a large-scale integration (LSI) chip for Sharp's Elsi Mini LED pocket calculator, developed in 1971 and released in 1972. Suwa Seikosha (now Seiko Epson) began developing a CMOS IC chip for a Seiko quartz watch in 1969, and began mass-production with the launch of the Seiko Analog Quartz 38SQW watch in 1971. The first mass-produced CMOS consumer electronic product was the Hamilton Pulsar \"Wrist Computer\" digital watch, released in 1970. Due to low power consumption, CMOS logic has been widely used for calculators and watches since the 1970s.\nThe earliest microprocessors in the early 1970s were PMOS processors, which initially dominated the early microprocessor industry. By the late 1970s, NMOS microprocessors had overtaken PMOS processors. CMOS microprocessors were introduced in 1975, with the Intersil 6100, and RCA CDP 1801. However, CMOS processors did not become dominant until the 1980s.\nCMOS was initially slower than NMOS logic, thus NMOS was more widely used for computers in the 1970s. The Intel 5101 (1kb SRAM) CMOS memory chip (1974) had an access time of 800ns, whereas the fastest NMOS chip at the time, the Intel 2147 (4kb SRAM) HMOS memory chip (1976), had an access time of 55/70ns. In 1978, a Hitachi research team led by Toshiaki Masuhara introduced the twin-well Hi-CMOS process, with its HM6147 (4kb SRAM) memory chip, manufactured with a 3 \u03bcm process. The Hitachi HM6147 chip was able to match the performance (55/70ns access) of the Intel 2147 HMOS chip, while the HM6147 also consumed significantly less power (15mA) than the 2147 (110mA). With comparable performance and much less power consumption, the twin-well CMOS process eventually overtook NMOS as the most common semiconductor manufacturing process for computers in the 1980s.\nIn the 1980s, CMOS microprocessors overtook NMOS microprocessors. NASA's Galileo spacecraft, sent to orbit Jupiter in 1989, used the RCA 1802 CMOS microprocessor due to low power consumption.\nIntel introduced a 1.5 \u03bcm process for CMOS semiconductor device fabrication in 1983. In the mid-1980s, Bijan Davari of IBM developed high-performance, low-voltage, deep sub-micron CMOS technology, which enabled the development of faster computers as well as portable computers and battery-powered handheld electronics. In 1988, Davari led an IBM team that demonstrated a high-performance 250 nanometer CMOS process.\nFujitsu commercialized a 700nm CMOS process in 1987, and then Hitachi, Mitsubishi Electric, NEC and Toshiba commercialized 500nm CMOS in 1989. In 1993, Sony commercialized a 350nm CMOS process, while Hitachi and NEC commercialized 250nm CMOS. Hitachi introduced a 160nm CMOS process in 1995, then Mitsubishi introduced 150nm CMOS in 1996, and then Samsung Electronics introduced 140nm in 1999.\nIn 2000, Gurtej Singh Sandhu and Trung T. Doan at Micron Technology invented atomic layer deposition High-\u03ba dielectric films, leading to the development of a cost-effective 90\u00a0nm CMOS process. Toshiba and Sony developed a 65 nm CMOS process in 2002, and then TSMC initiated the development of 45 nm CMOS logic in 2004. The development of pitch double patterning by Gurtej Singh Sandhu at Micron Technology led to the development of 30nm class CMOS in the 2000s.\nCMOS is used in most modern LSI and VLSI devices. As of 2010, CPUs with the best performance per watt each year have been CMOS static logic since 1976. As of 2019, planar CMOS technology is still the most common form of semiconductor device fabrication, but is gradually being replaced by non-planar FinFET technology, which is capable of manufacturing semiconductor nodes smaller than 20nm.\nTechnical details.\n\"CMOS\" refers to both a particular style of digital circuitry design and the family of processes used to implement that circuitry on integrated circuits (chips). CMOS circuitry dissipates less power than logic families with resistive loads. Since this advantage has increased and grown more important, CMOS processes and variants have come to dominate, thus the vast majority of modern integrated circuit manufacturing is on CMOS processes. CMOS logic consumes around one seventh the power of NMOS logic, and about 10 million times less power than bipolar transistor-transistor logic (TTL).\nCMOS circuits use a combination of p-type and n-type metal\u2013oxide\u2013semiconductor field-effect transistor (MOSFETs) to implement logic gates and other digital circuits. Although CMOS logic can be implemented with discrete devices for demonstrations, commercial CMOS products are integrated circuits composed of up to billions of transistors of both types, on a rectangular piece of silicon of often between 10 and 400\u00a0mm2.\nCMOS always uses all enhancement-mode MOSFETs (in other words, a zero gate-to-source voltage turns the transistor off).\nInversion.\nCMOS circuits are constructed in such a way that all p-type metal\u2013oxide\u2013semiconductor (PMOS) transistors must have either an input from the voltage source or from another PMOS transistor. Similarly, all NMOS transistors must have either an input from ground or from another NMOS transistor. The composition of a PMOS transistor creates low resistance between its source and drain contacts when a low gate voltage is applied and high resistance when a high gate voltage is applied. On the other hand, the composition of an NMOS transistor creates high resistance between source and drain when a low gate voltage is applied and low resistance when a high gate voltage is applied. CMOS accomplishes current reduction by complementing every nMOSFET with a pMOSFET and connecting both gates and both drains together. A high voltage on the gates will cause the nMOSFET to conduct and the pMOSFET not to conduct, while a low voltage on the gates causes the reverse. This arrangement greatly reduces power consumption and heat generation. However, during the switching time, both pMOS and nMOS MOSFETs conduct briefly as the gate voltage transitions from one state to another. This induces a brief spike in power consumption and becomes a serious issue at high frequencies.\nThe adjacent image shows what happens when an input is connected to both a PMOS transistor (top of diagram) and an NMOS transistor (bottom of diagram). Vdd is some positive voltage connected to a power supply and Vss is ground. A is the input and Q is the output.\nWhen the voltage of A is low (i.e. close to Vss), the NMOS transistor's channel is in a high resistance state, disconnecting Vss from Q. The PMOS transistor's channel is in a low resistance state, connecting Vdd to Q. Q, therefore, registers Vdd.\nOn the other hand, when the voltage of A is high (i.e. close to Vdd), the PMOS transistor is in a high resistance state, disconnecting Vdd from Q. The NMOS transistor is in a low resistance state, connecting Vss to Q. Now, Q registers Vss.\nIn short, the outputs of the PMOS and NMOS transistors are complementary such that when the input is low, the output is high, and when the input is high, the output is low. No matter what the input is, the output is never left floating (charge is never stored due to wire capacitance and lack of electrical drain/ground). Because of this behavior of input and output, the CMOS circuit's output is the inverse of the input.\nThe transistors' resistances are never exactly equal to zero or infinity, so Q will never exactly equal Vss or Vdd, but Q will always be closer to Vss than A was to Vdd (or vice versa if A were close to Vss). Without this amplification, there would be a very low limit to the number of logic gates that could be chained together in series, and CMOS logic with billions of transistors would be impossible.\nPower supply pins.\nThe power supply pins for CMOS are called \"VDD\" and \"VSS\", or \"VCC\" and \"ground (GND)\" depending on the manufacturer. \"VDD\" and \"VSS\" are carryovers from conventional MOS circuits and stand for the \"drain\" and \"source\" supplies. These do not apply directly to CMOS, since both supplies are really source supplies. \"VCC\" and \"ground\" are carryovers from TTL logic and that nomenclature has been retained with the introduction of the 54C/74C line of CMOS.\nDuality.\nAn important characteristic of a CMOS circuit is the duality that exists between its PMOS transistors and NMOS transistors. A CMOS circuit is created to allow a path always to exist from the output to either the power source or ground. To accomplish this, the set of all paths to the voltage source must be the complement of the set of all paths to ground. This can be easily accomplished by defining one in terms of the NOT of the other. Due to the logic based on De Morgan's laws, the PMOS transistors in parallel have corresponding NMOS transistors in series while the PMOS transistors in series have corresponding NMOS transistors in parallel.\nLogic.\nMore complex logic functions such as those involving AND and OR gates require manipulating the paths between gates to represent the logic. When a path consists of two transistors in series, both transistors must have low resistance to the corresponding supply voltage, modelling an AND. When a path consists of two transistors in parallel, either one or both of the transistors must have low resistance to connect the supply voltage to the output, modelling an OR.\nShown on the right is a circuit diagram of a NAND gate in CMOS logic. If both of the A and B inputs are high, then both the NMOS transistors (bottom half of the diagram) will conduct, neither of the PMOS transistors (top half) will conduct, and a conductive path will be established between the output and \"V\"ss (ground), bringing the output low. If both of the A and B inputs are low, then neither of the NMOS transistors will conduct, while both of the PMOS transistors will conduct, establishing a conductive path between the output and \"V\"dd (voltage source), bringing the output high. If either of the A or B inputs is low, one of the NMOS transistors will not conduct, one of the PMOS transistors will, and a conductive path will be established between the output and \"V\"dd (voltage source), bringing the output high. As the only configuration of the two inputs that results in a low output is when both are high, this circuit implements a NAND (NOT AND) logic gate.\nAn advantage of CMOS over NMOS logic is that both low-to-high and high-to-low output transitions are fast since the (PMOS) pull-up transistors have low resistance when switched on, unlike the load resistors in NMOS logic. In addition, the output signal swings the full voltage between the low and high rails. This strong, more nearly symmetric response also makes CMOS more resistant to noise.\nSee Logical effort for a method of calculating delay in a CMOS circuit.\nExample: NAND gate in physical layout.\nThis example shows a NAND logic device drawn as a physical representation as it would be manufactured. The physical layout perspective is a \"bird's eye view\" of a stack of layers. The circuit is constructed on a p-type substrate. The polysilicon, diffusion, and n-well are referred to as \"base layers\" and are actually inserted into trenches of the p-type substrate. (See steps 1 to 6 in the process diagram below right) The contacts penetrate an insulating layer between the base layers and the first layer of metal (metal1) making a connection.\nThe inputs to the NAND (illustrated in green color) are in polysilicon. The transistors (devices) are formed by the intersection of the polysilicon and diffusion; N diffusion for the N device &amp; P diffusion for the P device (illustrated in salmon and yellow coloring respectively). The output (\"out\") is connected together in metal (illustrated in cyan coloring). Connections between metal and polysilicon or diffusion are made through contacts (illustrated as black squares). The physical layout example matches the NAND logic circuit given in the previous example.\nThe N device is manufactured on a p-type substrate while the P device is manufactured in an n-type well (n-well). A p-type substrate \"tap\" is connected to VSS and an n-type n-well tap is connected to VDD to prevent latchup.\nPower: switching and leakage.\nCMOS logic dissipates less power than NMOS logic circuits because CMOS dissipates power only when switching (\"dynamic power\"). On a typical ASIC in a modern 90 nanometer process, switching the output might take 120 picoseconds, and happens once every ten nanoseconds. NMOS logic dissipates power whenever the transistor is on, because there is a current path from Vdd to Vss through the load resistor and the n-type network.\nStatic CMOS gates are very power efficient because they dissipate nearly zero power when idle. Earlier, the power consumption of CMOS devices was not the major concern while designing chips. Factors like speed and area dominated the design parameters. As the CMOS technology moved below sub-micron levels the power consumption per unit area of the chip has risen tremendously.\nBroadly classifying, power dissipation in CMOS circuits occurs because of two components, static and dynamic:\nStatic dissipation.\nBoth NMOS and PMOS transistors have a gate\u2013source threshold voltage (Vth), below which the current (called \"sub threshold\" current) through the device will drop exponentially. Historically, CMOS circuits operated at supply voltages much larger than their threshold voltages (Vdd might have been 5\u00a0V, and Vth for both NMOS and PMOS might have been 700\u00a0mV). A special type of the transistor used in some CMOS circuits is the native transistor, with near zero threshold voltage.\nSiO2 is a good insulator, but at very small thickness levels electrons can tunnel across the very thin insulation; the probability drops off exponentially with oxide thickness. Tunnelling current becomes very important for transistors below 130\u00a0nm technology with gate oxides of 20\u00a0\u00c5 or thinner.\nSmall reverse leakage currents are formed due to formation of reverse bias between diffusion regions and wells (for e.g., p-type diffusion vs. n-well), wells and substrate (for e.g., n-well vs. p-substrate). In modern process diode leakage is very small compared to sub threshold and tunnelling currents, so these may be neglected during power calculations.\nIf the ratios do not match, then there might be different currents of PMOS and NMOS; this may lead to imbalance and thus improper current causes the CMOS to heat up and dissipate power unnecessarily. Furthermore, recent studies have shown that leakage power reduces due to aging effects as a trade-off for devices to become slower.\nTo speed up designs, manufacturers have switched to constructions that have lower voltage thresholds but because of this a modern NMOS transistor with a Vth of 200\u00a0mV has a significant subthreshold leakage current. Designs (e.g. desktop processors) which include vast numbers of circuits which are not actively switching still consume power because of this leakage current. Leakage power is a significant portion of the total power consumed by such designs. Multi-threshold CMOS (MTCMOS), now available from foundries, is one approach to managing leakage power. With MTCMOS, high Vth transistors are used when switching speed is not critical, while low Vth transistors are used in speed sensitive paths. Further technology advances that use even thinner gate dielectrics have an additional leakage component because of current tunnelling through the extremely thin gate dielectric. Using high-\u03ba dielectrics instead of silicon dioxide that is the conventional gate dielectric allows similar device performance, but with a thicker gate insulator, thus avoiding this current. Leakage power reduction using new material and system designs is critical to sustaining scaling of CMOS.\nDynamic dissipation.\nCharging and discharging of load capacitances.\nCMOS circuits dissipate power by charging the various load capacitances (mostly gate and wire capacitance, but also drain and some source capacitances) whenever they are switched. In one complete cycle of CMOS logic, current flows from VDD to the load capacitance to charge it and then flows from the charged load capacitance (CL) to ground during discharge. Therefore, in one complete charge/discharge cycle, a total of Q=CLVDD is thus transferred from VDD to ground. Multiply by the switching frequency on the load capacitances to get the current used, and multiply by the average voltage again to get the characteristic switching power dissipated by a CMOS device: formula_1.\nSince most gates do not operate/switch at every clock cycle, they are often accompanied by a factor formula_2, called the activity factor. Now, the dynamic power dissipation may be re-written as formula_3.\nA clock in a system has an activity factor \u03b1=1, since it rises and falls every cycle. Most data has an activity factor of 0.1. If correct load capacitance is estimated on a node together with its activity factor, the dynamic power dissipation at that node can be calculated effectively.\nShort-circuit power.\nSince there is a finite rise/fall time for both pMOS and nMOS, during transition, for example, from off to on, both the transistors will be on for a small period of time in which current will find a path directly from VDD to ground, hence creating a short-circuit current, sometimes called a \"crowbar\" current. Short-circuit power dissipation increases with the rise and fall time of the transistors.\nThis form of power consumption became significant in the 1990s as wires on chip became narrower and the long wires became more resistive. CMOS gates at the end of those resistive wires see slow input transitions. Careful design which avoids weakly driven long skinny wires reduces this effect, but crowbar power can be a substantial part of dynamic CMOS power.\nInput protection.\nParasitic transistors that are inherent in the CMOS structure may be turned on by input signals outside the normal operating range, e.g. electrostatic discharges or line reflections. The resulting latch-up may damage or destroy the CMOS device. Clamp diodes are included in CMOS circuits to deal with these signals. Manufacturers' data sheets specify the maximum permitted current that may flow through the diodes.\nAnalog CMOS.\nBesides digital applications, CMOS technology is also used in analog applications. For example, there are CMOS operational amplifier ICs available in the market. Transmission gates may be used as analog multiplexers instead of signal relays. CMOS technology is also widely used for RF circuits all the way to microwave frequencies, in mixed-signal (analog+digital) applications.\nRF CMOS.\nRF CMOS refers to RF circuits (radio frequency circuits) which are based on mixed-signal CMOS integrated circuit technology. They are widely used in wireless telecommunication technology. RF CMOS was developed by Asad Abidi while working at UCLA in the late 1980s. This changed the way in which RF circuits were designed, leading to the replacement of discrete bipolar transistors with CMOS integrated circuits in radio transceivers. It enabled sophisticated, low-cost and portable end-user terminals, and gave rise to small, low-cost, low-power and portable units for a wide range of wireless communication systems. This enabled \"anytime, anywhere\" communication and helped bring about the wireless revolution, leading to the rapid growth of the wireless industry.\nThe baseband processors and radio transceivers in all modern wireless networking devices and mobile phones are mass-produced using RF CMOS devices. RF CMOS circuits are widely used to transmit and receive wireless signals, in a variety of applications, such as satellite technology (such as GPS), Bluetooth, Wi-Fi, near-field communication (NFC), mobile networks (such as 3G and 4G), terrestrial broadcast, and automotive radar applications, among other uses.\nExamples of commercial RF CMOS chips include Intel's DECT cordless phone, and 802.11 (Wi-Fi) chips created by Atheros and other companies. Commercial RF CMOS products are also used for Bluetooth and wireless LAN (WLAN) networks. RF CMOS is also used in the radio transceivers for wireless standards such as GSM, Wi-Fi, and Bluetooth, transceivers for mobile networks such as 3G, and remote units in wireless sensor networks (WSN).\nRF CMOS technology is crucial to modern wireless communications, including wireless networks and mobile communication devices. One of the companies that commercialized RF CMOS technology was Infineon. Its bulk CMOS RF switches sell over 1billion units annually, reaching a cumulative 5billion units, as of 2018[ [update]].\nTemperature range.\nConventional CMOS devices work over a range of \u221255\u00a0\u00b0C to +125\u00a0\u00b0C.\nThere were theoretical indications as early as August 2008 that silicon CMOS will work down to \u2212233\u00a0\u00b0C (40\u00a0K). Functioning temperatures near 40\u00a0K have since been achieved using overclocked AMD Phenom II processors with a combination of liquid nitrogen and liquid helium cooling.\nSilicon carbide CMOS devices have been tested for a year at 500\u00a0\u00b0C.\nSingle-electron MOS transistors.\nUltra small (L = 20\u00a0nm, W = 20\u00a0nm) MOSFETs achieve the single-electron limit when operated at cryogenic temperature over a range of \u2212269\u00a0\u00b0C (4\u00a0K) to about \u2212258\u00a0\u00b0C (15\u00a0K). The transistor displays Coulomb blockade due to progressive charging of electrons one by one. The number of electrons confined in the channel is driven by the gate voltage, starting from an occupation of zero electrons, and it can be set to one or many.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49421", "revid": "29", "url": "https://en.wikipedia.org/wiki?curid=49421", "title": "Edward I the Elder of England", "text": ""}
{"id": "49429", "revid": "12336988", "url": "https://en.wikipedia.org/wiki?curid=49429", "title": "Differentiable", "text": ""}
{"id": "49430", "revid": "23762609", "url": "https://en.wikipedia.org/wiki?curid=49430", "title": "Robert Abercromby (Jesuit)", "text": "Scottish Jesuit missionary\nRobert Abercromby (1536 \u2013 27 April 1613), whose surname was also spelled as Abrecromby and Abercrombie, and was known by such pseudonyms as Robert Sandiesoun and Sanders Robertson, was a Scottish Jesuit missionary.\nEarly life.\nHe was born and educated in Scotland, and studied in the Collegium Romanum in Rome, where on 19 August 1563 he became a Jesuit. From 1564 he lived in Braunsberg (then in Royal Prussia; present-day Braniewo) where he was professor of grammar in the biggest Polish Jesuit \"collegium\" (where teaching was in Latin) and a novice master.\nIn 1565 he was ordained a priest. In Braniewo he was in constant contact with Stanislaus Hosius. Learning Polish was difficult for him, and he had some problems with the finances of the school. Due to these problems he was permitted to leave Poland in 1580, when he met the Scottish king for the first time. In September 1580 he went back to Poland - from 1580 to 1587 he performed similar tasks in Krak\u00f3w, Pozna\u0144 and Wilno.\nIn 1587 he left Poland and returned to Scotland. During the journey to Scotland in 1580 and during his second stay there he was organizing transports of Scottish Catholic novices to be trained in Polish schools and seminaries.\nContact with Anne of Denmark.\nAbercromby claimed that he had reconciled Anne of Denmark, queen of James VI of Scotland, to the Catholic Church. James apparently allowed Abercromby to meet her at Holyroodhouse circa 1599. She made no outward sign of a change of religion.\nLater life.\nAbercromby remained in Scotland for some time, but a price of 10,000 crowns was put upon his head. He spent the period 1601\u201306 under the protection of George Gordon, 1st Marquess of Huntly.\nAbercromby went back to Braunsberg in 1606. His name was connected to the allegiance oath controversy when a pamphlet \"pasquil\", \"Exetasis epistol\u00e6 nomine regis\", written under the pseudonym Bartholus Pacenius against James I was traced to Braunsberg; but the investigation by Patrick Gordon was inconclusive. He died there on 27 April 1613.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49431", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=49431", "title": "Francis Pharcellus Church", "text": "American publisher and editor (1839\u20131906)\nFrancis Pharcellus Church (February 22, 1839 \u2013 April 11, 1906) was an American publisher and editor. In 1897, Church wrote the editorial \"Yes, Virginia, there is a Santa Claus\". Produced in response to eight-year-old Virginia O'Hanlon's letter asking whether Santa Claus was real, the widely republished editorial has become one of the most famous ever written.\nBorn in Rochester, New York, Church graduated from Columbia University and embarked on a career in journalism. With his brother, William Conant Church, Francis founded and edited several periodicals including \"The Army and Navy Journal\", \"The Galaxy\", and the \"Internal Revenue Record and Customs Journal\". He was a war correspondent for \"The New York Times\" during the American Civil War. He worked at \"The Sun\" in the early 1860s and again from 1874 until his death, writing thousands of editorials.\nChurch died in New York City and was buried at Sleepy Hollow Cemetery.\nEarly life and education.\nFrancis \"Frank\" Pharcellus Church was born in Rochester on February 22, 1839, to Pharcellus Church, a Baptist minister, and Chara Emily Church (n\u00e9e Conant). He had three sisters; an older brother, William Conant Church; and a younger brother, John Adams Church. As a child, Francis looked up to William \"as his 'big brother' and was his 'admiring satellite'.\" In 1848, the family moved to Boston, where Pharcellus preached at Bowdoin Square Baptist Church and edited the \"Watchman and Reflector,\" a weekly Baptist newspaper. In 1852, Pharcellus' health failed; he resigned his pastorship and moved the family to Chara's home in Vermont. The following year, the family moved a final time, to Brooklyn. Francis began to attend Manhattan's Columbia Grammar &amp; Preparatory School, whose headmaster was Charles Anthon. His education was centered around math and foreign languages.\nFrancis Church matriculated at Columbia College in New York City, where he graduated with honors in 1859. He earned a Master of Arts two years later. Although Church had entered university studying law and divinity, and spent a time studying under the judge Hooper C. Van Vorst, he soon switched his focus completely to writing and had graduated Columbia studying journalism.\nWriting and publishing career.\nAfter graduation, Church found work at \"The New York Chronicle\", which was published by his father and brother. For a time after William left to work at \"The Sun\", Francis Church was the chief assistant at the \"Chronicle\", but he eventually left to work at \"The Sun\" as well. In 1862, he covered the American Civil War for \"The New York Times\".\nIn 1863, Church, his brother William, and others established \"The Army and Navy Journal\" to promote loyalty to the Union during the Civil War and report on military affairs. During the war, Church worked for the \"Journal\" as a war correspondent, and from 1863 to 1865, he was an editor and publisher of the \"Journal\". He remained co-publisher until 1874.\nIn 1866, the brothers founded the \"Galaxy\" literary magazine as a competitor to \"The Atlantic Monthly\";137 Church was a publisher for two years and an editor there until 1872 or 1878. The \"Dictionary of Literary Biography\" credits Francis with doing \"most of the editorial work.\" As editors, the brothers became known for their heavy-handed style, for instance cutting major parts of Rebecca Harding Davis's \"Waiting for the Verdict\" when they serialized it. Supported by literary figures, notably Edmund Clarence Stedman, the brothers worked to attract the best authors possible to their publication, though they focused on New York authors and largely ignored the well-established literary society in New England. Stedman, while speaking about the editors in 1903, stated that the magazine focused on featuring authors from across the United States and did not focus on publishing works from popular authors.\nThey published the magazine fortnightly for a year, then switched to a monthly format. In 1870, Church proposed that Mark Twain contribute a \"Memoranda\" column in the magazine, a request Twain accepted; he edited the column from May 1870 to March 1871. Altogether, the magazine published the work of more than 600 authors, including Rebecca Harding Davis, Henry James, John William De Forest, Rose Terry Cooke, John Esten Cooke, and Constance Fenimore Woolson. The magazine's circulation peaked around 21,000 in 1871 and fell dramatically afterwards. The \"Galaxy\" merged with the \"Atlantic Monthly\" in 1878.137\nChurch also managed the \"Internal Revenue Record and Customs Journal\" with his brother from 1870 to 1895. He was re-hired as a part-time editor and writer at \"The Sun\" in 1874. He started working full-time there after leaving \"The Galaxy\". In this capacity, Church published thousands of editorials, most of which attracted little note. One of his more popular editorials was in response to a maid asking about etiquette, after which Church wrote a series of additional replies to letters asking for advice. He continued to work for \"The Sun\" until his death in 1906.\nEdward Page Mitchell, \"The Sun\"'s editor-in-chief, later said Church had \"a knowledge of journalistic history and an insight into journalistic character that could hardly be expected of any but a major figure in the profession.\" Mitchell also considered Church \"energetic and a brilliant conversationalist.\" An obituary published in \"The New York Times\" described Church as not being well known among literary circles because his reputation had been \"merged\" with that of \"The Sun\", but among those who knew him he was \"highly and justly esteemed.\" It said his editorial style specialized in treating theological topics \"from a secular point of view.\"\n He disliked politics.\n\"Yes, Virginia\".\nIn 1897, Mitchell gave Church a letter written to \"The Sun\" by eight-year-old Virginia O'Hanlon, who wanted to know whether there truly is a Santa Claus. In Church's 416-word response, he wrote that Santa exists \"as certainly as love and generosity and devotion exist\". \"Yes, Virginia, there is a Santa Claus\" became Church's best-known work and the most reprinted editorial in newspaper history.\nMitchell reported that Church, who was initially reluctant to write a response, produced it \"in a short time\" during an afternoon.90 Upon publication on September 21, 1897, journalist Charles Anderson Dana described Church's writing as \"Real literature,\" and said, \"Might be a good idea to reprint it every Christmas\u2014yes, and even tell who wrote it!\"\nThe editorial was first reprinted five years later to answer readers' demand for it. \"The Sun\" started reprinting the editorial annually in 1920 at Christmas, and continued until the paper's bankruptcy in 1950. Because \"The Sun\" traditionally did not byline their editorials, Church was not known to be the author until his death in 1906. The editorial is just one of two whose authorship \"The Sun\" disclosed.\nThe editorial, which has been described as \"the most famous editorial in history\", has been translated into 20 languages, set to music, and adapted into at least two movies. A book based on the editorial, \"Is there a Santa Claus\"?, was published in 1921.\nPersonal life and death.\nIn 1871, he married Elizabeth Wickham, who was from Philadelphia. In 1882 or 1883, Church moved from 107 East 35th Street to the Florence Apartment House, located at East 18th Street and East Union Place (now known as Park Avenue South). He and his wife lived there until 1890. They had no children.91\nHe was a member of the Sons of the Revolution, the National Sculpture Society, and the Century Association.\nChurch died in New York City on April 11, 1906, at the age of 67, at his home on 46 East 30th Street. He had an unknown illness for several months before his death. He was buried in Sleepy Hollow Cemetery in Sleepy Hollow, New York.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49434", "revid": "122969", "url": "https://en.wikipedia.org/wiki?curid=49434", "title": "Conjunction (astronomy)", "text": "When two astronomical objects have the same right ascension or the same ecliptic longitude\nIn astronomy, a conjunction occurs when two astronomical objects or spacecraft appear to be close to each other in the sky. This means they have either the same right ascension or the same ecliptic longitude, usually as observed from Earth.\nWhen two objects always appear close to the ecliptic\u2014such as two planets, the Moon and a planet, or the Sun and a planet\u2014this fact implies an apparent close approach between the objects as seen in the sky. A related word, \"appulse\", is the minimum apparent separation in the sky of two astronomical objects.\nConjunctions involve either two objects in the Solar System or one object in the Solar System and a more distant object, such as a star. A conjunction is an apparent phenomenon caused by the observer's perspective: the two objects involved are not actually close to one another in space. Conjunctions between two bright objects close to the ecliptic, such as two bright planets, can be seen with the naked eye.\nThe astronomical symbol for conjunction is (Unicode U+260C \u260c). \nThe conjunction symbol is not used in modern astronomy. It continues to be used in astrology.\nPassing close.\nMore generally, in the particular case of two planets, it means that they merely have the same right ascension (and hence the same hour angle). This is called conjunction in right ascension. However, there is also the term conjunction in ecliptic longitude. At such conjunction both objects have the same ecliptic longitude. Conjunction in right ascension and conjunction in ecliptic longitude do not normally take place at the same time, but in most cases nearly at the same time. However, at triple conjunctions, it is possible that a conjunction only in right ascension (or ecliptic length) occurs. At the time of conjunction\u00a0\u2013 it does not matter if in right ascension or in ecliptic longitude\u00a0\u2013 the involved planets are close together upon the celestial sphere. In the vast majority of such cases, one of the planets will appear to pass north or south of the other.\nPassing closer.\nHowever, if two celestial bodies attain the same declination (or ecliptic latitude) at the time of a conjunction then the one that is closer to the Earth will pass in front of the other. If one object moves into the shadow of another, the event is an eclipse. For example, the Moon passing through the shadow of Earth is called a lunar eclipse. If the visible disk of the nearer object is considerably smaller than that of the farther object, the event is called a transit, such as a transit of Mercury or a transit of Venus across the sun. When the nearer object appears larger than the farther one, it will completely obscure its smaller companion; this is called an occultation. An example of an occultation is when the Moon is relatively near and therefore large and passes between Earth and the Sun, causing the Sun to disappear entirely (a total solar eclipse). Occultations in which the larger body is neither the Sun nor the Moon are very rare. More frequent, however, is an occultation of a planet by the Moon. Several such events are visible every year from various places on Earth.\nPosition of the observer.\nA conjunction, as a phenomenon of perspective, is an event that involves two astronomical bodies seen by an observer on the Earth. Times and details depend only very slightly on the observer's location on the Earth's surface, with the differences being greatest for conjunctions involving the Moon because of its relative closeness, but even for the Moon the time of a conjunction never differs by more than a few hours.\nSuperior and inferior conjunctions with the Sun.\nA planet is said to be superior to another if it is farther from the sun. As seen from a superior planet, if an inferior planet is on the opposite side of the Sun, it is in superior conjunction, and in inferior conjunction if on the same side of the Sun. In an inferior conjunction, the superior planet is \"in opposition\" to the Sun as seen from the inferior planet.\nThe terms \"inferior conjunction\" and \"superior conjunction\" are used in particular for the planets Mercury and Venus, which are inferior planets as seen from Earth. However, this definition can be applied to any pair of planets, as seen from the one farther from the Sun.\nA planet (or asteroid or comet) is simply said to be in conjunction, when it is in conjunction with the Sun, as seen from Earth. The Moon is in conjunction with the Sun at New Moon.\nMultiple conjunctions and quasiconjunctions.\nConjunctions between two planets can be single, triple, or even quintuple. Quintuple conjunctions involve Mercury, because it moves rapidly east and west of the sun, in a synodic cycle just 116 days in length. An example will occur in 2048, when Venus, moving eastward behind the Sun, encounters Mercury five times (February 16, March 16, May 27, August 13, and September 5).\nThere is also a so-called quasiconjunction, when a planet in retrograde motion\u00a0\u2014 always either Mercury or Venus, from the point of view of the Earth\u00a0\u2014 will \"drop back\" in right ascension until it almost allows another planet to overtake it, but then the former planet will resume its forward motion and thereafter appear to draw away from it again. This will occur in the morning sky, before dawn. The reverse may happen in the evening sky after dusk, with Mercury or Venus entering retrograde motion just as it is about to overtake another planet (often Mercury \"and\" Venus are \"both\" of the planets involved, and when this situation arises they may remain in very close visual proximity for several days or even longer). The quasiconjunction is reckoned as occurring at the time the distance in right ascension between the two planets is smallest, even though, when declination is taken into account, they may appear closer together shortly before or after this.\nAverage interval between conjunctions.\nThe interval between two conjunctions involving the same two planets is not constant, but the average interval between two similar conjunctions can be calculated from the periods of the planets. The \"speed\" at which a planet goes around the Sun, in terms of revolutions per time, is given by the inverse of its period, and the speed difference between two planets is the difference between these. For conjunctions of two planets beyond the orbit of Earth, the average time interval between two conjunctions is the time it takes for 360\u00b0 to be covered by that speed difference, so the average interval is:\nformula_1\nThis does not apply of course to the intervals between the individual conjunctions of a triple conjunction.\nConjunctions between a planet inside the orbit of Earth (Venus or Mercury) and a planet outside are a bit more complicated. As the outer planet swings around from being in opposition to the Sun to being east of the Sun, then in superior conjunction with the Sun, then west of the Sun, and back to opposition, it will be in conjunction with Venus or Mercury an odd number of times. So the average interval between, say, the first conjunction of one set and the first of the next set will be equal to the average interval between its oppositions with the Sun. Conjunctions between Mercury and Mars are usually triple, and those between Mercury and planets beyond Mars may also be. Conjunctions between Venus and the planets beyond Earth may be single or triple.\nAs for conjunctions between Mercury and Venus, each time Venus goes from maximum elongation to the east of the Sun to maximum elongation west of the Sun and then back to east of the Sun (a so-called synodic cycle of Venus), an even number of conjunctions with Mercury take place. There are usually four, but sometimes just two, and sometimes six, as in the cycle mentioned above with a quintuple conjunction as Venus moves eastward, preceded by a singlet on August 6, 2047, as Venus moves westward. The average interval between corresponding conjunctions (for example the first of one set and the first of the next) is 1.599 years (583.9 days), based on the orbital speeds of Venus and Earth, but arbitrary conjunctions occur at least twice this often. The synodic cycle of Venus (1.599 years) is close to five times as long as that of Mercury (0.317 years). When they are in phase and move between the Sun and the Earth at the same time they remain close together in the sky for weeks.\nThe following table gives these average intervals, between corresponding conjunctions, in Julian years of 365.25 days, for combinations of the nine traditional planets. Conjunctions with the Sun are also included. Since Pluto is in resonance with Neptune the period used is 1.5 times that of Neptune, slightly different from the current value. The interval is then exactly thrice the period of Neptune.\nApproximate conjunctions of more than two planets.\nA conjunction in which three or more planets simultaneously have the same longitude will almost surely never happen, but because the ratios of the synodic cycles are not rational numbers, this situation can be approached arbitrarily closely. In 1953\u00a0BC Mercury, Venus, Mars, Jupiter, and Saturn were all in a longitude range of 4.3\u00b0 (see below). The graph below shows the standard deviation of the differences between the ecliptic longitudes of the five naked-eye planets (not including Uranus) and that of the sun, showing times when these five planets were fairly close together. In 1961 and again in 1997 Jean Meeus found several such groupings over the millennia.\nSince three planets having the same longitude and the same latitude involves four equations (equating longitude and latitude of the second and third planets to those of the first) and four variables (the positions of the earth and the three other planets in their orbits), it is in principle possible to have three planets perfectly lined up. In reality this will almost surely never happen, but is approached arbitrarily closely given enough time. But by the same token, a perfect alignment of four planets as seen from Earth is not possible (there are six equations to be solved but only five variables) unless the Solar System is very special.\nAs explained above, each planet has a synodic period, the average period between two moments when the planet comes back to any given point in its synodic trajectory, that is, when its longitude around the sun compared to that of the earth attains a given value. Although it does not happen that two planets come back to their starting points in their synodic trajectories at the same time, there are intervals of time after which this almost happens. The more planets are included, the more difficult it is for them all to return close to their points of origin relative to the earth, as seen in the following table of examples involving all five naked-eye planets. The numbers of cycles executed by each planet in the interval is expressed as a whole number plus or minus a fraction, and the column \"Maximum error\" gives the maximum of the fractional parts, which is attained by two of the planets (except in the case of 1768.068 years). Even in the interval of around 4249 years, this error is more than 0.012 cycles, equivalent to about 4\u00b0.\nSimilar intervals can be found involving fewer planets. Note that about four days before the end of the 1768.078-year interval, namely at 1768.068 years, Mars, Jupiter, and Saturn all come within 6\u00b0 of finishing whole numbers of synodic cycles at the same time. This corresponds to 89 great conjunctions (conjunctions between Jupiter and Saturn). At the end of the 4249-year interval (equivalent to 214 great conjunctions), these three planets are all around 0.013 from whole numbers of cycles, corresponding to 4\u00b0.\nNote that these intervals are not periods or cycles\u00a0\u2013 after a second interval, the plants will be twice as far from their original positions as after one interval.\nNotable conjunctions.\n1953 BC.\nOn February 27, 1953 BC, Mercury, Venus, Mars and Saturn formed a group with an angular diameter of 26.45 arc minutes. On the same day, Jupiter was only a few degrees away, so that on this day all five bright planets could be found in an area measuring only 4.33 degrees. This was described by David Pankenier in 1984 and later by Kevin Pang.\nThey, as well as David Nivison have suggested that this conjunction occurred at the beginning of the Xia dynasty in China.\n1576 BC.\nIn 1576 BC, at the time of the founding of the Shang dynasty, Chinese records say that \"the five planets moved in criss-cross fashion\". In early November, Mercury, Venus, Jupiter, and Saturn were together in the evening sky, with Mercury and Venus crossing Jupiter and Saturn, and in mid-December Mercury, Jupiter, and Saturn joined Mars in the morning sky, with Mars crossing Jupiter and Saturn, and Mercury crossing them twice, westward and then eastward.\n1059 BC.\nAnother five-planet conjunction occurred in 1059 BC and is mentioned in the Chinese \"Bamboo Annals\", though Nivison says that the Bamboo Annals moved the date one orbit of Jupiter earlier for political reasons.\nAD 710.\nOn 25 June, AD 710, the five naked-eye planets were in a span of just 6\u00b0 in the evening sky. This gathering was recorded by the Maya.\n929.\nA triple conjunction between Mars and Jupiter occurred. At the first conjunction on May 26, 929, Mars, whose brightness was \u22121.8 mag, stood 3.1 degrees south of Jupiter with a brightness of \u22122.6 mag. The second conjunction took place on July 4, 929, whereby Mars stood 5.7 degrees south of Jupiter. Both planets were \u22122.8 mag bright. On August 18, 929, the \u22121.9 mag bright Mars stood 4.7 degrees south of Jupiter, which was \u22122.6 mag bright.\n1054.\nOn July 5, 1054 a supernova brighter than Venus appeared in the eastern part of constellation Taurus in the proximity of the waning crescent Moon. The exact geocentric conjunction in right ascension took place at 07:58 UTC on this day with an angular separation of 3 degrees. It was perhaps the brightest star-like object in recorded history. The event is possibly shown on two petroglyphs in Arizona.\n1345.\nOn March 4, 1345, Mars, Jupiter, and Saturn were very close together, at the same time as a solar eclipse. Guy de Chauliac blamed the Black Death on this event.\n1503.\nBetween December 22, 1503, and December 27, 1503, all three bright outer planets Mars, Jupiter and Saturn reached their opposition to the Sun and stood therefore close together at the nocturnal sky. During the opposition period 1503 Mars stood 3 times in conjunction with Jupiter (October 5, 1503, January 19, 1504, and February 8, 1504) and 3 times in conjunction with Saturn (October 14, 1503, December 26, 1503, and March 7, 1504). Jupiter and Saturn stood on May 24, 1504, in close conjunction with an angular separation of 19 arcminutes.\n1604.\nOn October 9, 1604, a conjunction between Mars and Jupiter took place, whereby Mars passed Jupiter 1.8 degrees southward. Only two degrees away from Jupiter Kepler's Supernova appeared on the same day. This was perhaps the only time in recorded history a supernova took place near a conjunction of two planets.&lt;br&gt;\nSaturn passed Kepler's Supernova on December 12, 1604 33 arc minutes southly, which was however unobservable as the elongation to the sun was just 3.1 degrees. On December 24, 1604 Mercury stood in conjunction with Kepler's Supernova, whereby it was 1.8 degrees south of it. As the elongation of this event to the sun was 15 degree, it was in principle observable. On January 20, 1605 Venus passed Kepler's Supernova 29 arc minutes northwards at an elongation of 43.1 degrees to the sun.\n1899.\nIn early December 1899 the Sun and the naked-eye planets appeared to lie within a band 35 degrees wide along the ecliptic as seen from the Earth. As a consequence, over the period 1\u20134 December 1899, the Moon reached conjunction with, in order, Jupiter, Uranus, the Sun, Mercury, Mars, Saturn and Venus. Most of these conjunctions were not visible because of the glare of the Sun.\n1962.\nOver the period 4\u20136 February 1962, in a rare series of events, Mercury and Venus reached conjunction as observed from the Earth, followed by Venus and Jupiter, then by Mars and Saturn. Conjunctions took place between the Moon and, in turn, Mars, Saturn, the Sun, Mercury, Venus and Jupiter. Mercury also reached inferior conjunction with the Sun. The conjunction between the Moon and the Sun at new Moon produced a total solar eclipse visible in Indonesia and the Pacific Ocean,\nwhen these five naked-eye planets were visible in the vicinity of the Sun in the sky.\n1987.\nMercury, Venus and Mars separately reached conjunction with each other, and each separately with the Sun, within a 7-day period in August 1987 as seen from the Earth. The Moon also reached conjunction with each of these bodies on 24 August. However, none of these conjunctions were observable due to the glare of the Sun.\n2000.\nIn May 2000, in a very rare event, several planets lay in the vicinity of the Sun in the sky as seen from the Earth, and a series of conjunctions took place. Jupiter, Mercury and Saturn each reached conjunction with the Sun in the period 8\u201310 May. These three planets in turn were in conjunction with each other and with Venus over a period of a few weeks. However, most of these conjunctions were not visible from the Earth because of the glare from the Sun. NASA referred to May 5 as the date of the conjunction.\n2002.\nVenus, Mars and Saturn appeared close together in the evening sky in early May 2002, with a conjunction of Mars and Saturn occurring on 4 May. This was followed by a conjunction of Venus and Saturn on 7 May, and another of Venus and Mars on 10 May when their angular separation was only 18 arcminutes. A series of conjunctions between the Moon and, in order, Saturn, Mars and Venus took place on 14 May, although it was not possible to observe all these in darkness from any single location on the Earth.\n2007.\nA conjunction of the Moon and Mars took place on 24 December 2007, very close to the time of the full Moon and at the time when Mars was at opposition to the Sun. Mars and the full Moon appeared close together in the sky worldwide, with an occultation of Mars occurring for observers in some far northern locations.\nA similar conjunction took place on 21 May 2016 and on 8 December 2022.\n2008.\nA conjunction of Venus and Jupiter occurred on 1 December 2008, and several hours later both planets separately reached conjunction with the crescent Moon. An occultation of Venus by the Moon was visible from some locations. The three objects appeared close together in the sky from any location on the Earth.\n2013.\nAt the end of May, Mercury, Venus and Jupiter went through a series of conjunctions only a few days apart.\n2015.\nJune 30 \u2013 Venus and Jupiter come close together in a planetary conjunction; they came approximately 1/3 a degree apart. The conjunction had been nicknamed the \"Star of Bethlehem.\"\n2016.\nOn the morning of January 9, Venus and Saturn came together in a conjunction\nOn August 27, Mercury and Venus were in conjunction, followed by a conjunction of Venus and Jupiter, meaning that the three planets were very close together in the evening sky.\n2017.\nOn the morning of November 13, Venus and Jupiter were in conjunction, meaning that they appeared close together in the morning sky.\n2018.\nOn the early hours of January 7, Mars and Jupiter were in conjunction. The pair was only 0.25 degrees apart in the sky at its closest.\n2020.\nDuring most of February, March, and April, Mars, Jupiter, and Saturn were close to each other, and so they underwent a series of conjunctions: on March 20, Mars was in conjunction with Jupiter, and on March 31, Mars was in conjunction with Saturn.\nOn December 21, Jupiter and Saturn appeared at their closest separation in the sky since 1623, in an event known as a great conjunction.\n2022.\nPlanetoid Pallas passed Sirius, the brightest star in the night sky, on October 9 to the south at a distance of 8.5 arcminutes (source: Astrolutz 2022, ISBN 978-3-7534-7124-2). As Sirius is far south of the ecliptic only few objects of the solar system can be seen from earth close to Sirius.\nAt this occasion Pallas had not only the lowest angular distance to Sirius in the 21st century, but also since its discovery in 1802.\nIn the 19th century the greatest approach of Pallas and Sirius took place on October 11, 1879, when 8.6 mag bright Pallas passed Sirius 1.3\u00b0 southwest and in the 20th century the lowest distance between Pallas and Sirius was reached on October 12, 1962, when Pallas, whose brightness was also 8.6 mag, stood 1.4\u00b0 southwest of the brightest star in the sky.\n2024.\nOn August 15, 2024 there was an excellently visible conjunction between Mars and Jupiter in Taurus constellation.\n2025.\nOn June 29, 2025 there was the first conjunction of Saturn with Neptune with angular distance of 59.3 arc minutes. The second conjunction of this triple conjunction will be on August 6, 2025 whereby Saturn is 1.14 degrees south of Neptune.\nThe third and last conjunction of this triple will take place on February 16, 2026. On this day Saturn stands 54.7 arc minutes south of Neptune.\nAfter 2026 the next conjunction between Saturn and Neptune will be on June 7, 2061.\n2040.\nOn 9 September, 2040, all five naked-eye planets and the moon will be gathered close together in the evening sky.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49435", "revid": "39005691", "url": "https://en.wikipedia.org/wiki?curid=49435", "title": "Axel Oxenstierna", "text": "Swedish statesman (1583\u20131654)\nAxel Gustafsson Oxenstierna (; 1583\u20131654) was a Swedish statesman and Count of S\u00f6derm\u00f6re. He became a member of the Swedish Privy Council in 1609 and served as Lord High Chancellor of Sweden from 1612 until his death. He was a confidant of King Gustavus Adolphus and then Queen Christina, for whom he was at first regent. \nOxenstierna is widely considered one of the most influential people in Swedish history. He played an important role during the Thirty Years' War and was appointed Governor-General of occupied Prussia; he is also credited for having laid the foundations of the modern central administrative structure of the State, including the creation of counties ().\nEarly life and education.\nOxenstierna was born on 16 June 1583, at F\u00e5n\u00f6 in Uppland, the son of Gustaf Gabrielsson Oxenstierna (1551\u20131597) and Barbro Axelsdotter Bielke (1556\u20131624), as the oldest of nine siblings. His parents belonged to the ancient and influential high noble families of Oxenstierna and Bielke, both of which had held high offices in the state and the church for generations. After the death of her husband Gustaf, Axel's mother Barbro decided to let Axel and his brothers Christer and Gustaf finish their studies abroad. Thus, the brothers received their education at the universities of Rostock, Wittenberg and Jena. On returning home in 1603 he took up an appointment as valet de chambre \"(kammarjunkare)\" to King Charles IX of Sweden.\nOne of Oxenstierna's more unusual intellectual qualifications was his knowledge of the Scots language, reflecting the importance of the Scottish expatriate community in Sweden at that time. As Chancellor, he would regularly receive correspondence in Scots from his agent Sir James Spens, and he ventured into the language himself for an official letter to his Scottish counterpart, the Earl of Loudoun.\nCareer.\n1606\u20131611: Diplomat and Privy Councillor.\nIn 1606 he undertook his first diplomatic mission, to Mecklenburg and other German royal courts. While on diplomatic duty abroad, Oxenstierna gained appointment to the Privy Council (\"Riksr\u00e5det\"). Henceforth, Oxenstierna became one of the king's most trusted servants. In 1609 he travelled to Reval (present day Tallinn), on King Charles's behalf, to receive tributes from the city of Reval and the Estonian knighthood. Together with other councillors, Oxenstierna tried to warn the king of Denmark and the intentions of Danish King Christian IV. In 1610, Oxenstierna travelled to Copenhagen with the aim of preventing war with the neighbours, but unsuccessfully. The following year, Danish forces crossed the border, initiating the Kalmar War. In autumn 1611, King Charles died. Around New Year 1611\u201312, the parliament had to deal with the situation. According to the rules, the 17-year-old Gustavus Adolphus had not reached the proper age to be considered adult enough to rule as king. However, the estates agreed to disregard those rules. In return, the young king agreed to ensure the nobles further privileges and appoint Axel Oxenstierna Lord High Chancellor of Sweden.\n1612\u20131629: Lord High Chancellor and Governor-General.\nOn 6 January 1612 Oxenstierna became Lord High Chancellor (\"Rikskansler\") of the Privy Council. His controlling, organizing hand soon became apparent in every branch of the administration. Sweden was at the time troubled by three wars against Denmark (Kalmar War), Poland-Lithuania (Polish-Swedish War) and Russia (Ingrian War). Oxenstierna's first big task as Chancellor was to achieve peace in some of the wars. The war against Denmark was considered the most dangerous of the three as the enemy-controlled parts of Sweden itself. Negotiations began in Kn\u00e4red and Oxenstierna was first Swedish plenipotentiary. The negotiations led to the Treaty of Kn\u00e4red in 1613. For his efforts regarding these negotiations, Oxenstierna received the title of district judge in the hundred of Sn\u00e4vringe and, eventually, the barony of Kimito.\nDuring the frequent absences of Gustavus in Livonia and in Finland (1614\u20131616) Oxenstierna acted as his viceroy. One assignment Oxenstierna received while the king was in Livonia, was the task to finalize the negotiations regarding the marriage of John Casimir and the king's sister, Princess Catharina. At the coronation of Gustavus Adolphus, in October 1617, Oxenstierna was knighted. In 1620 he headed the embassy dispatched to Berlin to arrange the nuptial contract between Gustavus and Maria Eleonora of Brandenburg. During the king's Russian and Polish wars he had the principal duty of supplying the armies and the fleets with everything necessary, including men and money. Oxenstierna's ways of carrying out his assignments apparently gained King Gustavus's appreciation, since the king, in 1622, asked Oxenstierna to accompany him to Livonia and appointed him Governor-General and commandant of Riga, a strategically important town during the ongoing war against Poland. His services in Livonia gained him the reward of four castles (among others Burtnieki and Valmiera) and the whole bishopric of Wenden. Entrusted with the peace negotiations which led to the truce with Poland in 1623, he succeeded in averting a threatened rupture with Denmark in 1624. The Polish-Swedish War was reinitiated in 1626, and on 7 October that year, Oxenstierna became Governor-General in the newly acquired Swedish possession of Prussia. In 1629 he concluded the advantageous Truce of Altmark with Poland-Lithuania. Prior to this, in September 1628, he arranged a joint occupation of Stralsund with Denmark to prevent that important fortress from falling into the hands of the Imperialists.\nOxenstierna was not only highly successful within the diplomacy. During these years, he was entrusted with various important assignments in which he succeeded, such as gathering money and troops for the attack in Prussia in 1626. He played the leading organizational and administrational role in Prussia, as he had done earlier in Livonia. He was in charge of, for example, tolls, fortifications and the entire state grain trade. During the latter part of the 1620s, Elbl\u0105g (German: \"Elbing\"), where Oxenstierna resided and from where he governed the Swedish parts of Prussia, became a major Swedish centre of power, second only to Stockholm.\n1630\u20131636: Oxenstierna in the Thirty Years' War.\nWhen Sweden entered the Thirty Years' War in the summer of 1630, tolls from Oxenstierna-controlled Prussia, as well as food supplies acquired by Oxenstierna, were pivotal assets. He had also obtained credits from foreign businessmen, ensuring large sums of money making it possible to hire mercenary soldiers to the army used in Germany.\nAfter the Battle of Breitenfeld on 7 September 1631, Oxenstierna received a summons to assist the king with his counsels and co-operation in Germany. During the king's absence in Franconia and Bavaria in 1632 he held the appointment of \"legatus\" in the Rhineland, with plenipotentiary authority over all the German generals and princes in the Swedish service. Although he never fought a battle, he frustrated all the efforts of the Spanish troops by using strategically successful regulations. He managed to conduct large reinforcements to King Gustavus through the heart of Germany in the summer of 1632.\nIn the Battle of L\u00fctzen (1632), on 6 November 1632, Gustavus Adolphus died. This meant that Oxenstierna became supreme commander of the Swedish troops in Germany, although he let his subordinate generals be responsible for the military operations on a lower level. He moved his headquarters to Mainz, which in practice became the new Swedish capital. Oxenstierna was now absolute ruler of the significant area that the Swedish army had conquered in Germany. He was offered the position as prince-elector of Mainz, but, after serious considerations, the offer was turned down.\nWhen King Gustavus died in November 1632, his only legitimate and surviving child, Christina, was almost six years old. Until her declaration of majority at 18, a regency council ruled Sweden. This council was headed by Lord High Chancellor Oxenstierna, who wrote Instrument of Government (1634), a new constitution. During the years after the king's death, it became apparent that differences of opinion existed within the council. Some of Oxenstierna's colleagues recommended that Sweden should seek peace and withdraw from the war in Germany, not least after the defeat at N\u00f6rdlingen in 1634. However, Oxenstierna's opinion, that Sweden should remain in the war to ensure compensation for the sacrifices made, prevailed. The, for the Swedish side, disastrous outcome at N\u00f6rdlingen brought him, for an instant, to the verge of ruin and compelled him for the first time so far to depart from his policy of independence as to solicit direct assistance from France. But, well aware that Richelieu needed the Swedish armies as much as he himself needed money, he refused at the Conference of Compi\u00e8gne in 1635 to bind his hands in the future for the sake of some slight present relief. In 1636, nevertheless, he concluded a fresh subsidy-treaty with France at Wismar. Swedish troops remained in Germany all the way until 1648 and the Thirty Years' War's end. Oxenstierna, however, left Germany and returned to Stockholm in 1636, after ten years duty as premier Swedish representative in Prussia and Germany.\n1636\u20131654: Back in Sweden.\nOxenstierna more directly claimed his place within the regency of Queen Christina and became the young queen's teacher in statesmanship. His presence at home dominated all opposition, and such was the general confidence for Oxenstierna, that for the next nine years his voice, especially as regarding foreign affairs, remained omnipotent in the Privy Council.\nTorstenson War.\nIn May 1643, the Swedish Privy Council decided to attack Denmark. The Torstenson War was at large parts the work of Oxenstierna. The purpose was to gain territories from Denmark and be released from the Danish Sound Dues. Another factor might have been a will to avenge the tough peace Treaty of Kn\u00e4red in 1613. Whatever the reason, Oxenstierna considered the time was right to finally settle the score with Denmark. Swedish troops led by Field Marshal Lennart Torstensson attacked Danish Jutland from Germany, while Field Marshal Gustav Horn was in charge of the troops that attacked Scania. \nIn July 1644, Andries Bicker and Jacob de Witt were sent as envoy to Oxenstierna and the queen to mediate between Sweden and Denmark. Oxenstierna spoke High German, Christina Dutch. The outcome of the war was decided in the naval Battle of Fehmarn (1644) in October when the Royal Swedish Navy decisively defeated the Danish Navy. The defeat of the Danish Navy left the Danish isles open to a Swedish invasion, and Denmark sued for peace. The end of the war rested on the power of the Dutch naval dominance by Admiral Witte de With who arrived in the Sound in July to support the Dutch and Swedish point of view, a free passage. Oxenstierna was personally involved in the negotiations leading to the Treaty of Br\u00f6msebro at a creek in Blekinge. Sweden gained Gotland, Saaremaa (\u00d6sel), J\u00e4mtland, H\u00e4rjedalen and for thirty years Halland. Sweden had unrestricted access to the North Sea and was no longer encircled by Denmark\u2013Norway. Shortly after the peace treaty, Oxenstierna was created Count of S\u00f6derm\u00f6re.\nQueen Christina and her abdication.\nWhen Christina came of age, she tried to push Oxenstierna, her old mentor, aside. The relations between the two were not good and Oxenstierna always attributed the exiguousness of Sweden's gains under the Peace of Westphalia \u2212 Sweden gained only Western Pomerania, Usedom, Wollin, Wismar and Bremen-Verden \u2212 following the conference in Osnabr\u00fcck to Christina's undue interference. When the queen, a few years later, wanted to abdicate, Oxenstierna at first opposed this because he feared mischief to Sweden from the unruly and adventurous disposition of her preferred successor, Charles X Gustav. The chancellor changed his mind about Charles Gustav, however, and gave Christina the help she needed to go through with her abdication. Oxenstierna died two months after the ascension of the new king.\nDeath.\nOxenstierna died in Stockholm on 28 August 1654. He was interred in Storkyrkan, Stockholm on 18 March 1655. His body was then moved to J\u00e4der Church close to the Oxenstierna estate at Fiholm, in present-day Eskilstuna Municipality, where a vault had been built in accordance with his wishes. In the vault, \"Oxenstiernska gravvalvet\", several members of the Oxenstierna family have been buried, including Axel and his spouse Anna.\nPersonal life.\nFamily.\nOn 5 June 1608, Axel Oxenstierna married Anna \u00c5kesdotter B\u00e5\u00e5t (December 1579\u00a0\u2013 26 June 1649), a daughter of nobleman \u00c5ke Johansson B\u00e5\u00e5t and Christina Trolle. The wedding took place at Fiholm Castle, which was owned by the Oxenstierna family. Anna was constantly pregnant during her marriage, giving birth to 13 children in just 15 years, of which only five survived early childhood:\nProperties.\nOxenstierna was in possession of large estates and many mansions. During his life he owned palaces in, among others, Estonian Otep\u00e4\u00e4, in Latvian Burtnieki, Ropa\u017ei and Valmiera, in Finnish Nousiainen (Nousis) and in Stockholm (Oxenstiernska Palace). The foremost of the mansions was Tid\u00f6 Castle in V\u00e4stmanland. In 1647, Wallonian merchant Louis De Geer, who had returned from a Swedish trading expedition to West Africa, gifted four enslaved Africans to Oxenstierna.\nImpact and legacy.\nThe modernization of Sweden.\nAxel Oxenstierna is perhaps most remembered for the establishment of a uniform administrative system. He was ever-present during the vast reforms of the 1610s and 1620s, when the Swedish government was hugely modernized and made more effective. This was necessary for the war policies that would build the Swedish Empire. Among the areas reformed were army and navy organization and recruiting, trade and industrial policies, regional and local administration, the system of higher education, and the judicial system. The boundaries of the administrative counties of Sweden still to a large extent follow the boundaries established by Oxenstierna in the 17th century.\nRelationship with King Gustavus Adolphus.\nOxenstierna would not have had such an impact unless he had won the king's trust. From 1612, when Oxenstierna was appointed Lord High Chancellor, until 1632, when King Gustavus Adolphus died, the two men struck a long and successful partnership. They seem to have complemented each other. With Oxenstierna's own words, his \"cool\" balanced the king's \"heat\". More than once, the chancellor had to realize plans of the king, plans that sometimes were highly spontaneous and far from ready to be implemented in reality. When it came to entering the Thirty Years' War, Oxenstierna was not as enthusiastic as the king, but since the king's will was decisive, Oxenstierna accommodated himself to Gustavus's wish. At times, Oxenstierna stepped in to ease tense relations that the harsh behaviour of the king had caused. He regularly received the highest praise for his work from the king and there was almost no area in which King Gustavus did not consult his Lord High Chancellor Oxenstierna.\nInstrument of Government of 1634.\nThe Chancellor made large contributions to the Standing orders of the House of Knights (\"riddarhusordning\") of 1626. After the death of Gustavus Adolphus, Oxenstierna was the mind behind the Instrument of Government of 1634, in which, for example, the organization of the five Great Officers of the Realm was clarified. Five governmental branches, of which the Great Officers became heads, were established. Oxenstierna pushed through the Instrument of Government, but not without opposition. He claimed that the new form of government reflected the will of the late King Gustavus, making himself the interpreter of the king's thoughts and wishes, and leaving the opposition no possibility to control the truth in this.\nOpinions.\nOxenstierna is regarded as a brilliant pragmatist, willing to reconsider his positions. There are examples of discussions within the Privy Council when Oxenstierna rejected laws he himself had earlier introduced, admitting that he knew better now. His way of examining, reconsidering, testing, and sometimes rejecting his earlier opinions constitutes his legacy more than his ideas on particular points of policy.\nWhen he discovered that there were too few young noblemen to staff governmental positions, he worked to make it easier for boys outside the noble families to gain higher education, and gave them the possibility, eventually, to be raised to the nobility themselves. He could therefore be considered the father of Swedish meritocracy.\nOxenstierna was also a supporter of mercantilism and a believer in immigration and free enterprise.\nIn Germany, Oxenstierna became a fear-evoking character in a derived version of the popular German lullaby Schlaf, Kindlein, Schlaf!, in which he is referred to as \"Ochsenstern\".\nReception.\nDutch jurist and philosopher Hugo Grotius considered Oxenstierna \"the greatest man of the century\". French Cardinal Richelieu called him \"an inexhaustible source of fine advice\", while Richelieu's successor, Cardinal Mazarin, said that if all ministers of Europe were on the same ship, the helm would be handed to Oxenstierna. Pope Urban VIII claimed that Oxenstierna was one of the most excellent men the world had seen.\nQuotation.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"An nescis, mi fili, quantilla prudentia mundus regatur\"? \"Do you not know, my son, with how very little wisdom the world is governed?\"\nIn a letter to his son Johan written in 1648.\nAlthough attributed to Oxenstierna, the pope Julius III (1487-1555) is regarded to have been the first recorded author. Cardinal Richelieu has also been attributed to have been the author. This is probably the most famous Swedish quotation in the English-speaking world. The words were intended to encourage his son, a delegate to the negotiations that would lead to the Peace of Westphalia, who worried about his ability to hold his own amidst experienced and eminent statesmen and diplomats.\nIn fiction.\nFilm and TV.\nOxenstierna has been portrayed on the stage and on the screen several times, mainly due to his role as mentor and guardian to the enigmatic Queen Christina. He was played by Lewis Stone in Rouben Mamoulian's 1933 Hollywood movie \"Queen Christina\", with Greta Garbo as the female lead role, by Cyril Cusack in Anthony Harvey's \"The Abdication\" (1974) and by Michael Nyqvist in Mika Kaurism\u00e4ki's \"The Girl King\" (2015).\nOn stage.\nSamuel Ahlgren (1764\u20131816) played Oxenstierna in \"Drottning Kristina\" (1790), by the King Gustav III of Sweden who was an active playwright.\nIn August Strindberg's 1901 play \"Kristina\", Oxenstierna is portrayed as a cold realist criticising Christina's extravagant lifestyle and her gifts to favourites.\nThe bass part of Oxenstierna was first performed by Giovanni Carlo Casanova in Jacopo Foroni's 1849 opera \"Cristina, regina di Svezia\".\nLiterature.\nOxenstierna figures prominently in the \"Ring of Fire\" anthology Eric Flint and his 2011 novel \".\"\nGames.\nThe computer strategy game \"Europa Universalis IV\" has several in-game events related to Oxenstierna's reforms and regency.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49436", "revid": "15944312", "url": "https://en.wikipedia.org/wiki?curid=49436", "title": "Count Oxenstierna Axel Gustafsson", "text": ""}
{"id": "49441", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=49441", "title": "Klinefelter's Syndrome", "text": ""}
{"id": "49442", "revid": "2829238", "url": "https://en.wikipedia.org/wiki?curid=49442", "title": "Grand conjunction", "text": ""}
{"id": "49443", "revid": "1317285612", "url": "https://en.wikipedia.org/wiki?curid=49443", "title": "Religious humanism", "text": "Integration of humanist ethical philosophy\nReligious humanism or ethical humanism is an integration of humanist philosophy with congregational rites and community activity that center on human needs, interests, and abilities. Religious humanists set themselves apart from secular humanists by characterizing the nontheistic humanist life stance as a non-supernatural \"religion\" and structuring their organization around a congregational model.\nEthical Culture and religious humanist groups first formed in the United States from Unitarian ministers who sought to build a secular religion influenced by the thinking of French philosopher Auguste Comte.\nIn the 21st century, religious humanists commonly unite under the umbrella of Ethical Culture or Ethical Humanism. This phenomenon is primarily centered in the United States. While a British Ethical movement was notably active in the late 19th and early 20th centuries, it had gradually distanced itself from its \"religious\" aspects by the 1960s. Instead, it emphasized humanism less as a religious identity and more as a practical label describing rational and non-religious perspectives on morality and ethics. \nOrigins.\nIn the late 20th century the Humanist movement came into conflict only with conservative Christian groups in the United States . \"Secular humanism\" has become the most prominent form of organized Humanism. However, the American Humanist Association notes that it largely emerged from Ethical Culture, Unitarianism and Universalism.\nFrench Revolution.\nThe Cult of Reason () was an atheist philosophy devised during the French Revolution by Jacques H\u00e9bert, Pierre Gaspard Chaumette and their supporters.\nIn 1793 during the French Revolution, the cathedral Notre Dame de Paris was turned into a Temple to Reason and for a time Lady Liberty replaced the Virgin Mary on several altars.\nPositivism.\nIn the 1850s, Auguste Comte, the Father of Sociology, founded Positivism, a \"religion of humanity\". Auguste Comte was a student and secretary for Claude Henri de Rouvroy, Comte de Saint-Simon, the Father of French Socialism. Auguste Comte coined the term \"altruism\".\nThe BBC notes \"While atheism is merely the absence of belief, humanism is a positive attitude to the world, centered on human experience, thought, and hopes.\"\nHumanistic Religious Association.\nOne of the earliest forerunners of contemporary chartered humanist organizations was the Humanistic Religious Association formed in 1853 in London. This early group was democratically organized, with male and female members participating in the election of the leadership and promoted knowledge of the sciences, philosophy, and the arts.\nEthical Culture.\nThe Ethical Culture movement was founded in 1876. The movement's founder, Felix Adler, a former member of the Free Religious Association, conceived of Ethical Culture as a new religion that would strip away the accumulated unscientific dogmas of traditional religions while retaining and elevating the ethical message at the heart of all religions. Adler believed that traditional religions would ultimately prove to be incompatible with a scientific worldview. He felt that the vital aspects of religion should not be allowed to fall by the wayside. Religions provided vital functions in encouraging good works. And religions taught important truths about the world, albeit these truths were expressed through metaphors that were not always suited to modern understandings of the world. For example, monotheistic religions were based on a metaphor of an authoritarian monarchy, whereas democratic relationships were now understood to be the ideal.\nInitially, Ethical Culture involved little in the way of ceremony and ritual. Rather, Ethical Culture was religious in the sense of playing a defining role in people's lives and addressing issues of ultimate concern. Some Ethical Societies have subsequently added a degree of ritual as a means of marking special times or providing a tangible reminder of humanistic ideals.\nUnited States.\n19th century.\nBefore the term \"humanism\" was ever coined or even thought of being integrated into religion it had existed in America in at least an ideological sense for a very long time. Groups like the Free Religious Association (FRA) which was formed in 1867 and other less radical groups mainly consisting of extreme forms of early American Protestants such as the Unitarians and Quakers had existed from the first landings of the Europeans in the Western Hemisphere.\n20th century.\nIn 1915, a Positivist defined the term \"humanism\" in a magazine for the British Ethical Societies. Another Unitarian Minister John H. Dietrich read the magazine and adopted the term to describe his own religion. Dietrich is considered by some to be the \"Father of Religious Humanism\" (Olds 1996) particularly for his sermons while serving the First Unitarian Society of Minneapolis, which has since been considered the \"birthplace of Congregational Humanism\". \nIn 1929 Charles Francis Potter founded the First Humanist Society of New York whose advisory board included Julian Huxley, John Dewey, Albert Einstein and Thomas Mann. Potter was a minister from the Unitarian tradition and in 1930 he and his wife, Clara Cook Potter, published \"Humanism: A New Religion\". Throughout the 1930s Potter was a well known advocate of women's rights, access to birth control, \"civil divorce laws\", and an end to capital punishment.\nThe first \"Humanist Manifesto\" was written in 1933 primarily by Raymond Bragg and was published with thirty-four signatories. Unlike its subsequent revisions, the first manifesto described a new \"religion\", and referred to humanism as a religious movement meant to transcend and replace previous, deity-based religions. However, it is careful not to outline a creed or dogma. The document outlines a fifteen-point belief system, which, in addition to a secular outlook, opposes \"acquisitive and profit-motivated society\" and outlines a worldwide egalitarian society based on voluntary mutual cooperation. Bragg and eleven signatories were Unitarian ministers.\nThe Fellowship of Humanity was founded in 1935 by Reverend A. D. Faupel as one of a handful of \"humanist churches\" seeded in the early 20th century as part of the American Religious Humanism movement. It was the only such organization of that era to survive into the 21st century and is the first and oldest affiliate of the American Humanist Association.\nIn 1961 \"Webster's Third New International Dictionary\" defined religious humanism as \"A modern American movement composed chiefly of non-theistic humanists and humanist churches and dedicated to achieving the ethical goals of religion without beliefs and rites resting upon superstition.\"\nToday.\nSome of the US-based self-described Religious Humanist organizations currently active include:\nMany larger religious bodies include significant numbers of members and clergy who identify as being of humanist persuasion. These groups include\nSeeking to clarify that the word \"religious\" in Religious Humanism is not intended to imply a theistic or supernatural belief component, First Unitarian Society of Minneapolis, where Dietrich first used the term, has now rebranded the movement as \"Congregational Humanism\".\nUnited Kingdom.\nThe humanist movement first emerged in the UK as a religious \"ethical movement\" in the 19th century, with the South Place Religious Society in London being the largest \"ethical church\". The remaining UK ethical societies merged in the 1890s to become the Union of Ethical Societies, which was founded and presided over by Stanton Coit. Ethical societies in the United Kingdom had their heyday in the late 19th century and early 20th century, with hundreds still attending weekly Sunday services at the West London Ethical Society (now part of Humanists UK) and South Place Ethical Society (now Conway Hall) in London in the 1950s. But they did not persist in this form for much longer than that. As time went on, Coit believed it would be advantageous for humanists to consciously organise in church-like structures, and even to think of themselves as congregations as they did in the US, in order to be more appealing to people from a Christian background. But there was a difference of opinion within the movement as to how explicitly to project or emphasise that Ethical Culture was atheistic. Following Coit's tenure, much of his attempts to make humanism more \"congregational\" were swiftly reversed, and the trend went the other way. Both Conway Hall and the societies that made up the Ethical Union consciously moved away from the congregational model, becoming Conway Hall Ethical Society and the British Humanist Association (BHA) respectively. The BHA later became Humanists UK in the 2010s.\nIn 2013, the Sunday Assembly movement was founded in London as a \"godless congregation\" which was described in some places as \"church for atheists\", filling the niche vacated by other humanist groups.\nScandinavia.\nIn the Scandinavian countries, the popular Danish philosopher Harald H\u00f8ffding's positivist work \"Etik\" influenced the development of humanist societies, which in Sweden and Norway styled themselves as \"human-ethical associations\", alike the Ethical Humanists in America and formerly in Britain. In modern times, the religious humanist/secular humanist distinction has fallen away; Norway, Human-Etisk Forbund is the name of Norway's humanist association, but it is fully a part of the broader international humanist community, and uses both \"humanetikk\" and \"humanisme\" in describing its philosophy. In Sweden, the Human-Ethical Association rebranded as Humanisterna in 1999, dropping the congregational model as the British had done in the 1960s.\nBelgium.\nBelgium is broadly divided between its Flemish Community (Flanders) and French-speaking community (Wallonia). In French Belgium, as in France, the secular movement is more concerned with national political organising and manifests nationally as the . In Flemish Belgium, the group \"deMens.nu\" (Humanity Now) brings together local humanist associations who engage in a broader range of activities, including community-based work. As with Humanists UK in the UK, \"deMens.nu\" grew from the union of local liberal or freethought associations.\nDiscussion of terminology.\nPeople writing about religious humanism are careful to distinguish religious humanism from Jewish humanism (nonreligious Jews who are humanists), Christian humanism (religious Christians asserting the humanitarian aspects of their religion), and secular humanism (often simply \"humanism\", a non-religious approach to life), but confusion inevitably arises. Another such term is Secular Buddhism, which refers to an atheistic practice of Buddhist rituals.\nSome experts on humanism, including Andrew Copson, argue that there have been deliberate attempts to \"muddy the conceptual water... of a complicatedly imprecise philosophical term\" by adding the slew of qualifying adjectives to humanism. He asserts that the term \"Christian humanism\" was first used in 1944, and argues that it has largely been used by Christians \"as a way of co\u2010opting the (to them) amenable aspects of humanism for their religion.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49444", "revid": "15881234", "url": "https://en.wikipedia.org/wiki?curid=49444", "title": "Racial quota", "text": "Numerical requirements for hiring members of a particular racial group\nRacial quotas in employment and education are numerical requirements or quotas for hiring, promoting, admitting and/or graduating members of a particular racial group. Racial quotas are often established as means of diminishing racial discrimination, addressing under-representation and evident racism against those racial groups or, the opposite, against the disadvantaged majority group (see \"numerus clausus\" or \"bhumiputra\" systems). Conversely, quotas have also been used historically to promote discrimination against minority groups by limiting access to influential institutions in employment and education.\nThese quotas may be determined by governmental authority and backed by governmental sanctions. When the total number of jobs or enrollment slots is fixed, this proportion may get translated to a specific number.\nRegions and nations.\nAncient Mongolia.\nThe Mongols divided different races into a four-class system during the Yuan dynasty.\nThe Mongol Emperor Kublai Khan had introduced a hierarchy of reliability by dividing the population of the Yuan dynasty into the following classes:\nAncient China.\nSeveral laws enforcing racial segregation of foreigners were passed during the Tang dynasty. In 779 AD, the Tang dynasty issued an edict which forced Uighurs to wear their ethnic dress, and restricted them from marrying Chinese.\nIn 836 AD, Lu Chun was appointed as governor of Canton. He was disgusted to find Chinese living with foreigners and intermarriage. Lu enforced separation, banning interracial marriages, and prevented foreigners from owning properties.\nThe 836 law specifically banned Chinese from forming relationships with \"Dark peoples\" or \"People of colour\", terms referring to foreigners, such as \"Iranians, Sogdians, Arabs, Indians, Malays, Sumatrans\", etc.\nFrance.\nBy 1935, the French government enacted a series of racial quotas on certain professions.\nGermany.\nSee Nazi boycott of Jewish businesses.\nMalaysia.\nSee Bumiputera (Malaysia).\nUnited States.\nThe National Origins Formula was an American system of immigration quotas, between 1921 and 1965, which restricted immigration on the basis of existing proportions of the population. The goal was to maintain the existing ethnic composition of the United States. It had the effect of giving low quotas to Eastern and Southern Europe.\nSuch racial quotas were restored after the Civil Rights Act of 1964, especially during the 1970s. Richard Nixon's Labor Secretary George P. Shultz demanded that anti-black construction unions allow a certain number of black people into the unions. The Department of Labor began enforcing these quotas across the country. After a U.S. Supreme Court case, \"Griggs v. Duke Power Company\", found that neutral application tests and procedures that still resulted in \"de facto\" segregation of employees (if previous discrimination had existed) were illegal, more companies began implementing quotas on their own.\nIn a 1973 court case, a federal judge created one of the first mandated quotas when he ruled that half of the Bridgeport, Connecticut Police Department's new employees must be either black or Puerto Rican. In 1974, the Department of Justice and the United Steelworkers of America came to an agreement on the largest-to-then quota program, for steel unions.\nIn 1978, the U.S. Supreme Court ruled in \"Regents of the University of California v. Bakke\" that public universities (and other government institutions) could not set specific numerical targets based on race for admissions or employment. The Court said that \"goals\" and \"timetables\" for diversity could be set instead. A 1979 Supreme Court case, \"United Steelworkers v. Weber\", found that private employers could set rigid numerical quotas, if they chose to do so. In 1980, the Supreme Court found that a 10% racial quota for federal contractors was permitted.\nIn 1990 City University of New York was accused of discriminatory hiring practices against Italian-Americans.\nIn 1991, President George H. W. Bush made an attempt to abolish affirmative action altogether, maintaining that \"any regulation, rule, enforcement practice or other aspect of these programs that mandates, encourages, or otherwise involves the use of quotas, preferences, set-asides or other devices on the basis of race, sex, religion or national origin are to be terminated as soon as is legally feasible\". This claim led up to the creation of the Civil Rights Act of 1991; however, the document was not able to implement these changes. It only covered the terms for settling cases where discrimination has been confirmed to have occurred.\nCollege admissions in the United States have had racial quotas; see for details. These have notably included blanket bans on African-Americans, Jewish quotas from 1918 to the 1950s, and an alleged Asian quota from the 1980s and ongoing as of 2017[ [update]].\nSouth Africa.\nLocal trade unions commonly use the term \"Absolute representation\" in this regard.\nOpposition.\nOpponents of quotas object that one group is favored at the expense of another whenever a quota is invoked rather than factors such as grade point averages or test scores. They argue that using quotas displaces individuals that would normally be favored based on their individual achievements. Opponents of racial quotas believe that qualifications should be the only determining factor when competing for a job or admission to a school. It is argued this causes \"reverse discrimination\" where individuals in the majority to lose out to a minority.\nExamples.\nSome affirmative action programs openly involve quotas such as the admission program of the Universidade Federal do Rio Grande do Sul. (See also: Vestibular exam#Racial quotas.)\nThe law student organization Building a Better Legal Profession developed a method to encourage politically liberal students to avoid law firms whose racial makeup is markedly different from that of the population as a whole. In an October 2007 press conference reported in \"The Wall Street Journal\", and \"The New York Times\", the group released data publicizing the numbers of African-Americans, Hispanics, and Asian-Americans at America's top law firms. The group has sent information to top law schools around the country to encourage students who agree with its viewpoint to take the demographic data into account when they choose where to work after graduation. As more students choose where to work based on firms' diversity rankings, firms face an increasing market pressure to change theirs.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49448", "revid": "47154815", "url": "https://en.wikipedia.org/wiki?curid=49448", "title": "Horus", "text": "Egyptian war and sky deity\nHorus (), also known as Heru, Har, Her, or Hor () \u03e8\u2cb1\u2ca3 (Coptic), in Ancient Egyptian, is one of the most significant ancient Egyptian deities who served many functions, most notably as the god of kingship, healing, protection, the sun, and the sky. He was worshipped from at least the late prehistoric Egypt until the Ptolemaic Kingdom and Roman Egypt. Different forms of Horus are recorded in history, and these are treated as distinct gods by Egyptologists. These various forms may be different manifestations of the same multi-layered deity in which certain attributes or syncretic relationships are emphasized, not necessarily in opposition but complementary to one another, consistent with how the Ancient Egyptians viewed the multiple facets of reality. He was most often depicted as a falcon, most likely a lanner falcon or peregrine falcon, or as a man with a falcon head.\nThe earliest recorded form of Horus is the tutelary deity of Nekhen in Upper Egypt, who is the first known national god, specifically related to the ruling pharaoh who in time came to be regarded as a manifestation of Horus in life and Osiris in death. The most commonly encountered family relationship describes Horus as the son of Isis and Osiris, and he plays a key role in the Osiris myth as Osiris's heir and the rival to Set, the murderer and brother of Osiris. In another tradition, Hathor is regarded as his mother and sometimes as his wife.\nPracticing \"interpretatio romana\", Claudius Aelianus wrote that Egyptians called the god Apollo \"Horus\" in their own language. However, Plutarch, elaborating further on the same tradition reported by the Greeks, specified that the one \"Horus\" whom the Egyptians equated with the Greek Apollo was in fact \"Horus the Elder\", a primordial form of Horus whom Plutarch distinguishes from both Horus and Harpocrates.\nEtymology.\nHorus is recorded in Egyptian hieroglyphs as \"\u1e25r.w\" \"Falcon\", \ud80c\udd43; the original pronunciation has been reconstructed as in Old Egyptian and early Middle Egyptian, in later Middle Egyptian, and in Late Egyptian. Additional meanings are thought to have been \"the distant one\" or \"one who is above, over\". As the language changed over time, it appeared in Coptic varieties variously as or (\u03e8\u2cb1\u2ca3) and was adopted into ancient Greek as \"H\u014d\u0302ros\" (pronounced at the time as ). It also survives in Late Egyptian and Coptic theophoric name forms such as Siese\n\"son of Isis\" and Harsiese \"Horus, Son of Isis\".\nHorakhti or Hor-Akhti, \"Horus of the Two Horizons\", was the personification of the Sun on the horizon according to Egyptian mythology.\nHorus and the pharaoh.\nThe pharaoh was associated with many specific deities. He was identified directly with Horus, who represented kingship itself and was seen as a protector of the pharaoh, and he was seen as the son of Ra, who ruled and regulated nature as the pharaoh ruled and regulated society.\nThe Pyramid Texts (c.\u20092400\u20132300 BCE) describe the nature of the pharaoh in different characters as both Horus and Osiris. The pharaoh as Horus in life became the pharaoh as Osiris in death, where he was united with the other gods. New incarnations of Horus succeeded the deceased pharaoh on earth in the form of new pharaohs.The lineage of Horus, the eventual product of unions between the children of Atum, may have been a means to explain and justify pharaonic power. The gods produced by Atum were all representative of cosmic and terrestrial forces in Egyptian life. By identifying Horus as the offspring of these forces, then identifying him with Atum himself, and finally identifying the Pharaoh with Horus, the Pharaoh theologically had dominion over all the world.\nOrigin mythology.\nIn one tale, Horus was born after his mother Isis retrieved all the dismembered body parts of her murdered husband Osiris, except his penis, which was thrown into the Nile and eaten by a catfish/Medjed or, in some tellings, by a crab. Older Egyptian accounts have the penis of Osiris surviving. According to Plutarch's account, Isis used her magic powers to resurrect Osiris and fashion a phallus to conceive her son. After becoming pregnant, Isis fled to the Nile Delta marshlands to hide from her brother Set, who had jealously killed Osiris and who she knew would want to kill their son. There Isis bore a divine son, Horus. \nAs birth, death and rebirth are recurrent themes in Egyptian lore and cosmology, it is not particularly strange that another form of Horus is the brother of Osiris and Isis by Nut and Geb, together with Nephthys and Set. This elder Horus is called Heru-ur (also Hrw-wr or Hourou'Ur) in contrast to the younger Horus, Heru-pa-khered (also Hrw-P-Khrd), later adopted by the Greeks as Harpocrates.\nMythological roles.\nSky god.\nSince Horus was said to be the sky, he was considered to also contain the Sun and Moon. Egyptians believed that the Sun was his right eye and the Moon his left and that they traversed the sky when he, a falcon, flew across it. Later, the reason that the Moon was not as bright as the sun was explained by a tale, known as \"The Contendings of Horus and Seth\". In this tale, it was said that Seth, the patron of Upper Egypt, and Horus, the patron of Lower Egypt, had battled for Egypt brutally, with neither side victorious, until eventually, the gods sided with Horus.\nAs Horus was the ultimate victor he became known as \"\u1e25r.w or\" \"Horus the Great\", but more usually translated as \"Horus the Elder\". In the struggle, Set had lost a testicle, and Horus's eye was gouged out.\nHorus was occasionally shown in art as a naked boy with a finger in his mouth sitting on a lotus with his mother. In the form of a youth, Horus was referred to as \"nfr \u1e25r.w\" \"Good Horus\", transliterated Neferhor, Nephoros or Nopheros (reconstructed as ).\nThe Eye of Horus is an ancient Egyptian symbol of protection and royal power from deities, in this case from Horus or Ra. The symbol is seen on images of Horus's mother, Isis, and on other deities associated with her. In the Egyptian language, the word for this symbol was \"wedjat\" (\"w\u025ft\"). It was the eye of one of the earliest Egyptian deities, Wadjet, who later became associated with Bastet, Mut, and Hathor as well. Wadjet was a solar deity and this symbol began as her all-seeing eye. In early artwork, Hathor is also depicted with this eye. Funerary amulets were often made in the shape of the Eye of Horus. The Wedjat or Eye of Horus is \"the central element\" of seven \"gold, faience, carnelian and lapis lazuli\" bracelets found on the mummy of Shoshenq II. The Wedjat \"was intended to protect the king [here] in the afterlife\" and to ward off evil. Egyptian and Near Eastern sailors would frequently paint the symbol on the bow of their vessel to ensure safe sea travel.\nHorus was also thought to protect the sky.\nConflict between Horus and Set.\nHorus was told by his mother, Isis, to protect the people of Egypt from Set, the god of the desert, who had killed Horus's father, Osiris. Horus had many battles with Set, not only to avenge his father but to choose the rightful ruler of Egypt. In these battles, Horus came to be associated with Lower Egypt and became its patron.\nAccording to \"The Contendings of Horus and Seth\", Set is depicted as trying to prove his dominance by seducing Horus and then having sexual intercourse with him. However, Horus places his hand between his thighs and catches Set's semen, then subsequently throws it in the river so that he may not be said to have been inseminated by Set. Horus (or Isis herself in some versions) then deliberately spreads his semen on some lettuce, which was Set's favourite food. After Set had eaten the lettuce, they went to the gods to try to settle the argument over the rule of Egypt. The gods first listened to Set's claim of dominance over Horus, and call his semen forth, but it answered from the river, invalidating his claim. Then, the gods listened to Horus's claim of having dominated Set, and call his semen forth, and it answered from inside Set.\nHowever, Set still refused to relent, and the other gods were getting tired from over eighty years of fighting and challenges. Horus and Set challenged each other to a boat race, where they each raced in a boat made of stone. Horus and Set agreed, and the race started. But Horus had an edge: his boat was made of wood painted to resemble stone, rather than true stone. Set's boat, being made of heavy stone, sank, but Horus's did not. Horus then won the race, and Set stepped down and officially gave Horus the throne of Egypt. Upon becoming king after Set's defeat, Horus gives offerings to his deceased father Osiris, thus reviving and sustaining him in the afterlife. After the New Kingdom, Set was still considered the lord of the desert and its oases.\nIn many versions of the story, Horus and Set divide the realm between them. This division can be equated with any of several fundamental dualities that the Egyptians saw in their world. Horus may receive the fertile lands around the Nile, the core of Egyptian civilization, in which case Set takes the barren desert or the foreign lands that are associated with it; Horus may rule the earth while Set dwells in the sky; and each god may take one of the two traditional halves of the country, Upper and Lower Egypt, in which case either god may be connected with either region. Yet in the Memphite Theology, Geb, as judge, first apportions the realm between the claimants and then reverses himself, awarding sole control to Horus. In this peaceable union, Horus and Set are reconciled, and the dualities that they represent have been resolved into a united whole. Through this resolution, the order is restored after the tumultuous conflict.Egyptologists have often tried to connect the conflict between the two gods with political events early in Egypt's history or prehistory. The cases in which the combatants divide the kingdom, and the frequent association of the paired Horus and Set with the union of Upper and Lower Egypt, suggest that the two deities represent some kind of division within the country. Egyptian tradition and archaeological evidence indicate that Egypt was united at the beginning of its history when an Upper Egyptian kingdom, in the south, conquered Lower Egypt in the north. The Upper Egyptian rulers called themselves \"followers of Horus\", and Horus became the tutelary deity of the unified polity and its kings. Yet Horus and Set cannot be easily equated with the two halves of the country. Both deities had several cult centers in each region, and Horus is often associated with Lower Egypt and Set with Upper Egypt. Other events may have also affected the myth. Before even Upper Egypt had a single ruler, two of its major cities were Nekhen, in the far south, and Nagada, many miles to the north. The rulers of Nekhen, where Horus was the patron deity, are generally believed to have unified Upper Egypt, including Nagada, under their sway. Set was associated with Nagada, so it is possible that the divine conflict dimly reflects an enmity between the cities in the distant past. Much later, at the end of the Second Dynasty (c.\u20092890\u20132686 BCE), Pharaoh Seth-Peribsen used the Set animal to write his serekh name in place of the falcon hieroglyph representing Horus. His successor Khasekhemwy used both Horus and Set in the writing of his serekh. This evidence has prompted conjecture that the Second Dynasty saw a clash between the followers of the Horus king and the worshippers of Set led by Seth-Peribsen. Khasekhemwy's use of the two animal symbols would then represent the reconciliation of the two factions, as does the resolution of the myth.\nGolden Horus Osiris.\nHorus gradually took on the nature as both the son of Osiris and Osiris himself. He was referred to as Golden Horus Osiris. In the temple of Denderah he is given the full royal titulary of both that of Horus and Osiris. He was sometimes believed to be both the father of himself as well as his own son, and some later accounts have Osiris being brought back to life by Isis.\nForms of Horus.\nHeru-ur (Horus the Elder).\nHeru-ur, also known as Heru-wer, Semsem, Haroeris, Horus the Great, or Horus the Elder, was the mature representation of the god Horus. This manifestation of Horus was especially worshipped at Letopolis in Lower Egypt. The Greeks identified him with the Greek god Apollo.\nHis titles include: 'foremost of the two eyes', 'great god', 'lord of Ombos', 'possessor of the ijt-knife, who resides in Letopolis', 'Shu, son of Ra', 'Horus, strong of arm', 'great of power' and 'lord of the slaughter in the entire land'. 'Foremost of the two eyes' was a common epithet which was referring to the two eyes of the sky god. The two eyes represent the sun and the moon, as well as the Wadjet-eye, and played an important role in the cult of Heru-ur. His cult center was originally Letopolis; later he was also worshipped in Kom Ombo and Qus. In Kom Ombo, he was worshipped as the son of Ra and Heqet, the husband of his sister-wife Tasenetnofret and father of the child god Panebtawy.\nIn his Moralia, the Greek philosopher Plutarch mentions three additional parentage traditions that supposedly existed for Heru-ur during the Ptolemaic period. According to Plutarch's account, Heru-ur was believed to be the son of Geb and Nut, born on the second of the five intercalary days at the end of the year, after Osiris and before Set, Isis, and Nephthys. Plutarch also records a variant tradition that assigns different fathers to Nut's children: Osiris and Heru-ur are attributed to Nut and Ra, Isis to Nut and Thoth, while Nephthys and Set are said to be the children of Nut and Geb. Additionally, similar to other manifestations of Horus, Heru-ur is sometimes regarded as the child of Isis and Osiris, conceived by the pair while still within the womb of Nut.\nPlutarch aims to distinguish between the child form of Horus, the son of Osiris and Isis, and 'Haro\u00ebris' whom he refers to as 'the elder Horus'. Haro\u00ebris is the hellenized version of the Egyptian epithet 'Horus-wer', which directly translates to 'Horus the Great,' a term first appearing in Papyrus Spell 588, likely to differentiate Horus of the royal cult from lesser forms of Horus. However, ancient Egyptian texts do not maintain a distinction between a Horus the Elder and a 'younger' Horus. Horus-wer is also sometimes referred to as the son of Osiris and Isis, and 'wer' is a common epithet for ancient Egyptian gods and does not imply a separation between older and younger deities into two different generations.\nHeru-ur was sometimes depicted fully as a falcon; he was sometimes given the title Kemwer, meaning \"(the) great black (one)\". Heru-ur was also depicted as a Hieracosphinx (a falcon headed lion).\nOther variants include \"Hor Merti\" 'Horus of the two eyes' and \"Horkhenti Irti\".\nHeru-pa-khered (Horus the child).\nHeru-pa-khered (Harpocrates to the Ptolemaic Greeks), also known as Horus the child, is represented in the form of a youth wearing a lock of hair (a sign of youth) on the right of his head while sucking his finger. In addition, he usually wears the united crowns of Egypt, the crown of Upper Egypt and the crown of Lower Egypt. He is a form of the rising sun, representing its earliest light.\nAs early as the third millennium BCE, Ancient Egyptian texts such as the Pyramid Texts referenced the birth, youth, and adulthood of the god Horus. However, his image as a child deity was not firmly established until the first millennium BCE, when Egyptian theologians began associating child gods with adult gods. From a historical perspective, Harpocrates is an artificial creation, originating from the priesthood of Thebes and later gaining popularity in the cults of other cities. His first known depiction dates to a stele from Mendes, erected during the reign of Sheshonq III (22nd Libyan Dynasty), commemorating a donation by the flutist \u00c2nkhhorpakhered. Initially, Harpocrates originated as a duplicate of Khonsu-pa-khered, providing a child-god figure for the funerary gods Osiris and Isis. Unlike Horus, who was traditionally depicted as an adult, Khonsu, the lunar god, was inherently associated with youth. The cults of Harpocrates and Khonsu originally merged in a sanctuary within the Mut enclosure at Karnak. This sanctuary, later transformed into a mammisi (birth house) under the 21st Dynasty, celebrated the divine birth of the pharaoh, connecting the queen mother with the mother-goddesses Mut and Isis. The merging of local Theban beliefs with the Osiris cult endowed Harpocrates with dual ancestry, as seen in inscriptions at Wadi Hammamat which name him 'Horus-the-child, son of Osiris and Isis, the Elder, the first-born of Amun.' The Osirian tradition solidified Harpocrates as the archetype of child-gods, firmly integrated into the Osirian family.\nHeru-Behdeti (Horus of Behdet).\nThe winged sun of Horus of Edfu is a symbol in associated with divinity, royalty, and power in ancient Egypt. The winged sun was depicted on the top of pylons in the ancient temples throughout Egypt.\nHar-em-akhet (Horus in the Horizon).\nHar-em-akhet or Horemakhet (\"Harmakhis\" in Greek) represented the dawn and the early morning sun. He was often depicted as a sphinx with the head of a man (like the Great Sphinx of Giza), or as a hieracosphinx, a creature with a lion's body and a falcon's head and wings, sometimes with the head of a lion or ram (the latter providing a link to the god Khepri, the rising sun). It was believed that he was the inspiration for the Great Sphinx of Giza, constructed under the order of Khafre, whose head it depicts.\nHarpara (Horus the sun).\nHarpara (\"Horus the sun\") is the child of Montu and Raet-Tawy, and formed with them the divine triad of North Karnak and Armant. In Medamud, Harpara was worshipped as the firstborn son of Amun and Raet-Tawy. while he is elsewhere described as the son of Mehet-Weret. A local form of Thoth named Thoth-of-Armant was most likely worshipped as the adult form of Harpara. In his capacity as the young manifestation of Thoth, Harpara was considered a lunar deity at Armant.\nCelebrations of Horus.\nThe Festival of Victory (Egyptian: Heb Nekhtet) was an annual Egyptian festival dedicated to the god Horus. The Festival of Victory was celebrated at the Temple of Horus at Edfu, and took place during the second month of the Season of the Emergence (or the sixth month of the Egyptian calendar).\nThe ceremonies which took place during the Festival of Victory included the performance of a sacred drama which commemorated the victory of Horus over Set. The main actor in this drama was the king of Egypt himself, who played the role of Horus. His adversary was a hippopotamus, who played the role of Set. In the course of the ritual, the king would strike the hippopotamus with a harpoon. The destruction of the hippopotamus by the king commemorated the defeat of Set by Horus, which also legitimised the king.\nIt is unlikely that the king attended the Festival of Victory every year; in many cases he was probably represented by a priest. It is also unlikely that a real hippopotamus was used in the festival every year; in many cases it was probably represented by a model.\nThe 4th-century Roman author Macrobius mentions another annual Egyptian festival dedicated to Horus in his \"Chronicon\". Macrobius specifies this festival as occurring on the winter solstice. The 4th-century Christian bishop Epiphanius of Salamis also mentions a winter solstice festival of Horus in his \"Panarion\". However, this festival is not attested in any native Egyptian sources.\nSuggested influence on Christianity.\nWilliam R. Cooper's 1877 book and Acharya S's self-published 2008 book, among others, have suggested that there are many similarities between the story of Horus and the much later story of Jesus. This outlook remains very controversial and is disputed.\nIn popular culture.\nDeclan Hannigan portrays Horus in the Marvel Cinematic Universe (MCU) television series \"Moon Knight\" (2022).\nIn the film series \"Night at the Museum\", a group of underworld warrior deities appear in \"\" when Kahmunrah uses the combination to open the gate to the underworld and summon an army of Horus warriors. The warriors appear from the underworld carrying spears ready to attack and join Kahmunrah's fight to take over the world.\nHorus is a Warrior class God in the multiplayer online battle arena game \"Smite\" with the title of \"The Rightful Heir\".\nIn the book trilogy \"The Kane Chronicles\" by Rick Riordan, main character Carter Kane hosts the spirit of Horus when he is released in the British Museum along with four other Egyptian deities. Horus speaks to Carter throughout the trilogy, offering him his advice and wisdom.\nIn the fantasy action film \"Gods of Egypt\" Horus is portrayed by Nikolaj Coster-Waldau. In the film, he helps out a mortal named Bek to stop his uncle Set while also trying to reclaim his throne and bring peace to Egypt.\nHorus appears in a 1980 science fiction graphic novel \"La Foire aux immortels\" written and illustrated by French cartoonist and storyteller Enki Bilal.\nHorus appears as a god that can be prayed to in 2023's \"\". Praying to Horus increases the movement speed of the player's armies and makes them immune to attrition.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49451", "revid": "51038799", "url": "https://en.wikipedia.org/wiki?curid=49451", "title": "Bayeux Tapestry", "text": "The Bayeux Tapestry is an embroidered cloth nearly long and tall that depicts the events leading up to the Norman Conquest of England in 1066, led by William, Duke of Normandy, challenging Harold II, King of England, and culminating in the Battle of Hastings. It is thought to date to the 11th century, within a few years of the battle. Now widely accepted to have been made in England, perhaps as a gift for William, it tells the story from the point of view of the conquering Normans and for centuries has been preserved in Normandy.\nAccording to Sylvette Lemagnen, conservator of the tapestry, in her 2005 book \"La Tapisserie de Bayeux\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Bayeux tapestry is one of the supreme achievements of the Norman Romanesque\u00a0... Its survival almost intact over nine centuries is little short of miraculous\u00a0... Its exceptional length, the harmony and freshness of its colours, its exquisite workmanship, and the genius of its guiding spirit combine to make it endlessly fascinating.\nThe cloth consists of 58 scenes, many with Latin \"tituli\", embroidered on linen with coloured woollen yarns. It is likely that it was commissioned by Bishop Odo of Bayeux, William's maternal half-brother, and made for him in England in the 1070s. In 1729, the hanging was rediscovered by scholars at a time when it was being displayed annually in Bayeux Cathedral. The tapestry is now exhibited at the Mus\u00e9e de la Tapisserie de Bayeux in Bayeux, Normandy, France. It will return to England for the first time in 900 years, on loan from France for display at the British Museum from September 2026 to July 2027. \nThe designs on the Bayeux Tapestry are embroidered rather than in a tapestry weave, so it does not meet narrower definitions of a tapestry. It can be seen as a rare example of secular Romanesque art. Tapestries adorned both churches and wealthy houses in medieval Western Europe, though at , the Bayeux Tapestry is exceptionally large. The background is not embroidered, providing a large, clear field of cloth which allows the figures and decorative elements to stand out very clearly.\nHistory.\nOrigins.\nThe earliest known written reference to the tapestry is a 1476 inventory of Bayeux Cathedral, but its origins have been the subject of much speculation and controversy.\nFrench legend maintained the tapestry was commissioned and created by Queen Matilda, William the Conqueror's wife, and her ladies-in-waiting. Indeed, in France, it is occasionally known as (\"The Tapestry of Queen Matilda\"). However, scholarly analysis in the 20th century concluded it was probably commissioned by William's half-brother, Bishop Odo of Bayeux, who, after the Conquest, also became Earl of Kent and, when William was absent in Normandy, regent of England.\nThe reasons for the Odo commission theory include:\nAssuming Odo commissioned the tapestry, it was probably designed and constructed in England by Anglo-Saxon artists (Odo's main power base being by then in Kent); the Latin text contains hints of Anglo-Saxon; other embroideries originate from England at this time; and the vegetable dyes can be found in cloth traditionally woven there. Howard B. Clarke has proposed that the designer of the tapestry (i.e., the individual responsible for its overall narrative and political argument) was Scolland, the abbot of St Augustine's Abbey in Canterbury, because of his previous position as head of the scriptorium at Mont-Saint-Michel (famed for its illumination), his travels to Trajan's Column in Rome, and his connections to Wadard and Vital, two individuals identified in the tapestry. Alternatively, Christine Grainge has argued that the designer may have been Lanfranc, Archbishop of Canterbury 1070\u20131089. The actual physical work of stitching was most probably undertaken by women needleworkers. Anglo-Saxon needlework of the more detailed type known as Opus Anglicanum was famous across Europe. It was perhaps commissioned for display in the hall of Odo's palace in Bayeux and then bequeathed to the cathedral he had built, following the precedent of the documented but lost hanging of the Anglo-Saxon warrior Byrhtnoth, bequeathed by his widow to Ely Abbey.\nOther theories exist. Carola Hicks has suggested the tapestry could possibly have been commissioned by Edith of Wessex, widow of Edward the Confessor and sister of Harold. Wolfgang Grape has challenged the consensus that the embroidery is Anglo-Saxon, distinguishing between Anglo-Saxon and other Northern European techniques; Medieval material authority Elizabeth Coatsworth contradicted this: \"The attempt to distinguish Anglo-Saxon from other Northern European embroideries before 1100 on the grounds of technique cannot be upheld on the basis of present knowledge.\" George Beech has suggested that the tapestry was executed at the Abbey of Saint-Florent de Saumur in the Loire Valley, the detailed depiction of the Breton campaign implying additional sources in France. Andrew Bridgeford has suggested that the tapestry was actually of English design and encoded with secret messages meant to undermine Norman rule.\nRecorded history.\nThe first reference to the tapestry is from 1476 when it was listed in an inventory of the treasures of Bayeux Cathedral. It survived the sack of Bayeux by the Huguenots in 1562; and the next certain reference is from 1724. Antoine Lancelot sent a report to the \"Acad\u00e9mie Royale des Inscriptions et Belles-Lettres\" concerning a sketch he had received about a work concerning William the Conqueror. He had no idea where or what the original was, although he suggested it could have been a tapestry. Despite further enquiries he discovered no more.\nThe Benedictine scholar Bernard de Montfaucon made more successful investigations and found that the sketch was of a small portion of a tapestry preserved at Bayeux Cathedral. In 1729 and 1730, he published drawings and a detailed description of the complete work in the first two volumes of his \"Les Monuments de la Monarchie fran\u00e7aise\". The drawings were by Antoine Beno\u00eet, one of the ablest draughtsmen of that time.\nThe tapestry was first briefly noted in English in 1746 by William Stukeley, in his \"Palaeographia Britannica\". The first detailed account in English was written by the English antiquary Smart Lethieullier, who was living in Paris in 1732\u20133, and was acquainted with Lancelot and de Montfaucon: it was not published, however, until 1767, as an appendix to Andrew Ducarel's \"Anglo-Norman Antiquities\".\nDuring the French Revolution, in 1792, the tapestry was confiscated as public property to be used for covering military wagons. It was rescued from a wagon by a local lawyer who stored it in his house until the troubles were over, whereupon he sent it to the city administrators for safekeeping. After the Reign of Terror, the Fine Arts Commission, set up to safeguard national treasures in 1803, required it to be removed to Paris for display at the Mus\u00e9e Napol\u00e9on. When Napoleon abandoned his planned invasion of Britain the tapestry's propaganda value was lost and it was returned to Bayeux where the council displayed it on a winding apparatus of two cylinders. Despite scholars' concern that the tapestry was becoming damaged the council refused to return it to the cathedral.\nIn 1816, the Society of Antiquaries of London commissioned its historical draughtsman, Charles Stothard, to visit Bayeux to make an accurate hand-coloured facsimile of the tapestry. His drawings were subsequently engraved by James Basire jr. and published by the Society in 1819\u201323. Stothard's images are still of value as a record of the tapestry as it was before 19th-century restoration.\nBy 1842, the tapestry was displayed in a special-purpose room in the Biblioth\u00e8que Publique. It required special storage in 1870, with the threatened invasion of Normandy in the Franco-Prussian War, and again in 1939\u20131944 by the Ahnenerbe during the German occupation of France and the Normandy landings. On 27 June 1944 the Gestapo took the tapestry to the Louvre, and on 18 August, three days before the Wehrmacht withdrew from Paris, Himmler sent a message (intercepted by Bletchley Park) ordering it to be taken to \"a place of safety\", thought to be Berlin. It was only on 22 August that the SS attempted to take possession of the tapestry, by which time the Louvre was again in French hands. After the liberation of Paris, on 25 August, the tapestry was again put on public display in the Louvre, and in 1945 it was returned to Bayeux, where it is exhibited at the Mus\u00e9e de la Tapisserie de Bayeux ().\nLater reputation and history.\nThe inventory listing of 1476 shows that the tapestry was being hung annually in Bayeux Cathedral for the week of the Feast of St John the Baptist; this was still the case in 1728, although by that time the purpose was merely to air the hanging, which was otherwise stored in a chest. Clearly, the work was being well cared for. In the eighteenth century, the artistry was regarded as crude or even barbarous\u2014red and yellow multi-coloured horses upset some critics. It was thought to be unfinished because the linen was not covered with embroidery. However, its exhibition in the Louvre in 1797 caused a sensation, with \"Le Moniteur\", which normally dealt with foreign affairs, reporting on it on its first two pages. It inspired a popular musical, \"La Tapisserie de la Reine Mathilde\". It was because the tapestry was regarded as an antiquity rather than a work of art that in 1804 it was returned to Bayeux, where in 1823 one commentator, A. L. L\u00e9chaud\u00e9 d'Anisy, reported that \"there is a sort of purity in its primitive forms, especially considering the state of the arts in the eleventh century\".\nThe tapestry was becoming a tourist attraction, with Robert Southey complaining of the need to queue to see the work. In the 1843 \"Hand-book for Travellers in France\" by John Murray III, a visit was included on \"Recommended Route 26 (Caen to Cherbourg via Bayeux)\", and this guidebook led John Ruskin to go there; he would describe the tapestry as \"the most interesting thing in its way conceivable\". Charles Dickens, however, was not impressed: \"It is certainly the work of amateurs; very feeble amateurs at the beginning and very heedless some of them too.\"\nDuring the Second World War Heinrich Himmler coveted the work, regarding it as \"important for our glorious and cultured Germanic history\".\nIn 2007, UNESCO admitted the tapestry to its Memory of the World International Register which lists globally important documentary heritage. In 2018 French President Emmanuel Macron announced that the Bayeux Tapestry would be loaned to Britain for public display. An initial exhibition date at the British Museum in 2022 was announced but did not materialize. During a visit to the UK in July 2025, Macron announced that the tapestry would be loaned to the British Museum from September 2026 to June 2027 in exchange for items from the Sutton Hoo site, the Lewis chessmen or the Battersea Shield being loaned to museums in Rouen and Caen. It would be the first time in 900 years that the tapestry had been in Britain. Shortly afterwards, French art historian Didier Rykner launched a petition to block the loan, claiming that transporting the tapestry could damage its fabric. As of August, over 40,000 people had signed the petition.\nOn 19 September 2025, the tapestry was removed from its museum for the first time since 1983 as part of preparations for its loan to Britain.\nConstruction, design and technique.\nIn common with other embroidered hangings of the early medieval period, this piece is conventionally referred to as a \"tapestry\", although it is not a \"true\" tapestry in which the design is woven into the cloth in tapestry weave; it is technically an embroidery, although it meets the traditional broader definition of \"tapestry\" as: \"A textile fabric decorated with designs of ornament or pictorial subjects, painted, embroidered, or woven in colours, used for wall hangings, curtains, covers for seats, ...\"\nThe Bayeux tapestry is embroidered in crewel (wool yarn) on a tabby-woven linen ground 68.38 metres long and 0.5 metres wide () and using two methods of stitching: outline or stem stitch for lettering and the outlines of figures, and couching or laid work for filling in figures. Nine linen panels, between fourteen and three metres in length, were sewn together after each was embroidered and the joins were disguised with subsequent embroidery. At the first join (start of scene 14) the borders do not line up properly but the technique was improved so that the later joins are practically invisible. The design involved a broad central zone with narrow decorative borders top and bottom. By inspecting the woollen threads behind the linen it is apparent all these aspects were embroidered together at a session and the awkward placing of the \"tituli\" is not due to them being added later. Later generations have patched the hanging in numerous places and some of the embroidery (especially in the final scene) has been reworked. The tapestry may well have maintained much of its original appearance\u2014it now compares closely with a careful drawing made in 1730.\nThe end of the tapestry has been missing from time immemorial and the final \"titulus\" \"Et fuga verterunt Angli\" (\"and the English left fleeing\") is said to be \"entirely spurious\", added shortly before 1814 at a time of anti-English sentiment. Musset speculates the hanging was originally about 1.5 metres longer. In the last section still remaining, the embroidery has been almost completely restored, but this seems to have been done with at least some regard for the original stitching. The stylised tree is quite unlike any other tree in the tapestry. The start of the tapestry has also been restored but to a much lesser extent.\nNorton has reviewed the various measurements of the length of the tapestry itself and of its nine individual linen panels. He has also attempted to estimate the size and architectural design of the 11th-century Bayeux Cathedral. He considers the tapestry would have fitted well if it had been hung along the south, west, and north arcades of the nave and that the scenes it depicts can be correlated with positions of the arcade bays in a way that would have been dramatically satisfying. He agrees with earlier speculation that a final panel is missing\u2014one that shows William's coronation and which he thinks was some three metres long. Norton concludes that the tapestry was definitely designed to be hung in Bayeux Cathedral specifically; that it was designed to appeal to a Norman audience; and that it was probably designed for Bishop Odo so as to be displayed at the dedication of the cathedral in 1077 in the presence of William, Matilda, their sons, and Odo.\nThe main yarn colours are terracotta or russet, blue-green, dull gold, olive green, and blue, with small amounts of dark blue or black and sage green. Later repairs are worked in light yellow, orange, and light greens. Laid yarns are couched in place with yarn of the same or contrasting colour.\nThe tapestry's central zone contains most of the action, which sometimes overflows into the borders either for dramatic effect or because depictions would otherwise be very cramped (for example at Edward's death scene). Events take place in a long series of scenes which are generally separated by highly stylised trees. However, the trees are not placed consistently and the greatest scene shift, between Harold's audience with Edward after his return to England and Edward's burial scene, is not marked in any way at all.\nThe \"tituli\" are normally in the central zone but occasionally use the top border. The borders are otherwise mostly purely decorative and only sometimes does the decoration complement the action in the central zone. The decoration consists of birds, beasts, fish and scenes from fables, agriculture, and hunting. There are frequent oblique bands separating the vignettes. There are nude figures, some of corpses from battle, others of a ribald nature. A harrow, a newly invented implement, is depicted (scene 10) and this is the earliest known depiction. The picture of Halley's Comet, which appears in the upper border (scene 32), is the first known picture of this comet.\nIn 1724, a linen backing cloth was sewn on comparatively crudely and, in around the year 1800, large ink numerals were written on the backing which broadly enumerate each scene and which are still commonly used for reference.\nBackground.\nBackground of the events depicted.\nIn a series of pictures supported by a written commentary, the tapestry tells the story of the events of 1064\u20131066 culminating in the Battle of Hastings. The two main protagonists are Harold Godwinson, recently crowned King of England, leading the Anglo-Saxon English, and William, Duke of Normandy, leading a mainly Norman army, sometimes called the companions of William the Conqueror.\nWilliam was the illegitimate son of Robert the Magnificent, Duke of Normandy, and Herleva (or Arlette), a tanner's daughter. William became Duke of Normandy at the age of seven and was in control of Normandy by the age of nineteen. His half-brother was Bishop Odo of Bayeux.\nKing Edward the Confessor, king of England and about sixty years old at the time the tapestry starts its narration, had no children or any clear successor. Edward's mother, Emma of Normandy, was William's great aunt. At that time succession to the English throne was not by primogeniture but was decided jointly by the king and by an assembly of nobility, the Witenagemot.\nHarold Godwinson, Earl of Wessex and the most powerful noble in England, was Edward's brother-in-law. The Norman chronicler William of Poitiers reported that Edward had previously determined that William would succeed him on the throne, and Harold had sworn to honour this, and yet later that Harold had claimed Edward, on his deathbed, had made him heir over William. However, other sources, such as Eadmer dispute this claim.\nArtistic context.\nTapestry fragments have been found in Scandinavia dating from the ninth century and it is thought that Norman and Anglo-Saxon embroidery developed from this sort of work. Examples are to be found in the grave goods of the Oseberg ship and the \u00d6verhogdal tapestries.\nA monastic text from Ely, the \"Liber Eliensis\", mentions a woven narrative wall-hanging commemorating the deeds of Byrhtnoth, killed in 991. Wall-hangings were common by the tenth century, with English and Norman texts particularly commending the skill of Anglo-Saxon seamstresses. Mural paintings imitating draperies still exist in France and Italy, and there are twelfth-century mentions of other wall-hangings in Normandy and France. A poem by Baldric of Dol might even describe the Bayeux Tapestry itself. The Bayeux Tapestry was therefore not unique at the time it was created; rather it is remarkable for being the sole surviving example of medieval narrative needlework.\nVery few hangings from the 11th century survive, but the Tapestry of Creation, or Girona Tapestry, is a large Romanesque panel of needlework, in the Museum of Girona Cathedral, Catalonia, Spain. The hanging depicts a series of figures from the Book of Genesis and personifications of the months. The Cloth of Saint Gereon, in Germany, is the largest of a group of fragments from hangings based on decorative Byzantine silks, including animals, that are probably the earliest European survivals.\nContent.\nEvents depicted.\nThe tapestry begins with a panel of Edward the Confessor sending Harold to Normandy.(scene 1) Later Norman sources say that the mission was for Harold to pledge loyalty to William but the tapestry does not suggest any specific purpose. By mischance, Harold arrives at the wrong location in France and is taken prisoner by Guy, Count of Ponthieu.(scene 7) After exchanges of messages borne by mounted messengers, Harold is released to William, who then invites Harold to accompany him on a campaign against Conan II, Duke of Brittany. On the way, just outside the monastery of Mont Saint-Michel, the army becomes mired in quicksand and Harold saves two Norman soldiers.(scene 17) William's army chases Conan from Dol de Bretagne to Rennes, and Conan finally surrenders at Dinan.(scene 20) William gives Harold arms and armour (possibly knighting him) and Harold takes an oath on saintly relics.(scene 23) Although the writing on the tapestry explicitly states an oath is taken there is no clue as to what is being promised.\nHarold leaves for home and meets again with the old king Edward, who appears to be remonstrating with him.(scene 25) Harold is in a somewhat submissive posture and seems to be in disgrace. However, possibly deliberately, the king's intentions are not made clear. The scene then shifts by about one year to when Edward has become mortally ill and the tapestry strongly suggests that, on his deathbed, he bequeaths the crown to Harold. What is probably the coronation ceremony is attended by Stigand, whose position as Archbishop of Canterbury was controversial.(scene 31) Stigand is performing a liturgical function, possibly not the crowning itself. The tapestry labels the celebrant as \"Stigant Archieps\" (Stigand the archbishop) although by that time he had been excommunicated by the papacy who considered his appointment unlawful.\nA star with a streaming tail, now known to be Halley's Comet, then appears. At this point, the lower border of the tapestry shows a fleet of ghost-like ships thus hinting at a future invasion.(scene 33) The news of Harold's coronation is taken to Normandy, whereupon we are told that William is ordering a fleet of ships to be built although it is Bishop Odo shown issuing the instructions.(scene 35) The invaders reach England, and land unopposed. William orders his men to find food, and a meal is cooked.(scene 43) A house is burnt by two soldiers, which may indicate some ravaging of the local countryside on the part of the invaders, and underneath, on a smaller scale than the arsonists, a woman holds her boy's hand as she asks for humanity.(scene 47) News is brought to William. The Normans build a motte and bailey at Hastings to defend their position. Messengers are sent between the two armies, and William makes a speech to prepare his army for battle.(scene 51)\nThe Battle of Hastings was fought on 14 October 1066 less than three weeks after the Battle of Stamford Bridge but the tapestry does not provide this context. The English fight on foot behind a shield wall, whilst the Normans are on horses. Two fallen knights are named as Leofwine and Gyrth, Harold's brothers, but both armies are shown fighting bravely. Bishop Odo brandishes his baton or mace and rallies the Norman troops in battle.(scene 54) To reassure his knights that he is still alive and well, William raises his helmet to show his face. The battle becomes very bloody with troops being slaughtered and dismembered corpses littering the ground. King Harold is killed.(scene 57) This scene can be interpreted in different ways, as the name \"Harold\" appears above a number of knights, making it difficult to identify which character is Harold, since one character appears with an arrow shot in his head under the name \"Harold\" while another character is slain by a sword underneath the words \"was slain\" . The final remaining scene shows unarmoured English troops fleeing the battlefield. The last part of the tapestry is missing; however, it is thought that the story contained only one additional scene.\nPeople depicted.\nThe following is a list of known persons depicted on the Bayeux Tapestry:\n&lt;templatestyles src=\"Col-float/styles.css\" /&gt;\n&lt;templatestyles src=\"Col-float/styles.css\" /&gt;\nLatin text.\n\"Tituli\" are included in many scenes to point out names of people and places or to explain briefly the event being depicted. The text is in Latin but at times the style of words and spelling shows an English influence. A dark blue wool, almost black, is mostly used but towards the end of the tapestry other colours are used, sometimes for each word and other times for each letter. The complete text and English translation are displayed beside images of each scene at Bayeux Tapestry tituli.\nUnsettled questions.\nThe depiction of events on the tapestry has raised several questions which remain unsettled.\nThe identification of Harold II of England in the vignette depicting his death is disputed. Some recent historians disagree with the traditional view that Harold is the figure struck in the eye with an arrow, and that the arrow is a later 18th/19th century modification following a period of repair. Beno\u00eet's engraving of 1729, and Bernard de Montfaucon's engravings of the tapestry as it was in 1730, show a spear or lance in place of the arrow and no arrow fletchings. Further, needle holes in the linen suggest that something has been removed, or shortened, and fletchings added to form an arrow. A figure is slain with a sword in the subsequent plate, and the phrase above the figure refers to Harold's death (\"interfectus est\", \"he was killed\"). \nThis would appear to be more consistent with the labelling used elsewhere in the work. It was common medieval iconography that a perjurer was to die with a weapon through the eye. Therefore, the tapestry might be said to emphasise William's rightful claim to the throne by depicting Harold as an oath breaker. Whether he actually died in this way remains a mystery and is much debated.\nThere is a panel with what appears to be a clergyman touching or possibly striking a woman's face. No one knows the significance of this scene or the caption above it: \"ubi unus clericus et \u00c6lfgyva\" (\"where [or \"in which\"] a certain cleric and \u00c6lfgyva\"), where \u00c6lfgyva is the Latinised spelling of \u00c6lfgifu, a popular Anglo-Saxon woman's name (literally \"elf-gift\"). The use of the grapheme \u00c6 shows familiarity with English spelling. There are two naked male figures in the border below this figure; the one directly below the figure is in a pose mirroring that of the cleric, squatting and displaying his genitalia (a scene that was frequently censored in historical reproductions). However, similar naked figures appear elsewhere in the lower border where there seems to be no connection at all with the main action. \nHarold had a younger sister named \u00c6lfgifu (her name is spelt Alveva in the Domesday Book of 1086) who was possibly promised to William by Harold or even betrothed to him, but she died c.\u20091066, prior to the invasion. \u00c6lfgifu was also the name of the mother of Sweyn Knutsson and Harold Harefoot, past kings of Denmark and England respectively, via Cnut the Great. It has been speculated that this scene, occurring after the meeting of Harold and William, is to remind the contemporary viewers of a scandal that occurred between \u00c6lfgifu of Northampton and Emma of Normandy, Cnut's wives, that eventually led to the crowning of Edward the Confessor, child of Emma and her first husband, \u00c6thelred the Unready.\nAt least two panels of the tapestry are missing, perhaps even another in total. This missing area may have depicted William's coronation as King of England. A poem by Baldric of Dol describes a tapestry on the walls of the personal apartments of Adela of Normandy, which is very similar to the Bayeux depiction. He describes the closing scene as the coronation of William in London.\nHistorical accuracy.\nThe Bayeux Tapestry was probably commissioned by the House of Normandy and essentially depicts a Norman viewpoint. However, Harold is shown as brave, and his soldiers are not belittled. Throughout, William is described as \"dux\" (\"duke\"), whereas Harold, also called \"dux\" up to his coronation, is subsequently called \"rex\" (\"king\"). The fact that the narrative extensively covers Harold's activities in Normandy (in 1064) indicates that the intention was to show a strong relationship between that expedition and the Norman Conquest starting two years later. It is for this reason that the tapestry is generally seen by modern scholars as an apologia for the Norman Conquest.\nThe tapestry's narration seems to place stress on Harold's oath to William, although its rationale is not made clear. Norman sources claim that the English succession was being pledged to William, but English sources give varied accounts. Today it is thought that the Norman sources are to be preferred. Both the tapestry and Norman sources name Stigand, the excommunicated archbishop of Canterbury, as the man who crowned Harold, possibly to discredit Harold's kingship; one English source suggests that he was crowned by Ealdred, archbishop of York, and favoured by the papacy, making Harold's position as legitimate king more secure. Contemporary scholarship has not decided the matter, although it is generally thought that Ealdred performed the coronation.\nAlthough political propaganda or personal emphasis may have somewhat distorted the historical accuracy of the story, the Bayeux Tapestry constitutes a visual record of medieval arms, apparel, and other objects unlike any other artifact surviving from this period. There is no attempt at continuity between scenes, either in individuals' appearance or clothing. The knights carry shields, but show no system of hereditary coats of arms\u2014the beginnings of modern heraldic structure were in place, but would not become standard until the middle of the 12th century. It has been noted that the warriors are depicted fighting with bare hands, while other sources indicate the general use of gloves in battle and hunt.\nThe American historian Stephen D. White, in a study of the tapestry, has \"cautioned against reading it as an English or Norman story, showing how the animal fables visible in the borders may instead offer a commentary on the dangers of conflict and the futility of pursuing power\".\nReplicas and continuations.\nA number of replicas of the Bayeux Tapestry have been created, in various media.\nOther modern artists have attempted to complete the work by creating panels depicting subsequent events up to William's coronation, though the actual content of the missing panels is unknown. In 1997, the embroidery artist Jan Messent completed a reconstruction showing William accepting the surrender of English nobles at Berkhamsted (\"Beorcham\"), Hertfordshire, and his coronation. In early 2013, 416 residents of Alderney in the Channel Islands finished a continuation including William's coronation and the building of the Tower of London.\nIn popular culture.\nBecause it resembles a modern comic strip or movie storyboard, is widely recognised, and is so distinctive in its artistic style, the Bayeux Tapestry has frequently been used or reimagined in a variety of different popular culture contexts. George Wingfield Digby wrote in 1957:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It was designed to tell a story to a largely illiterate public; it is like a strip cartoon, racy, emphatic, colourful, with a good deal of blood and thunder and some ribaldry.\nIt has been cited by Scott McCloud in \"Understanding Comics\" as an example of early sequential-narrative art; and Bryan Talbot, a British comic book artist, has called it \"the first known British comic strip\".\nIt has inspired many modern political and other cartoons, including:\nThe tapestry has inspired modern embroideries, most notably and directly:\nOther embroideries more loosely inspired by it include the Hastings Embroidery (1966), the New World Tapestry (1980\u20132000), the Quaker Tapestry (1981\u201389), the Great Tapestry of Scotland (2013), the Scottish Diaspora Tapestry (2014\u201315), \"Magna Carta (An Embroidery)\" (2014\u201315), and (in this case a woven tapestry with embroidered details) the \"Game of Thrones Tapestry\" (2017\u201319).\nJapanese anime director Hayao Miyazaki was inspired by the tapestry during the creation of his manga and film \"Nausica\u00e4 of the Valley of the Wind\" (1984). The legendary tapestry tells the story of a chosen one who will save the earth.\nA number of films have used sections of the tapestry in their opening credits or closing titles, including Disney's \"Bedknobs and Broomsticks\", Anthony Mann's \"El Cid\", Franco Zeffirelli's \"Hamlet\", Frank Cassenti's \"La Chanson de Roland\", Kevin Reynolds' \"\", and Richard Fleischer's \"The Vikings\".\nThe tapestry is referred to in Tony Kushner's play \"Angels in America\". The apocryphal account of Queen Matilda's creation of the tapestry is used, perhaps in order to demonstrate that Louis, one of the main characters, holds himself to mythological standards.\nThe couch gag for the 2008 \"The Simpsons\" episode \"E Pluribus Wiggum\" retells the eponymous family's struggle with their neighbour in the style of the tapestry.\nIn 2022 the French documentary \"Mysteries of the Bayeux Tapestry\" was broadcast by BBC Four. It was written by Jonas Rosales, directed by Alexis de Favitski and produced by Antoine Bamas. The documentary covered investigations carried out on the tapestry by the Laboratoire d'Arch\u00e9ologie Mol\u00e9culaire et Structurale (LAMS) at the French National Centre for Scientific Research, which used a hyperspectral camera, measuring 215 different colours, to analyse the pigments which produced the original colours for the dyes, extracted from madder, weld and indigo.\nIn 2025, Bethesda Softworks commissioned a remake of the Bayeux Tapestry as part of a promotional campaign for their video game \"\". The tapestry, dubbed the Slayeux Tapestry, was displayed in the Royal Armouries Museum in Leeds.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nEmbroidery depicting the 1066 Norman invasion of England"}
{"id": "49453", "revid": "20813426", "url": "https://en.wikipedia.org/wiki?curid=49453", "title": "Seth", "text": "Third son of Adam and Eve\nIn the Abrahamic religions, Seth was the third son of Adam and Eve. The Hebrew Bible names two of his siblings (although it also states that he had others): his brothers Cain and Abel. According to , Seth was born after Abel's murder by Cain, and Eve believed that God had appointed him as a replacement for Abel.\nGenesis.\nAccording to the Book of Genesis, Seth was born when Adam was 130 years old (according to the Masoretic Text), or 230 years old (according to the Septuagint), \"a son in his likeness and image\". The genealogy repeated at . states that Adam fathered \"sons and daughters\" before his death, aged 930 years. According to Genesis, Seth died at the age of 912 (that is, 14 years before Noah's birth).\nJewish tradition.\nSeth figures in the biblical texts of the \"Life of Adam and Eve\" (the \"Apocalypse of Moses\"). It recounts the lives of Adam and Eve from after their expulsion from the Garden of Eden to their deaths. While the surviving versions were composed from the early third to the fifth century, the literary units in the work are considered to be older and predominantly of Jewish origin. There is wide agreement that the original was composed in a Semitic language in the first century AD/CE. In the Greek versions, Seth and Eve travelled to the doors of the Garden to beg for some oil of the Tree of Mercy (i.e. the Tree of Life). On the way, Seth was attacked and bitten by a wild beast, which went away when ordered by Seth. Michael refused to give them the oil at that time, but promised to give it at the end of time, when all flesh will be raised up, the delights of paradise will be given to the holy people and God will be in their midst. On their return, Adam said to Eve: \"What hast thou done? Thou hast brought upon us great wrath which is death.\" (chapters 5\u201314) Later, only Seth could witness the taking-up of Adam at his funeral in a divine chariot, which deposited him in the Garden of Eden.\nGenesis refers to Seth as the ancestor of Noah and hence the father of all mankind, all other humans having perished in the Great Flood. It is said that late in life, Adam gave Seth secret teachings that would become the Kabbalah. The Zohar refers to Seth as \"ancestor of all the generations of the Egyptians or Tsetsaudim\" (Hebrew: righteous ones). According to Seder Olam Rabbah, based on Jewish reckoning, he was born in 2130 BC AM. According to Aggadah, he had 2 sons and many wives. According to the Seder Olam Rabbah, he died in 1042 AM.\nJosephus.\nIn the \"Antiquities of the Jews\", Josephus refers to Seth as virtuous and of excellent character, and reports that his descendants invented the wisdom of the heavenly bodies, and built the \"pillars of the sons of Seth\", two pillars inscribed with many scientific discoveries and inventions, notably in astronomy. They were built by Seth's descendants based on Adam's prediction that the world would be destroyed at one time by fire and another time by global flood, in order to protect the discoveries and be remembered after the destruction. One was composed of brick, and the other of stone, so that if the pillar of brick should be destroyed, the pillar of stone would remain, both reporting the ancient discoveries, and informing humankind that a pillar of brick was also erected. Josephus reports that the pillar of stone remained in the land of Siriad in his day.\nWilliam Whiston, a 17/18th-century translator of the \"Antiquities\", stated in a footnote that he believed Josephus mistook Seth for Sesostris, king of Egypt, the erector of the pillar in Siriad (being a contemporary name for the territories in which Sirius was venerated, i.e. Egypt). He stated that there was no way for any pillars of Seth to survive the deluge, because the deluge buried all such pillars and edifices far underground in the sediment of its waters. The perennialist writer Nigel Jackson identifies the land of Siriad in Josephus' account with Syria, citing related Mandaean legends regarding the \"Oriental Land of Shyr\" in connection with the visionary mytho-geography of the prophetic traditions surrounding Seth.\nChristianity.\nThe second-century BC Book of Jubilees, regarded as noncanonical except in the Oriental Orthodox Churches, also dates his birth to 130 after creation (AM). According to it, in 231 AM Seth married his sister, Azura, who was four years younger than he was. In the year 235 AM, Azura gave birth to Enos.\nSeth is commemorated as one of the Holy Forefathers in the Calendar of Saints of the Armenian Apostolic Church, along with Adam, Abel, and others, with a feast day on July 26. He is also included in the Genealogy of Jesus, according to Luke 3:23\u201338.\nThe Sethians were a Christian Gnostic sect who may date their existence to before Christianity. Their thinking, although predominantly Judaic in foundation, was arguably strongly influenced by Platonism. Sethians were named for their veneration of Seth, depicted in their creation myths as a divine incarnation; consequently, the offspring or 'posterity' of Seth are held to comprise a superior elect within human society.\nIslam.\nThe Quran makes no mention of \u0160\u012b\u1e6f ibn \u0100dam. He is respected within Islamic traditions as the third and righteous son of Adam and Eve and seen as the gift bestowed on Adam after the death of Abel. The Sunni scholar and historian ibn Kathir in his \"tarikh\" (book of history), \"Al-Bid\u0101ya wa-n-nih\u0101ya\" (), records that Seth, a prophet like his father Adam, transfers God's Law to mankind after the death of Adam, and places him among the exalted antediluvian patriarchs of the Generations of Adam. Some sources say that Seth was the receiver of scriptures. These scriptures are said to be the \"first scriptures\" mentioned in the Quran 87:18. Medieval historian and exegete al-Tabari and other scholars say that Seth buried Adam and the secret texts in the tomb of Adam, i.e., the \"Cave of Treasures\".\nIslamic literature holds that Seth was born when Adam was past 100 and that Adam appointed Seth as guide to his people. The 11th-century Syrian historian and translator Al-Mubashshir ibn F\u0101tik recorded the maxims and aphorisms of the ancient philosophers in his book \"Kit\u0101b mukht\u0101r al-\u1e25ikam wa-ma\u1e25\u0101sin al-kalim\" and included a chapter on Seth. Within Islamic tradition Seth holds wisdom of several kinds; knowledge of time, prophecy of the future Great Flood, and inspiration on the methods of night prayer. Islam, Judaism and Christianity trace the genealogy of mankind back to Seth since Abel left no heirs and Cain's heirs, according to tradition, were destroyed by the Great Flood. Many traditional Islamic crafts are traced back to Seth, such as the making of horn combs. Seth also plays a role in Sufism, and Ibn Arabi includes a chapter in his \"Bezels of Wisdom\" on Seth, entitled \"The Wisdom of Expiration in the Word of Seth\".\nSome traditions locate Seth's tomb in the village of Al-Nabi Shayth (lit. \"The Prophet Seth\") in the mountains above the Beqaa Valley in Lebanon, where there is a mosque named after him. This tomb was described by the 12th-century geographer Ibn Jubayr. A rival tradition, mentioned by later medieval Arab geographers from the 13th century on, placed the tomb of \"Nabi Shith\" (\"Prophet Seth\") in the Palestinian village of Bashshit, southwest of Ramla village. According to the Palestine Exploration Fund, Bashshit means \"Beit Shith\", i.e. \"House of Seth\". The village was depopulated with the establishment of the State of Israel in 1948 , but the three-domed structure said to be Seth's tomb survives in the Israeli moshav Aseret built on the site. Another tomb in the city of Balkh, Afghanistan has been identified as the burial site of Seth.\nLocal Muslims in Ayodhya, Uttar Pradesh in India believe a grave in Hazrat Shees Jinnati Mosque to be the maqam of Hazrat Shees or the Prophet Seth. This belief is mentioned in a 16th-century Mughal document \"Ain-i-Akbari\" and is also mentioned in the work \"India of Aurangzeb\" of Jadunath Sarkar.\nMandaeism.\nAccording to the Mandaean scriptures, including the Qulasta, the Mandaean Book of John and Genz\u0101 Rabb\u0101, Seth is cognate with the angelic soteriological figure Shitil (), a son of Adam Kadmaya who taught John the Baptist with his brothers Anush (Enosh) and Hibil (Abel). He is variously spoken of as a son of Adam, a brother or son of Hibil, and the brother or father of Anush. Shitil is one of the revealers of Mandaeism and a prophet, identified as the biblical Seth.\nYazidism.\nIn Yazidism, Seth is known as Shehid ibn Jerr.\nAccording to Yazidi oral literature, Adam and Eve each deposited their seeds into separate jars. While Eve's seed developed into insects, Adam's seed gave birth to Shehid ibn Jerr, the ancestor of the Yazidis. Yazidis thus believe that they have been created separately and differently from all other human beings (Kreyenbroek 2005 : 31).\nShrines.\nIraq.\nOn July 26, 2014 , forces of the Islamic State of Iraq and the Levant (ISIL) blew up Nabi Shiyt (Prophet Seth) shrine in Mosul, Iraq. Sami al-Massoudi, the deputy head of the Shiite Endowment Office overseeing holy sites, confirmed that destruction. He added, ISIL took some of the artifacts to an unknown location.\nLebanon.\nThere is a village named after him in Lebanon, that is \"Al-Nabi Shayth\" or \"Al-Nabi Sheeth\" (meaning \"The Prophet Seth\"), which is also considered to contain his shrine.\nIsrael.\nThe tomb of Bashshit is believed to be the grave of Seth. The tomb now sits in Aseret.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49457", "revid": "17859592", "url": "https://en.wikipedia.org/wiki?curid=49457", "title": "Gateway Arch National Park", "text": "U.S. national park in St. Louis, Missouri\nGateway Arch National Park is a national park of the United States located in St. Louis, Missouri, near the starting point of the Lewis and Clark Expedition.\nIn its initial form as a national memorial, it was established in 1935 to commemorate:\nThe national park consists of the Gateway Arch, a steel catenary arch that has become the definitive icon of St. Louis; a park along the Mississippi River on the site of the earliest buildings of the city; the Old Courthouse, a former state and federal courthouse where the \"Dred Scott\" case originated; and the museum at the Gateway Arch. It is the smallest national park in the United States at , less than 2% the size of the next-smallest, Hot Springs National Park.\nThe immediate surroundings of the Gateway Arch were initially designated the Jefferson National Expansion Memorial by secretarial order on December 21, 1935. The Gateway Arch was completed on October 28, 1965. The park is maintained by the National Park Service (NPS).\nThe area surrounding the arch was redesignated as the Gateway Arch National Park in 2018. This change has been controversial due to the nature of the site (national parks typically include conserved natural landscapes and significant opportunities for nature recreation, whereas sites that have primarily historical and architectural significance are usually given other NPS designations). Several publications noted that the addition of Gateway Arch as a national park eroded the significance associated with the \"national park\" designation, and some suggested that the change was made in order to promote tourism rather than to conserve a nature area.\nComponents.\nThe Gateway Arch.\nThe Gateway Arch, known as the \"Gateway to the West,\" is the tallest monument in the United States. It was designed by the Finnish-American architect Eero Saarinen and the German-American structural engineer Hannskarl Bandel in 1947 and built between 1963 and October 1965. It stands tall and wide at its base. The legs are wide at the base, narrowing to at the arch. There is a unique tram system to carry passengers to the observation room at the top of the arch.\nOld Courthouse.\nThe Old Courthouse is built on land originally deeded by St. Louis founder Auguste Chouteau. It marks the location over which the arch reaches. Its dome was built during the American Civil War and is similar to the dome on the United States Capitol which was also built during the Civil War. It was the site of the local trials in the Dred Scott case.\nThe courthouse is the only portion of the memorial west of Interstate 44. To the west of the Old Courthouse is the Gateway Mall between Market and Chestnut Streets which is only interrupted by the Civil Courts Building which features a pyramid model of the Mausoleum of Mausolus (which was one of the Seven Wonders of the Ancient World) on its roof. When the Civil Courts building was built in the 1920s, the Chouteau family sued to regain the property belonging to the Old Courthouse because it had been deeded in perpetuity to be a courthouse.\nMuseum at the Gateway Arch.\nUnderneath the arch is a visitor center, entered from a circular entryway facing the Old Courthouse. Within the center, a project to rebuild the Museum at the Gateway Arch was completed in July 2018. The new museum features exhibits on a variety of topics including westward expansion and the construction of the Arch, all told through a St. Louis lens. Tucker Theater, finished in 1968 and renovated 30 years later, has about 285 seats and shows a documentary (\"Monument to the Dream\") on the arch's construction. A second theater was added in 1993 but removed in 2018 as part of the CityArchRiver renovation project. Also located in the visitor center are a gift shop and cafe.\nHistory.\n1930s.\nThe memorial was developed largely through the efforts of St. Louis civic booster Luther Ely Smith who first pitched the idea in 1933, was the long-term chairman of the committee that selected the area and persuaded Franklin Roosevelt in 1935 to make it a National Park Service unit after St. Louis passed a bond issue to begin building it and who partially financed the 1947 architectural contest that selected the arch.\nIn the early 1930s the United States began looking for a suitable memorial for Thomas Jefferson (the Washington Monument and the newly built Lincoln Memorial were the only large Presidential memorials at the time).\nShortly after Thanksgiving in 1933 Smith who had been on the commission to build the George Rogers Clark National Historical Park in Indiana, was returning via train when he noticed the poor condition of the original platted location of St. Louis along the Mississippi. He thought that the memorial to Jefferson should be on the actual location that was symbolic of one of Jefferson's greatest triumphs\u2014the Louisiana Purchase.\nThe originally platted area of St. Louis was the site of:\nAlmost all of the historic buildings associated with this period had been replaced by newer buildings. His idea was to raze all of the buildings in the original St. Louis platted area and replace it with a park with \"a central feature, a shaft, a building, an arch, or something which would symbolize American culture and civilization.\"\nSmith pitched the idea to Bernard Dickmann who quickly assembled a meeting of St. Louis civic leaders on December 15, 1933, at the Jefferson Hotel and they endorsed the plan and Smith became chairman of what would become the Jefferson National Expansion Memorial Association (a position he would hold until 1949 with a one-year exception).\nThe Commission then defined the area, got cost estimates of $30 million to buy the land, clear the buildings and erect a park and monument. With promises from the federal government (via the United States Territorial Expansion Memorial Commission) to join if the City of St. Louis could raise money.\nThe area to be included in the park was bounded by the Eads Bridge/Washington Avenue on the north and Poplar Street on the south, the Mississippi River on the east and Third Street (now Interstate 44) on the west. The Old Courthouse, just west of Third Street, was added in 1940.\nThe only building in this area not included was the Old Cathedral, which is on the site of St. Louis first church and was opposite the home of St. Louis founder Auguste Chouteau. The founders of the city were buried in its graveyard (but were moved in 1849 to Bellefontaine Cemetery during a cholera outbreak).\nTaking away 40 blocks in the center of St. Louis was bitterly fought by some sources\u2014particularly the \"St. Louis Post-Dispatch\". On September 10, 1935, the voters of St. Louis supposedly approved a $7.5 million bond issue to buy the property. Local architect Louis LaBeaume provided a preliminary design proposal for the site that included multiple museums, fountains and obelisks. Soon after, it was revealed that the election was rigged, and the true number of voters in favor of the demolition of the riverfront is unknown.\nThe buildings were bought for $7 million by the federal government via Eminent domain and was subject to considerable litigation but were ultimately bought at 131.99 percent of assessed valuation. Roosevelt inspected the memorial area on October 14, 1936, during the dedication of the St. Louis Soldiers Memorial. Included in the party was then Senator Harry S. Truman.\n1940s to 1960s.\nThe land was to be cleared by 1942. Among the buildings razed was the \"Old Rock House\" 1818 home of fur trader Manuel Lisa (now occupied by the stairs on the north side of the arch) and the 1819 home of original St. Louis pioneer Jean Pierre Chouteau at First (Main) and Washington streets.\nThe architectural competition for a monument was delayed by World War II. Interest in the monument was fed after the war as it was to be the first big monument in the post-World War II era. The estimated cost of the competition was $225,000 and Smith personally donated $40,000. Civic leaders held the nationwide competition in 1947 to select a design for the main portion of the Memorial space.\nArchitect Eero Saarinen won this competition with plans for a catenary arch to be placed on the banks of the Mississippi River. However, these plans were modified over the next 15 years, placing the arch on higher ground and adding in height and width.\nThe central architectural feature at the base of the arch is the Old Courthouse, which was once the tallest building in Missouri and has a dome similar to the United States Capitol and was placed on the building during the American Civil War at the same time as that on the U.S. Capitol.\nSaarinen developed the shape with the help of architectural engineer Hannskarl Bandel. It is not a pure inverted catenary. Saarinen preferred a shape that was slightly elongated and thinner towards the top, a shape that produces a subtle soaring effect, and transfers more of the structure's weight downward rather than outward at the base.\nWhen Saarinen won the competition, the official notification was sent to \"E. Saarinen\", thinking it to be the architect's father Eliel Saarinen, who had also submitted an entry. The family celebrated with a bottle of champagne, and two hours later an embarrassed official called to say the winner was, in fact, the younger Saarinen. The elder Saarinen then broke out a second bottle of champagne to celebrate his son's success.\nAmong the five finalists was local St. Louis architect Harris Armstrong.\nLand for the memorial was formally dedicated on June 10, 1950, by Harry S. Truman. However, the Korean War began and the project was put on hold.\nOn June 23, 1959, work began on covering railroad tracks that cut across the memorial grounds.\nOn February 11, 1961, excavation began, and that September 1, Saarinen died. On February 12, 1963, the first stainless steel triangle that formed the first section of the arch was set in place on the south leg. On October 28, 1965, it was completed, costing approximately $15 million to build. The adjacent park was designed by landscape architect Dan Kiley. Along with all other historical areas of the National Park Service, the memorial was listed on the National Register of Historic Places on October 15, 1966. Vice President Hubert Humphrey and Secretary of the Interior Stewart Udall dedicated the arch on May 25, 1968.\n1970s to 2000s.\nIn 1984, Congress authorized the enlargement of the Memorial to include up to on the east bank of the Mississippi River in East St. Louis, Illinois. Funds were authorized to begin land acquisition, but Congress placed a moratorium upon NPS land acquisitions in fiscal year 1998. The moratorium continued into the 21st century, with expansion becoming less likely because of the construction of a riverboat gambling facility and related amenities.\nDuring the Great Flood of 1993, Mississippi flood waters reached halfway up the Grand Staircase on the east.\nIn 1999, the arch tram queue areas were renovated at a cost of about $2.2 million. As well, the Ulysses S. Grant National Historic Site in St. Louis County, Missouri, was put under the jurisdiction of the Superintendent of the Memorial.\nThe arch was featured on the Missouri state quarter in 2003.\nIn 2007 St. Louis Mayor Francis Slay and former Missouri Senator John Danforth asked the National Park Service to create a more \"active\" use of the grounds of the memorial and model it on Millennium Park in Chicago including the possibility of restaurants, fountains, ice skating, swimming, and other activities. The National Park Service was not in favor of the plan noting that the only other overt development pressure on national park property has been at the Jackson Hole Airport in Grand Teton National Park\n2010s to present.\nRenovations.\nOn December 8, 2009, sponsored by nonprofit CityArchRiver2015, the international design competition \"Framing a Modern Masterpiece: The City + The Arch + The River 2015\" commenced. It aimed to \"design a plan to improve the riverfront park landscape, ease access for pedestrians across Memorial Drive and expand onto the East St. Louis riverfront,\" as well as to attract visitors. The contest consisted of three stages\u2014portfolio assessment (narrowed down to 8\u201310 teams), team interviews (narrowed down to 4\u20135 teams), and review of design proposals. The competition received 49 applicants, which were narrowed down to five in the first two stages. On August 17, 2010, the designs of the five finalists were revealed to the public and exhibited at the theater below the arch. On August 26, the finalists made their cases to an eight-member jury, and on September 21, Michael Van Valkenburgh Associates was selected as the winner. The initiative's plans included updating Kiener Plaza and the Old Courthouse, connecting the city to the Arch grounds with a park over Interstate 70, a re-imagined museum, and improved accessibility.\nFor most of its existence, the Memorial was largely separated from the rest of Downtown St. Louis by a sunken section of I-70 (later redesignated I-44). Ground broke on the \"Park over the Highway\" project, the first component of the CityArchRiver project, on August 2, 2013. This project features a landscaped structure over Interstate 70 and rerouted surface traffic that had previously formed a moat separating the Gateway Arch from the Old Courthouse. This project was completed in December 2014.\nIn November 2015, Saarinen's original master plan was brought to fruition. Building of the Gateway Arch Connector linking the Old Courthouse with the grounds of the arch was completed. The project, originally planned for completion in 2015 to coincide with the 50th anniversary of the opening of the arch, was completed in 2018. It includes:\nIn 2016, many ash trees on the grounds were removed to preempt damage from emerald ash borers. Prior to the work of CityArchRiver, there were 1,800 trees on the grounds. There are now 4,200.\nThe $380 million project was funded both privately and publicly. The public funding, provided largely by Proposition P, totaled $159 million. The remaining $221 million were secured via fundraising efforts of Gateway Arch Park foundation.\nRedesignation.\nOn June 26, 2017, Senator Roy Blunt (R-Missouri) introduced the Gateway Arch National Park Designation Act (Pub. L.\u00a0) to redesignate Jefferson National Expansion Memorial as Gateway Arch National Park. The legislation was cosponsored by Senator Claire McCaskill (D-Missouri).\nIn July 2017, Acting Deputy Director of the National Park Service Robert Vogel testified before the Senate Energy and Natural Resources Subcommittee on National Parks. Vogel stated that the Department of the Interior supported changing the name to Gateway Arch National Monument, rather than National Park, to maintain consistency with existing naming conventions. He likened the site to the Statue of Liberty National Monument, noting that existing national parks encompassed thousands of acres at minimum and that the memorial \"is too small and limited in the range of resources the site protects and interprets to be called a national park\".\nThe United States Congress approved the bill in early 2018. U.S. President Donald Trump signed the act into law on February 22, 2018, officially renaming the site Gateway Arch National Park.\nThe new designation has been seen as an attempt to increase tourism in St. Louis. The name has been criticized as inappropriate by some visitors.\nBrickline Greenway.\nThe Brickline Greenway, formerly the Chouteau Greenway Project, is a public-private partnership that aims to connect Forest Park and the Washington University in St. Louis Danforth Campus to Gateway Arch National Park. Among the partners leading this project are the Arch to Park Collaborative, St. Louis City, and Washington University in St. Louis.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49460", "revid": "39166520", "url": "https://en.wikipedia.org/wiki?curid=49460", "title": "Boole's syllogistic", "text": "Mathematical argument\nBoolean logic is a system of syllogistic logic invented by 19th-century British mathematician George Boole, which attempts to incorporate the \"empty set\", that is, a class of non-existent entities, such as round squares, without resorting to uncertain truth values.\nIn Boolean logic, the universal statements \"all S is P\" and \"no S is P\" (contraries in the traditional Aristotelian schema) are compossible provided that the set of \"S\" is the empty set. \"All S is P\" is construed to mean that \"there is nothing that is both S and not-P\"; \"no S is P\", that \"there is nothing that is both S and P\". For example, since there is nothing that is a round square, it is true both that nothing is a round square and purple, and that nothing is a round square and \"not\"-purple. Therefore, both universal statements, that \"all round squares are purple\" and \"no round squares are purple\" are true.\nSimilarly, the subcontrary relationship is dissolved between the existential statements \"some S is P\" and \"some S is not P\". The former is interpreted as \"there is some S such that S is P\" and the latter, \"there is some S such that S is not P\", both of which are clearly false where S is nonexistent.\nThus, the subaltern relationship between universal and existential also does not hold, since for a nonexistent S, \"All S is P\" is true but does not entail \"Some S is P\", which is false. Of the Aristotelian square of opposition, only the contradictory relationships remain intact.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49462", "revid": "31928", "url": "https://en.wikipedia.org/wiki?curid=49462", "title": "Pope Callistus I", "text": ""}
{"id": "49463", "revid": "50519108", "url": "https://en.wikipedia.org/wiki?curid=49463", "title": "Upper and Lower Egypt", "text": "Two regions of Ancient Egypt \nIn Egyptian history, the Upper and Lower Egypt period (also known as The Two Lands) was the final stage of prehistoric Egypt and directly preceded the unification of the realm. The conception of Egypt as the Two Lands was an example of the dualism in ancient Egyptian culture and frequently appeared in texts and imagery, including in the titles of Egyptian pharaohs.\nThe Egyptian title \"zm\ua723-t\ua723wj\" (Egyptological pronunciation \"sema-tawy\") is usually translated as \"Uniter of the Two Lands\" and was depicted as a human trachea entwined with the papyrus and lily plant. The trachea stood for unification, while the papyrus and lily plant represent Lower and Upper Egypt.\nStandard titles of the pharaoh included the prenomen, quite literally \"Of the Sedge and Bee\" (nswt-bjtj, the symbols of Upper and Lower Egypt) and \"lord of the Two Lands\" (written \"nb-t\ua723wj\"). Queens regnant were addressed as pharaohs and male. Queens consort might use the feminine versions of the second title, \"lady of The Two Lands\" (\"nbt-t\ua723wj\"), \"mistress of the Entire Two Lands\" (\"hnwt-t\ua723wy-tm\"), and \"mistress of the Two Lands\" (\"hnwt-t\ua723wy\").\nStructure.\nAncient Egypt was divided into two regions, namely Upper Egypt and Lower Egypt. To the north was Lower Egypt, where the Nile stretched out with its several branches to form the Nile Delta. To the south was Upper Egypt, stretching to Aswan. The terminology \"Upper\" and \"Lower\" derives from the flow of the Nile from the highlands of East Africa northwards to the Mediterranean Sea.\nThe two kingdoms of Upper and Lower Egypt were united c. 3000 BC, but each maintained its own regalia: the \"hedjet\" or White Crown for Upper Egypt and the \"deshret\" or Red Crown for Lower Egypt. Thus, the pharaohs were known as the rulers of the Two Lands, and wore the \"pschent\", a double crown, each half representing sovereignty of one of the kingdoms. Ancient Egyptian tradition credited Menes, now believed to be the same as Narmer, as the king who united Upper and Lower Egypt. On the Narmer Palette, the king is depicted wearing the Red Crown on one scene and the White crown in another, and thereby showing his rule over both Lands.\nSema Tawy and symbolism.\nThe union of Upper and Lower Egypt is symbolized by knotted papyrus and reed plants. This binding motif represents both harmony through unity and domination through containment. Duality plays a key role in royal iconography and is sometimes extended further depicting the knotted plants binding foreign enemies from both the North and the South.\nDuring the first dynasty, dualistic royal titles emerge, including the King of Upper and Lower Egypt (\"nswt bjtj\") title which combines the plant representing Upper Egypt and a bee representing Lower Egypt. The other dualistic title is the Two Ladies name or Nebty name. The two ladies are Nekhbet, the vulture goddess associated with Nekhen in Upper Egypt, and Wadjet, the cobra goddess associated with Buto in Lower Egypt.\nThere are many depictions of the ritual unifications of the Two Lands. It is not known if this was perhaps a rite that would have been enacted at the beginning of a reign, or merely a symbolic representation. Many of the depictions of the unification show two gods binding the plants. Often the gods are Horus and Set, or on occasion Horus and Thoth. There are several examples of Barque stands from the reigns of Amenhotep III (Hermopolis), Taharqa (Jebel Barkal), and Atlanersa (Jebel Barkal) that show two river gods performing the rite. This matches a scene from the Temple at Abu Simbel from the time of Ramesses II.\nThere are only a handful of scenes that show the King himself performing the ritual. All of these are from barque stands and date to the reigns of Amenhotep III, Seti I and Ramesses III. The latter two may be copies of the first one.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49468", "revid": "64", "url": "https://en.wikipedia.org/wiki?curid=49468", "title": "Lea Goldberg", "text": ""}
{"id": "49471", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=49471", "title": "Osiris myth", "text": "Story in ancient Egyptian mythology\nThe Osiris myth is the most elaborate and influential story in ancient Egyptian mythology. It concerns the murder of the god Osiris, a primeval king of Egypt, and its consequences. Osiris's murderer, his brother Set, usurps his throne. Meanwhile, Osiris's wife Isis restores her husband's body, allowing him to posthumously conceive their son, Horus. The remainder of the story focuses on Horus, the product of the union of Isis and Osiris, who is at first a vulnerable child protected by his mother and then becomes Set's rival for the throne. Their often violent conflict ends with Horus's triumph, which restores \"maat\" (cosmic and social order) to Egypt after Set's unrighteous reign and completes the process of Osiris's resurrection.\nThe myth, with its complex symbolism, is integral to ancient Egyptian conceptions of kingship and succession, conflict between order and disorder, and especially death and the afterlife. It also expresses the essential character of each of the four deities at its center, and many elements of their worship in ancient Egyptian religion were derived from the myth.\nThe Osiris myth reached its basic form in or before the 24th century BCE. Many of its elements originated in religious ideas, but the struggle between Horus and Set may have been partly inspired by a regional conflict in Predynastic or Early Dynastic times. Scholars have tried to discern the exact nature of the events that gave rise to the story, but they have reached no definitive conclusions.\nParts of the myth appear in a wide variety of Egyptian texts, from funerary texts and magical spells to short stories. The story is, therefore, more detailed and more cohesive than any other ancient Egyptian myth. Yet no Egyptian source gives a full account of the myth, and the sources vary widely in their versions of events. Greek and Roman writings, particularly \"On Isis and Osiris\" by Plutarch, provide more information but may not always accurately reflect Egyptian beliefs. Through these writings, the Osiris myth persisted after knowledge of most ancient Egyptian beliefs was lost, and it is still well known today.\nSources.\nThe myth of Osiris was deeply influential in ancient Egyptian religion and was popular among ordinary people. One reason for this popularity is the myth's primary religious meaning, which implies that any dead person can reach a pleasant afterlife. Another reason is that the characters and their emotions are more reminiscent of the lives of real people than those in most Egyptian myths, making the story more appealing to the general populace. In particular, the myth conveys a \"strong sense of family loyalty and devotion\", as the Egyptologist J. Gwyn Griffiths puts it, in the relationships between Osiris, Isis, and Horus.\nWith this widespread appeal, the myth appears in more ancient texts than any other myth and in an exceptionally broad range of Egyptian literary styles. These sources also provide an unusual amount of detail. Ancient Egyptian myths are fragmentary and vague; the religious metaphors contained within the myths were more important than coherent narration. Each text that contains a myth, or a fragment of one, may adapt the myth to suit its particular purposes, so different texts can contain contradictory versions of events. Because the Osiris myth was used in such a variety of ways, versions often conflict with each other. Nevertheless, the fragmentary versions, taken together, give it a greater resemblance to a cohesive story than most Egyptian myths.\nThough Osiris appears earlier in private tombs, the earliest mentions of the Osiris myth are in the Pyramid Texts, the first Egyptian funerary texts, which appeared on the walls of burial chambers in pyramids at the end of the Fifth Dynasty, during the 24th century BCE. These texts, made up of disparate spells or \"utterances\", contain ideas that are presumed to date from still earlier times. The texts are concerned with the afterlife of the king buried in the pyramid, so they frequently refer to the Osiris myth, which is deeply involved with kingship and the afterlife. Major elements of the story, such as the death and restoration of Osiris and the strife between Horus and Set, appear in the utterances of the Pyramid Texts. Funerary texts written in later times, such as the Coffin Texts from the Middle Kingdom (c. 2055\u20131650 BCE) and the \"Book of the Dead\" from the New Kingdom (c. 1550\u20131070 BCE), also contain elements of the myth.\nOther types of religious texts give evidence for the myth, such as two Middle Kingdom texts: the Dramatic Ramesseum Papyrus and the Ikhernofret Stela. The papyrus describes the coronation of Senusret I, whereas the stela alludes to events in the annual festival of Khoiak. Rituals in both these festivals reenacted elements of the Osiris myth. The most complete ancient Egyptian account of the myth is the Great Hymn to Osiris, an inscription from the Eighteenth Dynasty (c. 1550\u20131292 BCE) that gives the general outline of the entire story but includes little detail. Another important source is the Memphite Theology, a religious narrative that includes an account of Osiris's death as well as the resolution of the dispute between Horus and Set. This narrative associates the kingship that Osiris and Horus represent with Ptah, the creator deity of Memphis. The text was long thought to date back to the Old Kingdom (c. 2686\u20132181 BCE) and was treated as a source for information about the early stages in the development of the myth. Since the 1970s, however, Egyptologists have concluded that the text dates to the New Kingdom at the earliest.\nRituals in honor of Osiris are another major source of information. Some of these texts are found on the walls of temples that date from the New Kingdom, the Ptolemaic era (323\u201330 BCE), or the Roman era (30 BCE to the fourth century CE). Some of these late ritual texts, in which Isis and Nephthys lament their brother's death, were adapted into funerary texts. In these texts, the goddesses' pleas were meant to rouse Osiris\u2014and thus the deceased person\u2014to live again.\nMagical healing spells, which were used by Egyptians of all classes, are the source for an important portion of the myth, in which Horus is poisoned or otherwise sickened, and Isis heals him. The spells identify a sick person with Horus so that he or she can benefit from the goddess's efforts. The spells are known from papyrus copies, which serve as instructions for healing rituals, and from a specialized type of inscribed stone stela called a \"cippus\". People seeking healing poured water over these cippi, an act that was believed to imbue the water with the healing power contained in the text, and then drank the water in hope of curing their ailments. The theme of an endangered child protected by magic also appears on inscribed ritual wands from the Middle Kingdom, which were made centuries before the more detailed healing spells that specifically connect this theme with the Osiris myth.\nEpisodes from the myth were also recorded in writings that may have been intended as entertainment. Prominent among these texts is \"The Contendings of Horus and Set\", a humorous retelling of several episodes of the struggle between the two deities, which dates to the Twentieth Dynasty (c. 1190\u20131070 BCE). It vividly characterizes the deities involved; as the Egyptologist Donald B. Redford says, \"Horus appears as a physically weak but clever Puck-like figure, Seth [Set] as a strong-man buffoon of limited intelligence, Re-Horakhty [Ra] as a prejudiced, sulky judge, and Osiris as an articulate curmudgeon with an acid tongue.\" Despite its atypical nature, \"Contendings\" includes many of the oldest episodes in the divine conflict, and many events appear in the same order as in much later accounts, suggesting that a traditional sequence of events was forming at the time that the story was written.\nAncient Greek and Roman writers, who described Egyptian religion late in its history, recorded much of the Osiris myth. Herodotus, in the 5th century BCE, mentioned parts of the myth in his description of Egypt in the \"Histories\", and four centuries later, Diodorus Siculus provided a summary of the myth in his \"Bibliotheca historica\". In the early 2nd century CE, Plutarch wrote the most complete ancient account of the myth in \"On Isis and Osiris\", an analysis of Egyptian religious beliefs. Plutarch's account of the myth is the version that modern popular writings most frequently retell. The writings of these classical authors may give a distorted view of Egyptian beliefs. For instance, \"On Isis and Osiris\" includes many interpretations of Egyptian belief that are influenced by various Greek philosophies, and its account of the myth contains portions with no known parallel in Egyptian tradition. Griffiths concluded that several elements of this account were taken from Greek mythology, and that the work as a whole was not based directly on Egyptian sources. His colleague John Baines, on the other hand, says that temples may have kept written accounts of myths that were later lost, and that Plutarch could have drawn on such sources to write his narrative.\nSynopsis.\nDeath and resurrection of Osiris.\nAt the start of the story, Osiris rules Egypt, having inherited the kingship from his ancestors in a lineage stretching back to the creator of the world, Ra or Atum. His queen is Isis, who, along with Osiris and his murderer, Set, are the children of the earth god Geb and the sky goddess Nut. Little information about the reign of Osiris appears in Egyptian sources; the focus is on his death and the events that follow. Osiris is connected with life-giving power, righteous kingship, and the rule of \"maat\", the ideal natural order whose maintenance was a fundamental goal in ancient Egyptian culture. Set is closely associated with violence and chaos. Therefore, the slaying of Osiris symbolizes the struggle between order and disorder, and the disruption of life by death.\nSome versions of the myth provide Set's motive for killing Osiris. According to a spell in the \"Pyramid Texts\", Set is taking revenge for a kick Osiris gave him, whereas in a Late Period text, Set's grievance is that Osiris had sex with Nephthys, who is Set's consort and the fourth child of Geb and Nut. The murder itself is frequently alluded to, but never clearly described. The Egyptians believed that written words had the power to affect reality, so they avoided writing directly about profoundly negative events such as Osiris's death. Sometimes they denied his death altogether, even though the bulk of the traditions about him make it clear that he has been murdered. In some cases the texts suggest that Set takes the form of a wild animal, such as a crocodile or bull, to slay Osiris; in others they imply that Osiris's corpse is thrown in the water or that he is drowned. This latter tradition is the origin of the Egyptian belief that people who had drowned in the Nile were sacred. Even the identity of the victim can vary, as it is sometimes the god Haroeris, an elder form of Horus, who is murdered by Set and then avenged by another form of Horus, who is Haroeris's son by Isis.\nBy the end of the New Kingdom, a tradition had developed that Set had cut Osiris's body into pieces and scattered them across Egypt. Cult centers of Osiris all over the country claimed that the corpse, or particular pieces of it, were found near them. The dismembered parts could be said to number as many as forty-two, each piece being equated with one of the forty-two nomes, or provinces, in Egypt. Thus the god of kingship becomes the embodiment of his kingdom.\nOsiris's death is followed either by an interregnum or by a period in which Set assumes the kingship. Meanwhile, Isis searches for her husband's body with the aid of Nephthys. When searching for or mourning Osiris, the two goddesses are often likened to falcons or kites, possibly because kites travel far in search of carrion, because the Egyptians associated their plaintive calls with cries of grief, or because of the goddesses' connection with Horus, who is often represented as a falcon. In the New Kingdom, when Osiris's death and renewal came to be associated with the annual flooding of the Nile that fertilized Egypt, the waters of the Nile were equated with Isis's tears of mourning or with Osiris's bodily fluids. Osiris thus represented the life-giving divine power that was present in the river's water and in the plants that grew after the flood.\nThe goddesses find and restore Osiris's body, often with the help of other deities, including Thoth, a deity credited with great magical and healing powers, and Anubis, the god of embalming and funerary rites. Osiris becomes the first mummy, and the gods' efforts to restore his body are the mythological basis for Egyptian embalming practices, which sought to prevent and reverse the decay that follows death. This part of the story is often extended with episodes in which Set or his followers try to damage the corpse, and Isis and her allies must protect it. Once Osiris is made whole, Isis conceives his son and rightful heir, Horus. One ambiguous spell in the Coffin Texts may indicate that Isis is impregnated by a flash of lightning, while in other sources, Isis, still in bird form, fans breath and life into Osiris's body with her wings and copulates with him. Osiris's revival is apparently not permanent, and after this point in the story he is only mentioned as the ruler of the Duat, the distant and mysterious realm of the dead. Although he lives on only in the Duat, he and the kingship he stands for will, in a sense, be reborn in his son.\nThe cohesive account by Plutarch, which deals mainly with this portion of the myth, differs in many respects from the known Egyptian sources. Set\u2014whom Plutarch, using Greek names for many of the Egyptian deities, refers to as \"Typhon\"\u2014conspires against Osiris with seventy-two unspecified accomplices, as well as a queen from ancient Aethiopia (Nubia). Set has an elaborate chest made to fit Osiris's exact measurements and then, at a banquet, declares that he will give the chest as a gift to whoever fits inside it. The guests, in turn, lie inside the coffin, but none fit inside except Osiris. When he lies down in the chest, Set and his accomplices slam the cover shut, seal it, and throw it into the Nile. With Osiris's corpse inside, the chest floats out into the sea, arriving at the city of Byblos, where a tree grows around it. The king of Byblos has the tree cut down and made into a pillar for his palace, still with the chest inside. Isis must remove the chest from within the tree in order to retrieve her husband's body. Having taken the chest, she leaves the tree in Byblos, where it becomes an object of worship for the locals. This episode, which is not known from Egyptian sources, gives an etiological explanation for a cult of Isis and Osiris that existed in Byblos in Plutarch's time and possibly as early as the New Kingdom.\nPlutarch also states that Set steals and dismembers the corpse only after Isis has retrieved it. Isis then finds and buries each piece of her husband's body, with the exception of the penis, which she has to reconstruct with magic, because the original was eaten by fish in the river. According to Plutarch, this is the reason the Egyptians had a taboo against eating fish. In Egyptian accounts, however, the penis of Osiris is found intact, and the only close parallel with this part of Plutarch's story is in \"The Tale of Two Brothers\", a folk tale from the New Kingdom with similarities to the Osiris myth.\nA final difference in Plutarch's account is Horus's birth. The form of Horus that avenges his father has been conceived and born before Osiris's death. It is a premature and weak second child, Harpocrates, who is born from Osiris's posthumous union with Isis. Here, two of the separate forms of Horus that exist in Egyptian tradition have been given distinct positions within Plutarch's version of the myth.\nBirth and childhood of Horus.\nIn Egyptian accounts, the pregnant Isis hides from Set, to whom the unborn child is a threat, in a thicket of papyrus in the Nile Delta. This place is called \"Akh-bity\", meaning \"papyrus thicket of the king of Lower Egypt\" in Egyptian. Greek writers call this place \"Khemmis\" and indicate that it is near the city of Buto, but in the myth, the physical location is less important than its nature as an iconic place of seclusion and safety. The thicket's special status is indicated by its frequent depiction in Egyptian art; for most events in Egyptian mythology, the backdrop is minimally described or illustrated. In this thicket, Isis gives birth to Horus and raises him, and hence it is also called the \"nest of Horus\". The image of Isis nursing her child is a very common motif in Egyptian art.\nThere are texts such as the Metternich Stela that date to the Late Period in which Isis travels in the wider world. She moves among ordinary humans who are unaware of her identity, and she even appeals to these people for help. This is another unusual circumstance, for in Egyptian myth, gods and humans are normally separate. As in the first phase of the myth, she often has the aid of other deities, who protect her son in her absence. According to one magical spell, seven minor scorpion deities travel with and guard Isis as she seeks help for Horus. They even take revenge on a wealthy woman who has refused to help Isis by stinging the woman's son, making it necessary for Isis to heal the blameless child. This story conveys a moral message that the poor can be more virtuous than the wealthy and illustrates Isis's fair and compassionate nature.\nIn this stage of the myth, Horus is a vulnerable child beset by dangers. The magical texts that use Horus's childhood as the basis for their healing spells give him different ailments, from scorpion stings to simple stomachaches, adapting the tradition to fit the malady that each spell was intended to treat. Most commonly, the child god has been bitten by a snake, reflecting the Egyptians' fear of snakebite and the resulting poison. Some texts indicate that these hostile creatures are agents of Set. Isis may use her own magical powers to save her child, or she may plead with or threaten deities such as Ra or Geb, so they will cure him. As she is the archetypal mourner in the first portion of the story, so during Horus's childhood she is the ideal devoted mother. Through the magical healing texts, her efforts to heal her son are extended to cure any patient.\nConflict of Horus and Set.\nThe next phase of the myth begins when the adult Horus challenges Set for the throne of Egypt. The contest between them is often violent but is also described as a legal judgment before the Ennead, an assembled group of Egyptian deities, to decide who should inherit the kingship. The judge in this trial may be Geb, who, as the father of Osiris and Set, held the throne before they did, or it may be the creator gods Ra or Atum, the originators of kingship. Other deities also take important roles: Thoth frequently acts as a conciliator in the dispute or as an assistant to the divine judge, and in \"Contendings\", Isis uses her cunning and magical power to aid her son.\nThe rivalry of Horus and Set is portrayed in two contrasting ways. Both perspectives appear as early as the \"Pyramid Texts\", the earliest source of the myth. In some spells from these texts, Horus is the son of Osiris and nephew of Set, and the murder of Osiris is the major impetus for the conflict. The other tradition depicts Horus and Set as brothers. This incongruity persists in many of the subsequent sources, where the two gods may be called brothers or uncle and nephew at different points in the same text.\nThe divine struggle involves many episodes. \"Contendings\" describes the two gods appealing to various other deities to arbitrate the dispute and competing in different types of contests, such as racing in boats or fighting each other in the form of hippopotami, to determine a victor. In this account, Horus repeatedly defeats Set and is supported by most of the other deities. Yet the dispute drags on for eighty years, largely because the judge, the creator god, favors Set. In late ritual texts, the conflict is characterized as a great battle involving the two deities' assembled followers. The strife in the divine realm extends beyond the two combatants. At one point Isis attempts to harpoon Set as he is locked in combat with her son, but she strikes Horus instead, who then cuts off her head in a fit of rage. Thoth replaces Isis's head with that of a cow; the story gives a mythical origin for the cow-horn headdress that Isis commonly wears.\nIn a key episode in the conflict, Set sexually abuses Horus. Set's violation is partly meant to degrade his rival, but it also involves homosexual desire, in keeping with one of Set's major characteristics, his forceful and indiscriminate sexuality. In the earliest account of this episode, in a fragmentary Middle Kingdom papyrus, the sexual encounter begins when Set asks to have sex with Horus, who agrees on the condition that Set will give Horus some of his strength. The encounter puts Horus in danger, because in Egyptian tradition semen is a potent and dangerous substance, akin to poison. According to some texts, Set's semen enters Horus's body and makes him ill, but in \"Contendings\", Horus thwarts Set by catching Set's semen in his hands. Isis retaliates by putting Horus's semen on lettuce-leaves that Set eats. Set's defeat becomes apparent when this semen appears on his forehead as a golden disk. He has been impregnated with his rival's seed and as a result \"gives birth\" to the disk. In \"Contendings\", Thoth takes the disk and places it on his own head; other accounts imply that Thoth himself was produced by this anomalous birth.\nAnother important episode concerns mutilations that the combatants inflict upon each other: Horus injures or steals Set's testicles and Set damages or tears out one, or occasionally both, of Horus's eyes. Sometimes the eye is torn into pieces. Set's mutilation signifies a loss of virility and strength. The removal of Horus's eye is even more important, for this stolen Eye of Horus represents a wide variety of concepts in Egyptian religion. One of Horus's major roles is as a sky deity, and for this reason his right eye was said to be the sun and his left eye the moon. The theft or destruction of the Eye of Horus is therefore equated with the darkening of the moon in the course of its cycle of phases, or during eclipses. Horus may take back his lost Eye, or other deities, including Isis, Thoth, and Hathor, may retrieve or heal it for him. The Egyptologist Herman te Velde argues that the tradition about the lost testicles is a late variation on Set's loss of semen to Horus, and that the moon-like disk that emerges from Set's head after his impregnation is the Eye of Horus. If so, the episodes of mutilation and sexual abuse would form a single story, in which Set assaults Horus and loses semen to him, Horus retaliates and impregnates Set, and Set comes into possession of Horus's Eye when it appears on Set's head. Because Thoth is a moon deity in addition to his other functions, it would make sense, according to te Velde, for Thoth to emerge in the form of the Eye and step in to mediate between the feuding deities.\nIn any case, the restoration of the Eye of Horus to wholeness represents the return of the moon to full brightness, the return of the kingship to Horus, and many other aspects of \"maat\". Sometimes the restoration of Horus's eye is accompanied by the restoration of Set's testicles, so that both gods are made whole near the conclusion of their feud.\nResolution.\nAs with so many other parts of the myth, the resolution is complex and varied. Often, Horus and Set divide the realm between them. This division can be equated with any of several fundamental dualities that the Egyptians saw in their world. Horus may receive the fertile lands around the Nile, the core of Egyptian civilization, in which case Set takes the barren desert or the foreign lands that are associated with it; Horus may rule the earth while Set dwells in the sky; and each god may take one of the two traditional halves of the country, Upper and Lower Egypt, in which case either god may be connected with either region. Yet in the Memphite Theology, Geb, as judge, first apportions the realm between the claimants and then reverses himself, awarding sole control to Horus. In this peaceable union, Horus and Set are reconciled, and the dualities that they represent have been resolved into a united whole. Through this resolution, order is restored after the tumultuous conflict.\nA different view of the myth's end focuses on Horus's sole triumph. In this version, Set is not reconciled with his rival but utterly defeated, and sometimes he is exiled from Egypt or even destroyed. His defeat and humiliation is more pronounced in sources from later periods of Egyptian history, when he was increasingly equated with disorder and evil, and the Egyptians no longer saw him as an integral part of natural order.\nWith great celebration among the gods, Horus takes the throne, and Egypt finally has a rightful king. The divine decision that Set is in the wrong corrects the injustice created by Osiris's murder and completes the process of his restoration after death. Sometimes Set is made to carry Osiris's body to its tomb as part of his punishment. The new king performs funerary rites for his father and gives food offerings to sustain him\u2014often including the Eye of Horus, which in this instance represents life and plenty. According to some sources, only through these acts can Osiris be fully enlivened in the afterlife and take his place as king of the dead, paralleling his son's role as king of the living. Thereafter, Osiris is deeply involved with natural cycles of death and renewal, such as the annual growth of crops, that parallel his own resurrection.\nOrigins.\nAs the Osiris myth first appears in the \"Pyramid Texts\", most of its essential features must have taken shape sometime before the texts were written down. The distinct segments of the story\u2014Osiris's death and restoration, Horus's childhood, and Horus's conflict with Set\u2014may originally have been independent mythic episodes. If so, they must have begun to coalesce into a single story by the time of the \"Pyramid Texts\", which loosely connect those segments. In any case, the myth was inspired by a variety of influences. Much of the story is based in religious ideas and the general nature of Egyptian society: the divine nature of kingship, the succession from one king to another, the struggle to maintain \"maat\", and the effort to overcome death. For instance, the lamentations of Isis and Nephthys for their dead brother may represent an early tradition of ritualized mourning.\nThere are, however, important points of disagreement. The origins of Osiris are much debated, and the basis for the myth of his death is also somewhat uncertain. One influential hypothesis was given by the anthropologist James Frazer, who in 1906 said that Osiris, like other \"dying and rising gods\" across the ancient Near East, began as a personification of vegetation. His death and restoration, therefore, were based on the yearly death and re-growth of plants. Many Egyptologists adopted this explanation. But in the late 20th century, J. Gwyn Griffiths, who extensively studied Osiris and his mythology, argued that Osiris originated as a divine ruler of the dead, and his connection with vegetation was a secondary development. Meanwhile, some scholars of comparative religion have criticized the overarching concept of \"dying and rising gods\", or at least Frazer's assumption that all these gods closely fit the same pattern. More recently, the Egyptologist Rosalie David maintains that Osiris originally \"personified the annual rebirth of the trees and plants after the [Nile] inundation.\"\nAnother continuing debate concerns the opposition of Horus and Set, which Egyptologists have often tried to connect with political events early in Egypt's history or prehistory. The cases in which the combatants divide the kingdom, and the frequent association of the paired Horus and Set with the union of Upper and Lower Egypt, suggest that the two deities represent some kind of division within the country. Egyptian tradition and archaeological evidence indicate that Egypt was united at the beginning of its history when an Upper Egyptian kingdom, in the south, conquered Lower Egypt in the north. The Upper Egyptian rulers called themselves \"followers of Horus\", and Horus became the patron god of the unified nation and its kings. Yet Horus and Set cannot be easily equated with the two halves of the country. Both deities had several cult centers in each region, and Horus is often associated with Lower Egypt and Set with Upper Egypt. One of the better-known explanations for these discrepancies was proposed by Kurt Sethe in 1930. He argued that Osiris was originally the human ruler of a unified Egypt in prehistoric times, before a rebellion of Upper Egyptian Set-worshippers. The Lower Egyptian followers of Horus then forcibly reunified the land, inspiring the myth of Horus's triumph, before Upper Egypt, now led by Horus worshippers, became prominent again at the start of the Early Dynastic Period.\nIn the late 20th century, Griffiths focused on the inconsistent portrayal of Horus and Set as brothers and as uncle and nephew. He argued that, in the early stages of Egyptian mythology, the struggle between Horus and Set as siblings and equals was originally separate from the murder of Osiris. The two stories were joined into the single Osiris myth sometime before the writing of the \"Pyramid Texts\". With this merging, the genealogy of the deities involved and the characterization of the Horus\u2013Set conflict were altered so that Horus is the son and heir avenging Osiris's death. Traces of the independent traditions remained in the conflicting characterizations of the combatants' relationship and in texts unrelated to the Osiris myth, which make Horus the son of the goddess Nut or the goddess Hathor rather than of Isis and Osiris. Griffiths therefore rejected the possibility that Osiris's murder was rooted in historical events. This hypothesis has been accepted by more recent scholars such as Jan Assmann and George Hart.\nGriffiths sought a historical origin for the Horus\u2013Set rivalry, and he posited two distinct predynastic unifications of Egypt by Horus worshippers, similar to Sethe's theory, to account for it. Yet the issue remains unresolved, partly because other political associations for Horus and Set complicate the picture further. Before even Upper Egypt had a single ruler, two of its major cities were Nekhen, in the far south, and Naqada, many miles to the north. The rulers of Nekhen, where Horus was the patron deity, are generally believed to have unified Upper Egypt, including Naqada, under their sway. Set was associated with Naqada, so it is possible that the divine conflict dimly reflects an enmity between the cities in the distant past. Much later, at the end of the Second Dynasty (c. 2890\u20132686 BCE), King Peribsen used the Set animal in writing his \"serekh\"-name, in place of the traditional falcon hieroglyph representing Horus. His successor Khasekhemwy used both Horus and Set in the writing of his \"serekh\". This evidence has prompted conjecture that the Second Dynasty saw a clash between the followers of the Horus-king and the worshippers of Set led by Peribsen. Khasekhemwy's use of the two animal symbols would then represent the reconciliation of the two factions, as does the resolution of the myth.\nNoting the uncertainty surrounding these events, Herman te Velde argues that the historical roots of the conflict are too obscure to be very useful in understanding the myth and are not as significant as its religious meaning. He says that \"the origin of the myth of Horus and Seth is lost in the mists of the religious traditions of prehistory.\"\nInfluence.\nThe effect of the Osiris myth on Egyptian culture was greater and more widespread than that of any other myth. In literature, the myth was not only the basis for a retelling such as \"Contendings\"; it also provided the basis for more distantly related stories. \"The Tale of Two Brothers\", a folk tale with human protagonists, includes elements similar to the myth of Osiris. One character's penis is eaten by a fish, and he later dies and is resurrected. Another story, \"The Tale of Truth and Falsehood\", adapts the conflict of Horus and Set into an allegory, in which the characters are direct personifications of truth and lies rather than deities associated with those concepts.\nOsiris and funerary ritual.\nFrom at least the time of the \"Pyramid Texts\", kings hoped that after their deaths they could emulate Osiris's restoration to life and his rule over the realm of the dead. By the early Middle Kingdom (c. 2055\u20131650 BCE), non-royal Egyptians believed that they, too, could overcome death as Osiris had, by worshipping him and receiving the funerary rites that were partly based on his myth. Osiris thus became Egypt's most important afterlife deity. The myth also influenced the notion, which grew prominent in the New Kingdom, that only virtuous people could reach the afterlife. As the assembled deities judged Osiris and Horus to be in the right, undoing the injustice of Osiris's death, so a deceased soul had to be judged righteous in order for his or her death to be undone. As ruler of the land of the dead and as a god connected with \"maat\", Osiris became the judge in this posthumous trial, offering life after death to those who followed his example. New Kingdom funerary texts such as the \"Amduat\" and the \"Book of Gates\" liken Ra himself to a deceased soul. In them, he travels through the Duat and unites with Osiris to be reborn at dawn. Thus, Osiris was not only believed to enable rebirth for the dead; he renewed the sun, the source of life and \"maat\", and thus renewed the world itself.\nAs the importance of Osiris grew, so did his popularity. By late in the Middle Kingdom, the centuries-old tomb of the First Dynasty ruler Djer, near Osiris's main center of worship in the city of Abydos, was seen as Osiris's tomb. Accordingly, it became a major focus of Osiris worship. For the next 1,500 years, an annual festival procession traveled from Osiris's main temple to the tomb site. Kings and commoners from across Egypt built chapels, which served as cenotaphs, near the processional route. In doing so they sought to strengthen their connection with Osiris in the afterlife.\nAnother major funerary festival, a national event spread over several days in the month of Khoiak in the Egyptian calendar, became linked with Osiris during the Middle Kingdom. During Khoiak the \"djed\" pillar, an emblem of Osiris, was ritually raised into an upright position, symbolizing Osiris's restoration. By Ptolemaic times (305\u201330 BCE), Khoiak also included the planting of seeds in an \"Osiris bed\", a mummy-shaped bed of soil, connecting the resurrection of Osiris with the seasonal growth of plants.\nHorus, the Eye of Horus, and kingship.\nThe myth's religious importance extended beyond the funerary sphere. Mortuary offerings, in which family members or hired priests presented food to the deceased, were logically linked with the mythological offering of the Eye of Horus to Osiris. By analogy, this episode of the myth was eventually equated with other interactions between a human and a being in the divine realm. In temple offering rituals, the officiating priest took on the role of Horus, the gifts to the deity became the Eye of Horus, and whichever deity received these gifts was momentarily equated with Osiris.\nThe myth influenced popular religion as well. One example is the magical healing spells based on Horus's childhood. Another is the use of the Eye of Horus as a protective emblem in personal apotropaic amulets. Its mythological restoration made it appropriate for this purpose, as a general symbol of well-being.\nThe ideology surrounding the living king was also affected by the Osiris myth. The Egyptians envisioned the events of the Osiris myth as taking place sometime in Egypt's dim prehistory, and Osiris, Horus, and their divine predecessors were included in Egyptian lists of past kings such as the Turin Royal Canon. Horus, as a primeval king and as the personification of kingship, was regarded as the predecessor and exemplar for all Egyptian rulers. His assumption of his father's throne and pious actions to sustain his spirit in the afterlife were the model for all pharaonic successions to emulate. Each new king was believed to renew \"maat\" after the death of the preceding king, just as Horus had done. In royal coronations, rituals alluded to Osiris's burial, and hymns celebrated the new king's accession as the equivalent of Horus's own.\nSet.\nThe Osiris myth contributed to the frequent characterization of Set as a disruptive, harmful god. Although other elements of Egyptian tradition credit Set with positive traits, in the Osiris myth the sinister aspects of his character predominate. He and Horus were often juxtaposed in art to represent opposite principles, such as good and evil, intellect and instinct, and the different regions of the world that they rule in the myth. Egyptian wisdom texts contrast the character of the ideal person with the opposite type\u2014the calm and sensible \"Silent One\" and the impulsive, disruptive \"Hothead\"\u2014and one description of these two characters calls them the Horus-type and the Set-type. Yet the two gods were often treated as part of a harmonious whole. In some local cults they were worshipped together; in art they were often shown tying together the emblems of Upper and Lower Egypt to symbolize the unity of the nation; and in funerary texts they appear as a single deity with the heads of Horus and Set, apparently representing the mysterious, all-encompassing nature of the Duat.\nOverall Set was viewed with ambivalence, until during the first millennium BCE he came to be seen as a totally malevolent deity. This transformation was prompted more by his association with foreign lands than by the Osiris myth. Nevertheless, in these late times, the widespread temple rituals involving the ceremonial annihilation of Set were often connected with the myth.\nIsis, Nephthys, and the Greco-Roman world.\nBoth Isis and Nephthys were seen as protectors of the dead in the afterlife because of their protection and restoration of Osiris's body. The motif of Isis and Nephthys protecting Osiris or the mummy of the deceased person was very common in funerary art. Khoiak celebrations made reference to, and may have ritually reenacted, Isis's and Nephthys's mourning, restoration, and revival of their murdered brother. As Horus's mother, Isis was also the mother of every king according to royal ideology, and kings were said to have nursed at her breast as a symbol of their divine legitimacy. Her appeal to the general populace was based in her protective character, as exemplified by the magical healing spells. In the Late Period, she was credited with ever greater magical power, and her maternal devotion was believed to extend to everyone. By Roman times she had become the most important goddess in Egypt. The image of the goddess holding her child was used prominently in her worship\u2014for example, in panel paintings that were used in household shrines dedicated to her. Isis's iconography in these paintings closely resembles and may have influenced the earliest Christian icons of Mary holding Jesus.\nIn the late centuries BCE, the worship of Isis spread from Egypt across the Mediterranean world, and she became one of the most popular deities in the region. Although this new, multicultural form of Isis absorbed characteristics from many other deities, her original mythological nature as a wife and mother was key to her appeal. Horus and Osiris, being central figures in her story, spread along with her. The Greek and Roman cult of Isis developed a series of initiation rites dedicated to Isis and Osiris, based on earlier Greco-Roman mystery rites but colored by Egyptian afterlife beliefs. The initiate went through an experience that simulated descent into the underworld. Elements of this ritual resemble Osiris's merging with the sun in Egyptian funerary texts. Isis's Greek and Roman devotees, like the Egyptians, believed that she protected the dead in the afterlife as she had done for Osiris, and they said that undergoing the initiation guaranteed to them a blessed afterlife. It was to a Greek priestess of Isis that Plutarch wrote his account of the myth of Osiris.\nThrough the work of classical writers such as Plutarch, knowledge of the Osiris myth was preserved even after the middle of the first millennium AD, when Egyptian religion ceased to exist and knowledge of the writing systems that were originally used to record the myth were lost. The myth remained a major part of Western impressions of ancient Egypt. In modern times, when understanding of Egyptian beliefs is informed by the original Egyptian sources, the story continues to influence and inspire new ideas, from works of fiction to scholarly speculation and new religious movements.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49482", "revid": "1303258547", "url": "https://en.wikipedia.org/wiki?curid=49482", "title": "Das Rheingold", "text": "1869 opera by Richard Wagner\nDas Rheingold (; The Rhinegold), WWV 86A, is the first of the four epic music dramas that constitute Richard Wagner's cycle \"Der Ring des Nibelungen\" (English: \"The Ring of the Nibelung\"). It premiered as a single opera at the National Theatre of Munich on 22 September 1869, and received its first performance as part of the \"Ring\" cycle at the Bayreuth Festspielhaus on 13 August 1876.\nWagner wrote the \"Ring\" librettos in reverse order, so that \"Das Rheingold\" was the last of the texts to be written; it was, however, the first to be set to music. The score was completed in 1854, but Wagner was unwilling to sanction its performance until the whole cycle was complete; he worked intermittently on this music until 1874. The 1869 Munich premiere of \"Das Rheingold\" was staged, against Wagner's wishes, on the orders of King Ludwig II of Bavaria, his patron. Following its 1876 Bayreuth premiere, the \"Ring\" cycle was introduced into the worldwide repertory, with performances in all the main opera houses, in which it has remained a regular and popular fixture.\nIn his 1851 essay \"Opera and Drama\", Wagner had set out new principles as to how music dramas should be constructed, under which the conventional forms of opera (arias, ensembles, choruses) were rejected. Rather than providing word-settings, the music would interpret the text emotionally, reflecting the feelings and moods behind the work, by using a system of recurring leitmotifs to represent people, ideas and situations. \"Das Rheingold\" was Wagner's first work that adopted these principles, and his most rigid adherence to them, despite a few deviations \u2013 the Rhinemaidens frequently sing in ensemble.\nAs the \"preliminary evening\" within the cycle, \"Das Rheingold\" gives the background to the events that drive the main dramas of the cycle. It recounts Alberich's theft of the Rhine gold after his renunciation of love; his fashioning of the all-powerful ring from the gold and his enslavement of the Nibelungs; Wotan's seizure of the gold and the ring, to pay his debt to the giants who have built his fortress Valhalla; Alberich's curse on the ring and its possessors; Erda's warning to Wotan to forsake the ring; the early manifestation of the curse's power after Wotan yields the ring to the giants; and the gods' uneasy entry into Valhalla, under the shadow of their impending doom.\nBackground and context.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n Structure of the \"Ring\" cycle\nHaving completed his opera \"Lohengrin\" in April 1848, Richard Wagner chose as his next subject Siegfried, the legendary hero of Germanic myth. In October of that year he prepared a prose outline for \"Siegfried's Death\", which during the following months he developed into a full libretto. After his flight from Dresden and relocation in Switzerland, he continued to develop and expand his Siegfried project, having decided meantime that a single work would not suffice for his purposes; in his enlarged concept, \"Siegfried's Death\" would be the culmination of a series of musical dramas incorporating a network of myths from his sources and imagination, each telling a stage of the story. In 1851 he outlined his purpose in his essay \"A Communication to My Friends\": \"I propose to produce my myth in three complete dramas, preceded by a lengthy prelude (Vorspiel).\" Each of these dramas would, he said, constitute an independent whole, but would not be performed separately. \"At a specially-appointed Festival, I propose, some future time, to produce those three dramas with their prelude, in the course of three days and a fore-evening.\nIn accordance with this scheme, \"Siegfried's Death\", much revised from its original form, eventually became \"G\u00f6tterd\u00e4mmerung\" (\"The Twilight of the Gods\"). It was preceded by the story of Siegfried's youth, \"Young Siegfried\", later renamed \"Siegfried\", itself preceded by \"Die Walk\u00fcre\" (\"The Valkyrie\"). Finally, to these three works Wagner added a prologue which he named \"Das Rheingold\".\nSynopsis.\nPrelude\nScene 1.\nAt the bottom of the Rhine, the three Rhinemaidens, Woglinde, Wellgunde, and Flo\u00dfhilde, play together. Alberich, a Nibelung dwarf, appears from a deep chasm and tries to woo them. The maidens mock his advances and he grows angry \u2013 he chases them, but they elude, tease and humiliate him. A sudden ray of sunshine pierces the depths, to reveal the Rhinegold. The maidens rejoice in the gold's gleam. Alberich asks what it is. They explain that the gold, which their father has ordered them to guard, can be made into a magic ring which gives power to rule the world, if its bearer first renounces love. The maidens think they have nothing to fear from the lustful dwarf, but Alberich, embittered by their mockery, curses love, seizes the gold and returns to his chasm, leaving them screaming in dismay.\nOrchestral interlude\nScene 2.\nWotan, ruler of the gods, is asleep on a mountaintop, with a magnificent castle behind him. His wife, Fricka, wakes Wotan, who salutes their new home. Fricka reminds him of his promise to the giants Fasolt and Fafner, who built the castle, that he would give them Fricka's sister Freia, the goddess of youth and beauty, as payment. Fricka is worried for her sister, but Wotan trusts that Loge, the cunning demigod of fire, will find an alternative payment.\nFreia enters in a panic, followed by Fasolt and Fafner. Fasolt demands that Freia be given up. He points out that Wotan's authority is sustained by the treaties carved into his spear, including his contract with the giants, which Wotan therefore cannot violate. Donner, god of thunder, and Froh, god of sunshine, arrive to defend Freia, but Wotan cannot permit the use of force to break the agreement. Hoping that Loge will arrive with the alternative payment he has promised, Wotan tries to stall.\nWhen Loge arrives, his initial report is discouraging: nothing is more valuable to men than love, so there is apparently no possible alternative payment besides Freia. Loge was able to find only one instance where someone willingly gave up love for something else: Alberich the Nibelung has renounced love, stolen the Rhine gold, and made a powerful magic ring out of it. A discussion of the ring and its powers ensues, and everyone finds good reasons for wanting to own it. Fafner makes a counter-offer: the giants will accept the Nibelung's treasure in payment, instead of Freia. When Wotan tries to haggle, the giants depart, taking Freia with them as hostage and threatening to keep her forever unless the gods ransom her by obtaining and giving them the Nibelung's gold by the end of the day.\nFreia's golden apples had kept the gods eternally young, but in her absence they begin to age and weaken. In order to redeem Freia, Wotan resolves to travel with Loge to Alberich's subterranean kingdom to obtain the gold.\nOrchestral interlude \u2013 Abstieg nach Nibelheim (\"Descent into Nibelheim\")\nScene 3.\nIn Nibelheim, Alberich has enslaved the rest of the Nibelung dwarves with the power of the ring. He has forced his brother Mime, a skillful smith, to create a magic helmet, the Tarnhelm. Alberich demonstrates the Tarnhelm's power by making himself invisible to better torment his subjects.\nWotan and Loge arrive and happen upon Mime, who tells them of the dwarves' misery under Alberich's rule. Alberich returns, driving his slaves to pile up a huge mound of gold. He boasts to the visitors about his plans to conquer the world using the power of the ring. Loge asks how he can protect himself against a thief while he sleeps. Alberich replies the Tarnhelm will hide him, by allowing him to turn invisible or change his form. Loge expresses doubt and requests a demonstration. Alberich complies by transforming himself into a giant snake; Loge acts suitably impressed, and then asks whether Alberich can also reduce his size, which would be very useful for hiding. Alberich transforms himself into a toad. Wotan and Loge seize him, tie his hands, and drag him up to the surface.\nOrchestral Interlude - Aufstieg von Nibelheim (\"Ascent from Nibelheim\")\nScene 4.\nBack on the mountaintop, Wotan and Loge force Alberich to exchange his wealth for his freedom. He summons the Nibelungen, who bring up the hoard of gold. He then asks for the return of the Tarnhelm, but Loge says that it is part of his ransom. Alberich still hopes he can keep the ring, but Wotan demands it, and when Alberich refuses, Wotan tears it from Alberich's hand and puts it on his own finger. Crushed by his loss, Alberich lays a curse on the ring: until it returns to him, it will inspire restless jealousy in those who own it, and murderous envy in those who do not, thus condemning all the possessors of the ring.\nThe gods reconvene. Fasolt and Fafner return with Freia. Fasolt, reluctant to release her, insists that the gold be piled high enough to hide her from view. Wotan is forced to relinquish the Tarnhelm, to help cover Freia completely. However, Fasolt spots a remaining crack in the gold, through which one of Freia's eyes can be seen. Loge says that there is no more gold, but Fafner, who has noticed the ring on Wotan's finger, demands that Wotan add it to the pile, to block the crack. Loge protests that the ring belongs to the Rheinmaidens, and Wotan angrily declares that he intends to keep it for his own. As the giants seize Freia and start to leave, Erda, the earth goddess, appears and warns Wotan of impending doom, urging him to give up the cursed ring. Troubled, Wotan calls the giants back and surrenders the ring. The giants release Freia and begin dividing the treasure, but they quarrel over the ring itself. Fafner clubs Fasolt to death. Wotan, horrified, realizes that Alberich's curse has terrible power.\nDonner summons a thunderstorm to clear the air, after which Froh creates a rainbow bridge that stretches to the gate of the castle. Wotan leads the gods across the bridge to the castle, which he names Valhalla. Loge does not follow; he says in an aside that he is tempted to destroy the complacent gods by fire \u2013 he will think it over. Far below, the Rhinemaidens mourn the loss of their gold and condemn the gods as false and cowardly.\nWriting history.\nText, sources, characters.\nBecause Wagner developed his \"Ring\" scheme in reverse chronological order, the \"poem\" (libretto) for \"Das Rheingold\" was the last of the four to be written. He finished his prose plan for the work in March 1852, and on 15 September began writing the full libretto, which he completed on 3 November. In February 1853, at the Hotel Baur au Lac in Z\u00fcrich, Wagner read the whole \"Ring\" text to an invited audience, after which all four parts were published in a private edition limited to 50 copies. The text was not published commercially until 1863.\nOf the principal sources that Wagner used in creating the \"Ring\" cycle, the Scandinavian Eddas \u2013 the Poetic Edda and the Prose Edda \u2013 provided most of the material for \"Das Rheingold\". These are poems and texts from 12th and 13th-century Iceland, which relate the doings of various Norse gods. Among these stories, a magic ring and a hoard of gold held by the dwarf Andvari (Wagner's Alberich) are stolen by the gods Odin (Wotan) and Loki (Loge) and used to redeem a debt to two brothers. One of these, Fafnir, kills his brother and turns himself into a dragon to guard the gold. The Eddas also introduce the gods Thor (Donner), Frey (Froh) and the goddesses Frigg (Fricka) and Freyja (Freia). The idea of Erda, the earth mother, may have been derived from the character Jord (meaning \"Earth\"), who appears in the Eddas as the mother of Thor.\nA few Rhinegold characters originate from outside the Eddas. Mime appears in the Thidriks saga, as a human smith rather than as an enslaved Nibelung. The three Rhinemaidens do not appear in any of the sagas and are substantially Wagner's own invention; he also provided their individual names Woglinde, Wellgunde and Flo\u00dfhilde. In his analysis of \"The Ring\" Deryck Cooke suggests the Rhinemaidens' origin may be in the \"Nibelungenlied\", where three water sprites tease the characters Hagen and Gunther. Wagner may also have been influenced by the Rhine-based German legend of Lorelei, who lures fishermen on to the rocks by her singing, and by the Greek Hesperides myth in which three maidens guard a golden treasure.\nRobert Jacobs, in his biography of the composer, observes that the \"Nibelung Myth\" on which Wagner based his entire \"Ring\" story was \"very much a personal creation\", the result of Wagner's \"brilliant manipulation\" of his sources. In the \"Rheingold\" text, Wagner used his imaginative powers to adapt, change and distort the stories and characters from the sagas. J.K. Holman, in his \"Listener's Guide and Concordance\" (2001), cites the Alberich character as typifying Wagner's ability to \"consolidate selected aspects from diverse stories to create ... vivid, consistent and psychologically compelling portrait[s]\". While some characters' importance is enhanced in Wagner's version, others, such as Donner, Froh, and Freia, who are major figures in the sagas, are reduced by Wagner to roles of largely passive impotence.\nWagner originally conceived the first scene of \"Das Rheingold\" as a prologue to the three scenes that follow it. As such, the structure replicates that of \"G\u00f6tterd\u00e4mmerung\", and also that of the full \"Ring\" cycle.\nComposition.\nAs early as 1840, in his novella \"A Pilgrimage to Beethoven\", Wagner had anticipated a form of lyric drama in which the standard operatic divisions would disappear. In early 1851 he published his book-length essay \"Opera and Drama\", in which he expounded his emerging ideas around the concept of \"Gesamtkunstwerk\" \u2013 \"total work of art\". In the new kind of musical drama, he wrote, the traditional operatic norms of chorus, arias and vocal numbers would have no part. The vocal line would, in Gutman's words, \"interpret the text emotionally through artificially calculated juxtapositions of rhythm, accent, pitch and key relationships\". The orchestra, as well as providing the instrumental colour appropriate to each stage situation, would use a system of leitmotifs, each representing musically a person, an idea or a situation. Wagner termed these \"motifs of reminiscence and presentiment\", which carry intense emotional experience through music rather than words. According to Jacobs, they should \"permeate the entire tissue of the music drama\". The Rheingold score is structured around many such motifs; analysts have used different principles in determining the total number. Holman counts 42, while Roger Scruton, in his 2017 philosophical analysis of the \"Ring\", numbers them at 53.\nApart from some early sketches in 1850, relating to \"Siegfried's Death\", Wagner composed the \"Ring\" music in its proper sequence. Thus, \"Das Rheingold\" was his first attempt to adopt the principles set out in \"Opera and Drama\". According to his memoirs, Wagner's first inspiration for the music came to him in a half-dream, on 4 September 1853, while he was in Spezia in Italy. He records a feeling of \"sinking in swiftly flowing water. The rushing sound formed itself in my brain into a musical sound, the chord of E flat major, which continually re-echoed in broken forms ... I at once recognised that the orchestral overture to the Rheingold, which must long have lain latent within me, though it had been unable to find definite form, had at last been revealed to me\". Some authorities (for example Millington et al., 1992) have disputed the validity of this tale, which Nikolaus Bacht refers to as an \"acoustic hallucination\".\nAfter an extended tour, Wagner was back in Z\u00fcrich by late October and began writing down the \"Rheingold\" music on 1 November. He finished the first draft in mid-January 1854, and by the end of May had completed the full orchestral score. According to Holman, the result was \"a stunning break from Wagner's earlier musical output\" In the three years following his completion of the \"Rheingold\" score, Wagner wrote the music for \"Die Walk\u00fcre\", and for the first two acts of \"Siegfried\". At that point, in 1857, he set \"Siegfried\" aside in order to work on \"Tristan und Isolde\", and did not return to the \"Ring\" project for 12 years.\nPerformances.\nPremiere, Munich, 22 September 1869.\nLong before \"Das Rheingold\" was ready for performance, Wagner conducted excerpts of the music from scenes 1, 2 and 4, at a concert in Vienna on 26 December 1862. The work remained unstaged, but by 1869 Wagner's principal financial sponsor, King Ludwig of Bavaria, was pressing for an early performance in Munich. Wagner wanted to wait until the cycle was completed, when he would stage the work himself; also, his return to Munich would likely have precipitated a scandal, in view of his, at the time, affair with the married Cosima von B\u00fclow. Wagner was horrified at the idea of his work being presented in accordance with Ludwig's eccentric tastes. However, Ludwig, who possessed the copyright, was insistent that \"Rheingold\" be produced at the Munich Hofoper without further delay. Wagner did all he could to sabotage this production, fixed for August 1869, and persuaded the appointed conductor, Hans Richter to stand down after a troublesome dress rehearsal. Ludwig was unmoved; he denounced Wagner, sacked Richter, appointed another conductor, Franz W\u00fcllner, and rescheduled the premiere for 22 September. Wagner was refused admission to the rehearsals at the theatre, and returned, angry and defeated, to his home in Tribschen.\nAccounts differ as to the success or otherwise of the Munich premiere. Osborne maintains that the performance was successful, as does Holman, while Oliver Hilmes in his biography of Cosima describes it as \"an artistic disaster\". Cosima's diary entries for 24 and 27 September note that the performance was portrayed in the Munich press as a \"succ\u00e8s d'estime\", or otherwise as \"a lavishly decorated, boring work\". Gutman maintains that much of the adverse comment on the Munich premiere derives from later Bayreuth propaganda, and concludes that, \"in many ways, these Munich performances surpassed the level of the first Bayreuth festival\". As to the public's reaction, the audience's main interest was in the novel scenery and stage effects; Wagner's new approach to composition largely passed them by.\nBayreuth premiere, 13 August 1876.\nIn 1876, with the Bayreuth Festspielhaus built, Wagner was ready to stage the first Bayreuth Festival with his own production of the now complete \"Ring\" cycle, beginning with a performance of \"Das Rheingold\" on 13 August. This event was preceded by months of preparation in which Wagner was deeply engaged; according to witnesses, he was \"director, producer, coach, conductor, singer, actor, stage manager, stage hand and prompter\". He searched Europe for the finest orchestral players, and selected a largely new cast of singers \u2013 of the Munich cast, only Heinrich Vogl (Loge) was engaged, although Richter, deposed as conductor in Munich, was given the baton in Bayreuth.\nThe 13 August premiere was an event of international importance, and attracted a distinguished audience which included Kaiser Wilhelm I, Emperor Pedro II of Brazil and numerous representatives of the various European royal houses. King Ludwig, unwilling to face contact with his fellow-royals or the assembled crowd, attended the dress rehearsals incognito, but left Bayreuth before the opening night. Most of Europe's leading composers were also present, including Tchaikovsky, Gounod, Bruckner, Grieg, Saint-Sa\u00ebns and Wagner's father-in law Franz Liszt, together with a large corps of music critics and opera house managers. The huge influx of visitors overwhelmed the resources of the modest-sized town and caused considerable discomfort to some of the most distinguished of the guests; Tchaikovsky later described his sojourn at Bayreuth as a \"struggle for existence\".\nDespite the careful preparation, the first Bayreuth performance of \"Das Rheingold\" was punctuated by several mishaps. Some scene changes were mishandled; at one point a backdrop was prematurely lifted to reveal a number of stagehands and stage machinery; early in scene 4, Franz Betz (as Wotan) mislaid the ring and had to go backstage to look for it; the gas lighting failed repeatedly, plunging the auditorium into darkness. Some innovations worked well \u2013 the wheeled machinery used by the Rhinemaidens to simulate swimming was successful, and the quality of the singing pleased even Wagner, who was otherwise in despair and refused to present himself to the audience despite their clamouring for him. The critics made much of the technical shortcomings, which were largely overcome during the course of the festival, although, to Wagner's fury, they failed to acknowledge this fact.\nRevivals.\nTraditional productions.\nAfter the 1876 festival, \"Das Rheingold\" was not seen again at Bayreuth for 20 years, until Cosima revived the \"Ring\" cycle for the 1896 festival. Meanwhile, opera houses across Europe sought to mount their own productions, the first to do so being the Vienna State Opera, which staged \"Das Rheingold\" on 24 January 1878. In April 1878 \"Das Rheingold\" was produced in Leipzig, as part of the first full \"Ring\" cycle to be staged outside Bayreuth. London followed suit in May 1882, when \"Rheingold\" began a cycle at Her Majesty's Theatre, Haymarket, under the baton of Anton Seidl. In the years following the London premiere, \"Ring\" cycles were staged in many European capitals. In Budapest on 26 January 1889, the first Hungarian performance of \"Das Rheingold\", conducted by the young Gustav Mahler, was briefly interrupted when the prompt-box caught fire and a number of patrons fled the theatre.\nThe American premiere of \"Das Rheingold\" was given by the New York Metropolitan Opera in January 1889, as a single opera, with Seidl conducting. The production used Carl Emil Doepler's original Bayreuth costume designs, and scenery was imported from Germany. According to \"The New York Times\", \"[t]he scenery, costumes and effects were all designed and executed with great art and caused admirable results.\" Of particular note was the performance of Joseph Beck who sang Alberich: \"a fine example of Wagnerian declamatory singing, His delivery of the famous curse of the ring was notably excellent in its distinctness and dramatic force\". On 4 March 1889, with largely the January cast, Seidl conducted \"Das Rheingold\" to begin the first American \"Ring\" cycle. Thereafter, \"Das Rheingold\", either alone or as part of the \"Ring\", became a regular feature of the international opera repertory, being seen in Saint Petersburg (1889), Paris (1901), Buenos Aires (1910), Melbourne (1913), and Rio de Janeiro (1921), as well many other major venues.\nAfter the 1896 revival, \"Das Rheingold\" was being performed regularly at the Bayreuth Festival, although not every year, within a variety of \"Ring\" productions. Until the Second World War, under the successive artistic control of Cosima (from 1896 to 1907), her son Siegfried (1908 to 1930) and Siegfried's widow Winifred (1931 to 1943), these productions did not deviate greatly from the stagings devised by Wagner for the 1876 premiere. With few exceptions, this generally conservative, even reverential approach \u2013 which extended to all Wagner's operas \u2013 tended to be mirrored in performances outside Bayreuth.\nNew Bayreuth and experimentation.\nThe Bayreuth Festival, suspended after the Second World War, resumed in 1951 under Wieland Wagner, Siegfried's son, who introduced his first \"Ring\" cycle in the \"New Bayreuth\" style. This was the antithesis of all that had been seen at Bayreuth before, as scenery, costumes and traditional gestures were abandoned and replaced by a bare disc, with evocative lighting effects to signify changes of scene or mood. The stark New Bayreuth style dominated most \"Rheingold\" and \"Ring\" productions worldwide until the 1970s, when a reaction to its bleak austerity produced a number of fresh approaches. The Bayreuth centenary \"Ring\" production of 1976, directed by Patrice Ch\u00e9reau provided a significant landmark in the history of Wagner stagings: \"Ch\u00e9reau's demythologization of the tetralogy entailed an anti-heroic view of the work ... his setting of the action in an industrialized society ... along with occasional 20th century costumes and props, suggested a continuity between Wagner's time and our own\". Many of this production's features were highly controversial: the opening of \"Das Rheingold\" revealed a vast hydro-electric dam in which the gold is stored, guarded by the Rhinemaidens who were portrayed, in Spotts's words, as \"three voluptuous tarts\" \u2013 a depiction, he says, which \"caused a shock from which no one quite recovered\". According to \"The Observer\"'s critic, \"I had not experienced in the theatre protest as furious as that which greeted \"Das Rheingold\".\" Eventually this hostility was overcome; the final performance of this production, in 1980, was followed by an ovation that lasted ninety minutes.\nPost-1980s.\nThe iconoclastic centenary \"Ring\" was followed by numerous original interpretations, at Bayreuth and elsewhere, in the late 20th and early 21st centuries. The 1988 festival opened with Harry Kupfer's grim interpretation of \"Das Rheingold\", in which Wotan and the other gods were represented as gangsters in mafioso sunglasses. This entire \"Ring\", says Spotts, was \"a parable of how the power-hungry cheat, lie, bully, terrorise and kill to get what they want\". In August Everding's Chicago \"Reingold\" (which would become part of a full \"Ring\" cycle four years later), the Rhinemaidens were attached to elasticated ropes manipulated from the wings, which enabled them to cavort freely through the air, using lip sync to co-ordinate with off-stage singers. Edward Rothstein, writing in \"The New York Times\", found the production \"a puzzle ... cluttered with contraptions and conceits\" which, he imagined, were visual motifs which would be clarified in later operas. Keith Warner, in his 2004 production for Covent Garden, portrayed, according to Barry Millington's analysis, \"the shift from a deistic universe to one controlled by human beings\". The dangers of subverted scientific progress were demonstrated in the third \"Rheingold\" scene, where Nibelheim was represented as a medical chamber of horrors, replete with vivisections and \"unspeakable\" genetic experiments.\nFrom the late 1980s a backlash against the tendency towards ever more outlandish interpretations of the \"Ring\" cycle led to several new productions in the more traditional manner. Otto Schenk's staging of \"Das Rheingold\", first seen at the New York Met in 1987 and forming the prelude to his full \"Ring\" cycle two years later, was described by \"The New York Times\" as \"charmingly old fashioned\", and as \"a relief to many beleaguered Wagnerites\". James Morris, who sang Wotan in the 1987 production, and James Levine, the original conductor, both returned in 2009 when Schenk brought his \"Ring\" cycle back to the Met for a final performance.\nMusic.\n\"Das Rheingold\" was Wagner's first attempt to write dramatic music in accordance with the principles he had enunciated in \"Opera and Drama\", hence the general absence in the score of conventional operatic \"numbers\" in the form of arias, ensembles and choruses. Rather than acting as the accompanist to the voices, the orchestra combines with them on equal terms to propel the drama forward. According to Barry Millington's analysis, \"Das Rheingold\" represents Wagner's purest application of the \"Opera and Drama\" principles, a rigorous stance that he would eventually modify. Even in \"Rheingold\", as Jacobs indicates, Wagner was flexible when the dramatic occasion warranted it; thus, the Rhinemaidens sing in the disavowed ensembles, and there are several instances in which characters sing melodies that appear to be musically independent from the general flow. The music is continuous, with instrumental entr'actes linking the actions of the four discrete scenes.\nPrelude.\nThe prelude to \"Das Rheingold\" consists of an extended (136-bar) chord in E\u266d major, which begins almost inaudibly in the lowest register of eight double-basses. The note of B\u266d is added by the bassoons and the chord is further embellished as the horns enter with a rising arpeggio to announce the \"Nature\" motif, outlining the lower partials of an harmonic series with an E\u266d fundamental. This is further elaborated in the strings; the lower-register instruments sustain the E\u266d note throughout the prelude, while the chord is increasingly enhanced by the orchestra. The \"Rhine\" motif emerges, representing what Osborne describes as \"the calm, majestic course of the river's character The composer Robert Erickson describes the prelude as drone music \u2013 \"the only well-known drone piece in the concert repertory\". Millington suggests that the protracted chord does not simply represent the depths of the Rhine, rather \"the birth of the world, the act of creation itself\".\nFirst scene.\nWhen the prelude reaches its climax the curtain rises and the key shifts to A\u266d as Woglinde sings a \"greeting to the waters\". The first two and last two notes of this short, lilting passage form a falling musical step which, in different guises, will recur throughout the opera, signifying variously the Rhinemaidens' innocence, their joy in the gold and conversely, in the minor key, Alberich's woe at his rejection by the maidens, and his enslavement of the Nibelungs. The first appearance of the gold is signified by a muted horn call in the lower register, played under a shimmer of undulating strings, conveying, says Holman, \"the shining, innocent beauty of the Rhinegold in its unfashioned state.\" The motif for the ring itself first appears in the woodwind, as Wellgunde reveals that a ring fashioned from the gold would confer on its owner the power to win the wealth of the world. This is followed by what is sometimes known as the \"renunciation\" motif, when Woglinde sings that to fashion such a ring, the owner must first renounce love. Confusion arises because this same motif is used later in the \"Ring\" cycle to represent affirmation rather than rejection of love; Roger Scruton suggests the motif would be more appropriately labelled \"existential choice\". Alberich duly curses love, seizes the gold and departs, to the sounds of the despairing shrieks of the Rhinemaidens.\nSecond scene.\nDuring the first entr'acte, the Ring motif is transformed into the multipart and oft-reiterated \"Valhalla\" music \u2013 four intertwined motifs which represent the majesty of the gods and the extent of Wotan's power. Scene two begins on the mountaintop, in sight of the newly completed castle, where Fricka and Wotan bicker over Wotan's contract with the giants. This duologue is characterised by Fricka's \"Love's longing\" motif, in which she sighs for a home that will satisfy Wotan and halt his infidelities. Freia's distressed entrance is illustrated by \"Love\", a fragment that will recur and develop as the \"Ring\" cycle unfolds. The Giants' entrance is signified by heavy, stamping music that reflects both their simple nature and their brute strength. The \"Golden Apples\" motif, of \"remarkable beauty\" according to Scruton, is sung by Fafner as a threatening reminder to the gods that the loss of Freia means the loss of their youth and vigour; it is later used by Loge to mock the gods for their weakness after Freia's departure with the giants. The \"Spear\" motif, a rapidly descending scale, represents the moral basis of Wotan's power and the sanctity of the treaties engraved on it. The phrase of five descending notes known as \"Woman's Worth\", first sung by Loge, is described by Holman as one of the most pervasive and appealing motifs in the entire \"Ring\" cycle \u2013 he lists 43 occurrences of the motif throughout the cycle. Many of the \"Ring\"'s characters \u2013 Wotan, Froh, Alberich, Fasolt and Erda in \"Das Rheingold\" \u2013 either sing this phrase or are orchestrally referenced by it.\nThird scene.\nThe descent of Wotan and Loge into Nibelheim is represented musically in the second entr'acte, which begins with the \"renunciation\" and \"spear\" motifs but is quickly overwhelmed with the insistent, rhythmic 9/8 beat of the Nibelung motif in B\u266d, briefly foreshadowed in Loge's scene 2 soliloquy. In the climax to the entr'acte this rhythm is hammered out on eighteen anvils. This motif is thereafter used, not just to represent the Nibelungs but also their enslavement in a state of relentless misery. During the scene's opening interaction between Alberich and Mime, the soft, mysterious \"Tarnhelm\" motif is heard on muted horns; this is later combined with the \"serpent\" motif as, at Loge's behest, Alberich uses the Tarnhelm to transform himself into a giant snake. The transition back to the mountaintop, following Alberich's entrapment, references a number of motifs, among them Alberich's woe, the ring, renunciation and the Nibelungs' enslavement.\nFourth scene.\nAfter Wotan seizes the ring from the captive Alberich, the dwarf's agonised, self-pitying monologue (\"Am I now free?\") ends with his declamation of the \"Curse\" motif \u2013 \"one of the most sinister musical ideas ever to have entered the operatic repertoire\", according to Scruton's analysis: \"It rises through a half-diminished chord, and then falls through an octave to settle on a murky C major triad, with clarinets in their lowest register over a timpani pedal in F sharp\". This motif will recur throughout the cycle; it will be heard later in this scene, when Fafner clubs Fasolt to death over possession of the ring. Tranquil, ascending harmonies introduce the reconvention of the gods and giants. The subsequent dispute over Wotan's reluctance to part with the ring ends with Erda's appearance; her motif is a minor-key variation of the \"Nature\" motif from the prelude. After her warning she departs to the sounds of the \"Downfall\" motif, an inversion of Erda's entry that resembles \"Woman's Worth\". The scene ends with a rapid succession of motifs: \"Donner's Call\", a horn fanfare by which he summons the thunderstorm; Froh's \"Rainbow Bridge\" which provides a path for the gods into Valhalla; the \"Sword\" motif, a C major arpeggio that will become highly significant in later \"Ring\" operas, and the haunting \"Rhinemaidens' Lament\", developed from the falling step which earlier signified the maidens' joy in the gold. Scruton writes of this lament: \"And yet, ever sounding in the depths, is the lament of the Rhine-daughters, singing of a natural order that preceded the conscious will that has usurped it. This lament sounds in the unconsciousness of us all, as we pursue our paths to personality, sovereignty and freedom...\". These are the last voices that are heard in the opera, \"piercing our hearts with sudden longing, melting our bones with nostalgic desire\", before the gods, \"marching in empty triumph to their doom\", enter Valhalla to a thunderous orchestral conclusion, made up from several motifs including \"Valhalla\", \"Rainbow Bridge\" and the \"Sword\".\nOrchestral forces.\n\"Das Rheingold\" is scored for the following instrumental forces:\nCritical assessment.\nAlthough it is sometimes performed independently, \"Das Rheingold\" is not generally considered outside the structure of the \"Ring\" cycle. However, as Millington points out, it is a substantial work in its own right, and has several characteristics not shared by the other works in the tetralogy. It is comparatively short, with continuous music; no interludes or breaks. The action moves forward relatively swiftly, unencumbered, as Arnold Whittall observes, by the \"retarding explanations\" \u2013 pauses in the action to clarify the context of what is going on \u2013 that permeate the later, much longer works. Its lack of the conventional operatic devices (arias, choruses, ensembles) further enable the story to progress briskly.\nSince it was written as a prelude to the main events, \"Das Rheingold\" is in itself inconclusive, leaving numerous loose ends to be picked up later; its function, as Jacobs says, is \"to expound, not to draw conclusions\". The fact that most of its characters display decidedly human emotions makes it seem, according to a recent writer, \"much more a present-day drama than a remote fable\". Nevertheless, Philip Kennicott, writing in \"The Washington Post\" describes it as \"the hardest of the four installments to love, with its family squabbles, extensive exposition, and the odd, hybrid world Wagner creates, not always comfortably balanced between the mythic and the recognizably human.\" Certain presumptions are challenged or overturned; John Louis Gaetani, in a 2006 essay, notes that, in Loge's view, the gods are far more culpable than the Nibelungs, and that Wotan, for all his prestige as the ruler of the gods, \"does much more evil than Alberich ever dreams of\".\nRecording.\nLive performances of \"Das Rheingold\" from Bayreuth and elsewhere were recorded from 1937 onwards, but were not issued for many years. The first studio recording, and the first to be issued commercially, was Georg Solti's 1958 Decca version, part of his complete \"Ring\" cycle, 1958\u20131966, which marked the beginning of a new era in recorded opera. Since then \"Das Rheingold\" has been recorded, as part of the cycle, on many occasions, with regular new issues.\nNotes and references.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "49484", "revid": "1306352", "url": "https://en.wikipedia.org/wiki?curid=49484", "title": "Volatilisation", "text": ""}
{"id": "49491", "revid": "49829305", "url": "https://en.wikipedia.org/wiki?curid=49491", "title": "King's Cross", "text": "King's Cross or Kings Cross may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "49492", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=49492", "title": "Divisor", "text": "Integer that is a factor of another integer\nIn mathematics, a divisor of an integer formula_1 also called a factor of formula_1 is an integer formula_3 that may be multiplied by some integer to produce formula_4 In this case, one also says that formula_5 is a \"multiple\" of formula_6 An integer formula_5 is divisible or evenly divisible by another integer formula_3 if formula_3 is a divisor of formula_5; this implies dividing formula_5 by formula_3 leaves no remainder.\nDefinition.\nAn integer formula_5 is divisible by a nonzero integer formula_3 if there exists an integer formula_15 such that formula_16 This is written as\n formula_17\nThis may be read as that formula_3 divides formula_1 formula_3 is a divisor of formula_1 formula_3 is a factor of formula_1 or formula_5 is a multiple of formula_6 If formula_3 does not divide formula_1 then the notation is formula_28\nThere are two conventions, distinguished by whether formula_3 is permitted to be zero:\nGeneral.\nDivisors can be negative as well as positive, although often the term is restricted to positive divisors. For example, there are six divisors of 4; they are 1, 2, 4, \u22121, \u22122, and \u22124, but only the positive ones (1, 2, and 4) would usually be mentioned.\n1 and \u22121 divide (are divisors of) every integer. Every integer (and its negation) is a divisor of itself. Integers divisible by 2 are called even, and integers not divisible by 2 are called odd.\n1, \u22121, formula_5 and formula_37 are known as the trivial divisors of formula_4 A divisor of formula_5 that is not a trivial divisor is known as a non-trivial divisor (or strict divisor). A nonzero integer with at least one non-trivial divisor is known as a composite number, while the units \u22121 and 1 and prime numbers have no non-trivial divisors.\nThere are divisibility rules that allow one to recognize certain divisors of a number from the number's digits.\nFurther notions and facts.\nThere are some elementary rules:\nIf formula_64 and formula_65 then formula_66 This is called Euclid's lemma.\nIf formula_67 is a prime number and formula_68 then formula_69 or formula_70\nA positive divisor of formula_5 that is different from formula_5 is called a &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;proper divisor or an &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;aliquot part of formula_5 (for example, the proper divisors of 6 are 1, 2, and 3). A number that does not evenly divide formula_5 but leaves a remainder is sometimes called an &lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;aliquant part of formula_4\nAn integer formula_76 whose only proper divisor is 1 is called a prime number. Equivalently, a prime number is a positive integer that has exactly two positive factors: 1 and itself.\nAny positive divisor of formula_5 is a product of prime divisors of formula_5 raised to some power. This is a consequence of the fundamental theorem of arithmetic.\nA number formula_5 is said to be perfect if it equals the sum of its proper divisors, deficient if the sum of its proper divisors is less than formula_1 and abundant if this sum exceeds formula_4\nThe total number of positive divisors of formula_5 is a multiplicative function formula_83 meaning that when two numbers formula_3 and formula_5 are relatively prime, then formula_86 For instance, formula_87; the eight divisors of 42 are 1, 2, 3, 6, 7, 14, 21 and 42. However, the number of positive divisors is not a totally multiplicative function: if the two numbers formula_3 and formula_5 share a common divisor, then it might not be true that formula_86 The sum of the positive divisors of formula_5 is another multiplicative function formula_92 (for example, formula_93). Both of these functions are examples of divisor functions.\nIf the prime factorization of formula_5 is given by\n formula_95\nthen the number of positive divisors of formula_5 is\n formula_97\nand each of the divisors has the form\n formula_98\nwhere formula_99 for each formula_100\nFor every natural formula_1 formula_102\nAlso,\n formula_103\nwhere formula_104 is Euler\u2013Mascheroni constant.\nOne interpretation of this result is that a randomly chosen positive integer \"n\" has an average\nnumber of divisors of about formula_105 However, this is a result from the contributions of numbers with \"abnormally many\" divisors.\nIn abstract algebra.\nDivision lattice.\nIn definitions that allow the divisor to be 0, the relation of divisibility turns the set formula_106 of non-negative integers into a partially ordered set that is a complete distributive lattice. The largest element of this lattice is 0 and the smallest is 1. The meet operation \u2227 is given by the greatest common divisor and the join operation \u2228 by the least common multiple. This lattice is isomorphic to the dual of the lattice of subgroups of the infinite cyclic group Z.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49497", "revid": "414836", "url": "https://en.wikipedia.org/wiki?curid=49497", "title": "Pascal's triangle", "text": "Triangular array of the binomial coefficients in mathematics\nformula_1 A diagram showing the first eight rows of Pascal's triangle.In mathematics, Pascal's triangle is an infinite triangular array of the binomial coefficients which play a crucial role in probability theory, combinatorics, and algebra. In much of the Western world, it is named after the French mathematician Blaise Pascal, although other mathematicians studied it centuries before him in Persia, India, China, Germany, and Italy.\nThe rows of Pascal's triangle are conventionally enumerated starting with row formula_2 at the top (the 0th row). The entries in each row are numbered from the left beginning with formula_3 and are usually staggered relative to the numbers in the adjacent rows. The triangle may be constructed in the following manner: In row 0 (the topmost row), there is a unique nonzero entry 1. Each entry of each subsequent row is constructed by adding the number above and to the left with the number above and to the right, treating blank entries as 0. For example, the initial number of row 1 (or any other row) is 1 (the sum of 0 and 1), whereas the numbers 1 and 3 in row 3 are added to produce the number 4 in row 4.\nFormula.\nIn the formula_4th row of Pascal's triangle, the formula_5th entry is denoted formula_6, pronounced \"n choose k\". For example, the topmost entry is formula_7. With this notation, the construction of the previous paragraph may be written as\nformula_8\nfor any positive integer formula_4 and any integer formula_10. This recurrence for the binomial coefficients is known as Pascal's rule.\nHistory.\nThe pattern of numbers that forms Pascal's triangle was known well before Pascal's time. The Persian mathematician Al-Karaji (953\u20131029) wrote a now-lost book which contained the first description of Pascal's triangle. In India, the \"Chanda\u1e25\u015b\u0101stra\" by the Indian poet and mathematician Pi\u1e45gala (3rd or 2nd century BC) somewhat cryptically describes a method of arranging two types of syllables to form metres of various lengths and counting them; as interpreted and elaborated by Pi\u1e45gala's 10th-century commentator Hal\u0101yudha his \"method of pyramidal expansion\" (\"meru-prast\u0101ra\") for counting metres is equivalent to Pascal's triangle. It was later repeated by Omar Khayy\u00e1m (1048\u20131131), another Persian mathematician; thus the triangle is also referred to as Khayyam's triangle () in Iran. Several theorems related to the triangle were known, including the binomial theorem. Khayyam used a method of finding \"n\"th roots based on the binomial expansion, and therefore on the binomial coefficients.\nPascal's triangle was known in China during the 11th century through the work of the Chinese mathematician Jia Xian (1010\u20131070). During the 13th century, Yang Hui (1238\u20131298) defined the triangle, and it is known as Yang Hui's triangle () in China.\nIn Europe, Pascal's triangle appeared for the first time in the \"Arithmetic\" of Jordanus de Nemore (13th century).\nThe binomial coefficients were calculated by Gersonides during the early 14th century, using the multiplicative formula for them. Petrus Apianus (1495\u20131552) published the full triangle on the frontispiece of his book on business calculations in 1527. Michael Stifel published a portion of the triangle (from the second to the middle column in each row) in 1544, describing it as a table of figurate numbers. In Italy, Pascal's triangle is referred to as Tartaglia's triangle, named for the Italian algebraist Tartaglia (1500\u20131577), who published six rows of the triangle in 1556. Gerolamo Cardano also published the triangle as well as the additive and multiplicative rules for constructing it in 1570.\nPascal's (\"Treatise on Arithmetical Triangle\") was published posthumously in 1665. In this, Pascal collected several results then known about the triangle, and employed them to solve problems in probability theory. The triangle was later named for Pascal by Pierre Raymond de Montmort (1708) who called it (French: Mr. Pascal's table for combinations) and Abraham de Moivre (1730) who called it (Latin: Pascal's Arithmetic Triangle), which became the basis of the modern Western name.\nBinomial expansions.\nPascal's triangle determines the coefficients which arise in binomial expansions. For example, in the expansion\nformula_11\nthe coefficients are the entries in the second row of Pascal's triangle: formula_12, formula_13, formula_14.\nIn general, the binomial theorem states that when a binomial like formula_15 is raised to a positive integer power formula_4, the expression expands as\nformula_17\nwhere the coefficients formula_18 are precisely the numbers in row formula_19 of Pascal's triangle:\nformula_20\nThe entire left diagonal of Pascal's triangle corresponds to the coefficient of formula_21 in these binomial expansions, while the next left diagonal corresponds to the coefficient of formula_22, and so on.\nTo see how the binomial theorem relates to the simple construction of Pascal's triangle, consider the problem of calculating the coefficients of the expansion of formula_23 in terms of the corresponding coefficients of formula_24, where we set formula_25 for simplicity. Suppose then that\nformula_26\nNow\nformula_27\nformula_28 The first six rows of Pascal's triangle as binomial coefficients\nThe two summations can be reindexed with formula_29 and combined to yield\nformula_30\nThus the extreme left and right coefficients remain as 1, and for any given formula_31, the coefficient of the formula_32 term in the polynomial formula_33 is equal to formula_34, the sum of the formula_35 and formula_32 coefficients in the previous power formula_37. This is indeed the downward-addition rule for constructing Pascal's triangle.\nIt is not difficult to turn this argument into a proof (by mathematical induction) of the binomial theorem.\nSince formula_38, the coefficients are identical in the expansion of the general case.\nAn interesting consequence of the binomial theorem is obtained by setting both variables formula_39, so that\nformula_40\nIn other words, the sum of the entries in the formula_4th row of Pascal's triangle is the formula_4th power of\u00a02. This is equivalent to the statement that the number of subsets of an formula_4-element set is formula_44, as can be seen by observing that each of the formula_4 elements may be independently included or excluded from a given subset.\nCombinations.\nA second useful application of Pascal's triangle is in the calculation of combinations. The number of combinations of formula_4 items taken formula_5 at a time, i.e. the number of subsets of formula_5 elements from among formula_4 elements, can be found by the equation\nformula_50.\nThis is equal to entry formula_5 in row formula_4 of Pascal's triangle. Rather than performing the multiplicative calculation, one can simply look up the appropriate entry in the triangle (constructed by additions). For example, suppose 3 workers need to be hired from among 7 candidates; then the number of possible hiring choices is 7 choose 3, the entry 3 in row 7 of the above table (taking into consideration the first row is the 0th row), which is formula_53.\nRelation to binomial distribution and convolutions.\nWhen divided by formula_54, the formula_55th row of Pascal's triangle becomes the binomial distribution in the symmetric case where formula_56. By the central limit theorem, this distribution approaches the normal distribution as formula_55 increases. This can also be seen by applying Stirling's formula to the factorials involved in the formula for combinations.\nThis is related to the operation of discrete convolution in two ways. First, polynomial multiplication corresponds exactly to discrete convolution, so that repeatedly convolving the sequence formula_58 with itself corresponds to taking powers of formula_59, and hence to generating the rows of the triangle. Second, repeatedly convolving the distribution function for a random variable with itself corresponds to calculating the distribution function for a sum of \"n\" independent copies of that variable; this is exactly the situation to which the central limit theorem applies, and hence results in the normal distribution in the limit. (The operation of repeatedly taking a convolution of something with itself is called the convolution power.)\nPatterns and properties.\nPascal's triangle has many properties and contains many patterns of numbers.\nDiagonals.\nThe diagonals of Pascal's triangle contain the figurate numbers of simplices:\nformula_77\nThe symmetry of the triangle implies that the \"n\"th d-dimensional number is equal to the \"d\"th \"n\"-dimensional number.\nAn alternative formula that does not involve recursion is\nformula_78\nwhere \"n\"(\"d\") is the rising factorial.\nThe geometric meaning of a function \"P\"\"d\" is: \"P\"\"d\"(1) = 1 for all \"d\". Construct a \"d\"-dimensional triangle (a 3-dimensional triangle is a tetrahedron) by placing additional dots below an initial dot, corresponding to \"P\"\"d\"(1) = 1. Place these dots in a manner analogous to the placement of numbers in Pascal's triangle. To find P\"d\"(\"x\"), have a total of \"x\" dots composing the target shape. P\"d\"(\"x\") then equals the total number of dots in the shape. A 0-dimensional triangle is a point and a 1-dimensional triangle is simply a line, and therefore \"P\"0(\"x\") = 1 and \"P\"1(\"x\") = \"x\", which is the sequence of natural numbers. The number of dots in each layer corresponds to \"P\"\"d\"\u00a0\u2212\u00a01(\"x\").\nCalculating a row or diagonal by itself.\nThere are simple algorithms to compute all the elements in a row or diagonal without computing other elements or factorials.\nTo compute row formula_4 with the elements formula_80, begin with formula_81. For each subsequent element, the value is determined by multiplying the previous value by a fraction with slowly changing numerator and denominator:\nformula_82\nFor example, to calculate row 5, the fractions are\u00a0 formula_83,\u00a0 formula_84,\u00a0 formula_85,\u00a0 formula_86 and formula_87, and hence the elements are\u00a0 formula_88, \u00a0 formula_89, \u00a0 formula_90, etc. (The remaining elements are most easily obtained by symmetry.)\nTo compute the diagonal containing the elements formula_91 begin again with formula_92 and obtain subsequent elements by multiplication by certain fractions:\nformula_93\nFor example, to calculate the diagonal beginning at formula_94, the fractions are\u00a0 formula_95, and the elements are formula_96, etc. By symmetry, these elements are equal to formula_97, etc.\nAs the proportion of black numbers tends to zero with increasing \"n\", a corollary is that the proportion of odd binomial coefficients tends to zero as \"n\" tends to infinity.\nOverall patterns and properties.\nPascal's triangle overlaid on a grid gives the number of distinct paths to each square, assuming only rightward and downward steps to an adjacent square are considered.\nConstruction as matrix exponential.\nformula_98\nDue to its simple construction by factorials, a very basic representation of Pascal's triangle in terms of the matrix exponential can be given: Pascal's triangle is the exponential of the matrix which has the sequence 1, 2, 3, 4, ... on its sub-diagonal and zero everywhere else.\nConstruction of Clifford algebra using simplices.\nLabelling the elements of each n-simplex matches the basis elements of Clifford algebra used as forms in Geometric Algebra rather than matrices. Recognising the geometric operations, such as rotations, allows the algebra operations to be discovered. Just as each row, n, starting at 0, of Pascal's triangle corresponds to an (n-1)-simplex, as described below, it also defines the number of named basis forms in n dimensional Geometric algebra. The binomial theorem can be used to prove the geometric relationship provided by Pascal's triangle. This same proof could be applied to simplices except that the first column of all 1's must be ignored whereas in the algebra these correspond to the real numbers, formula_99, with basis 1.\nRelation to geometry of polytopes.\nEach row of Pascal's triangle gives the number of elements (such as edges and corners) of each dimension in a corresponding simplex (such as a triangle or tetrahedron). In particular, for \"k\" &gt; 0, the kth entry in the nth row is the number of (\"k\" \u2212 1)-dimensional elements in a (\"n\" \u2212 1)-dimensional simplex. For example, a triangle (the 2-dimensional simplex) one 2-dimensional element (itself), three 1-dimensional elements (lines, or edges), and three 0-dimensional elements (vertices, or corners); this corresponds to the third row 1, 3, 3, 1 of Pascal's triangle. This fact can be explained by combining Pascal's rule for generating the triangle with the geometric construction of simplices: each simplex is formed from a simplex of one lower dimension by the addition of a new vertex, outside the space in which the lower-dimensional simplex lies. Then each d-dimensional element in the smaller simplex remains a d-dimensional element of the higher simplex, and each (\"d\" \u2212 1)-dimensional element when joined to the new vertex forms a new d-dimensional element of the higher simplex.\nA similar pattern is observed relating to squares, as opposed to triangles. To find the pattern, one must construct an analog to Pascal's triangle, whose entries are the coefficients of (\"x\" + 2)row number, instead of (\"x\" + 1)row number. There are a couple ways to do this. The simpler is to begin with row 0 = 1 and row 1 = 1, 2. Proceed to construct the analog triangles according to the following rule:\nformula_100\nThat is, choose a pair of numbers according to the rules of Pascal's triangle, but double the one on the left before adding. This results in:\nformula_101\nThe other way of producing this triangle is to start with Pascal's triangle and multiply each entry by 2k, where k is the position in the row of the given number. For example, the 2nd value in row 4 of Pascal's triangle is 6 (the slope of 1s corresponds to the zeroth entry in each row). To get the value that resides in the corresponding position in the analog triangle, multiply 6 by 2position number = 6 \u00d7 22 = 6 \u00d7 4 = 24. Now that the analog triangle has been constructed, the number of elements of any dimension that compose an arbitrarily dimensioned cube (called a hypercube) can be read from the table in a way analogous to Pascal's triangle. For example, the number of 2-dimensional elements in a 2-dimensional cube (a square) is one, the number of 1-dimensional elements (sides, or lines) is 4, and the number of 0-dimensional elements (points, or vertices) is 4. This matches the 2nd row of the table (1, 4, 4). A cube has 1 cube, 6 faces, 12 edges, and 8 vertices, which corresponds to the next line of the analog triangle (1, 6, 12, 8). This pattern continues indefinitely.\nTo understand why this pattern exists, first recognize that the construction of an \"n\"-cube from an (\"n\" \u2212 1)-cube is done by simply duplicating the original figure and displacing it some distance (for a regular \"n\"-cube, the edge length) orthogonal to the space of the original figure, then connecting each vertex of the new figure to its corresponding vertex of the original. This initial duplication process is the reason why, to enumerate the dimensional elements of an \"n\"-cube, one must double the first of a pair of numbers in a row of this analog of Pascal's triangle before summing to yield the number below. The initial doubling thus yields the number of \"original\" elements to be found in the next higher \"n\"-cube and, as before, new elements are built upon those of one fewer dimension (edges upon vertices, faces upon edges, etc.). Again, the last number of a row represents the number of new vertices to be added to generate the next higher \"n\"-cube.\nIn this triangle, the sum of the elements of row \"m\" is equal to 3\"m\". Again, to use the elements of row 4 as an example: 1 + 8 + 24 + 32 + 16 = 81, which is equal to formula_102.\nCounting vertices in a cube by distance.\nEach row of Pascal's triangle gives the number of vertices at each distance from a fixed vertex in an \"n\"-dimensional cube. For example, in three dimensions, the third row (1 3 3 1) corresponds to the usual three-dimensional cube: fixing a vertex \"V\", there is one vertex at distance 0 from \"V\" (that is, \"V\" itself), three vertices at distance 1, three vertices at distance \u221a2 and one vertex at distance \u221a3 (the vertex opposite \"V\"). The second row corresponds to a square, while larger-numbered rows correspond to hypercubes in each dimension.\nFourier transform of sin(\"x\")\"n\"+1/\"x\".\nAs stated previously, the coefficients of (\"x\"\u00a0+\u00a01)\"n\" are the nth row of the triangle. Now the coefficients of (\"x\"\u00a0\u2212\u00a01)\"n\" are the same, except that the sign alternates from +1 to \u22121 and back again. After suitable normalization, the same pattern of numbers occurs in the Fourier transform of sin(\"x\")\"n\"+1/\"x\". More precisely: if \"n\" is even, take the real part of the transform, and if \"n\" is odd, take the imaginary part. Then the result is a step function, whose values (suitably normalized) are given by the \"n\"th row of the triangle with alternating signs. For example, the values of the step function that results from:\nformula_103\ncompose the 4th row of the triangle, with alternating signs. This is a generalization of the following basic result (often used in electrical engineering):\nformula_104\nis the boxcar function. The corresponding row of the triangle is row 0, which consists of just the number\u00a01.\nIf n is congruent to 2 or to 3 mod 4, then the signs start with\u00a0\u22121. In fact, the sequence of the (normalized) first terms corresponds to the powers of i, which cycle around the intersection of the axes with the unit circle in the complex plane: formula_105\nExtensions.\nUpwards.\nPascal's triangle may be extended upwards, above the 1 at the apex, preserving the additive property, but there is more than one way to do so.\nTo higher dimensions.\nPascal's triangle has higher dimensional generalizations. The three-dimensional version is known as \"Pascal's pyramid\" or \"Pascal's tetrahedron\", while the general versions are known as \"Pascal's simplices\".\nTo complex numbers.\nWhen the factorial function is defined as formula_106, Pascal's triangle can be extended beyond the integers to formula_107, since formula_108 is meromorphic to the entire complex plane.\nTo arbitrary bases.\nIsaac Newton once observed that the first five rows of Pascal's triangle, when read as the digits of an integer, are the corresponding powers of eleven. He claimed without proof that subsequent rows also generate powers of eleven. In 1964, Robert L. Morton presented the more generalized argument that each row formula_4 can be read as a radix formula_110 numeral, where formula_111 is the hypothetical terminal row, or limit, of the triangle, and the rows are its partial products. He proved the entries of row formula_4, when interpreted directly as a place-value numeral, correspond to the binomial expansion of formula_113. More rigorous proofs have since been developed. To better understand the principle behind this interpretation, here are some things to recall about binomials:\nBy setting the row's radix (the variable formula_110) equal to one and ten, row formula_4 becomes the product formula_135 and formula_136, respectively. To illustrate, consider formula_137, which yields the row product formula_138. The numeric representation of formula_139 is formed by concatenating the entries of row formula_4. The twelfth row denotes the product:\n formula_141\nwith compound digits (delimited by \":\") in radix twelve. The digits from formula_142 through formula_143 are compound because these row entries compute to values greater than or equal to twelve. To normalize the numeral, simply carry the first compound entry's prefix, that is, remove the prefix of the coefficient formula_144 from its leftmost digit up to, but excluding, its rightmost digit, and use radix-twelve arithmetic to sum the removed prefix with the entry on its immediate left, then repeat this process, proceeding leftward, until the leftmost entry is reached. In this particular example, the normalized string ends with formula_145 for all formula_4. The leftmost digit is formula_147 for formula_148, which is obtained by carrying the formula_149 of formula_150 at entry formula_143. It follows that the length of the normalized value of formula_139 is equal to the row length, formula_153. The integral part of formula_154 contains exactly one digit because formula_4 (the number of places to the left the decimal has moved) is one less than the row length. Below is the normalized value of formula_156. Compound digits remain in the value because they are radix formula_157 residues represented in radix ten:\n formula_158\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49498", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=49498", "title": "Pascals triangle", "text": ""}
{"id": "49503", "revid": "33839581", "url": "https://en.wikipedia.org/wiki?curid=49503", "title": "Inductively coupled plasma mass spectrometry", "text": "Type of mass spectrometry that uses an inductively coupled plasma to ionize the sample\nInductively coupled plasma mass spectrometry (ICP-MS) is a type of mass spectrometry that uses an inductively coupled plasma to ionize the sample. It atomizes the sample and creates atomic and small polyatomic ions, which are then detected. It is known and used for its ability to detect metals and several non-metals in liquid samples at very low concentrations. It can detect different isotopes of the same element, which makes it a versatile tool in isotopic labeling.\nCompared to atomic absorption spectroscopy, ICP-MS has greater speed, precision, and sensitivity. However, compared with other types of mass spectrometry, such as thermal ionization mass spectrometry (TIMS) and glow discharge mass spectrometry (GD-MS), ICP-MS introduces many interfering species: argon from the plasma, component gases of air that leak through the cone orifices, and contamination from glassware and the cones.\nComponents.\nInductively coupled plasma.\nAn inductively coupled plasma is a plasma that is energized (ionized) by inductively heating the gas with an electromagnetic coil, and contains a sufficient concentration of ions and electrons to make the gas electrically conductive. Not all of the gas needs to be ionized for the gas to have the characteristics of a plasma; as little as 1% ionization creates a plasma. The plasmas used in spectrochemical analysis are essentially electrically neutral, with each positive charge on an ion balanced by a free electron. In these plasmas the positive ions are almost all singly charged and there are few negative ions, so there are nearly equal numbers of ions and electrons in each unit volume of plasma.\nThe ICPs have two operation modes, called capacitive (E) mode with low plasma density and inductive (H) mode with high plasma density, and E to H heating mode transition occurs with external inputs. The Inductively Coupled Plasma Mass Spectrometry is operated in the H mode.\nWhat makes Inductively Coupled Plasma Mass Spectrometry (ICP-MS) unique to other forms of inorganic mass spectrometry is its ability to sample the analyte continuously, without interruption. This is in contrast to other forms of inorganic mass spectrometry; Glow Discharge Mass Spectrometry (GDMS) and Thermal Ionization Mass Spectrometry (TIMS), that require a two-stage process: Insert sample(s) into a vacuum chamber, seal the vacuum chamber, pump down the vacuum, energize sample, thereby sending ions into the mass analyzer. With ICP-MS the sample to be analyzed is sitting at atmospheric pressure. Through the effective use of differential pumping; multiple vacuum stages separate by differential apertures (holes), the ions created in the argon plasma are, with the aid of various electrostatic focusing techniques, transmitted through the mass analyzer to the detector(s) and counted. Not only does this enable the analyst to radically increase sample throughput (amount of samples over time), but has also made it possible to do what is called \"time resolved acquisition\". Hyphenated techniques like Liquid Chromatography ICP-MS (LC-ICP-MS); Laser Ablation ICP-MS (LA-ICP-MS); Flow Injection ICP-MS (FIA-ICP-MS), etc. have benefited from this relatively new technology. It has stimulated the development new tools for research including geochemistry and forensic chemistry; biochemistry and oceanography. Additionally, increases in sample throughput from dozens of samples a day to hundreds of samples a day have revolutionized environmental analysis, reducing costs. Fundamentally, this is all due to the fact that while the sample resides at environmental pressure, the analyzer and detector are at 1/10,000,000 of that same pressure during normal operation.\nAn inductively coupled plasma (ICP) for spectrometry is sustained in a torch that consists of three concentric tubes, usually made of quartz, although the inner tube (injector) can be sapphire if hydrofluoric acid is being used. The end of this torch is placed inside an induction coil supplied with a radio-frequency electric current. A flow of argon gas (usually 13 to 18 liters per minute) is introduced between the two outermost tubes of the torch and an electric spark is applied for a short time to introduce free electrons into the gas stream. These electrons interact with the radio-frequency magnetic field of the induction coil and are accelerated first in one direction, then the other, as the field changes at high frequency (usually 27.12 million cycles per second). The accelerated electrons collide with argon atoms, and sometimes a collision causes an argon atom to part with one of its electrons. The released electron is in turn accelerated by the rapidly changing magnetic field. The process continues until the rate of release of new electrons in collisions is balanced by the rate of recombination of electrons with argon ions (atoms that have lost an electron). This produces a \u2018fireball\u2019 that consists mostly of argon atoms with a rather small fraction of free electrons and argon ions. The temperature of the plasma is very high, of the order of 10,000 K. The plasma also produces ultraviolet light, so for safety should not be viewed directly.\nThe ICP can be retained in the quartz torch because the flow of gas between the two outermost tubes keeps the plasma away from the walls of the torch. A second flow of argon (around 1 liter per minute) is usually introduced between the central tube and the intermediate tube to keep the plasma away from the end of the central tube. A third flow (again usually around 1 liter per minute) of gas is introduced into the central tube of the torch. This gas flow passes through the centre of the plasma, where it forms a channel that is cooler than the surrounding plasma but still much hotter than a chemical flame. Samples to be analyzed are introduced into this central channel, usually as a mist of liquid formed by passing the liquid sample into a nebulizer.\nTo maximise plasma temperature (and hence ionisation efficiency) and stability, the sample should be introduced through the central tube with as little liquid (solvent load) as possible, and with consistent droplet sizes. A nebuliser can be used for liquid samples, followed by a spray chamber to remove larger droplets, or a desolvating nebuliser can be used to evaporate most of the solvent before it reaches the torch. Solid samples can also be introduced using laser ablation. The sample enters the central channel of the ICP, evaporates, molecules break apart, and then the constituent atoms ionise. At the temperatures prevailing in the plasma a significant proportion of the atoms of many chemical elements are ionized, each atom losing its most loosely bound electron to form a singly charged ion. The plasma temperature is selected to maximise ionisation efficiency for elements with a high first ionisation energy, while minimising second ionisation (double charging) for elements that have a low second ionisation energy.\nMass spectrometry.\nFor coupling to mass spectrometry, the ions from the plasma are extracted through a series of cones into a mass spectrometer, usually a quadrupole. The ions are separated on the basis of their mass-to-charge ratio and a detector receives an ion signal proportional to the concentration.\nThe concentration of a sample can be determined through calibration with certified reference material such as single or multi-element reference standards. ICP-MS also lends itself to quantitative determinations through isotope dilution, a single point method based on an isotopically enriched standard. In order to increase reproducibility and compensate for errors by sensitivity variation, an internal standard can be added.\nOther mass analyzers coupled to ICP systems include double focusing magnetic-electrostatic sector systems with both single and multiple collector, as well as time of flight systems (both axial and orthogonal accelerators have been used).\nApplications.\nOne of the largest volume uses for ICP-MS is in the medical and forensic field, specifically, toxicology. A physician may order a metal assay for a number of reasons, such as suspicion of heavy metal poisoning, metabolic concerns, and even hepatological issues. Depending on the specific parameters unique to each patient's diagnostic plan, samples collected for analysis can range from whole blood, urine, plasma, serum, to even packed red blood cells. Another primary use for this instrument lies in the environmental field. Such applications include water testing for municipalities or private individuals all the way to soil, water and other material analysis for industrial purposes.\nIn recent years, industrial and biological monitoring has presented another major need for metal analysis via ICP-MS. Individuals working in factories where exposure to metals is likely and unavoidable, such as a battery factory, are required by their employer to have their blood or urine analyzed for metal toxicity on a regular basis. This monitoring has become a mandatory practice implemented by the U.S. Occupational Safety and Health Administration, in an effort to protect workers from their work environment and ensure proper rotation of work duties (i.e. rotating employees from a high exposure position to a low exposure position).\nICP-MS is also used widely in the geochemistry field for radiometric dating, in which it is used to analyze relative abundance of different isotopes, in particular uranium and lead. ICP-MS is more suitable for this application than the previously used thermal ionization mass spectrometry, as species with high ionization energy such as osmium and tungsten can be easily ionized. For high precision ratio work, multiple collector instruments are normally used to reduce the effect noise on the calculated ratios.\nIn the field of flow cytometry, a new technique uses ICP-MS to replace the traditional fluorochromes. Briefly, instead of labelling antibodies (or other biological probes) with fluorochromes, each antibody is labelled with a distinct combinations of lanthanides. When the sample of interest is analysed by ICP-MS in a specialised flow cytometer, each antibody can be identified and quantitated by virtue of a distinct ICP \"footprint\". In theory, hundreds of different biological probes can thus be analysed in an individual cell, at a rate of ca. 1,000 cells per second. Because elements are easily distinguished in ICP-MS, the problem of compensation in multiplex flow cytometry is effectively eliminated.\nLaser ablation inductively coupled plasma mass spectrometry (LA-ICP-MS) is a powerful technique for the elemental analysis of a wide variety of materials encountered in forensic casework. (LA-ICP-MS) has already successfully been applied to applications in forensics, metals, glasses, soils, car paints, bones and teeth, printing inks, trace elemental, fingerprint, and paper. Among these, forensic glass analysis stands out as an application for which this technique has great utility to provide highly.\nCar hit and runs, burglaries, assaults, drive-by shootings and bombings such as these situations may cause glass fragments that could be used as evidence of association in glass transfer conditions. LA-ICP-MS is considered one of the best techniques for analysis of glass due to the short time for sample preparation and sample, small sample size of less than 250 nanograms. In addition there is no need for complex procedure and handling of dangerous materials that is used for digestion of the samples. This allows detecting major, minor and tracing elements with high level of precision and accuracy. There are set of properties that are used to measure glass sample such as physical and optical properties including color, thickness, density, refractive index (RI) and also, if necessary, elemental analysis can be conducted in order to enhance the value of an association.\nPharmaceutical industry.\nIn the pharmaceutical industry, ICP-MS is used for detecting inorganic impurities in pharmaceuticals and their ingredients. New and reduced maximum permitted exposure levels of heavy metals from dietary supplements, introduced in USP (United States Pharmacopeia) \u00ab\"\u3008232\u3009Elemental Impurities\u2014Limits\"\u00bb and USP \u00ab\"\u3008232\u3009Elemental Impurities\u2014Procedures\"\u00bb, will increase the need for ICP-MS technology, where, previously, other analytic methods have been sufficient.\nCosmetics, such as lipstick, recovered from a crime scene may provide valuable forensic information. Lipstick smears left on cigarette butts, glassware, clothing, bedding; napkins, paper, etc. may be valuable evidence. Lipstick recovered from clothing or skin may also indicate physical contact between individuals. Forensic analysis of recovered lipstick smear evidence can provide valuable information on the recent activities of a victim or suspect. Trace elemental analysis of lipstick smears could be used to complement existing visual comparative procedures to determine the lipstick brand and color.\nSingle Particle Inductively Coupled Plasma Mass Spectroscopy (SP ICP-MS) was designed for particle suspensions in 2000 by Claude Degueldre. He first tested this new methodology at the Forel Institute of the University of Geneva and presented this new analytical approach at the 'Colloid 2oo2' symposium during the spring 2002 meeting of the EMRS, and in the proceedings in 2003. This study presents the theory of SP ICP-MS and the results of tests carried out on clay particles (montmorillonite) as well as other suspensions of colloids. This method was then tested on thorium dioxide nanoparticles by Degueldre &amp; Favarger (2004), zirconium dioxide by Degueldre \"et al\" (2004) and gold nanoparticles, which are used as a substrate in nanopharmacy, and published by Degueldre \"et al\" (2006). Subsequently, the study of uranium dioxide nano- and micro-particles gave rise to a detailed publication, Ref. Degueldre \"et al\" (2006). Since 2010 the interest for SP ICP-MS has exploded.\nPrevious forensic techniques employed for the organic analysis of lipsticks by compositional comparison include thin layer chromatography (TLC), gas chromatography (GC), and high-performance liquid chromatography (HPLC). These methods provide useful information regarding the identification of lipsticks. However, they all require long sample preparation times and destroy the sample. Nondestructive techniques for the forensic analysis of lipstick smears include UV fluorescence observation combined with purge-and-trap gas chromatography, microspectrophotometry and scanning electron microscopy-energy dispersive spectroscopy (SEM-EDS), and Raman spectroscopy.\nMetal speciation.\nA growing trend in the world of elemental analysis has revolved around the speciation, or determination of oxidation state of certain metals such as chromium and arsenic. The toxicity of those elements varies with the oxidation state, so new regulations from food authorities requires speciation of some elements. One of the primary techniques to achieve this is to separate the chemical species with high-performance liquid chromatography (HPLC) or field flow fractionation (FFF) and then measure the concentrations with ICP-MS.\nQuantification of proteins and biomolecules.\nThere is an increasing trend of using ICP-MS as a tool in speciation analysis, which normally involves a front end chromatograph separation and an elemental selective detector, such as AAS and ICP-MS. For example, ICP-MS may be combined with size exclusion chromatography and preparative native PAGE for identifying and quantifying metalloproteins in biofluids. Also the phosphorylation status of proteins can be analyzed.\nIn 2007, a new type of protein tagging reagents called metal-coded affinity tags (MeCAT) were introduced to label proteins quantitatively with metals, especially lanthanides. The MeCAT labelling allows relative and absolute quantification of all kind of proteins or other biomolecules like peptides. MeCAT comprises a site-specific biomolecule tagging group with at least a strong chelate group which binds metals. The MeCAT labelled proteins can be accurately quantified by ICP-MS down to low attomol amount of analyte which is at least 2\u20133 orders of magnitude more sensitive than other mass spectrometry based quantification methods. By introducing several MeCAT labels to a biomolecule and further optimization of LC-ICP-MS detection limits in the zeptomol range are within the realm of possibility. By using different lanthanides MeCAT multiplexing can be used for pharmacokinetics of proteins and peptides or the analysis of the differential expression of proteins (proteomics) e.g. in biological fluids. Breakable PAGE SDS-PAGE (DPAGE, dissolvable PAGE), two-dimensional gel electrophoresis or chromatography is used for separation of MeCAT labelled proteins. Flow-injection ICP-MS analysis of protein bands or spots from DPAGE SDS-PAGE gels can be easily performed by dissolving the DPAGE gel after electrophoresis and staining of the gel. MeCAT labelled proteins are identified and relatively quantified on peptide level by MALDI-MS or ESI-MS.\nElemental analysis.\nThe ICP-MS allows determination of elements with atomic mass ranges 7 to 250 (Li to U), and sometimes higher. Some masses are prohibited, such as 40 Da, due to the abundance of argon in the sample. Other interference regions may include mass 80 (due to the argon dimer) and mass 56 (due to ArO), the latter of which greatly hinders Fe detection unless the instrument is fitted with a reaction chamber. Such interferences can be reduced by using a high resolution ICP-MS (HR-ICP-MS) which uses two or more slits to constrict the beam and distinguish between nearby peaks. This comes at the cost of sensitivity. For example, distinguishing iron from argon requires a resolving power of about 10,000, which may reduce the iron sensitivity by around 99%. Interfering species can alternatively be distinguished through the use of a collision chamber, which can filter gasses by either chemical reaction or physical collision. \nA single collector ICP-MS may use a multiplier in pulse counting mode to amplify very low signals, an attenuation grid or a multiplier in analogue mode to detect medium signals, and a Faraday cup/bucket to detect larger signals. A multi-collector ICP-MS may have more than one of any of these, typically Faraday buckets which are more cost-effective than other collectors. With this combination, a dynamic range of 12 orders of magnitude, from 1 part per quadrillion (ppq) to 100 parts per million (ppm) is possible.\nICP-MS is a common method for the determination of cadmium in biological samples.\nUnlike atomic absorption spectroscopy, which can only measure a single element at a time, ICP-MS has the capability to scan for all elements simultaneously. This allows rapid sample processing. A simultaneous ICP-MS that can record the entire analytical spectrum from lithium to uranium in every analysis won the Silver Award at the 2010 Pittcon Editors' Awards. An ICP-MS may use multiple scan modes, each one striking a different balance between speed and precision. Using the magnet alone to scan is slow due to hysteresis but is precise. Electrostatic plates can be used in addition to the magnet to increase the speed, and with multiple collectors can allow a scan of every element from Lithium 6 to Uranium Oxide 256 in less than a quarter of a second. For low detection limits, interfering species and high precision, the counting time can increase substantially. The rapid scanning, large dynamic range and large mass range of ICP-MS is ideally suited to measuring multiple unknown concentrations and isotope ratios in samples that have had minimal preparation (an advantage over TIMS). The analysis of seawater, urine, and digested whole rock samples are examples of industry applications. These properties also lend well to laser-ablated rock samples, where the scanning rate is fast enough to enable a real-time plot of any number of isotopes. This also allows easy spatial mapping of mineral grains.\nHardware.\nIn terms of input and output, ICP-MS instrument consumes prepared sample material and translates it into mass-spectral data. Actual analytical procedure takes some time; after that time the instrument can be switched to work on the next sample. Series of such sample measurements requires the instrument to have plasma ignited, meanwhile a number of technical parameters has to be stable in order for the results obtained to have feasibly accurate and precise interpretation. Maintaining the plasma requires a constant supply of carrier gas (usually, pure argon) and increased power consumption of the instrument. When these additional running costs are not considered justified, plasma and most of auxiliary systems can be turned off. In such standby mode only pumps are working to keep proper vacuum in mass-spectrometer.\nThe constituents of ICP-MS instrument are designed to allow for reproducible and/or stable operation.\nSample introduction.\nThe first step in analysis is the introduction of the sample. This has been achieved in ICP-MS through a variety of means.\nThe most common method is the use of \"analytical nebulizers.\" A nebulizer converts liquids into an aerosol, and that aerosol can then be swept into the plasma to create the ions. Nebulizers work best with simple liquid samples (i.e. solutions). However, there have been instances of their use with more complex materials like a slurry. Many varieties of nebulizers have been coupled to ICP-MS, including pneumatic, cross-flow, Babington, ultrasonic, and desolvating types. The aerosol generated is often treated to limit it to only smallest droplets, commonly by means of a Peltier cooled double pass or cyclonic spray chamber. Use of autosamplers makes this easier and faster, especially for routine work and large numbers of samples. A Desolvating Nebuliser (DSN) may also be used; this uses a long heated capillary, coated with a fluoropolymer membrane, to remove most of the solvent and reduce the load on the plasma. Matrix removal introduction systems are sometimes used for samples, such as seawater, where the species of interest are at trace levels, and are surrounded by much more abundant contaminants.\nLaser ablation is another method. Though less common in the past, it has become popular as a means of sample introduction, thanks to increased ICP-MS scanning speeds. In this method, a pulsed UV laser is focused on the sample and creates a plume of ablated material, which can be swept into the plasma. This allows geochemists to spatially map the isotope composition in cross-sections of rock samples, a tool which is lost if the rock is digested and introduced as a liquid sample. Lasers for this task are built to have highly controllable power outputs and uniform radial power distributions, to produce craters which are flat bottomed and of a chosen diameter and depth.\nFor both Laser Ablation and Desolvating Nebulisers, a small flow of nitrogen may also be introduced into the argon flow. Nitrogen exists as a dimer, so has more vibrational modes and is more efficient at receiving energy from the RF coil around the torch.\nOther methods of sample introduction are also utilized. Electrothermal vaporization (ETV) and in torch vaporization (ITV) use hot surfaces (graphite or metal, generally) to vaporize samples for introduction. These can use very small amounts of liquids, solids, or slurries. Other methods like vapor generation are also known.\nPlasma torch.\nThe plasma used in an ICP-MS is made by partially ionizing argon gas (Ar \u2192 Ar+ + e\u2212). The energy required for this reaction is obtained by pulsing an alternating electric current in load coil that surrounds the plasma torch with a flow of argon gas.\nAfter the sample is injected, the plasma's extreme temperature causes the sample to separate into individual atoms (atomization). Next, the plasma ionizes these atoms (M \u2192 M+ + e\u2212) so that they can be detected by the mass spectrometer.\nAn inductively coupled plasma (ICP) for spectrometry is sustained in a torch that consists of three concentric tubes, usually made of quartz. The two major designs are the Fassel and Greenfield torches. The end of this torch is placed inside an induction coil supplied with a radio-frequency electric current. A flow of argon gas (usually 14 to 18 liters per minute) is introduced between the two outermost tubes of the torch and an electrical spark is applied for a short time to introduce free electrons into the gas stream. These electrons interact with the radio-frequency magnetic field of the induction coil and are accelerated first in one direction, then the other, as the field changes at high frequency (usually 27.12 MHz or 40 MHz). The accelerated electrons collide with argon atoms, and sometimes a collision causes an argon atom to part with one of its electrons. The released electron is in turn accelerated by the rapidly changing magnetic field. The process continues until the rate of release of new electrons in collisions is balanced by the rate of recombination of electrons with argon ions (atoms that have lost an electron). This produces a \u2018fireball\u2019 that consists mostly of argon atoms with a rather small fraction of free electrons and argon ions.\nAdvantage of argon.\nMaking the plasma from argon, instead of other gases, has several advantages. First, argon is abundant (in the atmosphere, as a result of the radioactive decay of potassium) and therefore cheaper than other noble gases. Argon also has a higher first ionization potential than all other elements except He, F, and Ne. Because of this high ionization energy, the reaction (Ar+ + e\u2212 \u2192 Ar) is more energetically favorable than the reaction (M+ + e\u2212 \u2192 M). This ensures that the sample remains ionized (as M+) so that the mass spectrometer can detect it.\nArgon can be purchased for use with the ICP-MS in either a refrigerated liquid or a gas form. However it is important to note that whichever form of argon purchased, it should have a guaranteed purity of 99.9% Argon at a minimum. It is important to determine which type of argon will be best suited for the specific situation. Liquid argon is typically cheaper and can be stored in a greater quantity as opposed to the gas form, which is more expensive and takes up more tank space. If the instrument is in an environment where it gets infrequent use, then buying argon in the gas state will be most appropriate as it will be more than enough to suit smaller run times and gas in the cylinder will remain stable for longer periods of time, whereas liquid argon will suffer loss to the environment due to venting of the tank when stored over extended time frames. However, if the ICP-MS is to be used routinely and is on and running for eight or more hours each day for several days a week, then going with liquid argon will be the most suitable. If there are to be multiple ICP-MS instruments running for long periods of time, then it will most likely be beneficial for the laboratory to install a bulk or micro bulk argon tank which will be maintained by a gas supply company, thus eliminating the need to change out tanks frequently as well as minimizing loss of argon that is left over in each used tank as well as down time for tank changeover.\nHelium can be used either in place of, or mixed with, argon for plasma generation. Helium's higher first ionisation energy allows greater ionisation and therefore higher sensitivity for hard-to-ionise elements. The use of pure helium also avoids argon-based interferences such as ArO. However, many of the interferences can be mitigated by use of a collision cell, and the greater cost of helium has prevented its use in commercial ICP-MS.\nTransfer of ions into vacuum.\nThe carrier gas is sent through the central channel and into the very hot plasma. The sample is then exposed to radio frequency which converts the gas into a plasma. The high temperature of the plasma is sufficient to cause a very large portion of the sample to form ions. This fraction of ionization can approach 100% for some elements (e.g. sodium), but this is dependent on the ionization potential. A fraction of the formed ions passes through a ~1\u00a0mm hole (sampler cone) and then a ~0.4\u00a0mm hole (skimmer cone). The purpose of which is to allow a vacuum that is required by the mass spectrometer.\nThe vacuum is created and maintained by a series of pumps. The first stage is usually based on a roughing pump, most commonly a standard rotary vane pump. This removes most of the gas and typically reaches a pressure of around 133 Pa. Later stages have their vacuum generated by more powerful vacuum systems, most often turbomolecular pumps. Older instruments may have used oil diffusion pumps for high vacuum regions.\nIon optics.\nBefore mass separation, a beam of positive ions has to be extracted from the plasma and focused into the mass-analyzer. It is important to separate the ions from UV photons, energetic neutrals and from any solid particles that may have been carried into the instrument from the ICP. Traditionally, ICP-MS instruments have used transmitting ion lens arrangements for this purpose. Examples include the Einzel lens, the Barrel lens, Agilent's Omega Lens and Perkin-Elmer's Shadow Stop. Another approach is to use ion guides (quadrupoles, hexapoles, or octopoles) to guide the ions into mass analyzer along a path away from the trajectory of photons or neutral particles. Yet another approach is Varian patented used by Analytik Jena ICP-MS 90 degrees reflecting parabolic \"Ion Mirror\" optics, which are claimed to provide more efficient ion transport into the mass-analyzer, resulting in better sensitivity and reduced background. Analytik Jena ICP-MS PQMS is the most sensitive instrument on the market.\nA sector ICP-MS will commonly have four sections: an extraction acceleration region, steering lenses, an electrostatic sector and a magnetic sector. The first region takes ions from the plasma and accelerates them using a high voltage. The second uses may use a combination of parallel plates, rings, quadrupoles, hexapoles and octopoles to steer, shape and focus the beam so that the resulting peaks are symmetrical, flat topped and have high transmission. The electrostatic sector may be before or after the magnetic sector depending on the particular instrument, and reduces the spread in kinetic energy caused by the plasma. This spread is particularly large for ICP-MS, being larger than Glow Discharge and much larger than TIMS. The geometry of the instrument is chosen so that the instrument the combined focal point of the electrostatic and magnetic sectors is at the collector, known as Double Focusing (or Double Focussing).\nIf the mass of interest has a low sensitivity and is just below a much larger peak, the low mass tail from this larger peak can intrude onto the mass of interest. A Retardation Filter might be used to reduce this tail. This sits near the collector, and applies a voltage equal but opposite to the accelerating voltage; any ions that have lost energy while flying around the instrument will be decelerated to rest by the filter.\nCollision reaction cell and iCRC.\nThe collision/reaction cell is used to remove interfering ions through ion/neutral reactions. Collision/reaction cells are known under several names. The dynamic reaction cell is located before the quadrupole in the ICP-MS device. The chamber has a quadrupole and can be filled with reaction (or collision) gases (ammonia, methane, oxygen or hydrogen), with one gas type at a time or a mixture of two of them, which reacts with the introduced sample, eliminating some of the interference.\nThe integrated Collisional Reaction Cell (iCRC) used by Analytik Jena ICP-MS is a mini-collision cell installed in front of the parabolic ion mirror optics that removes interfering ions by injecting a collisional gas (He), or a reactive gas (H2), or a mixture of the two, directly into the plasma as it flows through the skimmer cone and/or the sampler cone. The iCRC removed interfering ions using a collisional kinetic energy discrimination (KED) phenomenon and chemical reactions with interfering ions similarly to traditionally used larger collision cells.\nRoutine maintenance.\nAs with any piece of instrumentation or equipment, there are many aspects of maintenance that need to be encompassed by daily, weekly and annual procedures. The frequency of maintenance is typically determined by the sample volume and cumulative run time that the instrument is subjected to.\nOne of the first things that should be carried out before the calibration of the ICP-MS is a sensitivity check and optimization. This ensures that the operator is aware of any possible issues with the instrument and if so, may address them before beginning a calibration. Typical indicators of sensitivity are Rhodium levels, Cerium/Oxide ratios and DI water blanks. One common standard practice is to measure a standard tuning solution provided by the ICP manufacturer every time the plasma torch is started. Then the instrument is auto-calibrated for optimum sensitivity and the operator obtains a report providing certain parameters such as sensitivity, mass resolution and estimated amount of oxidized species and double-positive charged species.\nOne of the most frequent forms of routine maintenance is replacing sample and waste tubing on the peristaltic pump, as these tubes can get worn fairly quickly resulting in holes and clogs in the sample line, resulting in skewed results. Other parts that will need regular cleaning and/or replacing are sample tips, nebulizer tips, sample cones, skimmer cones, injector tubes, torches and lenses. It may also be necessary to change the oil in the interface roughing pump as well as the vacuum backing pump, depending on the workload put on the instrument.\nSample preparation.\nFor most clinical methods using ICP-MS, there is a relatively simple and quick sample prep process. The main component to the sample is an internal standard, which also serves as the diluent. This internal standard consists primarily of deionized water, with nitric or hydrochloric acid and indium and/or gallium. The addition of volatile acids allows for the sample to decompose into its gaseous components in the plasma which minimizes the ability for concentrated salts and solvent loads to clog the cones and contaminate the instrument. Depending on the sample type, usually 5 mL of the internal standard is added to a test tube along with 10\u2013500 microliters of sample. This mixture is then vortexed for several seconds or until mixed well and then loaded onto the autosampler tray. For other applications that may involve very viscous samples or samples that have particulate matter, a process known as sample digestion may have to be carried out before it can be pipetted and analyzed. This adds an extra first step to the above process and therefore makes the sample prep more lengthy.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49505", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=49505", "title": "1654 BC", "text": ""}
{"id": "49508", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=49508", "title": "Die Walk\u00fcre", "text": "1870 opera by Richard Wagner\n' (; The Valkyrie\"\"'), WWV 86B, is the second of the four epic music dramas that constitute Richard Wagner's cycle \"Der Ring des Nibelungen\" (English: \"The Ring of the Nibelung\"). It was performed, as a single opera, at the National Theatre Munich on 26 June 1870, and received its first performance as part of the \"Ring\" cycle at the Bayreuth Festspielhaus on 14 August 1876.\nAs the \"Ring\" cycle was conceived by Wagner in reverse order of performance, \"Die Walk\u00fcre\" was the third of the four texts to be written, although Wagner composed the music in performance sequence. The text was completed by July 1852, and the music by March 1856. \nWagner largely followed the principles related to the form of musical drama, which he had set out in his 1851 essay \"Opera and Drama\" under which the music would interpret the text emotionally, reflecting the feelings and moods behind the work, using a system of recurring leitmotifs to represent people, ideas, and situations rather than the conventional operatic units of arias, ensembles, and choruses. Wagner showed flexibility in the application of these principles here, particularly in Act III, when the Valkyries engage in frequent ensemble singing.\nAs with \"Das Rheingold\", Wagner wished to defer any performance of the new work until it could be shown in the context of the completed cycle, but the 1870 Munich premiere was arranged at the insistence of his patron, King Ludwig II of Bavaria. \"Die Walk\u00fcre\" has achieved some popularity as a stand-alone work and continues to be performed independently from its role in the tetralogy.\nThe story of \"Die Walk\u00fcre\" is based on the Norse mythology told in the \"V\u00f6lsunga saga\" and the \"Poetic Edda\". In this version, the Volsung twins Sieglinde and Siegmund, separated in childhood, meet and fall in love. This union angers the gods, who demand that Siegmund must die. Sieglinde and the couple's unborn child are saved by the defiant actions of Wotan's daughter, the title character, Valkyrie Br\u00fcnnhilde, who as a result faces the gods' retribution.\nBackground and context.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n Structure of the \"Ring\" cycle\nWagner began work on what became his \"Ring\" project in October 1848, when he prepared a prose outline for \"Siegfried's Death\", based on the legendary hero of Germanic myth. During the following months, he developed the outline into a full \"poem\" or libretto.\nAfter his travels to Switzerland in May 1849, Wagner continued to expand his project, having decided that a single work would not suffice for his purposes. He would, therefore, create a series of music dramas, each telling a stage of the story, basing the narrative on a combination of myth and imagination; \"Siegfried's Death\" would provide the culmination. In 1851, he outlined his purposes in his essay \"A Communication to My Friends\": \"I propose to produce my myth in three complete dramas, preceded by a lengthy Prelude (Vorspiel)\". Each of these dramas would, he said, constitute an independent whole, but would not be performed separately. \"At a specially appointed Festival, I propose, some future time, to produce those three Dramas with their Prelude, in the course of three days and a fore-evening\".\nIn accordance with this scheme, Wagner preceded \"Siegfried's Death\" (later \"G\u00f6tterd\u00e4mmerung\" (\"The Twilight of the Gods\") with the story of Siegfried's youth, \"Young Siegfried\", later renamed \"Siegfried\". This was, in turn, preceded by \"Die Walk\u00fcre\" (\"The Valkyrie\"), dealing with Siegfried's origins, the whole tetralogy being fronted by a prologue, \"Das Rheingold\". Because Wagner prepared his texts in reverse chronological sequence, \"Die Walk\u00fcre\" was the third of the dramas to be conceived and written, but appears second in the tetralogy.\nSynopsis.\nPrior history.\nDuring the lengthy time that has passed since the gods entered Valhalla at the end of \"Das Rheingold\", Fafner has used the Tarnhelm to assume the form of a dragon, and guards the gold and the ring in the depths of the forest. Wotan has visited Erda seeking wisdom, and by her has fathered a daughter, Br\u00fcnnhilde; he has fathered eight other daughters, possibly also by Erda. These, with Br\u00fcnnhilde, are the Valkyries, whose task is to recover heroes fallen in battle and bring them to Valhalla, where they will protect the fortress from Alberich's assault should the dwarf recover the ring. Wotan has also wandered the earth, and with a woman of the V\u00f6lsung race has fathered the twins Siegmund and Sieglinde, who have grown up separately and unaware of each other. From the V\u00f6lsungs Wotan hopes for a hero who, unencumbered by the gods' treaties, will obtain the ring from Fafner.\nAct 1.\nPrelude to Act I\n\"Scene 1\"\nAs a large storm rages, Siegmund finds shelter from his enemies in a large dwelling built around a massive ash tree. Unarmed and wounded, he collapses with exhaustion. Sieglinde enters; she gives Siegmund some water and some honeyed mead, and tells him that she is the wife of Hunding, and that he may rest there until Hunding's return. As they talk, they look at each other with growing interest and emotion. Siegmund gets ready to leave, telling Sieglinde that misfortune follows him, and he does not want to bring it on her; she replies that misfortune dwells with her already.\nOrchestral Interlude\n\"Scene 2\"\nHunding returns and questions Siegmund's presence. Calling himself Wehwalt (\"woeful\"), Siegmund explains that he grew up in the forest with his parents and twin sister. One day he found their home burned down, his mother killed and his sister gone. Recently he fought with the relatives of a girl being forced into marriage. His weapons were destroyed, the bride was killed, and he was forced to flee. Hunding reveals that he is kin to Siegmund's pursuers; Siegmund may stay, he says, but they must fight in the morning. Before leaving, Sieglinde gives a meaningful glance to a particular spot on the tree in which, the firelight reveals, a sword is buried to the hilt.\n\"Scene 3\"\nSieglinde returns, having drugged Hunding's drink. She reveals that she was forced into the marriage and that during their wedding feast, an old man appeared and plunged a sword into the trunk of the ash tree which neither Hunding nor any of his companions have been able to remove. She is longing for the hero who will draw the sword and save her. When Siegmund expresses his love for her, she reciprocates, and when he speaks the name of his father, W\u00e4lse, she recognises him as Siegmund, and realises that the sword was left for him. Siegmund then draws the sword from the tree. She reveals herself as Sieglinde, his twin sister. Siegmund names the sword \"Nothung\" and declares that it will be her protection. The two sing of their passionate love for each other, as the act ends.\nAct 2.\nPrelude to Act 2\n\"Scene 1\"\nOn a high mountain ridge, Wotan instructs Br\u00fcnnhilde, his Valkyrie daughter, to protect Siegmund in his forthcoming battle with Hunding. Fricka arrives, and in her role as goddess of family values demands that Siegmund and Sieglinde be punished for their adultery and incest. She scorns Wotan's argument that he requires Siegmund as a \"free hero\", who can further his plans to recover the ring from Fafner, uninhibited by Wotan's contracts. She retorts that Siegmund is not free but is Wotan's pawn, whose every move the god seeks to direct. Realising that Siegmund is indeed not the free hero he needs for his grand plan, Wotan reluctantly agrees that he will not protect his son. \n\"Scene 2\"\nAfter Fricka leaves, the troubled Wotan gives Br\u00fcnnhilde the full story, and with great sorrow rescinds his earlier instruction; he orders her to give the victory to Hunding and then departs.\nOrchestral Interlude\n\"Scene 3\"\nSieglinde, who has suffered a panic attack during the night, is running away from Siegmund, who is pursuing her in alarm. He catches up with her, and she reveals that she regards herself as unworthy of her brother's love due to her forced marriage with Hunding. Siegmund tries in vain to comfort her. With a vision of Hunding's dogs pulling Siegmund down by his feet, Sieglinde faints, consumed with guilt and exhaustion. \nOrchestral Interlude\n\"Scene 4\"\nBr\u00fcnnhilde suddenly appears to Siegmund and tells him of his impending death; he refuses to follow Br\u00fcnnhilde to Valhalla when she tells him Sieglinde cannot accompany him. Siegmund still believes that his father's sword will assure him of victory over Hunding, but Br\u00fcnnhilde tells him it has lost its power. Siegmund threatens to kill Sieglinde rather than abandon her, friendless, after his death. Br\u00fcnnhilde attempts to dissuade him by revealing that Sieglinde is pregnant with their child, but Siegmund simply prepares to kill both Sieglinde and the unborn child. Much moved, Br\u00fcnnhilde decides to defy her father and grant victory to Siegmund.\n\"Scene 5\"\nHunding's horn is heard; he arrives and attacks Siegmund. Under Br\u00fcnnhilde's power, Siegmund begins to overpower Hunding, but Wotan appears and shatters Siegmund's sword with his spear. Hunding stabs Siegmund to death. Br\u00fcnnhilde gathers up the fragments of the sword and flees on horseback with Sieglinde. Contemptuously, Wotan kills Hunding with a simple wave of hand, and sets out in pursuit of Br\u00fcnnhilde, vowing to punish her harshly for her disobedience.\nAct 3.\nPrelude to Act 3 - \"Walk\u00fcrenritt\" (\"The Ride of the Valkyries\")\n\"Scene 1\"\nThe Valkyries congregate on the mountain-top, each carrying a dead hero and chattering excitedly. Br\u00fcnnhilde arrives with Sieglinde, and begs her sisters for help, but they dare not defy Wotan. Sieglinde tells Br\u00fcnnhilde that without Siegmund she no longer wishes to live. Br\u00fcnnhilde tells Sieglinde that she is pregnant by Siegmund, and urges her to remain alive for her child's sake, and to name the child Siegfried. Br\u00fcnnhilde gives the fragments of the sword Nothung to Sieglinde, who thanks her for her loyalty and comfort, and resolves to save the child. As she departs, Wotan is heard approaching with great wrath.\n\"Scene 2\"\nWhen Wotan arrives, the Valkyries vainly try to hide Br\u00fcnnhilde. He faces her and declares her punishment: she is to be stripped of her Valkyrie status and become a mortal woman, to be held in defenceless sleep on the mountain, prey to any man who finds her. The other Valkyries protest, but when Wotan threatens them with the same, they flee. \nOrchestral Interlude\n\"Scene 3\"\nIn a long discourse with Wotan, Br\u00fcnnhilde explains that she decided to protect Sieglinde knowing that this was Wotan's true desire. Wotan consents to her request that he surround her resting place with a circle of fire that will protect her from all but the bravest of heroes. He bids her a loving farewell and lays her sleeping form down on a rock. He then invokes Loge, the demigod of fire, and creates a circle of perpetual fire around her. Before slowly departing, Wotan pronounces that anyone who fears his spear shall never pass through the fire.\nWriting history.\nText, sources, characters.\nWagner's original title for the work was \"Siegfried und Sieglind: der Walk\u00fcre Bestrafung\" (\"Siegfried and Sieglinde: The Valkyrie Punished\"), but he quickly simplified this to \"Die Walk\u00fcre\". Prose sketches for the first two acts were prepared in November 1851, and for the third act early in the following year. These sketches were expanded to a more detailed prose plan in May 1852, and the full libretto was written in June 1852. It was privately printed, with the other \"Ring\" libretti, in February 1853.\nWagner constructed his \"Die Walk\u00fcre\" libretto from a range of ancient Norse and Germanic sources, principally the V\u00f6lsunga saga, the Poetic Edda, the Prose Edda, the Nibelungenlied and other fragments of Teutonic literature. From this plethora of material he selected particular elements and transformed them, to create his own narrative through the compression of events, the rearrangement of chronology and the fusion of characters. For example, in the V\u00f6lsunga saga Siegmund is not Wotan's son, although he arranges the latter's conception by a V\u00f6lsung woman. Sigurd (Siegfried) is not the child of Siegmund's incestuous marriage to his sister, but of a later wife who preserves the sword fragments. Likewise, in the sagas Sieglinde is a somewhat different character, Signy; she is Siegmund's twin sister, but the son she bears him is not Siegfried, and the manner of her death is quite different from that depicted by Wagner. Hunding is a conflation of several characters in the sagas, notably Siggeir who is wedded to Signy, and the villainous King Hunding who is Siegmund's mortal enemy in the Poetic Edda.\nWotan (Odin) appears in the northern sagas as the god of all life as well as of battles, although he is by no means omnipotent. Fricka (Frigg) has most of the hallmarks of her counterpart in the Poetic and Prose Eddas, as wife of Wotan and goddess of family values. Br\u00fcnnhilde is a less central figure in the sagas than she is in the \"Ring\" cycle. In an early lay, she is sought as a wife by Gunther, who seeks the help of Siegfried in overcoming her superhuman strength. Certain aspects of her \"Ring\" character appear in the Eddas and the Nibelungenlied, such as her encirclement by Wotan in a ring of fire, and her rescue by a hero without fear.\nThe Valkyries have a basis in historical fact, within the primitive Teutonic war-cult. According to Cooke, originally they were \"grisly old women who officiated at the sacrificial rites when prisoners were put to death.\" They became entwined in legend: in the Poetic Edda they emerge as supernatural warrior maidens carrying out Odin's orders as to who should die. In the Poetic Edda the Valkyries are given names: Skuld, Skogul, Gunn, Hild, Gondul and Geirskogul. Some of these names differ in other sources. The names that Wagner gave to his Valkyries were his own invention, apart from Br\u00fcnnhilde and Siegrune.\nWagner effected a number of changes between his original draft and the final text. For example, in the first sketch, Wotan appeared in person in Act I to drive the sword into the tree. Siegmund withdrew the sword much earlier in the act, and in Act II Hunding was not slain by Wotan, but left alive to follow Wotan's instruction: \"Get hence, slave! Bow before Fricka.\"\nComposition.\nApart from some rough sketches, including an early version of what became Siegmund's \"Spring Song\" in Act I of \"Die Walk\u00fcre\", Wagner composed the \"Ring\" music in its proper sequence. Having completed the music for \"Das Rheingold\" in May 1854, he began composing \"Die Walk\u00fcre\" in June, and finished the full orchestral score nearly two years later, in March 1856. This extended period is explained by several concurrent events and distractions, including Wagner's burgeoning friendship with Mathilde Wesendonck, and a lengthy concert tour in London at the invitation of the Royal Philharmonic Society, when he conducted a full season amid some controversy, although his own \"Tannh\u00e4user\" overture was well received.\nThe system of leitmotifs, integral to the \"Opera and Drama\" principles, is used to the full in \"Die Walk\u00fcre\"; Holman numbers 36 such motifs that are introduced in the work. The well-known \"Valkyrie\" motif, used to introduce Br\u00fcnnhilde in Act II, forms the basis of the famous Ride of the Valkyries that opens Act III. Wagner wrote the concert version of the Ride in 1862, for performance in concerts at Vienna and Leipzig.\nPerformances.\nPremiere, Munich, 26 June 1870.\nAs with \"Das Rheingold\", many years elapsed between the completion of the \"Die Walk\u00fcre\" score and its first performance. Seeing little chance of the \"Ring\" project coming to any immediate fruition, and in need of money, in August 1857 Wagner abandoned work on it and concentrated instead on \"Tristan und Isolde\" and \"Die Meistersinger von N\u00fcrnberg\", and on a revised version of \"Tannh\u00e4user\". However, King Ludwig of Bavaria, to whom Wagner had sold the copyrights to the \"Ring\" works, was insistent that the two completed \"Ring\" operas be staged, and over Wagner's bitter protests arranged for them to be performed at the Munich Hofoper, \"Das Rheingold\" on 22 September 1869 and \"Die Walk\u00fcre\" on 26 June 1870. The core of Wagner's objection to a Munich performance was his relationship with the married Cosima von B\u00fclow, with whom he was cohabiting in Switzerland; he could not return to Munich without provoking scandal, and therefore could not directly control the performances.\nAs the date for the \"Die Walk\u00fcre\" premiere approached, Wagner grew more distressed and sullen; a letter from the critic Franz M\u00fcller reporting that everything was going well did nothing to console him. Cosima wrote in her diary that his distress \"pierces my heart like a dagger, and I ask myself whether this disgraceful act will really go unavenged?\". The premiere was attended by leading figures from the musical world, including Liszt, Brahms, Camille Saint-Sa\u00ebns, and the violinist Joseph Joachim. The reception from audience and critics was much more positive than had been the case a year earlier with \"Rheingold\", although Osborne quotes one dissenting voice, from the critic of the \"S\u00fcddeutsche Presse\". Having described the first act as \"for the most part, drearily long-winded\", this critic thought that the second act dragged and came to life only occasionally. He went on: \"The third act begins so deafeningly that total stupor would be ensured even if the rest were less long-winded ... The overall effect of the music is not agreeable...\". Cosima kept all communications from Munich away from Wagner, and tore up the more critical newspaper reviews.\nAfter a second performance, \"Die Walk\u00fcre\" was performed at the Hofoper three further times, alternating with \"Das Rheingold\" in a \"Ring\" semi-cycle. King Ludwig, who was absent from the premiere, attended one of the later performances. The Munich festival had taken place amid a mounting war fever, as relations between France and the German states rapidly deteriorated; the Franco-Prussian War broke out on 19 July.\nBayreuth premiere, 14 August 1876.\nIn May 1872, Wagner laid the foundation stone for the Festspielhaus in Bayreuth, Northern Bavaria. He had originally envisaged holding the first Bayreuth Festival there in 1873, but delays in the building work, and in completing the \"Ring\" music, led to multiple postponements. Finally, the festival was scheduled for August 1876; \"Die Walk\u00fcre\" would be performed on 14 August, the second day of the festival. \nWagner was involved in every stage of the preparations; according to Ernest Newman's biography he was \"a far better conductor than any of his conductors, a far better actor than any of his actors, a far better singer than any of his singers in everything but tone\". Heinrich Porges, a contemporary chronicler, describes Wagner demonstrating to Amalie Materna, as Br\u00fcnnhilde, how to sing the scene in which she tells Sieglinde of the impending birth of Siegfried: \"He sang [the final words] with truly thrilling force\". The singer Lilli Lehmann (Helmwige), in her 1913 memoirs, remembered Wagner acting the role of Sieglinde in rehearsals: \"Never yet has a Sieglinde known how to approach him, even approximately\".\nThe \"Die Walk\u00fcre\" performance on 14 August was free from the mechanical problems that had affected \"Das Rheingold\" the day before, and was generally well received by the distinguished audience that included the Kaiser Wilhelm I, Emperor Pedro II of Brazil, representatives from various European royal houses and many of Europe's leading composers. Wagner, however, was far from pleased. He was unnerved by an incident involving the kaiser, when the 79-year-old Wilhelm stumbled and almost fell over a doorstep, and was very critical of two of his main singers, Niemann and Betz, whom he deemed \"theatre parasites\" and said he would never employ again \u2013 a view he later revised. Among the scenes that he felt had not come off were those on the mountain-top: \"I'll change that some day when I produce \"Walk\u00fcre\" in heaven, at the right hand of God, and the old gent and I are watching it together\".\nThree \"Ring\" cycles were performed in the first Bayreuth Festival. The stage designs used in \"Die Walk\u00fcre\", and the other operas, were based on sketches by Josef Hoffman which were converted to stage sets by the Br\u00fcckner brothers from the Coburg State Opera. These designs, and Carl D\u00f6pler's costumes, influenced productions well into the 20th century.\nRevivals.\nAfter the 1876 festival, \"Die Walk\u00fcre\" was not seen again at Bayreuth for 20 years, until Cosima revived the \"Ring\" cycle for the 1896 festival. It was quickly taken up by other opera houses: Vienna and New York in 1877, Rotterdam in 1878 and London in 1882. The New York performance, on 2 April 1877, was conducted by Adolf Neuendorff as part of a Wagner festival organised by the Academy of Music; it preceded the Metropolitan Opera premiere by nearly eight years.\nThe London performance, at Her Majesty's Theatre on 6 May 1882, was the first \"Ring\" cycle to be performed anywhere, after the 1876 Bayreuth premiere. It was conducted by Anton Seidl, who had been an assistant to Wagner at Bayreuth, and had Albert Niemann in his Bayreuth role as Siegmund. \"The Musical Times\", in a long review, mixed approval with criticism: its reviewer noted a number of empty seats in the auditorium, thought the stage sets compared unfavourably with those at Bayreuth, and deemed the orchestra \"inefficient\". However, the critic praised individual performances, and judged that the music and the drama had held the attention of an audience whose enthusiasm far exceeded that shown to \"Das Rheingold\" the previous day. The correspondent for \"The Era\" newspaper was distressed by the incestuous nature of the story, which it described as \"brutal and degrading\", despite the quality of the music: \"A composer must have lost all sense of decency and all respect for the dignity of human nature who could thus employ his genius and skill\".\nThe New York Met performance, on 30 January 1885, was part of a Wagner festival conducted by Leopold Damrosch \u2013 no other \"Ring\" operas were staged. Amalie Materna, Bayreuth's original Br\u00fcnnhilde, reprised the part here. The stage designer, Wilhelm Hock, recreated the original Bayreuth designs. The performance was received with great enthusiasm by the audience, who demanded numerous curtain calls. Damrosch fell ill just before the festival ended, and died on 15 February 1885.\nDuring the 1880s and 1890s, \"Die Walk\u00fcre\" was shown in many European cities, sometimes as part of a \"Ring\" cycle but often as an independent work: Brussels, Venice, Strasbourg and Budapest in 1883, Prague in 1885, St Petersburg in 1889, Copenhagen in 1891 and Stockholm in 1895. By then it was travelling worldwide: to Mexico in 1891, Argentina in 1899, Australia in 1907, South Africa in 1912 and Brazil in 1913. These productions often followed the Bayreuth line of staying close to the staging and costumes that Wagner had approved for the original festival, although there were some deviations, such as Adolphe Appia's productions which replaced pictorialism with stylised sets using colours and lights.\nAt Bayreuth, no significant changes in presentation occurred until after the Second World War, when Wieland Wagner revealed his \"New Bayreuth\" style of largely bare stages and unadorned costumes. Although traditional productions continued outside Bayreuth, many performances followed the new trend. From the 1970s onwards there was increasing innovation; the Leipzig \"Die Walk\u00fcre\" began the trend of placing the opening scene of Act II within Valhalla rather than on a mountain-top; Harry Kupfer at Bayreuth in 1988 set the whole cycle in a post-nuclear dystopia; J\u00fcrgen Flimm's Bayreuth 2000 production had Wotan as a corrupt businessman, sitting in an office surrounded by 21st-century paraphernalia, including a paper-shredder used to cover his tracks. A few prominent productions adhered to traditional staging, including Otto Schenk's at the New York Met in 1989, which was described by the \"New York Times\" as \"charmingly old fashioned\", and as \"a relief to many beleaguered Wagnerites\".\nAccording to Charles Osborne, \"Die Walk\u00fcre\" is sufficiently self-contained to be quite frequently staged on its own. The 2018 Bayreuth Festival broke new ground when for the first time it scheduled \"Die Walk\u00fcre\" as a stand-alone, outside the context of the \"Ring\" cycle.\nMusic.\nOsborne writes that, like \"Das Rheingold\", \"Die Walk\u00fcre\" is primarily a work for solo voices, but with better integration of the vocal parts into the overall musical structure. As with its predecessor, Wagner composed \"Die Walk\u00fcre\" under the principles he had defined in his book-length 1851 essay \"Opera and Drama\", eschewing the traditional operatic norms of chorus, arias and vocal \"numbers\". There is, however, some division of opinion as the extent that these principles were fully observed. The critic Barry Millington opines that of all Wagner's works, \"Die Walk\u00fcre\" is the fullest embodiment of the \"Opera and Drama\" precepts, achieving a complete synthesis of music and poetry. This, he says, is achieved without any notable sacrifice in musical expression\". In his analytical essay \"The Perfect Wagnerite\", Bernard Shaw praises the synthesis of music and drama: \"There is not ... a note in it that has any other point than the single direct point of giving musical expression to the drama\". Gutman's view, however, is that this only applies to the first two acts; the \"apogee\" of this style, he says, is found in the later opera \"Tristan and Isolde\". Roger Scruton refers to deviations in \"Die Walk\u00fcre\" such as the \"Spring Song\" (\"Winterst\u00fcrme\"), in which Siegmund holds up the action to declare his love for Sieglinde in what is to all intents and purposes an aria, while Osborne notes the \"impressive ensembles\" in Act III, as the Valkyries sing together.\nAct I.\nThe act opens in the key of D minor, which frames the music until Siegmund's death in Act II. The short prelude depicts a storm; a stamping rhythm in the basses rises to a climax in which \"Donner's Call\" from \"Das Rheingold\" is heard. As the scene proceeds, several new motifs are introduced; that representing Siegmund, derived from the \"Spear\" motif from \"Das Rheingold\"; that for Sieglinde, a gentle melody on strings, which Holman says \"conveys at once Sieglinde's inner beauty and misfortune\"; and the motif which Newman names \"The Dawning of Love\", which will recur in the final love duet of the act. These three motifs, and their developed variants, are prominent throughout the act. Among other motifs, the aggressive brass staccato that identifies Hunding is a particularly striking phrase, \"as dark and dour as the man himself\".\nWagner uses other \"Rheingold\" motifs to deliver key information. Thus, the parentage of Siegmund and Sieglinde is revealed to the audience as Wotan, when the Valhalla music plays softly on trombones. The same theme references Wotan again, when Sieglinde recounts the visit of the old man at her wedding. A repeated falling octave in G\u266d, extracted from the \"Sword\" motif, illustrates Siegmund's desperate desire for the sword, and recurs at the end of the act, together with the full \"Sword\" motif in triumphant brass, as he draws the sword from the tree.\nAct II.\nThe second act opens exuberantly, with a short prelude that prefigures the celebrated Valkyrie motif that in the following act will form the basis of the \"Ride of the Valkyries\" in Act III. This motif was first sketched in 1851, for intended use in \"Siegfried's Tot\", before the full plan of the \"Ring\" cycle was developed. The first scene of the act introduces Br\u00fcnnhilde's energetic \"Hojotoho!\", as she answers Wotan's summons, expressing what Holman describes as her \"manly enthusiasm\" for her role as warrior maiden. The Wotan-Fricka dialogue that follows is illustrated by motifs that express Fricka's sour disillusion with her marriage, and Wotan's bitterness and frustration as he is unable to answer his wife's forceful arguments.\nIn the colloquies between Wotan and Br\u00fcnnhilde, several soundings of the \"Woman's Worth\" motif are heard. The \"Annunciation of Death\" motif is the crucial point, where the two narratives (Wotan/Br\u00fcnnhilde and Siegmund/Sieglinde) come together. Wagner chose the tonality of F\u266f minor for this scene, eventually modulating to B minor in preparation for the Valkyries' entry in Act III.\nAct III.\nThe act opens with the famous sequence known as the \"Ride of the Valkyries\", formed by combining the Valkyries' own belligerent theme with Br\u00fcnnhilde's war cry from act II. The Ride has achieved lasting popularity as an orchestral concert piece outside the framework of the \"Ring\" cycle; according to Newman, in the orchestral version and sometimes within the opera itself, the basis staccato phrasing is corrupted by eliding the second and third notes and emphasising the fourth rather than the first as originally written.\nAt the midpoint of the act, prior to Wotan's vengeful entrance and the long denouement with Br\u00fcnnhilde, we hear the \"Reconciliation\" motif (\"Redemption by Love\" per Newman) in which Sieglinde praises Br\u00fcnnhilde for her rescue: \"O highest of wonders! Noblest of maids!\". The motif will next be heard at the very end of the \"Ring\" cycle, bringing the entire tetralogy to a close on a note of reconciliation and optimism.\nThe final section of the act is marked by what Millington describes as \"a succession of carefully controlled climaxes\", of which the most affecting is that of Wotan's farewell to his errant daughter. The music is eventually dominated by the five falling notes of Br\u00fcnnhilde's \"Sleep\" motif which, when Wotan has used his spear to summon Loge, is transformed into the \"Magic Fire\" music as Br\u00fcnnhilde is encircled in the ring of fire, and Wotan sadly departs.\nInstrumentation.\n\"Die Walk\u00fcre\" is scored for the following instrumental forces:\nCritical assessment.\nThe first Munich performances of \"Die Walk\u00fcre\" were generally hailed as successes by audiences and critics; leading composers who were present greeted the work with acclaim, recognising in it evidence of Wagner's genius. One dissident voice was presented by the critic of the \"S\u00fcddeutsche Presse\", who was scathing about the dearth of moral standards expressed in the story and furthermore found the whole experience tedious: the first act was, for the most part, \"wearyingly long-winded\"; the second act only occasionally sprang to life, while in the third it was \"barely possible to hear isolated shrieks from the singers through the tumult of the orchestra\". The overall effect was \"not agreeable ... permeated with what one can only call pagan sensuality, and ... produces finally nothing but an enervating dullness\". This harsh, if isolated judgement, found some echo six years later, when \"Die Walk\u00fcre\" was first performed at Bayreuth as part of the \"Ring\" cycle. Critics could now form relative views on the merits of the four operas. Although there was general admiration for the first act, \"Die Walk\u00fcre\" emerged as the least-liked of the four, in particular on account of the second act, deemed \"a great failure\" and an \"abyss of boredom\".\nMany modern critics of \"Die Walk\u00fcre\" have recorded much more positive opinions. To Charles Osborne it is \"marvellously rich ... Wagner has found a way to integrate his voice parts into the overall structure without sacrificing their lyrical independence\". It is, he says, the opera that stands up most strongly outside the tetralogy, and is popular enough to be staged frequently on its own, even within Bayreuth festivals.\nWriting in 2006, Millington thought that, notwithstanding the liberal use of ensembles in the third act, \"Die Walk\u00fcre\" showed the greatest fidelity of the four operas to the theoretical principles expressed by Wagner in \"Opera and Drama\": \"A thoroughgoing synthesis of poetry and music is achieved without any notable sacrifice in musical expression\". The modern view is that, of the \"Ring\" operas, \"Die Walk\u00fcre\" is both the most approachable and the one that can most successfully be performed in extracts.\nNotes and references.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49510", "revid": "12331483", "url": "https://en.wikipedia.org/wiki?curid=49510", "title": "Standard normal distribution", "text": ""}
{"id": "49511", "revid": "10239913", "url": "https://en.wikipedia.org/wiki?curid=49511", "title": "Siegfried (opera)", "text": "1876 opera by Richard Wagner\nSiegfried (), WWV 86C, is the third of the four epic music dramas that constitute Richard Wagner's cycle \"Der Ring des Nibelungen\" (English: \"The Ring of the Nibelung\"). It premiered at the Bayreuth Festspielhaus on 16 August 1876, as part of the first complete performance of \"The Ring\" cycle.\nThe autograph manuscript of the work is preserved in the Richard Wagner Foundation.\nBackground and context.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n Structure of the \"Ring\" cycle\nThe libretto of \"Siegfried\" was drafted by Wagner in November\u2013December 1852, based on an earlier version he had prepared in May\u2013June 1851 and originally entitled \"Jung-Siegfried\" (\"Young Siegfried\"), later changed to \"Der junge Siegfried\". The musical composition was commenced in 1856, but not finally completed until 1871.\nThe libretto arose from Wagner's gradual reconception of the project he had initiated with his libretto \"Siegfrieds Tod\" (\"Siegfried's Death\") which was eventually to be incarnated as \"G\u00f6tterd\u00e4mmerung\", the final section of the Ring cycle. Having sketched music and worked with the text for \"Siegfrieds Tod\" in 1851, he realized that it would need a 'preface'. At this point he conceived that the prefatory opera, \"Der junge Siegfried\", could act as a comic foil to the tragedy of \"Siegfrieds Tod\". Preliminary musical sketches for \"Der junge Siegfried\" in 1851 were however quickly abandoned, although Wagner had written to his friend Theodor Uhlig that \"the musical phrases are making themselves for these stanzas and periods, without my even having to take pains for them. It's all growing out of the ground as if it were wild.\" Shortly afterwards he wrote to Uhlig that he was now planning to tell the Siegfried story in the form of \"three dramas, plus a prologue in three acts\"\u2014a clear prefiguring of the Ring cycle.\nFull work was finally commenced on the music of \"Siegfried\", as the composer henceforth referred to it, in 1856, when Wagner prepared concurrently two drafts, a complete draft in pencil and a version in ink on up to three staves in which he worked out details of instrumentation and vocal line. The composition of Acts I and II was completed by August 1857. Wagner then left off work on \"Siegfried\" to write the operas \"Tristan und Isolde\" and \"Die Meistersinger.\" He did not resume work on \"Siegfried\" until 1869, when he composed the third act. The final revision of the score was undertaken in February 1871. Performance was withheld until the first complete production of the Ring cycle, at Bayreuth in August 1876.\nSynopsis.\nAct 1.\nPrelude to Act 1\n\"Scene 1\"\nA cave in rocks in the forest. An orchestral introduction includes references to leitmotifs including themes relating to the original hoard plundered by the Nibelung Alberich, and one in B-flat minor associated with the Nibelungs themselves. As the curtain rises, Alberich's brother, the dwarf Mime, is forging a sword (\"Zwangvolle Plage!\"). Mime is plotting to obtain the ring of power originally created by his brother Alberich. He has raised the human boy Siegfried as a foster child, to kill Fafner, who obtained the ring and other treasures in the opera \"Das Rheingold\" and has since transformed himself from a giant to a dragon. Mime needs a sword for Siegfried to use, but the youth has contemptuously broken every sword Mime has made. Siegfried returns from his wanderings in the forest driving before him a large bear that terrifies Mime, and immediately breaks the new sword. After a whining speech by Mime about ingratitude, and how Mime has brought him up from a mewling infant (\"Als zullendes Kind\"), Siegfried senses why he keeps coming back to Mime although he despises him: he wants to know his parentage. Mime is forced to explain that he encountered Siegfried's mother, Sieglinde, when she was in labor; she died giving birth to Siegfried. He shows Siegfried the broken pieces of the sword Nothung, which she had left in his custody. Siegfried orders him to reforge the sword; however, Mime is unable to accomplish this. Siegfried departs, leaving Mime in despair (\"Aus dem Wald fort in die Welt zieh\u2019n\").\n\"Scene 2\"\nAn old man (Wotan in disguise) arrives at the door and introduces himself as the Wanderer (\"Heil dir, weiser Schmied!\"). In return for the hospitality due a guest, he wagers his head on answering any three questions Mime may ask. The dwarf asks the Wanderer to name the races that live beneath the ground, on the earth, and in the skies. These are the Nibelungs, the Giants, and the Gods, as the Wanderer answers correctly. When Mime still refuses hospitality, the Wanderer demands that Mime answer \"his\" three questions, or yield his own head. The Wanderer asks Mime to identify: the race most beloved of Wotan, but most harshly treated; the name of the sword that can destroy Fafner; and the person who can repair the sword. Mime can answer only the first two questions: the W\u00e4lsungs (Siegmund and Sieglinde whose tale is told in the opera \"Die Walk\u00fcre\") and the sword Nothung. Wotan tells him that only \"he who does not know fear\" can reforge Nothung, and abstains from taking Mime's head, leaving it for that person.\n\"Scene 3\"\nMime despairs as he imagines the ferocity of the dragon Fafner, while \"the orchestra paints a dazzling picture of flickering lights and roaring flames\" (\"Verfluchtes Licht!\"). Siegfried returns and is annoyed by Mime's lack of progress. Mime realizes that Siegfried is \"the one who does not know fear\" and that unless he can instill fear in him, Siegfried will kill him as the Wanderer foretold. He tells Siegfried that fear is an essential craft; Siegfried is eager to learn it, and Mime promises to teach him by taking him to Fafner (\"F\u00fchltest du nie im finst\u2019ren Wald\"). Since Mime was unable to forge Nothung, Siegfried decides to do it himself (\"Nothung! Nothung! Neidliches Schwert!\"). He succeeds by shredding the metal, melting it, and casting it anew. In the meantime, Mime brews a poisoned drink to offer Siegfried after the youth has defeated the dragon. After he finishes forging the sword, Siegfried demonstrates its strength by chopping the anvil in half with it (\"Hoho! Hoho! Hohei! Schmiede, mein Hammer, ein hartes Schwert!\" - \"Siegfrieds Schmiedelied\" - \"Siegfried's Forging Song\").\nAct 2.\nPrelude to Act 2 - \"Fafners Ruhe\" (\"Fafner's Rest\")\n\"Scene 1\"\nDeep in the forest. The Wanderer arrives at the entrance to Fafner's cave, near which Alberich secretly keeps vigil by a rocky cliff (\"Im Wald und Nacht\"). The two enemies recognize each other. Alberich boasts of his plans to regain the ring and rule the world. Wotan states that he does not intend to interfere, only to observe. He even offers to awaken the dragon so that Alberich can bargain with him. Alberich warns the dragon that a hero is coming to kill him, and offers to prevent the fight in exchange for the ring. Fafner dismisses the threat, declines Alberich's offer, and returns to sleep. Wotan mysteriously advises Alberich that all things follow their own necessary ways which no one will change. He then rides away on his horse, leaving Alberich alone. Alberich withdraws and hides himself again in the rocks.\nOrchestral Interlude\n\"Scene 2\"\nAt daybreak, Siegfried and Mime arrive. After assuring Siegfried that the dragon will teach him what fear is, Mime withdraws. As Siegfried waits for the dragon to appear, he hears a woodbird singing from the trees (\"Waldweben\" - \"Forest Murmurs\"). He attempts to mimic the bird's song using a reed pipe, but is unsuccessful. He then plays a tune on his horn (\"Siegfrieds Hornruf\" - \"Siegfried's Horn Call\"), which unintentionally wakes Fafner in his cave. After a short exchange, they fight; Siegfried stabs Fafner in the heart with Nothung. Regretful about his own life Fafner in his last moments learns the boy's name and tells Siegfried to beware the might of the curse, which condemns every lord of the Ring to death, just as it has now brought death to him. When Siegfried withdraws his sword from Fafner's body, his hands are burned by the dragon's hot blood and he puts his finger in his mouth. On tasting the blood, he finds that he can understand the woodbird's song (\"Hei! Siegfried geh\u00f6rt nun der Niblungen Hort!\"). Following its instructions, he takes the ring and the magic helmet Tarnhelm from Fafner's hoard.\n\"Scene 3\"\nOutside the cave, Alberich and Mime meet and quarrel over the treasure (\"Wohin schleichst du eilig und schlau\"). Alberich hides as Siegfried comes out of the cave. Siegfried contemplates the ring but doesn't know what could be its use, viewing it just innocently as a valueless object of nature (\"Was ihr mir n\u00fctzt, wei\u00df ich nicht\"); nevertheless, on the forest bird's advice he decides to keep it. Then he complains to Mime that not even the dragon Fafner has taught him the meaning of fear. Mime congratulates him on having won his battle, and offers him the poisoned drink; however, the magic power of the dragon's blood allows Siegfried to read Mime's treacherous thoughts, and he stabs him to death (\"Willkommen, Siegfried!\"). Hidden nearby, Alberich is heard laughing spitefully at his brother's death. Siegfried puts Mime's body into the treasure cave and places Fafner's body in the cave entrance to block it. The woodbird now sings of a woman sleeping on a rock surrounded by magic fire (\"Nun sing! Ich lausche dem Gesang\"). Siegfried, wondering if he can learn fear from this woman, follows the bird towards the rock.\nAct 3.\nPrelude to Act 3\n\"Scene 1\"\nAt the foot of Br\u00fcnnhilde's rock. The Wanderer summons Erda, the earth goddess (\"Wache, Wala!\"). Erda, appearing confused, is unable to offer any advice (\"M\u00e4nnerthaten umd\u00e4mmern mir den Muth\"). Wotan informs her that he no longer fears the end of the gods; indeed, he wills it, because he now recognizes that through his own demise the true heritage of his life will be left to the independent pair free from envy in their love, to Siegfried the W\u00e4lsung, who took Alberich's ring without succumbing to its corrupting influence thanks to his fearlessness, and Br\u00fcnnhilde (Erda's and Wotan's child), who will work the deed that redeems the World (\"Dir Unweisen ruf\u2019 ich\u2019s in\u2019s Ohr\"). Dismissed, Erda sinks back into the earth.\n\"Scene 2\"\nSiegfried arrives, and the Wanderer questions the youth (\"Mein V\u00f6glein schwebte mir fort\"). Siegfried, who does not recognize his grandfather, answers insolently and starts down the path toward Br\u00fcnnhilde's rock. The Wanderer blocks his path, but Siegfried mocks him, laughing at his floppy hat and his missing eye, and breaks his spear (the symbol and source of Wotan's authority and power) with a blow from Nothung. Wotan, accepting his fate, calmly gathers up the pieces and vanishes (\"Zieh\u2019 hin! Ich kann dich nicht halten!\").\nOrchestral Interlude\n\"Scene 3\"\nThanks to his fearlessness Siegfried passes through the ring of fire, emerging on Br\u00fcnnhilde's rock (\"Selige \u00d6de auf sonniger H\u00f6h\u2019!\"). At first, he thinks the sleeping armored figure is a man. However, when he removes the armor, he finds a woman beneath. At the sight of the first woman he has ever seen and struck with the feeling of love, Siegfried at last experiences fear. In desperation, he kisses Br\u00fcnnhilde, waking her from her magic sleep. Upon waking she hails the Sun and greets Siegfried as the World's Light (\"Heil dir, Sonne! Heil dir, Licht!\"). Afraid and hesitant at first to do so, Br\u00fcnnhilde is eventually won over by Siegfried's love and renounces through her love for him the world of the gods and with it her own powers (\"Ewig war ich, ewig bin ich\"). Together, they solemnly sing praises to love, in comparison with which the glory of the gods itself seems dimmed and in which even dying can be jubilant: \"radiant love, laughing death!\" (\"Leuchtende Liebe, lachender Tod!\")\nSources.\nElements of the plot of \"Siegfried\" come from a variety of sources.\nIn a letter to Uhlig, Wagner recounted \"The Story of the Youth Who Went Forth to Learn What Fear Was\", based on a fairy-tale of the Brothers Grimm. It concerns a boy so stupid he had never learned to be afraid. Wagner wrote that the boy and Siegfried are the same character. The boy is taught to fear by his wife, and Siegfried learns it when he discovers the sleeping Br\u00fcnnhilde.\nSiegfried's ability in Act Two to see through Mime's deceitful words seems to have been derived from a 19th-century street theatre version of the story of Faust.\nSome elements of the story are derived from legends of Sigurd, notably the \"V\u00f6lsunga saga\" and the \"Thidrekssaga\". Scene 1 of Act III (between The Wanderer and Erda) has a parallel in the Eddic poem \"Baldrs draumar\", in which Odin questions a v\u00f6lva about the future of the gods.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49513", "revid": "2308770", "url": "https://en.wikipedia.org/wiki?curid=49513", "title": "Bernhard von Reesen", "text": "Bernhard von Reesen (1491\u00a0\u2013\u00a01521) was a successful merchant born to a patrician family in the Hanseatic city of Danzig (Gda\u0144sk). The Reesen name, with its prefix \"von\" (of), indicates that the family had its origins in the city of Rees.\nHe was the son of Bernd von Reesen (c. 1460\u00a0\u2013\u00a01506) and Brigitte Proite (d. 1506) who were married in 1489. His younger brother Heinrich (1497\u00a0\u2013\u00a01532) was also a merchant; his eldest sister, Margarethe (b. 1490), married, in 1516, Hans (Johann) von Schwarzwald (1468\u00a0\u2013\u00a01521), an alderman of Danzig.\nReesen was highly educated as was the custom for men of his background at that time. The wealthy businessman was evidently one of a circle of merchants with whom Albrecht D\u00fcrer of Nuremberg and his wife were in regular contact while the artist was in Antwerp. In early 1521, when Reesen was about thirty, D\u00fcrer painted his portrait. The painting can be seen in the permanent exhibition of Old Masters in Dresden, Germany.\nHe died in October 1521, \"tempore pestis\" according to the chronicler, Stenzel Bornbach. The day, place and cause of his death are unknown.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "49514", "revid": "39721018", "url": "https://en.wikipedia.org/wiki?curid=49514", "title": "Henri Lebesgue", "text": "French mathematician (1975\u20131941)\nHenri L\u00e9on Lebesgue (; ; June 28, 1875 \u2013 July 26, 1941) was a French mathematician known for his theory of integration, which was a generalization of the 17th-century concept of integration\u2014summing the area between an axis and the curve of a function defined for that axis. His theory was published originally in his dissertation \"Int\u00e9grale, longueur, aire\" (\"Integral, length, area\") at the University of Nancy during 1902.\nPersonal life.\nHenri Lebesgue was born on 28 June 1875 in Beauvais, Oise. Lebesgue's father was a typesetter and his mother was a school teacher. His parents assembled at home a library that the young Henri was able to use. His father died of tuberculosis when Lebesgue was still very young and his mother had to support him by herself. As he showed a remarkable talent for mathematics in primary school, one of his instructors arranged for community support to continue his education at the Coll\u00e8ge de Beauvais and then at Lyc\u00e9e Saint-Louis and Lyc\u00e9e Louis-le-Grand in Paris.\nIn 1894, Lebesgue was accepted at the \u00c9cole Normale Sup\u00e9rieure, where he continued to focus his energy on the study of mathematics, graduating in 1897. After graduation he remained at the \u00c9cole Normale Sup\u00e9rieure for two years, working in the library, where he became aware of the research on discontinuity done at that time by Ren\u00e9-Louis Baire, a recent graduate of the school. At the same time he started his graduate studies at the Sorbonne, where he learned about \u00c9mile Borel's work on the incipient measure theory and Camille Jordan's work on the Jordan measure. In 1899 he moved to a teaching position at the Lyc\u00e9e Central in Nancy, while continuing work on his doctorate. In 1902 he earned his PhD from the Sorbonne with the seminal thesis on \"Integral, Length, Area\", submitted with Borel, four years older, as advisor.\nLebesgue married the sister of one of his fellow students, and he and his wife had two children, Suzanne and Jacques.\nAfter publishing his thesis, Lebesgue was offered in 1902 a position at the University of Rennes, lecturing there until 1906, when he moved to the Faculty of Sciences of the University of Poitiers. In 1910 Lebesgue moved to the Sorbonne as a ma\u00eetre de conf\u00e9rences, being promoted to professor starting in 1919. In 1921 he left the Sorbonne to become professor of mathematics at the Coll\u00e8ge de France, where he lectured and did research for the rest of his life. In 1922 he was elected a member of the Acad\u00e9mie des Sciences. Henri Lebesgue died on 26 July 1941 in Paris.\nMathematical career.\nLebesgue's first paper was published in 1898 and was titled \"Sur l'approximation des fonctions\". It dealt with Weierstrass's theorem on approximation to continuous functions by polynomials. Between March 1899 and April 1901 Lebesgue published six notes in \"Comptes Rendus.\" The first of these, unrelated to his development of Lebesgue integration, dealt with the extension of Baire's theorem to functions of two variables. The next five dealt with surfaces applicable to a plane, the area of skew polygons, surface integrals of minimum area with a given bound, and the final note gave the definition of Lebesgue integration for some function f(x). Lebesgue's great thesis, \"Int\u00e9grale, longueur, aire\", with the full account of this work, appeared in the Annali di Matematica in 1902. The first chapter develops the theory of measure (see Borel measure). In the second chapter he defines the integral both geometrically and analytically. The next chapters expand the \"Comptes Rendus\" notes dealing with length, area and applicable surfaces. The final chapter deals mainly with Plateau's problem. This dissertation is considered to be one of the finest ever written by a mathematician.\nHis lectures from 1902 to 1903 were collected into a \"Borel tract\" \"Le\u00e7ons sur l'int\u00e9gration et la recherche des fonctions primitives\". The problem of integration regarded as the search for a primitive function is the keynote of the book. Lebesgue presents the problem of integration in its historical context, addressing Augustin-Louis Cauchy, Peter Gustav Lejeune Dirichlet, and Bernhard Riemann. Lebesgue presents six conditions which it is desirable that the integral should satisfy, the last of which is \"If the sequence fn(x) increases to the limit f(x), the integral of fn(x) tends to the integral of f(x).\" Lebesgue shows that his conditions lead to the theory of measure and measurable functions and the analytical and geometrical definitions of the integral.\nHe turned next to trigonometric functions with his 1903 paper \"Sur les s\u00e9ries trigonom\u00e9triques\". He presented three major theorems in this work: that a trigonometrical series representing a bounded function is a Fourier series, that the nth Fourier coefficient tends to zero (the Riemann\u2013Lebesgue lemma), and that a Fourier series is integrable term by term. In 1904-1905 Lebesgue lectured once again at the Coll\u00e8ge de France, this time on trigonometrical series and he went on to publish his lectures in another of the \"Borel tracts\". In this tract he once again treats the subject in its historical context. He expounds on Fourier series, Cantor-Riemann theory, the Poisson integral and the Dirichlet problem.\nIn a 1910 paper, \"Repr\u00e9sentation trigonom\u00e9trique approch\u00e9e des fonctions satisfaisant a une condition de Lipschitz\" deals with the Fourier series of functions satisfying a Lipschitz condition, with an evaluation of the order of magnitude of the remainder term. He also proves that the Riemann\u2013Lebesgue lemma is a best possible result for continuous functions, and gives some treatment to Lebesgue constants.\nLebesgue once wrote, \"R\u00e9duites \u00e0 des th\u00e9ories g\u00e9n\u00e9rales, les math\u00e9matiques seraient une belle forme sans contenu.\" (\"Reduced to general theories, mathematics would be a beautiful form without content.\")\nIn measure-theoretic analysis and related branches of mathematics, the Lebesgue\u2013Stieltjes integral generalizes Riemann\u2013Stieltjes and Lebesgue integration, preserving the many advantages of the latter in a more general measure-theoretic framework.\nDuring the course of his career, Lebesgue also made forays into the realms of complex analysis and topology. He also had a disagreement with \u00c9mile Borel about whose integral was more general. However, these minor forays pale in comparison to his contributions to real analysis; his contributions to this field had a tremendous impact on the shape of the field today and his methods have become an essential part of modern analysis. These have important practical implications for fundamental physics of which Lebesgue would have been completely unaware, as noted below.\nLebesgue's theory of integration.\nIntegration is a mathematical operation that corresponds to the informal idea of finding the area under the graph of a function. The first theory of integration was developed by Archimedes in the 3rd century BC with his method of quadratures, but this could be applied only in limited circumstances with a high degree of geometric symmetry. In the 17th century, Isaac Newton and Gottfried Wilhelm Leibniz discovered the idea that integration was intrinsically linked to differentiation, the latter being a way of measuring how quickly a function changed at any given point on the graph. This surprising relationship between two major geometric operations in calculus, differentiation and integration, is now known as the fundamental theorem of calculus. It has allowed mathematicians to calculate a broad class of integrals for the first time. However, unlike Archimedes' method, which was based on Euclidean geometry, mathematicians felt that Newton's and Leibniz's integral calculus did not have a rigorous foundation.\nThe mathematical notion of limit and the closely related notion of convergence are central to any modern definition of integration. In the 19th century, Karl Weierstrass developed the rigorous epsilon-delta definition of a limit, which is still accepted and used by mathematicians today. He built on previous but non-rigorous work by Augustin Cauchy, who had used the non-standard notion of infinitesimally small numbers, today rejected in standard mathematical analysis. Before Cauchy, Bernard Bolzano had laid the fundamental groundwork of the epsilon-delta definition. See here for more.\nBernhard Riemann followed up on this by formalizing what is now called the Riemann integral. To define this integral, one fills the area under the graph with smaller and smaller rectangles and takes the limit of the sums of the areas of the rectangles at each stage. For some functions, however, the total area of these rectangles does not approach a single number. Thus, they have no Riemann integral.\nLebesgue invented a new method of integration to solve this problem. Instead of using the areas of rectangles, which put the focus on the domain of the function, Lebesgue looked at the codomain of the function for his fundamental unit of area. Lebesgue's idea was to first define measure, for both sets and functions on those sets. He then proceeded to build the integral for what he called simple functions; measurable functions that take only finitely many values. Then he defined it for more complicated functions as the least upper bound of all the integrals of simple functions smaller than the function in question.\nLebesgue integration has the property that every function defined over a bounded interval with a Riemann integral also has a Lebesgue integral, and for those functions the two integrals agree. Furthermore, every bounded function on a closed bounded interval has a Lebesgue integral and there are many functions with a Lebesgue integral that have no Riemann integral. All of these advantages over Riemann integration made Lebesgue's method a more powerful tool for mathematicians.\nAs part of the development of Lebesgue integration, Lebesgue invented the concept of measure, which extends the idea of length from intervals to a very large class of sets, called measurable sets. This larger class of sets meant that Lebesgue's technique for turning a measure into an integral generalises easily to many other situations with measurable sets, leading to the modern field of measure theory.\nThe Lebesgue integral is deficient in one respect. The Riemann integral generalises to the improper Riemann integral to measure functions whose domain of definition is not a closed interval. The Lebesgue integral integrates many of these functions (always reproducing the same answer when it does), but not all of them. For functions on the real line, the Henstock integral is an even more general notion of integral (based on Riemann's theory rather than Lebesgue's) that subsumes both Lebesgue integration and improper Riemann integration. However, the Henstock integral depends on specific ordering features of the real line and so does not generalise to allow integration in more general spaces (say, manifolds), while the Lebesgue integral extends to such spaces quite naturally.\nImplications for statistical mechanics.\nIn 1947 Norbert Wiener claimed that the Lebesgue integral had unexpected but important implications in establishing the validity of Willard Gibbs' work on the foundations of statistical mechanics. The notions of \"average\" and \"measure\" were urgently needed to provide a rigorous proof of Gibbs' ergodic hypothesis.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "49517", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=49517", "title": "Diabetic Ketoacidosis", "text": ""}
{"id": "49519", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=49519", "title": "Transportation (disambiguation)", "text": "Transportation refers to transport, the movement of people and goods from place to place.\nTransportation may also refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "49520", "revid": "3727527", "url": "https://en.wikipedia.org/wiki?curid=49520", "title": "Printed circuit", "text": ""}
{"id": "49522", "revid": "10841396", "url": "https://en.wikipedia.org/wiki?curid=49522", "title": "Anwar Sadat", "text": "President of Egypt from 1970 to 1981\nMuhammad Anwar es-Sadat (25 December 1918\u00a0\u2013 6 October 1981) was an Egyptian politician and military officer who served as the third president of Egypt, from 15 October 1970 until his assassination by members of the Egyptian Islamic Jihad on 6 October 1981. Sadat was a senior member of the Free Officers who overthrew King Farouk I in the Egyptian Revolution of 1952, and a close confidant of President Gamal Abdel Nasser, under whom he served as vice president twice and whom he succeeded as president in 1970. In 1978, Sadat and Menachem Begin, Prime Minister of Israel, signed a peace treaty in cooperation with United States President Jimmy Carter, for which they were recognized with the Nobel Peace Prize.\nIn his 11 years as president, he changed Egypt's trajectory, departing from many political and economic tenets of Nasserism, reinstituting a multi-party system, and launching the Infitah economic policy. As President, he led Egypt in the Yom Kippur War of 1973 to regain Egypt's Sinai Peninsula, which Israel had occupied since the Six-Day War of 1967, making him a hero in Egypt and, for a time, the wider Arab world. Afterwards, he engaged in negotiations with Israel, culminating in the Camp David Accords and the Egypt\u2013Israel peace treaty.\nAlthough reaction to the treaty\u00a0\u2013 which resulted in the return of Sinai to Egypt\u00a0\u2013 was generally favorable among Egyptians, it was rejected by the country's Muslim Brotherhood and the left, which felt Sadat had abandoned efforts to ensure a State of Palestine. With the exception of Sudan, the Arab world and the Palestine Liberation Organization (PLO) strongly opposed Sadat's efforts to make a separate peace with Israel without prior consultations with the Arab states. His refusal to reconcile with them over the Palestinian issue resulted in Egypt being suspended from the Arab League from 1979 to 1989. The peace treaty was also one of the primary factors that led to his assassination; on 6 October 1981, militants led by Khalid Al-Islambuli opened fire on Sadat with automatic rifles during the 6 October parade in Cairo, killing him.\nEarly life and revolutionary activities.\nAnwar Sadat was born on 25 December 1918 in Mit Abu El Kom, part of Monufia Governorate in what was then the Sultanate of Egypt, to a poor family, and he had 14 siblings. One of his brothers, Atef Sadat, later became a pilot and was killed in action in 1973 during the Yom Kippur War. His father, Anwar Mohammed El Sadat, was an Upper Egyptian, and his mother, Sit Al-Berain, was born to an Egyptian mother and a Sudanese father.\nHe graduated from the Royal Military Academy in Cairo, the capital of what was then the Kingdom of Egypt, in 1938 and was appointed to the Signal Corps. He entered the army as a second lieutenant and was posted to the Anglo-Egyptian Sudan (the Sudan being a condominium under joint British and Egyptian rule at the time). There, he met Gamal Abdel Nasser, and along with several other junior officers they formed the Free Officers, an organization committed to overthrowing British rule in Egypt and eliminating state corruption.\nDuring World War II, Sadat, a member of the fascist and ultranationalist Young Egypt Party, collaborated with Nazi Germany in Egypt as part of Operation Salam. There were meetings and talk amongst some pro-Axis junior officers of a potential revolt, and Sadat later said that he'd wanted to make Egypt \"a second Iraq\". After the Abdeen Palace incident, Sadat furthered his plans for a coup, but Muslim Brotherhood leader Hassan al-Banna declined to support the plot. He then met with two German agents in Cairo to discuss his plan, but was arrested and imprisoned for much of the war. By the end of the conflict, Sadat had already met with the secret society that decided to assassinate Amin Osman, Minister of Finance in the Wafd Party government, and the head of the Egyptian-British Friendship Society, due to his strong sympathies towards the British. Osman was assassinated in January 1946. Following the assassination of Amin Osman, Sadat returned again and finally to prison. \nIn Qarmidan prison, he faced the most difficult ordeals of imprisonment by being held in solitary confinement, but the first accused in the Hussein Tawfiq case, escaped, and after there is no criminality evidence all the charges fall and the suspected went free. Salah Zulfikar, then young police officer, at that time was the officer in charge in the prison. He believed in his heart of Sadat's heroism and that he played a patriotic role towards his country and that he was convicted and imprisoned because of his love for his country. Zulfikar brought with him food, newspapers and cigarettes and helped his family a lot in obtaining visitor permits to check on him. Anwar Sadat was active in many political movements, including the Muslim Brotherhood, the fascist Young Egypt Party, the pro-Axis and pro-Royalist Iron Guard of Egypt, and the secret military group called the Free Officers. Along with his fellow Free Officers, Sadat participated in the military coup that launched the Egyptian Revolution of 1952, which overthrew King Farouk I on 23 July of that year. Sadat gave the first statement of the revolution over the radio to the Egyptian people.\nDuring Nasser's presidency.\nDuring the presidency of Gamal Abdel Nasser, Sadat was appointed minister of State in 1954. He was also appointed editor of the newly founded daily \"Al Gomhuria\". In 1959, he assumed the position of secretary to the National Union. Sadat was the president of the National Assembly (1960\u20131968) and then Vice President of Egypt and member of the presidential council in 1964. He was reappointed as vice president again in December 1969.\nPresidency.\nSome of the major events of Sadat's presidency were his \"Corrective Revolution\" to consolidate power in 1971, the break with Egypt's long-time ally and aid-giver the USSR, the Yom Kippur War with Israel, the Camp David Accords and the Egypt\u2013Israel peace treaty, the \"opening up\" (or Infitah) of Egypt's economy, and lastly his assassination in 1981.\nSadat succeeded Nasser as president after the latter's death in October 1970. Sadat's presidency was widely expected to be short-lived. Viewing him as having been little more than a puppet of the former president, Nasser's supporters in government settled on Sadat as someone they could manipulate easily. Sadat surprised everyone with a series of astute political moves by which he was able to retain the presidency and emerge as a leader in his own right. \nOn 15 May 1971, Sadat announced his corrective revolution, purging the government, political and security establishments of the most ardent Nasserists. Sadat encouraged the emergence of an Islamist movement, which had been suppressed by Nasser. Believing Islamists to be socially conservative he gave them \"considerable cultural and ideological autonomy\" in exchange for political support.\nIn 1971, as part of the Jarring Mission, three years into the War of Attrition in the Suez Canal zone, Sadat endorsed in a letter the peace proposals of UN negotiator Gunnar Jarring, which seemed to lead to a full peace with Israel on the basis of Israel's withdrawal to its pre-war borders. This peace initiative failed as neither Israel nor the United States of America accepted the terms as discussed then.\nCorrective Revolution.\nShortly after taking office, Sadat shocked many Egyptians by dismissing and imprisoning two of the most powerful figures in the regime, Vice President Ali Sabri, who had close ties with Soviet officials, and Sharawi Gomaa, the Interior Minister, who controlled the secret police. \nSadat's rising popularity would accelerate after he cut back the powers of the hated secret police, expelled Soviet military from the country and reformed the Egyptian army for a renewed confrontation with Israel.\nYom Kippur War.\nOn 6 October 1973, in conjunction with Hafez al-Assad of Syria, Sadat launched the October War, also known as the Yom Kippur War (and less commonly as the Ramadan War), a surprise attack against the Israeli forces occupying the Egyptian Sinai Peninsula, and the Syrian Golan Heights in an attempt to retake these respective Egyptian and Syrian territories that had been occupied by Israel since the Six Day War six years earlier. The Egyptian and Syrian performance in the initial stages of the war astonished both Israel, and the Arab World. The most striking achievement (Operation Badr, also known as The Crossing) was the Egyptian military's advance approximately 15\u00a0km into the occupied Sinai Peninsula after penetrating and largely destroying the Bar Lev Line. This line was popularly thought to have been an impregnable defensive chain.\nAs the war progressed, three divisions of the Israeli army led by General Ariel Sharon had crossed the Suez Canal, trying to encircle first the Egyptian Second Army. Although this failed, prompted by an agreement between the United States of America and the Soviet Union, the United Nations Security Council passed Resolution 338 on 22 October 1973, calling for an immediate ceasefire. Although agreed upon, the ceasefire was immediately broken. Alexei Kosygin, the chairman of the USSR Council of Ministers, cancelled an official meeting with Danish Prime Minister Anker J\u00f8rgensen to travel to Egypt where he tried to persuade Sadat to sign a peace treaty. During Kosygin's two-day long stay it is unknown if he and Sadat ever met in person.\nThe Israeli military then continued their drive to encircle the Egyptian army. The encirclement was completed on 24 October, three days after the ceasefire was broken. This development prompted superpower tension, but a second ceasefire was imposed cooperatively on 25 October to end the war. At the conclusion of hostilities, Israeli forces were 40 kilometres (25\u00a0mi) from Damascus and 101 kilometres (63\u00a0mi) from Cairo.\nPeace with Israel.\nThe initial Egyptian and Syrian victories in the war restored popular morale throughout Egypt and the Arab World and, for many years after, Sadat was known as the \"Hero of the Crossing\". Israel recognized Egypt as a formidable foe, and Egypt's renewed political significance eventually led to regaining and reopening the Suez Canal through the peace process. His new peace policy led to the conclusion of two agreements on disengagement of forces with the Israeli government. The first of these agreements was signed on 18 January 1974, and the second on 4 September 1975.\nOne major aspect of Sadat's peace policy was to gain some religious support for his efforts. Already during his visit to the US in October\u2013November 1975, he invited Evangelical pastor Billy Graham for an official visit, which was held a few days after Sadat's visit. In addition to cultivating relations with Evangelical Christians in the US, he also built some cooperation with the Vatican. On 8 April 1976, he visited the Vatican for the first time, and got a message of support from Pope Paul VI regarding achieving peace with Israel, to include a just solution to the Palestinian issue. Sadat, on his part, extended to the Pope a public invitation to visit Cairo.\nSadat also used the media to promote his purposes. In an interview he gave to the Lebanese magazine \"Al Hawadeth\" in early February 1976, he claimed he had secret commitment from the US government to put pressure on the Israeli government for a major withdrawal in Sinai and the Golan Heights. This statement caused some concern to the Israeli government, but Secretary of State Henry Kissinger denied such a promise was ever made.\nIn January 1977, a series of 'Bread Riots' protested Sadat's economic liberalization and specifically a government decree lifting price controls on basic necessities like bread. The riots lasted for two days and included hundreds of thousands in Cairo. 120 buses and hundreds of buildings were destroyed in Cairo alone. The riots ended with the deployment of the army and the re-institution of the subsidies/price controls. During this time, Sadat was also taking a new approach towards improving relations with the West.\nThe United States and the Soviet Union agreed on 1 October 1977, on principles to govern a Geneva conference on the Middle East. Syria continued to resist such a conference. Not wanting either Syria or the Soviet Union to influence the peace process, Sadat decided to take more progressive stance towards building a comprehensive peace agreement with Israel.\nThe 1977 visit by Anwar Sadat to Jerusalem was the first time an Arab leader officially visited Israeli-controlled territory. Sadat met with Israeli prime minister Menachem Begin, and spoke before the Knesset in Jerusalem about his views on how to achieve a comprehensive peace to the Arab\u2013Israeli conflict, which included the full implementation of UN Resolutions 242 and 338.\nThe peace treaty was finally signed by Anwar Sadat and Israeli prime minister Menachem Begin in Washington, D.C., United States, on 26 March 1979, following the Camp David Accords, a series of meetings between Egypt and Israel facilitated by US president Jimmy Carter. Both Sadat and Begin were awarded the Nobel Peace Prize for creating the treaty. In his acceptance speech, Sadat referred to the long-awaited peace desired by both Arabs and Israelis.\nThe main features of the agreement were the mutual recognition of each country by the other, the cessation of the state of war that had existed since the 1948 Arab\u2013Israeli War, and the complete withdrawal by Israel of its armed forces and civilians from the rest of the Sinai Peninsula, which Israel had captured during the 1967 Six-Day War.\nThe agreement also provided for the free passage of Israeli ships through the Suez Canal and recognition of the Strait of Tiran and the Gulf of Aqaba as international waterways. The agreement notably made Egypt the first Arab country to officially recognize Israel. The peace agreement between Egypt and Israel has remained in effect since the treaty was signed.\nThe treaty was extremely unpopular in most of the Arab World and the wider Muslim World. His predecessor Nasser had made Egypt an icon of Arab nationalism, an ideology that appeared to be sidelined by an Egyptian orientation following the 1973 war (see National identity of Egyptians). The neighboring Arab countries believed that in signing the accords, Sadat had put Egypt's interests ahead of Arab unity, betraying Nasser's pan-Arabism, and destroyed the vision of a united \"Arab front\" for the support of the Palestinians against the \"Zionist Entity\". However, Sadat decided early on that peace was the solution. Sadat's shift towards a strategic relationship with the US was also seen as a betrayal by many Arabs. In the United States his peace moves gained him popularity among some Evangelical circles. He was awarded the Prince of Peace Award by Pat Robertson.\nIn 1979, the Arab League suspended Egypt in the wake of the Egyptian\u2013Israel peace agreement, and the League moved its headquarters from Cairo to Tunis. Arab League member states believed in the elimination of the \"Zionist Entity\" and Israel at that time. It was not until 1989 that the League re-admitted Egypt as a member, and returned its headquarters to Cairo. As part of the peace deal, Israel withdrew from the Sinai Peninsula in phases, completing its withdrawal from the entire territory except the town of Taba by 25 April 1982 (withdrawal from which did not occur until 1989). The improved relations Egypt gained with the West through the Camp David Accords soon gave the country resilient economic growth. By 1980, however, Egypt's strained relations with the Arab World would result in a period of rapid inflation.\nRelationship with Mohammad Reza Shah Pahlavi of Iran.\nThe relationship between Iran and Egypt had fallen into open hostility during Gamal Abdel Nasser's presidency. Following his death in 1970, President Sadat turned this around quickly into an open and close friendship.\nIn 1971, Sadat addressed the Iranian parliament in Tehran in fluent Persian, describing the 2,500-year-old historic connection between the two lands.\nOvernight, the Egyptian and Iranian governments were turned from bitter enemies into fast friends. The relationship between Cairo and Tehran became so friendly that the Shah of Iran, Mohammad Reza Pahlavi, called Sadat his \"dear brother\".\nAfter the 1973 war with Israel, Iran assumed a leading role in cleaning up and reactivating the blocked Suez Canal with heavy investment. The country also facilitated the withdrawal of Israel from the occupied Sinai Peninsula by promising to substitute the loss of the oil to the Israelis with free Iranian oil if they withdrew from the Egyptian oil wells in western Sinai.\nAll these added more to the personal friendship between Sadat and the Shah of Iran. (The Shah's first wife was Princess Fawzia of Egypt. She was the eldest daughter of Sultan Fuad I of Egypt and Sudan (later King Fuad I) and his second wife Nazli Sabri.)\nAfter his overthrow, the deposed Shah spent the last months of his life in exile in Egypt. When the Shah died, Sadat ordered that he be given a state funeral and be interred at the Al-Rifa'i Mosque in Cairo, the resting place of Egyptian Khedive Isma'il Pasha, his mother Khushyar Hanim, and numerous other members of the royal family of Egypt and Sudan.\nAssassination.\nThe last months of Sadat's presidency were marked by internal uprising. Sadat dismissed allegations that the rioting was incited by domestic issues, believing that the Soviet Union was recruiting its regional allies in Libya and Syria to incite an uprising that would eventually force him out of power. Following a failed military coup in June 1981, Sadat ordered a major crackdown that resulted in the arrest of numerous opposition figures. Although Sadat still maintained high levels of popularity in Egypt, it has been said that he was assassinated \"at the peak\" of his unpopularity.\nEarlier in his presidency, Islamists had benefited from the 'rectification revolution' and the release from prison of activists jailed under Nasser. But Sadat's Sinai treaty with Israel enraged Islamists, particularly the radical Egyptian Islamic Jihad. According to interviews and information gathered by journalist Lawrence Wright, the group was recruiting military officers and accumulating weapons, waiting for the right moment to launch \"a complete overthrow of the existing order\" in Egypt. Chief strategist of El-Jihad was Abbud al-Zumar, a colonel in the military intelligence whose \"plan was to kill the main leaders of the country, capture the headquarters of the army and State Security, the telephone exchange building, and of course the radio and television building, where news of the Islamic revolution would then be broadcast, unleashing\u00a0\u2013 he expected\u00a0\u2013 a popular uprising against secular authority all over the country\".\nIn February 1981, Egyptian authorities were alerted to El-Jihad's plan by the arrest of an operative carrying crucial information. In September, Sadat ordered a highly unpopular roundup of more than 1,500 people, including many Jihad members, but also the Coptic Pope and other Coptic clergy, intellectuals and activists of all ideological stripes. All non-government press was banned as well. The roundup missed a Jihad cell in the military led by Lieutenant Khalid Al-Islambuli, who would succeed in assassinating Anwar Sadat that October. The assassination plot altogether was led by Muhammad Abd al-Salam Faraj, an Egyptian Islamic Jihad member who was a civilian engineer.\nAccording to Tala'at Qasim, ex-head of the Gama'a Islamiyya interviewed in \"Middle East Report\", it was not Islamic Jihad but his organization, known in English as the \"Islamic Group\", that organized the assassination and recruited the assassin (Al-Islambuli). Members of the Group's 'Majlis el-Shura' ('Consultative Council') \u2013 headed by the famed 'blind shaykh' \u2013 were arrested two weeks before the killing, but they did not disclose the existing plans and Al-Islambuli succeeded in assassinating Sadat.\nOn 6 October 1981, Sadat was assassinated during the annual victory parade held in Cairo to celebrate Egypt's crossing of the Suez Canal. Al-Islambuli emptied his assault rifle into Sadat's body while in the front of the grandstand, mortally wounding the President. In addition to Sadat, eleven others were killed, including the Cuban ambassador, an Omani general, a Coptic Orthodox bishop and Samir Helmy, the head of Egypt's Central Auditing Agency (CAA). Twenty-eight were wounded, including Vice President Hosni Mubarak, Irish Defence Minister James Tully, and four US military liaison officers.\nThe assassination squad was led by Lieutenant Al-Islambuli after a fatw\u0101 approving the assassination had been obtained from Omar Abdel-Rahman. Al-Salam Faraj and four defendants who directly attacked Sadat, including Al-Islambuli, were found guilty, sentenced to death, and executed on April 15, 1982. Sadat's two military assailants, al-Islambuli and Sergeant Hussein Abbas Mohammed, would be executed by a firing squad, while the three civilian perpetrators, al-Salam Faraj, engineer and former army officer Abdel-Salam Abdel Aal and engineer Atta Tayel Hemida Raheel, were hung. By April 1982, 19 people would be tried for Sadat's assassination, with 17 being jailed and two being acquitted.\nAftermath.\nSadat was succeeded by his vice president Hosni Mubarak, whose hand was injured during the attack. Sadat's funeral was attended by a record number of dignitaries from around the world, including a rare simultaneous attendance by three former US presidents: Gerald Ford, Jimmy Carter and Richard Nixon. Sudan's President Gaafar Nimeiry was the only Arab head of state to attend the funeral. Only 3 of 24 states in the Arab League\u00a0\u2013 Oman, Somalia and Sudan\u00a0\u2013 sent representatives at all. Israel's prime minister, Menachem Begin, considered Sadat a personal friend and insisted on attending the funeral, walking throughout the funeral procession so as not to desecrate the Sabbath. Sadat was buried in the unknown soldier memorial in Cairo, across the street from the stand where he was assassinated.\nOver three hundred Islamic radicals were indicted in the trial of assassin Khalid Al-Islambuli, including future al-Qaeda leader Ayman al-Zawahiri, Omar Abdel-Rahman, and Abd al-Hamid Kishk. The trial was covered by the international press and Zawahiri's knowledge of English made him the de facto spokesman for the defendants. Zawahiri was released from prison in 1984. Abboud al-Zomor and Tareq al-Zomor, two Islamic Jihad leaders imprisoned in connection with the assassination, were released on 11 March 2011.\nDespite these facts, the nephew of the late president, Talaat Sadat, claimed that the assassination was an international conspiracy. On 31 October 2006, he was sentenced to a year in prison for defaming Egypt's armed forces, less than a month after he gave the interview accusing Egyptian generals of masterminding his uncle's assassination. In an interview with a Saudi television channel, he also claimed both the United States and Israel were involved noting that no one from the special personal protection group of Sadat fired a single shot during the killing, and not one of them has been put on trial.\nMedia portrayals of Anwar Sadat.\nIn 1983, \"Sadat\", a miniseries based on the life of Anwar Sadat, aired on US television with Oscar-winning actor Louis Gossett Jr. in the title role. The film was promptly banned by the Egyptian government, as were all other movies produced and distributed by Columbia Pictures, over allegations of historical inaccuracies. A civil lawsuit was brought by Egypt's artists' and film unions against Columbia Pictures and the film's directors, producers and scriptwriters before a court in Cairo, but was dismissed, since the alleged slanders, having taken place outside the country, fell outside the Egyptian courts' jurisdiction.\nThe film was critically acclaimed in North America, but was unpopular among Egyptians and in the Egyptian press. Western authors attributed the film's poor reception in Egypt to racism \u2013 Gossett being African-American \u2013 in the Egyptian government or Egypt in general. Either way, one Western source wrote that Sadat's portrayal by Gossett \"bothered race-conscious Egyptians and wouldn't have pleased [the deceased] Sadat,\" who identified as Egyptian and Northeast African, not black. The two-part series earned Gossett an Emmy nomination in the United States.\nHe was portrayed by Robert Loggia in the 1982 television movie \"A Woman Called Golda\", opposite Ingrid Bergman as Golda Meir.\nThe first Egyptian depiction of Sadat's life came in 2001, when \"Ayyam El Sadat\" (English: \"The Days of Sadat\") was released in Egyptian cinemas. The movie was a major success in Egypt, and was hailed as Ahmed Zaki's greatest performance to date.\nSadat was a recurring character on \"Saturday Night Live\", played by Garrett Morris, who bore a resemblance to Sadat.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
