{"id": "51459", "revid": "28903366", "url": "https://en.wikipedia.org/wiki?curid=51459", "title": "South Platte River", "text": "River in Colorado and Nebraska, United States\nThe South Platte River (Sioux: \"Wa\u0161\u00ed\u014b-Wakp\u00e1\" ] lit. \u201cbison tallow river\u201d) is one of the two principal tributaries of the Platte River. Flowing through the U.S. states of Colorado and Nebraska, it is itself a major river of the American Midwest and the American Southwest/Mountain West. Its drainage basin includes much of the eastern flank of the Rocky Mountains in Colorado, much of the populated region known as the Colorado Front Range and Eastern Plains, and a portion of southeastern Wyoming in the vicinity of the city of Cheyenne. It joins the North Platte River in western Nebraska to form the Platte, which then flows across Nebraska to the Missouri. The river serves as the principal source of water for eastern Colorado. In its valley along the foothills in Colorado, it has permitted agriculture in an area of the Colorado Piedmont and Great Plains that is otherwise arid.\nDescription.\nThe river is formed in Park County, Colorado, southwest of Denver in the South Park grassland basin by the confluence of the South Fork and Middle Fork, about southeast of Fairplay. Both forks rise along the eastern flank of the Mosquito Range, on the western side of South Park, which is drained by the tributaries at the headwaters of the river. From South Park, it passes through of the Platte Canyon and its lower section, Waterton Canyon. Here, it is joined by the North Fork before emerging from the foothills southwest of the Denver suburb of Littleton. At Littleton, the river is impounded to form Chatfield Reservoir, a flood-control basin for the Denver metropolitan area.\nThe river flows north through central Denver, which was founded along its banks at its confluence with Cherry Creek. The valley through Denver is highly industrialized, serving generally as the route for both the railroad lines, as well as Interstate 25. On the north side of Denver, it is joined somewhat inconspicuously by Clear Creek, which descends from the mountains to the west in a canyon that was the cradle of the Pike's Peak Gold Rush. North of Denver, it flows through the agricultural heartland of the Piedmont (a shale region that was formed through erosion by the ancestor of the river following the creation of the Rockies). It flows directly past the communities of Brighton and Fort Lupton, and is joined in succession by Saint Vrain Creek, the Little Thompson River, the Big Thompson River, and the Cache la Poudre River, which it receives just east of Greeley.\nEast of Greeley, it turns eastward, flowing across the Colorado Eastern Plains, past Fort Morgan and Brush, where it turns northeastward. It continues past Sterling, and runs into Nebraska between Julesburg, Colorado, and Big Springs, Nebraska. In Nebraska, it passes south of Ogallala and joins the North Platte River near the city of North Platte.\nThe South Platte River through Denver is on the U.S. Environmental Protection Agency's (US EPA) list of impaired waterbodies for pathogen impairment, with \"E. coli \"as the representative pathogen species. Other water issues involve the appearance of the New Zealand mud snail and zebra mussel.\nThe flows of the South Platte have been greatly modified by human activity. Originally the river was seasonal north of Fort Morgan to its confluence with the North Platte. In the 1800s the river would disappear in July, August, and September due to low flows and a sandy river bottom. By 1910 the increase in agriculture caused the river to reach the border of Colorado and Wyoming due to return flows and dams holding back water later in the year. Increasing diversions from the upper Colorado River in the 20th century mean that flows reach the North Platte year round.\nHistory.\nThe South Platte was originally called Niin\u00e9niiniic\u00edih\u00e9he by the native Arapaho people who lived on its banks. The early Spanish explorers called it the Rio Chato (calm river). In 1702, it was named the Rio Jesus Maria by Captain Jose Naranjo, the Indigenous-Spanish scout and captain of war of the New Mexico Indian Auxiliaries, who was ordered by the viceroy of New Spain to search the Tierra Incognita for a French incursion into New Mexico. The South Platte also served as a vital water source in Colorado. Long before the city of Denver was created, many travelers came to the South Platte River to escape the arid Great Plains. These people could survive the heat, but not without the vital water source that the South Platte gave them. Buckets and wells sufficed as a water system for a while, but eventually, the Denver Water System was created.\nIn the late 1830s, four fur trading outposts were established on the river.\nDams.\nIn an arid region of the United States, the South Platte is marked with several dams. The first notable water impoundment on the South Platte is Antero Reservoir. \"Antero\" is derived from the Spanish word \"delantero\", which means \"foremost\" or \"head\", as the reservoir was the first dam on the South Platte River near the river's origin.\nThe next dam is Spinney Mountain Reservoir. At capacity, Spinney Mountain covers . A bottom-release dam, Spinney releases to the east of the inlet.\nTwo miles below Spinney Mountain Reservoir, the river enters Eleven Mile Reservoir, with a capacity of . The Eleven Mile Reservoir Dam drains into Eleven Mile Canyon, which runs through Forest Service land. Three former Colorado towns, Howbert, Idlewild, and Freshwater Station, were submerged under the reservoir when it was built.\nFrom Eleven Mile Canyon, the South Platte runs northeast to Cheesman Reservoir, named for Denver water pioneer Walter S. Cheesman. At its completion in 1905, the dam was the world's tallest gravity dam, at above the streambed. The reservoir and related facilities were purchased in November 1918 by the Denver Water Board. Cheesman was the first reservoir of Denver's mountain storage facilities, and has been designated a National Historic Civil Engineering Landmark. Cheesman Reservoir feeds Cheesman Canyon. Six miles below Cheesman Reservoir is the town of Deckers; there, the river bends north for about to the confluence with the North Fork of the South Platte.\nIn the late 1980s, a proposal was put forth for the Two Forks Dam, which would have created a reservoir flooding the entire section from the North Fork confluence to the town of Deckers. In 1990, the US EPA vetoed the permit, calling the project an \"environmental catastrophe.\"\nFrom the confluence, the river flows towards Denver and enters Strontia Springs Reservoir.\nBelow Strontia Springs, the South Platte runs through Waterton Canyon before entering Chatfield Reservoir. Chatfield marks the seventh and final dam on the South Platte until it merges with the North Platte.\nFly fishing overview.\nThe South Platte River is a gold medal western trout river on the eastern slope of Colorado, meaning it produces at least 12 trout of greater than 14\u00a0inches in length or greater per acre in areas accessible to the public. The river is well known for its wild trophy population of brown and rainbow trout. As a result of its close proximity to Denver, the river has thousands of fly fishing enthusiasts visit each year. With seven dams on the river, the South Platte is considered a tailwater fishery. Most of these dams are bottom released, which allows for both stable water temperatures and year-round fly fishing. Popular fly-fishing stretches of the river include Waterton Canyon, Cheesman Canyon, and the Dream Stream.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51462", "revid": "6908984", "url": "https://en.wikipedia.org/wiki?curid=51462", "title": "Machine", "text": "Powered mechanical device\nA machine is a thermodynamic system that uses power to apply forces and control movement to perform an action. The term is commonly applied to artificial devices, such as those employing engines or motors, but also to natural biological macromolecules, such as molecular machines. Machines can be driven by animals and people, by natural forces such as wind and water, and by chemical, thermal, or electrical power, and include a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement. They can also include computers and sensors that monitor performance and plan movement, often called mechanical systems.\nRenaissance natural philosophers identified six simple machines which were the elementary devices that put a load into motion, and calculated the ratio of output force to input force, known today as mechanical advantage.\nModern machines are complex systems that consist of structural elements, mechanisms and control components and include interfaces for convenient use. Examples include: a wide range of vehicles, such as trains, automobiles, boats and airplanes; appliances in the home and office, including computers, building air handling and water handling systems; as well as farm machinery, machine tools and factory automation systems and robots.\nEtymology.\nThe English word \"machine\" comes through Middle French from Latin , which in turn derives from the Greek (Doric , Ionic 'contrivance, machine, engine', a derivation from 'means, expedient, remedy'). The word \"mechanical\" (Greek: ) comes from the same Greek roots. A wider meaning of 'fabric, structure' is found in classical Latin, but not in Greek usage. This meaning is found in late medieval French, and is adopted from the French into English in the mid-16th century.\nIn the 17th century, the word machine could also mean a scheme or plot, a meaning now expressed by the derived machination. The modern meaning develops out of specialized application of the term to stage engines used in theater and to military siege engines, both in the late 16th and early 17th centuries. The OED traces the formal, modern meaning to John Harris' \"Lexicon Technicum\" (1704), which has:\n\"Machine, or Engine, in Mechanicks, is whatsoever hath Force sufficient either to raise or stop the Motion of a Body. Simple Machines are commonly reckoned to be Six in Number, viz. the Ballance, Leaver, Pulley, Wheel, Wedge, and Screw. Compound Machines, or Engines, are innumerable.\"\nThe word \"engine\" used as a (near-) synonym both by Harris and in later language derives ultimately (via Old French) from Latin 'ingenuity, an invention'.\nHistory.\nThe hand axe, made by chipping flint to form a wedge, in the hands of a human transforms force and movement of the tool into a transverse splitting forces and movement of the workpiece. The hand axe is the first example of a wedge, the oldest of the six classic simple machines, from which most machines are based. The second oldest simple machine was the inclined plane (ramp), which has been used since prehistoric times to move heavy objects.\nThe other four simple machines were invented in the ancient Near East. The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC. The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale, and to move large objects in ancient Egyptian technology. The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c.\u20093000 BC, and then in ancient Egyptian technology c.\u20092000 BC. The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC, and ancient Egypt during the Twelfth Dynasty (1991\u20131802 BC). The screw, the last of the simple machines to be invented, first appeared in Mesopotamia during the Neo-Assyrian period (911\u2013609) BC. The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever.\nThree of the simple machines were studied and described by Greek philosopher Archimedes around the 3rd century BC: the lever, pulley and screw. Archimedes discovered the principle of mechanical advantage in the lever. Later Greek philosophers defined the classic five simple machines (excluding the inclined plane) and were able to roughly calculate their mechanical advantage. Hero of Alexandria (c.\u200910\u201375 AD) in his work \"Mechanics\" lists five mechanisms that can \"set a load in motion\"; lever, windlass, pulley, wedge, and screw, and describes their fabrication and uses. However, the Greeks' understanding was limited to statics (the balance of forces) and did not include dynamics (the tradeoff between force and distance) or the concept of work.\nThe earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD. The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi ad-Din Muhammad ibn Ma'ruf in Ottoman Egypt.\nThe cotton gin was invented in India by the 6th century AD, and the spinning wheel was invented in the Islamic world by the early 11th century, both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny.\nThe earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their \"Book of Ingenious Devices\", in the 9th century. In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns.\nDuring the Renaissance, the dynamics of the \"Mechanical Powers\", as the simple machines were called, began to be studied from the standpoint of how much useful work they could perform, leading eventually to the new concept of mechanical work. In 1586 Flemish engineer Simon Stevin derived the mechanical advantage of the inclined plane, and it was included with the other simple machines. The complete dynamic theory of simple machines was worked out by Italian scientist Galileo Galilei in 1600 in \"Le Meccaniche\" (\"On Mechanics\"). He was the first to understand that simple machines do not create energy, they merely transform it.\nThe classic rules of sliding friction in machines were discovered by Leonardo da Vinci (1452\u20131519), but remained unpublished in his notebooks. They were rediscovered by Guillaume Amontons (1699) and were further developed by Charles-Augustin de Coulomb (1785).\nJames Watt patented his parallel motion linkage in 1782, which made the double acting steam engine practical. The Boulton and Watt steam engine and later designs powered steam locomotives, steam ships, and factories.\nThe Industrial Revolution was a period from 1750 to 1850 where changes in agriculture, manufacturing, mining, transportation, and technology had a profound effect on the social, economic and cultural conditions of the times. It began in the United Kingdom, then subsequently spread throughout Western Europe, North America, Japan, and eventually the rest of the world.\nStarting in the later part of the 18th century, there began a transition in parts of Great Britain's previously manual labour and draft-animal-based economy towards machine-based manufacturing. It started with the mechanisation of the textile industries, the development of iron-making techniques and the increased use of refined coal.\nSimple machines.\nThe idea that a machine can be decomposed into simple movable elements led Archimedes to define the lever, pulley and screw as simple machines. By the time of the Renaissance this list increased to include the wheel and axle, wedge and inclined plane. The modern approach to characterizing machines focusses on the components that allow movement, known as joints.\nWedge (hand axe): Perhaps the first example of a device designed to manage power is the hand axe, also called biface and Olorgesailie. A hand axe is made by chipping stone, generally flint, to form a bifacial edge, or wedge. A wedge is a simple machine that transforms lateral force and movement of the tool into a transverse splitting force and movement of the workpiece. The available power is limited by the effort of the person using the tool, but because power is the product of force and movement, the wedge amplifies the force by reducing the movement. This amplification, or mechanical advantage is the ratio of the input speed to output speed. For a wedge this is given by 1/tan\u03b1, where \u03b1 is the tip angle. The faces of a wedge are modeled as straight lines to form a sliding or prismatic joint.\nLever: The lever is another important and simple device for managing power. This is a body that pivots on a fulcrum. Because the velocity of a point farther from the pivot is greater than the velocity of a point near the pivot, forces applied far from the pivot are amplified near the pivot by the associated decrease in speed. If \"a\" is the distance from the pivot to the point where the input force is applied and \"b\" is the distance to the point where the output force is applied, then \"a/b\" is the mechanical advantage of the lever. The fulcrum of a lever is modeled as a hinged or revolute joint.\nWheel: The wheel is an important early machine, such as the chariot. A wheel uses the law of the lever to reduce the force needed to overcome friction when pulling a load. To see this notice that the friction associated with pulling a load on the ground is approximately the same as the friction in a simple bearing that supports the load on the axle of a wheel. However, the wheel forms a lever that magnifies the pulling force so that it overcomes the frictional resistance in the bearing.\nThe classification of simple machines to provide a strategy for the design of new machines was developed by Franz Reuleaux, who collected and studied over 800 elementary machines. He recognized that the classical simple machines can be separated into the lever, pulley and wheel and axle that are formed by a body rotating about a hinge, and the inclined plane, wedge and screw that are similarly a block sliding on a flat surface.\nSimple machines are elementary examples of kinematic chains or linkages that are used to model mechanical systems ranging from the steam engine to robot manipulators. The bearings that form the fulcrum of a lever and that allow the wheel and axle and pulleys to rotate are examples of a kinematic pair called a hinged joint. Similarly, the flat surface of an inclined plane and wedge are examples of the kinematic pair called a sliding joint. The screw is usually identified as its own kinematic pair called a helical joint.\nThis realization shows that it is the joints, or the connections that provide movement, that are the primary elements of a machine. Starting with four types of joints, the rotary joint, sliding joint, cam joint and gear joint, and related connections such as cables and belts, it is possible to understand a machine as an assembly of solid parts that connect these joints called a mechanism.\nTwo levers, or cranks, are combined into a planar four-bar linkage by attaching a link that connects the output of one crank to the input of another. Additional links can be attached to form a six-bar linkage or in series to form a robot.\nMechanical systems.\nA mechanical system manages power to accomplish a task that involves forces and movement. Modern machines are systems consisting of (i) a power source and actuators that generate forces and movement, (ii) a system of mechanisms that shape the actuator input to achieve a specific application of output forces and movement, (iii) a controller with sensors that compare the output to a performance goal and then directs the actuator input, and (iv) an interface to an operator consisting of levers, switches, and displays. This can be seen in Watt's steam engine in which the power is provided by steam expanding to drive the piston. The walking beam, coupler and crank transform the linear movement of the piston into rotation of the output pulley. Finally, the pulley rotation drives the flyball governor which controls the valve for the steam input to the piston cylinder.\nThe adjective \"mechanical\" refers to skill in the practical application of an art or science, as well as relating to or caused by movement, physical forces, properties or agents such as is dealt with by mechanics. Similarly Merriam-Webster Dictionary defines \"mechanical\" as relating to machinery or tools.\nPower flow through a machine provides a way to understand the performance of devices ranging from levers and gear trains to automobiles and robotic systems. The German mechanician Franz Reuleaux wrote, \"a machine is a combination of resistant bodies so arranged that by their means the mechanical forces of nature can be compelled to do work accompanied by certain determinate motion.\" Notice that forces and motion combine to define power.\nMore recently, Uicker et al. stated that a machine is \"a device for applying power or changing its direction.\"McCarthy and Soh describe a machine as a system that \"generally consists of a power source and a mechanism for the controlled use of this power.\"\nPower sources.\nHuman and animal effort were the original power sources for early machines.\nWaterwheel: Waterwheels appeared around the world around 300 BC to use flowing water to generate rotary motion, which was applied to milling grain, and powering lumber, machining and textile operations. Modern water turbines use water flowing through a dam to drive an electric generator.\nWindmill: Early windmills captured wind power to generate rotary motion for milling operations. Modern wind turbines also drives a generator. This electricity in turn is used to drive motors forming the actuators of mechanical systems.\nEngine: The word engine derives from \"ingenuity\" and originally referred to contrivances that may or may not be physical devices. A steam engine uses heat to boil water contained in a pressure vessel; the expanding steam drives a piston or a turbine. This principle can be seen in the aeolipile of Hero of Alexandria. This is called an external combustion engine.\nAn automobile engine is called an internal combustion engine because it burns fuel (an exothermic chemical reaction) inside a cylinder and uses the expanding gases to drive a piston. A jet engine uses a turbine to compress air which is burned with fuel so that it expands through a nozzle to provide thrust to an aircraft, and so is also an \"internal combustion engine.\"\nPower plant: The heat from coal and natural gas combustion in a boiler generates steam that drives a steam turbine to rotate an electric generator. A nuclear power plant uses heat from a nuclear reactor to generate steam and electric power. This power is distributed through a network of transmission lines for industrial and individual use.\nMotors: Electric motors use either AC or DC electric current to generate rotational movement. Electric servomotors are the actuators for mechanical systems ranging from robotic systems to modern aircraft.\nFluid Power: Hydraulic and pneumatic systems use electrically driven pumps to drive water or air respectively into cylinders to power linear movement.\nElectrochemical: Chemicals and materials can also be sources of power. They may chemically deplete or need re-charging, as is the case with batteries, or they may produce power without changing their state, which is the case for solar cells and thermoelectric generators. All of these, however, still require their energy to come from elsewhere. With batteries, it is the already existing chemical potential energy inside. In solar cells and thermoelectrics, the energy source is light and heat respectively.\nMechanisms.\nThe \"mechanism\" of a mechanical system is assembled from components called \"machine elements\". These elements provide structure for the system and control its movement.\nThe structural components are, generally, the frame members, bearings, splines, springs, seals, fasteners and covers. The shape, texture and color of covers provide a styling and operational interface between the mechanical system and its users.\nThe assemblies that control movement are also called \"mechanisms.\" Mechanisms are generally classified as gears and gear trains, which includes belt drives and chain drives, cam and follower mechanisms, and linkages, though there are other special mechanisms such as clamping linkages, indexing mechanisms, escapements and friction devices such as brakes and clutches.\nThe number of degrees of freedom of a mechanism, or its mobility, depends on the number of links and joints and the types of joints used to construct the mechanism. The general mobility of a mechanism is the difference between the unconstrained freedom of the links and the number of constraints imposed by the joints. It is described by the Chebychev\u2013Gr\u00fcbler\u2013Kutzbach criterion.\nGears and gear trains.\nThe transmission of rotation between contacting toothed wheels can be traced back to the Antikythera mechanism of Greece and the south-pointing chariot of China. Illustrations by the renaissance scientist Georgius Agricola show gear trains with cylindrical teeth. The implementation of the involute tooth yielded a standard gear design that provides a constant speed ratio. Some important features of gears and gear trains are:\nCam and follower mechanisms.\nA cam and follower is formed by the direct contact of two specially shaped links. The driving link is called the cam (also see cam shaft) and the link that is driven through the direct contact of their surfaces is called the follower. The shape of the contacting surfaces of the cam and follower determines the movement of the mechanism.\nLinkages.\nA linkage is a collection of links connected by joints. Generally, the links are the structural elements and the joints allow movement. Perhaps the single most useful example is the planar four-bar linkage. However, there are many more special linkages:\nPlanar mechanism.\nA planar mechanism is a mechanical system that is constrained so the trajectories of points in all the bodies of the system lie on planes parallel to a ground plane. The rotational axes of hinged joints that connect the bodies in the system are perpendicular to this ground plane.\nSpherical mechanism.\nA spherical mechanism is a mechanical system in which the bodies move in a way that the trajectories of points in the system lie on concentric spheres. The rotational axes of hinged joints that connect the bodies in the system pass through the center of these circle.\nSpatial mechanism.\nA spatial mechanism is a mechanical system that has at least one body that moves in a way that its point trajectories are general space curves. The rotational axes of hinged joints that connect the bodies in the system form lines in space that do not intersect and have distinct common normals.\nFlexure mechanisms.\nA flexure mechanism consists of a series of rigid bodies connected by compliant elements (also known as flexure joints) that is designed to produce a geometrically well-defined motion upon application of a force.\nMachine elements.\nThe elementary mechanical components of a machine are termed machine elements. These elements consist of three basic types (i) \"structural components\" such as frame members, bearings, axles, splines, fasteners, seals, and lubricants, (ii) \"mechanisms\" that control movement in various ways such as gear trains, belt or chain drives, linkages, cam and follower systems, including brakes and clutches, and (iii) \"control components\" such as buttons, switches, indicators, sensors, actuators and computer controllers. While generally not considered to be a machine element, the shape, texture and color of covers are an important part of a machine that provide a styling and operational interface between the mechanical components of a machine and its users.\nStructural components.\nA number of machine elements provide important structural functions such as the frame, bearings, splines, spring and seals.\nControllers.\nControllers combine sensors, logic, and actuators to maintain the performance of components of a machine. Perhaps the best known is the flyball governor for a steam engine. Examples of these devices range from a thermostat that as temperature rises opens a valve to cooling water to speed controllers such as the cruise control system in an automobile. The programmable logic controller replaced relays and specialized control mechanisms with a programmable computer. Servomotors that accurately position a shaft in response to an electrical command are the actuators that make robotic systems possible.\nComputing machines.\nCharles Babbage designed machines to tabulate logarithms and other functions in 1837. His Difference engine can be considered an advanced mechanical calculator and his Analytical Engine a forerunner of the modern computer, though none of the larger designs were completed in Babbage's lifetime.\nThe Arithmometer and the Comptometer are mechanical computers that are precursors to modern digital computers. Models used to study modern computers are termed State machine and Turing machine.\nMolecular machines.\nThe biological molecule myosin reacts to ATP and ADP to alternately engage with an actin filament and change its shape in a way that exerts a force, and then disengage to reset its shape, or conformation. This acts as the molecular drive that causes muscle contraction. Similarly the biological molecule kinesin has two sections that alternately engage and disengage with microtubules causing the molecule to move along the microtubule and transport vesicles within the cell, and dynein, which moves cargo inside cells towards the nucleus and produces the axonemal beating of motile cilia and flagella. \"In effect, the motile cilium is a nanomachine composed of perhaps over 600 proteins in molecular complexes, many of which also function independently as nanomachines. Flexible linkers allow the mobile protein domains connected by them to recruit their binding partners and induce long-range allostery via . \" Other biological machines are responsible for energy production, for example ATP synthase which harnesses energy from proton gradients across membranes to drive a turbine-like motion used to synthesise ATP, the energy currency of a cell. Still other machines are responsible for gene expression, including DNA polymerases for replicating DNA, RNA polymerases for producing mRNA, the spliceosome for removing introns, and the ribosome for synthesising proteins. These machines and their nanoscale dynamics are far more complex than any molecular machines that have yet been artificially constructed. These molecules are increasingly considered to be nanomachines.\nResearchers have used DNA to construct nano-dimensioned four-bar linkages.\nImpact.\nMechanization and automation.\nMechanization (or mechanisation in BE) is providing human operators with machinery that assists them with the muscular requirements of work or displaces muscular work. In some fields, mechanization includes the use of hand tools. In modern usage, such as in engineering or economics, mechanization implies machinery more complex than hand tools and would not include simple devices such as an un-geared horse or donkey mill. Devices that cause speed changes or changes to or from reciprocating to rotary motion, using means such as gears, pulleys or sheaves and belts, shafts, cams and cranks, usually are considered machines. After electrification, when most small machinery was no longer hand powered, mechanization was synonymous with motorized machines.\nAutomation is the use of control systems and information technologies to reduce the need for human work in the production of goods and services. In the scope of industrialization, automation is a step beyond mechanization. Whereas mechanization provides human operators with machinery to assist them with the muscular requirements of work, automation greatly decreases the need for human sensory and mental requirements as well. Automation plays an increasingly important role in the world economy and in daily experience.\nAutomata.\nAn automaton (plural: automata or automatons) is a self-operating machine. The word is sometimes used to describe a robot, more specifically an autonomous robot. A \"Toy Automaton\" was patented in 1863.\nMechanics.\nUsher reports that Hero of Alexandria's treatise on \"Mechanics\" focussed on the study of lifting heavy weights. Today mechanics refers to the mathematical analysis of the forces and movement of a mechanical system, and consists of the study of the kinematics and dynamics of these systems.\nDynamics of machines.\nThe dynamic analysis of machines begins with a rigid-body model to determine reactions at the bearings, at which point the elasticity effects are included. The rigid-body dynamics studies the movement of systems of interconnected bodies under the action of external forces. The assumption that the bodies are rigid, which means that they do not deform under the action of applied forces, simplifies the analysis by reducing the parameters that describe the configuration of the system to the translation and rotation of reference frames attached to each body.\nThe dynamics of a rigid body system is defined by its equations of motion, which are derived using either Newtons laws of motion or Lagrangian mechanics. The solution of these equations of motion defines how the configuration of the system of rigid bodies changes as a function of time. The formulation and solution of rigid body dynamics is an important tool in the computer simulation of mechanical systems.\nKinematics of machines.\nThe dynamic analysis of a machine requires the determination of the movement, or kinematics, of its component parts, known as kinematic analysis. The assumption that the system is an assembly of rigid components allows rotational and translational movement to be modeled mathematically as Euclidean, or rigid, transformations. This allows the position, velocity and acceleration of all points in a component to be determined from these properties for a reference point, and the angular position, angular velocity and angular acceleration of the component.\nMachine design.\nMachine design refers to the procedures and techniques used to address the three phases of a machine's lifecycle:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51464", "revid": "1698931", "url": "https://en.wikipedia.org/wiki?curid=51464", "title": "Great Plains", "text": "Flat expanse in western North America\nThe Great Plains is a broad expanse of flatland in North America. The region stretches east of the Rocky Mountains, much of it covered in prairie, steppe, and grassland. They are the western part of the Interior Plains, which include the mixed grass prairie, the Tallgrass prairie between the Great Lakes and Appalachian Plateau, and the Taiga Plains and Boreal Plains ecozones in Northern Canada. \"Great Plains\", or Western Plains, is also the ecoregion of the Great Plains or the western portion of the Great Plains, some of which in the farthest west is known as the High Plains.\nThe Great Plains lie across both the Central United States and Western Canada, encompassing:\nUsage.\nThe term \"Great Plains\" is used in the United States to describe a sub-section of the even more vast Interior Plains physiographic division, which covers much of the interior of North America. It also has currency as a region of human geography, referring to the Plains Indians or the Plains states.\nIn Canada the term is rarely used; Natural Resources Canada, the government department responsible for official mapping, treats the Interior Plains as one unit consisting of several related plateaus and plains. There is no region referred to as the \"Great Plains\" in the \"Atlas of Canada\". In terms of human geography, the term \"prairie\" is more commonly used in Canada, and the region is known as the Canadian Prairies, prairie provinces or simply \"the prairies\".\nThe \"North American Environmental Atlas\", produced by the Commission for Environmental Cooperation, a North American Free Trade Agreement (NAFTA) agency composed of the geographical agencies of the Mexican, American, and Canadian governments, uses the \"Great Plains\" as an ecoregion synonymous with predominant prairies and grasslands rather than as physiographic region defined by topography. The Great Plains ecoregion includes five sub-regions: Temperate Prairies, West-Central Semi-Arid Prairies, South-Central Semi-Arid Prairies, Texas Louisiana Coastal Plains, and Tamaulipas-Texas Semi-Arid Plain, which overlap or expand upon other Great Plains designations.\nExtent.\nThe region is about east to west and north to south. Much of the region was home to American bison herds until they were hunted to near extinction during the mid/late-19th century. It has an area of approximately . Current thinking regarding the geographic boundaries of the Great Plains is shown by this https:// at the Center for Great Plains Studies, University of Nebraska\u2013Lincoln. This definition, however, is primarily ecological, not physiographic. The Boreal Plains of Western Canada are physiographically the same, but differentiated by their tundra and forest (rather than grassland) appearance.\nThe term \"Great Plains\", for the region west of about the 96th meridian west and east of the Rocky Mountains, was not generally used before the early 20th century. Nevin Fenneman's 1916 study \"Physiographic Subdivision of the United States\" brought the term Great Plains into more widespread usage. Before that the region was almost invariably called the High Plains, in contrast to the lower elevation Prairie Plains of the Midwestern states. Today the term \"High Plains\" is used for a subregion of the Great Plains. The term still remains little-used in Canada compared to the more common \"prairie\".\nGeography.\nThe Great Plains are the westernmost portion of the vast North American Interior Plains, which extend east to the Appalachian Plateau. The United States Geological Survey divides the Great Plains in the United States into ten physiographic subdivisions:\nFurther to this can be added Canadian physiographic sub-regions such as the Alberta Plain, Cypress Hills, Manitoba Escarpment (eastward), Manitoba Plain, Missouri Coteau (shared), Rocky Mountain Foothills (eastward), and Saskatchewan Plain.\nThe Great Plains consist of a broad stretch of country underlain by nearly horizontal strata extending westward from the 97th meridian west to the base of the Rocky Mountains, a distance of . It extends northward from the Mexican boundary far into Canada. Although the altitude of the plains increases gradually from or on the east to or near the mountains, the local relief is generally small. The semi-arid climate excludes tree growth and opens far-reaching views.\nThe plains are by no means a simple unit. They are of diverse structure and of various stages of erosional development. They are occasionally interrupted by buttes and escarpments. They are frequently broken by valleys. Yet on the whole, a broadly extended surface of moderate relief so often prevails that the name, Great Plains, for the region as a whole is well-deserved.\nThe western boundary of the plains is usually well-defined by the abrupt ascent of the mountains. The eastern boundary of the plains (in the United States) is more climatic than topographic. The line of of annual rainfall trends a little east of northward near the 97th meridian. If a boundary must be drawn where nature presents only a gradual transition, this rainfall line may be taken to divide the drier plains from the moister prairies. However, in Canada the eastern boundary of the plains is well defined by the presence of the Canadian Shield to the northeast.\nThe plains (within the United States) may be described in northern, intermediate, central and southern sections, in relation to certain peculiar features. In Canada, no such division is used: the climatic and vegetation regions are more impactful on human settlement than mere topography, and therefore the region is split into (from north to south), the taiga plains, boreal plains, aspen parkland, and prairie ecoregion regions.\nNorthern Great Plains.\nThe northern section of the Great Plains, north of latitude 44\u00b0, includes eastern Montana, eastern Wyoming, most of North Dakota and South Dakota, southwestern Minnesota and portions of the Canadian provinces including southeastern Alberta, southern Saskatchewan and southwestern Manitoba. The strata here are Cretaceous or early Tertiary, lying nearly horizontal. The surface is shown to be a plain of degradation by a gradual ascent here and there to the crest of a ragged escarpment, the escarpment-remnant of a resistant stratum. There are also the occasional lava-capped mesas and dike formed ridges, surmounting the general level by or more and manifestly demonstrating the widespread erosion of the surrounding plains. All these reliefs are more plentiful towards the mountains in central Montana. The peneplain is no longer in the cycle of erosion that witnessed its production. It appears to have suffered a regional uplift or increase in elevation, for the upper Missouri River and its branches no longer flow on the surface of the plain, but in well graded, maturely opened valleys, several hundred feet below the general level. A significant exception to the rule of mature valleys occurs, however, in the case of the Missouri, the largest river, which is broken by several falls on hard sandstones about east of the mountains. This peculiar feature is explained as the result of displacement of the river from a better graded preglacial valley by the Pleistocene ice sheet. Here, the ice sheet overspread the plains from the moderately elevated Canadian highlands far on the north-east, instead of from the much higher mountains nearby on the west. The present altitude of the plains near the mountain base is .\nThe northern plains are interrupted by several small mountain areas. The Black Hills, chiefly in western South Dakota, are the largest group. They rise like a large island from the sea, occupying an oval area of about north-south by east-west. At Black Elk Peak, they reach an altitude of and have an effective relief over the plains of This mountain mass is of flat-arched, dome-like structure, now well dissected by radiating consequent streams. The weaker uppermost strata have been eroded down to the level of the plains where their upturned edges are evenly truncated. The next following harder strata have been sufficiently eroded to disclose the core of underlying igneous and metamorphic crystalline rocks in about half of the domed area.\nIntermediate Great Plains.\nIn the intermediate section of the plains, between latitudes 44\u00b0 and 42\u00b0, including southern South Dakota and northern Nebraska, the erosion of certain large districts is peculiarly elaborate. Known as the Badlands, it is a minutely dissected form with a relief of a few hundred feet. This is due to several causes:\nCentral Great Plains.\nThe central section of the Great Plains, between latitudes 42\u00b0 and 36\u00b0, occupying eastern Colorado and western Kansas, is mostly a dissected fluviatile plain. That is, this section was once smoothly covered with a gently sloping plain of gravel and sand that had been spread far forward on a broad denuded area as a piedmont deposit by the rivers which issued from the mountains. Since then, it has been more or less dissected by the erosion of valleys. The central section of the plains thus presents a marked contrast to the northern section.\nWhile the northern section owes its smoothness to the removal of local gravels and sands from a formerly uneven surface by the action of degrading rivers and their inflowing tributaries, the southern section owes its smoothness to the deposition of imported gravels and sands upon a previously uneven surface by the action of aggrading rivers and their outgoing distributaries. The two sections are also alike in that residual eminences still here and there surmount the peneplain of the northern section, while the fluviatile plain of the central section completely buried the pre-existent relief. An exception to this statement must be made for the southwest, close to the mountains in southern Colorado, where some lava-capped mesas (Mesa de Maya, Raton Mesa) stand several thousand feet above the general plain level, and thus testify to the widespread erosion of this region before it was aggraded.\nSouthern Great Plains.\nThe southern section of the Great Plains, between latitudes 35.5\u00b0 and 25.5\u00b0, lies in western Texas, eastern New Mexico, and western Oklahoma. Like the central section, it is for the most part a dissected fluviatile plain. However, the lower lands which surround it on all sides place it in such strong relief that it stands up as a table-land, known from the time of Mexican occupation as the Llano Estacado. It measures roughly east-west and north-south. It is of very irregular outline, narrowing to the south. Its altitude is at the highest western point, nearest the mountains whence its gravels were supplied. From there, it slopes southeastward at a decreasing rate, first about , then about , to its eastern and southern borders, where it is in altitude. Like the High Plains farther north, it is extraordinarily smooth.\nIt is very dry, except for occasional shallow and temporary water sheets after rains. Llano is separated from the plains on the north by the mature consequent valley of the Canadian River, and from the mountains on the west by the broad and probably mature valley of the Pecos River. On the east, it is strongly undercut by the retrogressive erosion of the headwaters of the Red, Brazos, and Colorado rivers of Texas and presents a ragged escarpment approximately high, overlooking the central denuded area of that state. There, between the Brazos and Colorado rivers, occurs a series of isolated outliers capped by limestone that underlies both the Llano Uplift on the west and the Grand Prairies escarpment on the east. The southern and narrow part of the table-land, called the Edwards Plateau, is more dissected than the rest, and falls off to the south in a frayed-out fault scarp. This scarp overlooks the coastal plain of the Rio Grande embayment. The central denuded area, east of the Llano, resembles the east-central section of the plains in exposing older rocks. Between these two similar areas, in the space limited by the Canadian and Red Rivers, rise the subdued forms of the Wichita Mountains in Oklahoma, the westernmost member of the Ouachita system.\nOther terminology.\nThe term \"Western Plains\" is used to describe the ecoregion of the Great Plains,\n or alternatively the western portion of the Great Plains.\nNatural history.\nClimate.\nIn general, the Great Plains have a wide range of weather, with very cold and harsh winters and very hot and humid summers. Wind speeds are often very high, especially in winter.\nThe 100th meridian roughly corresponds with the line that divides the Great Plains into an area that receives or more of rainfall per year and an area that receives less than . In this context, the High Plains, as well as Southern Alberta, south-western Saskatchewan and Eastern Montana are mainly semi arid steppe land and are generally characterised by rangeland or marginal farmland. The region (especially the High Plains) is periodically subjected to extended periods of drought; high winds in the region may then generate devastating dust storms. The eastern Great Plains near the eastern boundary falls in the humid subtropical climate zone in the southern areas, and the northern and central areas fall in the humid continental climate.\nMany thunderstorms occur in the plains in the spring through summer. The southeastern portion of the Great Plains is the most tornado active area in the world and is sometimes referred to as Tornado Alley.\nFlora.\nThe Great Plains are part of the floristic North American Prairies province, which extends from the Rocky Mountains to the Appalachians.\nFauna.\nMammals.\nAlthough the American bison (\"Bison bison\") historically ranged throughout much of North America (from New York to Oregon and Canada to northern Mexico), they are strongly associated with the Great Plains where they once roamed in immense herds. Pronghorn (\"Antilocapra americana\") range into western areas of the region. The black-tailed prairie dog (\"Cynomys ludovicianus\") is another iconic species among several rodents that are linked to the region including the thirteen-lined ground squirrel (\"Ictidomys tridecemlineatus\"), spotted ground squirrel (\"Xerospermophilus spilosoma\"), Franklin's ground squirrel (\"Poliocitellus franklinii\"), plains pocket gopher (\"Geomys bursarius\"), hispid pocket mouse (\"Chaetodipus hispidus\"), olive-backed pocket mouse (\"Perognathus fasciatus\"), plains pocket mouse (\"Perognathus flavescens\"), and plains harvest mouse (\"Reithrodontomys montanus\"), Two carnivores associated with the Great Plains include the swift fox (\"Vulpes velox\") and the endangered black-footed ferret (\"Mustela nigripes\").\nBirds.\nThe lesser prairie-chicken (\"Tympanuchus pallidicinctus\") is endemic to the Great Plains and the distribution of the greater prairie-chicken (\"Tympanuchus cupido\") predominantly occurs in the region, although the latter historically ranged further eastward. The Harris's sparrow (\"Zonotrichia querula\") spends winter months in southern areas of the region. Other species migrate from the south in the spring and spend their breeding season on the plains, including the white-faced ibis (\"Plegadis chihi\"), mountain plover (\"Charadrius montanus\"), marbled godwit (\"Limosa fedoa\"), Sprague's pipit (\"Anthus spragueii\"), Cassin's sparrow (\"Peucaea cassinii\"), Baird's sparrow (\"Centronyx bairdii\"), lark bunting (\"Calamospiza melanocorys\"), chestnut-collared longspur (\"Calcarius ornatus\"), thick-billed longspur or McCown's longspur (\"Rhynchophanes mccownii\"), and dickcissel (\"Spiza americana\").\nReptiles.\nThe prairie rattlesnake (\"Crotalus viridis\") ranges throughout much of the Great Plains and into the valleys and lower elevations of the eastern Rocky Mountains and portions of the American southwest. Other snakes include the plains hog-nosed snake (\"Heterodon nasicus\"), western milksnake (\"Lampropeltis gentilis\"), Great Plains ratsnake (\"Pantherophis emoryi\"), bullsnake (\"Pituophis catenifer sayi\"), plains black-headed snake (\"Tantilla nigriceps\"), plains gartersnake (\"Thamnophis radix\"), and lined snake (\"Tropidoclonion lineatum\"). Reptile diversity increases significantly in southern regions of the Great Plains. The ornate box turtle (\"Terrapene ornata\") and Great Plains skink (\"Plestiodon obsoletus\") occur in southern areas.\nAmphibians.\nAlthough few salamanders are strongly associated with the region, the western tiger salamander (\"Ambystoma mavortium\") ranges through much of the Great Plains and the Rocky Mountains, as does the Rocky Mountain toad (\"Anaxyrus w. woodhousi\"). Other anurans related to region include the Great Plains toad (\"Anaxyrus cognatus\"), plains leopard frog (\"Lithobates blairi\"), and plains spadefoot toad (\"Spea bombifrons\").\nFish.\nSome species predominantly associated with various river basins in the Great Plains include sturgeon chub (\"Macrhybopsis gelida\"), peppered chub (\"Macrhybopsis tetranema\"), prairie chub (\"Macrhybopsis australis\"), western silvery minnow (\"Hybognathus argyritis\"), plains minnow (\"Hybognathus placitus\"), smalleye shiner (\"Notropis buccula\"), Arkansas River shiner (\"Notropis girardi\"), Red River shiner (\"Notropis bairdi\"), Topeka shiner (\"Notropis topeka\"), plains topminnow (\"Fundulus sciadicus\"), plains killifish (\"Fundulus zebrinus\"), Red River pupfish (\"Cyprinodon rubrofluviatilis\"), and Arkansas darter (\"Etheostoma cragini\").\nInvertebrates.\nThe Great Plains also has many invertebrate species living here both alive and extinct such as the American burying beetle (\"Nicrophorus americus\"), Salt Creek tiger beetle (\"Cinidela nevadica lincolniana\"), Great Plains giant tiger beetle (\"Amblycheila chylindriformis\"), \"Microstylum morosum\", bean leaf beetle (\"Cerotoma trifurcata\"), Great Plains camel cricket (\"Daihinia brevipes\"), and the Great Plains spittlebug (\"Lepyronia gibbosa\"). Some species in the Great Plains have gone extinct like the Rocky Mountain locust (\"Melanoplus spretus\").\nPaleontology.\nDuring the Cretaceous Period (145\u201366\u00a0million years ago), the Great Plains were covered by a shallow inland sea called the Western Interior Seaway. However, during the Late Cretaceous to the Paleocene (65\u201355\u00a0million years ago), the seaway had begun to recede, leaving behind thick marine deposits and a relatively flat terrain which the seaway had once occupied.\nDuring the Cenozoic era, specifically about 25\u00a0million years ago during the Miocene and Pliocene epochs, the continental climate became favorable to the evolution of grasslands. Existing forest biomes declined and grasslands became much more widespread. The grasslands provided a new niche for mammals, including many ungulates and glires, that switched from browsing diets to grazing diets. Traditionally, the spread of grasslands and the development of grazers have been strongly linked. However, an examination of mammalian teeth suggests that it is the open, gritty habitat and not the grass itself which is linked to diet changes in mammals, giving rise to the \"grit, not grass\" hypothesis.\nPaleontological finds in the area have yielded bones of mammoths, saber-toothed cats and other ancient animals, as well as dozens of other megafauna (large animals over ) \u2013 such as giant sloths, horses, mastodons, and American lion \u2013 that dominated the area of the ancient Great Plains for thousands to millions of years. The vast majority of these animals became extinct in North America at the end of the Pleistocene (around 13,000 years ago).\nA number of significant fossil sites are located in the Great Plains including Agate Fossil Beds National Monument (Nebraska), Ashfall Fossil Beds (Nebraska), Clayton Lake State Park (New Mexico), Dinosaur Valley State Park (Texas), Hudson-Meng Bison Kill (Nebraska), Makoshika State Park (Montana), and The Mammoth Site (South Dakota).\nPublic and protected lands.\nPublic and protected lands in the Great Plains include National Parks and National Monuments, administers by the National Park Service with the responsibility of preserving ecological and historical places and making them available to the public. The U.S. Fish &amp; Wildlife Service manages the National Wildlife Refuges, with the primary responsibility of conserving and protecting fish, wildlife, plants, and habitat in the public trust. Both are agencies of the Department of the Interior.\nIn contrast, U.S. Forest Service, an agency of the U. S. Department of Agriculture, administers the National Forests and National Grasslands, under a multiple-use concept. By law, the U.S. Forest Service must consider all resources, with no single resource emphasized to the detriment of others, including water, soil, grazing, timber harvesting, and minerals (mining and drilling), as well as recreation and conservation of fish and wildlife. Each individual state also administers state lands, typically smaller areas, for various purposes including conservation and recreation.\nGrasslands are among the least protected biomes. Humans have converted much of the prairies for agricultural purposes or to create pastures. Several of the protected lands in the region are centered around aberrant and uncharacteristic features of the region, such as mountains, outcrops, and canyons (e.g. Devil's Tower National Monument, Wind Cave National Park, Scotts Bluff National Monument), and as splendid and worthy as they are, they are not primarily focused on conserving the plains and prairies.\nUnited States:\nCanada:\nEcological changes.\nthe Great Plains biome is found to be at the brink of collapse due to woody plant encroachment, with 62% of Northern American grassland lost to date.\nHistory to 1850.\nOriginal American contact.\nThe first Peoples (Paleo-Indians) arrived on the Great Plains thousands of years ago. The introduction of corn around 800 CE allowed the development of the mound-building Mississippian culture along rivers that crossed the Great Plains and that included trade networks west to the Rocky Mountains. Mississippians settled the Great Plains at sites now in Oklahoma and South Dakota.\nSiouan language speakers may have originated in the lower Mississippi River region. They were agriculturalists and may have been part of the Mound Builder civilization during the 9th\u201312th centuries.\nPressure from other Indian tribes, themselves driven west and south by the encroachment of European settlers as well as economic incentives such as the fur trade, alongside the arrival of the horse and firearms from Europe pushed multiple tribes onto the Great Plains.\nAmong those to have lived on the Great Plains were the Blackfoot, Crow, Sioux, Cheyenne, Arapaho, Comanche, and others. Eastern portions of the Great Plains were inhabited by tribes who lived at Etzanoa and in semi-permanent villages of earth lodges, such as the Arikara, Mandan, Pawnee, and Wichita.\nWars with the Ojibwe and Cree peoples pushed the Lakota (Teton Sioux) west onto the Great Plains in the mid- to late-17th century. The Shoshone originated in the western Great Basin and spread north and east into present-day Idaho and Wyoming. By 1500, some Eastern Shoshone had crossed the Rocky Mountains into the Great Plains. After 1750, warfare and pressure from the Blackfoot, Crow, Lakota, Cheyenne, and Arapaho pushed Eastern Shoshone south and westward. Some of them moved as far south as Texas, emerging as the Comanche by 1700.\nArrival of horses.\nThe first known contact between Europeans and Indians in the Great Plains occurred in what is now Texas, Kansas, and Nebraska from 1540 to 1542 with the arrival of Francisco V\u00e1zquez de Coronado, a Spanish conquistador. In that same period, Hernando de Soto crossed a west-northwest direction in what is now Oklahoma and Texas which is now known as the De Soto Trail. The Spanish thought that the Great Plains were the location of the mythological \"Quivira and C\u00edbola\", a place said to be rich in gold.\nPeople in the southwest began to acquire horses in the 16th century by trading or stealing them from Spanish colonists in New Mexico. As horse culture moved northward, the Comanche were among the first to commit to a fully mounted nomadic lifestyle. This occurred by the 1730s, when they had acquired enough horses to put all their people on horseback.\nThe real beginning of the horse culture of the plains began with the Pueblo Revolt of 1680 in New Mexico and the capture of thousands of horses and other livestock. In 1683 a Spanish expedition into Texas found horses among Native people. In 1690, a few horses were found by the Spanish among the Indians living at the mouth of the Colorado River of Texas and the Caddo of eastern Texas had a sizeable number.\nThe French explorer Claude Charles Du Tisne found 300 horses among the Wichita on the Verdigris River in 1719, but they were still not plentiful. Another Frenchman, Bourgmont, could only buy seven at a high price from the Kaw in 1724, indicating that horses were still scarce among tribes in Kansas. By 1770, that Plains Indians culture was mature, consisting of mounted buffalo-hunting nomads from Saskatchewan and Alberta southward nearly to the Rio Grande.\nThe milder winters of the southern Plains favored a pastoral economy by the Indians. On the northeastern Plains of Canada, the Indians were less favored, with families owning fewer horses, remaining more dependent upon dogs for transporting goods, and hunting bison on foot. The scarcity of horses in the north encouraged raiding and warfare in competition for the relatively small number of horses that survived the severe winters.\nComanche power peaked in the 1840s when they conducted large-scale raids hundreds of miles into Mexico proper, while also warring against the Anglo-Americans and Tejanos who had settled in independent Texas.\nFur trade.\nThe fur trade brought thousands of colonial settlers into the Great Plains over the next 100 years. Fur trappers made their way across much of the region, making regular contacts with Indians.\nThe Hudson's Bay Company (HBC) had first been granted in 1670 a commercial monopoly over the huge Hudson Bay drainage area known as Rupert's Land covering a northern portion of the Great Plains. The North West Company fur trade incumbent had also been present in the area until acquired by the HBC during the early 1820s.\nThe United States acquired the Louisiana Purchase in 1803 and conducted the Lewis and Clark Expedition in 1804\u20131806, and more information became available concerning the Plains, and various pioneers entered the areas. Fur trading posts were often the basis of later settlements. Through the 19th century, more settlers migrated to the Great Plains as part of a vast westward expansion of population, and new settlements became dotted across the Great Plains.\nThe settlers also brought diseases against which the Indians had no resistance. Between a half and two-thirds of the Plains Indians are thought to have died of smallpox by the time of the Louisiana Purchase. The 1837 Great Plains smallpox epidemic spread across the Great Plains, killing many thousands between 1837 and 1840. In the end, it is estimated that two-thirds of the Blackfoot population died, along with half of the Assiniboines and Arikaras, a third of the Crows, and a quarter of the Pawnees.\nEuropean settlement.\nBeginning in 1821, the Santa Fe Trail ran from the Missouri River to New Mexico, skirting north of Comancheria. Beginning in the 1830s, the Oregon Trail led from the Missouri River across the Great Plains.\nMuch of the Great Plains became open range where cattle roamed free, hosting ranching operations where anyone was free to run cattle. In the spring and fall, ranchers held roundups where their cowboys branded new calves, treated animals, and sorted the cattle for sale. Such ranching began in Texas and gradually moved northward. Between 1866 and 1895, cowboys herded 10\u00a0million cattle north to rail heads such as Dodge City, Kansas and Ogallala, Nebraska; from there, cattle were shipped east.\nPassage of the Kansas-Nebraska Act in 1854 opened both territories to White settlement. The Homestead Acts of 1862 further encouraged settlement and agricultural development in the Great Plains; the population of Nebraska, for instance, increased from under 30,000 in 1860 to over one million in 1890. A homesteader was permitted to claim up to of land for only a small filing fee, provided that he or she lived on the land for a period of five years and cultivated it. The provisions were expanded under the Kinkaid Act of 1904 to include a homestead of an entire section. Hundreds of thousands of people claimed such homesteads, sometimes building houses out of the very turf of the land. Many of them were not skilled farmers, and failures were frequent. The Canadian \"Dominion Lands Act\" of 1871 served a similar function for establishing homesteads on the prairies in Canada.\nRailroads.\nAfter 1870, the new railroads across the Plains brought hunters who killed off almost all the bison for their hides. The railroads offered attractive packages of land and transportation to American farmers, who rushed to settle the land. They also took advantage of the homestead laws to obtain farms. Land speculators and local boosters identified many potential towns, and those reached by the railroad had a chance, while the others became ghost towns. Towns flourished if they were favored by proximity to the railroad.\nThe population of Minnesota, Dakota, Nebraska, and Kansas experienced significant growth during the 1870s. The total population in these states grew from 1.0 million in 1870 to 2.4 million in 1880, more than doubling in just 10 years. The number of farms in the region tripled, increasing from 99,000 in 1870 to 302,000 in 1880. The improved acreage (land under cultivation) quintupled, rising from 5.0 million acres to 24.6 million acres during the same period. the new settlers mostly purchased land on generous terms from transcontinental railroads that were given land grants by Washington. They focused on wheat and cattle. This rapid population influx and agricultural expansion was a hallmark of the settlement and development of the Great Plains in the late 19th century, as the region attracted waves of new settlers from Germany, Scandinavia, and Russia, as well as farmers who sold land in older states to move to larger farms.\nFirst American settlements.\nThe first White settlements in the Great Plains were forts, particularly along the Santa Fe Trail, and trading posts. Some of the first built were: &lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSocial life.\nThe railroads opened up the Great Plains for settlement, making it possible to ship wheat and other crops at low cost to the urban markets in the East and overseas. Homestead land was free for American settlers. Railroads sold their land at cheap rates to immigrants in the expectation that they would generate traffic as soon as farms were established. Immigrants poured in, especially from Germany and Scandinavia. On the plains, very few single men attempted to operate a farm or ranch by themselves; they understood the need for a hard-working wife and numerous children to handle the many responsibilities. During the early years of settlement, farm women played an integral role in assuring family survival by working outdoors. After approximately one generation, women increasingly left the fields, thus redefining their roles within the family. New technology encouraged women to turn to domestic roles, including sewing and washing machines. Media and government extension agents promoted the \"scientific housekeeping\" movement, along with county fairs which featured achievements in home cookery and canning, advice columns for women regarding farm book keeping, and home economics courses in the schools.\nThe eastern image of farm life in the prairies emphasized the isolation of the lonely farmer and wife, yet plains residents created busy social lives for themselves. They often sponsored activities which combined work, food, and entertainment, such as barn raisings, corn huskings, quilting bees, Grange meetings, church activities and school functions. Women organized shared meals and potluck events, as well as extended visits among families.\n20th century.\nProgressive Era.\nThe Progressive movement was a reform movement that took place in all parts of the country during the late 19th and early 20th centuries. The movement sought to address social, political, and economic problems that had arisen as a result of the Industrial Revolution. Progressives believed that the government could play a role in solving these problems by regulating businesses, protecting workers, and providing social welfare programs.\nThe Plains states were a hotbed of Progressive activity. Many of the reforms that were enacted at the national level were first implemented in the Plains states. For example, the initiative and referendum process, which allows voters to directly enact laws, was first adopted in South Dakota in 1898. The direct primary, which allows voters to choose their party's candidates in primary elections, was first adopted in Wisconsin in 1903.\nProgressive reformers in the Great Plains focused on high priority issues, especially:\nProgressives in the Great Plains were more likely to support direct democracy, woman suffrage, and Prohibition than their counterparts elsewhere. They were also more likely to favor state-owned enterprises, especially those devoted to economic development. Plains progressivism was more radical than progressivism in eastern states, with a greater focus on direct democracy, woman suffrage, and Prohibition. Plains progressives were more isolationist regarding foreign policy, largely in response to the large German and Scandinavian elements. Socialists were more active than elsewhere, Progressive reforms had a significant long-term impact on the region. They helped to improve the lives of workers, farmers, and consumers. They also helped to make the Plains states more democratic and responsive to the needs of their citizens.\nDust Bowl and water resources.\nThe region roughly centered on the Oklahoma Panhandle was known as the Dust Bowl during the late 1920s and early 1930s, including southeastern Colorado, southwestern Kansas, the Texas Panhandle, and extreme northeastern New Mexico. The effects of an extended drought, inappropriate cultivation, and financial crises of the Great Depression forced many farmers off the land throughout the Great Plains.\nFrom the 1950s on, many areas of the Great Plains have become productive crop-growing areas because of extensive irrigation on large land-holdings. The United States is a major exporter of agricultural products. The southern portion of the Great Plains lies over the Ogallala Aquifer, a huge underground layer of water-bearing strata. Center pivot irrigation is used extensively in drier sections of the Great Plains, resulting in aquifer depletion at a rate that is greater than the ground's ability to recharge.\nPopulation decline.\nThe rural Plains have lost a third of their population since 1920. Several hundred thousand square miles of the Great Plains have fewer than , the density standard that Frederick Jackson Turner used to declare the American frontier \"closed\" in 1893. Many have fewer than . According to Kansas historian Daniel Fitzgerald, there are more than 6,000 ghost towns in Kansas alone. This problem is often exacerbated by the consolidation of farms and the difficulty of attracting modern industry to the region. In addition, the smaller school-age population has forced the consolidation of school districts and the closure of high schools in some communities. The continuing population loss has led some to suggest that the current use of the drier parts of the Great Plains is not sustainable, and there has been a proposal to return approximately of these drier parts to native prairie land as a Buffalo Commons.\nWind power.\nThe Great Plains contributes substantially to wind power in the United States. T. Boone Pickens developed wind farms after a career as a petroleum executive, and he called for the U.S. to invest $1\u00a0trillion to build an additional 200,000\u00a0MW of wind power in the Plains as part of his Pickens Plan. He cited Sweetwater, Texas, as an example of economic revitalization driven by wind power development.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51465", "revid": "33625371", "url": "https://en.wikipedia.org/wiki?curid=51465", "title": "Moli\u00e8re", "text": "French playwright and actor (1622\u20131673)\nJean-Baptiste Poquelin (; 15 January 1622 (baptised) \u2013 17 February 1673), known by his stage name Moli\u00e8re (, ; ), was a French playwright, actor, and poet, widely regarded as one of the great writers in the French language and world literature. His extant works include comedies, farces, tragicomedies, com\u00e9die-ballets, and more. His plays have been translated into every major living language and are performed at the Com\u00e9die-Fran\u00e7aise more often than those of any other playwright today. His influence is such that the French language is often referred to as the \"language of Moli\u00e8re\".\nBorn into a prosperous family and having studied at the Coll\u00e8ge de Clermont (now Lyc\u00e9e Louis-le-Grand), Moli\u00e8re was well suited to begin a life in the theatre. Thirteen years as an itinerant actor helped him polish his comedic abilities while he began writing, combining Commedia dell'arte elements with the more refined French comedy.\nThrough the patronage of aristocrats including Philippe I, Duke of Orl\u00e9ans\u2014the brother of Louis XIV\u2014Moli\u00e8re procured a command performance before the King at the Louvre. Performing a classic play by Pierre Corneille and a farce of his own, \"The Doctor in Love\", Moli\u00e8re was granted the use of the \"grande salle\" of the Petit-Bourbon near the Louvre, a spacious room appointed for theatrical performances. Later, he was granted the use of the theatre in the Palais-Royal. In both locations, Moli\u00e8re found success among Parisians with plays such as \"The Affected Ladies\", \"The School for Husbands\", and \"The School for Wives\". This royal favour brought a royal pension to his troupe and the title \"Troupe du Roi\" (\"The King's Troupe\"). Moli\u00e8re continued as the official author of court entertainments.\nDespite the adulation of the court and Parisians, Moli\u00e8re's satires attracted criticism from other circles. For \"Tartuffe's\" impiety, the Catholic Church in France denounced this study of religious hypocrisy, which was followed by a ban by the Parlement, while \"Dom Juan\" was withdrawn and never restaged by Moli\u00e8re. His hard work in so many theatrical capacities took its toll on his health and, by 1667, he was forced to take a break from the stage. In 1673, during a production of his final play, \"The Imaginary Invalid\", Moli\u00e8re, who suffered from pulmonary tuberculosis, was seized by a coughing fit and a haemorrhage while playing the hypochondriac Argan; he finished the performance but collapsed again and died a few hours later.\nLife.\nMoli\u00e8re was born in Paris shortly before his christening as Jean Poquelin on 15 January 1622. Known as Jean-Baptiste, he was the first son of Jean Poquelin and Marie Cress\u00e9, who had married on 27 April 1621. His mother was the daughter of a prosperous bourgeois family. Upon seeing him for the first time, a maid exclaimed, \"Le nez!\", a reference to the infant's large nose. Moli\u00e8re was called \"Le Nez\" by his family from that time. He lost his mother when he was 10, and he does not seem to have been particularly close to his father. After his mother's death, he lived with his father above the \"Pavillon des Singes\" on the rue Saint-Honor\u00e9, an affluent area of Paris. It is likely that his education commenced with studies at a Parisian elementary school, followed by his enrollment in the prestigious Jesuit Coll\u00e8ge de Clermont, where he completed his studies in a strict academic environment and got a first taste of life on the stage.\nIn 1631, his father Jean Poquelin purchased from the court of Louis XIII the posts of \"\"valet de chambre ordinaire et tapissier du Roi\"\" (\"valet of the King's chamber and keeper of carpets and upholstery\"). His son assumed the same posts in 1641. The title required only three months' work and an initial cost of 1,200 livres; the title paid 300 livres a year and provided a number of lucrative contracts. Moli\u00e8re also studied as a provincial lawyer some time around 1642, probably in Orl\u00e9ans, but it is not documented that he ever qualified. So far he had followed his father's plans, which had served him well; he had mingled with nobility at the Coll\u00e8ge de Clermont and seemed destined for a career in office.\nIn June 1643, when Moli\u00e8re was 21, he decided to abandon his social class and pursue a career on the stage. Taking leave of his father, he joined the actress Madeleine B\u00e9jart, with whom he had crossed paths before, and founded the Illustre Th\u00e9\u00e2tre with 630 livres. They were later joined by Madeleine's brother and sister.\nThe theatre troupe went bankrupt in 1645. Moli\u00e8re had become head of the troupe, due in part, perhaps, to his acting prowess and his legal training. However, the troupe had acquired large debts, mostly for the rent of the theatre (a court for jeu de paume), for which they owed 2000 livres. Historians differ as to whether his father or the lover of a member of his troupe paid his debts; either way, after a 24-hour stint in prison he returned to the acting circuit. It was at this time that he began to use the pseudonym Moli\u00e8re, possibly inspired by a small village of the same name in the Midi near Le Vigan. It was likely that he changed his name to spare his father the shame of having an actor in the family (actors, although no longer vilified by the state under Louis XIV, were still not allowed to be buried in sacred ground).\nAfter his imprisonment, he and Madeleine began a theatrical circuit of the provinces with a new theatre troupe; this life was to last about twelve years, during which he initially played in the company of Charles Dufresne, and subsequently created a company of his own, which had sufficient success and obtained the patronage of Philippe I, Duke of Orl\u00e9ans. Few plays survive from this period. The most noteworthy are \"L'\u00c9tourdi ou les Contretemps\" \"(The Bungler)\" and \"Le Docteur Amoureux\" \"(The Doctor in Love)\"; with these two plays, Moli\u00e8re moved away from the heavy influence of the Italian improvisational Commedia dell'arte, and displayed his talent for mockery. In the course of his travels he met Armand, Prince of Conti, the governor of Languedoc, who became his patron, and named his company after him. This friendship later ended when Armand, having contracted syphilis from a courtesan, turned toward religion and joined Moli\u00e8re's enemies in the \"Parti des D\u00e9vots\" and the \"Compagnie de Saint Sacrement\".\nIn Lyon, Mademoiselle Du Parc, known as Marquise, joined the company. Marquise was courted, in vain, by Pierre Corneille and later became the lover of Jean Racine. Racine offered Moli\u00e8re his tragedy \"Th\u00e9ag\u00e8ne et Charicl\u00e9e\" (one of the early works he wrote after he had abandoned his theology studies), but Moli\u00e8re would not perform it, though he encouraged Racine to pursue his artistic career.\nReturn to Paris.\nMoli\u00e8re was forced to reach Paris in stages, staying outside for a few weeks in order to promote himself with society gentlemen and allow his reputation to feed in to Paris. Moli\u00e8re reached Paris in 1658 and performed in front of the King at the Louvre (then for rent as a theatre) in Corneille's tragedy \"Nicom\u00e8de\" and in the farce \"Le Docteur Amoureux\" with some success. He was awarded the title of \"Troupe de Monsieur\" (Monsieur being the honorific for the king's brother Philippe I, Duke of Orl\u00e9ans). With the help of Monsieur, his company was allowed to share the theatre in the large hall of the Petit-Bourbon with the Italian Commedia dell'arte company of Tiberio Fiorillo, famous for the character of Scaramouche. (The two companies performed in the theatre on different nights.) The premiere of Moli\u00e8re's \"Les Pr\u00e9cieuses Ridicules\" (\"The Affected Young Ladies\") took place at the Petit-Bourbon on 18 November 1659.\n\"Les Pr\u00e9cieuses Ridicules\" was the first of Moli\u00e8re's many attempts to satirize certain societal mannerisms and affectations then common in France. It is widely accepted that the plot was based on Samuel Chappuzeau's \"Le Cercle des Femmes\" of 1656. He primarily mocks the Acad\u00e9mie Fran\u00e7aise, a group created by Richelieu under a royal patent to establish the rules of the fledgling French theatre. The Acad\u00e9mie preached unity of time, action, and styles of verse. Moli\u00e8re is often associated with the claim that comedy \"castigat ridendo mores\" or \"criticises customs through humour\" (a phrase in fact coined by his contemporary Jean de Santeuil and sometimes mistaken for a classical Latin proverb).\nHeight of fame.\nDespite his own preference for tragedy, which he had tried to further with the Illustre Th\u00e9\u00e2tre, Moli\u00e8re became famous for his farces, which were generally in one act and performed after the tragedy. Some of these farces were only partly written, and were played in the style of Commedia dell'arte with improvisation over a canovaccio (a vague plot outline). He began to write full, five-act comedies in verse (\"L'\u00c9tourdi\" (Lyon, 1654) and \"Le d\u00e9pit amoureux\" (B\u00e9ziers, 1656)), which although immersed in the gags of contemporary Italian troupes, were successful as part of Madeleine B\u00e9jart and Moli\u00e8re's plans to win aristocratic patronage and, ultimately, move the troupe to a position in a Paris theater-venue. Later Moli\u00e8re concentrated on writing musical comedies, in which the drama is interrupted by songs and/or dances, but for years the fundamentals of numerous comedy-traditions would remain strong, especially Italian (e.g. the semi-improvisatory style that in the 1750s writers started calling commedia dell'arte), Spanish, and French plays, all also drawing on classical models (e.g. Plautus and Terence), especially the trope of the clever slave/servant.\n\"Les pr\u00e9cieuses ridicules\" won Moli\u00e8re the attention and the criticism of many, but it was not a popular success. He then asked Fiorillo to teach him the techniques of Commedia dell'arte. His 1660 play \"Sganarelle, ou Le Cocu imaginaire\" (\"The Imaginary Cuckold\") seems to be a tribute both to Commedia dell'arte and to his teacher. Its theme of marital relationships dramatizes Moli\u00e8re's pessimistic views on the falsity inherent in human relationships. This view is also evident in his later works and was a source of inspiration for many later authors, including (with different effect), 20th century Nobel Prize winner Luigi Pirandello. It describes a kind of round dance where two couples believe that each of their partners has been betrayed by the other's and is the first in Moli\u00e8re's \"Jealousy series\", which includes \"Dom Garcie de Navarre\", \"L'\u00c9cole des maris\" and \"L'\u00c9cole des femmes\".\nIn 1660, the Petit-Bourbon was demolished to make way for the eastern expansion of the Louvre, but Moli\u00e8re's company was allowed to move into the abandoned theatre in the east wing of the Palais-Royal. After a period of refurbishment they opened there on 20 January 1661. In order to please his patron, Monsieur, who was so enthralled with entertainment and art that he was soon excluded from state affairs, Moli\u00e8re wrote and played \"Dom Garcie de Navarre ou Le Prince jaloux\" (\"The Jealous Prince\", 4 February 1661), a heroic comedy derived from a work of Cicognini. Two other comedies of the same year were the successful \"L'\u00c9cole des maris\" (\"The School for Husbands\") and \"Les F\u00e2cheux\" (\"The Bores\"), subtitled \"Com\u00e9die faite pour les divertissements du Roi\" (a comedy for the King's amusements) because it was performed during a series of parties that Nicolas Fouquet gave in honor of the sovereign. These entertainments led Jean-Baptiste Colbert to demand the arrest of Fouquet for wasting public money, and he was condemned to life imprisonment.\nOn 20 February 1662, Moli\u00e8re married Armande B\u00e9jart, whom he believed to be the sister of Madeleine. (She may have been her illegitimate daughter with the Duke of Modena.) The same year, he premiered \"L'\u00c9cole des femmes\" (\"The School for Wives\"), subsequently regarded as a masterpiece. It poked fun at the limited education that was given to daughters of rich families and reflected Moli\u00e8re's own marriage. Both this work and his marriage attracted much criticism. The play sparked the protest called the \"Quarrel of L'\u00c9cole des femmes\". On the artistic side he responded with two lesser-known works: \"La Critique de \"L'\u00c9cole des femmes\"\", in which he imagined the spectators of his previous work attending it. The piece mocks the people who had criticised \"L'\u00c9cole des femmes\" by showing them at dinner after watching the play; it addresses all the criticism raised about the piece by presenting the critics' arguments and then dismissing them. This was the so-called \"Guerre comique\" (\"War of Comedy\"), in which the opposite side was taken by writers like Donneau de Vis\u00e9, Edm\u00e9 Boursault, and Montfleury.\nHowever, more serious opposition was brewing, focusing on Moli\u00e8re's politics and his personal life. A so-called \"parti des D\u00e9vots\" arose in French high society, who protested against Moli\u00e8re's excessive \"realism\" and irreverence, which were causing some embarrassment. These people accused Moli\u00e8re of having married his daughter. The Prince of Conti, once Moli\u00e8re's friend, joined them. Moli\u00e8re had other enemies, too, among them the Jansenists and some traditional authors. However, the king expressed support for the Moli\u00e8re, granting him a pension and agreeing to be the godfather of Moli\u00e8re's first son. Boileau also supported him through statements that he included in his \"Art po\u00e9tique\".\nMoli\u00e8re's friendship with Jean-Baptiste Lully influenced him towards writing his \"Le Mariage forc\u00e9\" and \"La Princesse d'\u00c9lide\" (subtitled as \"Com\u00e9die galante m\u00eal\u00e9e de musique et d'entr\u00e9es de ballet\"), written for \"Les Plaisirs de l'Isle enchant\u00e9e\", royal \"divertissements\", at the Palace of Versailles.\n\"Tartuffe, ou L'Imposteur\" was also performed at Versailles, in 1664, and created the greatest scandal of Moli\u00e8re's artistic career. Its depiction of the hypocrisy of the dominant classes was taken as an outrage and violently contested. It also aroused the wrath of the Jansenists and the play was banned.\nMoli\u00e8re was always careful not to attack the institution of monarchy. He earned a position as one of the king's favourites and enjoyed his protection from the attacks of the court. The king allegedly suggested that Moli\u00e8re suspend performances of \"Tartuffe\", and the author rapidly wrote \"Dom Juan ou le Festin de Pierre\" to replace it. It was a strange work, derived from a work by Tirso de Molina and rendered in a prose that still seems modern today. It describes the story of an atheist who becomes a religious hypocrite and, for this, is punished by God. This work too was quickly suspended. The king, demonstrating his protection once again, became the new official sponsor of Moli\u00e8re's troupe.\nWith music by Lully, Moli\u00e8re presented \"L'Amour m\u00e9decin\" (\"Love Doctor\" or \"Medical Love\"). Subtitles on this occasion reported that the work was given \"par ordre du Roi\" (by order of the king) and this work was received much more warmly than its predecessors.\nIn 1666, \"Le Misanthrope\" was produced. It is now widely regarded as Moli\u00e8re's most refined masterpiece, the one with the highest moral content, but it was little appreciated at the time. It caused the \"conversion\" of Donneau de Vis\u00e9, who became fond of his theatre. But it was a commercial flop, forcing Moli\u00e8re to immediately write \"Le m\u00e9decin malgr\u00e9 lui\" (\"The Doctor Despite Himself\"), a satire against the official sciences. This was a success despite a moral treatise by the Prince of Conti, criticizing the theatre in general and Moli\u00e8re in particular. In several of his plays, Moli\u00e8re depicted the physicians of his day as pompous individuals who speak (poor) Latin to impress others with false erudition, and know only clysters and bleedings as (ineffective) remedies.\nAfter the \"M\u00e9licerte\" and the \"Pastorale comique\", he tried again to perform a revised \"Tartuffe\" in 1667, this time with the name of \"Panulphe\" or \"L'Imposteur\". As soon as the King left Paris for a tour, Lamoignon and the archbishop banned the play. The King finally imposed respect for \"Tartuffe\" a few years later, after he had gained more power over the clergy.\nMoli\u00e8re, now ill, wrote less. \"Le Sicilien ou L'Amour peintre\" was written for festivities at the castle of Saint-Germain-en-Laye, and was followed in 1668 by \"Amphitryon\", inspired both by Plautus' work of the same name and Jean Rotrou's successful reconfiguration of the drama. With some conjecture, Moli\u00e8re's play can be seen to allude to the love affairs of Louis XIV, then king of France. \"George Dandin, ou Le mari confondu\" (\"The Confounded Husband\") was little appreciated, but success returned with \"L'Avare\" (\"The Miser\"), now very well known.\nWith Lully, he again used music for \"Monsieur de Pourceaugnac\", for \"Les Amants magnifiques\", and finally for \"Le Bourgeois gentilhomme\" (\"The Middle Class Gentleman\"), another of his masterpieces. It is claimed to be particularly directed against Colbert, the minister who had condemned his old patron Fouquet. The collaboration with Lully ended with a \"trag\u00e9die et ballet\", \"Psych\u00e9\", written in collaboration with Pierre Corneille and Philippe Quinault.\nIn 1672, Madeleine B\u00e9jart died, and Moli\u00e8re suffered from this loss and from the worsening of his own illness. Nevertheless, he wrote a successful \"Les Fourberies de Scapin\" (\"Scapin's Deceits\"), a farce and a comedy in five acts. His following play, \"La Comtesse d'Escarbagnas\", is considered one of his lesser works.\n\"Les Femmes savantes\" (\"The Learned Ladies\") of 1672 is considered another of Moli\u00e8re's masterpieces. It was born from the termination of the legal use of music in theatre, since Lully had patented the opera in France (and taken most of the best available singers for his own performances), so Moli\u00e8re had to go back to his traditional genre. It was a great success, and it led to his last work, which is still held in high esteem.\nIn his 14 years in Paris, Moli\u00e8re single-handedly wrote 31 of the 85 plays performed on his stage.\nLes Com\u00e9dies-Ballets.\nIn 1661, Moli\u00e8re introduced the \"com\u00e9dies-ballets\" in conjunction with \"Les F\u00e2cheux\". These ballets were a transitional form of dance performance between the court ballets of Louis XIV and the art of professional theatre which was developing in the advent of the use of the proscenium stage. The \"com\u00e9dies-ballets\" developed accidentally when Moli\u00e8re was enlisted to mount both a play and a ballet in the honor of Louis XIV and found that he did not have a big enough cast to meet these demands. Moli\u00e8re therefore decided to combine the ballet and the play so that his goal could be met while the performers catch their breath and change costume. The risky move paid off and Moli\u00e8re was asked to produce twelve more \"com\u00e9dies-ballets\" before his death. During the \"com\u00e9dies-ballets\", Moli\u00e8re collaborated with Pierre Beauchamp. who codified the five balletic positions of the feet and arms and was largely responsible for the creation of the Beauchamp-Feuillet dance notation. Moli\u00e8re also collaborated with Jean-Baptiste Lully. Lully was a dancer, choreographer, and composer, whose dominant reign at the Paris Op\u00e9ra lasted 15 years. Under his command, ballet and opera rightly became professional arts unto themselves. The \"com\u00e9dies-ballets\" closely integrated dance with music and the action of the play and the style of continuity distinctly separated these performances from the court ballets of the time; additionally, the \"com\u00e9dies-ballets\" demanded that both the dancers and the actors play an important role in advancing the story. Similar to the court ballets, both professionally trained dancers and courtiers socialized together at the \"com\u00e9dies-ballets\" - Louis XIV even played the part of an Egyptian in Moli\u00e8re's \"Le Mariage forc\u00e9\" (1664) and also appeared as Neptune and Apollo in his retirement performance of \"Les Amants magnifiques\" (1670).\nDeath.\nMoli\u00e8re suffered from pulmonary tuberculosis, possibly contracted when he was imprisoned for debt as a young man. The circumstances of Moli\u00e8re's death, on 17 February 1673, became legend. He collapsed on stage in a fit of coughing and haemorrhaging while performing in the last play he had written, which had lavish ballets performed to the music of Marc-Antoine Charpentier and which ironically was titled \"Le Malade imaginaire\" (\"The Imaginary Invalid\"). Moli\u00e8re insisted on completing his performance. Afterwards he collapsed again with another, larger haemorrhage before being taken home, where he died a few hours later, without receiving the last rites because two priests refused to visit him while a third arrived too late. The superstition that green brings bad luck to actors is said to originate from the colour of the clothing he was wearing at the time of his death.\nUnder French law at the time, actors were not allowed to be buried in the sacred ground of a cemetery. However, Moli\u00e8re's widow, Armande, asked the King if her spouse could be granted a normal funeral at night. The King agreed and Moli\u00e8re's body was buried in the part of the cemetery reserved for unbaptised infants.\nIn 1792, his remains were brought to the museum of French monuments, and in 1817, transferred to P\u00e8re Lachaise Cemetery in Paris, close to those of La Fontaine.\nReception of his works.\nThough conventional thinkers, religious leaders and medical professionals in Moli\u00e8re's time criticised his work, their ideas did not really diminish his widespread success with the public. Other playwrights and companies began to emulate his dramatic style in England and in France. Moli\u00e8re's works continued to garner positive feedback in 18th-century England, but they were not so warmly welcomed in France at this time. However, during the French Restoration of the 19th century, Moli\u00e8re's comedies became popular with both the French public and the critics. Romanticists admired his plays for the unconventional individualism they portrayed. 20th-century scholars have carried on this interest in Moli\u00e8re and his plays and have continued to study a wide array of issues relating to this playwright. Many critics now are shifting their attention from the philosophical, religious and moral implications in his comedies to the study of his comic technique.\nMoli\u00e8re's works were translated into English prose by John Ozell in 1714, but the first complete version in English, by Baker and Miller in 1739, remained \"influential\" and was long reprinted. The first to offer full translations of Moli\u00e8re's verse plays such as \"Tartuffe\" into English verse was Curtis Hidden Page, who produced blank verse versions of three of the plays in his 1908 translation. Since then, notable translations have been made by Richard Wilbur, Donald M. Frame, Justin Fleming and many others.\nIn his memoir \"A Terrible Liar\", actor Hume Cronyn writes that, in 1962, celebrated actor Laurence Olivier criticized Moli\u00e8re. According to Cronyn, he mentioned to Olivier that he (Cronyn) was about to play the title role in \"The Miser\", and that Olivier then responded \"Moli\u00e8re? Funny as a baby's open grave.\" Cronyn comments on the incident: \"You may imagine how that made me feel. Fortunately, he was dead wrong.\"\nAuthor Martha Bellinger points out that:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[Moli\u00e8re] has been accused of not having a consistent, organic style, of using faulty grammar, of mixing his metaphors, and of using unnecessary words for the purpose of filling out his lines. All these things are occasionally true, but they are trifles in comparison to the wealth of character he portrayed, to his brilliancy of wit, and to the resourcefulness of his technique. He was wary of sensibility or pathos; but in place of pathos he had \"melancholy \u2014 a puissant and searching melancholy, which strangely sustains his inexhaustible mirth and his triumphant gaiety\".\nInfluence on French culture.\nMoli\u00e8re is considered the creator of modern French comedy. Many words or phrases introduced in Moli\u00e8re's plays are still used in current French:\nPortrayals of Moli\u00e8re.\nMoli\u00e8re plays a small part in Alexandre Dumas's novel \"The Vicomte of Bragelonne\", in which he is seen taking inspiration from the musketeer Porthos for his central character in \"Le Bourgeois gentilhomme\".\nRussian writer Mikhail Bulgakov wrote a semi-fictitious biography-tribute to Moli\u00e8re, titled \"Life of Mr. de Moli\u00e8re\". It was written in 1932\u20131933 and first published 1962.\nThe French 1978 film simply titled \"Moli\u00e8re\" directed by Ariane Mnouchkine and starring Philippe Caub\u00e8re presents his complete biography. It was in competition for the Palme d'Or at Cannes in 1978.\nHe is portrayed among other writers in \"The Blasphemers' Banquet\" (1989).\nThe 2000 film \"Le Roi Danse\" (\"The King Dances\"), in which Moli\u00e8re is played by Tch\u00e9ky Karyo, shows his collaborations with Jean-Baptiste Lully, as well as his illness and on-stage death.\nThe 2007 French film \"Moli\u00e8re\" was more loosely based on the life of Moli\u00e8re, starring Romain Duris, Fabrice Luchini and Ludivine Sagnier.\nDavid Hirson's play \"La B\u00eate\", written in the style of Moli\u00e8re, includes the character Elomire as an anagrammatic parody of him.\nThe 2023 musical \"Moli\u00e8re, l'Op\u00e9ra Urbain\", directed by Bruno Berberes and staged at the D\u00f4me de Paris from 11 November 2023, to 18 February 2024, is a retelling of the life of Moli\u00e8re using a blend of historical costuming with contemporary artistic styles in staging and musical genres.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51467", "revid": "40192293", "url": "https://en.wikipedia.org/wiki?curid=51467", "title": "Moliere", "text": ""}
{"id": "51468", "revid": "1739907", "url": "https://en.wikipedia.org/wiki?curid=51468", "title": "King Zahir of Afghanistan", "text": ""}
{"id": "51469", "revid": "1934512", "url": "https://en.wikipedia.org/wiki?curid=51469", "title": "Mold", "text": "Wooly, dust-like fungal structure or substance\nA mold (US, PH) or mould (UK, CW) is one of the structures that certain fungi can form. The dust-like, colored appearance of molds is due to the formation of spores containing fungal secondary metabolites. The spores are the dispersal units of the fungi. Not all fungi form molds. Some fungi form mushrooms; others grow as single cells and are called microfungi (for example, yeasts).\nA large and taxonomically diverse number of fungal species form molds. The growth of hyphae results in discoloration and a fuzzy appearance, especially on food. The network of these tubular branching hyphae, called a mycelium, is considered a single organism. The hyphae are generally transparent, so the mycelium appears like very fine, fluffy white threads over the surface. Cross-walls (septa) may delimit connected compartments along the hyphae, each containing one or multiple, genetically identical nuclei. The dusty texture of many molds is caused by profuse production of asexual spores (conidia) formed by differentiation at the ends of hyphae. The mode of formation and shape of these spores is traditionally used to classify molds. Many of these spores are colored, making the fungus much more obvious to the human eye at this stage in its life-cycle.\nMolds are microbes that do not form a specific taxonomic or phylogenetic grouping, but can be found in the divisions Zygomycota and Ascomycota. In the past, most molds were classified within the Deuteromycota. Mold was the common name for water molds or slime molds, which were formerly classified as fungi.\nMolds cause biodegradation of natural materials, which can be unwanted when it becomes food spoilage or damage to property. They also play important roles in biotechnology and food science in the production of various pigments, foods, beverages, antibiotics, pharmaceuticals and enzymes. Some diseases of animals and humans can be caused by certain molds: disease may result from allergic sensitivity to mold spores, from growth of pathogenic molds within the body, or from the effects of ingested or inhaled toxic compounds (mycotoxins) produced by molds.\nBiology.\nThere are thousands of known species of mold fungi with diverse life-styles including saprotrophs, mesophiles, psychrophiles and thermophiles, and a very few opportunistic pathogens of humans. They all require moisture for growth and some live in aquatic environments. Like all fungi, molds derive energy not through photosynthesis but from the organic matter on which they live, utilizing heterotrophy. Typically, molds secrete hydrolytic enzymes, mainly from the hyphal tips. These enzymes degrade complex biopolymers such as starch, cellulose, and lignin into simpler substances that can be absorbed by the hyphae. In this way, molds play a major role in the decomposition of organic material, enabling the recycling of nutrients throughout ecosystems. Many molds also synthesize mycotoxins and siderophores that, together with lytic enzymes, inhibit the growth of competing microorganisms. Molds can also grow on stored food for animals and humans, making the food unpalatable or toxic, and are thus a major source of food losses and illness. Many strategies for food preservation (salting, pickling, jams, bottling, freezing, drying) are intended to prevent or slow mold growth as well as the growth of other microbes.\nMolds reproduce by producing large numbers of small spores, that may contain a single nucleus or be multinucleate. Mold spores can be asexual (the products of mitosis) or sexual (the products of meiosis); many species can produce both types. Some molds produce small, hydrophobic spores that are adapted for wind dispersal and may remain airborne for long periods; in some the cell walls are darkly pigmented, providing resistance to damage by ultraviolet radiation. Other mold spores have slimy sheaths and are more suited to water dispersal. Mold spores are often spherical or ovoid single cells, but can be multicellular and variously shaped. Spores may cling to clothing or fur; some are able to survive extremes of temperature and pressure.\nAlthough molds can grow on dead organic matter everywhere in nature, their presence is visible to the unaided eye only when they form large colonies. A mold colony does not consist of discrete organisms but is an interconnected network of hyphae called a mycelium. All growth occurs at hyphal tips, with cytoplasm and organelles flowing forwards as the hyphae advance over or through new food sources. Nutrients are absorbed at the hyphal tip. In artificial environments such as buildings, humidity and temperature are often stable enough to foster the growth of mold colonies, which are often visible as a downy or furry coating growing on food or other surfaces.\nFew molds can begin growing at temperatures of or below, so food is typically refrigerated to this temperature. When conditions do not enable growth to take place, molds can remain alive in a dormant state within a large range of temperatures that depends on the species. The many different mold species vary enormously in their tolerance for temperature and humidity extremes. Certain molds can survive harsh conditions such as the snow-covered soils of Antarctica, refrigeration, highly acidic solvents, anti-bacterial soap, and even petroleum products such as jet fuel.\nXerophilic molds are able to grow in relatively dry, salty, or sugary environments, where water activity (aw) is less than 0.85; other molds need more moisture.\nCommon molds.\nCommon genera of molds include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nFood production.\nThe K\u014dji molds are a group of \"Aspergillus\" species, notably \"Aspergillus oryzae\", and secondarily \"A. sojae\", that have been cultured in eastern Asia for many centuries. They are used to ferment a soybean and wheat mixture to make soybean paste and soy sauce. \"Koji\" molds break down the starch in rice, barley, sweet potatoes, etc., a process called saccharification, in the production of \"sake\", \"sh\u014dch\u016b\" and other distilled spirits. \"Koji\" molds are also used in the preparation of Katsuobushi.\nRed rice yeast is a product of the mold \"Monascus purpureus\" grown on rice, and is common in Asian diets. The yeast contains several compounds collectively known as monacolins, which are known to inhibit cholesterol synthesis. A study has shown that red rice yeast used as a dietary supplement, combined with fish oil and healthy lifestyle changes, may help reduce \"bad\" cholesterol as effectively as certain commercial statin drugs. Nonetheless, other work has shown it may not be reliable (perhaps due to non-standardization) and even toxic to liver and kidneys.\nSome sausages, such as salami, incorporate starter cultures of molds to improve flavor and reduce bacterial spoilage during curing. \"Penicillium nalgiovense\", for example, may appear as a powdery white coating on some varieties of dry-cured sausage.\nOther molds that have been used in food production include:\nPharmaceuticals from molds.\nAlexander Fleming's accidental discovery of the antibiotic penicillin involved a \"Penicillium\" mold then called \"Penicillium rubrum\" (although the species was later established to be \"Penicillium rubens\"). Fleming continued to investigate penicillin, showing that it could inhibit various types of bacteria found in infections and other ailments, but he was unable to produce the compound in amounts large enough for the production of a medicine. His work was expanded by a team at Oxford University: Clutterbuck, Lovell, and Raistrick, who began to work on the problem in 1931. This team was also unable to produce the pure compound in large amounts, and found that the purification process diminished its effectiveness and negated its anti-bacterial properties.\nHoward Florey, Ernst Chain, Norman Heatley, Edward Abraham, also all at Oxford, continued the work. They enhanced and developed the concentration technique by using organic solutions rather than water, and created the \"Oxford Unit\" to measure penicillin concentration within a solution. They managed to purify the solution, increasing its concentration by 45\u201350 times, and found that a higher concentration was possible. Experiments were conducted and the results published in 1941, though the quantities of penicillin produced were not always high enough for the treatments required. As this was during the Second World War, Florey sought US government involvement. With research teams in the UK and some in the US, industrial-scale production of crystallized penicillin was developed during 1941\u20131944 by the USDA and by Pfizer.\nSeveral statin cholesterol-lowering drugs (such as lovastatin, from \"Aspergillus terreus\") are derived from molds.\nThe immunosuppressant drug cyclosporine, used to suppress the rejection of transplanted organs, is derived from the mold \"Tolypocladium inflatum\".\nHealth effects.\nMolds are , and mold spores are a common component of household and workplace dust; however, when mold spores are present in large quantities, they can present a health hazard to humans, potentially causing allergic reactions and respiratory problems.\nSome molds also produce mycotoxins that can pose serious health risks to humans and animals. Some studies claim that exposure to high levels of mycotoxins can lead to neurological problems and, in some cases, death. Prolonged exposure, e.g., daily home exposure, may be particularly harmful. Research on the health impacts of mold has not been conclusive. The term \"toxic mold\" refers to molds that produce mycotoxins, such as \"Stachybotrys chartarum\", and not to all molds in general.\nMolds can also pose a hazard to human and animal health when they are consumed following the growth of certain mold species in stored food. Some species produce toxic secondary metabolites, collectively termed mycotoxins, including aflatoxins, ochratoxins, fumonisins, trichothecenes, citrinin, and patulin. These toxic properties may be used for the benefit of humans when the toxicity is directed against other organisms; for example, penicillin adversely affects the growth of Gram-positive bacteria (e.g. Clostridium species), certain spirochetes and certain fungi.\nGrowth in buildings and homes.\nMold growth in buildings generally occurs as fungi colonize porous building materials, such as wood. Many building products commonly incorporate paper, wood products, or solid wood members, such as paper-covered drywall, wood cabinets, and insulation. Interior mold colonization can lead to a variety of health problems as microscopic airborne reproductive spores, analogous to tree pollen, are inhaled by building occupants. High quantities of indoor airborne spores as compared to exterior conditions are strongly suggestive of indoor mold growth. Determination of airborne spore counts is accomplished by way of an air sample, in which a specialized pump with a known flow rate is operated for a known period of time. To account for background levels, air samples should be drawn from the affected area, a control area, and the exterior.\nThe air sampler pump draws in air and deposits microscopic airborne particles on a culture medium. The medium is cultured in a laboratory and the fungal genus and species are determined by visual microscopic observation. Laboratory results also quantify fungal growth by way of a spore count for comparison among samples. The pump operation time is recorded and when multiplied by pump flow rate results in a specific volume of air obtained. Although a small volume of air is actually analyzed, common laboratory reports extrapolate the spore count data to estimate spores that would be present in a cubic meter of air.\nMold spores are drawn to specific environments, making it easier for them to grow. These spores will usually only turn into a full-blown outbreak if certain conditions are met. Various practices can be followed to mitigate mold issues in buildings, the most important of which is to reduce moisture levels that can facilitate mold growth. Air filtration reduces the number of spores available for germination, especially when a High Efficiency Particulate Air (HEPA) filter is used. A properly functioning AC unit also reduces the relative humidity in rooms. The United States Environmental Protection Agency (EPA) currently recommends that relative humidity be maintained below 60%, ideally between 30% and 50%, to inhibit mold growth.\nEliminating the moisture source is the first step at fungal remediation. Removal of affected materials may also be necessary for remediation, if materials are easily replaceable and not part of the load-bearing structure. Professional drying of concealed wall cavities and enclosed spaces such as cabinet toekick spaces may be required. Post-remediation verification of moisture content and fungal growth is required for successful remediation. Many contractors perform post-remediation verification themselves, but property owners may benefit from independent verification. Left untreated, mold can potentially cause serious cosmetic and structural damage to a property.\nUse in art.\nVarious artists have used mold in various artistic fashions. Daniele Del Nero, for example, constructs scale models of houses and office buildings and then induces mold to grow on them, giving them an unsettling, reclaimed-by-nature look. Stacy Levy sandblasts enlarged images of mold onto glass, then allows mold to grow in the crevasses she has made, creating a macro-micro portrait. Sam Taylor-Johnson has made a number of time-lapse films capturing the gradual decay of classically arranged still lifes.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51470", "revid": "33625371", "url": "https://en.wikipedia.org/wiki?curid=51470", "title": "Mycelium", "text": "Vegetative part of a fungus\nMycelium (pl.: mycelia) is a root-like structure of a fungus consisting of a mass of branching, thread-like hyphae. Its normal form is that of branched, slender, entangled, anastomosing, hyaline threads. Fungal colonies composed of mycelium are found in and on soil and many other substrates. A typical single spore germinates into a monokaryotic mycelium, which cannot reproduce sexually; when two compatible monokaryotic mycelia join and form a dikaryotic mycelium, that mycelium may form fruiting bodies such as mushrooms. A mycelium may be minute, forming a colony that is too small to see, or may grow to span thousands of acres as in \"Armillaria\".\nThrough the mycelium, a fungus absorbs nutrients from its environment. It does this in a two-stage process. First, the hyphae secrete enzymes onto or into the food source, which break down biological polymers into smaller units such as monomers. These monomers are then absorbed into the mycelium by facilitated diffusion and active transport.\nMycelia are vital in terrestrial and aquatic ecosystems for their role in the decomposition of plant material. They contribute to the organic fraction of soil, and their growth releases carbon dioxide back into the atmosphere (see carbon cycle). Ectomycorrhizal extramatrical mycelium, as well as the mycelium of arbuscular mycorrhizal fungi, increase the efficiency of water and nutrient absorption of most plants and confers resistance to some plant pathogens. Mycelium is an important food source for many soil invertebrates. They are vital to agriculture and are important to almost all species of plants, many species co-evolving with the fungi. Mycelium is a primary factor in some plants' health, nutrient intake and growth, with mycelium being a major factor to plant fitness.\nNetworks of mycelia can transport water and spikes of electrical potential.\nSclerotia are compact or hard masses of mycelium.\nUses.\nAgriculture.\nOne of the primary roles of fungi in an ecosystem is to decompose organic compounds. Petroleum products and some pesticides (typical soil contaminants) are organic molecules (i.e., they are built on a carbon structure), and thereby show a potential carbon source for fungi. Hence, fungi have the potential to eradicate such pollutants from their environment unless the chemicals prove toxic to the fungus. This biological degradation is a process known as mycoremediation.\nMycelial mats have been suggested as having potential as biological filters, removing chemicals and microorganisms from soil and water. The use of fungal mycelium to accomplish this has been termed mycofiltration.\nKnowledge of the relationship between mycorrhizal fungi and plants suggests new ways to improve crop yields.\nWhen spread on logging roads, mycelium can act as a binder, holding disturbed new soil in place thus preventing washouts until woody plants can establish roots.\nFungi are essential for converting biomass into compost, as they decompose feedstock components such as lignin, which many other composting microorganisms cannot. Turning a backyard compost pile will commonly expose visible networks of mycelia that have formed on the decaying organic material within. Compost is an essential soil amendment and fertilizer for organic farming and gardening. Composting can divert a substantial fraction of municipal solid waste from landfills.\nCommercial.\nAlternatives to polystyrene and plastic packaging can be produced by growing mycelium in agricultural waste.\nMycelium has also been used as a material in furniture, and artificial leather.\nOne of the main commercial uses of mycelium is its use to create artificial leather. Animal leather contributes to a significant environmental footprint, as livestock farming is associated with deforestation, greenhouse gas emissions, and grazing. In addition, the production of synthetic leathers from polyvinyl chloride and polyurethane require the use of hazardous chemicals and fossil fuels, and they are not biodegradable (like plastic). Fungal-based artificial leather is cheaper to produce, has less of an environmental footprint, and is biodegradable. It costs between 18 and 28 cents to produce a square meter of raw mycelium, while it costs between $5.81 and $6.24 to produce a square meter of raw animal hide. Fungal growth is carbon neutral and pure mycelium is 94% biodegradable. However, the use of polymeric materials such as polyester or polylactic acid to improve artificial leather\u2019s properties can negatively affect the biodegradability of the material.\nTo create leather, fungal mycelium is grown either using liquid-state or solid-state fermentation. In liquid-state fermentation, companies typically use laboratory media or agricultural byproducts to grow fungal biomass. The fungal biomass is then separated into fibers and processed using fiber suspension, filtration, pressing, and drying. These techniques are also commonly utilized in traditional papermaking processes. In solid-state fermentation, mycelium is grown on forestry bioproducts, like sawdust, in an environment with high carbon dioxide concentrations and controlled humidity and temperature. The mycelium mat formed on top of the particle bed is dehydrated, chemically treated, and then compressed to a desired thickness and engraved with a pattern.\nConstruction material.\nMycelium is a strong candidate for sustainable construction primarily due to its lightweight biodegradable structure and its capacity to be grown from waste sources. In addition to this, mycelium has a relatively high strength-to-weight ratio and a much lower embodied energy compared to traditional building materials. Because mycelium takes the form of any mold it is grown in, it can also be advantageous for customization purposes, especially if it is employed as an architectural or aesthetic feature. Current research has also indicated that mycelium does not release toxic resins in the event of a fire because it has a charring effect similar to mass timber. Mycelium plays a role in acoustic insulation, boasting of an absorbance of 70\u201375% for frequencies of 1500\u00a0Hz or less.\nStrengths and weaknesses.\nMycelium bio-composites have shown strong potential for structural applications, with much higher strength-to-weight ratios than that of conventional materials due primarily to its low density. Compared to conventional building materials, mycelium also has a number of desirable properties that make it an attractive alternative. For example, it has low thermal conductivity and can provide high acoustic insulation. It is biodegradable, has much lower embodied energy, and can serve as a carbon sink, which makes mycelium bio-composites a possible solution to the emissions, energy, and waste associated with building construction.\nWhile mycelium has possibilities as a structural material, there are several significant disadvantages that make it difficult to be practically implemented in large-scale projects. For one, mycelium does not have particularly high compressive strength on its own, ranging from 0.1\u20130.2 MPa. This is in stark comparison to traditional concrete, which typically has a compressive strength of 17\u201328 MPa. Even more, because mycelium is considered a living material, it holds specific requirements that make it susceptible to environmental conditions. For instance, it requires a constant source of air in order to stay alive, needs a relatively humid habitat to grow, and cannot be exposed to large amounts of water for fear of contamination and decay.\nMechanical properties.\nThree separate fungi species (\"Colorius versicolor\", \"Trametes ochracea\", and \"Ganoderma sessile\") were mixed independently with 2 substrates (apple and vine) and tested under separate incubation conditions in order to quantify certain mechanical properties of mycelium. In order to do this, samples were grown in molds, incubated, and dried over the course of 12 days. Samples were tested for water absorption using https:// guidelines and compared against an EPS material. Tiles of uniform size were cut from the fabricated mold and put under an Instron 3345 machine going at 1\u00a0mm/min, up until 20% deformation.\nThroughout a 4 stage process, the impact of various substrate and fungal mixes was investigated along with properties of mycelium such as density, water absorption, and compressive strength. Samples were separated into two separate incubation methods and inspected for differences in color, texture, and growth. For the same fungi within each incubation method, minimal differences were recorded. However, across disparate substrate mixtures within the same fungi, colorization and external growth varied between the test samples. While loss of organic matter was calculated, no uniform correlation was found between substrate used and chemical properties of the material. For each of the substrate-fungi mixtures, average densities ranged from 174.1\u00a0kg/m3 to 244.9\u00a0kg/m3, with the Ganoderma sessile fungi and apple substrate combination being the most dense. Compression tests revealed the Ganoderma sessile fungi and vine substrate to have the highest strength of the samples tested, but no numerical value was provided. For reference, surrounding literature has provided a ballpark estimate of 1-72 kPa. Beyond this, mycelium has a thermal conductivity of 0.05\u20130.07W/m\u00b7K which is less than that of typical concrete.\nConstruction.\nThe construction of mycelium structures is primarily categorized into three approaches. These include growing blocks in molds, growing in place monolithic structures, and bio-welded units. The first approach cultivates mycelium and its substrate in forms, after which it is dried in ovens and then transported and assembled on site. The second approach uses existing formwork and adapts cast-in-place concrete techniques to grow monolithic mycelium structures in place. The third approach is a hybrid of the previous two referred to as myco-welding, where individual pre-grown units are grown together into a larger monolithic structure.\nStudies using grow-in-place methods and myco-welding have explored how to cultivate mycelium and re-use formwork in construction and investigated post-tensioning and friction connections. Research in fabrication has revealed some common challenges faced in construction of mycelium structures, mostly related to the growth of the fungi. It can be difficult to cultivate living material into formwork and it is susceptible to contamination if not properly sterilized. The fungi needs to be kept refrigerated to prevent hardening and properly manage growth and substrate consumption. Additionally, the thickness of fungal growth is limited by the presence of oxygen; if there is no oxygen, the center of the growth can die or be contaminated.\nEnvironmental impact.\nResearchers have performed life-cycle assessments to evaluate the environmental impact of mycelium bio-composites. Life cycle analysis showed the viability of mycelium as a carbon sink material and as a sustainable alternative to conventional building materials. Use of mycelium as a natural adhesive material may provide environmental benefits, as the fungal-based composites that mycelium is used to create are low cost, low emission, and sustainable. These composites also have a wide range of applications and uses, many of which are in industries responsible for significant environmental pollution, like construction and packaging.\nModern construction and packaging materials are industrially fabricated, non-recyclable, and pollutive: wood products lead to severe deforestation and weather fluctuation; cement is nonbiodegradable and causes high emissions both in production and demolition. Mycelium appears to be cheaper and more sustainable than its counterparts.\nMycelium\u2019s adhesive properties are largely responsible for its diverse array of applications, as it allows them to bind certain substances together. These properties are products of their biological processes, as they secrete corrosive enzymes that allow them to degrade and colonize organic substrates. During degradation, mycelium develops a dense network of thin strands that fuse together within the organic substrate, creating solid material that can hold multiple substrates together. This self-assembly property of mycelium is quite unique, and allows mycelium to grow on a wide range of organic material, including organic waste.\nPotential ecological role.\nPlants appear to communicate within an ecosystem using mycelium, the fungal network produced by mycorrhiza fungi. Mycelial networks constitute 20-30% of soil biomass, though traditional biomass measures fail to detect them. Some 83% of plants appear to exhibit mutualistic association with mycelium as an extension of their root systems, with varying levels of reliance. By some estimates, mycelial networks receive well over 10% of the photosynthesis output of their host plants.\nThis mutualism is initiated by hyphal connections in which mycelial strands infect and attach themselves to plant hyphae, penetrating the cell wall but not entering through the membrane into the plant cytoplasm. Mycelium interacts with the cell at the periarbuscular membrane, which behaves as a sort of exchange medium for nutrients and can produce electrical gradients allowing for electrophysiological signals to be sent and received. In modeling studies, different fungi supply different levels of nutrients and growth-promoting materials, with plants tending to root towards (and thus being infected by) fungi supplying most mineral phosphorus and nitrogen (both essential for plant growth).\nMycorrhizal mycelial associations may intensify competition between individuals of the same species, while alleviating competition between species, via the promotion of inferior competitors, thus promoting plant diversity within its network. In doing so, mycorrhizal fungi promote community ecology, with an added complexity of niche differentiation of different networks and types of mycorrhizal fungi that root at different depths, disperse different organic compounds and nutrients, and have unique interactions with specific species of plants.\nMycelial biology and memory.\nSeveral studies have documented the memory capacity of mycelial networks and their adaptability to specific environmental conditions. Mycelia have been specialized for different functions in various climates and develop symbiotic or pathogenic relationships with other organisms, such as the human pathogen \"Candida auris\", which has developed a unique approach of evading detection by human neutrophils through adaptive selection\u2013a process of fungal learning and memory. Additionally, these functions can change based on the scale of the mycelia and nature of the symbiotic relationship; commensal and mutual relationships between fungi and plants form through a separate process known as mycorrhizal association, which are called mycorrhiza. Additionally, hyphal organization into mycelial networks can be deterministic for a variety of functions including biomass retention, water recycling, expansion of future hyphae on a resource efficient approach towards desired nutrient gradients, and the subsequent distribution of these resources across the hyphal network. On a macroscopic scale, many mycelia operate with a sort of hierarchy having a \u201ctrunk\u201d or main mycelium, with smaller \u201cbranches\u201d branching off.\u00a0 Some saprotrophic basidiomycetes are able to remember past decisions about directional nutrition gradients and will build future mycelium in that direction.\nMycelial memory and intelligence.\nCurrent research on collective mycelial intelligence is limited, and while many studies have observed memory and the exchange of electric charge across mycelial networks, this is insufficient evidence to make conclusions about how sensory data is processed in these networks. However, some examples of increased thermal resistance in filamentous fungi suggest a power-law relationship for memory and exposure to a stimulus. Mycelia have also demonstrated the ability to edit their genetic structures within a lifetime due to antibiotic or other extracellular stressors, which can cause rapid acquisition of resistance genes, like those in \"C. auris\". Additionally, plasmodial slime molds demonstrate a similar method of information sharing, as both mycelia and slime molds make use of cAMP molecules for aggregation and signaling.\nSclerotium.\nSclerotium is a compact mass of hardened mycelium. For many years, sclerotia were mistaken for individual organisms and described as separate species. However, in the mid 19th century, it was proven that sclerotia was simply a stage in the life cycle of many fungi. Sclerotia are composed of thick, dense shells with dark cells. They are rich in hyphae emergency supplies, such as oil, and they contain small amounts of water. They can survive in dry environments for many years without losing the ability to grow. The size of sclerotia can range from less than a millimeter to tens of centimeters in diameter.\nReferences.\nFootnotes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51472", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=51472", "title": "Spore", "text": "Unit of reproduction adapted for dispersal and survival in unfavorable conditions\nIn biology, a spore is a unit of sexual (in fungi) or asexual reproduction that may be adapted for dispersal and for survival, often for extended periods of time, in unfavourable conditions. Spores form part of the life cycles of many plants, algae, fungi and protozoa. They were thought to have appeared as early as the mid-late Ordovician period as an adaptation of early land plants.\nBacterial spores are not part of a sexual cycle, but are resistant structures used for survival under unfavourable conditions. Myxozoan spores release amoeboid infectious germs (\"amoebulae\") into their hosts for parasitic infection, but also reproduce within the hosts through the pairing of two nuclei within the plasmodium, which develops from the amoebula.\nIn plants, spores are usually haploid and unicellular and are produced by meiosis in the sporangium of a diploid sporophyte. In some rare cases, a diploid spore is also produced in some algae, or fungi. Under favourable conditions, the spore can develop into a new organism using mitotic division, producing a multicellular gametophyte, which eventually goes on to produce gametes. Two gametes fuse to form a zygote, which develops into a new sporophyte. This cycle is known as alternation of generations.\nThe spores of seed plants are produced internally, and the megaspores (formed within the ovules) and the microspores are involved in the formation of more complex structures that form the dispersal units, the seeds and pollen grains.\nDefinition.\nThe term \"spore\" derives from Greek , , meaning 'seed, sowing', related to , , 'sowing', and , 'to sow'.\nIn common parlance, the difference between a \"spore\" and a \"gamete\" is that a spore will germinate and develop into a sporeling, while a gamete needs to combine with another gamete to form a zygote before developing further.\nThe main difference between spores and seeds as dispersal units is that spores are unicellular, the first cell of a gametophyte, while seeds contain within them a developing embryo (the multicellular sporophyte of the next generation), produced by the fusion of the male gamete of the pollen tube with the female gamete formed by the megagametophyte within the ovule. Spores germinate to give rise to haploid gametophytes, while seeds germinate to give rise to diploid sporophytes.\nClassification of spore-producing organisms.\nPlants.\nVascular plant spores are always haploid. Vascular plants are either homosporous (also known as isosporous) or heterosporous. Plants that are homosporous produce spores of the same size and type.\nHeterosporous plants, such as seed plants, spikemosses, quillworts, and ferns of the order Salviniales produce spores of two different sizes: the larger spore (megaspore) in effect functioning as a \"female\" spore and the smaller (microspore) functioning as a \"male\". Such plants typically give rise to the two kind of spores from within separate sporangia, either a megasporangium that produces megaspores or a microsporangium that produces microspores. In flowering plants, these sporangia occur within the carpel and anthers, respectively.\nFungi.\nFungi commonly produce spores during sexual and asexual reproduction. Spores are usually haploid and grow into mature haploid individuals through mitotic division of cells (Urediniospores and Teliospores among rusts are dikaryotic). Dikaryotic cells result from the fusion of two haploid gamete cells. Among sporogenic dikaryotic cells, karyogamy (the fusion of the two haploid nuclei) occurs to produce a diploid cell. Diploid cells undergo meiosis to produce haploid spores.\nClassification of spores.\nSpores can be classified in several ways such as by their spore producing structure, function, origin during life cycle, and mobility.\nBelow is a table listing the mode of classification, name, identifying characteristic, examples, and images of different spore species.\nExternal anatomy.\nUnder high magnification, spores often have complex patterns or ornamentation on their exterior surfaces. A specialized terminology has been developed to describe features of such patterns. Some markings represent apertures, places where the tough outer coat of the spore can be penetrated when germination occurs. Spores can be categorized based on the position and number of these markings and apertures. Alete spores show no lines. In monolete spores, there is a single narrow line (laesura) on the spore. Indicating the prior contact of two spores that eventually separated. In trilete spores, each spore shows three narrow lines radiating from a center pole. This shows that four spores shared a common origin and were initially in contact with each other forming a tetrahedron. A wider aperture in the shape of a groove may be termed a colpus. The number of colpi distinguishes major groups of plants. Eudicots have tricolpate spores (i.e. spores with three colpi).\nSpore tetrads and trilete spores.\nEnvelope-enclosed spore tetrads are taken as the earliest evidence of plant life on land, dating from the mid-Ordovician (early Llanvirn, ~https://\u00a0million years ago), a period from which no macrofossils have yet been recovered.\nIndividual trilete spores resembling those of modern cryptogamic plants first appeared in the fossil record at the end of the Ordovician period.\nDispersal.\nIn fungi, both asexual and sexual spores or sporangiospores of many fungal species are actively dispersed by forcible ejection from their reproductive structures. This ejection ensures exit of the spores from the reproductive structures as well as travelling through the air over long distances. Many fungi thereby possess specialized mechanical and physiological mechanisms as well as spore-surface structures, such as hydrophobins, for spore ejection. These mechanisms include, for example, forcible discharge of ascospores enabled by the structure of the ascus and accumulation of osmolytes in the fluids of the ascus that lead to explosive discharge of the ascospores into the air.\nThe forcible discharge of single spores termed \"ballistospores\" involves formation of a small drop of water (Buller's drop), which upon contact with the spore leads to its projectile release with an initial acceleration of more than 10,000 g. Other fungi rely on alternative mechanisms for spore release, such as external mechanical forces, exemplified by puffballs. Attracting insects, such as flies, to fruiting structures, by virtue of their having lively colours and a putrid odour, for dispersal of fungal spores is yet another strategy, most prominently used by the stinkhorns.\nIn Common Smoothcap moss (\"Atrichum undulatum\"), the vibration of sporophyte has been shown to be an important mechanism for spore release.\nIn the case of spore-shedding vascular plants such as ferns, wind distribution of very light spores provides great capacity for dispersal. Also, spores are less subject to animal predation than seeds because they contain almost no food reserve; however they are more subject to fungal and bacterial predation. Their chief advantage is that, of all forms of progeny, spores require the least energy and materials to produce.\nIn the spikemoss \"Selaginella lepidophylla\", dispersal is achieved in part by an unusual type of diaspore, a tumbleweed.\nOrigin.\nSpores have been found in microfossils dating back to the mid-late Ordovician period. Two hypothesized initial functions of spores relate to whether they appeared before or after land plants. The heavily studied hypothesis is that spores were an adaptation of early land plant species, such as embryophytes, that allowed for plants to easily disperse while adapting to their non-aquatic environment. This is particularly supported by the observation of a thick spore wall in cryptospores. These spore walls would have protected potential offspring from novel weather elements. The second more recent hypothesis is that spores were an early predecessor of land plants and formed during errors in the meiosis of algae, a hypothesized early ancestor of land plants.\nWhether spores arose before or after land plants, their contributions to topics in fields like paleontology and plant phylogenetics have been useful. The spores found in microfossils, also known as cryptospores, are well preserved due to the fixed material they are in as well as how abundant and widespread they were during their respective time periods. These microfossils are especially helpful when studying the early periods of earth as macrofossils such as plants are not common nor well preserved. Both cryptospores and modern spores have diverse morphology that indicate possible environmental conditions of earlier periods of Earth and evolutionary relationships of plant species.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n "}
{"id": "51474", "revid": "42861752", "url": "https://en.wikipedia.org/wiki?curid=51474", "title": "Seat belt", "text": "Vehicle safety device to protect against injury during collisions and sudden stop\nA seat belt or seatbelt, also known as a safety belt, is a vehicle safety device designed to secure the driver or a passenger of a vehicle against harmful movement that may result during a collision or a sudden stop. A seat belt reduces the likelihood of death or serious injury in a traffic collision by reducing the force of secondary impacts with interior strike hazards, by keeping occupants positioned correctly for maximum effectiveness of the airbag (if equipped), and by preventing occupants being ejected from the vehicle in a crash or if the vehicle rolls over.\nWhen in motion, the driver and passengers are traveling at the same speed as the vehicle. If the vehicle suddenly halts or crashes, the occupants continue at the same speed the vehicle was going before it stopped. A seat belt applies an opposing force to the driver and passengers to prevent them from falling out or making contact with the interior of the car (especially preventing contact with, or going through, the windshield). Seat belts are considered primary restraint systems (PRSs), because of their vital role in occupant safety.\nEffectiveness.\nAn analysis conducted in the United States in 1984 compared a variety of seat belt types alone and in combination with air bags. The range of fatality reduction for front seat passengers was broad, from 20% to 55%, as was the range of major injury, from 25% to 60%. More recently, the Centers for Disease Control and Prevention has summarized these data by stating \"seat belts reduce serious crash-related injuries and deaths by about half.\" Most malfunctions are a result of there being too much slack in the seat belt at the time of the accident.\nIt has been suggested that although seat belt usage reduces the probability of death in any given accident, mandatory seat belt laws have little or no effect on the overall number of traffic fatalities because seat belt usage also disincentivizes safe driving behaviors, thereby increasing the total number of accidents. This idea, known as compensating-behavior theory, is not supported by the evidence.\nIn case of vehicle rollover in a U.S. passenger car or SUV, from 1994 to 2004, wearing a seat belt reduced the risk of fatalities or incapacitating injuries and increased the probability of no injury:\nHistory.\nSeat belts were invented by English engineer George Cayley, to use on his glider, in the mid-19th century.\nBy 1928, all aircraft were required to have them.&lt;ref name=\"flyingmag/supplies/restrain\"&gt;&lt;/ref&gt;\nNash was the first American car manufacturer to offer seat belts as a factory option, in its 1949 models. They were installed in 40,000 cars, but buyers did not want them and requested that dealers remove them. The feature was \"met with insurmountable sales resistance\" and Nash reported that after one year \"only 1,000 had been used\" by customers.\nFord offered seat belts as an option in 1955. These were not popular, with only 2% of Ford buyers choosing to pay for seat belts in 1956.\nIn the early 1950s C. Hunter Shelden, a neurologist working at the Huntington Memorial Hospital in Pasadena, California, studied the high number of head injuries he saw due to auto accidents. Based on his study of the early seat belts whose primitive designs were implicated in these injuries and deaths, in a 1955 paper he proposed retractable seat belts, as well as recessed steering wheels, reinforced roofs, roll bars, and passive restraints such as air bags.\nGlenn W. Sheren, of Mason, Michigan, submitted a patent application on March 31, 1955, for an automotive seat belt and was awarded https:// in 1958. This was a continuation of an earlier patent application that Sheren had filed on September 22, 1952.\nThe first modern three-point seat belt (the so-called \"CIR-Griswold restraint\") commonly used in consumer vehicles was patented in 1955 https:// by the Americans Roger W. Griswold and Hugh DeHaven.\nSaab introduced seat belts as standard equipment in 1958. After the Saab GT 750 was introduced at the New York Motor Show in 1958 with safety belts fitted as standard, the practice became commonplace.\nVattenfall, the Swedish national electric utility, did a study of all fatal, on-the-job accidents among their employees. The study revealed that the majority of fatalities occurred while the employees were on the road on company business. In response, two Vattenfall safety engineers, Bengt Odelgard and Per-Olof Weman, started to develop a seat belt. Their work was presented to Swedish manufacturer Volvo in the late 1950s, and set the standard for seat belts in Swedish cars. The three-point seat belt was developed to its modern form by Swedish inventor Nils Bohlin for Volvo, which introduced it in 1959 as standard equipment. In addition to designing an effective three-point belt, Bohlin demonstrated its effectiveness in a study of 28,000 accidents in Sweden. Unbelted occupants sustained fatal injuries throughout the whole speed scale, whereas none of the belted occupants was fatally injured at accident speeds below 60\u00a0mph. No belted occupant was fatally injured if the passenger compartment remained intact. Bohlin was granted https:// for the device.\nSubsequently, in 1966, Congress passed the National Traffic and Motor Vehicle Safety Act, requiring all automobiles to comply with certain safety standards.\nThe first compulsory seat belt law was put in place in 1970, in the state of Victoria, Australia, requiring their use by drivers and front-seat passengers. This legislation was enacted after trialing Hemco seat belts, designed by Desmond Hemphill (1926\u20132001), in the front seats of police vehicles, lowering the incidence of officer injury and death. Mandatory seat belt laws in the United States began to be introduced in the 1980s and faced opposition, with some consumers going to court to challenge the laws. Some cut seat belts out of their cars.\nMaterial.\nThe 'belt' part of the typical seatbelt seen in vehicles worldwide is referred to as the 'webbing'. Modern seat belt webbing has a high tensile strength, about , to resist tearing at high loads such as during high-speed collisions or while restraining larger passengers.\nWhile nylon was used in some early seat belts (and is still used for lap belts), it was replaced by 100% polyester due to its better UV resistance, lower extensibility and higher stiffness. Nylon was also prone to stretching much more than polyester, and was prone to wear and tear, with tiny abrasions drastically reducing tensile strength, causing a lack of reliability in one of the most important safety measures in a vehicle. Seat belts are commonly 46 or 48\u00a0mm wide with a 2/2 herringbone twill weaving pattern to maximize the thread density. Modern seatbelt weaves also feature snag-proof selvedges reinforced with strong polyester threads to prevent the wear and tear, while remaining flexible. The weave features about 300 warp threads for every 46mm wide webbing, leading to around 150 ends per inch of webbing.\nAccident investigators often examine the webbing of a seatbelt to determine if an occupant of a vehicle was wearing their seatbelt during a collision. The material of the webbing may contain traces of the occupant's clothing. Certain materials such as nylons may become permanently affixed or melted onto the fabric as a result of heat produced by friction, whereas fiber-based clothing leaves no remains on modern webbing.\nTypes.\nTwo-point airplane seat belts.\nAn airplane seat belt is designed to protect from up and down movement and to keep passengers in their seats during turbulence or collisions on the runway. A simple strap was first used March 12, 1910, by pilot Benjamin Foulois, a pioneering aviator with the Aeronautical Division, U.S. Signal Corps, so he might remain at the controls during turbulence.\nWhile lap belts are exceedingly rare to spot in modern cars, they are the standard in commercial airliners. The lift-lever style of commercial aircraft buckles allows for the seatbelt to be easily clasped and unclasped, accessible quickly in case of an emergency where a passenger must evacuate, and fulfills the minimum safety requirements provided by the FAA while remaining low-cost to produce. Furthermore, in case of any collision, a passenger in economy class has only around 9 inches for their head to travel forward, meaning restraining the torso and head is relatively unnecessary as the head has little room to accelerate before collision.\nTwo-point land seat belts.\nA two-point belt attaches at its two endpoints, land vehicles move typically in a forward or backward or sideways motion, because they generally stay on the ground, and need a shoulder belt. The Irvin Air Chute Company made the seat belt for use by professional race car driver Barney Oldfield when his team decided the daredevil should have a \"safety harness\" for the 1923 Indianapolis 500.\nLap belt.\nA lap belt is a strap that goes over the waist. This was the most common type of belt prior to legislation requiring three-point belts and is found in older cars. Coaches are equipped with lap belts (although many newer coaches have three-point belts), as are passenger aircraft seats.\nUniversity of Minnesota professor James J. \"Crash\" Ryan was the inventor of, and held the patent for, the automatic retractable lap safety belt. Ralph Nader cited Ryan's work in \"Unsafe at Any Speed\" and, following hearings led by Senator Abraham Ribicoff, President Lyndon Johnson signed two bills in 1966 requiring safety belts in all passenger vehicles starting in 1968.\nUntil the 1980s, three-point belts were commonly available only in the front outboard seats of cars; the back seats were often only fitted with lap belts. Evidence of the potential of lap belts to cause separation of the lumbar vertebrae and the sometimes-associated paralysis, or \"seat belt syndrome\" led to the progressive revision of passenger safety regulations in nearly all developed countries to require three-point belts, first in all outboard seating positions, and eventually in all seating positions in passenger vehicles. Since September 1, 2007, all new cars sold in the U.S. require a lap and shoulder belt in the center rear seat. In addition to regulatory changes, \"seat belt syndrome\" has led to a liability for vehicle manufacturers. One Los Angeles case resulted in a $45 million jury verdict against Ford; the resulting $30 million judgment (after deductions for another defendant who settled prior to trial) was affirmed on appeal in 2006.\nShoulder harness.\nA \"sash\" or shoulder harness is a strap that goes diagonally over the vehicle occupant's outboard shoulder and is buckled inboard of their lap. The shoulder harness may attach to the lap belt tongue, or it may have a tongue and buckle completely separate from those of the lap belt. Shoulder harnesses of this separate or semi-separate type were installed in conjunction with lap belts in the outboard front seating positions of many vehicles in the North American market starting at the inception of the shoulder belt requirement of the U.S. National Highway Traffic Safety Administration's (NHTSA) Federal Motor Vehicle Safety Standard 208 on January 1, 1968. However, if the shoulder strap is used without the lap belt, the vehicle occupant is likely to \"submarine\", or slide forward in the seat and out from under the belt, in a frontal collision. In the mid-1970s, three-point belt systems such as Chrysler's \"Uni-Belt\" began to supplant the separate lap and shoulder belts in American-made cars, though such three-point belts had already been supplied in European vehicles such as Volvo, Mercedes-Benz, and Saab for some years.\nThree-point.\nA three-point belt is a Y-shaped arrangement, similar to the separate lap and sash belts, but unified. Like the separate lap-and-sash belt, in a collision, the three-point belt spreads out the energy of the moving body over the chest, pelvis, and shoulders. Volvo introduced the first production three-point belt in 1959. The first car with a three-point belt was a Volvo PV 544 that was delivered to a dealer in Kristianstad on August 13, 1959. The first car model to have the three-point seat belt as a standard item was the 1959 Volvo 122, first outfitted with a two-point belt at initial delivery in 1958, replaced with the three-point seat belt the following year. The three-point belt was developed by Nils Bohlin, who had earlier also worked on ejection seats at Saab. Volvo then made the new seat belt design patent open in the interest of safety and made it available to other car manufacturers for free.\nBelt-in-Seat.\nThe Belt-in-Seat (BIS) is a three-point harness with the shoulder belt attached to the seat itself, rather than to the vehicle structure. The first car using this system was the Range Rover Classic, which offered BIS as standard on the front seats from 1970. Some cars, like the Renault Vel Satis, use this system for the front seats. A General Motors assessment concluded seat-mounted three-point belts offer better protection, especially to smaller vehicle occupants, though GM did not find a safety performance improvement in vehicles with seat-mounted belts versus belts mounted to the vehicle body.\nBelt-in-Seat type belts have been used by automakers in convertibles and pillarless hardtops, where there is no \"B\" pillar to affix the upper mount of the belt. Chrysler and Cadillac are well known for using this design. Antique auto enthusiasts sometimes replace original seats in their cars with BIS-equipped front seats, providing a measure of safety not available when these cars were new. However, modern BIS systems typically use electronics that must be installed and connected with the seats and the vehicle's electrical system in order to function properly.\n4-, 5-, and 6-point.\nFive-point harnesses are typically found in child safety seats and in racing cars. The lap portion is connected to a belt between the legs and there are two shoulder belts, making a total of five points of attachment to the seat. A 4-point harness is similar, but without the strap between the legs, while a 6-point harness has two belts between the legs. In NASCAR, the 6-point harness became popular after the death of Dale Earnhardt, who was wearing a five-point harness when he suffered his fatal crash. As it was first thought that his belt had broken, and broke his neck at impact, some teams ordered a six-point harness in response.\nSeven-point.\nAerobatic aircraft frequently use a combination harness consisting of a five-point harness with a redundant lap belt attached to a different part of the aircraft. While providing redundancy for negative-g maneuvers (which lift the pilot out of the seat), they also require the pilot to unlatch two harnesses if it is necessary to parachute from a failed aircraft.\nTechnology.\nLocking retractors.\nThe purpose of locking retractors (sometimes called \"emergency locking retractors\", or \"ELRs\") is to provide the seated occupant the convenience of some free movement of the upper torso within the compartment while providing a method of limiting this movement in the event of a crash. Starting in 1996, all passenger vehicles were required to lock pre-crash, meaning they have a locking mechanism in the retractor or in the latch plate. Seat belts are stowed on spring-loaded reels, called \"retractors\", equipped with inertial locking mechanisms that stop the belt from extending off the reel during severe deceleration.\nThere are two main types of inertial seat belt locks. A webbing-sensitive lock is based on a centrifugal clutch activated by the rapid acceleration of the strap (webbing) from the reel. The belt can be pulled from the reel only slowly and gradually, as when the occupant extends the belt to fasten it. A sudden rapid pull of the belt\u2014as in a sudden braking or collision event\u2014causes the reel to lock, restraining the occupant in position. The first automatic locking retractor for seat belts and shoulder harnesses in the U.S. was the Irving \"Dynalock\" safety device. These \"Auto-lock\" front lap belts were optional on AMC cars with bucket seats in 1967.\nA vehicle-sensitive lock is based on a pendulum swung away from its plumb position by rapid deceleration or rollover of the vehicle. In the absence of rapid deceleration or rollover, the reel is unlocked and the belt strap may be pulled from the reel against the spring tension of the reel. The vehicle occupant can move around with relative freedom while the spring tension of the reel keeps the belt taut against the occupant. When the pendulum swings away from its normal plumb position due to sudden deceleration or rollover, a pawl is engaged, the reel locks and the strap restrains the belted occupant in position. Dual-sensing locking retractors use both vehicle G-loading and webbing payout rate to initiate the locking mechanism.\nPretensioners and webclamps.\nSeat belts in many newer vehicles are also equipped with \"pretensioners\" or \"web clamps\", or both.\nPretensioners preemptively tighten the belt to prevent the occupant from jerking forward in a crash. Mercedes-Benz first introduced pretensioners on the 1981 S-Class. In the event of a crash, a pretensioner will tighten the belt almost instantaneously. This reduces the motion of the occupant in a violent crash. Like airbags, pretensioners are triggered by sensors in the car's body, and many pretensioners have used explosively expanding gas to drive a piston that retracts the belt. Pretensioners also lower the risk of \"submarining\", which occurs when a passenger slides forward under a loosely fitted seat belt.\nSome systems also pre-emptively tighten the belt during fast accelerations and strong decelerations, even if no crash has happened. This has the advantage that it may help prevent the driver from sliding out of position during violent evasive maneuvers, which could cause loss of control of the vehicle. These pre-emptive safety systems may \"prevent\" some collisions from happening, as well as reduce injuries in the event an actual collision occurs. Pre-emptive systems generally use electric pretensioners, which can operate repeatedly and for a sustained period, rather than pyrotechnic pretensioners, which can only operate a single time.\nWebclamps stop the webbing in the event of an accident and limit the distance the webbing can spool out (caused by the unused webbing tightening on the central drum of the mechanism). These belts also often incorporate an energy management loop (\"rip stitching\") in which a section of the webbing is looped and stitched with special stitching. The function of this is to \"rip\" at a predetermined load, which reduces the maximum force transmitted through the belt to the occupant during a violent collision, reducing injuries to the occupant.\nA study demonstrated that standard automotive three-point restraints fitted with pyrotechnic or electric pretensioners were not able to eliminate all interior passenger compartment head strikes in rollover test conditions. Electric pretensioners are often incorporated on vehicles equipped with precrash systems; they are designed to reduce seat belt slack in a potential collision and assist in placing the occupants in a more optimal seating position. The electric pretensioners also can operate on a repeated or sustained basis, providing better protection in the event of an extended rollover or a multiple collision accident.\nInflatable.\nThe inflatable seat belt was invented by Donald Lewis and tested at the Automotive Products Division of Allied Chemical Corporation. Inflatable seat belts have tubular inflatable bladders contained within an outer cover. When a crash occurs, the bladder inflates with gas to increase the area of the restraint contacting the occupant and also shortening the length of the restraint to tighten the belt around the occupant, improving the protection. The inflatable sections may be shoulder-only or lap and shoulder. The system supports the head during the crash better than a web-only belt. It also provides side impact protection. In 2013, Ford began offering rear-seat inflatable seat belts on a limited set of models, such as the Explorer and Flex.\nAutomatic.\nSeat belts that automatically move into position around a vehicle occupant once the adjacent door is closed and/or the engine is started were developed as a countermeasure against low usage rates of manual seat belts, particularly in the United States. The 1972 Volkswagen ESVW1 Experimental Safety Vehicle presented passive seat belts. Volvo tried to develop a passive three point seat belt. In 1973, Volkswagen announced they had a functional passive seat belt. The first commercial car to use automatic seat belts was the 1975 Volkswagen Golf.\nAutomatic seat belts received a boost in the United States in 1977 when Brock Adams, United States Secretary of Transportation in the Carter Administration, mandated that by 1983 every new car should have either airbags or automatic seat belts. There was strong lobbying against the passive restraint requirement by the auto industry. Adams was criticized by Ralph Nader, who said that the 1983 deadline was too late. The Volkswagen Rabbit also had automatic seat belts, and VW said that by early 1978, 90,000 cars had sold with them.\nGeneral Motors introduced a three-point non-motorized passive belt system in 1980 to comply with the passive restraint requirement. However, it was used as an active lap-shoulder belt because of unlatching the belt to exit the vehicle. Despite this common practice, field studies of belt use still showed an increase in wearing rates with this door-mounted system. General Motors began offering automatic seat belts on the Chevrolet Chevette. However, the company reported disappointing sales because of this feature. For the 1981 model year, the new Toyota Cressida became the first car to offer motorized automatic passive seat belts.\nA study released in 1978 by the United States Department of Transportation said that cars with automatic seat belts had a fatality rate of .78 per 100 million miles, compared with 2.34 for cars with regular, manual belts.\nIn 1981, Drew Lewis, the first Transportation Secretary of the Reagan Administration, influenced by studies done by the auto industry, dropped the mandate; the decision was overruled in a federal appeals court the following year, and then by the Supreme Court. In 1984, the Reagan Administration reversed its course, though in the meantime the original deadline had been extended; Elizabeth Dole, then Transportation Secretary, proposed that the two passive safety restraints be phased into vehicles gradually, from vehicle model year 1987 to vehicle model year 1990, when all vehicles would be required to have either automatic seat belts or driver side air bags. Though more awkward for vehicle occupants, most manufacturers opted to use less expensive automatic belts rather than airbags during this time period.\nWhen driver side airbags became mandatory on all passenger vehicles in model year 1995, most manufacturers stopped equipping cars with automatic seat belts. Exceptions include the 1995\u201396 Ford Escort/Mercury Tracer and the Eagle Summit Wagon, which had automatic safety belts along with dual airbags.\nDisadvantages.\nAutomatic belt systems generally offer inferior occupant crash protection. In systems with belts attached to the door rather than a sturdier fixed portion of the vehicle body, a crash that causes the vehicle door to open leaves the occupant without belt protection. In such a scenario, the occupant may be thrown from the vehicle and suffer greater injury or death.\nBecause many automatic belt system designs compliant with the U.S. passive-restraint mandate did not meet the anchorage requirements of Canada (CMVSS 210)\u2014which were not weakened to accommodate automatic belts\u2014vehicle models that had been eligible for easy importation in either direction across the U.S.-Canada border when equipped with manual belts became ineligible for importation in either direction once the U.S. variants obtained automatic belts and the Canadian versions retained manual belts, although some Canadian versions also had automatic seat belts. Two particular models affected were the Dodge Spirit and Plymouth Acclaim.\nAutomatic belt systems also present several operational disadvantages. Motorists who would normally wear seat belts must still fasten the manual lap belt, thus rendering redundant the automation of the shoulder belt. Those who do not fasten the lap belt wind up inadequately protected only by the shoulder belt. In a crash, without a lap belt, such a vehicle occupant is likely to \"submarine\" (be thrown forward under the shoulder belt) and be seriously injured. Motorized or door-affixed shoulder belts hinder access to the vehicle, making it difficult to enter and exit\u2014particularly if the occupant is carrying items such as a box or a purse. Vehicle owners tend to disconnect the motorized or door-affixed shoulder belt to relieve the nuisance when entering and exiting the vehicle, leaving only a lap belt for crash protection. Also, many automatic seat belt systems are incompatible with child safety seats, or only compatible with special modifications.\nHomologation and testing.\nStarting in 1971 and ending in 1972, the United States conducted a research project on seat belt effectiveness on a total of 40,000 vehicle occupants using car accident reports collected during that time. Of these 40,000 occupants, 18% were reported wearing lap belts, or two-point safety belts, 2% were reported wearing a three-point safety belt, and the remaining 80% were reported as wearing no safety belt. The results concluded that users of the two-point lap belt had a 73% lower fatality rate, a 53% lower serious injury rate, and a 38% lower injury rate than the occupants that were reported unrestrained. Similarly, users of the three-point safety belt had a 60% lower serious injury rate and a 41% lower rate of all other injuries. Out of the 2% described as wearing a three-point safety belt, no fatalities were reported.\nThis study and others led to the Restraint Systems Evaluation Program (RSEP), started by the NHTSA in 1975 to increase the reliability and authenticity of past studies. A study as part of this program used data taken from 15,000 tow-away accidents that involved only car models made between 1973 and 1975. The study found that for injuries considered \"moderate\" or worse, individuals wearing a three-point safety belt had a 56.5% lower injury rate than those wearing no safety belt. The study also concluded that the effectiveness of the safety belt did not differ with the size of a car. It was determined that the variation among results of the many studies conducted in the 1960s and 70s was due to the use of different methodologies, and could not be attributed to any significant variation in the effectiveness of safety belts.\nWayne State University's Automotive Safety Research Group, as well as other researchers, are testing ways to improve seat belt effectiveness and general vehicle safety apparatuses. Wayne State's Bioengineering Center uses human cadavers in their crash test research. The center's director, Albert King, wrote in 1995 that the vehicle safety improvements made possible since 1987 by the use of cadavers in research had saved nearly 8,500 lives each year, and indicated that improvements made to three-point safety belts save an average of 61 lives every year.\nThe New Car Assessment Program (NCAP) was put in place by the United States National Highway Traffic Safety Administration in 1979. The NCAP is a government program that evaluates vehicle safety designs and sets standards for foreign and domestic automobile companies. The agency developed a rating system and requires access to safety test results. As of \u00a02007[ [update]], manufacturers are required to place an NCAP star rating on the automobile price sticker.\nIn 2004, the European New Car Assessment Program (Euro NCAP) started testing seat belts and whiplash safety on all test cars at the Thatcham Research Centre with crash test dummies.\nExperimental.\nResearch and development efforts are ongoing to improve the safety performance of vehicle seat belts. Some experimental designs include:\nIn rear seats.\nFord offered optional lap belts for the rear seats as part of its 1956 \"Lifeguard\" safety package. Volvo fitted rear belt anchor points by 1958, began installing rear lap belts in 1967, and upgraded the rear restraints to three-point belts in 1972.\nRegulation.\nIn the United States, Federal Motor Vehicle Safety Standard (FMVSS) No. 208 required lap-shoulder belts at rear outboard seats for passenger cars manufactured after 11 December 1989 (and for light trucks, vans and SUVs after 1 September 1991); lap belts had previously been required at rear forward-facing positions. The U.S. also finalized a rule in 2024 to require rear seat-belt reminder systems on new vehicles beginning with the 2027 model year (with updated front-seat reminders from 2026).\nEffectiveness and risk.\nRear seat belts substantially reduce injury risk for rear occupants; analyses of U.S. crash data report that lap-shoulder belts reduce fatality risk by roughly 44\u201373% depending on vehicle type. Unbelted rear-seat passengers also elevate risk to belted front-seat occupants; multiple studies have found the driver's fatality risk is increased several-fold when the rear passenger is unrestrained.\nChild occupants.\nAs with adult drivers and passengers, the advent of seat belts was accompanied by calls for their use by child occupants, including legislation requiring such use. Generally, children using adult seat belts suffer significantly lower injury risk when compared to non-buckled children.\nThe UK extended compulsory seat belt wearing to child passengers under the age of 14 in 1989. It was observed that this measure was accompanied by a 10% \"increase\" in fatalities and a 12% \"increase\" in injuries among the target population. In crashes, small children who wear adult seat belts can suffer \"seat-belt syndrome\" injuries including severed intestines, ruptured diaphragms, and spinal damage. There is also research suggesting that children in inappropriate restraints are at significantly increased risk of head injury. One of the authors of this research said, \"The early graduation of kids into adult lap and shoulder belts is a leading cause of child-occupant injuries and deaths.\"\nAs a result of such findings, many jurisdictions now advocate or require child passengers to use specially designed child restraints. Such systems include separate child-sized seats with their own restraints and booster cushions for children using adult restraints. In some jurisdictions, children below a certain size are forbidden to travel in front car seats.\nAutomated reminders and engine start interlocks.\nIn Europe, the U.S., and some other parts of the world, most modern cars include a seat-belt reminder light for the driver and some also include a reminder for the passenger, when present, activated by a pressure sensor under the passenger seat. Some cars will intermittently flash the reminder light and sound the chime until the driver (and sometimes the front passenger, if present) fasten their seat belts.\nIn 2005, in Sweden, 70% of all cars that were newly registered were equipped with seat belt reminders for the driver. Since November 2014, seat belt reminders are mandatory for the driver's seat on new cars sold in Europe.\nTwo specifications define the standardseat belt reminders: UN Regulation 16, Section 8.4 and the Euro NCAP assessment protocol (Euro NCAP, 2013).\nEuropean Union seat belt reminder.\nIn the European Union, seat belt reminders are mandatory in all new passenger cars for the driver seat. In 2014, EC Regulation 661/2009 made UN Regulation 16 applicable.\nAn amendment of UN Regulation 16 made seat belt reminders mandatory in\nThis improvement applies from 1 September 2019 for new types of motor vehicles and from 1 September 2021 for all new motor vehicles.\nU.S. regulation history.\nFederal Motor Vehicle Safety Standard No. 208 (FMVSS 208) was amended by the NHTSA to require a seat belt/starter interlock system to prevent passenger cars from being started with an unbelted front-seat occupant. This mandate applied to passenger cars built after August 1973, i.e., starting with the 1974 model year. The specifications required the system to permit the car to be started only if the belt of an occupied seat were fastened after the occupant sat down, so pre-buckling the belts would not defeat the system.\nThe interlock systems used logic modules complex enough to require special diagnostic computers, and were not entirely dependable\u2014an override button was provided under the hood of equipped cars, permitting one (but only one) \"free\" starting attempt each time it was pressed. However, the interlock system spurred severe backlash from an American public who largely rejected seat belts. In 1974, Congress acted to prohibit NHTSA from requiring or permitting a system that prevents a vehicle from starting or operating with an unbelted occupant, or that gives an audible warning of an unfastened belt for more than 8 seconds after the ignition is turned on. This prohibition took effect on 27 October 1974, shortly after the 1975 model year began.\nIn response to the Congressional action, NHTSA once again amended FMVSS 208, requiring vehicles to come with a seat belt reminder system that gives an audible signal for four to eight seconds and a warning light for at least 60 seconds after the ignition is turned on if the driver's seat belt is not fastened. This is called a seat belt reminder (SBR) system. In the mid-1990s, the Swedish insurance company Folksam worked with Saab and Ford to determine the requirements for the most efficient seat belt reminder. One characteristic of the optimal SBR, according to their research, is that the audible warning becomes increasingly penetrating the longer the seat belt remains unfastened.\nEfficacy.\nIn 2001, the House Appropriations Committee directed NHTSA to study the benefits of technology meant to increase the use of seat belts. NHTSA found that seat belt usage had increased to 73% since the initial introduction of the SBR system. In 2002, Ford demonstrated that seat belts were used more in Fords with seat belt reminders than in those without: 76% and 71% respectively. In 2007, Honda conducted a similar study and found that 90% of people who drove Hondas with seat belt reminders used a seat belt, while 84% of people who drove Hondas without seat belt reminders used a seat belt.\nIn 2003, the Transportation Research Board Committee, chaired by two psychologists, reported that \"Enhanced SBRs\" (ESBRs) could save an additional 1,000 lives a year. Research by the Insurance Institute for Highway Safety found that Ford's ESBR, which provides an intermittent chime for up to five minutes if the driver is unbelted, sounding for six seconds then pausing for 30, increased seat belt use by five percentage points. Farmer and Wells found that driver fatality rates were 6% lower for vehicles with ESBR compared with otherwise-identical vehicles without.\nDelayed start.\nStarting with the 2020 model year, some Chevrolet cars refused to shift from Park to Drive for 20 seconds if the driver is unbuckled and the car is in \"teen driver\" mode. A similar feature was previously available on some General Motors fleet cars.\nRegulation by country.\nInternational regulations.\nSeveral countries apply UN-ECE vehicle regulations 14 and 16:\nLegislation.\nIn observational studies of car crash morbidity and mortality, experiments using both crash test dummies and human cadavers indicate that wearing seat belts greatly reduces the risk of death and injury in the majority of car crashes.\nThis has led many countries to adopt mandatory seat belt wearing laws. It is generally accepted that, in comparing like-for-like accidents, a vehicle occupant not wearing a properly fitted seat belt has a significantly and substantially higher chance of death and serious injury. One large observation studying using U.S. data showed that the odds ratio of crash death is 0.46 with a three-point belt when compared with no belt. In another study that examined injuries presenting to the ER pre- and post-seat belt law introduction, it was found that 40% more escaped injury and 35% more escaped mild and moderate injuries.\nThe effects of seat belt laws are disputed by those who observe that their passage did not reduce road fatalities. There has also been concern that instead of legislating for a general protection standard for vehicle occupants, laws that required a particular technical approach would rapidly become dated as motor manufacturers would tool up for a particular standard that could not easily be changed. For example, in 1969 there were competing designs for lap and three-point seat belts, rapidly tilting seats, and airbags being developed. As countries started to mandate seat belt restraints the global auto industry invested in the tooling and standardized exclusively on seat belts, and ignored other restraint designs such as airbags for several decades\nAs of 2016, seat belt laws can be divided into two categories: primary and secondary. A primary seat belt law allows an officer to issue a citation for lack of seat belt use without any other citation, whereas a secondary seat belt law allows an officer to issue a seat belt citation only in the presence of a different violation. In the United States, fifteen states enforce secondary laws, while 34 states, as well as the District of Columbia, American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the Virgin Islands enforce primary seat belt laws. New Hampshire lacks either a primary or secondary seat belt law.\nRisk compensation.\nSome have proposed that the number of deaths was influenced by the development of risk compensation, which says that drivers adjust their behavior in response to the increased sense of personal safety wearing a seat belt provides.\nIn one trial subjects were asked to drive go-karts around a track under various conditions. It was found that subjects who started driving unbelted drove consistently faster when subsequently belted. Similarly, a study of habitual non-seat belt wearers driving in freeway conditions found evidence that they had adapted to use by adopting higher driving speeds and closer following distances.\nA 2001 analysis of U.S. crash data aimed to establish the effects of legislation on driving fatalities and found that previous estimates of seat belt effectiveness had been significantly overstated.\nAccording to the analysis, seat belts decreased fatalities by 1.35% for each 10% increase in seat belt use. The study controlled for endogenous motivations for seat belt use, because they create an artificial correlation between seat belt use and fatalities, leading to the conclusion that seat belts cause fatalities. For example, drivers in high-risk areas are more likely to use seat belts and are more likely to be in accidents, creating a non-causal correlation between seat belt use and mortality. After accounting for the endogeneity of seat belt usage, Cohen and Einav found no evidence that the risk compensation effect makes seat belt-wearing drivers more dangerous, a finding at variance with other research.\nIncreased traffic.\nOther statistical analyses have included adjustments for factors such as increased traffic and age, and based on these adjustments, which results in a reduction of morbidity and mortality due to seat belt use. However, Smeed's law predicts a fall in accident rate with increasing car ownership and has been demonstrated independently of seat belt legislation.\nMass transit considerations.\nBuses.\nSchool buses.\nIn the U.S., six states\u2014California, Florida, Louisiana, New Jersey, New York, and Texas\u2014require seat belts on school buses.\nPros and cons have been debated about the use of seat belts in school buses. School buses, which are much bigger than the average vehicle, allow for the mass transportation of students from place to place. The American School Bus Council states in a brief article: \"The children are protected like eggs in an egg carton\u2014compartmentalized, and surrounded with padding and structural integrity to secure the entire container.\" Although school buses are considered safe for mass transit of students, this will not guarantee that the students will be injury-free if an impact were to occur. Seat belts in buses are sometimes believed to make recovering from a roll or tip harder for passengers, as they could be easily trapped in their own safety belts.\nIn 2015, for the first time, NHTSA endorsed seat belts on school buses.\nMotor coaches.\nIn the European Union, all new long-distance buses and coaches must be fitted with seat belts.\nAustralia has required lap/sash seat belts in new coaches since 1994. These must comply with Australian Design Rule 68, which requires the seat belt, seat and seat anchorage to withstand 20g deceleration and an impact by an unrestrained occupant to the rear.\nIn the United States, NHTSA now requires lap-shoulder seat belts in new \"over-the-road\" buses (includes most coaches) starting in 2016.\nTrains.\nThe use of seat belts in trains has been investigated. Concerns about survival space intrusion in train crashes and increased injuries to unrestrained or incorrectly restrained passengers led researchers to discourage the use of seat belts in trains.\nIt has been shown that there is no net safety benefit for passengers who choose to wear 3-point restraints on passenger-carrying rail vehicles. Generally, passengers who choose not to wear restraints in a vehicle modified to accept 3-point restraints receive marginally more severe injuries.\nIn the United States, recreational railbikes are fitted with lap belts.\nAirplanes.\nAll aerobatic aircraft and gliders (sailplanes) are fitted with four or five-point harnesses, as are many types of light aircraft and many types of military aircraft. The seat belts in these aircraft have the dual function of crash protection and keeping the pilot(s) and crew in their seat(s) during turbulence and aerobatic maneuvers. Airliner passenger seats are fitted with lap belts. Unlike road vehicles, airliner seat belts are not primarily designed for crash protection. Their main purpose is to keep passengers in their seats during events such as turbulence. Many civil aviation authorities require a \"fasten seat belt\" sign in the cabin that can be activated by a pilot during taxiing, takeoff, turbulence, and landing. The International Civil Aviation Organization recommends the use of child restraints. Some airline authorities, including the UK Civil Aviation Authority (CAA), permit the use of airline infant lap belts (sometimes known as an infant loop or belly belt) to secure an infant under age two sitting on an adult's lap.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51475", "revid": "19569748", "url": "https://en.wikipedia.org/wiki?curid=51475", "title": "The Islamic Republic of Afghanistan", "text": ""}
{"id": "51477", "revid": "372693", "url": "https://en.wikipedia.org/wiki?curid=51477", "title": "Mycorrihizae", "text": ""}
{"id": "51478", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=51478", "title": "Rolf Singer", "text": "German mycologist (1906\u20131994)\nRolf Singer (June 23, 1906 \u2013 January 18, 1994) was a German mycologist and taxonomist of gilled mushrooms (agarics).\nHe wrote the book \"The Agaricales in Modern Taxonomy\". He fled to various countries during the Nazi period, pursuing mycology in the Soviet Union, Argentina, and finally the United States, as mycologist at the Field Museum in Chicago.\nCareer.\nAfter receiving his Ph.D. at the University of Vienna in 1931 he worked in Munich. By 1933, however, Singer left Germany for Vienna due to the political deterioration in Germany. There he met his wife, Martha Singer. From Vienna, Singer and his wife went to Barcelona, Spain, where Singer was appointed assistant professor at the Autonomous University of Barcelona. Persecution by the Spanish authorities on behalf of the German government forced Singer to leave Spain for France in 1934. After a fellowship at the Museum d'Histoire Naturelle in Paris, Singer again moved, this time to Leningrad, where he was senior scientific expert at the Botanical Garden of the Academy of Sciences of the USSR. During his time at the academy, Singer made many expeditions to Siberia, the Altai Mountains, and Karelia. In 1941, Singer emigrated to the United States. He was offered a position at Harvard University's Farlow Herbarium as a research associate, then as Assistant Curator, then as acting Curator following the death of Dr. David Linder. He spent a total of seven years at the Farlow. During this time, Singer also received a Guggenheim Fellowship for studies in Florida, and taught at the Mountain Lake Biological Station of the University of Virginia.\nIn 1948, Singer left Harvard to become professor at the Universidad Nacional de Tucuman in Argentina. Later, in 1961, Singer became professor at the Universidad de Buenos Aires. During his time in South America, Singer, his wife, and his daughter Heidi collected extensively. Singer's last faculty appointment was at the University of Illinois at Chicago, from 1968 to 1977.\nSinger was an author (or co-author) of new fungal species, having formally described 2260 in his career. He was also a prolific writer, with more than 400 publications to his name.\nHonours.\nHe has been honoured in the naming of several taxa of fungi. Including; \"Singeriella\" Petr. in 1959 (in Vizellaceae family), \"Singera\" Bat. &amp; J.L.Bezerra 1960 (Vermiculariopsiellaceae family), Mesosingeria S.Archang. 1963 (Fossil, order Cycadales), \"Singeromyces\" M.M.Mosera, 1966 (in the family Boletaceae),\n\"Singerina\" Sathe &amp; S.D.Deshp. 1981 (in the family Agaricaceae), and lastly, \"Singerocomus\" T.W.Henkel &amp; M.E.Sm. 2016 (Boletaceae family).\n&lt;templatestyles src=\"Botanist/styles.css\"/&gt;The standard author abbreviation Singer is used to indicate this person as the author when citing a botanical name.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51480", "revid": "34839797", "url": "https://en.wikipedia.org/wiki?curid=51480", "title": "Hypha", "text": "Long, filamentous structure in fungi and Actinobacteria\n \nA hypha (from grc \" ' ()\"\u00a0'web'; pl.\u2009hyphae) is a long, branching, filamentous structure of a fungus, oomycete, or actinobacterium. In most fungi, hyphae are the main mode of vegetative growth, and are collectively called a mycelium.\nStructure.\nA hypha consists of one or more cells surrounded by a tubular cell wall. In most fungi, hyphae are divided into cells by internal cross-walls called \"septa\" (singular septum). Septa are usually perforated by pores large enough for ribosomes, mitochondria, and sometimes nuclei to flow between cells. The major structural polymer in fungal cell walls is typically chitin, in contrast to plants and oomycetes that have cellulosic cell walls. Some fungi have aseptate hyphae, meaning their hyphae are not partitioned by septa.\nHyphae have an average diameter of 4\u20136 \u03bcm.\nGrowth.\nHyphae grow at their tips. During tip growth, cell walls are extended by the external assembly and polymerization of cell wall components, and the internal production of new cell membrane. The Spitzenk\u00f6rper is an intracellular organelle associated with tip growth. It is composed of an aggregation of membrane-bound vesicles containing cell wall components. The Spitzenk\u00f6rper is part of the endomembrane system of fungi, holding and releasing vesicles it receives from the Golgi apparatus. These vesicles travel to the cell membrane via the cytoskeleton and release their contents (including various cysteine-rich proteins including cerato-platanins and hydrophobins) outside the cell by the process of exocytosis, where they can then be transported to where they are needed. Vesicle membranes contribute to growth of the cell membrane while their contents form new cell wall. The Spitzenk\u00f6rper moves along the apex of the hyphal strand and generates apical growth and branching; the apical growth rate of the hyphal strand parallels and is regulated by the movement of the Spitzenk\u00f6rper.\nAs a hypha extends, septa may be formed behind the growing tip to partition each hypha into individual cells. Hyphae can branch through the bifurcation of a growing tip, or by the emergence of a new tip from an established hypha.\nBehaviour.\nThe direction of hyphal growth can be controlled by environmental stimuli, such as the application of an electric field. Hyphae can also sense reproductive units from some distance, and grow towards them. Hyphae can weave through a permeable surface to penetrate it.\nModifications.\nHyphae may be modified in many different ways to serve specific functions. Some parasitic fungi form haustoria that function in absorption within the host cells. The arbuscules of mutualistic mycorrhizal fungi serve a similar function in nutrient exchange, so are important in assisting nutrient and water absorption by plants. Ectomycorrhizal extramatrical mycelium greatly increases the soil area available for exploitation by plant hosts by funneling water and nutrients to ectomycorrhizas, complex fungal organs on the tips of plant roots. Hyphae are found enveloping the gonidia in lichens, making up a large part of their structure. In nematode-trapping fungi, hyphae may be modified into trapping structures such as constricting rings and adhesive nets. Mycelial cords can be formed to transfer nutrients over larger distances. Bulk fungal tissues, cords, and membranes, such as those of mushrooms and lichens, are mainly composed of felted and often anastomosed hyphae.\nTypes.\nClassification based on cell wall and overall form.\nCharacteristics of hyphae can be important in fungal classification. In basidiomycete taxonomy, hyphae that comprise the fruiting body can be identified as generative, skeletal, or binding hyphae.\nBased on the generative, skeletal and binding hyphal types, in 1932 E. J. H. Corner applied the terms monomitic, dimitic, and trimitic to hyphal systems, in order to improve the classification of polypores.\nFungi that form fusiform skeletal hyphae bound by generative hyphae are said to have sarcodimitic hyphal systems. A few fungi form fusiform skeletal hyphae, generative hyphae, and binding hyphae, and these are said to have sarcotrimitic hyphal systems. These terms were introduced as a later refinement by E. J. H. Corner in 1966.\nClassification based on refractive appearance.\nHyphae are described as \"gloeoplerous\" (\"gloeohyphae\") if their high refractive index gives them an oily or granular appearance under the microscope. These cells may be yellowish or clear (hyaline). They can sometimes selectively be coloured by sulphovanillin or other reagents. The specialized cells termed cystidia can also be gloeoplerous.\nClassification based on growth location.\nHyphae might be categorized as 'vegetative' or 'aerial.' Aerial hyphae of fungi produce asexual reproductive spores.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51481", "revid": "17750005", "url": "https://en.wikipedia.org/wiki?curid=51481", "title": "Jurist", "text": "Person with expert knowledge of law\nA jurist is a person with expert knowledge of law; someone who analyzes and comments on law. This person is usually a specialist legal scholar, mostly (but not always) with a formal education in law (a law degree) and often a legal practitioner. \nIn the United Kingdom the term \"jurist\" is mostly used for legal academics, while in the United States the term may also be applied to a judge. With reference to Roman law, a \"jurist\" (in English) is a jurisconsult (\"iurisconsultus\").\nThe English term \"jurist\" is to be distinguished from similar terms in other European languages, where it may be synonymous with legal professional, meaning anyone with a professional law degree that qualifies for admission to the legal profession, including such positions as judge or attorney. In Germany, Scandinavia and a number of other countries \"jurist\" denotes someone with a professional law degree, and it may be a protected title, for example in Norway. Thus the term can be applied to attorneys, judges and academics, provided that they hold a qualifying professional law degree. In Germany \u2013 the term \"full jurist\" is sometimes used informally to denote someone who has completed the two state examinations in law that qualify for practising law, to distinguish from someone who may have only the first state examination or some other form of legal qualification that does not qualify for practising law.\nNotable jurists.\nSome notable historical jurists include:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51482", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=51482", "title": "Parsitism", "text": ""}
{"id": "51483", "revid": "27073799", "url": "https://en.wikipedia.org/wiki?curid=51483", "title": "Mutualism (biology)", "text": "Mutually beneficial interaction between species\nMutualism describes the ecological interaction between two or more species where each species has a net benefit. Mutualism is a common type of ecological interaction. Prominent examples are:\nMutualism can be contrasted with interspecific competition, in which each species experiences \"reduced\" fitness, and exploitation, and with parasitism, in which one species benefits at the expense of the other. However, mutualism may evolve from interactions that began with imbalanced benefits, such as parasitism.\nThe term \"mutualism\" was introduced by Pierre-Joseph van Beneden in his 1876 book \"Animal Parasites and Messmates\" to mean \"mutual aid among species\".\nMutualism is often conflated with two other types of ecological phenomena: cooperation and symbiosis. Cooperation most commonly refers to increases in fitness through within-species (intraspecific) interactions, although it has been used (especially in the past) to refer to mutualistic interactions, and it is sometimes used to refer to mutualistic interactions that are not obligate. Symbiosis involves two species living in close physical contact over a long period of their existence and may be mutualistic, parasitic, or commensal, so symbiotic relationships are not always mutualistic, and mutualistic interactions are not always symbiotic. Despite a different definition between mutualism and symbiosis, they have been largely used interchangeably in the past, and confusion on their use has persisted.\nMutualism plays a key part in ecology and evolution. For example, mutualistic interactions are vital for terrestrial ecosystem function as:\nA prominent example of pollination mutualism is with bees and flowering plants. Bees use these plants as their food source with pollen and nectar. In turn, they transfer pollen to other nearby flowers, inadvertently allowing for cross-pollination. Cross-pollination has become essential in plant reproduction and fruit/seed production. The bees get their nutrients from the plants, and allow for successful fertilization of plants, demonstrating a mutualistic relationship between two seemingly-unlike species.\nMutualism has also been linked to major evolutionary events, such as the evolution of the eukaryotic cell (symbiogenesis) and the colonization of land by plants in association with mycorrhizal fungi.\nTypes.\nResource-resource relationships.\nMutualistic relationships can be thought of as a form of \"biological barter\" in mycorrhizal associations between plant roots and fungi, with the plant providing carbohydrates to the fungus in return for primarily phosphate but also nitrogenous compounds. Other examples include rhizobia bacteria that fix nitrogen for leguminous plants (family Fabaceae) in return for energy-containing carbohydrates. Metabolite exchange between multiple mutualistic species of bacteria has also been observed in a process known as cross-feeding.\nService-resource relationships.\nService-resource relationships are common. Three important types are pollination, cleaning symbiosis, and zoochory.\nIn pollination, a plant trades food resources in the form of nectar or pollen for the service of pollen dispersal. However, daciniphilous \"Bulbophyllum\" orchid species trade sex pheromone precursor or booster components via floral synomones/attractants in a true mutualistic interactions with males of Dacini fruit flies (Diptera: Tephritidae: Dacinae).\nPhagophiles feed (resource) on ectoparasites, thereby providing anti-pest service, as in cleaning symbiosis.\n\"Elacatinus\" and \"Gobiosoma\", genera of gobies, feed on ectoparasites of their clients while cleaning them.\nZoochory is the dispersal of the seeds of plants by animals. This is similar to pollination in that the plant produces food resources (for example, fleshy fruit, overabundance of seeds) for animals that disperse the seeds (service). Plants may advertise these resources using colour and a variety of other fruit characteristics, e.g., scent. Fruit of the aardvark cucumber \"(Cucumis humifructus)\" is buried so deeply that the plant is solely reliant upon the aardvark's keen sense of smell to detect its ripened fruit, extract, consume and then scatter its seeds; \"C.\u00a0humifructus\"'s geographical range is thus restricted to that of the aardvark.\nAnother type is ant protection of aphids, where the aphids trade sugar-rich honeydew (a by-product of their mode of feeding on plant sap) in return for defense against predators such as ladybugs.\nService-service relationships.\nStrict service-service interactions are very rare, for reasons that are far from clear. One example is the relationship between sea anemones and anemone fish in the family Pomacentridae: the anemones provide the fish with protection from predators (which cannot tolerate the stings of the anemone's tentacles) and the fish defend the anemones against butterflyfish (family Chaetodontidae), which eat anemones. However, in common with many mutualisms, there is more than one aspect to it: in the anemonefish-anemone mutualism, waste ammonia from the fish feeds the symbiotic algae that are found in the anemone's tentacles. Therefore, what appears to be a service-service mutualism in fact has a service-resource component. A second example is that of the relationship between some ants in the genus \"Pseudomyrmex\" and trees in the genus \"Acacia\", such as the whistling thorn and bullhorn acacia. The ants nest inside the plant's thorns. In exchange for shelter, the ants protect acacias from attack by herbivores (which they frequently eat when those are small enough, introducing a resource component to this service-service relationship) and competition from other plants by trimming back vegetation that would shade the acacia. In addition, another service-resource component is present, as the ants regularly feed on lipid-rich food-bodies called Beltian bodies that are on the \"Acacia\" plant.\nIn the neotropics, the ant \"Myrmelachista schumanni\" makes its nest in special cavities in \"Duroia hirsuta\". Plants in the vicinity that belong to other species are killed with formic acid. This selective gardening can be so aggressive that small areas of the rainforest are dominated by \"Duroia hirsute\". These peculiar patches are known by local people as \"devil's gardens\".\nIn some of these relationships, the cost of the ant's protection can be quite expensive. \"Cordia\" sp. trees in the Amazon rainforest have a kind of partnership with \"Allomerus\" sp. ants, which make their nests in modified leaves. To increase the amount of living space available, the ants will destroy the tree's flower buds. The flowers die and leaves develop instead, providing the ants with more dwellings. Another type of \"Allomerus\" sp. ant lives with the \"Hirtella\" sp. tree in the same forests, but in this relationship, the tree has turned the tables on the ants. When the tree is ready to produce flowers, the ant abodes on certain branches begin to wither and shrink, forcing the occupants to flee, leaving the tree's flowers to develop free from ant attack.\nThe term \"species group\" can be used to describe the manner in which individual organisms group together. In this non-taxonomic context one can refer to \"same-species groups\" and \"mixed-species groups.\" While same-species groups are the norm, examples of mixed-species groups abound. For example, zebra (\"Equus burchelli\") and wildebeest (\"Connochaetes taurinus\") can remain in association during periods of long distance migration across the Serengeti as a strategy for thwarting predators. \"Cercopithecus mitis\" and \"Cercopithecus ascanius\", species of monkey in the Kakamega Forest of Kenya, can stay in close proximity and travel along exactly the same routes through the forest for periods of up to 12 hours. These mixed-species groups cannot be explained by the coincidence of sharing the same habitat. Rather, they are created by the active behavioural choice of at least one of the species in question.\nProtocooperation.\nProtocooperation is a form of mutualism, but the cooperating species do not depend on each other for survival. The term, initially used for intraspecific interactions, was popularized by Eugene Odum (1953), although it is now rarely used.\nEvolution.\nMutualistic symbiosis can sometimes evolve from parasitism or commensalism. Symbiogenesis, a leading theory on the evolution of Eukaryotes states the origin of the mitochondria and cell nucleus emerged from a parasitic relationship of ancient Archaea and Bacteria. Fungi's relationship to plants in the form of mycelium evolved from parasitism and commensalism. Under certain conditions species of fungi previously in a state of mutualism can turn parasitic on weak or dying plants. Likewise the symbiotic relationship of clown fish and sea anemones emerged from a commensalist relationship. Once a mutualistic relationship emerges both symbionts are pushed towards co-evolution with each other.\nMathematical modeling.\nMathematical treatments of mutualisms, like the study of mutualisms in general, have lagged behind those for predation, or predator-prey, consumer-resource, interactions. In models of mutualisms, the terms \"type I\" and \"type II\" functional responses refer to the linear and saturating relationships, respectively, between the \"benefit\" provided to an individual of species 1 (dependent variable) and the \"density\" of species 2 (independent variable).\nType I functional response.\nOne of the simplest frameworks for modeling species interactions is the Lotka\u2013Volterra equations. In this model, the changes in population densities of the two mutualists are quantified as:\nformula_1\nwhere\nMutualism is in essence the logistic growth equation modified for mutualistic interaction. The mutualistic interaction term represents the increase in population growth of one species as a result of the presence of greater numbers of another species. As the mutualistic interactive term \u03b2 is always positive, this simple model may lead to unrealistic unbounded growth. So it may be more realistic to include a further term in the formula, representing a saturation mechanism, to avoid this occurring.\nType II functional response.\nIn 1989, David Hamilton Wright modified the above Lotka\u2013Volterra equations by adding a new term, \"\u03b2M\"/\"K\", to represent a mutualistic relationship. Wright also considered the concept of saturation, which means that with higher densities, there is a decrease in the benefits of further increases of the mutualist population. Without saturation, depending on the size of parameter \u03b1, species densities would increase indefinitely. Because that is not possible due to environmental constraints and carrying capacity, a model that includes saturation would be more accurate. Wright's mathematical theory is based on the premise of a simple two-species mutualism model in which the benefits of mutualism become saturated due to limits posed by handling time. Wright defines handling time as the time needed to process a food item, from the initial interaction to the start of a search for new food items and assumes that processing of food and searching for food are mutually exclusive. Mutualists that display foraging behavior are exposed to the restrictions on handling time. Mutualism can be associated with symbiosis.\nIn 1959, C. S. Holling performed his classic disc experiment that assumed that\nformula_6\nwhere\nThe equation that incorporates Type II functional response and mutualism is:\nformula_7\nwhere\nor, equivalently,\nformula_8\nwhere\nThis model is most effectively applied to free-living species that encounter a number of individuals of the mutualist part in the course of their existences. Wright notes that models of biological mutualism tend to be similar qualitatively, in that the featured isoclines generally have a positive decreasing slope, and by and large similar isocline diagrams. Mutualistic interactions are best visualized as positively sloped isoclines, which can be explained by the fact that the saturation of benefits accorded to mutualism or restrictions posed by outside factors contribute to a decreasing slope.\nThe type II functional response is visualized as the graph of formula_9 \"vs.\" \"M\".\nStructure of networks.\nMutualistic networks made up out of the interaction between plants and pollinators were found to have a similar structure in very different ecosystems on different continents, consisting of entirely different species. The structure of these mutualistic networks may have large consequences for the way in which pollinator communities respond to increasingly harsh conditions and on the community carrying capacity.\nMathematical models that examine the consequences of this network structure for the stability of pollinator communities suggest that the specific way in which plant-pollinator networks are organized minimizes competition between pollinators, reduce the spread of indirect effects and thus enhance ecosystem stability and may even lead to strong indirect facilitation between pollinators when conditions are harsh. This means that pollinator species together can survive under harsh conditions. But it also means that pollinator species collapse simultaneously when conditions pass a critical point. This simultaneous collapse occurs, because pollinator species depend on each other when surviving under difficult conditions.\nSuch a community-wide collapse, involving many pollinator species, can occur suddenly when increasingly harsh conditions pass a critical point and recovery from such a collapse might not be easy. The improvement in conditions needed for pollinators to recover could be substantially larger than the improvement needed to return to conditions at which the pollinator community collapsed.\nHumans.\nHumans are involved in mutualisms with other species: their gut flora is essential for efficient digestion. Infestations of head lice \"might\" have been beneficial for humans by fostering an immune response that helps to reduce the threat of body louse borne lethal diseases.\nSome relationships between humans and domesticated animals and plants are to different degrees mutualistic. For example, domesticated cereals that provide food for humans have lost the ability to spread seeds by shattering, a strategy that wild grains use to spread their seeds.\nIn traditional agriculture, some plants have mutualistic relationships as companion plants, providing each other with shelter, soil fertility or natural pest control. For example, beans may grow up cornstalks as a trellis, while fixing nitrogen in the soil for the corn, a phenomenon that is used in Three Sisters farming.\nOne researcher has proposed that the key advantage \"Homo sapiens\" had over Neanderthals in competing over similar habitats was the former's mutualism with dogs.\nIntestinal microbiota.\nThe microbiota in the human intestine coevolved with the human species, and this relationship is considered to be a mutualism that is beneficial both to the human host and the bacteria in the gut population. The mucous layer of the intestine contains commensal bacteria that produce bacteriocins, modify the pH of the intestinal contents, and compete for nutrition to inhibit colonization by pathogens. The gut microbiota, containing trillions of microorganisms, possesses the metabolic capacity to produce and regulate multiple compounds that reach the circulation and act to influence the function of distal organs and systems. Breakdown of the protective mucosal barrier of the gut can contribute to the development of colon cancer.\nEvolution of mutualism.\nEvolution by type.\nEvery generation of every organism needs nutrients\u00a0\u2013 and similar nutrients\u00a0\u2013 more than they need particular defensive characteristics, as the fitness benefit of these vary heavily especially by environment. This may be the reason that hosts are more likely to evolve to become dependent on vertically transmitted bacterial mutualists which provide nutrients than those providing defensive benefits. This pattern is generalized beyond bacteria by Yamada et al. 2015's demonstration that undernourished \"Drosophila\" are heavily dependent on their fungal symbiont \"Issatchenkia orientalis\" for amino acids.\nMutualism breakdown.\nMutualisms are not static, and can be lost by evolution. Sachs and Simms (2006) suggest that this can occur via four main pathways:\nThere are many examples of mutualism breakdown. For example, plant lineages inhabiting nutrient-rich environments have evolutionarily abandoned mycorrhizal mutualisms many times independently. Evolutionarily, headlice may have been mutualistic as they allow for early immunity to various body-louse borne disease; however, as these diseases became eradicated, the relationship has become less mutualistic and more parasitic.\nMeasuring and defining mutualism.\nMeasuring the exact fitness benefit to the individuals in a mutualistic relationship is not always straightforward, particularly when the individuals can receive benefits from a variety of species, for example most plant-pollinator mutualisms. It is therefore common to categorise mutualisms according to the closeness of the association, using terms such as obligate and . Defining \"closeness\", however, is also problematic. It can refer to mutual dependency (the species cannot live without one another) or the biological intimacy of the relationship in relation to physical closeness (\"e.g.\", one species living within the tissues of the other species).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther references.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "51484", "revid": "790462290", "url": "https://en.wikipedia.org/wiki?curid=51484", "title": "Hyphae", "text": ""}
{"id": "51485", "revid": "48227547", "url": "https://en.wikipedia.org/wiki?curid=51485", "title": "German Workers' Party", "text": "Predecessor of the Nazi Party\nThe German Workers' Party (, DAP) was an obscure far-right political party established in the Weimar Republic after World War I. It lasted from 5 January 1919 until 24 February 1920. The DAP was the precursor of the National Socialist German Workers' Party (, NSDAP), commonly known as the Nazi Party.\nHistory.\nOrigins.\nOn 5 January 1919, the German Workers' Party (DAP) was founded in Munich in the hotel F\u00fcrstenfelder Hof by Anton Drexler, along with Dietrich Eckart, Gottfried Feder and Karl Harrer. It developed out of the \"Freien Arbeiterausschuss f\u00fcr einen guten Frieden\" (Free Workers' Committee for a Good Peace) league, a branch of which Drexler had founded in 1918. Thereafter in 1918, Harrer (a journalist and member of the Thule Society), convinced Drexler and several others to form the \"Politischer Arbeiterzirkel\" (Political Workers' Circle). The members met periodically for discussions with themes of nationalism and antisemitism. Drexler was encouraged to form the DAP in December 1918 by his mentor, Dr. Paul Tafel. Tafel was a leader of the Alldeutscher Verband (Pan-Germanist Union), a director of the Maschinenfabrik Augsburg-N\u00fcrnberg and a member of the Thule Society. Drexler's wish was for a political party which was both in touch with the masses and nationalist. When the DAP was founded in January 1919, Drexler was elected chairman and Harrer was made Reich Chairman, an honorary title. On 17 May, only ten members were present at the meeting, and a later meeting in August only noted 38 members attending. The members were mainly Drexler's colleagues from the Munich railway yards.\nAdolf Hitler's membership.\nAfter World War I ended, Adolf Hitler returned to Munich. Having no formal education or career prospects, he tried to remain in the army for as long as possible. In July 1919, he was appointed \"Verbindungsmann\" (intelligence agent) of an \"Aufkl\u00e4rungskommando\" (reconnaissance commando) of the \"Reichswehr\" to influence other soldiers and to investigate the DAP. While Hitler was initially unimpressed by the meetings and found them disorganised, he enjoyed the discussion that took place. During these investigations, Hitler became attracted to founder Anton Drexler's anti-Semitic, nationalist, anti-capitalist, and anti-Marxist ideas. While attending a party meeting at the \"Sterneckerbr\u00e4u\" beer hall on 12 September 1919, Hitler became involved in a heated political argument with a visitor, Professor Adalbert Baumann, who questioned the soundness of Gottfried Feder's arguments in support of Bavarian separatism and against capitalism. In vehemently attacking the man's arguments, he made an impression on the other party members with his oratory skills and, according to Hitler, Baumann left the hall acknowledging unequivocal defeat. Impressed with Hitler's oratory skills, Drexler encouraged him to join. On the orders of his army superiors, Hitler applied to join the party. Although Hitler initially wanted to form his own party, he claimed to have been convinced to join the DAP because it was small and he could eventually become its leader. He consequently encouraged the organisation to become less of a debating society, which it had been previously, and more of an active political party.\nIn less than a week, Hitler received a postcard stating he had officially been accepted as a member and he should come to a committee meeting to discuss it. Hitler attended the committee meeting held at the run-down Altes Rosenbad beer-house. Normally, enlisted army personnel were not allowed to join political parties. In this case, Hitler had Captain Karl Mayr's permission to join the DAP. Further, Hitler was allowed to stay in the army and receive his weekly pay of 20 gold marks. Unlike many other members of the organisation, this continued employment provided him with enough money to dedicate himself more fully to the DAP. At the time when Hitler joined the party, there were no membership numbers or cards. It was in January 1920 when a numeration was issued for the first time and listed in alphabetical order that Hitler received the number 555. In reality, he had been the 55th member, but the counting started at the number 501 in order to make the party appear larger. In his work \"Mein Kampf\", Hitler later claimed to be the seventh party member, but he was in fact the seventh executive member of the party's central committee.\nDuring 1919, the DAP set out an explicit program of being nationalistic, anti-Semitic, and anti-Marxist. Unlike other similar nationalist parties at the time, the DAP aimed its rhetoric towards working class Germans, hoping to cross class boundaries and recruit them. However, Hitler explicitly rejected the Marxist idea of dictatorship of the proletariat, and instead attempted to appeal to the working class to create a \"Volksgemeinschaft\", where German identity took precedence over class, religion, or other ideas.\nAfter giving his first speech for the DAP on 16 October at the \"Hofbr\u00e4ukeller\", Hitler quickly became the party's most active orator. Hitler's considerable oratory and propaganda skills were appreciated by the party leadership as crowds began to flock to hear his speeches during 1919\u20131920. Such was the popularity of Hitler's speaking skills, the party began charging an entry fee for visitors to hear his speeches. With the support of Drexler, Hitler became chief of propaganda for the party in early 1920. Hitler preferred that role as he saw himself as the drummer for a national cause. He saw propaganda as the way to bring nationalism to the public.\nFrom DAP to NSDAP.\nThe small number of party members were quickly won over to Hitler's political beliefs. He organized their biggest meeting yet of 2,000 people on 24 February 1920 in the \"Staatliches Hofbr\u00e4uhaus in M\u00fcnchen\" (a beer hall in Munich). Further in an attempt to make the party more broadly appealing to larger segments of the population, the DAP was renamed the National Socialist German Workers' Party (NSDAP) on 24 February. Such was the significance of Hitler's particular move in publicity that Harrer resigned from the party in disagreement. The new name was borrowed from a different Austrian party active at the time (the Deutsche Nationalsozialistische Arbeiterpartei; the German National Socialist Workers' Party), although Hitler earlier suggested the party to be renamed the Social Revolutionary Party in order to distance the party from association with socialism. It was Rudolf Jung who persuaded Hitler to adopt the NSDAP name. The name was intended to draw upon both left-wing and right-wing ideals, with \"Socialist\" and \"Workers'\" appealing to the left, and \"National\" and \"German\" appealing to the right.\nAlthough the \"German Folkish Party\" (Deutschv\u00f6lkische Partei), an ideological precursor of the Nazi party, started using the swastika for its party publication in January 1917, only after renaming the DAP was the swastika adopted as the symbol of the Nazi party in August 1920.\nMembership.\nEarly members of the party included:\nReferences.\nInformational notes\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "51486", "revid": "29615425", "url": "https://en.wikipedia.org/wiki?curid=51486", "title": "German Workers Party", "text": ""}
{"id": "51487", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=51487", "title": "Salvador Allende", "text": "President of Chile from 1970 to 1973\nSalvador Guillermo Allende Gossens (26 June 1908 \u2013 11 September 1973) was a Chilean socialist politician who served as the 28th president of Chile from 1970 until his death in 1973. As a socialist committed to democracy, he has been described as the first Marxist to be elected president in a liberal democracy in Latin America.\nAllende's involvement in Chilean politics spanned a period of nearly forty years, during which he held various positions including senator, deputy, and cabinet minister. As a life-long committed member of the Socialist Party of Chile, whose foundation he had actively contributed to, he unsuccessfully ran for the national presidency in the 1952, 1958, and 1964 elections. In 1970, he won the presidency as the candidate of the Popular Unity coalition in a close three-way race. He was elected in a run-off by Congress, as no candidate had gained a majority. In office, Allende pursued a policy he called \"The Chilean Path to Socialism\". The coalition government was far from unanimous. Allende said that he was committed to democracy and represented the more moderate faction of the Socialist Party, while the radical wing sought a more radical course. Instead, the Communist Party of Chile favored a gradual and cautious approach that sought cooperation with Christian democrats, which proved influential for the Italian Communist Party and the Historic Compromise.\nAs president, Allende sought to nationalize major industries, expand education, and improve the living standards of the working class. He clashed with the right-wing parties that controlled Congress and with the judiciary. On 11 September 1973, the military moved to oust Allende's democratically elected government in a coup d'\u00e9tat supported by the CIA. Declassified documents showed that US president Richard Nixon and his national security advisor Henry Kissinger were aware of the military's plans to overthrow Allende in the days before the coup d'\u00e9tat. As troops surrounded La Moneda Palace, Allende gave his last speech vowing not to resign. Later that day, Allende died by suicide in his office; the exact circumstances of his death are still disputed.\nFollowing Allende's death, General Augusto Pinochet refused to return authority to a civilian government, and Chile was later ruled by the Government Junta, ending more than four decades of uninterrupted democratic governance, a period known as the Presidential Republic. The military junta that took over dissolved Congress, suspended the Constitution of 1925, and initiated a program of persecuting alleged dissidents, in which at least 3,095 civilians disappeared or were killed. Pinochet's military dictatorship only ended after the successful internationally backed 1989 constitutional referendum led to the peaceful Chilean transition to democracy.\nEarly life.\nAllende was born on 26 June 1908 in Santiago. He was the son of Salvador Allende Castro and Laura Gossens Uribe. Allende's family belonged to the Chilean upper middle class and had a long tradition of political involvement in progressive and liberal causes. His grandfather was a prominent physician and a social reformist who founded one of the first secular schools in Chile. Salvador Allende was of Basque and Belgian descent. In 1909, he moved with his family to the city of Tacna (then under Chilean administration) until he returned to his country to live in Iquique in 1916. In 1918, he studied at the National Institute of Santiago, and from 1919 to 1921, he studied at the Liceo de Valdivia. In 1922, he entered the Eduardo de la Barra school at the age of 16, studying there until 1924.\nAs a teenager, his main intellectual and political influence came from the shoe-maker Juan De Marchi, an Italian-born anarchist. In 1925, he attended the military service in the Cuirassier Regiment of Tacna. Allende was a talented athlete in his youth, being a member of the Everton de Vi\u00f1a del Mar sports club (named after the more famous English football club of the same name). In 1926, at the age of 18, he studied medicine at the University of Chile in Santiago and was elected President of the Student Center in 1927. In 1928, he entered the Grand Lodge of Chile and was elected vice president of the Federation of Students of the University of Chile in 1929. In 1930, he became the representative of the students of the School of Medicine.\nDuring his time at medical school, Allende was influenced by Professor Max Westenhofer, a German pathologist who emphasized the social determinants of disease and social medicine. In 1931, he was expelled from the university and relegated to the north. That same year, he retook his sixth year of medical school and graduated at age 23. In 1932, he began practicing as a physician and anatomo-pathologist in the morgue of the Van Buren Hospital. He became the union leader of the Valpara\u00edso doctors, becoming 1st Regional Secretary in Valpara\u00edso. In 1935, at age 27, he was relegated to the city of Caldera for the second time and, in 1936, he founded the Popular Front in Valpara\u00edso. In 1937, he was elected Deputy of Valpara\u00edso and Aconcagua and, in 1938, he served as Undersecretary General of the Socialist Party of Chile.\nIn 1933, Allende co-founded with Marmaduque Grove and others a section of the Socialist Party of Chile in Valpara\u00edso and became its chairman. He married Hortensia Bussi with whom he had three daughters. He was a Freemason, a member of the Lodge Progreso No. 4 in Valpara\u00edso. In 1933, he published his doctoral thesis \"Higiene Mental y Delincuencia\" (Crime and Mental Hygiene) in which he criticized Cesare Lombroso's proposals.\nPolitical involvement up to 1970.\nIn 1938, Allende was in charge of the electoral campaign of the Popular Front headed by Pedro Aguirre Cerda. The Popular Front's slogan was \"Bread, a Roof and Work!\" After its electoral victory, he became Minister of Health in the Reformist Popular Front government which was dominated by the Radicals. While serving in that position, Allende was responsible for the passage of a wide range of progressive social reforms, including safety laws protecting workers in the factories, higher pensions for widows, maternity care, and free lunch programs for schoolchildren.\nUpon entering the government, Allende relinquished his congressional seat for Valpara\u00edso, which he had won in 1937. Around that time, he wrote \"La Realidad M\u00e9dico Social de Chile\" (\"The Social and Medical Reality of Chile\"). After Kristallnacht in Nazi Germany, Allende was one of 76 members of the Congress who sent a telegram to Adolf Hitler denouncing the persecution of Jews. Following President Aguirre Cerda's death in 1941, he was again elected deputy while the Popular Front was renamed Democratic Alliance.\nIn 1945, Allende became senator for the Valdivia, Llanquihue, Chilo\u00e9, Ais\u00e9n, and Magallanes provinces; then for Tarapac\u00e1 and Antofagasta in 1953; for Aconcagua and Valpara\u00edso in 1961; and once more for Chilo\u00e9, Ais\u00e9n, and Magallanes in 1969. He became president of the Chilean Senate in 1966. During the 1950s, Allende introduced legislation that established the Chilean national health service, the first program in the Americas to guarantee universal health care.\nHis three unsuccessful bids for the presidency (in the 1952, 1958, and 1964 elections) prompted Allende to joke that his epitaph would be \"Here lies the next president of Chile.\" In 1952, as candidate for the \"Frente de Acci\u00f3n Popular\" (Popular Action Front, FRAP), he obtained only 5.4% of the votes, partly due to a division within socialist ranks over support for Carlos Ib\u00e1\u00f1ez. In 1958, again as the FRAP candidate, Allende obtained 28.5% of the vote. This time, his defeat was attributed to votes lost to the populist Antonio Zamorano. This explanation has been questioned by modern research that suggest Zamorano's votes came from across the political spectrum.\nElectoral system.\nDeclassified documents show that from 1962 through 1964, the CIA spent a total of $2.6\u00a0million to finance the campaign of Eduardo Frei and $3\u00a0million in anti-Allende propaganda \"to scare voters away from Allende's FRAP coalition\". The CIA considered its role in the victory of Frei a great success.\nThey argued that \"the financial and organizational assistance given to Frei, the effort to keep Dur\u00e1n in the race, the propaganda campaign to denigrate Allende\u00a0\u2013 were 'indispensable ingredients of Frei's success'\", and they thought that his chances of winning and the good progress of his campaign would have been doubtful without the covert support of the government of the United States. Thus, in 1964 Allende lost once more as the FRAP candidate, polling 38.6% of the votes against 55.6% for Christian Democrat Eduardo Frei. As it became clear that the election would be a race between Allende and Frei, the political right\u00a0\u2013 which initially had backed Radical Julio Dur\u00e1n\u2013 settled for Frei as \"the lesser evil\".\n1970 election.\nAllende was considered part of the moderate wing of the socialists, with support from the communists who favored taking power via parliamentary democracy; in contrast, the left-wing of the socialists (led by Carlos Altamirano) and several other far-left parties called for violent insurrection. Some argue, however, that this was reversed at the end of his period in office.\nAllende won the 1970 Chilean presidential election as leader of the Unidad Popular (\"Popular Unity\") coalition. On 4 September 1970, he obtained a narrow plurality of 36.6% to 35.3% over Jorge Alessandri, a former president, with 27.8% going to a third candidate, Radomiro Tomic of the Christian Democratic Party (PDC). According to the Chilean constitution of the time, if no presidential candidate obtained a majority of the popular vote, Congress would choose one of the two candidates with the highest number of votes as the winner. Tradition was for Congress to vote for the candidate with the highest popular vote, regardless of margin. Former president Jorge Alessandri had been elected in 1958 with a plurality of 31.6% over Allende's 28.85%.\nOne month after the election, on 20 October, while the Senate had still to reach a decision and negotiations were actively in place between the Christian Democrats and the Popular Unity, General Ren\u00e9 Schneider, Commander in Chief of the Chilean Army, was shot resisting a kidnap attempt by a group led by General Roberto Viaux. Hospitalized, he died of his wounds three days later on 23 October. Schneider was a defender of the \"constitutionalist\" doctrine that the army's role is exclusively professional, its mission being to protect the country's sovereignty and not to interfere in politics.\nGeneral Schneider's death was widely disapproved of and, for the time, ended military opposition to Allende, whom the Congress finally chose on 24 October. On 26 October, President Eduardo Frei named General Carlos Prats as commander in chief of the army to replace Ren\u00e9 Schneider. Allende assumed the presidency on 3 November 1970 after signing a \"Statute of Constitutional Guarantees\" proposed by the Christian Democrats in return for their support in Congress. In an extensive interview with R\u00e9gis Debray in 1972, Allende explained his reasons for agreeing to the guarantees. Some critics have interpreted Allende's responses as an admission that signing the \"Statute\" was only a tactical move.\nPresidency.\n\"The Chilean Path to Socialism\".\nIn his speech to the Chilean legislature following his election, Allende made clear his intention to move Chile from a capitalist to a socialist society:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We are moving towards socialism, not from an academic love for a doctrinaire system, but encouraged by the strength of our people, who know that it is an inescapable demand if we are to overcome backwardness and who feel that a socialist regime is the only way available to modern nations who want to build rationally in freedom, independence and dignity. We are moving towards socialism because the people, through their vote, have freely rejected capitalism as a system which has resulted in a crudely unequal society, a society deformed by social injustice and degraded by the deterioration of the very foundations of human solidarity.\nUpon assuming the presidency, Allende began to carry out his platform of implementing a socialist program called \"La v\u00eda chilena al socialismo\" (\"the Chilean path to socialism\"). That included nationalization of large-scale industries (notably copper mining and banking), government administration of the healthcare system and of the educational system (with the help of a United States educator, Jane A. Hobson-Gonzalez from Kokomo, Indiana), a free-milk program for schoolchildren and in shanty towns of Chile, and an expansion of the land seizure\nand redistribution already begun under his predecessor President Eduardo Frei Montalva, who had nationalized between one-fifth and one-quarter of all the properties liable for takeover.\nAllende also intended to improve the socio-economic welfare of Chile's poorest citizens; a key element was to provide employment, either in the new nationalized enterprises or on public-work projects.\nAgrarian and literacy reforms.\nThe Allende government worked to transform Chilean popular culture through formal changes to school curriculum and through broader cultural education initiatives, such as state-sponsored music festivals and tours of Chilean folklorists and nueva canci\u00f3n musicians.\nEconomic policy.\nChilean presidents were allowed a maximum term of six years, which may explain Allende's haste to restructure the economy. Not only was a major restructuring program organized (the Vuskovic plan), he also had to make it a success if a left-wing successor to Allende was going to be elected. In the first year of Allende's term, the short-term economic results of the economy minister Pedro Vuskovic's expansive monetary policy were highly favorable: 12% industrial growth and an 8.6% increase in GDP, accompanied by major declines in inflation (down from 34.9% to 22.1%) and unemployment (down to 3.8%). By 1972, the Chilean escudo had an inflation rate of 140%. The average real GDP contracted between 1971 and 1973 at an annual rate of a 5.6% negative growth, and the government's fiscal deficit soared while foreign reserves declined. Unemployment rates had dropped from 6.3% in 1970 to 3.5% in 1972 before dropping again in 1973 to the lowest ever recorded.\nThe combination of inflation and price controls, together with the disappearance of basic commodities from supermarket shelves, led to the rise of black markets in rice, beans, sugar, and flour. The Chilean economic situation was also somewhat exacerbated due to a US-backed campaign to fund worker strikes in certain sectors of the economy. The Allende government announced it would default on debts owed to international creditors and foreign governments. Allende also froze all prices while raising salaries. His implementation of the policies was strongly opposed by landowners, employers, businessmen and transporters associations, and some civil servants and professional unions. The rightist opposition was led by the National Party, the Catholic Church (which in 1973 was displeased with the direction of educational policy), and eventually the Christian Democrats. There were growing tensions with foreign multinational corporations and the government of the United States.\nAllende undertook the pioneering Project Cybersyn, a distributed decision support system for decentralized economic planning, developed by British cybernetics expert Stafford Beer. Based on the experimental viable system model and the neural network approach to organizational design, the Project consisted of four modules: a network of telex machines (\"Cybernet\") in all state-run enterprises that would transmit and receive information with the government in Santiago. Information from the field would be fed into statistical modeling software (\"Cyberstride\") that would monitor production indicators, such as raw material supplies or high rates of worker absenteeism, in \"almost\" real time, alerting the workers in the first case and, in abnormal situations, if those parameters fell outside acceptable ranges by a very large degree, also the central government. The information would also be input into an economic simulation software (\"CHECO\", for \"CHilean ECOnomic simulator\") which featured a Bayesian filtering and control setting that the government could use to forecast the possible outcome of economic decisions. Finally, a sophisticated operations room (\"Opsroom\") would provide a space where managers could see relevant economic data, formulate feasible responses to emergencies, and transmit advice and directives to enterprises and factories in alarm situations by using the telex network. In conjunction with the system, the Cybersyn development team also planned the Cyberfolk device system, a closed television circuit connected to an interactive apparatus that would enable the citizenry to actively participate in economic and political decision-making.\nAllende raised wages on a number of occasions throughout 1970 and 1971, but the wage hikes were negated by ongoing inflation of Chile's fiat currency. Although price rises had been high even under Frei (27% a year between 1967 and 1970), a basic basket of consumer goods rose by 120% from 190 to 421 escudos in one month alone, August 1972. From 1970 to 1972, while Allende was in government, exports fell 24% and imports rose 26%, with imports of food rising an estimated 149%. Export income fell due to a hard-hit copper industry; the price of copper on international markets fell by almost a third, and post-nationalization copper production fell as well. Copper is Chile's single most important export, as more than half of Chile's export receipts were from that sole commodity. The price of copper fell from a peak of $66 per ton in 1970 to only $48\u201349 in 1971 and 1972. Chile was already dependent on food imports, and the decline in export earnings coincided with declines in domestic food production following Allende's agrarian reforms.\nThe rate of inflation fell from 36.1% in 1970 to 22.1% in 1971, while average real wages rose by 22.3% during 1971. Additionally, Allende government had reduced inflation to 14% in the first nine months of 1971.\nForeign policy.\nIn 1971, Chile re-established diplomatic relations with Cuba, joining Mexico and Canada in rejecting a previously established Organization of American States convention prohibiting governments in the Western Hemisphere from establishing diplomatic relations with Cuba. Shortly afterward, Cuban president Fidel Castro made a month-long visit to Chile. Originally, the visit was supposed to be one week; however, Castro enjoyed Chile and one week led to another. Despite his attitude of socialist solidarity, Castro was reportedly critical of Allende's policies. Castro was quoted as saying that \"Marxism is a revolution of production\", whereas \"Allende's was a revolution of consumption.\"\nSocioeconomic and political tensions.\nIn October 1972, the first of what were to be a wave of strikes was led first by truckers, and later by small businessmen, some (mostly professional) unions and some student groups. Other than the inevitable damage to the economy, the chief effect of the 24-day strike was to induce Allende to bring the head of the army, general Carlos Prats, into the government as Interior Minister. Allende also instructed the government to commandeer trucks to keep the nation from coming to a halt. Government supporters also helped to mobilize trucks and buses, but violence served as a deterrent to full mobilization, even with police protection for the strike-breakers. Allende's actions were eventually declared unlawful by the Chilean appeals court and the government was ordered to return trucks to their owners. Throughout his presidency, racial tensions between the poor descendants of indigenous people, who supported Allende's reforms, and the white elite increased.\nThroughout his presidency, Allende remained at odds with the Chilean Congress, which was dominated by the Christian Democratic Party. In 1964, Eduardo Frei had promised a \"Revolution in Liberty\", a middle-class revolution that was funded by the United States government's Alliance for Progress. Frei carried out a series of progressive reforms, including land reform, an issue that had not been touched since Chile's independence in the early 19th century. According to historian Marian Schlotterbeck, this was \"[John F.] Kennedy's vision \u2013?stave off the threat of communist revolution by improving standards of living across the continent\". The Christian Democrats had campaigned on a socialist platform in the 1970 elections but drifted away from those positions during Allende's presidency, and accused Allende of leading Chile toward a Cuban-style dictatorship and sought to overturn many of his more radical policies. They eventually formed a coalition with the National Party.\nAllende and his opponents in Congress repeatedly accused each other of undermining the Chilean Constitution and acting undemocratically. Allende's increasingly bold socialist policies (partly in response to pressure from some of the more radical members within his coalition), combined with his close contacts with Cuba, heightened fears in Washington. The Nixon administration continued exerting economic pressure on Chile via multilateral organizations and continued to back Allende's opponents in the Chilean Congress. Almost immediately after his election, Nixon directed CIA and US State Department officials to \"put pressure\" on the Allende government. His economic policies were used by economists Rudi Dornbusch and Sebasti\u00e1n Edwards to coin the term macroeconomic populism. In 1972, Chile's inflation stood at 150%.\nForeign relations during Allende's presidency.\nSalvador Allende took office in a difficult international context. Chile was aligned with the United States in 1970. Elsewhere in Latin America, Brazil, Argentina and Bolivia were ruled by conservative military dictatorships (soon to be joined by Uruguay). Colombia had a liberal government and Venezuela, a democratic christian one, both elected by the people. Many in Cuba, Peru and Mexico viewed the Chilean socialist experiment with sympathy. Under Allende's presidency, Chile joined the Non-Aligned Movement.\nChile, which until then had been fussy about ideological boundaries, diversified its diplomatic and trade relations, regardless of the internal political regime of each country. The government established diplomatic relations with Cuba, Guyana, seven African countries (Congo, Equatorial Guinea, Libya, Madagascar, Nigeria, Tanzania, and Zaire), three European countries (Albania, East Germany and Hungary) and seven Asian countries (Afghanistan, Bangladesh, Cambodia, North Korea, China, Mongolia, South Vietnam and North Vietnam).\nIt tried to promote Latin American integration. At the 1971 Latin American Economic and Social Council, the Chilean representative Gonzalo Martner Garc\u00eda formulated four major proposals, summarized by the historian Jorge Magasich: \"1) to ask the United States for a moratorium on external debt for a decade in order to allocate these sums to development policies; 2) to create a Latin American central bank to \"invest Latin America's reserves, 70% of which are in the United States\", to receive \"the region's deposits and assets\" and to coordinate the operations of the central banks in order to protect the region from financial turbulence; 3) Promote the creation of a global technology fund for development, fed by compulsory contributions of licenses, industrial processes and other funds for research, so as to limit the abuses associated with technological property; 4) Create a Latin American organisation for the development of science and technology appropriate to the region.\"\nHe began negotiations with Bolivia over the historical dispute between the two countries (Bolivia had lost access to the sea since the War of the Pacific between 1879 and 1884) and welcomed Bolivia's maritime request. Nevertheless, relations became tense again following a coup d'\u00e9tat by Bolivian General Hugo Banzer in August 1971. At the same time, Chile granted asylum to thousands of political exiles from Latin American countries.\nSalvador Allende openly rejected the influence of the Organization of American States (OAS), a body close to the United States government, and the General Agreement on Tariffs and Trade (GATT), which favored the interests of more developed countries. On the other hand, he was a fervent defender of the United Nations Conference on Trade and Development (UNCTAD), which he considered to be more representative since it allowed economic and trade issues to be negotiated on an equal legal footing. In a speech to UNCTAD, he also warned of the policy of the United States, Japan and the European Economic Community to progressively eliminate obstacles to free trade. He said that \"freeing up trade ... erases at a stroke the benefits that the Generalised System of Preferences brings to developing countries\".\nAllende's Popular Unity government tried to maintain normal relations with the United States. When Chile nationalized its copper industry, the United States government cut off support and increased its support to the opposition. Forced to seek alternative sources of trade and finance, Chile gained commitments from the Soviet Union to invest some $400\u00a0million in Chile in the next six years. The United States Department of State put it at $115 million from Eastern Europe and $65 million from China, while Soviet and Chilean Popular Unity sources put it at total of $620 million from socialist countries. Much of the credit was never utilized, and the Soviets were not willing to subsidize Chile the same way they did for Cuba.\nAllende's government was disappointed that it received far less economic assistance from the Soviets than it hoped for. Trade between the two countries did not significantly increase and the credits were mainly linked to the purchase of Soviet equipment. Moreover, credits from the Soviet Union were much less than those provided to the People's Republic of China and countries of the Eastern Bloc. When Allende visited the Soviet Union in late 1972 in search of more aid and additional lines of credit after three years, he was turned down.\nUnited States involvement.\nThe United States opposition to Allende started several years before he was elected President of Chile. Declassified documents show that from 1962 to 1964, the CIA spent $3\u00a0million on anti-Allende propaganda \"to scare voters away from Allende's FRAP coalition\" and spent a total of $2.6\u00a0million to finance the presidential campaign of Eduardo Frei.\nThe possibility of Allende winning Chile's 1970 election was deemed a disaster by the Nixon administration that wanted to protect American geopolitical interests by preventing the spread of Communism during the Cold War. In September 1970, then United States president Richard Nixon informed the CIA that an Allende government in Chile would not be acceptable and authorized $10\u00a0million to stop Allende from coming to power or unseat him. A CIA document declared, \"It is firm and continuing policy that Allende be overthrown by a coup.\" Henry Kissinger's 40 Committee and the CIA planned to impede Allende's investiture as President of Chile with covert efforts known as \"Track I\" and \"Track II\"; Track I sought to prevent Allende from assuming power via so-called \"parliamentary trickery\", while under the Track II initiative, the CIA tried to convince key Chilean military officers to carry out a coup.\nSome point to the involvement of the Defense Intelligence Agency agents that allegedly secured the missiles used to bombard La Moneda Palace. In fact, open US military aid to Chile continued during the Allende administration, and the national government was very much aware of that although there is no record that Allende himself believed that such assistance was anything but beneficial to Chile. During Richard Nixon's presidency, United States officials attempted to prevent Allende's election by financing political parties aligned with opposition candidate Jorge Alessandri and supporting strikes in the mining and transportation sectors. After the 1970 election, the Track I operation attempted to incite Chile's outgoing president, Eduardo Frei Montalva, to persuade his party (PDC) to vote in Congress for Alessandri.\nUnder the plan, Alessandri would resign his office immediately after assuming it and call new elections. Eduardo Frei would then be constitutionally able to run again (since the Chilean Constitution did not allow a president to hold two consecutive terms, but allowed multiple non-consecutive ones), and presumably easily defeat Allende. The Chilean Congress instead chose Allende as president, on the condition that he would sign a \"Statute of Constitutional Guarantees\" affirming that he would respect and obey the Chilean Constitution and that his reforms would not undermine any of its elements. Track II was aborted, as parallel initiatives already underway within the Chilean military rendered it moot. During the second term of office of Democratic President Bill Clinton, the CIA acknowledged having played a role in Chilean politics before the coup, but its degree of involvement is debated. The CIA was notified by its Chilean contacts of the impending coup two days in advance but contends it \"played no direct role in\" the coup.\nMuch of the internal opposition to Allende's policies came from the business sector, and recently released United States government documents confirm that the United States indirectly funded the truck drivers' strike, which exacerbated the already chaotic economic situation before the coup. The most prominent United States corporations in Chile before Allende's presidency were the Anaconda and Kennecott copper companies and ITT Corporation, International Telephone and Telegraph. Both copper corporations aimed to expand privatized copper production in the city of Sewell in the Chilean Andes, where the world's largest underground copper mine \"El Teniente\", was located.\nAt the end of 1968, according to United States Department of Commerce data, United States corporate holdings in Chile amounted to $964\u00a0million. Anaconda and Kennecott accounted for 28% of United States holdings, but ITT had by far the largest holding of any single corporation, with an investment of $200\u00a0million in Chile. In 1970, before Allende was elected, ITT owned 70% of Chitelco, the Chilean Telephone Company and funded El Mercurio, a Chilean right-wing newspaper. Documents released in 2000 by the CIA confirmed that before the elections of 1970, ITT gave $700,000 to Allende's conservative opponent, Jorge Alessandri, with help from the CIA on how to channel the money safely. ITT president Harold Geneen also offered $1\u00a0million to the CIA to help defeat Allende in the elections.\nAfter General Augusto Pinochet assumed power, United States Secretary of State Henry Kissinger told President Nixon that the United States \"didn't do it\" (referring to the coup) but \"we helped them... created the conditions as great as possible\". Recent documents declassified under the Clinton administration's Chile Declassification Project show that the United States government and the CIA sought to overthrow Allende in 1970 immediately before he took office (\"Project FUBELT\"). Many documents regarding the United States intervention in Chile remain classified. Those that have been declassified showed that Nixon, Kissinger, and the United States government were aware of the coup and the plans to overthrow Allende's democratically elected government.\nRelations with the Soviet Union.\nPolitical and moral support came mostly through the Communist Party and unions of the Soviet Union. For instance, Allende received the Lenin Peace Prize from the Soviet Union in 1972. At the same time, there were some fundamental differences between Allende and Soviet political analysts, who believed that some violence or measures that those analysts \"theoretically considered to be just\", should have been used. Declarations from KGB General Nikolai Leonov, former Deputy Chief of the First Chief Directorate of the KGB, confirmed that the Soviet Union supported Allende's government economically, politically and militarily. Leonov stated in an interview at the Chilean Center of Public Studies (CEP) that the Soviet economic support included over $100\u00a0million in credit, three fishing ships (that distributed 17,000 tons of frozen fish to the population), factories (as help after the 1971 earthquake), 3,100 tractors, 74,000 tons of wheat and more than a million tins of condensed milk. In mid-1973, the Soviets approved the delivery of weapons (artillery and tanks) to the Chilean Army. When news of an attempt from the Army to depose Allende through a coup d'\u00e9tat reached Soviet officials, the shipment was redirected to another country.\nAllende is mentioned in a book written by the official historian of the British Intelligence MI5, Christopher Andrew. According to SIS and Andrew, the book is based on the handwritten notes of KGB archivist defector Vasili Mitrokhin. Andrew alleged that the KGB said that Allende \"was made to understand the necessity of reorganizing Chile's army and intelligence services, and of setting up a relationship between Chile's and the USSR's intelligence services.\" The Soviets observed closely whether the alternative form of socialism could work, and they did not interfere with the Chileans' decisions. Nikolai Leonov affirmed that whenever he tried to give advice to Latin American leaders, he was usually turned down by them, and he was told that they had their own understanding on how to conduct political business in their countries. Leonov added that the relationships of KGB agents with Latin American leaders did not involve intelligence because their intelligence target was the United States. Since many North Americans were living in the region, the Soviets were focusing in recruiting agents from the United States. Latin America was also a better region for KGB agents to get in touch with their informants from the CIA or other contacts from the United States than inside that country.\nCrisis.\nOn 29 June 1973, Colonel Roberto Souper surrounded the presidential palace, La Moneda, with his tank regiment but failed to depose the government. That failed coup d'\u00e9tat \u2013 known as the \"Tanquetazo\" (\"tank putsch\") \u2013 organised by the nationalist \"Patria y Libertad\" paramilitary group, was followed by a general strike at the end of July that included the copper miners of El Teniente.\nIn August 1973, a constitutional crisis occurred, and the Supreme Court of Chile publicly complained about the inability of the Allende government to enforce the law of the land. On 22 August, the Chamber of Deputies (with the Christian Democrats uniting with the National Party) accused the government of unconstitutional acts through Allende's refusal to promulgate constitutional amendments, already approved by the Chamber, which would have prevented his government from continuing his massive nationalization plan and called upon the military to enforce constitutional order.\nFor months, Allende had feared calling upon the \"Carabineros\" (\"Carabineers\", the national police force), suspecting them of disloyalty to his government. On 9 August, President Allende appointed General Carlos Prats as Minister of Defence. On 24 August 1973, General Prats was forced to resign both as defense minister and as the commander-in-chief of the army, embarrassed by both the Alejandrina Cox incident and a public protest in front of his house by the wives of his generals. General Augusto Pinochet replaced him as Army commander-in-chief the same day.\nResolution by the Chamber of Deputies.\nOn 22 August 1973, the Christian Democrats and the National Party members of the Chamber of Deputies joined to vote 81 to 47 in favor of a resolution that made accusation of disregard by the government of the separation of powers and arrogating legislative and judicial prerogatives to the executive branch of government, among other alleged constitutional violations. The resolution asked the authorities to \"put an immediate end\" to \"breach[es of] the Constitution ... with the goal of redirecting government activity toward the path of law and ensuring the Constitutional order of our Nation, and the essential underpinnings of democratic co-existence among Chileans.\" The resolution declared that Allende's government sought \"to conquer absolute power with the obvious purpose of subjecting all citizens to the strictest political and economic control by the state ... [with] the goal of establishing ... a totalitarian system\" and claimed that the government had made \"violations of the Constitution ... a permanent system of conduct\".\nSpecifically, the government of Allende was accused of ruling by decree and thwarting the normal legislative system; refusing to enforce judicial decisions against its partisans; not carrying out sentences and judicial resolutions that contravened its objectives; ignoring the decrees of the independent General Comptroller's Office; sundry media offenses, including usurping control of the National Television Network and applying economic pressure against those media organizations that were not unconditional supporters of the government; allowing its supporters to assemble with arms, and preventing the same by its right-wing opponents; supporting more than 1,500 illegal takeovers of farms; illegal repression of the El Teniente miners' strike; and illegally limiting emigration. Finally, the resolution condemned the creation and development of government-protected socialist armed groups, which were said to be \"headed towards a confrontation with the armed forces\". President Allende's efforts to re-organize the military and the police forces were characterized as \"notorious attempts to use the armed and police forces for partisan ends, destroy their institutional hierarchy, and politically infiltrate their ranks\".\nAllende's response.\nThe resolution was later used by Pinochet as a way to justify the coup, which occurred two weeks later. On 24 August 1973, two days after the resolution, Allende responded. He accused the opposition of trying to incite a military coup by encouraging the armed forces to disobey civilian authorities. He described the Congress's declaration as \"destined to damage the country's prestige abroad and create internal confusion\", and predicted: \"It will facilitate the seditious intention of certain sectors.\" He observed that the declaration (passed 81\u201347 in the Chamber of Deputies) had not obtained the two-thirds Senate majority \"constitutionally required\" to convict the president of abuse of power, thus the Congress was \"invoking the intervention of the armed forces and of Order against a democratically-elected government\" and \"subordinat[ing] political representation of national sovereignty to the armed institutions, which neither can nor ought to assume either political functions or the representation of the popular will.\"\nAllende argued that he had obeyed constitutional means for including military men to the cabinet at the service of civic peace and national security, defending republican institutions against insurrection and terrorism. In contrast, he said that Congress was promoting a coup d\u2019\u00e9tat or a civil war with a declaration full of affirmations that had already been refuted beforehand and which in substance and process (directly handing it to the ministers rather than directly handing it to the president) violated a dozen articles of the then-current constitution. He further argued that the legislature was usurping the government's executive function.\nAllende wrote: \"Chilean democracy is a conquest by all of the people. It is neither the work nor the gift of the exploiting classes, and it will be defended by those who, with sacrifices accumulated over generations, have imposed it ... With a tranquil conscience ... I sustain that never before has Chile had a more democratic government than that over which I have the honor to preside ... I solemnly reiterate my decision to develop democracy and a state of law to their ultimate consequences...Congress has made itself a bastion against the transformations ... and has done everything it can to perturb the functioning of the finances and of the institutions, sterilizing all creative initiatives.\" Adding that economic and political means would be needed to relieve the country's current crisis, and that the Congress was obstructing said means; having already paralyzed the state, they sought to destroy it. He concluded by calling upon the workers and all democrats and patriots to join him in defending the Chilean constitution and the revolutionary process.\nCoup.\nIn early September 1973, Allende floated the idea of resolving the constitutional crisis with a plebiscite. His speech outlining such a solution was scheduled for Tuesday, 11 September but he was never able to deliver it. On that same day, the Chilean military under Pinochet, aided by the United States and its CIA, staged a coup against Allende, who was at the head of the first democratically elected Marxist government in Latin America. Historian Peter Winn described the 1973 coup as one of the most violent events in Chilean history. It led to a series of human rights abuses in Chile under Pinochet, who initiated a brutal and long-lasting campaign of political suppression through torture, murder, and exile, which significantly weakened leftist opposition to the military dictatorship of Chile, which ruled the country until 1990. Due to the coup's occurrence on the same date as the 11 September attacks in the United States, it has sometimes been referred to as \"the other 9/11\".\nDeath.\nJust before the capture of La Moneda (the Presidential Palace), with gunfire and explosions clearly audible in the background, Allende gave his farewell speech to Chileans on live radio, speaking of himself in the past tense, of his love for Chile and of his deep faith in its future. He stated that his commitment to Chile did not allow him to take an easy way out, and he would not be used as a propaganda tool by those he called \"traitors\" (he refused an offer of safe passage). Juan Seoane, Chief of President Allende's Bodyguard at the time of the events \u2013 and who was with Allende until moments before his death \u2013 declared in an interview reported by the University of Chile: \"Allende began to say goodbye to us one by one, he gave us a hug and told us \u2018\"Thank you for everything, comrade, thank you for everything\"'\u201d, and then he said that he was going to leave last. He walked to the end of the line with his AK, turned around behind a wall, and then he shouted, \"'Allende doesn't surrender\"\u2026!'. The shot was heard as fifteen meters from where we were\". (Reports in El Tiempo and other Latin-American media confirmed Allende\u00b4s last words).Shortly afterwards, the coup plotters announced that Allende had committed suicide. An official announcement declared that the weapon he had used was an automatic rifle. Before his death he had been photographed several times holding an AK-47, a gift from Fidel Castro. He was found dead with that gun, according to contemporaneous statements made by officials in the Pinochet regime. In an interview with David Frost, the daughter of Allende's first cousin, Isabel Allende, said that, at a family lunch nine days before his death, Allende had said that he would either stay till the end of this term of presidency or he would be taken out feet first. Lingering doubts regarding the manner of Allende's death persisted throughout the period of the Pinochet regime. Many Chileans and independent observers refused to accept on faith the government's version of events amid speculation that Allende had been murdered by government agents. Pinochet had long left power and died when in 2011 a Chilean court opened a criminal investigation into the circumstances of Allende's death.\nThe ongoing criminal investigation led to a May 2011 court order that Allende's remains be exhumed and autopsied by an international team of experts. Results of the autopsy were officially released in mid-July 2011. The team of experts concluded that the former president had shot himself with an AK-47 assault rifle. In December 2011 the judge in charge of the investigation affirmed the experts' findings and ruled Allende's death a suicide. On 11 September 2012, the 39th anniversary of Allende's death, a Chilean appeals court unanimously upheld the trial court's ruling, officially closing the case. \"The Guardian\" reported that a scientific autopsy of the remains had confirmed that \"Salvador Allende committed suicide during the 1973 coup that toppled his socialist government.\" It went on to say:&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;British ballistics expert David Prayer said Allende died of two shots fired from an assault rifle that was held between his legs and under his chin and was set to fire automatically. The bullets blew out the top of his head and killed him instantly. The forensics team's conclusion was unanimous. Spanish expert Francisco Etxeberria said: \"We have absolutely no doubt\" that Allende committed suicide.\nIsabel Allende Bussi, the daughter of Allende and a member of the Senate of Chile told the BBC that: \"The report conclusions are consistent with what we already believed. When faced with extreme circumstances, he made the decision of taking his own life, instead of being humiliated.\" The definitive and unanimous results produced by the 2011 Chilean judicial investigation appear to have laid to rest decades of nagging suspicions that Allende might have been assassinated by the Chilean Armed Forces. Public acceptance of the suicide theory had already been growing for much of the previous decade. In a post-junta Chile where restrictions on free speech were steadily eroding, independent and seemingly reliable witnesses began to tell their stories to the news media and to human rights researchers. The cumulative weight of the facts reported by those witnesses provided enough support for many previously unconfirmed details relating to Allende's death.\nFamily.\nWell-known relatives of Salvador Allende include his daughter Isabel Allende Bussi (a politician) and his cousin Isabel Allende Llona (a writer).\nMemorials.\nOn the 30th anniversary of his death, an Allende Museum opened in Chile, and an Allende foundation has since managed his estate.\nSouth America.\nMemorials to Allende include a statue in front of the Palacio de la Moneda. The placement of the statue was controversial; it was placed facing the eastern edge of the Plaza de la Ciudadan\u00eda, a plaza which contains memorials to a number of Chilean statesmen. However, the statue is not located in the plaza, but rather on a surrounding sidewalk facing an entrance to the plaza. His tomb is a major tourist attraction. Allende is buried in the general cemetery of Santiago.\nThere is a square in S\u00e3o Paulo, Brazil, named after Allende. Also in Brazil, Rio De Janeiro has a BRT station named after him. In Nicaragua, the tourist port of Managua is named after him. The Salvador Allende Port is located near downtown Managua. The broken glasses of Allende were given to the Chilean National History Museum in 1996 by a woman who had found them in La Moneda in 1973.\nEurope.\nIn 1984, a memorial stone dedicated to him was erected in the Gajnice neighbourhood of Zagreb. There is a bronze bust of him accompanied by a memorial stone in the Donaupark in Vienna. In Istanbul, a statue of Allende can be found side by side with Mustafa Kemal Atat\u00fcrk in Ata\u015fehir.\nEuropean landmarks named after Allende include one of the major streets in the Karaburma neighborhood of Belgrade, an avenue linking the parishes of Caxias and Pa\u00e7o de Arcos in Oeiras, Portugal, a park in La Spezia, Italy, a bridge in Terni, Italy, and a street in Sokol District, Moscow, which was named after Allende soon after his death. A memorial plaque is also installed there. Further tributes include Salvador Allende Square in the 7th arrondissement of Paris, near the Chilean embassy, the Plaza de Salvador Allende square in Viladecans, near Barcelona, and the Salvador-Allende-Stra\u00dfe avenue and a nearby bridge in Berlin, Streets in several other German cities, especially in former East Germany but also in the West, are named after Allende, as is a street in Szeksz\u00e1rd, Hungary, and Allende Park in Budapest. Allende Avenue (5th Avenue) in Harlow, Essex, was renamed to Zelenskyy Avenue in May 2023 in recognition of the president of Ukraine, Volodymyr Zelenskyy, during the Russian invasion of Ukraine.\nNorth America.\nIn 2009, the Salvador Allende Monument, Montreal, was installed in Parc Jean-Drapeau. A residential street in Toronto has also been named after him.\nAfrica.\nThere is a street named after Allende in the capital city of Maputo, Mozambique.\nAsia.\nThe Malaysian rock band Martin Vengadesan &amp; The Stalemate Factor paid tribute with a folk song called \"The Final Hours Of Salvador Allende\" which was released in 2018.\nPostage stamps.\nOne of the first postage stamps to commemorate Salvador Allende, released after the 1973 coup in Chile, was the Soviet one. It was issued just two months after the event and had a circulation of 3.8 million. The issue was designed by the painter A. Kovrizhkin and bore the title \"Salvador Allende, President of the Republic of Chile, Laureate of the Lenin Peace Prize, 1908 \u2013 11.IX.1973\".\nA stamp was released by Magyar Posta in Hungary in 1974 shortly after the 11 September, 1973 coup in Chile that ended the socialist government of Salvador Allende. The stamp bore an image of Allende that had become popular during his election campaign in 1970.\nTwo other stamps, both on the tenth anniversary of the coup, represent extreme reactions to the event. Scott Cuba #2605 shows the burning presidential palace of La Moneda (\"The Mint\") and a picture of Allende. The caption refers to him as having \"fallen in combat\".\nIn contrast, Chile, still under the military dictatorship of General Augusto Pinochet at that time, issued Scott Chile #656, labeled \"Ten Years of Liberty,\" celebrating the decade since the fall of Allende and the rise of the \"junta\".\nPublic perception.\nAllende is seen as a significant historical figure in Chile. The former social-democratic president Ricardo Lagos honored Allende as a humanist and a statesman.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nrole=\"presentation\" class=\"wikitable succession-box noprint\" style=\"margin:0.5em auto; font-size:small;clear:both;\""}
{"id": "51488", "revid": "1233913569", "url": "https://en.wikipedia.org/wiki?curid=51488", "title": "Rhone (disambiguation)", "text": "Rhone can refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "51489", "revid": "49712780", "url": "https://en.wikipedia.org/wiki?curid=51489", "title": "Poitiers", "text": "Prefecture and commune in Nouvelle-Aquitaine, France\n \nPoitiers is a university city on the river Clain in west-central France. It is a commune, the capital of the Vienne department, part of the Nouvelle-Aquitaine region of France, and the historical center of Poitou Province. In 2021, it had a population of 90,240. Its conurbation had 134,397 inhabitants in 2021 and is the municipal center of an urban area of 281,789 inhabitants. It is a city of art and history, still known popularly as the \"Ville aux cent clochers\" (\"City of a hundred bell towers\").\nWith more than 30,000 students, Poitiers has been a major university town since the creation of its university in 1431, having hosted world-renowned figures and thinkers such as Ren\u00e9 Descartes, Joachim du Bellay and Fran\u00e7ois Rabelais, among others. The plaza of the town is picturesque; its streets including predominantly preserved historical architecture and half-timbered houses, especially religious edifices, commonly from the Romanesque period. The latter includes notably the 4th century baptistery of Saint-Jean (Baptist\u00e8re Saint-Jean), the 7th century Merovingian underground chapel of the Hypogeum of the Dunes (L'Hypog\u00e9e des Dunes), the Church of Notre-Dame-la-Grande (12th century), the Church of Saint-Porchaire (12th century) or Poitiers Cathedral (end of the 12th century) as well as the Palace of Poitiers, until recently a courthouse (12th century), the former palace of the Counts of Poitou, Dukes of Aquitaine, where the Dowager Queen of France and England Eleanor of Aquitaine held her \"Court of Love\".\nThe city's pedigree is associated with two major battles that took place in the area. The first, in 732, also known as the Battle of Tours (to avoid inevitable confusion with the second), saw the defending Frankish warhost commanded by Charles Martel defeat the belligerent expeditionary army of the Umayyad Caliphate led by Muslim general Abd al-Rahman al-Ghafiqi. The second battle, in 1356, a lionized military capstone was the Battle of Poitiers which was one of the focal battles of the Hundred Years' War. It saw the defeat of a larger French royal army by the English and the capture of King John II of France by the triumphant Prince of Wales Edward.\nThe Poitiers agglomeration, located halfway between Paris and Bordeaux, is home to the Futuroscope Technopole, which includes major public (CNED, Canop\u00e9, etc.) and private companies of national scope, as well as leading European research laboratories. With two million visitors annually, Futuroscope is the leading tourist site in Nouvelle-Aquitaine, and the third most popular amusement park in France after Disneyland Paris and the Puy du Fou.\nGeography.\nLocation.\nThe city of Poitiers is strategically situated on the Seuil du Poitou, a shallow gap between the Armorican and the Central Massif. The Seuil du Poitou connects the Aquitaine Basin to the South to the Paris Basin to the North. This area is an important geographic crossroads in France and Western Europe.\nSituation.\nPoitiers's primary site sits on a vast promontory between the valleys of the Boivre and the Clain. The old town occupies the slopes and the summit of a plateau that rises above the streams which surround, and hence benefits from a very strong tactical situation. This was an especially important factor before and throughout the Middle Ages.\nInhabitants and demography.\nInhabitants of Poitiers are referred to as Poitevins or Poitevines, although this denomination can be used for anyone from the Poitou province.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nClimate.\nThe climate in the Poitiers area is mild with mild temperature amplitudes, and adequate rainfall throughout the year although with a drying tendency during summer. The K\u00f6ppen Climate Classification subtype for this type of climate is \"\" (Marine West Coast Climate/Oceanic climate).\nHistory.\nAntiquity.\nPoitiers was founded by the Celtic tribe of the Pictones and was known as the Oppidum \"Lemonum\" before Roman influence. The name is said to have come from the Celtic word for elm, \"Lemo\". But after, the Pax Romana settled, the town became known as \"Pictavium\", or later \"Pictavis\", after the original Pictones inhabitants themselves.\nThe Pictavis Period has provided the city with a rich wealth of archeological finds from the Roman-era in Poitiers. In fact until 1857, Poitiers hosted the ruins of a vast Roman Amphitheatre, which was larger than the Amphitheater of N\u00eemes. Furthermore, Roman baths, or \"Therm\u00e6\" built in the 1st century and demolished in the 3rd century, were uncovered after the Amphitheater in 1877.\nIn 1879, a burial-place and tombs of a number of Christian martyrs were discovered on the heights to the south-east of the town. The names of some of the Christians had been preserved in paintings and inscriptions. Not far from these tombs is a huge Dolmen (the \"Pierre Lev\u00e9e\"), which is long, wide and high, and around which the great fair of Saint Luke used to be held.\nThe Romans also built at least three aqueducts. This extensive ensemble of Roman constructions suggests Poitiers was a town of primary importance, possibly even the capital of the Roman province of \"Gallia Aquitania\" during the 2nd century.\nAs Christianity (Chalcedonian) was made official and gradually introduced across the Roman Empire during the 3rd and 4th centuries due to Constantine I's influence. The first Bishop of Poitiers from 350 to 367, was Hilary of Poitiers or Saint Hilarius, who proceeded to evangelize the town. Exiled by Constantius II, he risked death to return to Poitiers as Bishop. In tandem, the first foundations of the Baptist\u00e8re Saint-Jean can be traced to that era of open Christian conversion. This man was later named \"Doctor of The Church\" by Pope Pius IX.\nIn the 3rd century, a thick wall 6m wide and 10m high was built around the town. It was long and stood lower on the naturally defended east side and at the top of the promontory. Around this time, the town began to be known as Poitiers.\nFifty years later, Poitiers fell into the hands of the Arian Visigoths, and became one of the principal residences of their royals. Visigoth King Alaric II was defeated by Clovis I at Vouill\u00e9, not far from Poitiers, in 507, and the town thus came under Frankish dominion.\nMiddle Ages.\nDuring most of the Early Middle Ages, the town of Poitiers took advantage of the defensively tactical placement of its location, which was far from the nucleus of Frankish power. As the seat of an \"\u00e9v\u00each\u00e9\" (bishopric) since the 4th century, the town was a cynosure of notable importance and the capital of the county of Poitou. At the crux of their power, the Counts of Poitiers governed a sizeable domain, including both Nouvelle-Aquitaine and Poitou.\nThe town was often referred to as \"Poictiers\", a name commemorated in warships of the Royal Navy, after the Battle of Poitiers.\nThe first decisive victory of a Western European Christian army over a Islamic power, the Battle of Tours, was fought by Charles Martel's men in the vicinity of Poitiers on 10 October 732. For many historians, it was one of history's most pivotal moments as it marked the end of territorial end of Muslim expansion although the influence of the region would bourgeon for hundreds of years to come.\nEleanor of Aquitaine frequently resided in the town, which she embellished and fortified, and in 1199 entrusted with communal rights. In 1152 she married the future King of England Henry II in Poitiers Cathedral.\nDuring the Hundred Years' War, the Battle of Poitiers, an English victory, was fought near the town of Poitiers on 19 September 1356. Later in the war in 1418, under Charles VII, the royal parliament moved from Paris to Poitiers, where it remained in exile until the Plantagenets finally withdrew from the capital in 1436. During this interval, in 1429, Poitiers was the site of Joan of Arc's formal inquest.\nThe University of Poitiers was founded in 1431. During and after the Reformation, John Calvin had numerous converts in Poitiers and the town had its share of the violent proceedings which underlined the Wars of Religion throughout France.\nIn 1569, Poitiers was defended against an assailing siege by Guy de Daillon, Count of Lude, against Admiral of France Gaspard de Coligny, who after an unsuccessful bombardment and seven weeks, retired from a siege he had laid to the town.\n16th century.\nThe type of political organization existing in Poitiers during the late medieval and early modern period can be sheened through a speech given on 14 July 1595 by Maurice Roatin, the town's mayor. He compared it to the Roman state, which combined three types of government: monarchy, aristocracy, and democracy. He said the Roman Consulate corresponded to Poitiers' mayor, the Roman Senate to the town's peers and \"\u00e9chevins\", and the democratic element in Rome corresponded to the fact that most important matters \"can not be decided except by the advice of the \"Mois et Cent\" (the broad council).1 The mayor appears to have been an advocate of a mixed constitution; not all Frenchmen in 1595 would have agreed with him, at least in public; many spoke in favor of absolute monarchy which would be pioneered by the Father of Absolutism, Louis XIV Le Roi Soleil. The democratic element was not as strong as the mayor's words may have seemed to imply: In fact, Poitiers was similar to other French cities such as, Paris, Nantes, Marseille, Limoges, La Rochelle, and Dijon, in that the town's governing body (\"corps de ville\") was \"highly exclusive and oligarchical:\"\" With a small number of professionals and family groups controlling most of the city offices. In Poitiers many of these positions were granted for the lifetime of the office holder, an archaic byproduct of the Age of Absolutism in France.2\nThe city government in Poitiers based its claims to legitimacy on the theory of government where the mayor and \"\u00e9chevins\" held jurisdiction of the fief's administration separate from the monarchy: that is, they swore allegiance and promised support for him, and in return he granted them local authority. This gave them the advantage of being able to claim that any townsperson who challenged their preeminence was being treasonous to the king's decree. Annually the mayor and the 24 \"\u00e9chevins\" would swear an oath of allegiance \"between the hands\" of the king or his representative, usually the lieutenant general or the S\u00e9n\u00e9chauss\u00e9e. For example, in 1567, when Maixent Poitevin was mayor, King Henry III came for a visit, and, although some townspeople were disgruntled regarding the licentious behavior of his entourage, Henry smoothed things over with a warm speech acknowledging their allegiance and graciously thanking them for it.2\nIn this era, the mayor of Poitiers was preceded by sergeants wherever he went, consulted deliberative bodies, carried out their decisions, \"heard civil and criminal suits in first instance\", tried to ensure that the food supply would be adequate, and visited markets.2\nIn the 16th century, Poitiers impressed visitors because of its large size, and important features, including \"royal courts, universities, prolific printing shops, wealthy religious institutions, cathedrals, numerous parishes, markets, impressive domestic architecture, extensive fortifications, and castle.\"316th-century Poitiers is closely associated with the life of Fran\u00e7ois Rabelais and the community of Bitards.\n17th century.\nThe cosmopolitan town saw less activity during the Renaissance. Few changes were made in the urban landscape, except for laying the way for the \"Rue de la Tranch\u00e9e\". Moreover bridges were built where the inhabitants had colloquially used \"gu\u00e9s\". A few \"H\u00f4tels particuliers\" were built at that time, such as the H\u00f4tels Jean Bauc\u00e9, Fum\u00e9 and Berthelot. Poets Joachim du Bellay and Pierre Ronsard converged at the University of Poitiers, before leaving for Paris leaving an indelible mark on the city.\nDuring the 17th century, many people emigrated from Poitiers and the Poitou to the French settlements in the new world and thus many Acadians who would later be Cajuns living in North America contemporarily can trace their ancestry back to this region.\n18th century.\nDuring the 18th century, the town's ebb and flow mainly depended on its administrative functions as the regional capital: Poitiers served as the seat for the regional administration of royal justice, the \u00e9v\u00each\u00e9, the monasteries, and the intendance of the \"\".\nThe Viscount of Blossac, intendant of Poitou from 1750 to 1784, had a French garden landscaped in Poitiers. He also had Eleanor of Aquitaine's ancient wall razed and have modern boulevards built in its place.\n19th century.\nDuring the 19th century, many army bases were built in Poitiers because of its strategic location. Poitiers would also become a garrison town, despite its distance from France's borders.\nThe Poitiers train station was built in the 1850s, and connected Poitiers to the rest of France. The H\u00f4tel de Ville (city hall) on Place du Mar\u00e9chal-Leclerc was completed in 1875.\n20th century and contemporary Poitiers.\nPoitiers was bombed during World War II, particularly the area around the railway station which was heavily hit on 13 June 1944.\nFrom the late 1950s until the late 1960s when Charles de Gaulle ended the American military presence, the U.S. Army and U.S. Air Force had an array of military installations in France, including a major Army logistics and communications hub in Poitiers, part of what was called the Communication Zone (ComZ), and consisting of a logistics headquarters and communications agency located at Aboville Caserne, a military compound situated on a hill above the city. Hundreds of graduates (\"Military Brats\") of Poitiers American High School, a school operated by the Department of Defense School System (DODDS), have gone on to maintain successful careers, including the recent commander-in-chief of the U.S. Special Forces Command, Army General Bryan (Doug) Brown. The Caserne also housed a full support community, with a theater, commissary, recreation facilities and an affiliate radio station of the American Forces Network, Europe, headquartered in Frankfurt (now Mannheim, Germany.)\nThe town benefited from the industrial \"D\u00e9centralisation\" of France in the 1970s, for instance with the installation during that decade of the Michelin and Compagnie des compteurs Schlumberger factories. The \"Futuroscope\" theme-park and research park project, built in 1986\u20131987 in nearby Chasseneuil-du-Poitou, after an idea by French politician Ren\u00e9 Monory, consolidated Poitiers' place as a touristic destination and as a modern university center, opening the town to the era of information technology. \nSports.\nThe Stade Poitevin, founded in 1900, is a multi-sports club, which fields several top-level teams in a variety of sports. These include a volleyball team that play in the French Pro A volleyball league, a basketball team, an amateur football team and a professional rugby team (as of the 2008\u20132009 season).\nThe PB86 or Poitiers Basket 86 play in the French Pro A basketball league. In the 2009\u201310 season, three Americans played for PB86: Rasheed Wright, Kenny Younger and Tommy Gunn. The team played the French championship playoffs in the 2009\u201310 season and was the Pro B French Champion for the 2008\u20132009 season. The team's communication strategy is considered by some to be one of the best in the French basketball scene.\nBrian Joubert, the French figure skating champion, practices at Poitiers' Ice Rink and lives with his family in the city.\nTourism.\nHistoric churches, in particular Romanesque buildings, are the main attraction inside Poitiers itself. The town center is gorgeous, with generally well-preserved architecture and a recently re-zoned pedestrian area. There are numerous shops, cafes and restaurants in the town centre.\nSince 1987, Poitiers' tourist industry has indirectly benefited from the \"Futuroscope\" theme-park and research park in nearby Chasseneuil-du-Poitou. The kernel of town receives visits in complement to the theme-park and benefits from a larger proportion of European tourists, notably from the United Kingdom. In conjunction, Poitiers' tourism has also benefited from the TGV high-speed rail link to Paris.\nTransport.\nPoitiers' railway station lies on the TGV Atlantique line between Paris and Bordeaux. The station is in the valley to the west of the old town center. Services run to Angoul\u00eame, Limoges and La Rochelle in addition to Paris and Bordeaux. The direct TGV puts Poitiers 1 hour and 40 minutes from the Parisian Gare Montparnasse.\nPoitiers\u2013Biard Airport is located west of Poitiers with flights to Lyon-Saint Exup\u00e9ry, London Stansted, Edinburgh and Shannon, Ireland on Ryanair.\nUrban transportation in Poitiers is provided by a company dubbed Vitalis although their e-infrastructure is difficult to access. Regional ground transportation in the department of the Vienne is provided by private bus companies such as \"Ligne en Vienne.\" Rail transportation in the region is provided by the public TER Nouvelle-Aquitaine (regional express train).\nBetween January 2009 and December 2012, Poitiers' town center underwent significant transformations aimed at reducing motor vehicle access. The initiative, called \"Projet C\u0153ur d'Agglo,\" sought to rethink the role of individual car use for accessing the town center and daily transportation. On 29 September 2010, 12 streets were permanently closed to motor vehicles, creating a fully pedestrianized zone. Lastly, a new line of fast buses was added around 2017.\nEducation.\nThe city of Poitiers has a very old tradition as being a prestigious town where many good universities agglomerate, starting as far back as the Middle Ages. The University of Poitiers was established in 1431 and has welcomed many famous philosophers and scientists throughout the ages (notably Fran\u00e7ois Rabelais; Ren\u00e9 Descartes; Francis Bacon; Samir Amin).\nToday Poitiers has more students per inhabitant than any other large town or city in France. All around, there are over 27,000 university students in Poitiers, nearly 4,000 of which are foreigners, hailing from 117 countries. The University covers all major fields from sciences to geography, history, languages economics and law.\nThe law degree at the University of Poitiers is considered to be one of the best in France. The program was ranked second by in 2005.\nIn addition to the University, Poitiers also hosts two engineering schools and two business schools:\nSince 2001, the city of Poitiers has hosted the first cycle of \"The South America, Spain and Portugal\" program from the Paris Institute of Political Studies, also known as Sciences Po.\nInternational relations.\nPoitiers is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotable people.\nThis is a list of people of interest who were born or resided in Poitiers:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51490", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=51490", "title": "Rights", "text": "Legal, social, or ethical principles\nRights are legal, social, or ethical principles of freedom or entitlement; that is, rights are the fundamental normative rules about what is allowed of people or owed to people according to some legal system, social convention, or ethical theory. Rights are an important concept in law and ethics, especially theories of justice and deontology.\nThe history of social conflicts has often involved attempts to define and redefine rights. According to the \"Stanford Encyclopedia of Philosophy\", \"rights structure the form of governments, the content of laws, and the shape of morality as it is currently perceived\".\nTypes of rights.\nNatural versus legal.\nSome thinkers see rights in only one sense while others accept that both senses have a measure of validity. There has been considerable philosophical debate about these senses throughout history. For example, Jeremy Bentham believed that legal rights were the essence of rights, and he denied the existence of natural rights, whereas Thomas Aquinas held that rights purported by positive law but not grounded in natural law were not properly rights at all, but only a facade or pretense of rights.\nClaim versus liberty.\nLiberty rights and claim rights are the inverse of one another: a person has a liberty right permitting him to do something only if there is no other person who has a claim right forbidding him from doing so. Likewise, if a person has a claim right against someone else, then that other person's liberty is limited. For example, a person has a \"liberty right\" to walk down a sidewalk and can decide freely whether or not to do so, since there is no obligation either to do so or to refrain from doing so. But pedestrians may have an obligation not to walk on certain lands, such as other people's private property, to which those other people have a claim right. So a person's \"liberty right\" of walking extends precisely to the point where another's \"claim right\" limits his or her freedom.\nPositive versus negative.\nIn one sense, a right is a permission to do something or an entitlement to a specific service or treatment from others, and these rights have been called \"positive rights\". However, in another sense, rights may allow or require inaction, and these are called \"negative rights\"; they permit or require doing nothing. For example, in some countries, e.g. the United States, citizens have the \"positive right\" to vote and they have the \"negative right\" to not vote; people can choose not to vote in a given election without punishment. In other countries, e.g. Australia, however, citizens have a positive right to vote but they do not have a negative right to not vote, since voting is compulsory. Accordingly:\nThough similarly named, positive and negative rights should not be confused with \"active rights\" (which encompass \"privileges\" and \"powers\") and \"passive rights\" (which encompass \"claims\" and \"immunities\").\nIndividual versus group.\nThere can be tension between individual and group rights. A classic instance in which group and individual rights clash is conflicts between unions and their members. For example, individual members of a union may wish a wage higher than the union-negotiated wage, but are prevented from making further requests; in a so-called closed shop which has a union security agreement, only the union has a \"right\" to decide matters for the individual union members such as wage rates. So, do the supposed \"individual rights\" of the workers prevail about the proper wage? Or do the \"group rights\" of the union regarding the proper wage prevail?\nThe Austrian School of Economics holds that only individuals think, feel, and act whether or not members of any abstract group. The society should thus according to economists of the school be analyzed starting from the individual. This methodology is called methodological individualism and is used by the economists to justify individual rights. Similarly, the author Ayn Rand argued that only individuals have rights, according to her philosophy known as Objectivism. However, others have argued that there are situations in which a group of persons is thought to have rights, or \"group rights\".\nOther senses.\nOther distinctions between rights draw more on historical association or family resemblance than on precise philosophical distinctions. These include the distinction between civil and political rights and economic, social and cultural rights, between which the articles of the Universal Declaration of Human Rights are often divided. Another conception of rights groups them into three generations. These distinctions have much overlap with that between negative and positive rights, as well as between individual rights and group rights, but these groupings are not entirely coextensive.\nPolitics.\nRights are often included in the foundational questions that governments and politics have been designed to deal with. Often the development of these socio-political institutions have formed a dialectical relationship with rights.\nRights about particular issues, or the rights of particular groups, are often areas of special concern. Often these concerns arise when rights come into conflict with other legal or moral issues, sometimes even other rights. Issues of concern have historically included Indigenous rights, labor rights, LGBTQ rights, reproductive rights, disability rights, patient rights and prisoners' rights. With increasing monitoring and the information society, information rights, such as the right to privacy are becoming more important.\nSome examples of groups whose rights are of particular concern include animals, and amongst humans, groups such as children and youth, parents (both mothers and fathers), and men and women.\nAccordingly, politics plays an important role in developing or recognizing the above rights, and the discussion about which behaviors are included as \"rights\" is an ongoing political topic of importance. The concept of rights varies with political orientation. Positive rights such as a \"right to medical care\" are emphasized more often by left-leaning thinkers, while right-leaning thinkers place more emphasis on negative rights such as the \"right to a fair trial\".\nFurther, the term \"equality\" which is often bound up with the meaning of \"rights\" often depends on one's political orientation. Conservatives and right-wing libertarians and advocates of free markets often identify equality with equality of opportunity, and want what they perceive as equal and fair rules in the process of making things, while agreeing that sometimes these fair rules lead to unequal outcomes. In contrast, socialists see the power imbalance of employer-employee relationships in capitalism as a cause of inequality and often see unequal outcomes as a hindrance to equality of opportunity. They tend to identify equality of outcome as a sign of equality and therefore think that people have a right to portions of necessities such as health care or economic assistance or housing that align with their needs.\nPhilosophy.\nIn philosophy, meta-ethics is the branch of ethics that seeks to understand the nature of ethical properties, statements, attitudes, and judgments. Meta-ethics is one of the three branches of ethics generally recognized by philosophers, the others being normative ethics and applied ethics.\nWhile normative ethics addresses such questions as \"What should one do?\", thus endorsing some ethical evaluations and rejecting others, meta-ethics addresses questions such as \"What \"is\" goodness?\" and \"How can we tell what is good from what is bad?\", seeking to understand the nature of ethical properties and evaluations.\nRights ethics is an answer to the meta-ethical question of \"what normative ethics is concerned with\" (meta-ethics also includes a group of questions about how ethics comes to be known, true, etc. which is not directly addressed by rights ethics).\nRights ethics holds that normative ethics is concerned with rights. Alternative meta-ethical theories are that ethics is concerned with one of the following:\nRights ethics has had considerable influence on political and social thinking. The Universal Declaration of Human Rights gives some concrete examples of widely accepted rights.\nCriticism.\nSome philosophers have criticised some rights as ontologically dubious entities.\nHistory.\nThe specific enumeration of rights has differed greatly in different periods of history. In many cases, the system of rights promulgated by one group has come into sharp and bitter conflict with that of other groups. In the political sphere, a place in which rights have historically been an important issue, constitutional provisions of various states sometimes address the question of who has what legal rights.\nHistorically, many notions of rights were authoritarian and hierarchical, with different people granted different rights, and some having more rights than others. For instance, the right of a father to be respected by his son did not indicate a right of the son to receive something in return for that respect; and the divine right of kings, which permitted absolute power over subjects, did not leave much possibility for many rights for the subjects themselves.\nIn contrast, modern conceptions of rights have often emphasized liberty and equality as among the most important aspects of rights, as was evident in the American and French revolutions.\nImportant documents in the political history of rights include:\nSee also.\nOrganisations:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51491", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=51491", "title": "Samarkand", "text": "City in southeastern Uzbekistan\nSamarkand is a city in southeastern Uzbekistan and among the oldest continuously inhabited cities in Central Asia. Samarkand is the capital of the Samarkand Region and a district-level city, that includes the urban-type settlements Kimyogarlar, Farhod and Khishrav. With 551,700 inhabitants (2021), it is the third-largest city in Uzbekistan.\nThere is evidence of human activity in the area of the city dating from the late Paleolithic Era. Though there is no direct evidence of when Samarkand was founded, several theories propose that it was founded between the 8th and 7th centuries BC. Prospering from its location on the Silk Road between China, Persia and Europe, at times Samarkand was one of the largest cities in Central Asia, By the time of the Persian Achaemenid Empire, it was the capital of the Sogdian satrapy. The city was conquered by Alexander the Great in 329 BC, when it was known as Markanda, which was rendered in Greek as . The city was ruled by a succession of Iranian and Turkic rulers until it was conquered by the Mongols under Genghis Khan in 1220. \nSince the 11th century, the city has been under Turkic/Uzbek political control, but the city\u2019s culture and dominant language has always been Persian.\nThe city is noted as a centre of Islamic scholarly study and the birthplace of the Timurid Renaissance. In the 14th century, Timur made it the capital of his empire and the site of his mausoleum, the Gur-e Amir. The Bibi-Khanym Mosque, rebuilt during the Soviet era, remains one of the city's most notable landmarks. Samarkand's Registan square was the city's ancient centre and is bounded by three monumental religious buildings. The city has carefully preserved the traditions of ancient crafts: embroidery, goldwork, silk weaving, copper engraving, ceramics, wood carving, and wood painting. In 2001, UNESCO added the city to its World Heritage List as \"Samarkand \u2013 Crossroads of Cultures\".\nModern Samarkand is divided into two parts: the old city, which includes historical monuments, shops, and old private houses; and the new city, which was developed during the days of the Russian Empire and Soviet Union and includes administrative buildings along with cultural centres and educational institutions. On 15 and 16 September 2022, the city hosted the 2022 SCO summit.\nSamarkand has a multicultural and plurilingual history that was significantly modified by the process of national delimitation in Central Asia. Small group of inhabitants of the city are bilingual speakers of the Tajik language, whereas Uzbek is the official language and Russian is also widely used in the public sphere, as per Uzbekistan's language policy. \nEtymology.\nThe name comes from the Iranian languages Persian and Sogdian \"stone, rock\" and \"fort, town.\" In this respect, \"Samarkand\" shares the same meaning as the name of the Uzbek capital Tashkent, with \"tash-\" being the Turkic term for \"stone\" and \"-kent\" the Turkic analogue of \"kand\" borrowed from Iranian languages.\nAccording to 11th-century scholar Mahmud al-Kashghari, the city was known in Karakhanid Turkic as \"\" (\u0633\u064e\u0645\u0650\u0632\u0652\u06a9\u064e\u0646\u0652\u062f\u0652\u200e), meaning \"fat city.\" 16th-century Mughal emperor Babur also mentioned the city under this name, and 15th-century Castillian traveler Ruy Gonz\u00e1lez de Clavijo stated that Samarkand was simply a distorted form of it.\nHistory.\nEarly history.\nAlong with Bukhara, Samarkand is one of the oldest inhabited cities in Central Asia, prospering from its location on the trade route between China and Europe. There is no direct evidence of when it was founded. Researchers at the Institute of Archaeology of Samarkand date the city's founding around 700 BC.\nArchaeological excavations conducted within the city limits (Syob and midtown) as well as suburban areas (Hojamazgil, Sazag'on) unearthed 40,000-year-old evidence of human activity, dating back to the Upper Paleolithic. A group of Mesolithic (12th\u20137th millennia BC) archaeological sites were discovered in the suburbs of Sazag'on-1, Zamichatosh, and Okhalik. The Syob and Darg'om canals, supplying the city and its suburbs with water, appeared around the 7th\u20135th centuries BC (early Iron Age).\nFrom its earliest days, Samarkand was one of the main centres of Sogdian civilization. By the time of the Achaemenid dynasty of Persia, the city had become the capital of the Sogdian satrapy.\nHellenistic period.\nAlexander the Great conquered Samarkand in 329 BC. The city was known as Maracanda (\u039c\u03b1\u03c1\u03ac\u03ba\u03b1\u03bd\u03b4\u03b1) by the Greeks. Written sources offer small clues as to the subsequent system of government. They mention one Orepius who became ruler \"not from ancestors, but as a gift of Alexander.\"\nWhile Samarkand suffered significant damage during Alexander's initial conquest, the city recovered rapidly and flourished under the new Hellenic influence. There were also major new construction techniques. Oblong bricks were replaced with square ones and superior methods of masonry and plastering were introduced.\nAlexander's conquests introduced classical Greek culture into Central Asia and for a time, Greek aesthetics heavily influenced local artisans. This Hellenistic legacy continued as the city became part of various successor states in the centuries following Alexander's death, the Greek Seleucid Empire, Greco-Bactrian Kingdom, and Kushan Empire (even though the Kushana themselves originated in Central Asia). After the Kushan state lost control of Sogdia during the 3rd century AD, Samarkand went into decline as a centre of economic, cultural, and political power. It did not significantly revive until the 5th century.\nSasanian era.\nSamarkand was conquered by the Persian Sasanians c. 260 AD. Under Sasanian rule, the region became an essential site for Manichaeism and facilitated the dissemination of the religion throughout Central Asia.\nHephthalites and Turkic Khaganate era.\nBetween AD 350 and 375, Samarkand was conquered by the nomadic tribes of Xionites, the origin of which remains controversial. The resettlement of nomadic groups to Samarkand confirms archaeological material from the 4th century. The culture of nomads from the Middle Syrdarya basin is spreading in the region. Between 457 and 509, Samarkand was part of the Kidarite state.\nAfter the Hephthalites (\"White Huns\") conquered Samarkand, they controlled it until the G\u00f6kt\u00fcrks, in an alliance with the Sassanid Persians, won it at the Battle of Bukhara, c. 560 AD.\nIn the middle of the 6th century, a Turkic state was formed in Altai, founded by the Ashina dynasty. The new state formation was named the Turkic Khaganate after the people of the Turks, which were headed by the ruler \u2013 the Khagan. From 557 to 561, the Hephthalites empire was defeated by the joint actions of the Turks and Sassanids, which led to the establishment of a common border between the two empires.\nIn the early Middle Ages, Samarkand was surrounded by four rows of defensive walls and had four gates.\nAn ancient Turkic burial with a horse was investigated on the territory of Samarkand. It dates back to the 6th century.\nDuring the period of the ruler of the Western Turkic Khaganate, Tong Yabghu Qaghan (618\u2013630), family relations were established with the ruler of Samarkand \u2013 Tong Yabghu Qaghan gave him his daughter.\nSome parts of Samarkand have been Christian since the 4th century. In the 5th century, a Nestorian chair was established in Samarkand. At the beginning of the 8th century, it was transformed into a Nestorian metropolitanate. Discussions and polemics arose between the Sogdian followers of Christianity and Manichaeism, reflected in the documents.\nEarly Islamic era.\nThe armies of the Umayyad Caliphate under Qutayba ibn Muslim captured the city from the Tang dynasty c. 710 CE. During this period, Samarkand was a diverse religious community and was home to a number of religions, including Zoroastrianism, Buddhism, Hinduism, Manichaeism, Judaism, and Nestorian Christianity, with most of the population following Zoroastrianism.\nQutayba generally did not settle Arabs in Central Asia; he forced the local rulers to pay him tribute but largely left them to their own devices. Samarkand was the major exception to this policy: Qutayba established an Arab garrison and Arab governmental administration in the city, its Zoroastrian fire temples were razed, and a mosque was built. Much of the city's population converted to Islam.\nAs a long-term result, Samarkand developed into a center of Islamic and Arabic learning. At the end of the 740s, a movement of those dissatisfied with the power of the Umayyads emerged in the Arab Caliphate, led by the Abbasid commander Abu Muslim, who, after the victory of the uprising, became the governor of Khorasan and Maverannahr (750\u2013755). He chose Samarkand as his residence. His name is associated with the construction of a multi-kilometer defensive wall around the city and the palace.\nLegend has it that during Abbasid rule, the secret of papermaking was obtained from two Chinese prisoners from the Battle of Talas in 751, which led to the foundation of the first paper mill in the Islamic world at Samarkand. The invention then spread to the rest of the Islamic world and thence to Europe.\nAbbasid control of Samarkand soon dissipated and was replaced with that of the Samanids (875\u2013999), though the Samanids were still nominal vassals of the Caliph during their control of Samarkand. Under Samanid rule the city became a capital of the Samanid dynasty and an even more important node of numerous trade routes. The Samanids were overthrown by the Karakhanids around 999. Over the next 200 years, Samarkand would be ruled by a succession of Turkic tribes, including the Seljuqs and the Khwarazmshahs.\nThe 10th-century Persian author Istakhri, who travelled in Transoxiana, provides a vivid description of the natural riches of the region he calls \"Smarkandian Sogd\":\nI know no place in it or in Samarkand itself where if one ascends some elevated ground one does not see greenery and a pleasant place, and nowhere near it are mountains lacking in trees or a dusty steppe... Samakandian Sogd... [extends] eight days travel through unbroken greenery and gardens... . The greenery of the trees and sown land extends along both sides of the river [Sogd]... and beyond these fields is pasture for flocks. Every town and settlement has a fortress... It is the most fruitful of all the countries of Allah; in it are the best trees and fruits, in every home are gardens, cisterns and flowing water.\nKarakhanid (Ilek-Khanid) period (11th\u201312th centuries).\nAfter the fall of the Samanids state in 999, it was replaced by the Qarakhanid State, where the Turkic Qarakhanid dynasty ruled. After the state of the Qarakhanids split into two parts, Samarkand became a part of the West Karakhanid Khaganate and from 1040 to 1212 was its capital. The founder of the Western Qarakhanid Khaganate was Ibrahim Tamgach Khan (1040\u20131068). For the first time, he built a madrasah in Samarkand with state funds and supported the development of culture in the region. During his reign, a public hospital (bemoristan) and a madrasah were established in Samarkand, where medicine was also taught.\nThe memorial complex Shah-i-Zinda was founded by the rulers of the Karakhanid dynasty in the 11th century.\nThe most striking monument of the Qarakhanid era in Samarkand was the palace of Ibrahim ibn Hussein (1178\u20131202), which was built in the citadel in the 12th century. During the excavations, fragments of monumental painting were discovered. On the eastern wall, a Turkic warrior was depicted, dressed in a yellow caftan and holding a bow. Horses, hunting dogs, birds and periodlike women were also depicted here.\nMongol period.\nThe Mongols conquered Samarkand in 1220. Juvayni writes that Genghis killed all who took refuge in the citadel and the mosque, pillaged the city completely, and conscripted 30,000 young men along with 30,000 craftsmen. Samarkand suffered at least one other Mongol sack by Khan Baraq to get treasure he needed to pay an army. It remained part of the Chagatai Khanate (one of four Mongol successor realms) until 1370.\n\"The Travels\" of Marco Polo, where Polo records his journey along the Silk Road in the late 13th century, describes Samarkand as \"a very large and splendid city...\"\nThe Yenisei area had a community of weavers of Chinese origin, and Samarkand and Outer Mongolia both had artisans of Chinese origin, as reported by Changchun. After Genghis Khan conquered Central Asia, foreigners were chosen as governmental administrators; Chinese and Qara-Khitays (Khitans) were appointed as co-managers of gardens and fields in Samarkand, which Muslims were not permitted to manage on their own. The khanate allowed the establishment of Christian bishoprics (see below).\nTimur's rule (1370\u20131405).\nIbn Battuta, who visited in 1333, called Samarkand \"one of the greatest and finest of cities, and most perfect of them in beauty.\" He also noted that the orchards were supplied water via \"norias\".\nIn 1365, a revolt against Chagatai Mongol control occurred in Samarkand. In 1370, the conqueror Timur (Tamerlane), the founder and ruler of the Timurid Empire, made Samarkand his capital. Timur used various tools for legitimisation, including urban planning in his capital, Samarkand. Over the next 35 years, he rebuilt most of the city and populated it with great artisans and craftsmen from across the empire. Timur gained a reputation as a patron of the arts, and Samarkand grew to become the centre of the region of Transoxiana. Timur's commitment to the arts is evident in how, in contrast with the ruthlessness he showed his enemies, he demonstrated mercy toward those with special artistic abilities. The lives of artists, craftsmen, and architects were spared so that they could improve and beautify Timur's capital.\nTimur was also directly involved in construction projects, and his visions often exceeded the technical abilities of his workers. The city was in a state of constant construction, and Timur would often order buildings to be done and redone quickly if he was unsatisfied with the results. By his orders, Samarkand could be reached only by roads; deep ditches were dug, and walls in circumference separated the city from its surrounding neighbors. At this time, the city had a population of about 150,000.\nHenry III of Castile's ambassador Ruy Gonzalez de Clavijo, who visited Samarkand between 8 September and 20 November 1404, attested to the never-ending construction that went on in the city. \"The Mosque which Timur had built seemed to us the noblest of all those we visited in the city of Samarkand.\"\nUlugh Beg's period (1409\u20131449).\nBetween 1417 and 1420, Timur's grandson Ulugh Beg built a madrasah in Samarkand, which became the first building in the architectural ensemble of Registan. Ulugh Beg invited a large number of astronomers and mathematicians of the Islamic world to this madrasah. Under Ulugh Beg, Samarkand became one of the world centers of medieval science. In the first half of the 15th century, a whole scientific school arose around Ulugh Beg, uniting prominent astronomers and mathematicians including Jamshid al-Kashi, Q\u0101\u1e0d\u012b Z\u0101da al-R\u016bm\u012b, and Ali Qushji. Ulugh Beg's main interest in science was astronomy, and he constructed an observatory in 1428. Its main instrument was the wall quadrant, which was unique in the world. It was known as the \"Fakhri Sextant\" and had a radius of 40 meters. Seen in the image on the left, the arc was finely constructed with a staircase on either side to provide access for the assistants who performed the measurements.\n16th\u201318th centuries.\nIn 1500, nomadic Uzbek warriors took control of Samarkand. The Shaybanids emerged as the city's leaders at or about this time. In 1501, Samarkand was finally taken by Muhammad Shaybani from the Uzbek dynasty of Shaybanids, and the city became part of the newly formed \u201cBukhara Khanate\u201d. Samarkand was chosen as the capital of this state, in which Muhammad Shaybani Khan was crowned. In Samarkand, Muhammad Shaybani Khan ordered to build a large madrasah, where he later took part in scientific and religious disputes. The first dated news about the Shaybani Khan madrasah dates back to 1504 (it was completely destroyed during the years of Soviet power). Muhammad Salikh wrote that Sheibani Khan built a madrasah in Samarkand to perpetuate the memory of his brother Mahmud Sultan.\nFazlallah ibn Ruzbihan in \"Mikhmon-namei Bukhara\" expresses his admiration for the majestic building of the madrasah, its gilded roof, high hujras, spacious courtyard and quotes a verse praising the madrasah. Zayn ad-din Vasifi, who visited the Sheibani-khan madrasah several years later, wrote in his memoirs that the veranda, hall and courtyard of the madrassah are spacious and magnificent.\nAbdulatif Khan, the son of Mirzo Ulugbek's grandson Kuchkunji Khan, who ruled in Samarkand from 1540 to 1551, was considered an expert in the history of Maverannahr and the Shibanid dynasty. He patronized poets and scientists. Abdulatif Khan himself wrote poetry under the literary pseudonym Khush.\nDuring the reign of the Ashtarkhanid Imam Quli Khan (1611\u20131642) famous architectural masterpieces were built in Samarkand. In 1612\u20131656, the governor of Samarkand, Yalangtush Bahadur, built a cathedral mosque, Tillya-Kari madrasah and Sherdor madrasah.\nZarafshan Water Bridge is a brick bridge built on the left bank of the Zarafshan River, 7\u20138\u00a0km northeast of the center of Samarkand, built by Shaibani Khan at the beginning of the 16th century.\nAfter an assault by the Afshar Shahanshah Nader Shah, the city was abandoned in the early 1720s. From 1599 to 1756, Samarkand was ruled by the Ashtrakhanid branch of the Khanate of Bukhara.\nSecond half of the 18th\u201319th centuries.\nFrom 1756 to 1868, it was ruled by the Manghud Emirs of Bukhara. The revival of the city began during the reign of the founder of the Uzbek dynasty, the Mangyts, Muhammad Rakhim (1756\u20131758), who became famous for his strong-willed qualities and military art. Muhammad Rakhimbiy made some attempts to revive Samarkand.\nRussian Empire period.\nThe city came under imperial Russian rule after the citadel had been taken by a force under Colonel Konstantin Petrovich von Kaufman in 1868. Shortly thereafter the small Russian garrison of 500 men were themselves besieged. The assault, which was led by Abdul Malik Tura, the rebellious elder son of the Bukharan Emir, as well as Baba Beg of Shahrisabz and Jura Beg of Kitab, was repelled with heavy losses. General Alexander Konstantinovich Abramov became the first Governor of the Military Okrug, which the Russians established along the course of the Zeravshan River with Samarkand as the administrative centre. The Russian section of the city was built after this point, largely west of the old city.\nIn 1886, the city became the capital of the newly formed Samarkand Oblast of Russian Turkestan and regained even more importance when the Trans-Caspian railway reached it in 1888.\nSoviet period.\nSamarkand was the capital of Turkestan Provisional Government in 1922 and was the capital of the Uzbek SSR from 1925 to 1930, before being replaced by Tashkent. During World War II, after Nazi Germany invaded the Soviet Union, a number of Samarkand's citizens were sent to Smolensk to fight the enemy. Many were taken captive or killed by the Nazis. Additionally, thousands of refugees from the occupied western regions of the USSR fled to the city, and it served as one of the main hubs for the fleeing civilians in the Uzbek Soviet Socialist Republic and the Soviet Union as a whole.\nEuropean study of the history of Samarkand began after the conquest of Samarkand by the Russian Empire in 1868. The first studies of the history of Samarkand belong to N. Veselovsky, V. Bartold and V. Vyatkin. In the Soviet period, the generalization of materials on the history of Samarkand was reflected in the two-volume \"History of Samarkand\" edited by the academician of Uzbekistan Ibrohim Mo\u02bbminov.\nOn the initiative of Academician of the Academy of Sciences of the Uzbek SSR I. Muminov and with the support of Sharaf Rashidov, the 2500th anniversary of Samarkand was widely celebrated in 1970. In this regard, a monument to Ulugh Beg was opened, the Museum of the History of Samarkand was founded, and a two-volume history of Samarkand was prepared and published.\nAfter Uzbekistan gained independence, several monographs were published on the ancient and medieval history of Samarkand.\nGeography.\nSamarkand is located in southeastern Uzbekistan, in the Zarefshan River valley, 135\u00a0km from Qarshi. Road M37 connects Samarkand to Bukhara, 240\u00a0km away. Road M39 connects it to Tashkent, 270\u00a0km away. The Tajikistan border is about 35\u00a0km from Samarkand; the Tajik capital Dushanbe is 210\u00a0km away from Samarkand. Road M39 connects Samarkand to Mazar-i-Sharif in Afghanistan, which is 340\u00a0km away.\nClimate.\nSamarkand has a cold semi-arid climate (K\u00f6ppen climate classification: \"BSk\") with hot, dry summers and relatively wet, variable winters that alternate periods of warm weather with periods of cold weather. July and August are the hottest months of the year, with temperatures reaching and exceeding . Precipitation is sparse from June through October, but increases to a maximum from February to April. January 2008 was particularly cold; the temperature dropped to .\nPeople.\nAccording to official reports, a majority of Samarkand's inhabitants are Uzbeks, while many sources refer to the city as majority Tajik, up to 70 percent of the city's population. Tajiks are especially concentrated in the eastern part of the city, where the main architectural landmarks are.\nAccording to various independent sources, Tajiks are Samarkand's majority ethnic group. Ethnic Uzbeks are the second-largest group and are most concentrated in the west of Samarkand. Exact demographic figures are difficult to obtain since some people in Uzbekistan identify as \"Uzbek\" even though they speak Tajiki as their first language, often because they are registered as Uzbeks by the central government despite their Tajiki language and identity. As explained by Paul Bergne:\nDuring the census of 1926 a significant part of the Tajik population was registered as Uzbek. Thus, for example, in the 1920 census in Samarkand city the Tajiks were recorded as numbering 44,758 and the Uzbeks only 3301. According to the 1926 census, the number of Uzbeks was recorded as 43,364 and the Tajiks as only 10,716. In a series of kishlaks [villages] in the Khojand Okrug, whose population was registered as Tajik in 1920 e.g. in Asht, Kalacha, Akjar i Tajik and others, in the 1926 census they were registered as Uzbeks. Similar facts can be adduced also with regard to Ferghana, Samarkand, and especially the Bukhara oblasts.\nSamarkand is also home to large ethnic communities of \"Iranis\" (the old, Persian-speaking, Shia population of Merv city and oasis, deported en masse to this area in the late 18th century), Russians, Ukrainians, Belarusians, Armenians, Azeris, Tatars, Koreans, Poles, and Germans, all of whom live primarily in the centre and western neighborhoods of the city. These peoples have emigrated to Samarkand since the end of the 19th century, especially during the Soviet Era; by and large, they speak the Russian language.\nIn the extreme west and southwest of Samarkand is a population of Central Asian Arabs, who mostly speak Uzbek; only a small portion of the older generation speaks Central Asian Arabic. In eastern Samarkand there was once a large mahallah of Bukharian (Central Asian) Jews, but starting in the 1970s, hundreds of thousands of Jews left Uzbekistan for Israel, United States, Canada, Australia, and Europe. Only a few Jewish families are left in Samarkand today.\nAlso in the eastern part of Samarkand there are several quarters where (Lyuli, Djugi, Parya, and other groups) live. These peoples began to arrive in Samarkand several centuries ago from what are now India and Pakistan. They mainly speak a dialect of the Tajik language, as well as their own languages, most notably Parya.\nLanguage.\nThe state and official language in Samarkand, as in all Uzbekistan, is the Uzbek language. Uzbek is one of the Turkic languages and the mother tongue of Uzbeks, Turkmens, Samarkandian Iranians, and most Samarkandian Arabs living in Samarkand.\nAs in the rest of Uzbekistan, the Russian language is the de facto second official language in Samarkand, and about 5% of signs and inscriptions in Samarkand are in this language. Russians, Belarusians, Poles, Germans, Koreans, the majority of Ukrainians, the majority of Armenians, Greeks, some Tatars, and some Azerbaijanis in Samarkand speak Russian. Several Russian-language newspapers are published in Samarkand, the most popular of which is \"Samarkandskiy vestnik\" (Russian: \u0421\u0430\u043c\u0430\u0440\u043a\u0430\u043d\u0434\u0441\u043a\u0438\u0439 \u0432\u0435\u0441\u0442\u043d\u0438\u043a, lit. the Samarkand Herald). The Samarkandian TV channel STV conducts some broadcasts in Russian.\nDe facto, the most common native language in Samarkand is Tajik, which is a dialect or variant of the Persian language. Samarkand was one of the cities in which the Persian language developed. Many classical Persian poets and writers lived in or visited Samarkand over the millennia, the most famous being Abulqasem Ferdowsi, Omar Khayyam, Abdurahman Jami, Abu Abdullah Rudaki, Suzani Samarqandi, and Kamal Khujandi.\nWhile the official stance is that Uzbek is the most common language in Samarkand, some data indicate that only about 30% of residents speak it as a native tongue. For the other 70%, Tajik is the native tongue, with Uzbek the second language and Russian the third. However, as no population census has been taken in Uzbekistan since 1989, there are no accurate data on this matter. Despite Tajik being the second most common language in Samarkand, it does not enjoy the status of an official or regional language. Nevertheless, at Samarkand State University ten faculties offer courses in Tajiki, and the Tajik Language and Literature Department has an enrolment of over 170 students. Only one newspaper in Samarkand is published in Tajiki, in the Cyrillic Tajik alphabet: \"\" (Tajik: \"\u041e\u0432\u043e\u0437\u0438 \u0421\u0430\u043c\u0430\u0440\u049b\u0430\u043d\u0434\" \u2014\"Voice of Samarkand\"). Local Samarkandian STV and \"Samarkand\" TV channels offer some broadcasts in Tajik, as does one regional radio station. In 2022 a quarterly literary magazine in Tajiki, \"Durdonai Sharq\", was launched in Samarkand.\nIn addition to Uzbek, Tajik, and Russian, native languages spoken in Samarkand include Ukrainian, Armenian, Azerbaijani, Tatar, Crimean Tatar, Arabic (for a very small percentage of Samarkandian Arabs), and others.\nModern Samarkand is a vibrant city, and in 2019 the city hosted the first Samarkand Half Marathon. In 2022 this also included a full marathon for the first time.\nReligion.\nIslam.\nIslam entered Samarkand in the 8th century, during the invasion of the Arabs in Central Asia (Umayyad Caliphate). Before that, almost all inhabitants of Samarkand were Zoroastrians, and many Nestorians and Buddhists also lived in the city. From that point forward, throughout the reigns of many Muslim governing powers, numerous mosques, madrasahs, minarets, shrines, and mausoleums were built in the city. Many have been preserved. For example, there is of Imam Bukhari, an Islamic scholar who authored the hadith collection known as \"Sahih al-Bukhari\", which Sunni Muslims regard as one of the most authentic (\"sahih\") hadith collections. His other books included \"Al-Adab al-Mufrad\". Samarkand is also home to of Imam Maturidi, the founder of Maturidism and the , who is revered in Islam, Judaism, and Christianity.\nMost inhabitants of Samarkand are Muslim, primarily Sunni (mostly Hanafi) and Sufi. Approximately 80\u201385% of Muslims in the city are Sunni, comprising almost all Tajiks, Uzbeks, and Samarkandian Arabs living therein. Samarkand's best-known Islamic sacred lineages are the descendants of Sufi leaders such as Khodja Akhror Wali (1404\u20131490) and Makhdumi A\u2019zam (1461\u20131542), the descendants of Sayyid Ata (first half of 14th c.) and Mirakoni Khojas (Sayyids from Mirakon, a village in Iran). The liberal policy of President Shavkat Mirziyoyev opened up new opportunities for the expression of the religious identity. In Samarkand, since 2018, there has been an increase in the number of women wearing the hijab.\nShia Muslims.\nThe Samarqand Vilayat is one of the two regions of Uzbekistan (along with Bukhara Vilayat) that are home to a large number of Shiites. The total population of the Samarkand Vilayat is more than 3,720,000 people (2019).\nThere are no exact data on the number of Shiites in the city of Samarkand, but the city has several Shiite mosques and madrasas. The largest of these are the Punjabi Mosque, the Punjabi Madrassah, and the Mausoleum of Mourad Avliya. Every year, the Shiites of Samarkand celebrate Ashura, as well as other memorable Shiite dates and holidays.\nShiites in Samarkand are mostly , who call themselves \"Irani\". Their ancestors began to arrive in Samarkand in the 18th century. Some migrated there\nin search of a better life, others were sold as slaves there by Turkmen captors, and others were soldiers who were posted to Samarkand. Mostly they came from Khorasan, Mashhad, Sabzevar, Nishapur, and Merv; and secondarily from Iranian Azerbaijan, Zanjan, Tabriz, and Ardabil. Samarkandian Shiites also include Azerbaijanis, as well as small numbers of Tajiks and Uzbeks.\nWhile there are no official data on the total number of Shiites in Uzbekistan, they are estimated to be \"several hundred thousand.\" According to leaked diplomatic cables, in 2007\u20132008, the US Ambassador for International Religious Freedom held a series of meetings with Sunni mullahs and Shiite imams in Uzbekistan. During one of the talks, the imam of the Shiite mosque in Bukhara said that about 300,000 Shiites live in the Bukhara Vilayat and 1\u00a0million in the Samarkand Vilayat. The Ambassador slightly doubted the authenticity of these figures, emphasizing in his report that data on the numbers of religious and ethnic minorities provided by the government of Uzbekistan were considered a very \"delicate topic\" due to their potential to provoke interethnic and interreligious conflicts. All the ambassadors of the ambassador tried to emphasize that traditional Islam, especially Sufism and Sunnism, in the regions of Bukhara and Samarkand is characterized by great religious tolerance toward other religions and sects, including Shiism.\nChristianity.\nChristianity was introduced to Samarkand when it was part of Sogdiana, long before the penetration of Islam into Central Asia. The city then became one of the centers of Nestorianism in Central Asia. The majority of the population were then Zoroastrians, but since Samarkand was the crossroads of trade routes among China, Persia, and Europe, it was religiously tolerant. Under the Umayyad Caliphate, Zoroastrians and Nestorians were persecuted by the Arab conquerors; the survivors fled to other places or converted to Islam. Several Nestorian temples were built in Samarkand, but they have not survived. Their remains were found by archeologists at the ancient site of Afrasiyab and on the outskirts of Samarkand.\nIn the three decades of 1329\u20131359, the of the Roman Catholic Church served several thousand Catholics who lived in the city. According to Marco Polo and Johann Elemosina, a descendant of Chaghatai Khan, the founder of the Chaghatai dynasty, Eljigidey, converted to Christianity and was baptized. With the assistance of Eljigidey, the Catholic Church of St. John the Baptist was built in Samarkand. After a while, however, Islam completely supplanted Catholicism.\nChristianity reappeared in Samarkand several centuries later, from the mid-19th century onward, after the city was seized by the Russian Empire. Russian Orthodoxy was introduced to Samarkand in 1868, and several churches and temples were built. In the early 20th century several more Orthodox cathedrals, churches, and temples were built, most of which were demolished while Samarkand was part of the USSR.\nIn present time, Christianity is the second-largest religious group in Samarkand with the predominant form is the Russian Orthodox Church (Moscow Patriarchate). More than 5% of Samarkand residents are Orthodox, mostly Russians, Ukrainians, and Belarusians, and also some Koreans and Greeks. Samarkand is the center of the Samarkand branch (which includes the Samarkand, Qashqadarya, and Surkhandarya provinces of Uzbekistan) of the of the of the Russian Orthodox Church of the Moscow Patriarchate. The city has several active Orthodox churches: , , and . There are also a number of inactive Orthodox churches and temples, for example that of .\nThere are also a few tens of thousands of Catholics in Samarkand, mostly Poles, Germans, and some Ukrainians. In the center of Samarkand is St. John the Baptist Catholic Church, which was built at the beginning of the 20th century. Samarkand is part of the Apostolic Administration of Uzbekistan.\nThe third largest Christian sect in Samarkand is the Armenian Apostolic Church, followed by a few tens of thousands of Armenian Samarkandians. Armenian Christians began emigrating to Samarkand at the end of the 19th century, this flow increasing especially in the Soviet era. In the west of Samarkand is the .\nSamarkand also has several thousand Protestants, including Lutherans, Baptists, Mormons, Jehovah's Witnesses, Adventists, and members of the Korean Presbyterian church. These Christian movements appeared in Samarkand mainly after the independence of Uzbekistan in 1991.\nLandmarks.\nSilk Road Samarkand (Eternal city).\nSilk Road Samarkand is a modern multiplex which opened in early 2022 in eastern Samarkand. The complex covers 260 hectares and includes world-class business and medical hotels, eateries, recreational facilities, park grounds, an ethnographic corner and a large congress hall for hosting international events.\nEternal city situated in Silk Road Samarkand complex. This site which occupies 17 hectares accurately recreates the spirit of the ancient city backed up by the history and traditions of Uzbek lands and Uzbek people for the guests of the Silk Road Samarkand. The narrow streets here house multiple shops of artists, artisans, and craftsmen. The pavilions of the Eternal City were inspired by real houses and picturesque squares described in ancient books. This is where you can plunge into a beautiful oriental fairy tale: with turquoise domes, mosaics on palaces, and high minarets that pierce the sky.\nVisitors to the Eternal City can taste national dishes from different eras and regions of the country and also see authentic street performances. The Eternal City showcases a unique mix of Parthian, Hellenistic, and Islamic cultures so that the guests could imagine the versatile heritage of bygone centuries in full splendor. The project was inspired and designed by Bobur Ismoilov, a famous modern artist.\nArchitecture.\nTimur initiated the building of Bibi-Khanym after his 1398\u20131399 campaign in India. The Bibi-Khanym originally had about 450 marble columns, which were hauled there and set up with the help of 95 elephants that Timur had brought back from Hindustan. Artisans and stonemasons from India designed the mosque's dome, giving it its distinctive appearance amongst the other buildings. An 1897 earthquake destroyed the columns, which were not entirely restored in the subsequent reconstruction.\nThe best-known landmark of Samarkand is the mausoleum known as Gur-i Amir. It exhibits the influences of many cultures, past civilizations, neighboring peoples, and religions, especially those of Islam. Despite the devastation wrought by Mongols to Samarkand's pre-Timurid Islamic architecture, under Timur these architectural styles were revived, recreated, and restored. The blueprint and layout of the mosque itself, with their precise measurements, demonstrate the Islamic passion for geometry. The entrance to the Gur-i Amir is decorated with Arabic calligraphy and inscriptions, the latter a common feature in Islamic architecture. Timur's meticulous attention to detail is especially obvious inside the mausoleum: the tiled walls are a marvelous example of mosaic faience, an Iranian technique in which each tile is cut, colored, and fit into place individually. The tiles of the Gur-i Amir were also arranged so that they spell out religious words such as \"Muhammad\" and \"Allah.\"\nThe ornamentation of the Gur-i Amir's walls includes floral and vegetal motifs, which signify gardens; the floor tiles feature uninterrupted floral patterns. In Islam, gardens are symbols of paradise, and as such, they were depicted on the walls of tombs and grown in Samarkand itself. Samarkand boasted two major gardens, the New Garden and the Garden of Heart's Delight, which became the central areas of entertainment for ambassadors and important guests. In 1218, a friend of Genghis Khan named Yel\u00fc Chucai reported that Samarkand was the most beautiful city of all, as \"it was surrounded by numerous gardens. Every household had a garden, and all the gardens were well designed, with canals and water fountains that supplied water to round or square-shaped ponds. The landscape included rows of willows and cypress trees, and peach and plum orchards were shoulder to shoulder.\" Persian carpets with floral patterns have also been found in some Timurid buildings.\nThe elements of traditional Islamic architecture can be seen in traditional mud-brick Uzbek houses that are built around central courtyards with gardens. Most of these houses have painted wooden ceilings and walls. By contrast, houses in the west of the city are chiefly European-style homes built in the 19th and 20th centuries.\nTurko-Mongol influence is also apparent in Samarkand's architecture. It is believed that the melon-shaped domes of the mausoleums were designed to echo \"yurts\" or \"gers\", traditional Mongol tents in which the bodies of the dead were displayed before burial or other disposition. Timur built his tents from more-durable materials, such as bricks and wood, but their purposes remained largely unchanged. The chamber in which Timur's own body was laid included \"tugs\", poles whose tops were hung with a circular arrangement of horse or yak tail hairs. These banners symbolized an ancient Turkic tradition of sacrificing horses, which were valuable commodities, to honor the dead. Tugs were also a type of cavalry standard used by many nomads, up to the time of the Ottoman Turks.\nColors of buildings in Samarkand also have significant meanings. The dominant architectural color is blue, which Timur used to convey a broad range of concepts. For example, the shades of blue in the Gur-i Amir are colors of mourning; in that era, blue was the color of mourning in Central Asia, as it still is in various cultures today. Blue was also considered the color that could ward off \"the evil eye\" in Central Asia; this notion is evidenced by in the number of blue-painted doors in and around the city. Furthermore, blue represented water, a particularly rare resource in the Middle East and Central Asia; walls painted blue symbolized the wealth of the city.\nGold also has a strong presence in the city. Timur's fascination with vaulting explains the excessive use of gold in the Gur-i Amir, as well as the use of embroidered gold fabric in both the city and his buildings. The Mongols had great interests in Chinese- and Persian-style golden silk textiles, as well as \"nasij\" woven in Iran and Transoxiana. Mongol leaders like \u00d6gedei Khan built textile workshops in their cities to be able to produce gold fabrics themselves.\nSuburbs.\nSuburbs of the city include: Gulyakandoz, Superfosfatnyy, Bukharishlak, Ulugbek, Ravanak, Kattakishlak, Registan, Zebiniso, Kaftarkhona, Uzbankinty.\nTransport.\nLocal.\nSamarkand has a strong public-transport system. From Soviet times up through today, municipal buses and taxis (GAZ-21, GAZ-24, GAZ-3102, VAZ-2101, VAZ-2106 and VAZ-2107) have operated in Samarkand. Buses, mostly SamAuto and Isuzu buses, are the most common and popular mode of transport in the city. Taxis, which are mostly Chevrolets and Daewoo sedans, are usually yellow in color. Since 2017, there have also been several Samarkandian tram lines, mostly Vario LF.S Czech trams. From the Soviet Era up until 2005, Samarkandians also got around via trolleybus. Finally, Samarkand has the so-called \"Marshrutka,\" which are Daewoo Damas and GAZelle minibuses.\nUntil 1950, the main forms of transport in Samarkand were carriages and \"arabas\" with horses and donkeys. However, the city had a steam tram from 1924 to 1930, and there were more modern trams from 1947 to 1973.\nAir transport.\nIn the north of the city is Samarkand International Airport, which was opened in the 1930s, under the Soviets. As of spring 2019, Samarkand International Airport has flights to Tashkent, Nukus, Moscow, Saint Petersburg, Yekaterinburg, Kazan, Istanbul, and Dushanbe; charter flights to other cities are also available.\nRailway.\nModern Samarkand is an important rail junction of Uzbekistan, and all national east\u2013west railway routes pass through the city. The most important and longest of these is Tashkent\u2013Kungrad. High-speed Tashkent\u2013Samarkand high-speed rail line trains run between Tashkent, Samarkand, and Bukhara. Samarkand also has international railway connections: Saratov\u2013Samarkand, Moscow\u2013Samarkand, and Astana\u2013Samarkand.\nBetween 1879 and 1891, the Russian Empire built the Trans-Caspian Railway to facilitate its expansion into Central Asia. The railway originated in Krasnovodsk (now Turkmenbashi) on the Caspian Sea coast. Its terminus was originally Samarkand, whose station first opened in May 1888. However, a decade later, the railway was extended eastward to Tashkent and Andijan, and its name was changed to Central Asian Railways. Nonetheless, Samarkand remained one of the largest and most important stations of the Uzbek SSR and Soviet Central Asia.\nInternational relations.\nSamarkand is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSamarkand has friendly relations with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nIn literature.\nThe frame story of \"One Thousand and One Nights\" involves a Sasanian king who assigns his brother, Shah Zaman, to rule over Samarkand.\nThe poet James Elroy Flecker published the poem \"The Golden Journey to Samarkand\" in 1913. It was included in his play, \"Hassan\". \"Hassan\" (\"The Story of Hassan of Bagdad and How He Came to Make the Golden Journey to Samarkand\") is a five-act drama in prose with verse passages. It tells the story of Hassan, a young man from Baghdad who embarks on a journey to Samarkand. Along the way, he encounters various challenges and obstacles, including bandits, treacherous terrain, and political turmoil.\nIn 2002, Nobel Laureate Wole Soyinka titled his collection of poetry \"Samarkand and Other Markets I Have Known.\"\nEnglish author Jonathan Stroud published his book \"The Amulet of Samarkand\" in 2003. The book contains no allusions to Samarkand other than namesake.\nThe city of Samarkand is famous for being the subject of an Uzbek tale called \"The Rendezvous at Samarkand.\"\nIt was recounted by a 12th-century Persian storyteller and mystic, Farid Al-Din Attar.\nIn a legendary Baghdad, ruled by a powerful caliph, lived a young, healthy vizier. He seemed to have his whole life ahead of him. One day, he went to the city market, incognito, as he often did. Amidst the stalls of the spice merchants, he encountered a skeletal woman, who turned around as he passed and reached out to him. The vizier, a wise man, immediately recognized death. Terrified by what he saw, he begged his caliph to let him flee Baghdad, explaining that death was here and wanted to take him. His only hope was to immediately saddle his fastest horse and gallop off far from the city. The caliph therefore granted him permission to leave and asked him where he would be going. The vizier replies that to escape death, he is going to Samarkand, the desert city, on the edge of the kingdom, on the borders of Asia and the Middle East, thinking he will be safe there, far from the death that lurks in Baghdad! However, the caliph also decides to go to the market to check for the presence of death. He recognizes her very quickly and addresses her without fear, asking her the meaning of the gesture she made towards the vizier. \"It was only a gesture of surprise...\" replies death \"Because I saw him in Baghdad while I must take him tonight in Samarkand...\"\nThis tale illustrates the inevitability of human destiny in the face of death. \nThis tale also inspired Agatha Christie to title her novel \"Appointment with Death.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51492", "revid": "43252809", "url": "https://en.wikipedia.org/wiki?curid=51492", "title": "German Worker's Party", "text": ""}
{"id": "51493", "revid": "20318", "url": "https://en.wikipedia.org/wiki?curid=51493", "title": "Santiago (disambiguation)", "text": "Santiago is the capital city of Chile.\nSantiago may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "51497", "revid": "21604566", "url": "https://en.wikipedia.org/wiki?curid=51497", "title": "Battle of Verdun", "text": "Battle on the Western Front during the First World War\nThe Battle of Verdun ( ; ) was fought from 21 February to 18 December 1916 on the Western Front in France. The battle was the longest of the First World War and took place on the hills north of Verdun. The German 5th Army attacked the defences of the Fortified Region of Verdun (RFV, ) and those of the French Second Army on the right (east) bank of the Meuse. Using the experience of the Second Battle of Champagne in 1915, the Germans planned to capture the Meuse Heights, an excellent defensive position, with good observation for artillery-fire on Verdun. The Germans hoped that the French would commit their strategic reserve to recapture the position and suffer catastrophic losses at little cost to the German infantry.\nPoor weather delayed the beginning of the attack until 21 February but the Germans captured Fort Douaumont in the first three days. The advance then slowed for several days, despite inflicting many French casualties. By 6 March, &lt;templatestyles src=\"Fraction/styles.css\" /&gt;20+1\u20442 French divisions were in the RFV and a more extensive defence in depth had been organised. Philippe P\u00e9tain ordered there to be no retreat and that German attacks were to be counter-attacked, despite this exposing French infantry to the German artillery. By 29 March, French guns on the west bank had begun a constant bombardment of Germans on the east bank, causing many infantry casualties. The German offensive was extended to the west bank of the Meuse to gain observation and eliminate the French artillery firing over the river but the attacks failed to reach their objectives.\nIn early May, the Germans changed tactics again and made local attacks and counter-attacks; the French recaptured part of Fort Douaumont but the Germans ejected them and took many prisoners. The Germans tried alternating their attacks on either side of the Meuse and in June captured Fort Vaux. The Germans advanced towards the last geographical objectives of the original plan, at Fleury-devant-Douaumont and Fort Souville, driving a salient into the French defences. Fleury was captured and the Germans came within of the Verdun citadel but in July the offensive was limited to provide troops, artillery and ammunition for the Battle of the Somme, leading to a similar transfer of the French Tenth Army to the Somme front. From 23 June to 17 August, Fleury changed hands sixteen times and a German attack on Fort Souville failed. The offensive was reduced further but to keep French troops away from the Somme, ruses were used to disguise the change.\nIn September and December, French counter-offensives recaptured much ground on the east bank and recovered Fort Douaumont and Fort Vaux. The battle lasted for 302 days, one of the longest and costliest in human history. In 2000, Hannes Heer and Klaus Naumann calculated that the French suffered 377,231 casualties and the Germans 337,000, a total of 714,231 and an average of 70,000 a month. In 2014, William Philpott wrote of 714,000 casualties suffered by both sides during the Battle of Verdun in 1916 and that about 1,250,000 casualties were suffered in the vicinity of Verdun in the war. In France, the battle came to symbolise the determination of the French Army and the destructiveness of the war.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nBackground.\nStrategic developments.\nAfter the German invasion of France had been halted at the First Battle of the Marne in September 1914, the war of movement ended at the Battle of the Yser and the First Battle of Ypres. The Germans built field fortifications to hold the ground captured in 1914 and the French began siege warfare to break through the German defences and recover the lost territory. In late 1914 and in 1915, offensives on the Western Front had failed to gain much ground and been extremely costly in casualties. According to his memoirs written after the war, the Chief of the German General Staff, Erich von Falkenhayn, believed that although victory might no longer be achieved by a decisive battle, the French army could still be defeated if it suffered a sufficient number of casualties. Falkenhayn offered five corps from the strategic reserve for an offensive at Verdun at the beginning of February 1916 but only for an attack on the east bank of the Meuse. Falkenhayn considered it unlikely the French would be complacent about Verdun; he thought that they might send all their reserves there and begin a counter-offensive elsewhere or fight to hold Verdun while the British launched a relief offensive. After the war, Kaiser Wilhelm II and Gerhard Tappen, the Operations Officer at \"Oberste Heeresleitung\" (OHL, General Headquarters), wrote that Falkenhayn believed the last possibility was most likely.\nBy seizing or threatening to capture Verdun, the Germans anticipated that the French would send all their reserves, which would then have to attack secure German defensive positions supported by a powerful artillery reserve. In the Gorlice\u2013Tarn\u00f3w Offensive (1 May to 19 September 1915), the German and Austro-Hungarian Armies attacked Russian defences frontally, after pulverising them with large amounts of heavy artillery. During the Second Battle of Champagne ( [autumn battle]) 25 September to 6 November 1915, the French suffered \"extraordinary casualties\" from the German heavy artillery, which Falkenhayn considered offered a way out of the dilemma of material inferiority and the growing strength of the Allies. In the north, a British relief offensive would wear down British reserves, to no decisive effect but create the conditions for a German counter-offensive near Arras.\nHints about Falkenhayn's thinking were picked up by Dutch military intelligence and passed on to the British in December. The German strategy was to create a favourable operational situation without a mass attack, which had been costly and ineffective when tried by the Franco-British, Falkenhayn intended to rely on the power of heavy artillery to inflict mass casualties. A limited offensive at Verdun would lead to the destruction of the French strategic reserve in fruitless counter-attacks and the defeat of British reserves during a hopeless relief offensive, leading to the French accepting a separate peace. If the French refused to negotiate, the second phase of the strategy would follow, in which the German armies would attack terminally weakened Franco-British armies, mop up the remains of the French armies and expel the British from Europe. To fulfil this strategy, Falkenhayn needed to hold back enough of the strategic reserve to defeat the Anglo-French relief offensives and then conduct a counter-offensive, which limited the number of divisions which could be sent to the 5th Army at Verdun for (Operation Judgement).\nThe Fortified Region of Verdun (RFV) lay in a salient formed during the German invasion of 1914. General Joseph Joffre, the Commander-in-Chief of the French Army, had concluded from the swift capture of the Belgian fortresses at the Battle of Li\u00e8ge and at the Siege of Namur in 1914 that fortifications had been made obsolete by German super-heavy siege artillery. In a directive of the General Staff of 5 August 1915, the RFV was to be stripped of 54 artillery batteries and 128,000 rounds of ammunition. Plans to demolish forts Douaumont and Vaux to deny them to the Germans were made and of explosives had been placed in Douaumont by the time of the German offensive on 21 February. The 18 large forts and other batteries around Verdun were left with fewer than 300 guns and a small reserve of ammunition, while their garrisons had been reduced to small maintenance crews. The railway line from the south into Verdun had been cut during the Battle of Flirey in 1914, with the loss of Saint-Mihiel; the line west from Verdun to Paris was cut at Aubr\u00e9ville in mid-July 1915 by the German 3rd Army, which had attacked southwards through the Argonne Forest since the new year.\nR\u00e9gion Fortifi\u00e9e de Verdun.\nFor centuries, Verdun, on the Meuse river, had played an important role in the defence of the French hinterland. Attila the Hun failed to seize the town in the fifth century and when the empire of Charlemagne was divided under the Treaty of Verdun (843), the town became part of the Holy Roman Empire; the Peace of Westphalia of 1648 awarded Verdun to France. At the heart of the city was a citadel built by Vauban in the 17th century. A double ring of 28 forts and smaller works () had been built around Verdun on commanding ground, at least above the river valley, from the citadel. A programme had been devised by S\u00e9r\u00e9 de Rivi\u00e8res in the 1870s to build two lines of fortresses from Belfort to \u00c9pinal and from Verdun to Toul as defensive screens and to enclose towns intended to be the bases for counter-attacks.\nMany of the Verdun forts had been modernised and made more resistant to artillery, with a reconstruction programme begun at Douaumont in the 1880s. A sand cushion and thick, steel-reinforced concrete tops up to thick, buried under of earth, were added. The forts and were sited to overlook each other for mutual support and the outer ring had a circumference of . The outer forts had 79 guns in shellproof turrets and more than 200 light guns and machine-guns to protect the ditches around the forts. Six forts had 155 mm guns in retractable turrets and fourteen had retractable twin 75 mm turrets.\nIn 1903, Douaumont was equipped with a new concrete bunker (), containing two 75 mm field guns to cover the south-western approach and the defensive works along the ridge to . More guns were added from 1903 to 1913 in four retractable steel turrets. The guns could rotate for all-round defence and two smaller versions, at the north-eastern and north-western corners of the fort, housed twin Hotchkiss machine-guns. On the east side of the fort, an armoured turret with a 155 mm short-barrelled gun faced north and north-east and another housed twin 75 mm guns at the north end, to cover the intervals between the neighbouring forts.\nThe fort at Douaumont formed part of a complex of the village, fort, six , five shelters, six concrete batteries, an underground infantry shelter, two ammunition depots and several concrete infantry trenches. The Verdun forts had a network of concrete infantry shelters, armoured observation posts, batteries, concrete trenches, command posts and underground shelters between the forts. The artillery comprised c.\u20091,000 guns, with 250 in reserve; the forts and were linked by telephone and telegraph, a narrow-gauge railway system and a road network; on mobilisation, the RFV had a garrison of 66,000 men and rations for six months.\nPrelude.\nGerman preparations.\nVerdun had been isolated on three sides since 1914 and the mainline Paris\u2013St Menehould\u2013Les Islettes\u2013Clermont-en-Argonne\u2013Aubr\u00e9ville\u2013Verdun railway in the Forest of Argonne was closed in mid-July 1915, by the right flank divisions of the 5th Army (Generalmajor Crown Prince Wilhelm) when it reached the \u2013Hill 285 ridge, after continuous local attacks, rendering the railway unusable. Only a light railway remained to the French to carry bulk supplies; German-controlled mainline railways lay only to the north of the front line. A corps was moved to the 5th Army to provide labour for the preparation of the offensive. Areas were emptied of French civilians and buildings requisitioned. Thousands of kilometres of telephone cable were laid, a huge amount of ammunition and rations was dumped under cover and hundreds of guns were emplaced and camouflaged. Ten new rail lines with twenty stations were built and vast underground shelters () deep were dug, each to accommodate up to 1,200 infantry.\nThe III Corps, VII Reserve Corps and XVIII Corps were transferred to the 5th Army, each corps being reinforced by 2,400 experienced troops and 2,000 trained recruits. V Corps was placed behind the front line, ready to advance if necessary when the assault divisions were moving up. XV Corps, with two divisions, was in 5th Army reserve, ready to advance and mop up as soon as the French defence collapsed. Special arrangements were made to maintain a high rate of artillery-fire during the offensive; &lt;templatestyles src=\"Fraction/styles.css\" /&gt;33+1\u20442 munitions trains per day were to deliver ammunition sufficient for 2,000,000 rounds to be fired in the first six days and another 2,000,000 shells in the next twelve. Five repair shops were built close to the front to reduce delays for maintenance and factories in Germany were made ready, rapidly to refurbish artillery needing more extensive repairs. A redeployment plan for the artillery was devised, to move field guns and mobile heavy artillery forward, under the covering fire of mortars and the super-heavy artillery. A total of 1,201 guns were massed on the Verdun front, two thirds of which were heavy- and super-heavy artillery, which was obtained by stripping modern German artillery from the rest of the Western Front and substituting for it older types and captured Russian and Belgian guns. The German artillery could fire into the Verdun salient from three directions yet remain dispersed around the edges.\nGerman plan.\nThe 5th Army divided the attack front into areas, \"A\" occupied by the VII Reserve Corps, \"B\" by the XVIII Corps, \"C\" by the III Corps and \"D\" on the Wo\u00ebvre plain by the XV Corps. The preliminary artillery bombardment was to begin in the morning of 12 February. At 5:00 p.m., the infantry in areas \"A\" to \"C\" would advance in open order, supported by grenade and flame-thrower detachments. Wherever possible, the French advanced trenches were to be occupied and the second position reconnoitred for the artillery to bombard on the second day. Great emphasis was placed on limiting German infantry casualties by sending them to follow up destructive bombardments by the artillery, which was to carry the burden of the offensive in a series of large \"attacks with limited objectives\", to maintain a relentless pressure on the French. The initial objectives were the Meuse Heights, on a line from Froide Terre to Fort Souville and Fort Tavannes, which would provide a secure defensive position from which to repel French counter-attacks. \"Relentless pressure\" was a term added by the 5th Army staff and created ambiguity about the purpose of the offensive. Falkenhayn wanted land to be captured from which artillery could dominate the battlefield and the 5th Army wanted a quick capture of Verdun. The confusion caused by the ambiguity was left to the corps headquarters to sort out.\nControl of the artillery was centralised by an \"Order for the Activities of the Artillery and Mortars\", which stipulated that the corps Generals of Foot Artillery were responsible for local target selection, while co-ordination of flanking fire by neighbouring corps and the fire of certain batteries, was reserved to the 5th Army headquarters. French fortifications were to be engaged by the heaviest howitzers and enfilade fire. The heavy artillery was to maintain long-range bombardment of French supply routes and assembly areas; counter-battery fire was reserved for specialist batteries firing gas shells. Co-operation between the artillery and infantry was stressed, with accuracy of the artillery being given priority over rate of fire. The opening bombardment was to build up slowly and (a rate of fire so rapid that the sound of shell-explosions merged into a rumble) would not begin until the last hour. As the infantry advanced, the artillery would increase the range of the bombardment to destroy the French second position. Artillery observers were to advance with the infantry and communicate with the guns by field telephones, flares and coloured balloons. When the offensive began, the French were to be bombarded continuously, with harassing fire being maintained at night.\nFrench preparations.\nIn 1915, 237 guns and of ammunition in the forts of the RFV had been removed, leaving only the heavy guns in retractable turrets. The conversion of the RFV to a conventional linear defence, with trenches and barbed wire began but proceeded slowly, after resources were sent west from Verdun for the Second Battle of Champagne (25 September to 6 November 1915). In October 1915, building began on trench lines known as the first, second and third positions and in January 1916, an inspection by General No\u00ebl de Castelnau, Chief of Staff at French General Headquarters (GQG), reported that the new defences were satisfactory, except for small deficiencies in three areas. The fortress garrisons had been reduced to small maintenance crews and some of the forts had been readied for demolition. The maintenance garrisons were responsible to the central military bureaucracy in Paris and when the XXX Corps commander, Major-General Paul Chr\u00e9tien, attempted to inspect Fort Douaumont in January 1916, he was refused entry.\nDouaumont was the largest fort in the RFV and by February 1916, the only artillery left in the fort were the 75 mm and 155 mm turret guns and light guns covering the ditch. The fort was used as a barracks by 68 technicians under the command of Warrant Officer Chenot, the . One of the rotating turrets was partially manned and the other was left empty. The Hotchkiss machine-guns were stored in boxes and four 75 mm guns in the casemates had already been removed. The drawbridge had been jammed in the down position by a German shell and had not been repaired. The (wall bunkers) with Hotchkiss revolver-cannons protecting the moats, were unmanned and over of explosives had been placed in the fort to demolish it. Colonel \u00c9mile Driant was stationed at Verdun and criticised Joffre for removing the artillery guns and infantry from fortresses around Verdun. Joffre did not listen but Colonel Driant received the support of the Minister for War Joseph Gallieni. The formidable Verdun defences were a shell and were now threatened by a German offensive; Driant was to be proved correct by events.\nIn late January 1916, French intelligence obtained an accurate assessment of German military capacity and intentions at Verdun but Joffre considered that an attack would be a diversion, because of the lack of an obvious strategic objective. By the time of the German offensive, Joffre expected a bigger attack elsewhere but finally yielded to political pressure and ordered the VII Corps to Verdun on 23 January, to hold the north face of the west bank. XXX Corps held the salient east of the Meuse to the north and north-east and II Corps held the eastern face of the Meuse Heights; Herr had &lt;templatestyles src=\"Fraction/styles.css\" /&gt;8+1\u20442 divisions in the front line, with &lt;templatestyles src=\"Fraction/styles.css\" /&gt;2+1\u20442 divisions in close reserve. (GAC, General De Langle de Cary) had the I and XX corps with two divisions each in reserve, plus most of the 19th Division; Joffre had 25 divisions in the French strategic reserve. French artillery reinforcements had brought the total at Verdun to 388 field guns and 244 heavy guns, against 1,201 German guns, two thirds of which were heavy and super heavy, including and 202 mortars, some being . Eight specialist flame-thrower companies were also sent to the 5th Army.\nCastelnau met De Langle de Cary on 25 February, who doubted the east bank could be held. Castelnau disagreed and ordered General Fr\u00e9d\u00e9ric-Georges Herr the corps commander, to hold the right (east) bank of the Meuse at all costs. Herr sent a division from the west bank and ordered XXX Corps to hold a line from Bras to Douaumont, Vaux and Eix. P\u00e9tain took over command of the defence of the RFV at 11:00 p.m., with Colonel Maurice de Barescut as chief of staff and Colonel Bernard Serrigny as head of operations, only to hear that Fort Douaumont had fallen. P\u00e9tain ordered the remaining Verdun forts to be re-garrisoned.\nFour groups were established, under the command of Generals Adolphe Guillaumat, Balfourier and Denis Duch\u00eane on the right bank and Georges de Bazelaire on the left bank. A \"line of resistance\" was established on the east bank from Souville to Thiaumont, around Fort Douaumont to Fort Vaux, Moulainville and along the ridge of the Wo\u00ebvre. On the west bank, the line ran from Cumi\u00e8res to Mort Homme, C\u00f4te 304 and Avocourt. A \"line of panic\" was planned in secret as a final line of defence north of Verdun, through forts Belleville, St Michel and Moulainville. I Corps and XX Corps arrived from 24 to 26 February, increasing the number of divisions in the RFV to &lt;templatestyles src=\"Fraction/styles.css\" /&gt;14+1\u20442. By 6 March, the arrival of the XIII, XXI, XIV and XXXIII corps had increased the total to &lt;templatestyles src=\"Fraction/styles.css\" /&gt;20+1\u20442 divisions.\nBattle.\nFirst phase, 21 February \u2013 1 March.\n21\u201326 February.\n (Operation Judgement) was due to begin on 12 February but fog, heavy rain and high winds delayed the offensive until 7:15 a.m. on 21 February, when a 10-hour artillery bombardment by 808 guns began. The German artillery fired c.\u20091,000,000 shells along a front about long by wide. The main concentration of fire was on the right (east) bank of the Meuse river. Twenty-six super-heavy, long-range guns, up to 17-inch (420 mm), fired on the forts and the city of Verdun; a rumble that could be heard away.\nThe bombardment was paused at midday as a ruse to prompt French survivors to reveal themselves and German artillery-observation aircraft were able to fly over the battlefield unchallenged. The III Corps, VII Corps and XVIII Corps attacked at 4:00 p.m.; the Germans used flamethrowers and stormtroopers followed closely with rifles slung, using hand grenades to kill the remaining defenders. This tactic had been developed by Captain Willy Rohr and which delivered the attack. French survivors engaged the attackers, yet the Germans suffered only c.\u2009600 casualties.\nBy 22 February, German troops had advanced and captured at the edge of the village of Flabas. Two French battalions had held the (wood) for two days but were forced back to Samogneux, Beaumont-en-Auge and Ornes. Driant was killed, fighting with the 56th and 59th and only 118 of the Chasseurs managed to escape. Poor communications meant that only then did the French High Command realise the seriousness of the attack. The Germans managed to take the village of Haumont but French forces repulsed a German attack on the village of . On 23 February, a French counter-attack at was defeated.\nFighting for continued until the Germans outflanked the French defenders from . The German attackers suffered many casualties during their attack on and the French held on to Samogneux. German attacks continued on 24 February and the French XXX Corps was forced out of the second line of defence; XX Corps (General Maurice Balfourier) arrived at the last minute and was rushed forward. That evening Castelnau advised Joffre that the Second Army, under General P\u00e9tain, should be sent to the RFV. The Germans had captured Beaumont-en-Verdunois, and and were moving up , which led to Fort Douaumont.\nAt 3:00 p.m. on 25 February, infantry of Brandenburg Regiment 24 advanced with the II and III battalions side-by-side, each formed into two waves composed of two companies each. A delay in the arrival of orders to the regiments on the flanks, led to the III Battalion advancing without support on that flank. The Germans rushed French positions in the woods and on C\u00f4te 347, with the support of machine-gun fire from the edge of . The German infantry took many prisoners as the French on C\u00f4te 347 were outflanked and withdrew to Douaumont village. The German infantry had reached their objectives in under twenty minutes and pursued the French, until fired on by a machine-gun in Douaumont church. Some German troops took cover in woods and a ravine which led to the fort, when German artillery began to bombard the area, the gunners having refused to believe claims sent by field telephone that the German infantry were within a few hundred metres of the fort. Several German parties were forced to advance to find cover from the German shelling and two parties independently made for the fort. The Germans did not know that the French garrison was made up of only a small maintenance crew led by a warrant officer, since most of the Verdun forts had been partly disarmed, after the demolition of Belgian forts in 1914, by the German super-heavy Krupp 420 mm mortars.\nThe German party of c.\u2009100 soldiers tried to signal to the artillery with flares but they were not seen due to the twilight and falling snow. Some of the party began to cut through the wire around the fort, while French machine-gun fire from Douaumont village ceased. The French had seen the German flares and took the Germans on the fort to be Zouaves retreating from C\u00f4te 378. The Germans were able to reach the north-east end of the fort before the French resumed firing. The German party found a way through the railings on top of the ditch and climbed down without being fired on, since the machine-gun bunkers () at each corner of the ditch had been left unmanned. The German parties continued and found a way inside the fort through one of the unoccupied ditch bunkers and then reached the central .\nAfter quietly moving inside, the Germans heard voices and persuaded a French prisoner, captured in an observation post, to lead them to the lower floor, where they found Warrant Officer Chenot and about 25 French troops, most of the skeleton garrison of the fort, and took them prisoner. On 26 February, the Germans had advanced on a front; French losses were 24,000 men and German losses were c.\u200925,000 men. A French counter-attack on Fort Douaumont failed and P\u00e9tain ordered that no more attempts were to be made; existing lines were to be consolidated and other forts were to be occupied, rearmed and supplied to withstand a siege if surrounded.\n27\u201329 February.\nThe German advance gained little ground on 27 February, after a thaw turned the ground into a swamp and the arrival of French reinforcements increased the effectiveness of the defence. Some German artillery became unserviceable and other batteries became stranded in the mud. German infantry began to suffer from exhaustion and unexpectedly high losses, 500 casualties being suffered in the fighting around Douaumont village. On 29 February, the German advance was contained at Douaumont by a heavy snowfall and the defence of French 33rd Infantry Regiment. Delays gave the French time to bring up 90,000 men and of ammunition from the railhead at Bar-le-Duc to Verdun. The swift German advance had gone beyond the range of artillery covering fire and the muddy conditions made it very difficult to move the artillery forward as planned. The German advance southwards brought it into range of French artillery west of the Meuse, whose fire caused more German infantry casualties than in the earlier fighting, when French infantry on the east bank had fewer guns in support.\nSecond phase, 6 March \u2013 15 April.\n6\u201311 March.\nBefore the offensive, Falkenhayn had expected that French artillery on the west bank would be suppressed by counter-battery fire but this had failed. The Germans set up a specialist artillery force to counter French artillery fire from the west bank but this also failed to reduce German infantry casualties. The 5th Army asked for more troops in late February but Falkenhayn refused, due to the rapid advance already achieved on the east bank and because he needed the rest of the OHL reserve for an offensive elsewhere, once the attack at Verdun had attracted and consumed French reserves. The pause in the German advance on 27 February led Falkenhayn to have second thoughts to decide between terminating the offensive or reinforcing it. On 29 February, Knobelsdorf, the 5th Army Chief of Staff, prised two divisions from the OHL reserve, with the assurance that once the heights on the west bank had been occupied, the offensive on the east bank could be completed. The VI Reserve Corps was reinforced with the X Reserve Corps, to capture a line from the south of Avocourt to C\u00f4te 304 north of Esnes, Le Mort Homme, Bois des Cumi\u00e8res and C\u00f4te 205, from which the French artillery on the west bank could be destroyed.\nThe artillery of the two-corps assault group on the west bank was reinforced by 25 heavy artillery batteries, artillery command was centralised under one officer and arrangements were made for the artillery on the east bank to fire in support. The attack was planned by General Heinrich von Gossler in two parts, on Mort-Homme and C\u00f4te 265 on 6 March, followed by attacks on Avocourt and C\u00f4te 304 on 9 March. The German bombardment reduced the top of C\u00f4te 304 from a height of to ; Mort-Homme sheltered batteries of French field guns, which hindered German progress towards Verdun on the right bank; the hills also provided commanding views of the left bank. After storming the and then losing it to a French counter-attack, the Germans launched another assault on Mort-Homme on 9 March, from the direction of B\u00e9thincourt to the north-west. was captured again at great cost in casualties, before the Germans took parts of Mort-Homme, C\u00f4te 304, Cumi\u00e8res and Chattancourt on 14 March.\n11 March \u2013 9 April.\nAfter a week, the German attack had reached the first-day objectives, to find that French guns behind C\u00f4te de Marre and Bois Bourrus were still operational and inflicting many casualties among the Germans on the east bank. German artillery moved to C\u00f4te 265, was subjected to systematic artillery fire by the French, which left the Germans needing to implement the second part of the west bank offensive, to protect the gains of the first phase. German attacks changed from large operations on broad fronts, to narrow-front attacks with limited objectives.\nOn 14 March a German attack captured C\u00f4te 265 at the west end of Mort-Homme but the French 75th Infantry Brigade managed to hold C\u00f4te 295 at the east end. On 20 March, after a bombardment by 13,000 trench mortar rounds, the 11th Bavarian and 11th Reserve divisions attacked and and reached their initial objectives easily. Gossler ordered a pause in the attack, to consolidate the captured ground and to prepare another big bombardment for the next day. On 22 March, two divisions attacked \"Termite Hill\" near C\u00f4te 304 but were met by a mass of artillery fire, which also fell on assembly points and the German lines of communication, ending the German advance.\nThe limited German success had been costly and French artillery inflicted more casualties as the German infantry tried to dig in. By 30 March, Gossler had captured at a cost of 20,000 casualties and the Germans were still short of C\u00f4te 304. On 30 March, the XXII Reserve Corps arrived as reinforcements and General Max von Gallwitz took command of a new Attack Group West (). Malancourt village was captured on 31 March, Haucourt fell on 5 April and B\u00e9thincourt on 8 April. On the east bank, German attacks near Vaux reached and the Vaux\u2013Fleury railway but were then driven back by the French 5th Division. An attack was made on a wider front along both banks by the Germans at noon on 9 April, with five divisions on the left bank but this was repulsed except at Mort-Homme, where the French 42nd Division was forced back from the north-east face. On the right bank an attack on failed.\nIn March the German attacks had no advantage of surprise and faced a determined and well-supplied adversary in superior defensive positions. German artillery could still devastate the French positions but could not prevent French artillery fire from inflicting many casualties on German infantry and isolating them from their supplies. Massed artillery fire could enable German infantry to make small advances but massed French artillery fire could do the same for French infantry when they counter-attacked, which often repulsed the German infantry and subjected them to constant losses, even when captured ground was held. The German effort on the west bank also showed that capturing a vital point was not sufficient, because it would be found to be overlooked by another terrain feature, which had to be captured to ensure the defence of the original point, which made it impossible for the Germans to terminate their attacks, unless they were willing to retire to the original front line of February 1916.\nBy the end of March the offensive had cost the Germans 81,607 casualties and Falkenhayn began to think of ending the offensive, lest it become another costly and indecisive engagement similar to the First Battle of Ypres in late 1914. The 5th Army staff requested more reinforcements from Falkenhayn on 31 March with an optimistic report claiming that the French were close to exhaustion and incapable of a big offensive. The 5th Army command wanted to continue the east bank offensive until a line from Ouvrage de Thiaumont, to Fleury, Fort Souville and Fort de Tavannes had been reached, while on the west bank the French would be destroyed by their own counter-attacks. On 4 April, Falkenhayn replied that the French had retained a considerable reserve and that German resources were limited and not sufficient to replace continuously men and munitions. If the resumed offensive on the east bank failed to reach the Meuse Heights, Falkenhayn was willing to accept that the offensive had failed and end it.\nThird phase, 16 April \u2013 1 July.\nApril.\nThe failure of German attacks in early April by , led Knobelsdorf to take soundings from the 5th Army corps commanders, who unanimously wanted to continue. The German infantry were exposed to continuous artillery fire from the flanks and rear; communications from the rear and reserve positions were equally vulnerable, which caused a constant drain of casualties. Defensive positions were difficult to build, because existing positions were on ground which had been swept clear by German bombardments early in the offensive, leaving German infantry with very little cover. General Berthold von Deimling, commander of XV Corps, also wrote that French heavy artillery and gas bombardments were undermining the morale of the German infantry, which made it necessary to keep going to reach safer defensive positions. Knobelsdorf reported these findings to Falkenhayn on 20 April, adding that if the Germans did not go forward, they must go back to the start line of 21 February.\nKnobelsdorf rejected the policy of limited piecemeal attacks tried by Mudra as commander of and advocated a return to wide-front attacks with unlimited objectives, swiftly to reach the line from Ouvrage de Thiaumont to Fleury, Fort Souville and Fort de Tavannes. Falkenhayn was persuaded to agree to the change and by the end of April, 21 divisions, most of the OHL reserve, had been sent to Verdun and troops had also been transferred from the Eastern Front. The resort to large, unlimited attacks was costly for both sides but the German advance proceeded only slowly. Rather than causing devastating French casualties by heavy artillery with the infantry in secure defensive positions, which the French were compelled to attack, the Germans inflicted casualties by attacks which provoked French counter-attacks and assumed that the process inflicted five French casualties for two German losses.\nIn mid-March, Falkenhayn had reminded the 5th Army to use tactics intended to conserve infantry, after the corps commanders had been allowed discretion to choose between the cautious, \"step by step\" tactics desired by Falkenhayn and maximum efforts, intended to obtain quick results. On the third day of the offensive, the 6th Division of the III Corps (General Ewald von Lochow), had ordered that Herbebois be taken \"regardless of loss\" and the 5th Division had attacked Wavrille to the accompaniment of its band. Falkenhayn urged the 5th Army to use (storm units) composed of two infantry squads and one of engineers, armed with automatic weapons, hand grenades, trench mortars and flame-throwers, to advance in front of the main infantry body. The would conceal their advance by shrewd use of terrain and capture any blockhouses which remained after the artillery preparation. Strongpoints which could not be taken were to be by-passed and captured by follow-up troops. Falkenhayn ordered that the command of field and heavy artillery units was to be combined, with a commander at each corps headquarters. Common observers and communication systems would ensure that batteries in different places could bring targets under converging fire, which would be allotted systematically to support divisions.\nIn mid-April, Falkenhayn ordered that infantry should advance close to the barrage, to exploit the neutralising effect of the shellfire on surviving defenders, because fresh troops at Verdun had not been trained in these methods. Knobelsdorf persisted with attempts to maintain momentum, which was incompatible with casualty conservation by limited attacks, with pauses to consolidate and prepare. Mudra and other commanders who disagreed were sacked. Falkenhayn also intervened to change German defensive tactics, advocating a dispersed defence with the second line to be held as a main line of resistance and jumping-off point for counter-attacks. Machine-guns were to be set up with overlapping fields of fire and infantry given specific areas to defend. When French infantry attacked, they were to be isolated by (barrage-fire) on their former front line, to increase French infantry casualties. The changes desired by Falkenhayn had little effect, because the main cause of German casualties was artillery fire, just as it was for the French.\n4\u201322 May.\nFrom 10 May German operations were limited to local attacks, either in reply to French counter-attacks on 11 April between Douaumont and Vaux and on 17 April between the Meuse and Douaumont, or local attempts to take points of tactical value. At the beginning of May, General P\u00e9tain was promoted to the command of (GAC) and General Robert Nivelle took over the Second Army at Verdun. From 4 to 24 May, German attacks were made on the west bank around Mort-Homme and on 4 May, the north slope of C\u00f4te 304 was captured; French counter-attacks from 5 to 6 May were repulsed. The French defenders on the crest of C\u00f4te 304 were forced back on 7 May but German infantry were unable to occupy the ridge, because of the intensity of French artillery fire. Cumieres and Caurettes fell on 24 May as a French counter-attack began at Fort Douaumont.\n22\u201324 May.\nIn May, General Nivelle, who had taken over the Second Army, ordered General Charles Mangin, commander of the 5th Division to plan a counter-attack on Fort Douaumont. The initial plan was for an attack on a front but several minor German attacks captured the and ravines on the south-east and west sides of the fort. A further attack took the ridge south of the , which gave the Germans better routes for counter-attacks and observation over the French lines to the south and south-west. Mangin proposed a preliminary attack to retake the area of the ravines, to obstruct the routes by which a German counter-attack on the fort could be made. More divisions were necessary but these were refused to preserve the troops needed for the forthcoming offensive on the Somme; Mangin was limited to one division for the attack with one in reserve. Nivelle reduced the attack to an assault on Morch\u00e9e Trench, Bonnet-d'Ev\u00e8que, Fontaine Trench, Fort Douaumont, a machine-gun turret and Hongrois Trench, which would require an advance of on a front.\nIII Corps was to command the attack by the 5th Division and the 71st Brigade, with support from three balloon companies for artillery observation and a fighter group. The main effort was to be conducted by two battalions of the 129th Infantry Regiment, each with a pioneer company and a machine-gun company attached. The 2nd Battalion was to attack from the south and the 1st Battalion was to move along the west side of the fort to the north end, taking Fontaine Trench and linking with the 6th Company. Two battalions of the 74th Infantry Regiment were to advance along the east and south-east sides of the fort and take a machine-gun turret on a ridge to the east. Flank support was arranged with neighbouring regiments and diversions were planned near Fort Vaux and the . Preparations for the attack included the digging of of trenches and the building of large numbers of depots and stores but little progress was made due to a shortage of pioneers. French troops captured on 13 May, disclosed the plan to the Germans, who responded by subjecting the area to more artillery harassing fire, which also slowed French preparations.\nThe French preliminary bombardment by four 370 mm mortars and 300 heavy guns, began on 17 May and by 21 May, the French artillery commander claimed that the fort had been severely damaged. During the bombardment the German garrison in the fort experienced great strain, as French heavy shells smashed holes in the walls and concrete dust, exhaust fumes from an electricity generator and gas from disinterred corpses polluted the air. Water ran short but until 20 May, the fort remained operational, reports being passed back and reinforcements moving forward until the afternoon, when the Bourges Casemate was isolated and the wireless station in the north-western machine-gun turret burnt down.\nConditions for the German infantry in the vicinity were far worse and by 18 May, the French destructive bombardment had obliterated many defensive positions, the survivors sheltering in shell-holes and dips of the ground. Communication with the rear was severed and food and water ran out by the time of the French attack on 22 May. The troops of Infantry Regiment 52 in front of Fort Douaumont had been reduced to 37 men near Thiaumont Farm and German counter-barrages inflicted similar losses on French troops. On 22 May, French Nieuport fighters attacked eight observation balloons and shot down six for the loss of one Nieuport 16; other French aircraft attacked the 5th Army headquarters at Stenay. German artillery fire increased and twenty minutes before zero hour, a German bombardment began, which reduced the 129th Infantry Regiment companies to about 45 men each.\nThe assault began at 11:50 a.m. on 22 May on a front. On the left flank the 36th Infantry Regiment attack quickly captured Morch\u00e9e Trench and Bonnet-d'Ev\u00e8que but suffered many casualties and the regiment could advance no further. The flank guard on the right was pinned down, except for one company which disappeared and in , a battalion of the 74th Infantry Regiment was unable to leave its trenches; the other battalion managed to reach its objectives at an ammunition depot, shelter \"DV1\" at the edge of and the machine-gun turret east of the fort, where the battalion found its flanks unsupported.\nDespite German small-arms fire, the 129th Infantry Regiment reached the fort in a few minutes and managed to get in through the west and south sides. By nightfall, about half of the fort had been recaptured and next day, the 34th Division was sent to reinforce the French troops in the fort. The attempt to reinforce the fort failed and German reserves managed to cut off the French troops inside and force them to surrender, 1,000 French prisoners being taken. After three days, the French had suffered 5,640 casualties from the 12,000 men in the attack and the Germans suffered 4,500 casualties in Infantry Regiment 52, Grenadier Regiment 12 and Leib-Grenadier Regiment 8 of the 5th Division.\n30 May \u2013 7 June.\nLater in May 1916, the German attacks shifted from the left bank at Mort-Homme and C\u00f4te 304 to the right bank, south of Fort Douaumont. A German attack to reach Fleury Ridge, the last French defensive line began. The attack was intended to capture , Fleury, Fort Souville and Fort Vaux at the north-east extremity of the French line, which had been bombarded by c.\u20098,000 shells a day since the beginning of the offensive. After a final assault on 1 June by about 10,000 German troops, the top of Fort Vaux was occupied on 2 June. Fighting went on underground until the garrison ran out of water, the 574 survivors surrendering on 7 June. When news of the loss of Fort Vaux reached Verdun, the Line of Panic was occupied and trenches were dug on the edge of the city. On the left bank, the German advanced from the line C\u00f4te 304, Mort-Homme and Cumi\u00e8res and threatened the French hold on Chattancourt and Avocourt. Heavy rains slowed the German advance towards Fort Souville, where both sides attacked and counter-attacked for the next two months. The 5th Army suffered 2,742 casualties in the vicinity of Fort Vaux from 1 to 10 June, 381 men being killed, 2,170 wounded and 191 missing; French counter-attacks on 8 and 9 June were costly failures.\n22\u201325 June.\nOn 22 June, German artillery fired over 116,000 Diphosgene (Green Cross) gas shells at French artillery positions, which caused over 1,600 casualties and silenced many of the French guns. Next day at 5:00 a.m., the Germans attacked on a front and drove a salient into the French defences. The advance was unopposed until 9:00 a.m., when some French troops were able to fight a rearguard action. The Ouvrage (shelter) de Thiaumont and the Ouvrage de Froidterre at the south end of the plateau were captured and the villages of Fleury and Chapelle Sainte-Fine were overrun. The attack came close to Fort Souville (which had been hit by c.\u200938,000 shells since April) bringing the Germans within of the Verdun citadel.\nOn 23 June 1916, Nivelle ordered,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Vous ne les laisserez pas passer, mes camarades (You will not let them pass, my comrades).\nNivelle had been concerned about declining French morale at Verdun; after his promotion to lead the Second Army in June 1916, , manifestations of indiscipline, occurred in five front line regiments. reappeared in the French army mutinies that followed the Nivelle Offensive (April\u2013May 1917).\nChapelle Sainte-Fine was quickly recaptured by the French and the German advance was halted. The supply of water to the German infantry broke down, the salient was vulnerable to fire from three sides and the attack could not continue without more Diphosgene ammunition. Chapelle Sainte-Fine became the furthest point reached by the Germans during the Verdun offensive. On 24 June the preliminary Anglo-French bombardment began on the Somme. Fleury changed hands sixteen times from 23 June to 17 August and four French divisions were diverted to Verdun from the Somme. The French artillery recovered sufficiently on 24 June to cut off the German front line from the rear. By 25 June, both sides were exhausted and Knobelsdorf suspended the attack.\nFourth phase 1 July \u2013 17 December.\nBy the end of May, French casualties at Verdun had risen to c.\u2009185,000 and in June, German losses had reached c.\u2009200,000 men. The Brusilov Offensive (4 June \u2013 20 September 1916) had begun and the opening of the Battle of the Somme (1 July \u2013 18 November 1916), forced the Germans to transfer some of their artillery from Verdun, which was the first strategic success of the Anglo-French offensive.\n9\u201315 July.\nFort Souville dominated a crest south-east of Fleury and was one of the original objectives of the February offensive. The capture of the fort would give the Germans control of the heights overlooking Verdun and allow the infantry to dig in on commanding ground. A German preparatory bombardment began on 9 July, with an attempt to suppress French artillery with over 60,000 gas shells, which had little effect, since the French had been equipped with an improved M2 gas mask. Fort Souville and its approaches were bombarded with more than 300,000 shells, including about 500 shells on the fort.\nAn attack by three German divisions began on 11 July, but German infantry bunched on the path leading to Fort Souville and came under bombardment from French artillery. The surviving troops were fired on by sixty French machine-gunners, who had emerged from the fort and taken positions on the superstructure. Thirty soldiers of Infantry Regiment 140 managed to reach the top of the fort on 12 July, from where the Germans could see the roofs of Verdun and the spire of the cathedral. After a small French counter-attack, the survivors retreated to their start lines or surrendered. During the evening of 11 July, Falkenhayn ordered Crown Prince Wilhelm to go onto the defensive and on 15 July, the French conducted a larger counter-attack which gained no ground; for the rest of the month the French made only small attacks.\n1 August \u2013 17 September.\nOn 1 August, a German surprise-attack advanced towards Fort Souville. This prompted French counter-attacks for two weeks, which were only able to retake a small amount of the captured ground. On 18 August, Fleury was recaptured and by September, French counter-attacks had recovered much of the ground lost in July and August. On 29 August Falkenhayn was replaced as Chief of the General Staff by Paul von Hindenburg and First Quartermaster-General Erich Ludendorff. On 3 September, an attack on both flanks at Fleury advanced the French line several hundred metres, against which German counter-attacks from 4 to 5 September failed. The French attacked again on 9, 13 and from 15 to 17 September. Losses were light except at the Tavannes railway tunnel, where 474 French troops died in a fire which began on 4 September.\n20 October \u2013 2 November.\nOn 20 October 1916, the French began the First Offensive Battle of Verdun (), to recapture Fort Douaumont, with an advance of more than . Seven of the 22 divisions at Verdun were replaced by mid-October and French infantry platoons were reorganised to contain sections of riflemen, grenadiers and machine-gunners. In a six-day preliminary bombardment, the French artillery fired 855,264 shells, including more than half a million 75 mm field-gun shells, a hundred thousand 155 mm medium artillery shells and three hundred and seventy-three 370 mm and 400 mm super-heavy shells, from more than 700 guns and howitzers.\nTwo French Saint-Chamond railway guns, to the south-west at Baleycourt, fired super-heavy shells, each weighing . The French had identified about 800 German guns on the right bank capable of supporting the 34th, 54th, 9th and 33rd Reserve divisions, with the 10th and 5th divisions in reserve. At least 20 of the super-heavy shells hit Fort Douaumont, the sixth penetrating to the lowest level and exploding in a pioneer depot, starting a fire next to 7,000 hand-grenades.\nThe 38th Division (General Guyot de Salins), 133rd Division (General Fenelon F.G. Passaga) and 74th Division (General Charles de Lardemelle) attacked at 11:40 a.m. The infantry advanced behind a creeping field-artillery barrage, moving at a rate of in two minutes, beyond which a heavy artillery barrage moved in lifts, as the field artillery barrage came within , to force the German infantry and machine-gunners to stay under cover. The Germans had partly evacuated Douaumont and it was recaptured on 24 October by French marines and colonial infantry; more than 6,000 prisoners and fifteen guns were captured by 25 October but an attempt on Fort Vaux failed.\nThe Haudromont quarries, Ouvrage de Thiaumont and Thiaumont Farm, Douaumont village, the northern end of Caillette Wood, Vaux pond, the eastern fringe of Bois Fumin and the Damloup battery were captured. The heaviest French artillery bombarded Fort Vaux for the next week and on 2 November, the Germans evacuated the fort, after a huge explosion caused by a 220 mm shell. French eavesdroppers overheard a German wireless message announcing the departure and a French infantry company entered the fort unopposed; on 5 November, the French reached the front line of 24 February and offensive operations ceased until December.\n15\u201317 December 1916.\nThe Second Offensive Battle of Verdun () was planned by P\u00e9tain and Nivelle and commanded by Mangin. The 126th Division (General Paul Muteau), 38th Division (General Guyot de Salins), 37th Division (General No\u00ebl Garnier-Duplessix) and the 133rd Division (General F\u00e9nelon Passaga) attacked with four more in reserve and 740 heavy guns in support. The attack began at 10:00 a.m. on 15 December, after a six-day bombardment of 1,169,000 shells, fired from 827 guns. The final French bombardment was directed from artillery-observation aircraft, falling on trenches, dugout entrances and observation posts. Five German divisions supported by 533 guns held the defensive position, which was deep, with &lt;templatestyles src=\"Fraction/styles.css\" /&gt;2\u20443 of the infantry in the battle zone and the remaining &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20443 in reserve back.\nTwo of the German divisions were understrength with only c.\u20093,000 infantry, instead of their normal establishment of c.\u20097,000. The French advance was preceded by a double creeping barrage, with shrapnel-fire from field artillery in front of the infantry and a high-explosive barrage ahead, which moved towards a standing shrapnel bombardment along the German second line, laid to cut off the German retreat and block the advance of reinforcements. The German defence collapsed and 13,500 men of the 21,000 in the five front divisions were lost, most having been trapped while under cover and taken prisoner when the French infantry arrived.\nThe French reached their objectives at Vacherauville and Louvemont which had been lost in February, along with Hardaumont and Louvemont-C\u00f4te-du-Poivre, despite attacking in very bad weather. German reserve battalions did not reach the front until the evening and two divisions, which had been ordered forward the previous evening, were still away at noon. By the night of 16/17 December, the French had consolidated a new line from Bezonvaux to C\u00f4te du Poivre, beyond Douaumont and north of Fort Vaux, before the German reserves and units could counter-attack. The 155 mm turret at Douaumont had been repaired and fired in support of the French attack. The closest German point to Verdun had been pushed back and all the dominating observation points had been recaptured. The French took 11,387 prisoners and 115 guns. Some German officers complained to Mangin about their lack of comfort in captivity and he replied, \"We do regret it, gentlemen, but then we did not expect so many of you\". Lochow, the 5th Army commander and General Hans von Zwehl, commander of XIV Reserve Corps, were sacked on 16 December.\nAftermath.\nAnalysis.\nFalkenhayn wrote in his memoirs that he sent an appreciation of the strategic situation to the Kaiser in December 1915,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The string in France has reached breaking point. A mass breakthrough\u2014which in any case is beyond our means\u2014is unnecessary. Within our reach there are objectives for the retention of which the French General Staff would be compelled to throw in every man they have. If they do so the forces of France will bleed to death.\u2014\u200a\nThe German strategy in 1916 was to inflict mass casualties on the French, a goal achieved against the Russians from 1914 to 1915, to weaken the French Army to the point of collapse. The French had to be drawn into circumstances from which the Army could not escape for reasons of strategy and prestige. The Germans planned to use a large number of heavy and super-heavy guns to inflict a greater number of casualties than French artillery, which relied mostly upon the 75 mm field gun. In 2007, Robert Foley wrote that Falkenhayn intended a battle of attrition from the beginning, contrary to the views of Wolfgang Foerster in 1937, Gerd Krumeich in 1996 and other historians but the loss of documents led to many interpretations of the strategy. In 1916, critics of Falkenhayn claimed that the battle demonstrated that he was indecisive and unfit for command, echoed by Foerster in 1937. In 1994, Holger Afflerbach questioned the authenticity of the \"Christmas Memorandum\"; after studying the evidence that had survived in the (Army Military History Research Institute) files, he concluded that the memorandum had been written after the war but that it was an accurate reflection of Falkenhayn's thinking at the end of 1915.\nKrumeich wrote that the Christmas Memorandum was fabricated to justify a failed strategy and that attrition had been substituted for the capture of Verdun only after the attack failed. Foley wrote that after the failure of the Ypres Offensive of 1914, Falkenhayn had returned to the pre-war strategic thinking of Moltke the Elder and Hans Delbr\u00fcck on (attrition strategy), because the coalition fighting Germany was too powerful to be defeated. Falkenhayn wanted to divide the Allies by forcing at least one of the Entente powers into a negotiated peace. Attrition lay behind the offensive in the east in 1915 but the Russians had refused to accept German peace feelers, despite the huge defeats inflicted on them by the Austro-Germans.\nWith insufficient forces to break through the Western Front and to overcome the reserves behind it, Falkenhayn tried to force the French to attack instead, by threatening a sensitive point close to the front line and chose Verdun as the place. Huge losses were to be inflicted on the French by German artillery on the dominating heights around the city. The 5th Army would begin a big offensive but with the objectives limited to seizing the Meuse Heights on the east bank, on which the German heavy artillery would dominate the battlefield. The French Army would \"bleed itself white\" in hopeless attempts to recapture the heights. The British would be forced to launch a hasty relief offensive and suffer an equally costly defeat. If the French refused to negotiate, a German offensive would mop up the remnants of the Franco\u2013British armies, breaking the Entente \"once and for all\".\nIn a revised instruction to the French Army in January 1916, the General Staff (GQG) wrote that equipment could not be fought by men. Firepower could conserve infantry but attrition prolonged the war and consumed troops that had been preserved in earlier battles. In 1915 and early 1916, German industry quintupled the output of heavy artillery and doubled the production of super-heavy artillery. French production had also recovered since 1914 and by February 1916 the army had 3,500 heavy guns. In May Joffre began to issue each division with two groups of 155 mm guns and each corps with four groups of long-range guns. Both sides at Verdun had the means to fire huge numbers of heavy shells to suppress the opposing defences before risking infantry in the open. At the end of May, the Germans had 1,730 heavy guns at Verdun and the French 548, sufficient to contain the Germans but not enough for a counter-offensive.\nFrench infantry survived bombardment better because their positions were dispersed and tended to be on dominating ground, not always visible to the Germans. As soon as a German attack began, the French replied with machine-gun and rapid field-artillery fire. On 22 April, the Germans suffered 1,000 casualties and in mid-April, the French fired 26,000 field artillery shells against an attack to the south-east of Fort Douaumont. A few days after taking over at Verdun, P\u00e9tain ordered the air commander, Commandant Charles Tricornot de Rose to sweep away German fighter aircraft and to provide artillery observation. German air superiority was reversed by concentrating the French fighters in rather than distributing them piecemeal across the front, unable to concentrate against large German formations. The fighter escadrilles drove away the German s and the two-seater reconnaissance and artillery-observation aircraft that they protected.\nThe fighting at Verdun was less costly to both sides than the war of movement in 1914, when the French suffered c.\u2009850,000 casualties and the Germans c.\u2009670,000 from August to the end of 1914. The 5th Army had a lower rate of loss than armies on the Eastern Front in 1915 and the French had a lower average rate of loss at Verdun than the rate over three weeks during the Second Battle of Champagne (September\u2013October 1915), which were not deliberately fought as battles of attrition. German loss rates increased relative to losses from 1:2.2 in early 1915 to close to 1:1 by the end of the battle, a trend which continued during the Nivelle Offensive in 1917. The penalty of attrition tactics was indecision, because limited-objective attacks under an umbrella of massed heavy artillery fire could succeed but led to battles of unlimited duration. P\u00e9tain used a (rotation) system quickly to relieve French troops at Verdun, which involved most of the French Army in the battle but for shorter periods than the German troops in the 5th Army. The symbolic importance of Verdun proved a rallying point and the French did not collapse. Falkenhayn was forced to conduct the offensive for much longer and commit far more infantry than intended. By the end of April, most of the German strategic reserve was at Verdun, suffering similar casualties to the French army.\nThe Germans believed that they were inflicting losses at a rate of 5:2; German military intelligence thought that by 11 March the French had suffered 100,000 casualties and Falkenhayn was confident that German artillery could easily inflict another 100,000 losses. In May, Falkenhayn estimated that French casualties had increased to 525,000 men against 250,000 German and that the French strategic reserve was down to 300,000 men. Actual French losses were c.\u2009130,000 by 1 May; 42 French divisions had been withdrawn and rested by the system, once infantry casualties reached 50 per cent. Of the 330 infantry battalions of the French metropolitan army, 259 (78 per cent) went to Verdun, against 48 German divisions, 25 per cent of the (western army). Afflerbach wrote that 85 French divisions fought at Verdun and that from February to August, the ratio of German to French losses was 1:1.1, not the third of French losses assumed by Falkenhayn. By 31 August, the 5th Army had suffered 281,000 casualties and the French 315,000.\nIn June 1916, the French had 2,708 guns at Verdun, including 1,138 field guns; from February to December, the French and German armies fired c.\u200910,000,000 shells, weighing . By May, the German offensive had been defeated by French reinforcements, difficulties of terrain and the weather. The 5th Army infantry was stuck in tactically dangerous positions, overlooked by the French on both banks of the Meuse, instead of dug in on the Meuse Heights. French casualties were inflicted by constant infantry attacks which were far more costly in men than destroying counter-attacks with artillery. The stalemate was broken by the Brusilov Offensive and the Anglo-French relief offensive on the Somme, which Falkehayn had expected to begin the collapse of the Anglo-French armies. Falkenhayn had begun to remove divisions from the Western Front in June for the strategic reserve but only twelve divisions could be spared. Four divisions were sent to the Somme, where three defensive positions had been built, based on the experience of the . Before the battle on the Somme began, Falkenhayn thought that German preparations were better than ever and the British offensive would easily be defeated. The 6th Army, further north, had &lt;templatestyles src=\"Fraction/styles.css\" /&gt;17+1\u20442 divisions and plenty of heavy artillery, ready to attack once the British had been defeated.\nThe strength of the Anglo-French attack on the Somme surprised Falkenhayn and his staff, despite the British casualties on 1 July. Artillery losses to \"overwhelming\" Anglo-French counter-battery fire and the German tactic of instant counter-attacks, led to far more German infantry casualties than at the height of the fighting at Verdun, where the 5th Army suffered 25,989 casualties in the first ten days, against 40,187 2nd Army casualties on the Somme. The Russians attacked again, causing more casualties in June and July. Falkenhayn was called on to justify his strategy to the Kaiser on 8 July and again advocated the minimal reinforcement of the east in favour of the \"decisive\" battle in France; the Somme offensive was the \"last throw of the dice\" for the Entente. Falkenhayn had already given up the plan for a counter-offensive by the 6th Army and sent 18 divisions to the 2nd Army and to the Russian front from the reserve and from the 6th Army; only one division remaining uncommitted by the end of August. The 5th Army had been ordered to limit its attacks at Verdun in June but a final effort was made in July to capture Fort Souville. The attack failed and on 12 July Falkenhayn ordered a strict defensive policy, permitting only small local attacks to limit the number of troops the French could transfer to the Somme.\nFalkenhayn had underestimated the French, for whom victory at all costs was the only way to justify the sacrifices already made; the French army never came close to collapsing and causing a premature British relief offensive. The ability of the German army to inflict disproportionate losses had also been overestimated, in part because the 5th Army commanders had tried to capture Verdun and attacked regardless of loss. Even when reconciled to the attrition strategy, they continued with (strategy of annihilation) and the tactics of (manoeuvre warfare). Failure to reach the Meuse Heights left the 5th Army in poor tactical positions and reduced to inflicting casualties by infantry attacks and counter-attacks. The length of the offensive made Verdun a matter of prestige for the Germans as it was for the French and Falkenhayn became dependent on a British relief offensive being destroyed to end the stalemate. When it came, the collapse in Russia and the power of the Anglo-French attack on the Somme reduced the German armies to holding their positions as best they could. On 29 August, Falkenhayn was sacked and replaced by Hindenburg and Ludendorff, who ended the German offensive at Verdun on 2 September.\nCasualties.\nIn 2013, Paul Jankowski wrote that since the beginning of the war, French army units had produced numerical loss states () every five days for the Bureau of Personnel at GQG. The Health Service () at the Ministry of War received daily counts of wounded taken in by hospitals and other services but casualty data was dispersed among regimental depots, GQG, the Registry Office (), which recorded deaths, the , which counted injuries and illnesses and (Family Liaison), which communicated with next of kin. Regimental depots were ordered to keep (position sheets) to record losses continuously and the of GQG began to compare the five-day with the records of hospital admissions. The new system was used to calculate losses back to August 1914, which took several months; the system had become established by February 1916. The were used to calculate casualty figures published in the , the French Official History and other publications.\nThe German armies compiled (loss lists) every ten days, which were published by the in the of 1924\u20131925. German medical units kept detailed records of medical treatment at the front and in hospital and in 1923 the (Central Information Office) published an amended edition of the lists produced during the war, incorporating medical service data not in the . Monthly figures of wounded and ill servicemen that received medical treatment were published in 1934 in the (Medical Report). Using such sources for comparison is difficult because the information recorded losses over time, rather than place. Losses calculated for a battle could be inconsistent, as in the \"Statistics of the Military Effort of the British Empire during the Great War 1914\u20131920\" (1922). In the early 1920s, Louis Marin reported to the Chamber of Deputies but could not give figures per battle, except for some by using numerical reports from the armies, which were unreliable unless reconciled with the system established in 1916.\nSome French data excluded those lightly wounded but some did not. In April 1917, GQG required that the discriminate between lightly wounded, treated locally for 20 to 30 days and severely wounded evacuated to hospitals. Uncertainty over the criteria had not been resolved before the war ended. excluded lightly wounded and the records included them. Churchill revised German statistics by adding 2 per cent for unrecorded wounded in \"The World Crisis\", written in the 1920s and James Edmonds, the British official historian, added 30 per cent. For the Battle of Verdun, the contained incomplete data for the Verdun area, did not define \"wounded\" and the 5th Army field reports exclude them. The Marin Report and covered different periods but included lightly wounded. Churchill used a figure of 428,000 casualties and took a figure of 532,500 casualties from the Marin Report, for March to June and November to December 1916, for all the Western Front.\nThe give French casualties as 348,000 to 378,000 and in 1930, Hermann Wendt recorded French Second Army and German 5th Army casualties of 362,000 and 336,831 respectively from 21 February to 20 December, not taking account of the inclusion or exclusion of lightly wounded. In 2006, McRandle and Quirk used the to increase the by c.\u200911 per cent, which gave 373,882 casualties, compared to the French Official History record to 20 December 1916, of 373,231 French casualties. The , which explicitly excluded lightly wounded, compared German losses at Verdun in 1916, averaging 37.7 casualties per thousand men, with the 9th Army in Poland 1914 which had a casualty average of 48.1 per 1,000, the 11th Army in Galicia 1915 averaging 52.4 per 1,000 men, the 1st Army on the Somme 1916 average of 54.7 per 1,000 and the 2nd Army average for the Somme 1916 of 39.1 per 1,000 men. Jankowski estimated an equivalent figure for the French Second Army of 40.9 men per 1,000 \"including\" lightly wounded. With a c.\u200911 per cent adjustment to the German figure of 37.7 per 1,000 to include lightly wounded, following the views of McRandle and Quirk; the loss rate is similar to the estimate for French casualties.\nIn the second edition of \"The World Crisis\" (1938), Churchill wrote that the figure of 442,000 was for other ranks and the figure of \"probably\" 460,000 casualties included officers. Churchill gave a figure of 278,000 German casualties, 72,000 fatal and expressed dismay that French casualties had exceeded German by about 3:2. Churchill wrote that an eighth needed to be deducted from his figures to account for casualties on other sectors, giving 403,000 French and 244,000 German casualties. In 1980, John Terraine calculated c.\u2009750,000 French and German casualties in 299 days; Dupuy and Dupuy (1993) 542,000 French casualties. In 2000, Hannes Heer and Klaus Naumann calculated 377,231 French and 337,000 German casualties, a monthly average of 70,000. In 2000, Holger Afflerbach used calculations made by Hermann Wendt in 1931 to give German casualties at Verdun from 21 February to 31 August 1916 as 336,000 and French as 365,000 at Verdun from February to December 1916. David Mason wrote in 2000 that there had been 378,000 French and 337,000 German casualties. In 2003, Anthony Clayton quoted 330,000 German casualties, of whom 143,000 were killed or missing; the French suffered 351,000 casualties, 56,000 killed, 100,000 missing or prisoners and 195,000 wounded.\nWriting in 2005, Robert A. Doughty gave French casualties (21 February to 20 December 1916) as 377,231 and casualties of 579,798 at Verdun and the Somme; 16 per cent of the casualties at Verdun were fatal, 56 per cent were wounded and 28 per cent missing, many of whom were eventually presumed dead. Doughty wrote that other historians had followed Winston Churchill (1927) who gave a figure of 442,000 casualties by mistakenly including all French losses on the Western Front. R. G. Grant gave a figure of 355,000 German and 400,000 French casualties in 2005. In 2005, Robert Foley used the Wendt calculations of 1931 to give German casualties at Verdun from 21 February to 31 August 1916 of 281,000, against 315,000 French. (In 2014, William Philpott recorded 377,000 French casualties, of whom 162,000 had been killed; German casualties were 337,000 and noted a recent estimate of casualties at Verdun from 1914 to 1918 of 1,250,000).\nMorale.\nFighting in such a small area devastated the land, resulting in miserable conditions for troops on both sides. Rain and the constant artillery bombardments turned the clayey soil into a wasteland of mud full of debris and human remains; shell craters filled with water and soldiers risked drowning in them. Forests were reduced to tangled piles of wood by artillery fire and eventually obliterated. The effect of the battle on many soldiers was profound and accounts of men breaking down with insanity and shell shock were common. Some French soldiers tried to desert to Spain and faced court-martial and execution if captured; on 20 March, French deserters disclosed details of French defences to the Germans, who were able to surround 2,000 men and force them to surrender.\nA French lieutenant wrote,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Humanity is mad. It must be mad to do what it is doing. What a massacre! What scenes of horror and carnage! I cannot find words to translate my impressions. Hell cannot be so terrible. Men are mad!\u2014\u200a\nDiscontent began to spread among French troops at Verdun; after the promotion of P\u00e9tain from the Second Army on 1 June and his replacement by Nivelle, five infantry regiments were affected by episodes of \"collective indiscipline\"; Lieutenants Henri Herduin and Pierre Millant were summarily shot on 11 June and Nivelle published an Order of the Day forbidding surrender. In 1926, after an inquiry into the cause c\u00e9l\u00e8bre, Herduin and Millant were exonerated and their military records expunged.\nSubsequent operations.\n20\u201326 August 1917.\nThe French planned an attack on a front on both sides of the Meuse; XIII Corps and XVI Corps to attack on the left bank with two divisions each and two in reserve. C\u00f4te 304, Mort-Homme and C\u00f4te (hill) de l'Oie were to be captured in a advance. On the right (east) bank, XV Corps and XXXII Corps were to advance a similar distance and take C\u00f4te de Talou, hills 344, 326 and the Bois de Cauri\u00e8res. About of wide road was rebuilt and paved for the supply of ammunition, along with a branch of the light railway. The French artillery prepared the attack with 1,280 field guns, 1.520 heavy guns and howitzers and 80 super-heavy guns and howitzers. The crowded 16 into the area to escort reconnaissance aircraft and protect observation balloons. The 5th Army had spent a year improving their defences at Verdun, including the excavation of tunnels linking Mort-Homme with the rear, to deliver supplies and infantry with impunity. On the right bank, the Germans had developed four defensive positions, the last on the French front line of early 1916.\nStrategic surprise was impossible; the Germans had 380 artillery batteries in the area and frequently bombarded French positions with the new mustard gas and made several spoiling attacks to disrupt French preparations. The French counter-attacked but Fayolle eventually limited ripostes to important ground only, the rest to be retaken during the main attack. A preliminary bombardment began on 11 August and the destructive bombardment began two days later but poor weather led to the infantry attack being put back to 20 August. The assembly of the 25th, 16th, and 31st divisions was obstructed by German gas bombardments but their attack captured all but Hill 304, which fell on 24 August. On the right bank, XV Corps had to cross the -wide C\u00f4te de Talou in the middle of no man's land. The French infantry reached their objectives except for a trench between hills 344, 326 and Samogneux, which was taken on 23 August. XXXII Corps reached its objectives in a costly advance but the troops found themselves too close to German trenches and under observed fire from German guns on high ground between Bezonvaux and Ornes. The French took 11,000 prisoners for 14,000 casualties of whom 4,470 were killed or missing.\n7\u20138 September.\nGuillaumat was ordered to plan an operation to capture several trenches and a more ambitious offensive on the east bank to take the last ground from which German artillery observers could see Verdun. P\u00e9tain questioned Guillaumat and Fayolle, who criticised the selection of objectives on the right bank and argued that the French must go on or go back. The Germans counter-attacked from higher ground several times in September; holding the ground captured in August proved more costly than taking it. Fayolle advocated a limited advance to make German counter-attacks harder, improve conditions in the front line and deceive the Germans about French intentions.\nA XV Corps attack on 7 September failed and on 8 September XXXII Corps gained a costly success. The attack continued and the trenches necessary for a secure defensive position were taken but not the last German observation point. More attacks were met by massed artillery fire and counter-attacks and the French ended the operation. On 25 November after a five-hour hurricane bombardment, the 128th and 37th divisions, supported by 18-field artillery, 24 heavy and 9 trench artillery groups conducted a raid on a front in appalling weather. A line of pillboxes were demolished and the infantry returned to their positions.\nMeuse\u2013Argonne Offensive.\nThe French Fourth Army and the American First Army attacked on a front from Moronvilliers to the Meuse on 26 September 1918 at 5:30 a.m., after a three-hour bombardment. American troops quickly captured Malancourt, Bethincourt and Forges on the left bank of the Meuse and by midday the Americans had reached Gercourt, Cuisy, the southern part of Montfaucon and Cheppy. German troops were able to repulse American attacks on Montfaucon ridge, until it was outflanked to the south and Montfaucon was surrounded. German counter-attacks from 27 to 28 September slowed the American advance but Ivoiry and Epinon-Tille were captured, then Montfaucon ridge with 8,000 prisoners and 100 guns. On the right bank of the Meuse, a combined Franco-American force under American command, took Brabant, Haumont, Bois d'Haumont and Bois des Caures and then crossed the front line of February 1916. By November, c.\u200920,000 prisoners, c.\u2009150 guns, c.\u20091,000 trench mortars and several thousand machine-guns had been captured. A German retreat began and continued until the Armistice.\nCommemoration.\nVerdun has become for the French the representative memory of the First World War, comparable to how the Battle of the Somme is viewed in the United Kingdom. Antoine Prost wrote, \"Like Auschwitz, Verdun marks a transgression of the limits of the human condition\". From 1918 to 1939, the French expressed two memories of the battle. One was a patriotic view embodied in memorials built on the battlefield and the Nivelle quote \"They shall not pass\". The other was the memory of the survivors who recalled the death, suffering and sacrifice of others. Verdun soon became a focal point for commemorations of the war. In 1920, a ceremony was held in the citadel of Verdun to choose a body to bury in the Tomb of the Unknown Soldier at the Arc de Triomphe.\nSix destroyed villages in the area were not rebuilt but were given special status as uninhabited communes of Beaumont-en-Verdunois, Bezonvaux, Cumi\u00e8res-le-Mort-Homme, Fleury-devant-Douaumont, Haumont-pr\u00e8s-Samogneux and Louvemont-C\u00f4te-du-Poivre. Alain Denizot included period photographs that show overlapping shell craters in an area of about . Forests planted in the 1930s have grown and hide most of the (Red Zone) but the battlefield remains a vast graveyard, containing the mortal remains of over 100,000 missing soldiers, except for those discovered by the French Forestry Service and laid in the Douaumont ossuary.\nIn the 1960s, Verdun became a symbol of Franco-German reconciliation, through remembrance of common suffering and in the 1980s it became a capital of peace. Organisations were formed and old museums were dedicated to the ideals of peace and human rights. On 22 September 1984, the German Chancellor Helmut Kohl (whose father had fought near Verdun) and French President Fran\u00e7ois Mitterrand (who had been taken prisoner nearby in the Second World War), stood at the Douaumont cemetery, holding hands for several minutes in driving rain as a gesture of Franco-German reconciliation.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nBooks.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nEncyclopaedias.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nJournals.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "51498", "revid": "79805", "url": "https://en.wikipedia.org/wiki?curid=51498", "title": "General Patton", "text": ""}
{"id": "51499", "revid": "25424244", "url": "https://en.wikipedia.org/wiki?curid=51499", "title": "Western Front (World War I)", "text": "Theatre of World War I in France and Belgium\nThe Western Front was one of the main theatres of war during World War I. Following the outbreak of war in August 1914, the German Army opened the Western Front by invading Luxembourg and Belgium, then gaining military control of important industrial regions in France. The German advance was halted with the Battle of the Marne. Following the Race to the Sea, both sides dug in along a meandering line of fortified trenches, stretching from the North Sea to the Swiss frontier with France, the position of which changed little except during early 1917 and again in 1918.\nBetween 1915 and 1917 there were several offensives along this front. The attacks employed massive artillery bombardments and massed infantry advances. Entrenchments, machine gun emplacements, barbed wire, and artillery repeatedly inflicted severe casualties during attacks and counter-attacks and no significant advances were made. Among the most costly of these offensives were the Battle of Verdun, in 1916, with a combined 700,000 casualties, the Battle of the Somme, also in 1916, with over a million casualties, and the Battle of Passchendaele, in 1917, with 487,000 casualties.\nTo break the deadlock of the trench warfare on the Western Front, both sides tried new military technology, including poison gas, aircraft, and tanks. The adoption of better tactics and the cumulative weakening of the armies in the west led to the return of mobility in 1918. The German spring offensive of 1918 was made possible by the Treaty of Brest-Litovsk that ended the war of the Central Powers against Russia and Romania on the Eastern Front. Using short, intense \"hurricane\" bombardments and infiltration tactics, the German armies moved nearly to the west, the deepest advance by either side since 1914, but the success was short-lived.\nThe unstoppable advance of the entente armies during the Hundred Days Offensive of 1918 caused a sudden collapse of the German armies and persuaded the German commanders that defeat was inevitable. The German government surrendered in the Armistice of 11 November 1918, and the terms of peace were settled by the Treaty of Versailles in 1919.\n1914.\nWar plans \u2013 Battle of the Frontiers.\nThe Western Front was the place where the most powerful military forces in Europe, the German and French armies, met and where the First World War was decided. At the outbreak of the war, the German Army, with seven field armies in the west and one in the east, executed a modified version of the Schlieffen Plan, bypassing French defenses along the common border by moving quickly through neutral Belgium, and then turning southwards to attack France and attempt to encircle the French Army and trap it on the German border. Belgian neutrality had been guaranteed by Britain under the Treaty of London, 1839; this caused Britain to join the war at the expiration of its ultimatum at midnight on 4 August. Armies under German generals Alexander von Kluck and Karl von B\u00fclow attacked Belgium on 4 August 1914. Luxembourg had been occupied without opposition on 2 August. The first battle in Belgium was the Battle of Li\u00e8ge, a siege that lasted from 5\u201316 August. Li\u00e8ge was well fortified and surprised the German Army under B\u00fclow with its level of resistance. German heavy artillery was able to demolish the main forts within a few days. Following the fall of Li\u00e8ge, most of the Belgian field army retreated to Antwerp, leaving the garrison of Namur isolated, with the Belgian capital, Brussels, falling to the Germans on 20 August. Although the German army bypassed Antwerp, it remained a threat to their flank. Another siege followed at Namur, lasting from about 20\u201323 August.\nThe French deployed five armies on the frontier. The French Plan XVII was intended to bring about the capture of Alsace\u2013Lorraine. On 7 August, the VII Corps attacked Alsace to capture Mulhouse and Colmar. The main offensive was launched on 14 August with the First and Second Armies attacking toward Sarrebourg-Morhange in Lorraine. In keeping with the Schlieffen Plan, the Germans withdrew slowly while inflicting severe losses upon the French. The French Third and Fourth Armies advanced toward the Saar and attempted to capture Saarburg, attacking Briey and Neufchateau but were repulsed. The French VII Corps captured Mulhouse after a brief engagement first on 7 August, and then again on 23 August, but German reserve forces engaged them in the Battle of Mulhouse and forced the French to retreat twice.\nThe German Army swept through Belgium, executing civilians and razing villages. The application of \"collective responsibility\" against a civilian population further galvanised the entente. Newspapers condemned the German invasion, violence against civilians and destruction of property, which became known as the \"Rape of Belgium.\" After marching through Belgium, Luxembourg and the Ardennes, the Germans advanced into northern France in late August, where they met the French Army, under Joseph Joffre, and the divisions of the British Expeditionary Force under Field Marshal Sir John French. A series of engagements known as the Battle of the Frontiers ensued, which included the Battle of Charleroi and the Battle of Mons. In the former battle the French Fifth Army was almost destroyed by the German 2nd and 3rd Armies and the latter delayed the German advance by a day. A general entente retreat followed, resulting in more clashes at the Battle of Le Cateau, the Siege of Maubeuge and the Battle of St. Quentin (also called the First Battle of Guise).\nFirst Battle of the Marne.\nThe German Army came within of Paris but at the First Battle of the Marne (6\u201312 September), French and British troops were able to force a German retreat by exploiting a gap which appeared between the 1st and 2nd Armies, ending the German advance into France. The German Army retreated north of the Aisne and dug in there, establishing the beginnings of a static western front that was to last for the next three years. Following this German retirement, the opposing forces made reciprocal outflanking manoeuvres, known as the Race to the Sea and quickly extended their trench systems from the Swiss frontier to the North Sea. The territory occupied by Germany held 64 percent of French pig-iron production, 24 percent of its steel manufacturing and 40 percent of the coal industry \u2013 dealing a serious blow to French industry.\nOn the entente side (those countries opposing the German alliance), the final lines were occupied with the armies of each nation defending a part of the front. From the coast in the north, the primary forces were from Belgium, the British Empire and then France. Following the Battle of the Yser in October, the Belgian army controlled a length of West Flanders along the coast, known as the Yser Front, along the Yser and the Ieperlee from Nieuwpoort to Boezinge. Meanwhile, the British Expeditionary Force (BEF) occupied a position on the flank, having occupied a more central position.\nFirst Battle of Ypres.\nFrom 19 October until 22 November, the German forces made their final breakthrough attempt of 1914 during the First Battle of Ypres, which ended in a mutually-costly stalemate. After the battle, Erich von Falkenhayn judged that it was no longer possible for Germany to win the war by purely military means and on 18 November 1914 he called for a diplomatic solution. The Chancellor, Theobald von Bethmann Hollweg; \"Generalfeldmarschall\" Paul von Hindenburg, commanding \"Ober Ost\" (Eastern Front high command); and his deputy, Erich Ludendorff, continued to believe that victory was achievable through decisive battles. During the Lodz offensive in Poland (11\u201325 November), Falkenhayn hoped that the Russians would be made amenable to peace overtures. In his discussions with Bethmann Hollweg, Falkenhayn viewed Germany and Russia as having no insoluble conflict and that the real enemies of Germany were France and Britain. A peace with only a few annexations of territory also seemed possible with France and that with Russia and France out of the war by negotiated settlements, Germany could concentrate on Britain and fight a long war with the resources of Europe at its disposal. Hindenburg and Ludendorff continued to believe that Russia could be defeated by a series of battles which cumulatively would have a decisive effect, after which Germany could finish off France and Britain.\nTrench warfare.\nTrench warfare in 1914, while not new, quickly improved and provided a very high degree of defense. According to two prominent historians:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Trenches were longer, deeper, and better defended by steel, concrete, and barbed wire than ever before. They were far stronger and more effective than chains of forts, for they formed a continuous network, sometimes with four or five parallel lines linked by interfacings. They were dug far below the surface of the earth out of reach of the heaviest artillery... Grand battles with the old maneuvers were out of the question. Only by bombardment, sapping, and assault could the enemy be shaken, and such operations had to be conducted on an immense scale to produce appreciable results. Indeed, it is questionable whether the German lines in France could ever have been broken if the Germans had not wasted their resources in unsuccessful assaults, and the blockade by sea had not gradually cut off their supplies. In such warfare no single general could strike a blow that would make him immortal; the \"glory of fighting\" sank down into the dirt and mire of trenches and dugouts.\n1915.\nBetween the coast and the Vosges was a westward bulge in the trench line, named the Noyon salient after the French town at the maximum point of the German advance near Compi\u00e8gne. Joffre's plan for 1915 was to attack the salient on both flanks to cut it off. The Fourth Army had attacked in Champagne from 20 December 1914\u00a0\u2013 17 March 1915 but the French were not able to attack in Artois at the same time. The Tenth Army formed the northern attack force and was to attack eastwards into the Douai plain on a front between Loos and Arras. On 10 March, as part of the larger offensive in the Artois region, the British Army fought the Battle of Neuve Chapelle to capture Aubers Ridge. The assault was made by four divisions on a front. Preceded by a hurricane bombardment lasting only 35 minutes, the village was captured within four hours. The advance then slowed because of supply and communication difficulties. The Germans brought up reserves and counterattacked, forestalling the attempt to capture the ridge. Since the British had used about a third of their artillery ammunition, General Sir John French blamed the failure on the Shell Crisis of 1915, despite the early success.\nGas warfare.\nAll sides had signed the Hague Conventions of 1899 and 1907, which prohibited the use of chemical weapons in warfare. In 1914, there had been small-scale attempts by both the French and Germans to use various tear gases, which were not strictly prohibited by the early treaties but which were also ineffective. The first use of more lethal chemical weapons on the Western Front was against the French near the Belgian town of Ypres. The Germans had already deployed gas against the Russians in the east at the Battle of Humin-Bolim\u00f3w.\nDespite the German plans to maintain the stalemate with the French and British, Albrecht, Duke of W\u00fcrttemberg, commander of the 4th Army planned an offensive at Ypres, site of the First Battle of Ypres in November 1914. The Second Battle of Ypres, April 1915, was intended to divert attention from offensives in the Eastern Front and disrupt Franco-British planning. After a two-day bombardment, the Germans released of of chlorine onto the battlefield. Though primarily a powerful irritant, it can asphyxiate in high concentrations or prolonged exposure. Being heavier than air, the gas crept across no man's land and drifted into the French trenches. The green-yellow cloud started killing some defenders and those in the rear fled in panic, creating an undefended gap in the entente line. The Germans were unprepared for the level of their success and lacked sufficient reserves to exploit the opening. Canadian troops on the right drew back their left flank and halted the German advance. The gas attack was repeated two days later and caused a withdrawal of the Franco-British line but the opportunity had been lost.\nThe success of this attack would not be repeated, as the entente countered by introducing gas masks and other countermeasures. An example of the success of these measures came a year later, on 27 April in the Gas attacks at Hulluch to the south of Ypres, where the 16th (Irish) Division withstood several German gas attacks. The British retaliated, developing their own chlorine gas and using it at the Battle of Loos in September 1915. Fickle winds and inexperience led to more British casualties from the gas than German. French, British and German forces all escalated the use of gas attacks through the rest of the war, developing the more deadly phosgene gas in 1915, then the infamous mustard gas in 1917, which could linger for days and could kill slowly and painfully. Countermeasures also improved and the stalemate continued.\nAir warfare.\nSpecialised aeroplanes for aerial combat were introduced in 1915. Aircraft were already in use for scouting and on 1 April, the French pilot Roland Garros became the first to shoot down an enemy aircraft by using a machine-gun that shot forward through the propeller blades. This was achieved by crudely reinforcing the blades to deflect bullets. Several weeks later Garros force-landed behind German lines. His aeroplane was captured and sent to Dutch engineer Anthony Fokker, who soon produced a significant improvement, the interrupter gear, in which the machine gun is synchronised with the propeller so it fires in the intervals when the blades of the propeller are out of the line of fire. This advance was quickly ushered into service, in the Fokker E.I (\"Eindecker\", or monoplane, Mark 1), the first single seat fighter aircraft to combine a reasonable maximum speed with an effective armament. Max Immelmann scored the first confirmed kill in an \"Eindecker\" on 1 August. Both sides developed improved weapons, engines, airframes and materials, until the end of the war. It also inaugurated the cult of the ace, the most famous being Manfred von Richthofen (the Red Baron). Contrary to the myth, anti-aircraft fire claimed more kills than fighters.\nSpring offensive.\nThe final entente offensive of the spring was the Second Battle of Artois, an offensive to capture Vimy Ridge and advance into the Douai plain. The French Tenth Army attacked on 9 May after a six-day bombardment and advanced to capture Vimy Ridge. German reinforcements counter-attacked and pushed the French back towards their starting points because French reserves had been held back and the success of the attack had come as a surprise. By 15 May the advance had been stopped, although the fighting continued until 18 June. In May the German Army captured a French document at La Ville-aux-Bois describing a new system of defence. Rather than relying on a heavily fortified front line, the defence was to be arranged in a series of echelons. The front line would be a thinly manned series of outposts, reinforced by a series of strongpoints and a sheltered reserve. If a slope was available, troops were deployed along the rear side for protection. The defence became fully integrated with command of artillery at the divisional level. Members of the German high command viewed this new scheme with some favour and it later became the basis of an elastic defence in depth doctrine against entente attacks.\nDuring the autumn of 1915, the \"Fokker Scourge\" began to have an effect on the battlefront as entente reconnaissance aircraft were nearly driven from the skies. These reconnaissance aircraft were used to direct gunnery and photograph enemy fortifications but now the entente were nearly blinded by German fighters. However, the impact of German air superiority was diminished by their primarily defensive doctrine in which they tended to remain over their own lines, rather than fighting over entente held territory.\nAutumn offensive.\nIn September 1915 the entente launched another offensive, with the French Third Battle of Artois, Second Battle of Champagne and the British at Loos. The French had spent the summer preparing for this action, with the British assuming control of more of the front to release French troops for the attack. The bombardment, which had been carefully targeted by means of aerial photography, began on 22 September. The main French assault was launched on 25 September and, at first, made good progress in spite of surviving wire entanglements and machine gun posts. Rather than retreating, the Germans adopted a new defence-in-depth scheme that consisted of a series of defensive zones and positions with a depth of up to .\nOn 25 September, the British began the Battle of Loos, part of the Third Battle of Artois, which was meant to supplement the larger Champagne attack. The attack was preceded by a four-day artillery bombardment of 250,000 shells and a release of 5,100 cylinders of chlorine gas. The attack involved two corps in the main assault and two corps performing diversionary attacks at Ypres. The British suffered heavy losses, especially due to machine gun fire during the attack and made only limited gains before they ran out of shells. A renewal of the attack on 13 October fared little better. In December, French was replaced by General Douglas Haig as commander of the British forces.\n1916.\nFalkenhayn believed that a breakthrough might no longer be possible and instead focused on forcing a French defeat by inflicting massive casualties. His new goal was to \"bleed France white.\" As such, he adopted two new strategies. The first was the use of unrestricted submarine warfare to cut off entente supplies arriving from overseas. The second would be attacks against the French army intended to inflict maximum casualties; Falkenhayn planned to attack a position from which the French could not retreat, for reasons of strategy and national pride and thus trap the French. The town of Verdun was chosen for this because it was an important stronghold, surrounded by a ring of forts, that lay near the German lines and because it guarded the direct route to Paris.\nFalkenhayn limited the size of the front to to concentrate artillery firepower and to prevent a breakthrough from a counter-offensive. He also kept tight control of the main reserve, feeding in just enough troops to keep the battle going. In preparation for their attack, the Germans had amassed a concentration of aircraft near the fortress. In the opening phase, they swept the air space of French aircraft, which allowed German artillery-observation aircraft and bombers to operate without interference. In May, the French countered by deploying \"escadrilles de chasse\" with superior Nieuport fighters and the air over Verdun turned into a battlefield as both sides fought for air superiority.\nBattle of Verdun.\nThe Battle of Verdun began on 21 February 1916 after a nine-day delay due to snow and blizzards. After a massive eight-hour artillery bombardment, the Germans did not expect much resistance as they slowly advanced on Verdun and its forts. Sporadic French resistance was encountered. The Germans took Fort Douaumont and then reinforcements halted the German advance by 28 February.\nThe Germans turned their focus to Le Mort Homme on the west bank of the Meuse which blocked the route to French artillery emplacements, from which the French fired across the river. After some of the most intense fighting of the campaign, the hill was taken by the Germans in late May. After a change in French command at Verdun from the defensive-minded Philippe P\u00e9tain to the offensive-minded Robert Nivelle, the French attempted to re-capture Fort Douaumont on 22 May but were easily repulsed. The Germans captured Fort Vaux on 7 June and with the aid of diphosgene gas, came within of the last ridge before Verdun before being contained on 23 June.\nOver the summer, the French slowly advanced. With the development of the rolling barrage, the French recaptured Fort Vaux in November and by December 1916 they had pushed the Germans back from Fort Douaumont, in the process rotating 42 divisions through the battle. The Battle of Verdun\u2014also known as the 'Mincing Machine of Verdun' or 'Meuse Mill'\u2014became a symbol of French determination and self-sacrifice.\nBattle of the Somme.\nIn the spring, entente commanders had been concerned about the ability of the French Army to withstand the enormous losses at Verdun. The original plans for an attack around the River Somme were modified to let the British make the main effort. This would serve to relieve pressure on the French, as well as the Russians who had also suffered great losses. On 1 July, after a week of heavy rain, British divisions in Picardy began the Battle of the Somme with the Battle of Albert, supported by five French divisions on their right flank. The attack had been preceded by seven days of heavy artillery bombardment. The experienced French forces were successful in advancing but the British artillery cover had neither blasted away barbed wire, nor destroyed German trenches as effectively as was planned. They suffered the greatest number of casualties (killed, wounded and missing) in a single day in the history of the British Army, about 57,000.\nThe Verdun lesson learnt, the entente tactical aim became the achievement of air superiority and until September, German aircraft were swept from the skies over the Somme. The success of the entente air offensive caused a reorganisation of the German air arm and both sides began using large formations of aircraft rather than relying on individual combat. After regrouping, the battle continued throughout July and August, with some success for the British despite the reinforcement of the German lines. By August, General Haig had concluded that a breakthrough was unlikely and instead, switched tactics to a series of small unit actions. The effect was to straighten out the front line, which was thought necessary in preparation for a massive artillery bombardment with a major push.\nThe final phase of the battle of the Somme saw the first use of the tank on the battlefield. The entente prepared an attack that would involve 13 British and Imperial divisions and four French corps. The attack made early progress, advancing in places but the tanks had little effect due to their lack of numbers and mechanical unreliability. The final phase of the battle took place in October and early November, again producing limited gains with heavy loss of life. All told, the Somme battle had made penetrations of only and failed to reach the original objectives. The British had suffered about 420,000 casualties and the French around 200,000. It is estimated that the Germans lost 465,000, although this figure is controversial.\nThe Somme led directly to major new developments in infantry organisation and tactics; despite the terrible losses of 1 July, some divisions had managed to achieve their objectives with minimal casualties. In examining the reasons behind losses and achievements, once the British war economy produced sufficient equipment and weapons, the army made the platoon the basic tactical unit, similar to the French and German armies. At the time of the Somme, British senior commanders insisted that the company (120 men) was the smallest unit of manoeuvre; less than a year later, the section of ten men would be so.\nHindenburg line.\nIn August 1916 the German leadership along the Western Front had changed as Falkenhayn resigned and was replaced by Hindenburg and Ludendorff. The new leaders soon recognised that the battles of Verdun and the Somme had depleted the offensive capabilities of the German Army. They decided that the German Army in the west would go over to the strategic defensive for most of 1917, while the Central Powers would attack elsewhere.\nDuring the Somme battle and through the winter months, the Germans created a fortification behind the Noyon Salient that would be called the Hindenburg Line, using the defensive principles elaborated since the defensive battles of 1915, including the use of Eingreif divisions. This was intended to shorten the German front, freeing 10 divisions for other duties. This line of fortifications ran from Arras south to St Quentin and shortened the front by about . British long-range reconnaissance aircraft first spotted the construction of the Hindenburg Line in November 1916.\n1917.\nThe Hindenburg Line was built between and behind the German front line. On 25 February the German armies west of the line began Operation Alberich a withdrawal to the line and completed the retirement on 5 April, leaving a supply desert of scorched earth to be occupied by the entente. This withdrawal negated the French strategy of attacking both flanks of the Noyon salient, as it no longer existed. The British continued offensive operations as the War Office claimed, with some justification, that this withdrawal resulted from the casualties the Germans received during the Battles of the Somme and Verdun, despite the entente suffering greater losses.\nOn 6 April the United States declared war on Germany. In early 1915, following the sinking of the , Germany had stopped unrestricted submarine warfare in the Atlantic because of concerns of drawing the United States into the conflict. With the growing discontent of the German public due to the food shortages, the government resumed unrestricted submarine warfare in February 1917. They calculated that a successful submarine and warship siege of Britain would force that country out of the war within six months, while American forces would take a year to become a serious factor on the Western Front. The submarine and surface ships had a long period of success before Britain resorted to the convoy system, bringing a large reduction in shipping losses.\nBy 1917, the size of the British Army on the Western Front had grown to two-thirds of the size of the French force. In April 1917 the BEF began the Battle of Arras. The Canadian Corps and the 5th Division of the First Army, fought the Battle of Vimy Ridge, completing the capture of the ridge and the Third Army to the south achieved the deepest advance since trench warfare began. Later attacks were confronted by German reinforcements defending the area using the lessons learned on the Somme in 1916. British attacks were contained and, according to Gary Sheffield, a greater rate of daily loss was inflicted on the British than in \"any other major battle\".\nDuring the winter of 1916\u20131917, German air tactics had been improved, a fighter training school was opened at Valenciennes and better aircraft with twin guns were introduced. The result was higher losses of Allied aircraft, particularly for the British, Portuguese, Belgians and Australians who were struggling with outmoded aircraft, poor training and tactics. The Allied air successes over the Somme were not repeated. During their attack at Arras, the British lost 316 air crews and the Canadians lost 114 compared to 44 lost by the Germans. This became known to the Royal Flying Corps as Bloody April.\nNivelle Offensive.\nThe same month, the French Commander-in-chief, General Robert Nivelle, ordered a new offensive against the German trenches, promising that it would end the war within 48 hours. The 16 April attack, dubbed the Nivelle Offensive (also known as the Second Battle of the Aisne), would be 1.2\u00a0million men strong, preceded by a week-long artillery bombardment and accompanied by tanks. The offensive proceeded poorly as the French troops, with the help of two Russian brigades, had to negotiate rough, upward-sloping terrain in extremely bad weather. Planning had been dislocated by the voluntary German withdrawal to the Hindenburg Line. Secrecy had been compromised and German aircraft gained air superiority, making reconnaissance difficult and in places, the creeping barrage moved too fast for the French troops. Within a week the French suffered 120,000 casualties. Despite the casualties and his promise to halt the offensive if it did not produce a breakthrough, Nivelle ordered the attack to continue into May.\nOn 3 May the weary French 2nd Colonial Division, veterans of the Battle of Verdun, refused orders, arriving drunk and without their weapons. Lacking the means to punish an entire division, its officers did not immediately implement harsh measures against the mutineers. Mutinies occurred in 54 French divisions and 20,000 men deserted. Other entente forces attacked but suffered massive casualties. Appeals to patriotism and duty followed, as did mass arrests and trials. The French soldiers returned to defend their trenches but refused to participate in further offensive action. On 15 May Nivelle was removed from command, replaced by P\u00e9tain who immediately stopped the offensive. The French would go on the defensive for the following months to avoid high casualties and to restore confidence in the French High Command, while the British assumed greater responsibility.\nAmerican Expeditionary Force.\nOn 25 June the first US troops began to arrive in France, forming the American Expeditionary Force. However, the American units did not enter the trenches in divisional strength until October. The incoming troops required training and equipment before they could join in the effort, and for several months American units were relegated to support efforts. Despite this, however, their presence provided a much-needed boost to entente morale, with the promise of further reinforcements that could tip the manpower balance towards the entente.\nFlanders offensive.\nIn June, the British launched an offensive in Flanders, in part to take the pressure off the French armies on the Aisne, after the French part of the Nivelle Offensive failed to achieve the strategic victory that had been planned and French troops began to mutiny. The offensive began on 7 June, with a British attack on Messines Ridge, south of Ypres, to retake the ground lost in the First and Second battles in 1914. Since 1915 specialist Royal Engineer tunnelling companies had been digging tunnels under the ridge, and about of explosives had been planted in 21 mines under the German defences. Following several weeks of bombardment, the explosives in 19 of these mines were detonated, killing up to 7,000 German troops. The infantry advance that followed relied on three creeping barrages which the British infantry followed to capture the plateau and the east side of the ridge in one day. German counter-attacks were defeated and the southern flank of the Gheluvelt plateau was protected from German observation.\nOn 11 July 1917, during \"Unternehmen Strandfest\" (Operation Beachparty) at Nieuport on the coast, the Germans introduced a new weapon into the war when they fired a powerful blistering agent Sulfur mustard (Yellow Cross) gas. The artillery deployment allowed heavy concentrations of the gas to be used on selected targets. Mustard gas was persistent and could contaminate an area for days, denying it to the British, an additional demoralising factor. The entente powers increased production of gas for chemical warfare but took until late 1918 to copy the Germans and begin using mustard gas.\nFrom 31 July to 10 November the Third Battle of Ypres included the First Battle of Passchendaele and culminated in the Second Battle of Passchendaele. The battle had the original aim of capturing the ridges east of Ypres then advancing to Roulers and Thourout to close the main rail line supplying the German garrisons on the Western front north of Ypres. If successful the northern armies were then to capture the German submarine bases on the Belgian coast. It was later restricted to advancing the British Army onto the ridges around Ypres, as the unusually wet weather slowed British progress. The Canadian Corps relieved the II ANZAC Corps and took the village of Passchendaele on 6 November, despite rain, mud and many casualties. The offensive was costly in manpower for both sides for relatively little gain of ground against determined German resistance but the ground captured was of great tactical importance. In the drier periods, the British advance was inexorable and during the unusually wet August and in the Autumn rains that began in early October, the Germans achieved only costly defensive successes, which led the German commanders in early October to begin preparations for a general retreat. Both sides lost a combined total of over a half million men during this offensive. The battle has become a byword among some British revisionist historians for bloody and futile slaughter, whilst the Germans called Passchendaele \"the greatest martyrdom of the war.\"\nBattle of Cambrai.\nOn 20 November the British launched the first massed tank attack and the first attack using predicted artillery-fire (aiming artillery without firing the guns to obtain target data) at the Battle of Cambrai. The entente attacked with 324 tanks (with one-third held in reserve) and twelve divisions, advancing behind a hurricane bombardment, against two German divisions. The machines carried fascines on their fronts to bridge trenches and the German tank traps. Special \"grapnel tanks\" towed hooks to pull away the German barbed wire. The attack was a great success for the British, who penetrated further in six hours than at the Third Ypres in four months, at a cost of only 4,000 British casualties. The advance produced an awkward salient and a surprise German counter-offensive began on 30 November, which drove back the British in the south and failed in the north. Despite the reversal, the attack was seen as a success by the entente, proving that tanks could overcome trench defences. The Germans realised that the use of tanks by the entente posed a new threat to any defensive strategy they might mount. The battle had also seen the first mass use of German \"Stosstruppen\" on the Western front in the attack, who used infantry infiltration tactics to penetrate British defences, bypassing resistance and quickly advancing into the British rear.\n1918.\nFollowing the successful entente attack and penetration of the German defences at Cambrai, Ludendorff and Hindenburg determined that the only opportunity for German victory lay in a decisive attack along the Western front during the spring, before American manpower became overwhelming. On 3 March 1918, the Treaty of Brest-Litovsk was signed and Russia withdrew from the war. This would now have a dramatic effect on the conflict as 33 divisions were released from the Eastern Front for deployment to the west. The Germans occupied almost as much Russian territory under the provisions of the Treaty of Brest-Litovsk as they did in the Second World War but this considerably restricted their troop redeployment. The Germans achieved an advantage of 192 divisions in the west to the 178 entente divisions, which allowed Germany to pull veteran units from the line and retrain them as \"Stosstruppen\" (40 infantry and 3 cavalry divisions were retained for German occupation duties in the east).\nThe entente lacked unity of command and suffered from morale and manpower problems, the British and French armies were severely depleted and not in a position to attack in the first half of the year, while the majority of the newly arrived American troops were still training, with just six complete divisions in the line. Ludendorff decided on an offensive strategy beginning with a big attack against the British on the Somme, to separate them from the French and drive them back to the channel ports. The attack would combine the new storm troop tactics with over 700 aircraft, tanks and a carefully planned artillery barrage that would include gas attacks.\nGerman spring offensives.\nOperation Michael, the first of the German spring offensives, very nearly succeeded in driving the entente armies apart, advancing to within shelling distance of Paris for the first time since 1914. As a result of the battle, the entente agreed on unity of command. General Ferdinand Foch was appointed commander of all entente forces in France. The unified entente were better able to respond to each of the German drives and the offensive turned into a battle of attrition. In May, the American divisions also began to play an increasing role, winning their first victory in the Battle of Cantigny. By summer, between 250,000 and 300,000 American soldiers were arriving every month. A total of 2.1\u00a0million American troops would be deployed on this front before the war came to an end. The rapidly increasing American presence served as a counter for the large numbers of redeployed German forces.\nEntente counter-offensives.\nIn July, Foch began the Second Battle of the Marne, a counter-offensive against the Marne salient which was eliminated by August. The Battle of Amiens began two days later, with Franco-British forces spearheaded by Australian and Canadian troops, along with 600 tanks and 800 aircraft. Hindenburg named 8 August as the \"Black Day of the German army.\" The Italian 2nd Corps, commanded by General Alberico Albricci, also participated in the operations around Reims. German manpower had been severely depleted after four years of war and its economy and society were under great internal strain. The entente fielded 216 divisions against 197 German divisions. The Hundred Days Offensive beginning in August proved the final straw and following this string of military defeats, German troops began to surrender in large numbers. As the entente forces advanced, Prince Maximilian of Baden was appointed as Chancellor of Germany in October to negotiate an armistice. Ludendorff was forced out and fled to Sweden. The German retreat continued and the German Revolution put a new government in power. The Armistice of Compi\u00e8gne was quickly signed, stopping hostilities on the Western Front on 11 November 1918, later known as Armistice Day. The German Imperial Monarchy collapsed when General Groener, the successor to Ludendorff, backed the moderate Social Democratic Government under Friedrich Ebert, to forestall a revolution like those in Russia the previous year.\nAftermath.\nThe war along the Western Front led the German government and its allies to sue for peace in spite of German success elsewhere. As a result, the terms of the peace were dictated by France, Britain and the United States, during the 1919 Paris Peace Conference. The result was the Treaty of Versailles, signed in June 1919 by a delegation of the new German government. The terms of the treaty constrained Germany as an economic and military power. The Versailles treaty returned the border provinces of Alsace-Lorraine to France, thus limiting the quantity of coal available to German industry. The Saar, which formed the west bank of the Rhine, would be demilitarized and controlled by Britain and France, while the Kiel Canal opened to international traffic. The treaty also drastically reshaped Eastern Europe. It severely limited the German armed forces by restricting the size of the army to 100,000 and prohibiting a navy or air force. The navy was sailed to Scapa Flow under the terms of surrender but was later scuttled as a reaction to the treaty.\nCasualties.\nThe war in the trenches of the Western Front left tens of thousands of maimed soldiers and war widows. The unprecedented loss of life had a lasting effect on popular attitudes toward war, resulting later in an entente reluctance to pursue an aggressive policy toward Adolf Hitler. Belgium suffered 30,000 civilian dead and France 40,000 (including 3,000 merchant sailors). The British lost 16,829 civilian dead; 1,260 civilians were killed in air and naval attacks, 908 civilians were killed at sea and there were 14,661 merchant marine deaths. Another 62,000 Belgian, 107,000 British and 300,000 French civilians died due to war-related causes.\nEconomic costs.\nGermany in 1919 was bankrupt, the people living in a state of semi-starvation and having no commerce with the remainder of the world. The entente occupied the Rhine cities of Cologne, Koblenz and Mainz, with restoration dependent on payment of reparations. In Germany a Stab-in-the-back myth () was propagated by Hindenburg, Ludendorff and other defeated generals, that the defeat was not the fault of the 'good core' of the army but due to certain left-wing groups within Germany who signed a disastrous armistice; this would later be exploited by nationalists and the Nazi party propaganda to excuse the overthrow of the Weimar Republic in 1930 and the imposition of the Nazi dictatorship after March 1933.\nFrance suffered more casualties relative to its population than any other great power and the industrial north-east of the country was devastated by the war. The provinces overrun by Germany had produced 40 percent of French coal and 58 percent of its steel output. Once it was clear that Germany was going to be defeated, Ludendorff had ordered the destruction of the mines in France and Belgium. His goal was to cripple the industries of Germany's main European rival. To prevent similar German attacks in the future, France later built a massive series of fortifications along the German border known as the Maginot Line.\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "51501", "revid": "16752040", "url": "https://en.wikipedia.org/wiki?curid=51501", "title": "Rolling barrage", "text": ""}
{"id": "51503", "revid": "40943757", "url": "https://en.wikipedia.org/wiki?curid=51503", "title": "Progressive rock", "text": "Genre of rock music\nProgressive rock (shortened to prog rock or simply prog) is a broad genre of rock music that primarily developed in the United Kingdom through the mid- to late 1960s, peaking in the early to mid-1970s. Initially termed \"progressive pop\", the style emerged from psychedelic bands who abandoned standard pop or rock traditions in favour of instrumental and compositional techniques more commonly associated with jazz, folk, or classical music, while retaining the instrumentation typical of rock music. Additional elements contributed to its \"progressive\" label: lyrics were more poetic, technology was harnessed for new sounds, music approached the condition of \"art\", and the studio, rather than the stage, became the focus of musical activity, which often involved creating music for listening rather than dancing.\nProgressive rock includes a fusion of styles, approaches and genres, and tends to be diverse and eclectic. Progressive rock is often associated with long solos, extended pieces, fantastic lyrics, grandiose stage sets and costumes, and an obsessive dedication to technical skill. While the genre is often cited for its merging of high culture and low culture, few artists incorporated classical themes in their work to a significant degree, and only a handful of groups, such as Emerson, Lake &amp; Palmer and Renaissance, intentionally emulated or referenced classical music.\nIn the early to mid-1970s, progressive rock groups such as Pink Floyd and Yes experienced great worldwide success; in the late 1970s, it declined in popularity, and has never fully recovered. Conventional wisdom holds that the rise of punk rock caused this, but several more factors contributed to the decline. Music critics, who often labelled the style of progressive rock as \"pretentious\" and the sounds as \"pompous\" and \"overblown\", tended to be hostile towards the genre or to completely ignore it. After the late 1970s, progressive rock fragmented into numerous forms. Some bands achieved commercial success well into the 1980s (albeit with changed lineups and more compact song structures) or crossed into symphonic pop, arena rock, or new wave.\nEarly groups who exhibited progressive features are retroactively described as \"proto-prog\". The Canterbury scene, originating in the late 1960s, denotes a subset of progressive rock bands who emphasised the use of wind instruments, complex chord changes and long improvisations. Rock in Opposition, from the late 1970s, was more avant-garde, and when combined with the Canterbury style, created avant-prog. In the 1980s, a new subgenre, neo-prog, enjoyed some commercial success, although it was also accused of being derivative and lacking in innovation. Post-progressive draws upon newer developments in popular music and the avant-garde since the mid-1970s.\nDefinition and characteristics.\nScope and related terms.\nThe term \"progressive rock\" is related to and sometimes considered synonymous with \"art rock\", \"classical rock\" (not to be confused with classic rock), and \"symphonic rock\". Historically, \"art rock\" has been used to describe at least two related, but distinct, types of rock music. The first is progressive rock as it is generally understood, while the second usage refers to groups who rejected psychedelia and the hippie counterculture in favour of a modernist, avant-garde approach. Similarities between the two terms are that they both describe a mostly British attempt to elevate rock music to new levels of artistic credibility. However, art rock is more likely to have experimental or avant-garde influences. \"Prog\" was devised in the 1990s as a shorthand term, but later became a transferable adjective, also suggesting a wider palette than that drawn on by the most popular 1970s bands.\nProgressive rock is varied and is based on fusions of styles, approaches, and genres, tapping into broader cultural resonances that connect to avant-garde art, classical music and folk music, performance and the moving image. Although a unidirectional English \"progressive\" style emerged in the late 1960s, by 1967, progressive rock had come to constitute a diversity of loosely associated style codes. When the \"progressive\" label arrived, the music was dubbed \"progressive pop\" before it was called \"progressive rock\", with the term \"progressive\" referring to the wide range of attempts to break with standard pop music formula. A number of additional factors contributed to the acquired \"progressive\" label: lyrics were more poetic; technology was harnessed for new sounds; music approached the condition of \"art\"; some harmonic language was imported from jazz and 19th-century classical music; the album format overtook singles; and the studio, rather than the stage, became the focus of musical activity, which often involved creating music for listening, not dancing.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nOne of the best ways to define progressive rock is that it is a heterogeneous and troublesome genre \u2013 a formulation that becomes clear the moment we leave behind characterizations based only on the most visible bands of the early to mid-1970s\n\u2013 Paul Hegarty and Martin Halliwell\nCritics of the genre often limit its scope to a stereotype of long solos, overlong albums, fantasy lyrics, grandiose stage sets and costumes, and an obsessive dedication to technical skill. While progressive rock is often cited for its merging of high culture and low culture, few artists incorporated literal classical themes in their work to any great degree, and only a handful of groups purposely emulated or referenced classical music. Writer Emily Robinson says that the narrowed definition of \"progressive rock\" was a measure against the term's loose application in the late 1960s, when it was \"applied to everyone from Bob Dylan to the Rolling Stones\". Debate over the genre's criterion continued to the 2010s, particularly on Internet forums dedicated to prog.\nAccording to musicologists Paul Hegarty and Martin Halliwell, Bill Martin and Edward Macan authored major books about progressive rock while \"effectively accept[ing] the characterization of progressive rock offered by its critics. ... they each do so largely unconsciously.\" Academic John S. Cotner contests Macan's view that progressive rock cannot exist without the continuous and overt assimilation of classical music into rock. Author Kevin Holm-Hudson agrees that \"progressive rock is a style far more diverse than what is heard from its mainstream groups and what is implied by unsympathetic critics.\"\nRelation to art and social theories.\nIn early references to the music, \"progressive\" was partly related to progressive politics, but those connotations were lost during the 1970s. On \"progressive music\", Holm-Hudson writes that it \"moves continuously between explicit and implicit references to genres and strategies derived not only from European art music, but other cultural domains (such as East Indian, Celtic, folk, and African) and hence involves a continuous aesthetic movement between formalism and eclecticism\". Cotner also says that progressive rock incorporates both formal and eclectic elements, \"It consists of a combination of factors \u2013 some of them intramusical ('within'), others extramusical or social ('without').\"\nOne way of conceptualising rock and roll in relation to \"progressive music\" is that progressive music pushed the genre into greater complexity while retracing the roots of romantic and classical music. Sociologist Paul Willis believes: \"We must never be in doubt that 'progressive' music followed rock 'n' roll, and that it could not have been any other way. We can see rock 'n' roll as a deconstruction and 'progressive' music as a reconstruction.\" Author Will Romano states that \"rock itself can be interpreted as a progressive idea\u00a0... Ironically, and quite paradoxically, 'progressive rock', the classic era of the late 1960s through the mid- and late 1970s, introduces not only the explosive and exploratory sounds of technology\u00a0... but traditional music forms (classical and European folk) and (often) a pastiche compositional style and artificial constructs (concept albums) which suggests postmodernism.\"\nUse of synthesizers.\nSynthesizers have a long history in the creation of progressive rock, beginning with the perhaps most well known early electronic instrument, the Theremin. Synthesizers rose in stature in the 1960s and 1970s as groups like Emerson, Lake &amp; Palmer, Genesis and Yes incorporated synthesizers as part of their music.\nHistory.\n1966\u20131970: Origins.\nBackground, roots, and etymology.\nIn 1966, the level of social and artistic correspondence among British and American rock musicians dramatically accelerated for bands like the Beatles, the Beach Boys and the Byrds who fused elements of cultivated music with the vernacular traditions of rock. Progressive rock, originating in the later 1960s, was inspired by progressive pop groups from the 1960s: those who combined rock and roll with various other music styles such as Indian ragas, oriental melodies and Gregorian chants, like the Beatles and the Yardbirds. The Beatles' Paul McCartney said in 1967 that the band \"got a bit bored with 12 bars all the time, so we tried to get into something else. Then came Dylan, the Who, and the Beach Boys. ... We're all trying to do vaguely the same kind of thing\". The term \"progressive\" here refers to an intentional break from the more standard, predictable, and popular conventions of rock music of the time.\nRock musicians started to take their music more seriously, paralleling earlier movements in jazz (such as how swing gave way to bop, a shift appealing to smaller but devoted audiences). In this period, the popular song began signalling a new possible means of expression that went beyond the three-minute love song, leading to an intersection between the \"underground\" and the \"establishment\" for listening publics.\nHegarty and Halliwell identify the Beatles, the Beach Boys, the Doors, the Pretty Things, the Zombies, the Byrds, the Grateful Dead and Pink Floyd \"not merely as precursors of progressive rock but as essential developments of progressiveness in its early days\". According to musicologist Walter Everett, the Beatles' \"experimental timbres, rhythms, tonal structures, and poetic texts\" on their albums \"Rubber Soul\" (1965) and \"Revolver\" (1966) \"encouraged a legion of young bands that were to create progressive rock in the early 1970s\". Dylan's poetry, the Mothers of Invention's album \"Freak Out!\" (1966) and the Beatles' \"Sgt. Pepper's Lonely Hearts Club Band\" (1967) were all important in progressive rock's development. The productions of Phil Spector were key influences, as they introduced the possibility of using the recording studio to create music that otherwise could never be achieved. Other influences include the Beach Boys' \"Pet Sounds\" (1966), which Brian Wilson intended as an answer to \"Rubber Soul\" and which in turn influenced the Beatles when they made \"Sgt. Pepper\".\nDylan introduced a literary element to rock through his fascination with the Surrealists and the French Symbolists, and his immersion in the New York City art scene of the early 1960s. The trend of bands with names drawn from literature, such as the Doors, Steppenwolf and the Ides of March, were a further sign of rock music aligning itself with high culture. Dylan also led the way in blending rock with folk music styles. This was followed by folk rock groups such as the Byrds, who based their initial sound on that of the Beatles. In turn, the Byrds' vocal harmonies inspired those of Yes, and British folk rock bands like Fairport Convention, who emphasised instrumental virtuosity. Some of these artists, such as the Incredible String Band and Shirley and Dolly Collins, would prove influential through their use of instruments borrowed from world music and early music.\n\"Pet Sounds\" and \"Sgt. Pepper\".\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n Many groups and musicians played important roles in this development process, but none more than the Beach Boys and the Beatles\u00a0... [They] brought expansions in harmony, instrumentation (and therefore timbre), duration, rhythm, and the use of recording technology. Of these elements, the first and last were the most important in clearing a pathway toward the development of progressive rock.\n\u2013 Bill Martin\n\"Pet Sounds\" and \"Sgt. Pepper\", with their lyrical unity, extended structure, complexity, eclecticism, experimentalism, and influences derived from classical music forms, are largely viewed as beginnings in the progressive rock genre and as turning points wherein rock, which previously had been considered dance music, became music that was made for listening to. Between \"Pet Sounds\" and \"Sgt. Pepper\", the Beach Boys released the single \"Good Vibrations\" (1966), dubbed a \"pocket symphony\" by Derek Taylor, the band's publicist. The song contained an eclectic array of exotic instruments and several disjunctive key and modal shifts. Scott Interrante of \"Popmatters\" wrote that its influence on progressive rock and the psychedelic movement \"can't be overstated\". Martin likened the song to the Beatles' \"A Day in the Life\" from \"Sgt. Pepper\", in that they showcase \"the same reasons why much progressive rock is difficult to dance to\".\nAlthough \"Sgt. Pepper\" was preceded by several albums that had begun to bridge the line between \"disposable\" pop and \"serious\" rock, it successfully gave an established \"commercial\" voice to an alternative youth culture and marked the point at which the LP record emerged as a creative format whose importance was equal to or greater than that of the single. Bill Bruford, a veteran of several progressive rock bands, said that \"Sgt. Pepper\" transformed both musicians' ideas of what was possible and audiences' ideas of what was acceptable in music. He believed that: \"Without the Beatles, or someone else who had done what the Beatles did, it is fair to assume that there would have been no progressive rock.\" In the aftermath of \"Sgt. Pepper\", magazines such as Melody Maker drew a sharp line between \"pop\" and \"rock\". Americans increasingly used the adjective \"progressive\" for groups like Jethro Tull, Family, East of Eden, Van der Graaf Generator and King Crimson.\nProto-prog and psychedelia.\nAccording to AllMusic: \"Prog-rock began to emerge out of the British psychedelic scene in 1967, specifically a strain of classical/symphonic rock led by the Nice, Procol Harum, and the Moody Blues (\"Days of Future Passed\").\" The availability of newly affordable recording equipment coincided with the rise of a London underground scene at which the psychedelic drug LSD was commonly used. Pink Floyd and Soft Machine functioned as house bands at all-night events at locations such as Middle Earth and the UFO Club, where they experimented with sound textures and long-form songs. Many psychedelic, folk rock and early progressive bands were aided by exposure from BBC Radio 1 DJ John Peel. Jimi Hendrix, who rose to prominence in the London scene and recorded with a band of English musicians, initiated the trend towards guitar virtuosity and eccentricity in rock music. The Scottish band 1-2-3, later renamed Clouds, were formed in 1966 and began performing at London clubs a year later. According to \"Mojo\"'s George Knemeyer: \"some claim [that they] had a vital influence on prog-rockers such as Yes, The Nice and Family.\"\nSymphonic rock artists in the late 1960s had some chart success, including the singles \"Nights in White Satin\" (the Moody Blues, 1967) and \"A Whiter Shade of Pale\" (Procol Harum, 1967). The Moody Blues established the popularity of symphonic rock when they recorded \"Days of Future Passed\" together with the London Festival Orchestra. Classical influences sometimes took the form of pieces adapted from or inspired by classical works, such as Jeff Beck's \"Beck's Bolero\", Love Sculpture's \"Farandole (Arl\u00e9sienne Suite No 2. Movement 4)\" and parts of the Nice's \"Ars Longa Vita Brevis\". The latter, along with such tracks as \"Rondo\" and \"America\", reflect a greater interest in music that is entirely instrumental. \"Sgt. Pepper's\" and \"Days\" both represent a growing tendency towards song cycles and suites made up of multiple movements.\nFocus incorporated and articulated jazz-style chords, and irregular off-beat drumming into their later rock-based riffs, and several bands that included jazz-style horn sections appeared, including Blood, Sweat &amp; Tears and Chicago. Of these, Martin highlights Chicago in particular for their experimentation with suites and extended compositions, such as the \"Ballet for a Girl in Buchannon\" on \"Chicago II\". Jazz influences appeared in the music of British bands such as Traffic, Colosseum and If, together with Canterbury scene bands such as Soft Machine and Caravan. Canterbury scene bands emphasised the use of wind instruments, complex chord changes and long improvisations. Martin writes that in 1968, \"full-blown progressive rock\" was not yet in existence; however, albums were released by three bands who would later come to the forefront of the music: Jethro Tull, Caravan and Soft Machine.\nThe term \"progressive rock\", which appeared in the liner notes of Caravan's 1968 self-titled debut LP, came to be applied to bands that used classical music techniques to expand the styles and concepts available to rock music. The Nice, the Moody Blues, Procol Harum and Pink Floyd all contained elements of what is now called progressive rock, but none represented as complete an example of the genre as several bands that formed soon after. Almost all of the genre's major bands, including Jethro Tull, King Crimson, Yes, Genesis, Van der Graaf Generator, ELP, Gentle Giant, Barclay James Harvest and Renaissance, released their debut albums during the years 1968\u20131970. Most of these were folk-rock albums that gave little indication of what the bands' mature sound would become, but King Crimson's \"In the Court of the Crimson King\" (1969) and Yes' self-titled debut album (1969) were early, fully formed examples of the genre.\n1970s\u20131980s.\nPeak years (1971\u20131976).\nMost of the genre's major bands released their most critically acclaimed albums during the years 1971\u20131976. The genre experienced a high degree of commercial success during the early 1970s. Between them, the bands Jethro Tull, ELP, the Moody Blues, Yes, and Pink Floyd had five albums that reached number one in the US charts, and sixteen that reached the top ten. Mike Oldfield's \"Tubular Bells\" (1973), an excerpt of which was used as the theme for the film \"The Exorcist\", sold 16 million copies.\nProgressive rock came to be appreciated overseas, but it mostly remained a European, and especially British, phenomenon. Few American bands engaged in it, and the purest representatives of the genre, such as Starcastle and Happy the Man, remained limited to their own geographic regions. This is at least in part due to music industry differences between the US and Great Britain. Cultural factors were also involved, as US musicians tended to come from a blues background, while Europeans tended to have a foundation in classical music.\nNorth American progressive rock bands and artists often represented hybrid styles such as the complex arrangements of Todd Rundgren's Utopia and Rush, the eclectic psychedelia of Spirit, the hard rock of Captain Beyond, the Southern rock-tinged prog of Kansas, the jazz fusion of Frank Zappa and Return to Forever, and the eclectic fusion of the all-instrumental Dixie Dregs. British progressive rock acts had their greatest US success in the same geographic areas in which British heavy metal bands experienced their greatest popularity. The overlap in audiences led to the success of arena rock bands, such as Boston, Kansas, and Styx, who combined elements of the two styles.\nProgressive rock achieved popularity in Continental Europe more quickly than it did in the US. Italy remained generally uninterested in rock music until the strong Italian progressive rock scene developed in the early 1970s. Progressive rock scene emerged in Yugoslavia in the late 1960s, dominating the Yugoslav rock scene until the late 1970s. Few of the European groups were successful outside of their own countries, with the exceptions of Dutch bands like Focus and Golden Earring who wrote English-language lyrics, and the Italians Le Orme and PFM, whose English lyrics were written by Peter Hammill and Peter Sinfield, respectively. Some European bands played in a style derivative of English bands.\nIn Germany, the \"krautrock\" scene is frequently cited as part of the progressive rock genre or an entirely distinct phenomenon. Krautrock bands such as Can, which included two members who had studied under Karlheinz Stockhausen, tended to be more strongly influenced by 20th-century classical music than the British progressive rock bands, whose musical vocabulary leaned more towards the Romantic era. Many of these groups were very influential even among bands that had little enthusiasm for the symphonic variety of progressive rock.\nAvant-prog.\nAvant-prog (originally known as avant-garde progressive rock or experimental prog) is a subgenre of progressive rock that emerged in the 1970s. Originally pioneered by artists such as Frank Zappa and Captain Beefheart, the genre was further developed by the London Canterbury scene and later Rock in Opposition movement. The style drew influences from avant-garde music, modern classical, avant-garde jazz, experimental rock, jazz fusion and psychedelic rock. French band Magma later merged the style with orchestral and neoclassical music, which they coined \"zeuhl\".\nProgressive soul.\nConcurrently, Black American popular musicians drew from progressive rock's conceptual album-oriented approach. This led to a progressive-soul movement in the 1970s that inspired a newfound sophisticated musicality and ambitious lyricism in black pop. Among these musicians were Sly Stone, Stevie Wonder, Marvin Gaye, Curtis Mayfield, and George Clinton. In discussing the development, Bill Martin cites 1970s albums by Wonder (\"Talking Book\", \"Innervisions\", \"Songs in the Key of Life\"), War (\"All Day Music\", \"The World Is a Ghetto\", \"War Live\"), and the Isley Brothers (\"3 + 3\"), while noting that the Who's progressive rock-influenced \"Who Are You\" (1978) also drew from the soul variant. Dominic Maxwell of \"The Times\" calls Wonder's mid-1970s albums \"prog soul of the highest order, pushing the form yet always heartfelt, ambitious and listenable\".\nDecline and fragmentation.\nPolitical and social trends of the late 1970s shifted away from the early 1970s hippie attitudes that had led to the genre's development and popularity. The rise in punk cynicism made the utopian ideals expressed in progressive rock lyrics unfashionable. Virtuosity was rejected, as the expense of purchasing quality instruments and the time investment of learning to play them were seen as barriers to rock's energy and immediacy. There were also changes in the music industry, as record companies disappeared and merged into large media conglomerates. Promoting and developing experimental music was not part of the marketing strategy for these large corporations, who focused their attention on identifying and targeting profitable market niches.\nFour of progressive rock's most successful bands \u2013 King Crimson, Yes, ELP and Genesis \u2013 went on hiatus or experienced major personnel changes during the mid-1970s. Macan notes the September 1974 breakup of King Crimson as particularly significant, noting that Fripp (much later) referred to 1974 as the point when \"all English bands in the genre should have ceased to exist\". More of the major bands, including Van der Graaf Generator, Gentle Giant and U.K., dissolved between 1978 and 1980. Many bands had by the mid-1970s reached the limit of how far they could experiment in a rock context, and fans had wearied of the extended, epic compositions. The sounds of the Hammond, Minimoog and Mellotron had been thoroughly explored, and their use became clich\u00e9d. Those bands who continued to record often simplified their sound, and the genre fragmented from the late 1970s onwards. In Robert Fripp's opinion, once \"progressive rock\" ceased to cover new ground \u2013 becoming a set of conventions to be repeated and imitated \u2013 the genre's premise had ceased to be \"progressive\".\nThe era of record labels investing in their artists, giving them freedom to experiment and limited control over their content and marketing ended with the late 1970s. Corporate artists and repertoire staff exerted an increasing amount of control over the creative process that had previously belonged to the artists, and established acts were pressured to create music with simpler harmony and song structures and fewer changes in meter. A number of symphonic pop bands, such as Supertramp, 10cc, the Alan Parsons Project and the Electric Light Orchestra, brought the orchestral-style arrangements into a context that emphasised pop singles while allowing for occasional instances of exploration. Jethro Tull, Gentle Giant and Pink Floyd opted for a harder sound in the style of arena rock.\nFew new progressive rock bands formed during this era, and those who did found that record labels were not interested in signing them. The short-lived supergroup U.K. was a notable exception since its members had established reputations; they produced two albums that were stylistically similar to previous artists and did little to advance the genre. Part of the genre's legacy in this period was its influence on other styles, as several European guitarists brought a progressive rock approach to heavy metal and laid the groundwork for progressive metal. Michael Schenker, of UFO; and Uli Jon Roth, who replaced Schenker in Scorpions, expanded the modal vocabulary available to guitarists. Roth studied classical music with the intent of using the guitar in the way that classical composers used the violin. Finally, the Dutch-born and classically trained Alex and Eddie Van Halen formed Van Halen, featuring ground-breaking whammy-bar, tapping and cross-picking guitar performances that influenced \"shred\" music in the 1980s.\nCommercialisation.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nBy the early 1980s, progressive rock was thought to be all but dead as a style, an idea reinforced by the fact that some of the principal progressive groups had developed a more commercial sound. ... What went out of the music of these now ex-progressive groups ... was any significant evocation of art music.\n\u2013 John Covach\nSome established artists moved towards music that was simpler and more commercially viable. Arena rock bands like Journey, Kansas, Styx, GTR, ELO and Foreigner either had begun as progressive rock bands or included members with strong ties to the genre. These groups retained some of the song complexity and orchestral-style arrangements, but they moved away from lyrical mysticism in favour of more conventional themes such as relationships. These radio-friendly groups have been called \"prog lite\". Genesis transformed into a successful pop act, the prog supergroup Asia (consisting of members of Yes, King Crimson, and ELP) scored a number-one album in 1982, and a re-formed Yes released the relatively mainstream \"90125\" (1983), which yielded their only US number-one single, \"Owner of a Lonely Heart\". One band who remained successful into the 1980s while maintaining a progressive approach was Pink Floyd, who released \"The Wall\" late in 1979. The album, which brought punk anger into progressive rock, was a huge success and was later filmed as \"Pink Floyd \u2013 The Wall\".\nPost-punk and post-progressive.\nPunk and progressive rock were not necessarily as opposed as is commonly believed. Both genres reject commercialism, and punk bands did see a need for musical advancement. Author Doyle Green noted that post-punk emerged as \"a kind of 'progressive punk'\". Post-punk artists rejected the high cultural references of 1960s rock artists like the Beatles and Bob Dylan as well as paradigms that defined rock as \"progressive\", \"art\", or \"studio perfectionism\". In contrast to punk rock, it balances punk's energy and skepticism with art school consciousness, Dadaist experimentalism, and atmospheric, ambient soundscapes. World music, especially African and Asian traditions, was also a major influence.\nProgressive rock's impact was felt in the work of some post-punk artists, although they tended not to emulate classical rock or Canterbury groups but rather Roxy Music, King Crimson, and German krautrock bands, particularly Can, Neu!, and Faust as key inspirations. Punishment of Luxury's music borrowed from both progressive and punk rock, whilst Alternative TV, who were fronted by the founder of the influential punk fanzine \"Sniffin' Glue\" Mark Perry, toured and released a split live album with Gong offshoot Here &amp; Now.\nThe term \"post-progressive\" identifies progressive rock that returns to its original principles while dissociating from 1970s progressive rock styles, and may be located after 1978. Martin credits Roxy Music's Brian Eno as the sub-genre's most important catalyst, explaining that his 1973\u201377 output merged aspects of progressive rock with a prescient notion of new wave and punk. New wave, which surfaced around 1978\u201379 with some of the same attitudes and aesthetic as punk, was characterised by Martin as \"progressive\" multiplied by \"punk\". Bands in the genre tended to be less hostile towards progressive rock than the punks, and there were crossovers, such as Fripp and Eno's involvement with Talking Heads, and Yes' replacement of Rick Wakeman and Jon Anderson with the pop duo the Buggles.\nWhen King Crimson reformed in 1981, they released an album, \"Discipline\", which Macan says \"inaugurated\" the new post-progressive style. The new King Crimson line-up featured guitarist and vocalist Adrian Belew, who also collaborated with Talking Heads, playing live with the band and featuring on their 1980 album \"Remain in Light\". According to Martin, Talking Heads also created \"a kind of new-wave music that was the perfect synthesis of punk urgency and attitude and progressive-rock sophistication and creativity. A good deal of the more interesting rock since that time is clearly 'post-Talking Heads' music, but this means that it is post-progressive rock as well.\"\nNeo-prog.\nA second wave of progressive rock bands appeared in the early 1980s and have since been categorised as a separate \"neo-prog\" subgenre. These largely keyboard-based bands played extended compositions with complex musical and lyrical structures. Several of these bands were signed by major record labels, including Marillion, IQ, Pendragon, Pallas and Twelfth Night. Most of the genre's major acts released debut albums between 1983 and 1985 and shared the same manager, Keith Goodwin, a publicist who had been instrumental in promoting progressive rock during the 1970s. The previous decade's bands had the advantage of appearing during a prominent countercultural movement that provided them with a large potential audience, but the neo-prog bands were limited to a relatively niche demographic and found it difficult to attract a following. Only Marillion and Saga experienced international success.\nNeo-prog bands tended to use Peter Gabriel-era Genesis as their \"principal model\". They were also influenced by funk, hard rock and punk rock. The genre's most successful band, Marillion, suffered particularly from accusations of similarity to Genesis, although they used a different vocal style, incorporated more hard rock elements, and were very influenced by bands including Camel and Pink Floyd. Authors Paul Hegarty and Martin Halliwell have pointed out that the neo-prog bands were not so much plagiarising progressive rock as they were creating a new style from progressive rock elements, just as the bands of a decade before had created a new style from jazz and classical elements. Author Edward Macan counters by pointing out that these bands were at least partially motivated by a nostalgic desire to preserve a past style rather than a drive to innovate.\nBrutal prog.\nBrutal prog is a subgenre of avant-prog coined by Weasel Walter to describe the aggressively dissonant and musically eclectic strain of prog music associated with his band the Flying Luttenbachers of the Chicago no wave scene. The style originated in the early 2000s and drew influences from math rock, noise rock, no wave and free jazz. \n1990s\u20132000s.\nThird wave.\nA third wave of progressive rock bands, who can also be described as a second generation of neo-prog bands, emerged in the 1990s. The use of the term \"progressive\" to describe groups that follow in the style of bands from ten to twenty years earlier is somewhat controversial, as it has been seen as a contradiction of the spirit of experimentation and progress. These new bands were aided in part by the availability of personal computer-based recording studios, which reduced album production expenses, and the Internet, which made it easier for bands outside of the mainstream to reach widespread audiences. Record stores specialising in progressive rock appeared in large cities.\nThe shred music of the 1980s was a major influence on the progressive rock groups of the 1990s. Some of the newer bands, such as the Flower Kings, Spock's Beard and Glass Hammer, played a 1970s-style symphonic prog, but with an updated sound. A number of them began to explore the limits of the CD in the way that earlier groups had stretched the limits of the vinyl LP.\nProgressive metal.\nProgressive rock and heavy metal have similar timelines. Both emerged from late-1960s psychedelia to achieve great early-1970s success despite a lack of radio airplay and support from critics, then faded in the mid-to-late 1970s and experienced revivals in the early 1980s. Each genre experienced a fragmentation of styles at this time, and many metal bands from the new wave of British heavy metal \u2013 most notably Iron Maiden \u2013 onwards displayed progressive rock influences. Progressive metal reached a point of maturity with Queensr\u00ffche's 1988 concept album \",\" Voivod's 1989 \"Nothingface\", which featured abstract lyrics and a King Crimson-like texture, and Dream Theater's 1992 \"Images and Words\".\nProgressive rock elements appear in other metal subgenres. Death metal's guttural vocals are sometimes used by bands who can be classified as progressive, such as Mastodon, Mudvayne and Opeth. Symphonic metal is an extension of the tendency towards orchestral passages in early progressive rock. Progressive rock has also served as a key inspiration for genres such as post-rock, post-metal and avant-garde metal, math rock, power metal and neo-classical metal.\nNew prog.\nNew prog describes the wave of progressive rock bands in the 2000s who revived the genre. According to \"Entertainment Weekly\"'s Evan Serpick, \"success stories like System of a Down and up-and-comers like the Dillinger Escape Plan, Lightning Bolt, Coheed and Cambria, and the Mars Volta create incredibly complex and inventive music that sounds like a heavier, more aggressive version of '70s behemoths such as Led Zeppelin and King Crimson.\"\nWider influence.\nThe turn of the millennium also saw the rise of a number of bands to mainstream success who, whilst not explicitly identifying as progressive rock, have incorporated influences from it and embraced an experimentalism and complexity in their music which has been regarded as being in keeping with the genre's traditions. Radiohead have been embraced by some progressive rock fans and musicians, with Rush's Geddy Lee declaring \"OK Computer\" as one of his favourite albums and arguing that they carried on the torch from bands like Yes and Genesis. Radiohead's drummer Philip Selway has cited King Crimson and Pink Floyd as influences, and the band's eclectic and experimental style has drawn on a range of sources, including jazz, electronic music, art rock, krautrock, post-rock and the work of Captain Beefheart.\nTool and King Crimson have acknowledged each other as influences and toured together. Other bands embracing progressive influences include Muse and Mansun, whose second album \"Six\" has been listed as a favourite British rock album of Porcupine Tree's Steven Wilson, who has collaborated and toured with former Mansun frontman Paul Draper. Ween's 1997 album \"The Mollusk\" has also been noted for its progressive rock influences, complete with Storm Thorgerson-designed cover art.\n2010s\u20132020s.\nThe Progressive Music Awards were launched in 2012 by the British magazine \"Prog\" to honour the genre's established acts and to promote its newer bands.\nIn 2019, the Prog Report named Mike Portnoy and Neal Morse artists of the decade for 2010\u20132019. During this time, Portnoy released 40 albums, 24 of them with Morse, while Morse released an additional 5 albums of his own.\nDuring the COVID-19 pandemic in 2020, the Prog Report launched a virtual concert, \"Prog From Home\", bringing together many of the biggest artists active in the genre.\nOn April 3, 2022, \"The Alien\" won a Grammy Award for Best Metal Performance, giving Dream Theater their first Grammy.\nFestivals.\nMany prominent progressive rock bands got their initial exposure at large rock festivals that were held in Britain during the late 1960s and early 1970s. King Crimson made their first major appearance at the 1969 Hyde Park free concert, before a crowd estimated to be as large as 650,000, in support of the Rolling Stones. Emerson, Lake &amp; Palmer debuted at the 1970 Isle of Wight Festival, at which Supertramp, Family and Jethro Tull also appeared. Jethro Tull were also present at the 1969 Newport Jazz Festival, the first year in which that festival invited rock bands to perform. Hawkwind appeared at many British festivals throughout the 1970s, although they sometimes showed up uninvited, set up a stage on the periphery of the event, and played for free.\nRenewed interest in the genre in the 1990s led to the development of progressive rock festivals. ProgFest, organised by Greg Walker and David Overstreet in 1993, was first held in UCLA's Royce Hall, and featured Sweden's \u00c4nglag\u00e5rd, the UK's IQ, Quill and Citadel. CalProg was held annually in Whittier, California during the early 2000s. The North East Art Rock Festival, or NEARfest, held its first event in 1999 in Bethlehem, Pennsylvania and held annual sold-out concerts until 2012's NEARfest Apocalypse, which featured headliners U.K. and Renaissance.\nOther festivals include the annual ProgDay (the longest-running and only outdoor progressive music festival) in Chapel Hill, North Carolina, the annual Rites of Spring Festival (RoSfest) in Sarasota, Florida, The Rogue Independent Music Festival in Atlanta, Georgia, Baja Prog in Mexicali, Mexico, ProgPower USA in Atlanta, Georgia, ProgPower Europe in Baarlo, Netherlands, and ProgStock in Rahway, NJ, which held its first event in 2017.\nReception.\nThe genre has received both critical acclaim and criticism throughout the years. Progressive rock has been described as parallel to the classical music of Igor Stravinsky and B\u00e9la Bart\u00f3k. This desire to expand the boundaries of rock, combined with some musicians' dismissiveness toward mainstream rock and pop, dismayed critics and led to accusations of elitism. Its intellectual, fantastic and apolitical lyrics, and shunning of rock's blues roots, were abandonments of the very things that many critics valued in rock music. Progressive rock also represented the maturation of rock as a genre, but there was an opinion among critics that rock was and should remain fundamentally tied to adolescence, so rock and maturity were mutually exclusive. Criticisms over the complexity of their music provoked some bands to create music that was even more complex. BBC radio DJ John Peel, who was one of the first people to play progressive rock on British radio, later said \"the one distinguishing feature about early-70s progressive rock was that it didn't progress\".\nMost of the musicians involved were male, as was the case for most rock of the time, although Annie Haslam of Renaissance was a notable exception. Female singers were better represented in progressive folk bands, who displayed a broader range of vocal styles than the progressive rock bands with whom they frequently toured and shared band members.\nBritish and European audiences typically followed concert hall behaviour protocols associated with classical music performances and were more reserved in their behaviour than audiences for other forms of rock. This confused musicians during US tours, as they found American audiences less attentive and more prone to outbursts during quiet passages.\nThese aspirations towards high culture reflect progressive rock's origins as a music created largely by upper- and middle-class, white-collar, college-educated males from Southern England. The music never reflected the concerns of or was embraced by working-class listeners, except in the US, where listeners appreciated the musicians' virtuosity. Progressive rock's exotic, literary topics were considered particularly irrelevant to British youth during the late 1970s, when the nation suffered from a poor economy and frequent strikes and shortages. Even King Crimson leader Robert Fripp dismissed progressive rock lyrics as \"the philosophical meanderings of some English half-wit who is circumnavigating some inessential point of experience in his life\". Bands whose darker lyrics avoided utopianism, such as King Crimson, Pink Floyd and Van der Graaf Generator, experienced less critical disfavour.\nIn 2002, Pink Floyd guitarist David Gilmour said, \"I wasn't a big fan of most of what you'd call progressive rock. I'm like Groucho Marx: I don't want to belong to any club that would have me for a member.\" In 2014, Peter Gabriel remarked, \"Despite prog probably being the most derided musical genre of all time there were\u2014as today\u2014a lot of extraordinary musicians trying to break down the barriers to reject the rules of music. It was genuinely pioneering at the time. We didn't always get it right, but when it did work we could move people and get some magic happening. I see it all as a very healthy part of growing up.\"\nIan Anderson, the frontman of Jethro Tull, commented:\nI still like the original term that comes from 1969: progressive rock \u2013 but that was with a small \"p\" and a small \"r\". Prog Rock, on the other hand, has different connotations \u2013 of grandeur and pomposity [...] I think looking back on it that most of it was a pretty good experience for musicians and listeners alike. Some of it was a little bit overblown, but in the case of much of the music, it was absolutely spot on.\nWhile music fans for years have declared progressive rock to be dead, the scene is still active with many sub-genres.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "51504", "revid": "74306", "url": "https://en.wikipedia.org/wiki?curid=51504", "title": "Dell Hymes", "text": "American anthropologist and linguist (1927\u20132009)\nDell Hathaway Hymes (June 7, 1927, in Portland, Oregon \u2013 November 13, 2009, in Charlottesville, Virginia) was a linguist, sociolinguist, anthropologist, and folklorist who established disciplinary foundations for the comparative, ethnographic study of language use. His research focused upon the languages of the Pacific Northwest. He was one of the first to call the fourth subfield of anthropology \"linguistic anthropology\" instead of \"anthropological linguistics\". The terminological shift draws attention to the field's grounding in anthropology rather than in what, by that time, had already become an autonomous discipline (linguistics). In 1972 Hymes founded the journal \"Language in Society\" and served as its editor for 22 years.\nEarly life and education.\nHe was educated at Reed College, studying under David H. French; and after a stint in prewar Korea, he graduated in 1950. His work in the United States Army as a decoder is part of what influenced him to become a linguist. Hymes earned his PhD in linguistics from Indiana University Bloomington in 1955. As a young Ph.D. graduate, Hymes carefully analyzed a corpus, within the publication by Melville Jacobs of the songs and stories of Victoria Howard, developing new approaches to the interpretation of oral narratives. He went on to take a job at Harvard University.\nEven at that young age, Hymes had a reputation as a strong linguist; his dissertation, completed in one year, was a grammar of the Kathlamet language spoken near the mouth of the Columbia and known primarily from Franz Boas\u2019s work at the end of the 19th century.\nCareer.\nFrom 1955, Hymes taught at Harvard University for five years, leaving in 1960 to join the faculty of the University of California, Berkeley, where he spent another five years before joining the Department of Anthropology at the University of Pennsylvania in 1965 (where he succeeded A. Irving Hallowell). In 1972 he joined the Department of Folklore and Folklife and in 1975 he became Dean of the University of Pennsylvania Graduate School of Education.\nHe served as president of the American Folklore Society in 1973, the Linguistic Society of America in 1982, and the American Anthropological Association in 1983\u2014the last person to have held all three positions.\nWhile at Penn, Hymes was a founder of the journal \"Language in Society\".\nHymes later joined the Departments of Anthropology and English at the University of Virginia, where he became the Commonwealth Professor of Anthropology and English, and from which he retired in 2000, continuing as emeritus professor until his death from complications of Alzheimer's disease on November 13, 2009.\nSexual harassment allegations.\nHymes was accused of sexual harassment in the later years of his tenure at the University of Pennsylvania. According to The Daily Pennsylvanian, this \nincluded:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...his female graduate advisees being nicknamed 'Hymes' Harem,' female GSE faculty testifying against Hymes for sexual harassment, and Penn faculty in other schools severing their formal ties with GSE after multiple women filed sexual discrimination lawsuits for being denied tenure.\nAs a result of these allegations, in 2018 his portrait was removed from the Graduate School of Education.\nInfluences on his work.\nHymes was influenced by a number of linguists, anthropologists and sociologists; notably Franz Boas, Edward Sapir and Harry Hoijer of the Americanist Tradition; Roman Jakobson and others of the Prague Linguistic Circle; sociologist Erving Goffman and anthropologist Ray L. Birdwhistell, both his colleagues at Penn; and ethnomethodologists Harold Garfinkel, Harvey Sacks, Emanuel Schegloff and Gail Jefferson.\nHymes' career can be divided into at least two phases. In his early career Hymes adapted Prague School Functionalism to American Linguistic Anthropology, pioneering the study of the relationship between language and social context. Together with John Gumperz, Erving Goffman and William Labov, Hymes defined a broad multidisciplinary concern with language in society.\nHymes' later work focuses on poetics, particularly the poetic organization of Native American oral narratives. He and Dennis Tedlock defined ethnopoetics as a field of study within linguistic anthropology and folkloristics. Hymes considers literary critic Kenneth Burke his biggest influence on this latter work, saying, \"My sense of what I do probably owes more to KB than to anyone else.\" Hymes studied with Burke in the 1950s. Burke's work was theoretically and topically diverse, but the idea that seems most influential on Hymes is the application of rhetorical criticism to poetry.\nHymes has included many other literary figures and critics among his influences, including Robert Alter, C. S. Lewis, A. L. Kroeber, and Claude L\u00e9vi-Strauss.\nSignificance of his work.\nAs one of the first sociolinguists, Hymes helped to pioneer the connection between speech and social relations, placing linguistic anthropology at the center of the performative turn within anthropology and the social sciences more generally.\nHymes formulated a response to Noam Chomsky's influential distinction between competence (knowledge of grammatical rules necessary to decoding and producing language) and performance (actual language use in context). Hymes objected to the marginalization of performance from the center of linguistic inquiry and proposed the notion of communicative competence, or knowledge necessary to use language in social context, as an object of linguistic inquiry.\nSince appropriate language use is conventionally defined, and varies across different communities, much of Hymes early work frames a project for ethnographic investigation into contrasting patterns of language use across speech communities. Hymes termed this approach \"the ethnography of speaking\".\nThe SPEAKING acronym, described below, was presented as a lighthearted heuristic to aid fieldworkers in their attempt to document and analyze instances of language use, which he termed \"speech events\". Embedded in the acronym is an application and extension of Roman Jakobson's arguments concerning the multifunctionality of language. He articulated other, more technical, often typologically oriented approaches to variation in patterns of language use across speech communities in a series of articles.\nAs a result of discussions primarily with Ray Birdwhistell at the University of Pennsylvania, in his later work, Hymes renamed the \"ethnography of speaking\" the \"ethnography of communication\" to reflect the broadening of focus from instances of language production to the ways in which communication (including oral, written, broadcast, acts of receiving/listening) is conventionalized in a given community of users, and to include nonverbal as well as verbal behavior.\nWith Erving Goffman and John Szwed, he established the Center for Urban Ethnography in 1969. The goal was to fund research by both faculty and students at Penn that used urban ethnography as the primary method, and much innovative research resulted. The first major grant came from the National Institute of Mental Health, funding much research emphasizing different racial and ethnic groups; the second from the U.S. National Institute of Education, funding classroom ethnography. With Erving Goffman he co-edited the series Conduct and Communication for the University of Pennsylvania Press as a way to support research they considered most valuable.\nHymes promoted what he and others call \"ethnopoetics\", an anthropological method of transcribing and analyzing folklore and oral narrative that pays attention to poetic structures within speech. In reading the transcriptions of Indian myths, for example, which were generally recorded as prose by the anthropologists who came before, Hymes noticed that there are commonly poetic structures in the wording and structuring of the tale. Patterns of words and word use follow patterned, artistic forms.\nHymes' goal, in his own mind, is to understand the artistry and \"the competence... that underlies and informs such narratives\". He created the Dell Hymes Model of Speaking and coined the term communicative competence within language education.\nNarratives can be entertaining stories or important myths about the nature of the world; in addition, narratives can also convey the importance of aboriginal environmental management knowledge such as fish spawning cycles in local rivers or the disappearance of grizzly bears from Oregon. Hymes believes that all narratives in the world are organized around implicit principles of form which convey important knowledge and ways of thinking and of viewing the world. He argues that understanding narratives will lead to a fuller understanding of the language itself and those fields informed by storytelling, in which he includes ethnopoetics, sociolinguistics, psycholinguistics, rhetoric, semiotics, pragmatics, narrative inquiry and literary criticism.\nHymes clearly considers folklore and narrative a vital part of the fields of linguistics, anthropology and literature; and has bemoaned the fact that so few scholars in those fields are willing and able to adequately include folklore in its original language in their considerations. He feels that the translated versions of the stories are inadequate for understanding the stories' roles in the social or mental system in which they existed. He provides an example that in Navajo, the particles (utterances such as \"uh,\" \"so,\" \"well,\" etc. that have linguistic if not semantic meaning) omitted in the English translation are essential to understanding how the story is shaped and how repetition defines the structure that the text embodies.\nHymes was the founding editor for the journal \"Language in Society\", which he edited for 22 years.\nThe \"S-P-E-A-K-I-N-G\" model.\nHymes developed a valuable model to assist the identification and labeling of components of linguistic interaction that was driven by his view that, in order to speak a language correctly, one needs not only to learn its vocabulary and grammar, but also the context in which words are used.\nThe model had sixteen components that can be applied to many sorts of discourse: message form; message content; setting; scene; speaker/sender; addressor; hearer/receiver/audience; addressee; purposes (outcomes); purposes (goals); key; channels; forms of speech; norms of interaction; norms of interpretation; and genres.\nHymes constructed the acronym SPEAKING, under which he grouped the sixteen components within eight divisions:\nSetting and scene.\n\"Setting refers to the time and place of a speech act and, in general, to the physical circumstances\" - The living room in the grandparents' home might be a setting for a family story. Scene is the \"psychological setting\" or \"cultural definition\" of a setting, including characteristics such as range of formality and sense of play or seriousness. The family story may be told at a reunion celebrating the grandparents' anniversary. At times, the family would be festive and playful; at other times, serious and commemorative.\nParticipants.\nSpeaker and audience - Linguists will make distinctions within these categories; for example, the audience can be distinguished as addressees and other hearers. At the family reunion, an aunt might tell a story to the young female relatives, but males, although not addressed, might also hear the narrative.\nEnds.\nPurposes, goals, and outcomes - The aunt may tell a story about the grandmother to entertain the audience, teach the young women, and honor the grandmother.\nAct sequence.\nForm and order of the event - The aunt's story might begin as a response to a toast to the grandmother. The story's plot and development would have a sequence structured by the aunt. Possibly there would be a collaborative interruption during the telling. Finally, the group might applaud the tale and move onto another subject or activity.\nKey.\nClues that establish the \"tone, manner, or spirit\" of the speech act - The aunt might imitate the grandmother's voice and gestures in a playful way, or she might address the group in a serious voice emphasizing the sincerity and respect of the praise the story expresses.\nInstrumentalities.\nForms and styles of speech - The aunt might speak in a casual register with many dialect features or might use a more formal register and careful grammatically \"standard\" forms.\nNorms.\nSocial rules governing the event and the participants' actions and reaction - In a playful story by the aunt, the norms might allow many audience interruptions and collaboration, or possibly those interruptions might be limited to participation by older females. A serious, formal story by the aunt might call for attention to her and no interruptions as norms.\nGenre.\nThe kind of speech act or event; for the example used here, the kind of story - The aunt might tell a character anecdote about the grandmother for entertainment, or an exemplum as moral instruction. Different disciplines develop terms for kinds of speech acts, and speech communities sometimes have their own terms for types.\nFamily and personal life.\nHymes' spouse, Virginia Dosch Hymes, was also a sociolinguist and folklorist. They met at Indiana University, marrying in 1954.\nReligious associations.\nHymes was a member of the Guild of Scholars of The Episcopal Church.\nHe was a congregant of St. Paul Memorial Church and Peace Lutheran Church in Charlottesville, Virginia.\nAwards.\nHymes was awarded the Gold Medal of Philology in 2006.\nPublications.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51505", "revid": "11952314", "url": "https://en.wikipedia.org/wiki?curid=51505", "title": "History of the Islamic Republic of Afghanistan", "text": ""}
{"id": "51507", "revid": "39974728", "url": "https://en.wikipedia.org/wiki?curid=51507", "title": "Islamic Republic of Afghanistan (2004\u20132021)", "text": ""}
{"id": "51509", "revid": "6941696", "url": "https://en.wikipedia.org/wiki?curid=51509", "title": "County seat", "text": "Administrative center for a county or civil parish\nA county seat is an administrative center, seat of government, or capital city of a county or civil parish. The term is in use in six countries: Canada, China, Hungary, Romania, Taiwan, and the United States. An equivalent term, shire town, is used in the U.S. state of Vermont and in several other English-speaking jurisdictions.\nCanada.\nIn Canada, the provinces of Ontario, Quebec, New Brunswick, Prince Edward Island, and Nova Scotia have counties as an administrative division of government below the provincial level, and thus county seats. In the provinces of Prince Edward Island, New Brunswick, and Nova Scotia, the term \"shire town\" is used in place of county seat.\nChina.\nCounty seats in China are the administrative centers of the counties in the People's Republic of China.\nThey have existed since the Warring States period and were set up nationwide by the Qin dynasty. The number of counties in China proper gradually increased from dynasty to dynasty. As Qin Shi Huang reorganized the counties after his unification, there were about 1,000. Under the Eastern Han dynasty, the number of counties increased to above 1,000. About 1400 existed when the Sui dynasty abolished the commandery level (\u90e1 j\u00f9n), which was the level just above counties, and demoted some commanderies to counties.\nIn Imperial China, the county was a significant administrative unit because it marked the lowest level of the imperial bureaucratic structure; in other words, it was the lowest level that the government reached. Government below the county level was often undertaken through informal non-bureaucratic means, varying between dynasties. The head of a county was the magistrate, who oversaw both the day-to-day operations of the county as well as civil and criminal cases.\nThe current number of counties mostly resembled that of the later years of the Qing dynasty. Changes of location and names of counties in Chinese history have been a major field of research in Chinese historical geography, especially from the 1960s to the 1980s. There are 1,355 counties in mainland China out of a total of 2,851 county-level divisions.\nTaiwan.\nIn Taiwan, the first counties were first established in 1661 by the Kingdom of Tungning. The later ruler Qing empire inherited this type of administrative divisions. With the increase of Han Chinese population in Taiwan, the number of counties also grew by time. By the end of Qing era, there were 11 counties in Taiwan. Protestant missionaries in China first romanized the term as hien. When Taiwan became a Japanese colony in 1895, the hierarchy of divisions also incorporated into the Japanese system in the period when Taiwan was under Japanese rule.\nBy September 1945, Taiwan was divided into 8 prefectures ( and ), which remained after the Republic of China took over.\nThere are 13 county seats in Taiwan, which function as county-administered cities, urban townships, or rural townships.\nUnited States.\nFunction.\nIn most of the United States, a county is an administrative or political subdivision of a state that consists of a geographic area with specific boundaries and usually some level of governmental authority. The city, town, or populated place that houses county government is known as the seat of its county. Generally, the county legislature, county courthouse, sheriff's department headquarters, hall of records, jail and correctional facility are located in the county seat, though some functions (such as highway maintenance, which usually requires a large garage for vehicles, along with asphalt and salt storage facilities) may also be located or conducted in other parts of the county, especially if it is geographically large.\nA county seat is usually an incorporated municipality. The exceptions include the county seats of counties that have no incorporated municipalities within their borders, such as Arlington County, Virginia, where the county seat is the entire county. Ellicott City, the county seat of Howard County, Maryland, is the largest unincorporated county seat in the United States, followed by Towson, the county seat of Baltimore County, Maryland. Likewise, some county seats may not be incorporated in their own right, but are located within incorporated municipalities. For example, Cape May Court House, New Jersey, though unincorporated, is a section of Middle Township, an incorporated municipality. In some states, often those that were among the original Thirteen Colonies, county seats include or formerly included \"Court House\" as part of their name, such as Spotsylvania Courthouse, Virginia.\nU.S. counties with more than one county seat.\nMost counties have only one county seat. However, some counties in Alabama, Arkansas, California, Georgia, Iowa, Kentucky, Massachusetts, Mississippi, Missouri, New Hampshire, New York, and Vermont have two or more county seats, usually located on opposite sides of the county. Examples include Harrison County, Mississippi, which has both Biloxi and Gulfport as county seats, and Hinds County, Mississippi, which has both Raymond and the state capital of Jackson. The practice of multiple county seat towns dates from the days when travel was difficult. There have been few efforts to eliminate the two-seat arrangement, since a county seat is a source of civic pride for the towns involved, along with providing employment opportunities. By virtue of the unique term endemic only to its state, Bennington County, Vermont is US only county to have two \"shire towns\", being Manchester, Vermont as the North Shire and the eponymously named Bennington as the South Shire; the rugged topography in the rural county necessitates two accessible seats of government.\nThere are 33 counties with multiple county seats in 11 states:\nOther variations.\nAlaska.\nAlaska is divided into boroughs rather than counties; the county seat in these case is referred to as the \"borough seat\"; this includes six consolidated city-borough governments (one of which is styled as a \"municipality\"). The Unorganized Borough, Alaska, which covers 49% of the state's area, has no borough government or borough seat. One borough, the Lake and Peninsula Borough, has its borough seat located in another borough, namely King Salmon in Bristol Bay Borough.\nLouisiana.\nIn Louisiana, which is divided into parishes rather than counties, county seats are referred to as \"parish seats\".\nNew England.\nIn New England, counties have served mainly as dividing lines for the states' judicial systems. Rhode Island has no county level of government and thus no county seats, and Massachusetts has dissolved many but not all of its county governments. In Vermont, Massachusetts, and Maine county government consists only of a Superior Court and Sheriff (as an officer of the court), both located in a designated \"shire town\". Bennington County, Vermont has two shire towns; the court for \"North Shire\" is in the shire town Manchester, and the Sheriff for the county and court for \"South Shire\" are in the shire town Bennington.\nIn 2024, Connecticut, which had not defined their counties for anything but statistical, historical and weather warning purposes since 1960, along with ending the use of county seats in particular, will fully transition with the permission of the United States Census Bureau to a system of councils of government for the purposes of boundary definition and as county equivalents.\nSouth Dakota.\nTwo counties in South Dakota, Oglala Lakota and Todd, have their county seat and government services centered in a neighboring county. Their county-level services are provided by Fall River County and Tripp County, respectively.\nVirginia.\nIn Virginia, a county seat may be an independent city surrounded by, but not part of, the county of which it is the administrative center; for example, Fairfax City is both the county seat of Fairfax County, Virginia and completely surrounded by Fairfax County, but the city is politically independent of the county. When the county seat is in the independent city, government offices such as the courthouse may be in the independent city under an agreement, such as in Albemarle, or may in be enclaves of the county surrounded by the independent city, such as in Fairfax. Others, such as Prince William, have the courthouse in an enclave surrounded by the independent city and have the county government, the Board of Supervisors, in a different part of the county, far from the county seat. The following counties have their county seat in an independent city:\nBedford was an independent city from 1968 to 2013, while also being the county seat of Bedford County. Bedford reverted to an incorporated town, and remains the county seat, though is now part of the county.\nLists of U.S. county seats by state.\nThe state with the most counties is Texas, with 254, and the state with the fewest counties is Delaware, with 3.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51510", "revid": "38874043", "url": "https://en.wikipedia.org/wiki?curid=51510", "title": "Silk", "text": "Fine, lustrous, natural fiber produced by various arthropods\nSilk is a natural protein fiber, some forms of which can be woven into textiles. The protein fiber of silk is composed mainly of fibroin. It is most commonly produced by certain insect larvae to form cocoons. The best-known silk is obtained from the cocoons of the larvae of the mulberry silkworm \"Bombyx mori\", which are reared in captivity (sericulture). The shimmery appearance of silk is due to the triangular prism-like structure of the silk fiber, which causes silk cloth to refract incoming light at different angles, thus producing different colors.\nHarvested silk is produced by numerous insects; generally, only the silk of various moth caterpillars has been used for textile manufacturing. Research into other types of silk, which differ at the molecular level, has been conducted. Silk is produced primarily by the larvae of insects undergoing complete metamorphosis, but some insects, such as webspinners and raspy crickets, produce silk throughout their lives. Silk production also occurs in hymenoptera (bees, wasps, and ants), silverfish, caddisflies, mayflies, thrips, leafhoppers, beetles, lacewings, fleas, flies, and midges. Other types of arthropods also produce silk, most notably various arachnids, such as spiders.\nEtymology.\nThe word silk comes from , from and , \"silken\", ultimately from the Chinese word \"s\u012b\" and other Asian sources\u2014compare Standard Chinese \"silk\", Mongolian .\nHistory.\nThe production of silk originated in central China in the Neolithic period, although it would eventually reach other places of the world ( culture, 4th millennium BC). Silk production remained confined to China until the Silk Road opened at some point during the latter part of the 1st millennium BC, though China maintained its virtual monopoly over silk production for another thousand years.\nWild silk.\nSeveral kinds of wild silk, produced by caterpillars other than the mulberry silkworm, have been known and spun in China, South Asia, and Europe since ancient times. However, the scale of production was always far smaller than for cultivated silks. There are several reasons for this: first, they differ from the domesticated varieties in colour and texture and are therefore less uniform; second, cocoons gathered in the wild have usually had the pupa emerge from them before being discovered so the silk thread that makes up the cocoon has been torn into shorter lengths; and third, many wild cocoons are covered in a mineral layer that prevents attempts to reel from them long strands of silk. Thus, the only way to obtain silk suitable for spinning into textiles in areas where commercial silks are not cultivated was by tedious and labor-intensive carding.\nSome natural silk structures have been used without being unwound or spun. Spider webs were used as a wound dressing in ancient Greece and Rome, and as a base for painting from the 16th century. Butterfly caterpillar nests were pasted together to make a fabric in the Aztec Empire.\nCommercial silks originate from reared silkworm pupae, which are bred to produce a white-colored silk thread with no mineral on the surface. The pupae are killed by either dipping them in boiling water before the adult moths emerge or by piercing them with a needle. These factors all contribute to the ability of the whole cocoon to be unravelled as one continuous thread, permitting a much stronger cloth to be woven from the silk.\nWild silks also tend to be more difficult to dye than silk from the cultivated silkworm. A technique known as demineralizing allows the mineral layer around the cocoon of wild silk moths to be removed, leaving only variability in color as a barrier to creating a commercial silk industry based on wild silks in the parts of the world where wild silk moths thrive, such as in Africa and South America.\nChina.\nSilk use in fabric was first developed in ancient China. The earliest evidence for silk is the presence of the silk protein fibroin in soil samples from two tombs at the neolithic site Jiahu in Henan, which date back about 8,500 years. The earliest surviving example of silk fabric dates from about 3630 BC, and was used as the wrapping for the body of a child at a Yangshao culture site in Qingtaicun near Xingyang, Henan.\nLegend gives credit for developing silk to a Chinese empress, Leizu (Hsi-Ling-Shih, Lei-Tzu). Silks were originally reserved for the emperors of China for their own use and gifts to others, but spread gradually through Chinese culture and trade both geographically and socially, and then to many regions of Asia. Because of its texture and lustre, silk rapidly became a popular luxury fabric in the many areas accessible to Chinese merchants. Silk was in great demand, and became a staple of pre-industrial international trade. Silk was also used as a surface for writing, especially during the Warring States period (475\u2013221 BCE). The fabric was light, it survived the damp climate of the Yangtze region, absorbed ink well, and provided a white background for the text. In July 2007, archaeologists discovered intricately woven and dyed silk textiles in a tomb in Jiangxi province, dated to the Eastern Zhou dynasty roughly 2,500 years ago. Although historians have suspected a long history of a formative textile industry in ancient China, this find of silk textiles employing \"complicated techniques\" of weaving and dyeing provides direct evidence for silks dating before the Mawangdui-discovery and other silks dating to the Han dynasty (202 BC \u2013 220 AD).\nSilk is described in a chapter of the \"Fan Shengzhi shu\" from the Western Han empire (202 BC \u2013 9 AD). There is a surviving calendar for silk production in an Eastern Han empire (25\u2013220 AD) document. The two other known works on silk from the Han empire are lost. The first evidence of the long distance silk trade is the finding of silk in the hair of an Egyptian mummy of the 21st dynasty, c.1070 BC. The silk trade reached as far as the Indian subcontinent, the Middle East, Europe, and North Africa. This trade was so extensive that the major set of trade routes between Europe and Asia came to be known as the Silk Road.\nThe Chinese emperors strove to keep knowledge of sericulture secret to maintain the Chinese monopoly. Nonetheless, sericulture partially reached Korea with technological aid from China around 200 BC, the ancient Kingdom of Khotan by AD 50, and India by AD 140.\nIn the ancient era, silk from China was the most lucrative and sought-after luxury item traded across the Eurasian continent, and many civilizations, such as the Persians, benefited economically from trade.\nJapan.\nArchaeological evidence indicates that sericulture has been practiced since the Yayoi period. The silk industry was dominant from the 1930s to 1950s, but is less common now.\nSilk from East Asia had declined in importance after silkworms were smuggled from China to the Byzantine Empire. However, in 1845, an epidemic of flacherie among European silkworms devastated the silk industry there. This led to a demand for silk from China and Japan, where as late as the nineteenth and early twentieth centuries, Japanese exports competed directly with Chinese in the international market in such low value-added, labor-intensive products as raw silk.\nBetween 1850 and 1930, raw silk ranked as the leading export for both countries, accounting for 20%\u201340% of Japan's total exports and 20%\u201330% of China's. Between the 1890s and the 1930s, Japanese silk exports quadrupled, making Japan the largest silk exporter in the world. This increase in exports was mostly due to the economic reforms during the Meiji period and the decline of the Qing dynasty in China, which led to rapid industrialization of Japan whilst the Chinese industries stagnated.\nDuring World War II, embargoes against Japan had led to adoption of synthetic materials such as nylon, which led to the decline of the Japanese silk industry and its position as the lead silk exporter of the world. Today, China exports the largest volume of raw silk in the world.\nIndia.\nSilk has a long history in India. It is known as \"Resham\" in eastern and north India, and \"Pattu\" in southern parts of India. Recent archaeological discoveries in Harappa and Chanhu-daro suggest that sericulture, employing wild silk threads from native silkworm species, existed in South Asia during the time of the Indus Valley civilisation (now in Pakistan and India) dating between 2450 BC and 2000 BC. Shelagh Vainker, a silk expert at the Ashmolean Museum in Oxford, who sees evidence for silk production in China \"significantly earlier\" than 2500\u20132000 BC, suggests, \"people of the Indus civilization either harvested silkworm cocoons or traded with people who did, and that they knew a considerable amount about silk.\"\nIndia is the second largest producer of silk in the world after China. About 97% of India's raw mulberry silk comes from six states, namely, Andhra Pradesh, Karnataka, Jammu and Kashmir, Tamil Nadu, Bihar, and West Bengal. North Bangalore, the upcoming site of a $20 million \"Silk City\" Ramanagara and Mysore, contribute to a majority of silk production in Karnataka.\nIn Tamil Nadu, mulberry cultivation is concentrated in the Coimbatore, Erode, Bhagalpuri, Tiruppur, Salem, and Dharmapuri districts. Hyderabad, Andhra Pradesh, and Gobichettipalayam, Tamil Nadu, were the first locations in India to have automated silk reeling units.\nIn the northeastern state of Assam, three different types of indigenous variety of silk are produced, collectively called Assam silk: Muga silk, Eri silk and Pat silk. Muga, the golden silk, and Eri are produced by silkworms that are native only to Assam. They have been reared since ancient times.\nThailand.\nSilk is produced year-round in Thailand by two types of silkworms, the cultured Bombycidae and wild Saturniidae. Most production is after the rice harvest in the southern and northeastern parts of the country. Women traditionally weave silk on hand looms and pass the skill on to their daughters, as weaving is considered to be a sign of maturity and eligibility for marriage. Thai silk textiles often use complicated patterns in various colours and styles. Most regions of Thailand have their own typical silks. A single thread filament is too thin to use on its own so women combine many threads to produce a thicker, usable fiber. They do this by hand-reeling the threads onto a wooden spindle to produce a uniform strand of raw silk. The process takes around 40 hours to produce a half kilogram of silk. Many local operations use a reeling machine for this task, but some silk threads are still hand-reeled. The difference is that hand-reeled threads produce three grades of silk: two fine grades that are ideal for lightweight fabrics, and a thick grade for heavier material.\nThe silk fabric is soaked in extremely cold water and bleached before dyeing to remove the natural yellow coloring of Thai silk yarn. To do this, skeins of silk thread are immersed in large tubs of hydrogen peroxide. Once washed and dried, the silk is woven on a traditional hand-operated loom.\nBangladesh.\nThe Rajshahi Division of northern Bangladesh is the hub of the country's silk industry. There are three types of silk produced in the region: mulberry, endi, and tassar. Bengali silk was a major item of international trade for centuries. It was known as Ganges silk in medieval Europe. Bengal was the leading exporter of silk between the 16th and 19th centuries.\nCentral Asia.\nThe 7th century CE murals of Afrasiyab in Samarkand, Sogdiana, show a Chinese Embassy carrying silk and a string of silkworm cocoons to the local Sogdian ruler.\nMiddle East.\nIn the Torah, a scarlet cloth item called in Hebrew \"sheni tola'at\" \u05e9\u05e0\u05d9 \u05ea\u05d5\u05dc\u05e2\u05ea \u2013 literally \"crimson of the worm\" \u2013 is described as being used in purification ceremonies, such as those following a leprosy outbreak (Leviticus 14), alongside cedar wood and hyssop (za'atar). Eminent scholar and leading medieval translator of Jewish sources and books of the Bible into Arabic, Rabbi Saadia Gaon, translates this phrase explicitly as \"crimson silk\" \u2013 \u05d7\u05e8\u05d9\u05e8 \u05e7\u05e8\u05de\u05d6 \u062d\u0631\u064a\u0631 \u0642\u0631\u0645\u0632.\nIn Islamic teachings, Muslim men are forbidden to wear silk. Many religious jurists believe the reasoning behind the prohibition lies in avoiding clothing for men that can be considered feminine or extravagant. There are disputes regarding the amount of silk a fabric can consist of (e.g., whether a small decorative silk piece on a cotton caftan is permissible or not) for it to be lawful for men to wear, but the dominant opinion of most Muslim scholars is that the wearing of silk by men is forbidden. Modern attire has raised a number of issues, including, for instance, the permissibility of wearing silk neckties, which are masculine articles of clothing.\nAncient Mediterranean.\nIn the \"Odyssey\", 19.233, when Odysseus, while pretending to be someone else, is questioned by Penelope about her husband's clothing, he says that he wore a shirt \"gleaming like the skin of a dried onion\" (varies with translations, literal translation here) which could refer to the lustrous quality of silk fabric.\nAristotle wrote of \"Coa vestis\", a wild silk textile from Kos.\nSea silk from certain large sea shells was also valued.\nThe Roman Empire knew of and traded in silk, and Chinese silk was the most highly priced luxury good imported by them. During the reign of emperor Tiberius, sumptuary laws were passed that forbade men from wearing silk garments, but these proved ineffectual. The Historia Augusta mentions that the third-century emperor Elagabalus was the first Roman to wear garments of pure silk, whereas it had been customary to wear fabrics of silk/cotton or silk/linen blends. Despite the popularity of silk, the secret of silk-making only reached Europe around AD 550, via the Byzantine Empire. Contemporary accounts state that monks working for the emperor Justinian I smuggled silkworm eggs to Constantinople from China inside hollow canes. All top-quality looms and weavers were located inside the Great Palace complex in Constantinople, and the cloth produced was used in imperial robes or in diplomacy, as gifts to foreign dignitaries. The remainder was sold at very high prices.\nMedieval and modern Europe.\nItaly was the most important producer of silk during the Medieval age. The first center to introduce silk production to Italy was the city of Catanzaro during the 11th century in the region of Calabria. The silk of Catanzaro supplied almost all of Europe and was sold in a large market fair in the port of Reggio Calabria, to Spanish, Venetian, Genovese, and Dutch merchants. Catanzaro became the lace capital of the world with a large silkworm breeding facility that produced all the laces and linens used in the Vatican. The city was world-famous for its fine fabrication of silks, velvets, damasks, and brocades.\nAnother notable center was the Italian city-state of Lucca which largely financed itself through silk-production and silk-trading, beginning in the 12th century. Other Italian cities involved in silk production were Genoa, Venice, and Florence. The Piedmont area of Northern Italy became a major silk producing area when water-powered silk throwing machines were developed.\nThe Silk Exchange in Valencia from the 15th century\u2014where previously in 1348 also \"perxal\" (percale) was traded as some kind of silk\u2014illustrates the power and wealth of one of the great Mediterranean mercantile cities.\nSilk was produced in and exported from the province of Granada, Spain, especially the Alpujarras region, until the Moriscos, whose industry it was, were expelled from Granada in 1571.\nSince the 15th century, silk production in France has been centered around the city of Lyon where many mechanic tools for mass production were first introduced in the 17th century.\nJames I attempted to establish silk production in England, purchasing and planting 100,000 mulberry trees, some on land adjacent to Hampton Court Palace, but they were of a species unsuited to the silk worms, and the attempt failed. In 1732 John Guardivaglio set up a silk throwing enterprise at Logwood mill in Stockport; in 1744, Burton Mill was erected in Macclesfield; and in 1753 Old Mill was built in Congleton. These three towns remained the centre of the English silk throwing industry until silk throwing was replaced by silk waste spinning. British enterprise also established silk filature in Cyprus in 1928. In England in the mid-20th century, raw silk was produced at Lullingstone Castle in Kent. Silkworms were raised and reeled under the direction of Zoe Lady Hart Dyke, later moving to Ayot St Lawrence in Hertfordshire in 1956.\nDuring World War II, supplies of silk for UK parachute manufacture were secured from the Middle East by Peter Gaddum.\nNorth America.\nWild silk taken from the nests of native butterfly and moth caterpillars was used by the Aztecs to make containers and as paper. Silkworms were introduced to Oaxaca from Spain in the 1530s and the region profited from silk production until the early 17th century, when the king of Spain banned export to protect Spain's silk industry. Silk production for local consumption has continued until the present day, sometimes spinning wild silk.\nKing James I introduced silk-growing to the British colonies in America around 1619, ostensibly to discourage tobacco planting. The Shakers in Kentucky adopted the practice.\nThe history of industrial silk in the United States is largely tied to several smaller urban centers in the Northeast region. Beginning in the 1830s, Manchester, Connecticut emerged as the early center of the silk industry in America, when the Cheney Brothers became the first in the United States to properly raise silkworms on an industrial scale; today the Cheney Brothers Historic District showcases their former mills. With the mulberry tree craze of that decade, other smaller producers began raising silkworms. This economy particularly gained traction in the vicinity of Northampton, Massachusetts and its neighboring Williamsburg, where a number of small firms and cooperatives emerged. Among the most prominent of these was the cooperative utopian Northampton Association for Education and Industry, of which Sojourner Truth was a member. Following the destructive Mill River Flood of 1874, one manufacturer, William Skinner, relocated his mill from Williamsburg to the then-new city of Holyoke. Over the next 50 years he and his sons would maintain relations between the American silk industry and its counterparts in Japan, and expanded their business to the point that by 1911, the Skinner Mill complex contained the largest silk mill under one roof in the world, and the brand Skinner Fabrics had become the largest manufacturer of silk satins internationally. Other efforts later in the 19th century would also bring the new silk industry to Paterson, New Jersey, with several firms hiring European-born textile workers and granting it the nickname \"Silk City\" as another major center of production in the United States.\nWorld War II interrupted the silk trade from Asia, and silk prices increased dramatically. U.S. industry began to look for substitutes, which led to the use of synthetics such as nylon. Synthetic silks have also been made from lyocell, a type of cellulose fiber, and are often difficult to distinguish from real silk (see spider silk for more on synthetic silks).\nMalaysia.\nIn Terengganu, which is now part of Malaysia, a second generation of silkworm was being imported as early as 1764 for the country's silk textile industry, especially songket. However, since the 1980s, Malaysia is no longer engaged in sericulture but does plant mulberry trees.\nVietnam.\nIn Vietnamese legend, silk appeared in the first millennium AD and is still being woven today.\nProduction process.\nThe process of silk production is known as sericulture. The entire production process of silk can be divided into several steps. Extracting raw silk starts by cultivating the silkworms on mulberry leaves. Once the worms start pupating in their cocoons, they are dissolved in boiling water in order for individual long fibers to be extracted and fed into the spinning reel.\nTo produce 1\u00a0kg of silk, 104\u00a0kg of mulberry leaves must be eaten by 3000 silkworms. It takes about 5000 silkworms to make a pure silk kimono. The most important silk producers are China (54%) and India (14%). Other statistics:\nThe environmental impact of silk production is potentially large when compared with other natural fibers. A life-cycle assessment of Indian silk production shows that the production process has a large carbon and water footprint, mainly due to the fact that it is an animal-derived fiber and more inputs such as fertilizer and water are needed per unit of fiber produced.\nProperties.\nPhysical properties.\nSilk fibers from the \"Bombyx mori\" silkworm have a triangular cross section with rounded corners, 5\u201310\u00a0\u03bcm wide. The fibroin-heavy chain is composed mostly of beta-sheets, due to a 59-mer amino acid repeat sequence with some variations. The flat surfaces of the fibrils reflect light at many angles, giving silk a natural sheen. The cross-section from other silkworms can vary in shape and diameter: crescent-like for \"Anaphe\" and elongated wedge for \"tussah\". Silkworm fibers are naturally extruded from two silkworm glands as a pair of primary filaments (brin), which are stuck together, with sericin proteins that act like glue, to form a bave. Bave diameters for tussah silk can reach 65\u00a0\u03bcm. See cited reference for cross-sectional SEM photographs.\nSilk has a smooth, soft texture that is not slippery, unlike many synthetic fibers.\nSilk is one of the strongest natural fibers, but it loses up to 20% of its strength when wet. It has a good moisture regain of 11%. Its elasticity is moderate to poor: if elongated even a small amount, it remains stretched. It can be weakened if exposed to too much sunlight. It may also be attacked by insects, especially if left dirty.\nOne example of the durable nature of silk over other fabrics is demonstrated by the recovery in 1840 of silk garments from a wreck of 1782: 'The most durable article found has been silk; for besides pieces of cloaks and lace, a pair of black satin breeches, and a large satin waistcoat with flaps, were got up, of which the silk was perfect, but the lining entirely gone ... from the thread giving way ... No articles of dress of woollen cloth have yet been found.'\nSilk is a poor conductor of electricity and thus susceptible to static cling. Silk has a high emissivity for infrared light, making it feel cool to the touch.\nUnwashed silk chiffon may shrink up to 8% due to a relaxation of the fiber macrostructure, so silk should either be washed prior to garment construction, or dry cleaned. Dry cleaning may still shrink the chiffon up to 4%. Occasionally, this shrinkage can be reversed by a gentle steaming with a press cloth. There is almost no gradual shrinkage nor shrinkage due to molecular-level deformation.\nNatural and synthetic silk is known to manifest piezoelectric properties in proteins, probably due to its molecular structure.\nSilkworm silk was used as the standard for the denier, a measurement of linear density in fibers. Silkworm silk therefore has a linear density of approximately 1 den, or 1.1 dtex.\nChemical properties.\nSilk emitted by the silkworm consists of two main proteins, sericin and fibroin, fibroin being the structural center of the silk, and sericin being the sticky material surrounding it. Fibroin is made up of the amino acids Gly-Ser-Gly-Ala-Gly-Ala and forms beta pleated sheets. Hydrogen bonds form between chains, and side chains are oriented above and below the plane of the hydrogen bond network.\nThe high proportion (50%) of glycine allows tight packing. This is because glycine has no side chain and is therefore unencumbered by steric strain. The addition of alanine and serine makes the fibres strong and resistant to breaking. This tensile strength is due to the many interceded hydrogen bonds, and when stretched the force is applied to these numerous bonds and they do not break.\nSilk resists most mineral acids, except for sulfuric acid, which dissolves it. It is yellowed by perspiration. Chlorine bleach will also destroy silk fabrics.\nVariants.\nRegenerated silk fiber.\nRSF is produced by chemically dissolving silkworm cocoons, leaving their molecular structure intact. The silk fibers dissolve into tiny thread-like structures known as microfibrils. The resulting solution is extruded through a small opening, causing the microfibrils to reassemble into a single fiber. The resulting material is reportedly twice as stiff as silk.\nApplications.\nClothing.\nSilk's absorbency makes it comfortable to wear in warm weather and while active. Its low conductivity keeps warm air close to the skin during cold weather. It is often used for clothing such as shirts, ties, blouses, formal dresses, high-fashion clothes, lining, lingerie, pajamas, robes, dress suits, sun dresses, and traditional Asian clothing. Silk is also excellent for insect-proof clothing, protecting the wearer from mosquitoes and horseflies.\nFabrics that are often made from silk include satin, charmeuse, habutai, chiffon, taffeta, cr\u00eape de chine, dupioni, noil, tussah, and shantung, among others.\nFurniture.\nSilk's attractive lustre and drape makes it suitable for many furnishing applications. It is used for upholstery, wall coverings, window treatments (if blended with another fiber), rugs, bedding, and wall hangings.\nIndustry.\nSilk had many industrial and commercial uses, such as in parachutes, bicycle tires, comforter filling, and artillery gunpowder bags.\nMedicine.\nA special manufacturing process removes the outer sericin coating of the silk, which makes it suitable as non-absorbable surgical sutures. Sometimes wearing silk is suggested for people with atopic dermatitis but, even though it is safe for the skin, it does not improve symptoms of the condition.\nBiomaterial.\nSilk began to serve as a biomedical material for sutures in surgeries as early as the second century CE. In the past 30 years, it has been widely studied and used as a biomaterial due to its mechanical strength, biocompatibility, tunable degradation rate, ease to load cellular growth factors (for example, BMP-2), and its ability to be processed into several other formats such as films, gels, particles, and scaffolds. Silks from \"Bombyx mori\", a kind of cultivated silkworm, are the most widely investigated silks.\nSilks derived from \"Bombyx mori\" are generally made of two parts: the silk fibroin fiber which contains a light chain of 25\u00a0kDa and a heavy chain of 350\u00a0kDa (or 390\u00a0kDa) linked by a single disulfide bond and a glue-like protein, sericin, comprising 25 to 30 percentage by weight. Silk fibroin contains hydrophobic beta sheet blocks, interrupted by small hydrophilic groups. The beta-sheets contribute much to the high mechanical strength of silk fibers, which achieves 740\u00a0MPa, tens of times that of poly(lactic acid) and hundreds of times that of collagen. This impressive mechanical strength has made silk fibroin very competitive for applications in biomaterials. Indeed, silk fibers have found their way into tendon tissue engineering, where mechanical properties matter greatly. In addition, mechanical properties of silks from various kinds of silkworms vary widely, which provides more choices for their use in tissue engineering.\nMost products fabricated from regenerated silk are weak and brittle, with only \u22481\u20132% of the mechanical strength of native silk fibers due to the absence of appropriate secondary and hierarchical structure,\nBiocompatibility.\nBiocompatibility, i.e., to what level the silk will cause an immune response, is a critical issue for biomaterials. The issue arose during its increasing clinical use. Wax or silicone is usually used as a coating to avoid fraying and potential immune responses when silk fibers serve as suture materials. Although the lack of detailed characterization of silk fibers, such as the extent of the removal of sericin, the surface chemical properties of coating material, and the process used, make it difficult to determine the real immune response of silk fibers in literature, it is generally believed that sericin is the major cause of immune response. Thus, the removal of sericin is an essential step to assure biocompatibility in biomaterial applications of silk. However, further research fails to prove clearly the contribution of sericin to inflammatory responses based on isolated sericin and sericin based biomaterials. In addition, silk fibroin exhibits an inflammatory response similar to that of tissue culture plastic in vitro when assessed with human mesenchymal stem cells (hMSCs) or lower than collagen and PLA when implant rat MSCs with silk fibroin films in vivo. Thus, appropriate degumming and sterilization will assure the biocompatibility of silk fibroin, which is further validated by in vivo experiments on rats and pigs. There are still concerns about the long-term safety of silk-based biomaterials in the human body in contrast to these promising results. Even though silk sutures serve well, they exist and interact within a limited period depending on the recovery of wounds (several weeks), much shorter than that in tissue engineering. Another concern arises from biodegradation because the biocompatibility of silk fibroin does not necessarily assure the biocompatibility of the decomposed products. In fact, different levels of immune responses and diseases have been triggered by the degraded products of silk fibroin.\nBiodegradability.\nBiodegradability (also known as biodegradation)\u2014the ability to be disintegrated by biological approaches, including bacteria, fungi, and cells\u2014is another significant property of biomaterials. Biodegradable materials can minimize the pain of patients from surgeries, especially in tissue engineering, since there is no need for surgery in order to remove the implanted scaffold. Wang et al. showed the in vivo degradation of silk via aqueous 3D scaffolds implanted into Lewis rats. Enzymes are the means used to achieve degradation of silk in vitro. Protease XIV from Streptomyces griseus and \u03b1-chymotrypsin from bovine pancreases are two popular enzymes for silk degradation. In addition, gamma radiation, as well as cell metabolism, can also regulate the degradation of silk.\nCompared with synthetic biomaterials such as polyglycolides and polylactides, silk is advantageous in some aspects of biodegradation. The acidic degraded products of polyglycolides and polylactides will decrease the pH of the ambient environment and thus adversely influence the metabolism of cells, which is not an issue for silk. In addition, silk materials can retain strength over a desired period from weeks to months on an as-needed basis, by mediating the content of beta sheets.\nGenetic modification.\nGenetic modification of domesticated silkworms has been used to alter the composition of the silk. As well as possibly facilitating the production of more useful types of silk, this may allow other industrially or therapeutically useful proteins to be made by silkworms.\nCultivation.\nSilk moths lay eggs on specially prepared paper. The eggs hatch and the caterpillars (silkworms) are fed fresh mulberry leaves. After about 35 days and 4 moltings, the caterpillars are 10,000 times heavier than when hatched and are ready to begin spinning a cocoon. A straw frame is placed over the tray of caterpillars, and each caterpillar begins spinning a cocoon by moving its head in a pattern. Two glands produce liquid silk and force it through openings in the head called spinnerets. Liquid silk is coated in sericin, a water-soluble protective gum, and solidifies on contact with the air. Within 2\u20133 days, the caterpillar spins about of filament and is completely encased in a cocoon. The silk farmers then heat the cocoons to kill them, leaving some to metamorphose into moths to breed the next generation of caterpillars. Harvested cocoons are then soaked in boiling water to soften the sericin holding the silk fibers together in a cocoon shape. The fibers are then unwound to produce a continuous thread. Since a single thread is too fine and fragile for commercial use, anywhere from three to ten strands are spun together to form a single thread of silk.\nAnimal rights.\nAs the process of harvesting the silk from the cocoon kills the larvae by boiling, sericulture has been criticized by animal welfare activists, including People for the Ethical Treatment of Animals (PETA), who urge people not to buy silk items.\nMahatma Gandhi was critical of silk production because of his Ahimsa (non-violent) philosophy, which led to the promotion of cotton and Ahimsa silk, a type of wild silk made from the cocoons of wild and semi-wild silk moths.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51511", "revid": "10412272", "url": "https://en.wikipedia.org/wiki?curid=51511", "title": "Prince", "text": "Son of a ruler or a title of nobility\nA prince is a male ruler (ranked below a king, grand prince, and grand duke) or a male member of a monarch's or former monarch's family. \"Prince\" is also a title of nobility (often highest), often hereditary, in some European states. The female equivalent is a princess. The English word derives, via the French word \"prince\", from the Latin noun , from (first) and (head), meaning \"the first, foremost, the chief, most distinguished, noble ruler, prince\".\nIn a related sense, now not commonly used, all more or less sovereign rulers over a state, including kings, were \"princes\" in the language of international politics. They normally had another title, for example king or duke. Many of these were Princes of the Holy Roman Empire.\nHistorical background.\nThe Latin word (older Latin *pr\u012bsmo-kaps, lit.\u2009'the one who takes the first [place/position]'), became the usual title of the informal leader of the Roman senate some centuries before the transition to empire, the \"princeps senatus\".\nEmperor Augustus established the formal position of monarch on the basis of principate, not dominion. He also tasked his grandsons as summer rulers of the city when most of the government were on holiday in the country or attending religious rituals, and, for that task, granted them the title of princeps.\nThe title has generic and substantive meanings:\nPrince as generic term for ruler.\nThe original but now less common use of the word was the application of the Latin word \"\", from late Roman law and the classical system of government that eventually gave way to the European feudal society. In this sense, a prince is a ruler of a territory that is sovereign or quasi-sovereign, i.e., exercising substantial (though not all) prerogatives associated with monarchs of independent nations, such as the immediate states within the historical boundaries of the Holy Roman Empire. In medieval and early modern Europe, there were as many as two hundred such territories, especially in Italy, Germany, and Gaelic Ireland. In this sense, \"prince\" is used of any and all rulers, regardless of actual title or precise rank. This is the Renaissance use of the term found in Niccol\u00f2 Machiavelli's famous work, \"Il Principe\". It is also used in this sense in the United States Declaration of Independence.\nAs a title, by the end of the medieval era, \"prince\" was borne by rulers of territories that were either substantially smaller than those of or exercised fewer of the rights of sovereignty than did emperors and kings. A lord of even a quite small territory might come to be referred to as a \"prince\" before the 13th century, either from translations of a native title into the Latin \"\" (as for the hereditary ruler of Wales) or when the lord's territory was allodial. The lord of an allodium owned his lands and exercised prerogatives over the subjects in his territory absolutely, owing no feudal homage or duty as a vassal to a liege lord, nor being subject to any higher jurisdiction. Most small territories designated as principalities during feudal eras were allodial, e.g. the Princedom of Dombes.\nLords who exercised lawful authority over territories and people within a feudal hierarchy were also sometimes regarded as \"princes\" in the general sense, especially if they held the rank of count or higher. This is attested in some surviving styles for e.g., British earls, marquesses, and dukes are still addressed by the Crown on ceremonial occasions as \"high and noble princes\" (cf. Royal and noble styles).\nIn parts of the Holy Roman Empire in which primogeniture did not prevail (e.g., Germany), all legitimate agnates had an equal right to the family's hereditary titles. While titles such as emperor, king, and elector could only be legally occupied by one dynast at a time, holders of such other titles as duke, margrave, landgrave, count palatine, and prince could only differentiate themselves by adding the name of their appanage to the family's original title. This tended to proliferate unwieldy titles (e.g. Princess Katherine of Anhalt-Zerbst; Karl, Count Palatine of Zweibr\u00fccken-Neukastell-Kleeburg; or Prince Christian Charles of Schleswig-Holstein-Sonderburg-Pl\u00f6n-Norburg) and, as agnatic primogeniture gradually became the norm in the Holy Roman Empire by the end of the 18th century, another means of distinguishing the monarch from other members of his dynasty became necessary. Gradual substitution of the title of \"Prinz\" for the monarch's title of \"F\u00fcrst\" occurred, and became customary for cadets in all German dynasties except in the grand duchies of Mecklenburg and Oldenburg. Both and \"\" are translated into English as \"prince\", but they reflect not only different but mutually exclusive concepts.\nThis distinction had evolved before the 18th century (although Liechtenstein long remained an exception, with cadets and females using into the 19th century) for dynasties headed by a \"F\u00fcrst\" in Germany. The custom spread through the Continent to such an extent that a renowned imperial general who belonged to a cadet branch of a reigning ducal family, remains best known to history by the generic dynastic title, \"Prince Eugene of Savoy\". Note that the princely title was used as a prefix to his Christian name, which also became customary.\nCadets of France's other affected similar usage under the Bourbon kings. Always facing the scepticism of Saint-Simon and like-minded courtiers, these quasi-royal aristocrats' assumption of the princely title as a personal, rather than territorial, designation encountered some resistance. In writing \"Histoire Genealogique et Chonologique\", P\u00e8re Anselme accepts that, by the end of the 17th century, the heir apparent to the House of La Tour d'Auvergne's sovereign duchy bears the title \"Prince de Bouillon\", but he would record in 1728 that the heir's \"La Tour\" cousin, the Count of Oliergues, is \"\"known as\" the Prince Frederick\" (\"\"dit\" le prince Fr\u00e9d\u00e9ric\").\nThe post-medieval rank of (princely count) embraced but elevated the German equivalent of the intermediate French, English and Spanish nobles. In the Holy Roman Empire, these nobles rose to dynastic status by preserving from the Imperial crown ( after the Peace of Westphalia in 1648) the exercise of such sovereign prerogatives as the minting of money; the muster of military troops and the right to wage war and contract treaties; local judicial authority and constabulary enforcement; and the habit of inter-marrying with sovereign dynasties. By the 19th century, cadets of a ' would become known as '.\nPrinces consort and princes of the blood.\nThe husband of a queen regnant is usually titled \"prince consort\" or simply \"prince\", whereas the wives of male monarchs take the female equivalent (e.g., empress, queen) of their husband's title. In Brazil, Portugal and Spain, however, the husband of a female monarch is accorded the masculine equivalent of her title (e.g., emperor, king), at least after he fathered her heir. In previous epochs, husbands of queens regnant were often deemed entitled to the crown matrimonial, sharing their consorts' regnal title and rank .\nHowever, in cultures which allow the ruler to have several wives (e.g., four in Islam) or official concubines (e.g., Imperial China, Ottoman Empire, Thailand, the Zulu monarchy), these women, sometimes collectively referred to as a harem, often have specific rules determining their relative hierarchy and a variety of titles, which may distinguish between those whose offspring can be in line for the succession or not, or specifically who is mother to the heir to the throne.\nTo complicate matters, the style \"His/Her (Imperial/Royal) Highness\", a prefix often accompanying the title of a dynastic prince, may be awarded/withheld separately (as a compromise or consolation prize, in some sense, e.g., Duke of C\u00e1diz, Duchess of Windsor, Princesse de R\u00e9thy, Prince d'Orl\u00e9ans-Braganza).\nAlthough the arrangement set out above is the one that is most commonly understood, there are also different systems. Depending on country, epoch, and translation, other usages of \"prince\" are possible.\nForeign-language titles such as , , , (non-reigning descendant of a reigning monarch), Ukrainian and , etc., are usually translated as \"prince\" in English.\nSome princely titles are derived from those of national rulers, such as tsarevich from tsar. Other examples are (all using the Persian patronymic suffix \"-zada\", meaning \"son, descendant\"). However, some princely titles develop in unusual ways, such as adoption of a style for dynasts which is not pegged to the ruler's title, but rather continues an old tradition (e.g., \"grand duke\" in Romanov Russia or \"archduke\" in Habsburg Austria), claims dynastic succession to a lost monarchy (e.g. for the La Tr\u00e9mo\u00eflle heirs to the Neapolitan throne), or descends from a ruler whose princely title or sovereign status was not de jure hereditary, but attributed to descendants as an international courtesy, (e.g., Bibesco-Bassaraba de Brancovan, Poniatowski, Ypsilanti).\nSpecific titles.\nIn some dynasties, a specific style other than prince has become customary for dynasts, such as in the House of Capet, and . was borne by children of the monarch other than the heir apparent in all of the Iberian monarchies. Some monarchies used a specific princely title for their heirs, such as Prince of Asturias in Spain, Prince of Brazil in Portugal, and (customarily but not automatically) Prince of Wales in the United Kingdom.\nSometimes a specific title is commonly used by various dynasties in a region, e.g. Mian in various of the Punjabi princely Hill States (lower Himalayan region in British India).\nEuropean dynasties usually awarded appanages to princes of the blood, typically attached to a feudal noble title, such as Prince of Orange in the Netherlands, Britain's royal dukes, the in France, the Count of Flanders in Belgium, and the Count of Syracuse in Sicily. Sometimes appanage titles were princely, e.g. Prince of Achaia (Courtenay), (Bourbon), Prince of Carignan (Savoy), but it was that their owners were of princely \"rank\" rather than that they held a princely \"title\" which was the source of their pre-eminence.\nFor the often specific terminology concerning an heir apparent, see Crown prince.\nPrince as a substantive title.\nOther princes derive their title not from dynastic membership as such, but from inheritance of a title named for a specific and historical territory. The family's possession of prerogatives or properties in that territory might be long past. Such were most of the \"princedoms\" of France's \"ancien r\u00e9gime\", so resented for their pretentiousness in the memoirs of Saint-Simon. These included the princedoms of Arches-Charleville, Boisbelle-Henrichemont, Chalais, Ch\u00e2teau-Regnault, Gu\u00e9m\u00e9n\u00e9e, Martigues, Merc\u0153ur, Sedan, Talmond, Tingrey, and the \"kingship\" of Yvetot, among others.\nPrince as a reigning monarch.\nA prince or princess who is the head of state of a territory that has a monarchy as a form of government is a reigning prince.\nMicronations.\nIn the same tradition, some self-proclaimed monarchs of so-called micronations style themselves as princes:\nPrince exercising head of state's authority.\nVarious monarchies provide for different modes in which princes of the dynasty can temporarily or permanently share in the style and/or office of the monarch, e.g. as regent or viceroy.\nThough these offices may not be reserved legally for members of the ruling dynasty, in some traditions they are filled by dynasts, a fact which may be reflected in the style of the office, e.g. \"prince-president\" for Napoleon III as French head of state but not yet emperor, or \"prince-lieutenant\" in Luxembourg, repeatedly filled by the crown prince before the grand duke's abdication, or in form of .\nSome monarchies even have a practice in which the monarch can formally abdicate in favour of his heir and yet retain a kingly title with executive power, e.g. \"Maha Upayuvaraja\" (Sanskrit for \"Great Joint King\" in Cambodia), though sometimes also conferred on powerful regents who exercised executive powers.\nNon-dynastic princes.\nIn several countries of the European continent, such as France, prince can be an aristocratic title of someone having a high rank of nobility or as lord of a significant fief, but not ruling any actual territory and without any necessary link to the royal family, such as Andora, which makes it difficult to compare with the British system of royal princes.\nFrance and the Holy Roman Empire.\nThe kings of France started to bestow the style of prince, as a title among the nobility, from the 16th century onwards. These titles were created by elevating a to the nominal status of a principality\u2014although prerogatives of sovereignty were never conceded in the letters patent. Princely titles self-assumed by the and by the were generally tolerated by the king and used at the royal court, outside the Parlement of Paris. These titles held no official place in the hierarchy of the nobility, but were often treated as ranking just below ducal peerages, since they were often inherited (or assumed) by ducal heirs:\nThis can even occur in a monarchy within which an identical but real and substantive feudal title exists, such as \"\" in German. An example of this is:\nSpain, France and Netherlands.\nIn other cases, such titular princedoms are created in chief of an event, such as a treaty or a victory. Examples include:\nEastern Europe.\nIn the former Polish-Lithuanian Commonwealth, the titles of prince dated either to the times before the Union of Lublin or were granted to Polish nobles by foreign monarchs, as the law in Poland forbade the king from dividing nobility by granting them hereditary titles: see The Princely Houses of Poland.\nIn Ukraine, landlords and rulers of Kievan Rus' were called \u043a\u043d\u044f\u0437\u044c (\"knjaz\u02b9\"), translated as \"prince\". Similarly, foreign titles of \"prince\" were translated as \"knyaz\" in Ukrainian (e. g. Ivan Mazepa, \"knyaz of Holy Roman Empire\"). Princes of Rurik Dynasty obeyed their oldest brother, who was taking the title of Grand Prince of Kiev. In 14th their ruling role was taken by Lithuanian princes, which used the title of Grand Prince of Lithuania and Ruthenia. With the rise of cossacks, many former Ukrainian princes were incorporated into the new Cossack nobility.\nIn the Russian system, \"knyaz was\" the highest degree of official nobility. Members of older dynasties, whose realms were eventually annexed to the Russian Empire, were also accorded the title of \"\"\u2014sometimes after first being allowed to use the higher title of tsarevich (e.g. the Princes Gruzinsky and Sibirsky).\nTitle in various European traditions and languages.\nIn the Netherlands, Belgium, France, Italy, Ukraine, Japan, Lithuania, Portugal, Russia, Spain, Belarus and Hungary the title of \"prince\" has also been used as the highest title of nobility (without membership in a ruling dynasty), above the title of \"duke\", while the same usage (then as \"F\u00fcrst\") has occurred in Germany and Austria but then one rank below the title of \"duke\" and above \"count\".\nIn each case, the title is followed (when available) by the female form and then (not always available, and obviously rarely applicable to a prince of the blood without a principality) the name of the territory associated with it, each separated by a slash. If a second title (or set) is also given, then that one is for a Prince of the blood, the first for a principality. Be aware that the absence of a separate title for a prince of the blood may not always mean no such title exists; alternatively, the existence of a word does not imply there is also a reality in the linguistic territory concerned; it may very well be used exclusively to render titles in other languages, regardless whether there is a historical link with any (which often means that linguistic tradition is adopted)\nEtymologically, we can discern the following traditions (some languages followed a historical link, e.g. within the Holy Roman Empire, not their language family; some even fail to follow the same logic for certain other aristocratic titles):\nTitle in non-European traditions and languages.\nThe below is essentially the story of European, Christian dynasties and other nobility, also 'exported' to their colonial and other overseas territories and otherwise adopted by rather westernized societies elsewhere (e.g. Haiti).\nApplying these essentially western concepts, and terminology, to other cultures even when they don't do so, is common but in many respects rather dubious. Different (historical, religious...) backgrounds have also begot significantly different dynastic and nobiliary systems, which are poorly represented by the 'closest' western analogy.\nIt therefore makes sense to treat these per civilization.\nBrunei.\nIt's crucial to use the proper title while speaking to members of the royal family because Brunei is an absolute monarchy, and inappropriate use might be uncomfortable. The heir apparent and crown prince, styled as \"Duli Yang Teramat Mulia Paduka Seri\" (His Royal Highness), is officially known as \"Pengiran Muda Mahkota\" (Crown Prince); A blood prince is officially known as \"Pengiran Muda\" (Prince); their names are styled differently: If they do not have additional titles, the Sultan's sons are addressed as \"Duli Yang Teramat Mulia Paduka Seri\" (His Royal Highness); The Pengiran Muda Mahkota's sons are addressed as \"Yang Teramat Mulia\" (His Royal Highness).\nChina.\nBefore Qin dynasty, prince (in the sense of royal family member) had no special title. Princes of the Zhou dynasty were specifically referred to as \"Wangzi\" (\u738b\u5b50) and \"Wangsun\" (\u738b\u5b6b), which mean \"son of the king\" and \"grandson of the king,\" while princes of the vassal states were referred to as \"Gongzi\" (\u516c\u5b50) and \"Gongsun\" (\u516c\u5b6b), which mean \"son of the lord\" and \"grandson of the lord,\" respectively. Sons of the vassals may receive nobility titles like Jun (\u541b), Qing (\u537f), Daifu (\u5927\u592b) and Shi (\u4ed5).\nSince Han dynasty, royal family members were entitled \"Wang\" (, lit. King), the former highest title which was then replaced by \"Huang Di\" (, lit. Emperor). Since Western Jin, the \"Wang\" rank was divided into two ranks, \"Qin Wang\" (, lit. King of the Blood) and \"Jun Wang\" (, lit. King of the Commandery). Only family of the Emperor can be entitled \"Qin Wang\", so prince is usually translated as \"Qin Wang\", e.g. \u83f2\u5229\u666e\u89aa\u738b (Prince Philip). For the son of the ruler, prince is usually translated as \"Huang Zi\" (, lit. Son of the Emperor) or \"Wang Zi\" ( lit., Son of the King), e.g. \u67e5\u723e\u65af\u738b\u5b50 (Prince Charles).\nAs a title of nobility, prince can be translated as \"Qin Wang\" according to tradition, \"Da Gong\" (\u5927\u516c, lit., Grand Duke) if one want to emphasize that it is a very high rank but below the King (\"Wang\"), or just \"Zhu Hou\" (, lit. princes) which refers to princes of all ranks in general. For example, \u6469\u7d0d\u54e5\u89aa\u738b (Prince of Monaco).\nJapan.\nIn Japan, the title \"K\u014dshaku\" () was used as the highest title of \"Kazoku\" ( Japanese modern nobility) before the present constitution. \"K\u014dshaku\", however, is more commonly translated as \"Duke\" to avoid confusion with the following royal ranks in the Imperial Household: \"Shinn\u014d\" ( literally, Prince of the Blood); \"Naishinn\u014d\" ( lit., Princess of the Blood in her own right); and \"Shinn\u014dhi\" lit., Princess Consort); or \"\u014c\" ( lit., Prince); \"Jyo-\u014c\" ( lit., Princess (in her own right)); and \"\u014chi\" ( lit., Princess Consort). The former is the higher title of a male member of the Imperial family while the latter is the lower.\nKorea.\nIn the Joseon Dynasty, the title \"Prince\" was used for the king's male-line descendants. The title was divided into two: the king's legitimate son used the title \"daegun\" (\ub300\uad70, \u5927\u541b, literally \"grand prince\"), but any other male royals used the title \"gun\" (\uad70, \u541b, lit. \"prince\"). These included the descendants of the king up to the grandsons of illegitimate sons of the king and the crown prince, and up to the great grandsons of \"daegun\"s, with other royals being able to be named \"gun\" if they reached the second rank. But the title of \"gun\" wasn't limited to the royal family. It was also granted as an honorary title to the king's father-in-law and to \"gongsin\" (\uacf5\uc2e0, \u529f\u81e3, lit. \"servant of merit\") and was only conditionally hereditary for \"gongsin\"s.\nAs noble titles no longer exist in modern Korea, the English word \"Prince\" is now usually translated as (\uc655\uc790, \u738b\u5b50, lit. \"king's son\"), referring to princes from non-Korean royal families. Princes and principalities in continental Europe are almost always confused with dukes and duchies in Korean speech, both being translated as \"gong\" (\uacf5, \u516c, lit. \"duke\") and (\uacf5\uad6d, \u516c\u570b, lit. \"duchy\").\nSri Lanka.\nThe title 'Prince' was used for the King's son in Sinhalese generation in Sri Lanka.\nIndia.\nIn the former Empire of India, during the British Colonial Era, the title of a Prince was conferred upon by the Emperor to the hereditary Indian rulers of the numerous principalities of varying sizes in the empire called \"princely states\", who ruled their territories in the name of the Emperor of India, who was also simultaneously the Monarch of the United Kingdom and the Dominions. They acknowledged the Emperor as their imperial sovereign till the empire's dissolution in 1947 and subsequently acceded to the newly formed Union of India and the Dominion of Pakistan between 1947 and 1949.\nIndochina.\n\"See\" Cambodia, Vietnam, and Laos\nPhilippines.\n\"See\" Principalia, the Sultanate of Maguindanao and the Sultanate of Sulu.\nThailand.\nIn Thailand (formerly Siam), the title of Prince was divided into three classes depending on the rank of their mothers. Those who were born of a king and had a royal mother (a queen or princess consort) are titled \"Chaofa Chai\" (: literally, \"Male Celestial Lord\"). Those born of a king and a commoner, or children of Chaofas and royal consorts, are tilted \"Phra Ong Chao\" (\u0e1e\u0e23\u0e30\u0e2d\u0e07\u0e04\u0e4c\u0e40\u0e08\u0e49\u0e32). The children of Chaofas and commoners, or children of Phra Ong Chaos, are titled \"Mom Chao\" (\u0e2b\u0e21\u0e48\u0e2d\u0e21\u0e40\u0e08\u0e49\u0e32), abbreviated as M.C. (or \u0e21.\u0e08.).\nAfrican traditions.\nA Western model was sometimes copied by emancipated colonial regimes (e.g. Bokassa I's short-lived Central-African Empire in Napoleonic fashion). Otherwise, most of the styles for members of ruling families do not lend themselves well to English translation. Nonetheless, in general the princely style has gradually replaced the colonialist title of \"chief\", which does not particularly connote dynastic rank to Westerners, e.g. Swazi Royal Family and Zulu Royal Family. Nominally ministerial chiefly titles, such as the Yoruba \"Oloye\" and the Zulu \"InDuna\", still exist as distinct titles in kingdoms all over Africa.\nTitle in religious traditions.\nIn states with an element of theocracy, this can affect princehood in several ways, such as the style of the ruler (e.g. with a secondary title meaning son or servant of a named divinity), but also the mode of succession (even reincarnation and recognition).\nChristianity.\nCertain religious offices may be considered of princely rank, or imply comparable temporal rights. Pope, Hereditary Prince-Cardinals, Cardinals, Prince-Lord Bishops, Prince Bishops, Lord Bishops, Prince-Provost, and Prince-abbots are referred to as Princes of the Church.\nAlso, in Christianity, Jesus Christ is sometimes referred to as the \"Prince of Peace\". Other titles for Jesus Christ are \"Prince of Princes\", \"Prince of the Covenant\", \"Prince of Life\", and \"Prince of the Kings of the Earth\". Further, Satan is popularly titled the \"Prince of Darkness\"; and in the Christian faith he is also referred to as the \"Prince of this World\" and the \"Prince of the Power of the Air\". Another title for Satan, not as common today but apparently so in approximately 30 A.D. by the Pharisees of the day, was the title \"Prince of the Devils\". \"Prince of Israel\", \"Prince of the Angels\", and \"Prince of Light\" are titles given to the Archangel Michael. Some Christian churches also believe that since all Christians, like Jesus Christ, are children of God, then they too are princes and princesses of Heaven. Saint Peter, a disciple of Jesus, is also known as the Prince of the Apostles.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51512", "revid": "11203371", "url": "https://en.wikipedia.org/wiki?curid=51512", "title": "Etiquette", "text": "Customary code of polite behaviour\nEtiquette () can be defined as a set of norms of personal behavior in polite society, usually occurring in the form of an ethical code of the expected and accepted social behaviors that accord with the conventions and norms observed and practiced by a society, a social class, or a social group. In modern English usage, the French word \"\u00e9tiquette\" (label and tag) dates from the year 1750 and also originates from the French word for \"ticket,\" possibly symbolizing a person\u2019s entry into society through proper behavior. There are many important historical figures that have helped to shape the meaning of the term as well as provide varying perspectives. \nHistory.\nIn \u00a0BCE, the Ancient Egyptian vizier Ptahhotep wrote \"The Maxims of Ptahhotep\" (\u00a0BCE), a didactic book of precepts extolling civil virtues such as truthfulness, self-control, and kindness towards other people. Recurrent thematic motifs in the maxims include learning by listening to other people, being mindful of the imperfection of human knowledge, that avoiding open conflict whenever possible should not be considered weakness, and that the pursuit of justice should be foremost. Yet, in human affairs, the command of a god ultimately prevails in all matters. Some of Ptahhotep's maxims indicate a person's correct behaviours in the presence of great personages (political, military, religious), and instructions on how to choose the right master and how to serve him. Other maxims teach the correct way to be a leader through openness and kindness, that greed is the base of all evil and should be guarded against, and that generosity towards family and friends is praiseworthy.\nConfucius (\u00a0BCE) was a Chinese intellectual and philosopher whose works emphasized personal and governmental morality, correctness of social relationships, the pursuit of justice in personal dealings, and sincerity in all personal relations.\nBaldassare Castiglione (\u00a0CE), count of Casatico, was an Italian courtier and diplomat, soldier, and author of \"The Book of the Courtier\" (1528), an exemplar courtesy book dealing with questions of the etiquette and morality of the courtier during the Italian Renaissance.\nLouis XIV (1638\u20131715), King of France, used a codified etiquette to tame the French nobility and assert his supremacy as the absolute monarch of France. In consequence, the ceremonious royal court favourably impressed foreign dignitaries whom the king received at the seat of French government, the Palace of Versailles, to the south-west of Paris.\nBenjamin Franklin (1706\u20131790), an American inventor and Founding Father, contributed to the American understanding of etiquette through his emphasis on practical morality and social harmony. In his autobiography published in 1791, Franklin outlined a personal system of self-improvement centered around thirteen virtues, including sincerity, humility, and temperance. He viewed etiquette as a means of fostering effective communication, avoiding unnecessary conflict, and promoting cooperation in both personal and public life. Franklin distrusted ostentatious formality and believed manners should serve a purpose rooted in usefulness, sincerity, and democratic ideals.\nGeorge Washington (1732\u20131799), the first President of the United States and commander of the Continental Army during the American Revolutionary War, was heavily influenced in his youth by a set of social maxims titled \"Rules of Civility &amp; Decent Behaviour in Company and Conversation\". Adapted from an earlier French text, these 110 rules emphasized humility, respect for others, restraint, and the importance of maintaining decorum in public life. Though Washington did not write the rules himself, copying them by hand served as early moral training and significantly shaped his public persona. The maxims promoted the idea that civil behavior was a reflection of personal virtue and that etiquette could serve as a tool for cultivating leadership and moral character. Despite George Washington\u2019s strong public support for education, many of his contemporaries criticized his intellect, labeling him as poorly educated and lacking eloquence. Figures like Aaron Burr, Thomas Jefferson, and John Adams described him as unrefined, grammatically weak, and intellectually limited. Due to Washington\u2019s personal sensitivity to the level of his academic exposure, these critiques only increased the motivation to copy the 110 rules. Although there may not be any evidence of George Washington verbally passing on the maxims, his actions and character served as a physical example of these beliefs.\nPoliteness.\nIn the 18th century, during the Age of Enlightenment, the adoption of etiquette was a self-conscious process for acquiring the conventions of politeness and the normative behaviours (charm, manners, demeanour) which symbolically identified the person as a genteel member of the upper class. To identify with the social \u00e9lite, the upwardly mobile middle class and the bourgeoisie adopted the behaviours and the artistic preferences of the upper class. To that end, socially ambitious people of the middle classes occupied themselves with learning, knowing, and practising the rules of social etiquette, such as the arts of elegant dress and gracious conversation, when to show emotion, and courtesy with and towards women.\nIn the early 18th century, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury, wrote influential essays that defined \"politeness\" as the art of being pleasing in company; and discussed the function and nature of politeness in the social discourse of a commercial society:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;'Politeness' may be defined as dext'rous management of our words and actions, whereby we make other people have better opinion of us and themselves.\nPeriodicals, such as \"The Spectator\", a daily publication founded in 1711 by Joseph Addison and Richard Steele, regularly advised their readers on the etiquette required of a gentleman, a man of good and courteous conduct; their stated editorial goal was \"to enliven morality with wit, and to temper wit with morality\u2026 to bring philosophy out of the closets and libraries, schools and colleges, to dwell in clubs and assemblies, at tea-tables and coffeehouses\"; to which end, the editors published articles written by educated authors, which provided topics for civil conversation, and advice on the requisite manners for carrying a polite conversation, and for managing social interactions.\nConceptually allied to etiquette is the notion of \"civility\" (social interaction characterised by sober and reasoned debate) which for socially ambitious men and women also became an important personal quality to possess for social advancement. In the event, gentlemen's clubs, such as Harrington's Rota Club, published an in-house etiquette that codified the civility expected of the members. Besides \"The Spectator\", other periodicals sought to infuse politeness into English coffeehouse conversation, the editors of \"The Tatler\" were explicit that their purpose was the reformation of English manners and morals; to those ends, etiquette was presented as the virtue of morality and a code of behaviour.\nIn the mid-18th century, the first, modern English usage of \"etiquette\" (the conventional rules of personal behaviour in polite society) was by Philip Stanhope, 4th Earl of Chesterfield, in the book \"Letters to His Son on the Art of Becoming a Man of the World and a Gentleman\" (1774), a correspondence of more than 400 letters written from 1737 until the death of his son, in 1768; most of the letters were instructive, concerning varied subjects that a worldly gentleman should know. The letters were first published in 1774, by Eugenia Stanhope, the widow of the diplomat Philip Stanhope, Chesterfield's bastard son. Throughout the correspondence, Chesterfield endeavoured to decouple the matter of social manners from conventional morality, with perceptive observations that pragmatically argue to Philip that mastery of etiquette was an important means for social advancement, for a man such as he. Chesterfield's elegant, literary style of writing epitomised the emotional restraint characteristic of polite social intercourse in 18th-century society:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I would heartily wish that you may often be seen to smile, but never heard to laugh while you live. Frequent and loud laughter is the characteristic of folly and ill-manners; it is the manner in which the mob express their silly joy at silly things; and they call it being merry. In my mind there is nothing so illiberal, and so ill-bred, as audible laughter. I am neither of a melancholy nor a cynical disposition, and am as willing and as apt to be pleased as anybody; but I am sure that since I have had the full use of my reason nobody has ever heard me laugh.\nIn the 19th century, Victorian era (1837\u20131901) etiquette developed into a complicated system of codified behaviours, which governed the range of manners in society\u2014from the proper language, style, and method for writing letters, to correctly using cutlery at table, and to the minute regulation of social relations and personal interactions between men and women and among the social classes.\nIn the 21st century, specifically in the early 2020s as digital communication became more readily available and used in everyday life, the notion of digital etiquette, or \"netiquette\", evolved into a flexible, socially negotiated code of conduct guiding behavior in online spaces. Unlike traditional etiquette, which often revolved around visible symbols of status and formal conduct, digital etiquette today is platform-dependent, highly situational, and subtly influenced by unspoken social norms. For instance, a video call may press for visible presence (\u201ccamera on\u201d) and active engagement such as contributing ideas or giving visual cues of attention, while sending an email might demand carefully crafted language, formal greetings, and rapid response times to signal competence and respect. An essential aspect of today\u2019s netiquette is the management of presence and attention. The expectation to be responsive has become a symbol of respect, while behaviors such as multitasking during meetings or disabling cameras may be interpreted as disrespect or disengagement. \nManners.\nSociological perspectives.\nIn a society, manners are described as either good manners or as bad manners to indicate whether a person's behaviour is acceptable to the cultural group. As such, manners enable \"ultrasociality\" and are integral to the functioning of the social norms and conventions that are informally enforced through self-regulation. The perspectives of sociology indicate that manners are a means for people to display their social status, and a means of demarcating, observing, and maintaining the boundaries of social identity and of social class.\nIn \"The Civilizing Process\" (1939), sociologist Norbert Elias said that manners arose as a product of group living, and persist as a way of maintaining social order. Manners proliferated during the Renaissance in response to the development of the 'absolute state'\u2014the progression from small-group living to large-group living characterised by the centralized power of the State. The rituals and manners associated with the royal court of England during that period were closely bound to a person's social status. Manners demonstrate a person's position within a social network, and a person's manners are a means of negotiation from that social position.\nFrom the perspective of public health, in \"The Healthy Citizen\" (1995), Alana R. Petersen and Deborah Lupton said that manners assisted the diminishment of the social boundaries that existed between the public sphere and the private sphere of a person's life, and so gave rise to \"a highly reflective self, a self who monitors his or her behavior with due regard for others with whom he or she interacts, socially\"; and that \"the public behavior of individuals came to signify their social standing; a means of presenting the self and of evaluating others, and thus the control of the outward self was vital.\"\nSociologist Pierre Bourdieu applied the concept of \"habitus\" to define the societal functions of manners. The \"habitus\" is the set of mental attitudes, personal habits, and skills that a person possesses\u2014his or her \"dispositions\" of character that are neither self-determined, nor pre-determined by the external environment, but which are produced and reproduced by social interactions\u2014and are \"inculcated through experience and explicit teaching\", yet tend to function at the subconscious level. Manners are likely to be a central part of the \"dispositions\" that guide a person's ability to decide upon socially-compliant behaviours.\nAnthropologic perspective.\nIn \"Purity and Danger: An Analysis of Concepts of Pollution and Taboo\" (2003) the anthropologist Mary Douglas said that manners, social behaviors, and group rituals enable the local cosmology to remain ordered and free from those things that may pollute or defile the integrity of the culture. Ideas of pollution, defilement, and disgust are attached to the margins of socially acceptable behaviour in order to curtail unacceptable behaviour, and so maintain \"the assumptions by which experience is controlled\" within the culture.\nEvolutionary perspectives.\nIn studying the expression of emotion by humans and animals, naturalist Charles Darwin noted the universality of facial expressions of disgust and shame among infants and blind people, and concluded that the emotional responses of shame and disgust are innate behaviours.\nPublic health specialist Valerie Curtis said that the development of facial responses was concomitant with the development of manners, which are behaviours with an evolutionary role in preventing the transmission of diseases, thus, people who practise personal hygiene and politeness will most benefit from membership in their social group, and so stand the best chance of biological survival, by way of opportunities for reproduction.\nFrom the study of the evolutionary bases of prejudice, social psychologists Catherine Cottrell and Steven Neuberg said that human behavioural responses to 'otherness' might enable the preservation of manners and social norms. The feeling of \"foreignness\"\u2014which people experience in their first social interaction with someone from another culture\u2014might partly serve an evolutionary function: 'Group living surrounds one with individuals [who are] able to physically harm fellow group members, to spread contagious disease, or to \"free ride\" on their efforts'; therefore, a commitment to sociality is a risk: 'If threats, such as these, are left unchecked, the costs of sociality will quickly exceed its benefits. Thus, to maximize the returns on group \"living\", individual group members should be attuned to others' features or behaviors.'\nTherefore, people who possess the social traits common to the cultural group are to be trusted, and people without the common social traits are to be distrusted as 'others', and thus treated with suspicion or excluded from the group. That pressure of social exclusivity, born from the shift towards communal living, excluded uncooperative people and persons with poor personal hygiene. The threat of social exclusion led people to avoid personal behaviours that might embarrass the group or that might provoke revulsion among the group.\nTo demonstrate the transmission of social conformity, anthropologists Joseph Henrich and Robert Boyd developed a behavioural model in which manners are a means of mitigating social differences, curbing undesirable personal behaviours, and fostering co-operation within the social group. Natural selection favoured the acquisition of genetically transmitted mechanisms for learning, thereby increasing a person's chances for acquiring locally adaptive behaviours: \"Humans possess a reliably developing neural encoding that compels them both to punish individuals who violate group norms (common beliefs or practices) and [to] punish individuals who do not punish norm-violators.\"\nCategories.\nSocial manners are in three categories: (i) manners of hygiene, (ii) manners of courtesy, and (iii) manners of cultural norm. Each category accounts for an aspect of the functional role that manners play in a society. The categories of manners are based upon the social outcome of behaviour, rather than upon the personal motivation of the behaviour. As a means of social management, the rules of etiquette encompass most aspects of human social interaction; thus, a rule of etiquette reflects an underlying ethical code and a person's fashion and social status.\nCourtesy books.\n\"The Book of the Courtier\" (1528), by Baldassare Castiglione, identified the manners and the morals required by socially ambitious men and women for success in a royal court of the Italian Renaissance (14th\u201317th c.); as an etiquette text, \"The Courtier\" was an influential courtesy book in 16th-century Europe.\n\"On Civility in Children\" (1530), by Erasmus of Rotterdam, instructs boys in the means of becoming a young man; how to walk and talk, speak and act in the company of adults. The practical advice for acquiring adult self-awareness includes explanations of the symbolic meanings\u2014for adults\u2014of a boy's body language when he is fidgeting and yawning, scratching and bickering. On completing Erasmus's curriculum of etiquette, the boy has learnt that civility is the point of good manners: the adult ability to 'readily ignore the faults of others, but avoid falling short, yourself,' in being civilised.\n\"Etiquette in Society, in Business, in Politics, and at Home\" (1922), by Emily Post documents the \"trivialities\" of desirable conduct in daily life, and provided pragmatic approaches to the practice of good manners\u2014the social conduct expected and appropriate for the events of life, such as a baptism, a wedding, and a funeral.\nAs didactic texts, books of etiquette (the conventional rules of personal behaviour in polite society) usually feature explanatory titles, such as \"The Ladies' Book of Etiquette, and Manual of Politeness: A Complete Hand Book for the Use of the Lady in Polite Society\" (1860), by Florence Hartley; \"Amy Vanderbilt's Complete Book of Etiquette\" (1957); \"Miss Manners' Guide to Excruciatingly Correct Behavior\" (1979), by Judith Martin; and \"Peas &amp; Queues: The Minefield of Modern Manners\" (2013), by Sandi Toksvig. Such books present ranges of civility, socially acceptable behaviours for their respective times. Each author cautions the reader that to be a well-mannered person they must practise good manners in their public and private lives.\nThe \"How Rude!\" comic-book series addresses and discusses adolescent perspectives and questions of etiquette, social manners, and civility.\nBusiness.\nIn commerce, the purpose of etiquette is to facilitate the social relations necessary for realising business transactions; in particular, social interactions among workers, and between labour and management. Business etiquette varies by culture, such as the Chinese and Australian approaches to conflict resolution. The Chinese business philosophy is based upon (personal connections), whereby person-to-person negotiation resolves difficult matters, whereas Australian business philosophy relies upon attorneys-at-law to resolve business conflicts through legal mediation; thus, adjusting to the etiquette and professional ethics of another culture is an element of culture shock for businesspeople.\nIn 2011, etiquette trainers formed the Institute of Image Training and Testing International (IITTI) a non-profit organisation to train personnel departments in measuring and developing and teaching social skills to employees, by way of education in the rules of personal and business etiquette, in order to produce business workers who possess standardised manners for successfully conducting business with people from other cultures.\nIn the retail branch of commerce, the saying \"the customer is always right\" summarises the profit-orientation of good manners, between the buyer and the seller of goods and services:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There are always two sides to the case, of course, and it is a credit to good manners that there is scarcely ever any friction in stores and shops of the first class. Salesmen and women are usually persons who are both patient and polite, and their customers are most often ladies in fact as well as \"by courtesy.\" Between those before and those behind the counters, there has sprung up in many instances a relationship of mutual goodwill and friendliness. It is, in fact, only the woman who is afraid that someone may encroach upon her exceedingly insecure dignity, who shows neither courtesy nor consideration to any except those whom she considers it to her advantage to please.\u2014\u200a\nSee also.\n&lt;templatestyles src=\"Col-begin/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "51513", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=51513", "title": "Arrow", "text": "Shafted projectile that is shot with a bow\nAn arrow is a fin-stabilized projectile launched by a bow. A typical arrow usually consists of a long, stiff, straight shaft with a weighty (and usually sharp and pointed) arrowhead attached to the front end, multiple fin-like stabilizers called fletchings mounted near the rear, and a slot at the rear end called a nock for engaging the bowstring. A container or bag carrying additional arrows for convenient reloading is called a quiver.\nThe use of bows and arrows by humans predates recorded history and is common to most cultures. A craftsman who makes arrows is a fletcher, and one who makes arrowheads is an arrowsmith.\nHistory.\nThe oldest evidence of likely arrowheads, dating to c.\u200964,000 years ago, were found in Sibudu Cave, current South Africa. Likely arrowheads made from animal bones have been discovered in the Fa Hien Cave in Sri Lanka which are also the oldest evidence for the use of arrows outside of Africa dating to c.\u200948,000 years ago. The oldest evidence of the use of bows to shoot arrows dates to about 10,000 years ago; it is based on pinewood arrows found in the Ahrensburg valley north of Hamburg. They had shallow grooves on the base, indicating that they were shot from a bow. The oldest bow so far recovered is about 8,000 years old, found in the Holmeg\u00e5rd swamp in Denmark. Archery seems to have arrived in the Americas with the Arctic small tool tradition, about 4,500 years ago.\nSize.\nArrow sizes vary greatly across cultures, ranging from eighteen inches to five feet (45\u00a0cm to 152\u00a0cm). However, most modern arrows are to in length. Arrows recovered from the \"Mary Rose\", an English warship that sank in 1545 whose remains were raised in 1982, were mostly long. Very short arrows have been used, shot through a guide attached either to the bow (an \"overdraw\") or to the archer's wrist (the Turkish \"siper\"). These may fly farther than heavier arrows, and an enemy without suitable equipment may find himself unable to return them.\nComponents.\nShaft.\nThe shaft is the primary structural element of the arrow, to which the other components are attached. Traditional arrow shafts are made from strong, lightweight wood, bamboo, or reeds, while modern shafts may be made from aluminium, carbon fibre reinforced plastic, or a combination of materials. Such shafts are typically made from an aluminium core wrapped with a carbon fibre outer. A traditional premium material is Port Orford Cedar.\nSpine.\nThe stiffness of the shaft is known as its \"spine\", referring to how little the shaft bends when compressed, hence an arrow which bends less is said to have more spine. In order to strike consistently, a group of arrows must be similarly spined. \"Center-shot\" bows, in which the arrow passes through the central vertical axis of the bow riser, may obtain consistent results from arrows with a wide range of spines. However, most traditional bows are not center-shot and the arrow has to deflect around the handle in the archer's paradox; such bows tend to give most consistent results with a narrower range of arrow spine that allows the arrow to deflect correctly around the bow. Bows with higher draw weight will generally require stiffer arrows, with more spine (less flexibility) to give the correct amount of flex when shot.\nGPI rating.\nThe weight of an arrow shaft can be expressed in GPI (grains per inch). The length of a shaft in inches multiplied by its GPI rating gives the weight of the shaft in grains. For example, a shaft that is long and has a GPI of 9.5 weighs . This does not include the other elements of a finished arrow, so a complete arrow will be heavier than the shaft alone.\nFooted arrows.\nSometimes a shaft will be made of two different types of wood fastened together, resulting in what is known as a footed arrow. Known by some as the finest of wood arrows, footed arrows were used both by early Europeans and Native Americans. Footed arrows will typically consist of a short length of hardwood near the head of the arrow, with the remainder of the shaft consisting of softwood. By reinforcing the area most likely to break, the arrow is more likely to survive impact, while maintaining overall flexibility and lighter weight.\nBarreled arrow shafts.\nA barreled arrow shaft is one that tapers in diameter bi-directionally. This allows for an arrow that has an optimum weight yet retains enough strength to resist flex.\nBarreled arrow shafts are considered the zenith of pre-industrial archery technology, reaching their peak design among the Ottomans.\nArrowhead.\nThe arrowhead or projectile point is the primary functional part of the arrow, and plays the largest role in determining its purpose. Some arrows may simply use a sharpened tip of the solid shaft, but it is far more common for separate arrowheads to be made, usually from metal, horn, or some other hard material. Arrowheads are usually separated by function:\nThere are two main types of broadheads used by hunters: the fixed-blade and the mechanical types. While the fixed-blade broadhead keeps its blades rigid and unmovable on the broadhead at all times, the mechanical broadhead deploys its blades upon contact with the target, its blades swinging out to wound the target. The mechanical head flies better because it is more streamlined, but has less penetration as it uses some of the kinetic energy in the arrow to deploy its blades. Preferences for fixed or mechanical broadheads vary widely in the hunting community, typically varying by species. In general, both types are widely used on game up to the size of white-tailed deer; as game size increases, fixed blades become more common. Broadheads used for hunting dangerous game such as African buffalo are almost always fixed-blade.\nArrowheads may be attached to the shaft with a cap, a socketed tang, or inserted into a split in the shaft and held by a process called hafting. Points attached with caps are simply slid snugly over the end of the shaft, or may be held on with hot glue. Split-shaft construction involves splitting the arrow shaft lengthwise, inserting the arrowhead, and securing it using a ferrule, sinew, or wire.\nFletchings.\nFletchings are found at the back of the arrow and act as airfoils to provide a small amount of force used to stabilize the flight of the arrow. They are designed to keep the arrow pointed in the direction of travel by strongly damping down any tendency to pitch or yaw. Some cultures, for example most in New Guinea, did not use fletching on their arrows. Also, arrows without fletching (called bare shaft) are used for training purposes, because they make certain errors by the archer more visible.\nFletchings are traditionally made from feathers (often from a goose or turkey) bound to the arrow's shaft, but are now often made of plastic (known as \"vanes\"). Historically, some arrows used for the proofing of armour used copper vanes. Flight archers may use razor blades for fletching, in order to reduce air resistance. With conventional three-feather fletching, one feather, called the \"cock\" feather, is at a right angle to the nock, and is normally nocked so that it will not contact the bow when the arrow is shot. Four-feather fletching is usually symmetrical and there is no preferred orientation for the nock; this makes nocking the arrow slightly easier.\nNatural feathers are usually prepared by splitting and sanding the quill before gluing. Further, the feather may be trimmed to shape, die-cut or burned by a hot electrically heated wire. It is crucial that all the feathers of an arrow have the same drag, so manual trimming is rarely used by modern fletchers. The burning-wire method is popular because different shapes are possible by bending the wire, and the fletching can be symmetrically trimmed after gluing by rotating the arrow on a fixture.\nSome fletchings are dyed. Two-toned fletchings usually make each fletching from two feathers knit together. The front fletching is often camouflaged, and the rear fletching bright so that the archer can easily track the arrow.\nArtisans who make arrows by hand are known as \"fletchers\", a word related to the French word for arrow, \"fl\u00e8che.\" This is the same derivation as the verb \"fletch\", meaning to provide an arrow with its feathers. Glue and thread are the traditional methods of attaching fletchings. A \"fletching jig\" is often used in modern times, to hold the fletchings in exactly the right orientation on the shaft while the glue hardens.\nWhenever natural fletching is used, the feathers on any one arrow must come from the same wing of the bird, the most common being the right-wing flight feathers of turkeys. The slight cupping of natural feathers requires them to be fletched with a right-twist for right wing, a left-twist for left wing. This rotation, through a combination of gyroscopic stabilization and increased drag on the rear of the arrow, helps the arrow to fly straight away. Artificial \"helical\" fletchings have the same effect. Most arrows will have three fletches, but some have four or even more. Fletchings generally range from in length; flight arrows intended to travel the maximum possible distance typically have very low fletching, while hunting arrows with broadheads require long and high fletching to stabilize them against the aerodynamic effect of the head. Fletchings may also be cut in different ways, the two most common being \"parabolic\" (i.e. a smooth curved shape) and \"shield\" (i.e. shaped as one-half of a very narrow shield) cut.\nIn modern archery with screw-in points, right-hand rotation is generally preferred as it makes the points self-tighten. In traditional archery, some archers prefer a left rotation because it gets the hard (and sharp) quill of the feather farther away from the arrow-shelf and the shooter's hand.\nA flu-flu is a type of fletching normally made by using long sections of full length feathers taken from a turkey; in most cases, six or more sections are used rather than the traditional three. Alternatively two long feathers can be spiraled around the end of the arrow shaft. The extra fletching generates more drag and slows the arrow down rapidly after a short distance of about or so. Flu-flu arrows are often used for hunting birds, or for children's archery, and can also be used to play flu-flu golf.\nWraps.\nWraps are thin pre-cut sheets of material, often vinyl or plastic, used to wrap the nock end of an arrow, primarily as an aid in bonding vanes and feather fletchings to the shaft. Wraps can also make the eventual removal of vanes and vane-glue easier. Additionally, they add a decorative aspect to arrow building, which can provide archers an opportunity to personalize their arrows. Brightly colored wraps can also make arrows much easier to find in the brush, and to see in downrange targets.\nNocks.\nIn English it is common to say \"nock an arrow\" when one readies a shot. A nock is a notch in the rearmost end of an arrow. It helps keep the arrow correctly rotated, keeps the arrow from slipping sideways during the draw or after the release, and helps maximize the arrow's energy (i.e. its range and lethality) by helping an archer place the arrow at the fastest-moving place on the bowstring. Some archers mark the nock position with beads, knots or wrappings of thread. Most compound bow shooters use a D-loop, a length of string material (or sometimes a metal bracket) attached to the string above and below the nocking point. A release aid is typically attached to the D-loop in preparation for a shot.\nThe main purpose of a nock is to control the rotation of the arrow. Arrows bend when released. If the bend hits the bowstave, the arrow's aim will be thrown off. Wooden arrows have a preferred bending-plane. Synthetic arrows have a designed bending plane. Usually this plane is determined by the grain of the wood of the arrow, or the structure of a synthetic arrow. The nock's slot should be rotated at an angle chosen so that when the arrow bends, it avoids or slides on the bowstave. Almost always this means that the slot of the nock must be perpendicular to the wood's grain, viewed from behind.\n\"Self nocks\" are slots cut in the back of the arrow. These are simple, but can break at the base of the slot. Self nocks are often reinforced with glued servings of fiber near the base of the slot. The sturdiest nocks are separate pieces made from wood, plastic, or horn that are then attached to the end of the arrow.\nModern nocks, and traditional Turkish nocks, are often constructed so as to curve around the string or even pinch it slightly, so that the arrow is unlikely to slip off.\nAncient Arab archery sometimes used \"nockless arrows\". In shooting at enemies, Arabs saw them pick up Arab arrows and shoot them back. So Arabs developed bowstrings with a small ring tied where the nock would normally be placed. The rear end of the arrow would be sharpened to a point, rather than slit for a nock. The rear end of the arrow would slip into the ring. The arrow could be drawn and released as usual. Then the enemy could collect the arrows, yet not shoot them back with a conventional bow. Also, since there was no nock, the nock could not break, and the arrow was less expensive. A piece of battle advice was to have several rings tied to the bowstring in case one broke. A practical disadvantage compared to a nock would be preserving the optimal rotation of the arrow, so that when it flexes, it does not hit the bowstave. The bend direction of the arrow might have been indicated by its fletching.\nFinishes and cresting.\nArrows are usually finished so that they are not softened by rain, fog or condensation. Traditional finishes are varnishes or lacquers. Arrows sometimes need to be repaired, so it's important that the paints be compatible with glues used to attach arrowheads, fletchings, and nocks. For this reason, arrows are rarely protected by waxing.\nCrests are rings or bands of paint, often brightly colored, applied to arrows on a lathe-like tool called a cresting machine, usually for the purpose of personalization. Like wraps, cresting may also be done to make arrows easier to see.\nSymbolism.\nAn arrow symbol (\u2192) is a simple graphical or typographical representation of an arrow, consisting of a triangle or chevron at the end of a straight line. It is used to indicate a direction, such as on signs and as road surface markings.\nA symbol often used by aromantic people is arrows or an arrow as the word \"arrow\" is a homophone to the shortened word \"aro\" used by aromantic people to refer to themselves.\nAncient Indian astronomers often associate the interdependent trigonometrical components with the picture of a bow and arrow, the arrow (\"utkrama-jy\u0101\") equivalent to the present day secant.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51518", "revid": "13667633", "url": "https://en.wikipedia.org/wiki?curid=51518", "title": "Dam", "text": "Barrier that stops or restricts the flow of surface or underground streams\nA dam is a barrier that stops or restricts the flow of surface water or underground streams. Reservoirs created by dams not only suppress floods but also provide water for activities such as irrigation, human consumption, industrial use, aquaculture, and navigability. Hydropower is often used in conjunction with dams to generate electricity. A dam can also be used to collect or store water which can be evenly distributed between locations. Dams generally serve the primary purpose of retaining water, while other structures such as floodgates or levees (also known as dikes) are used to manage or prevent water flow into specific land regions.\nThe word \"dam\" can be traced back to Middle English, and before that, from Middle Dutch, as seen in the names of many old cities, such as Amsterdam and Rotterdam.\nAncient dams were built in Mesopotamia, the Middle East, and China for water control. Possibly the earliest known dam is the Jawa Dam in Jordan, dating to 3,000 BC. Dams of a similar age have also been attributed to the Liangzhu culture, of the Yangtze Delta. Egyptians also built dams, such as Sadd-el-Kafara Dam for flood control. In modern-day India, Dholavira had an intricate water-management system with 16 reservoirs and dams. The Great Dam of Marib in Yemen, built between 1750 and 1700 BC, was an engineering wonder, and Eflatun Pinar, a Hittite dam and spring temple in Turkey, dates to the 15th and 13th centuries BC. The Kallanai Dam in South India, built in the 2nd century AD, is one of the oldest water regulating structures still in use.\nRoman engineers built dams with advanced techniques and materials, such as hydraulic mortar and Roman concrete, which allowed for larger structures. They introduced reservoir dams, arch-gravity dams, arch dams, buttress dams, and multiple arch buttress dams. In Iran, bridge dams were used for hydropower and water-raising mechanisms.\nDuring the Middle Ages, dams were built in the Netherlands to regulate water levels and prevent sea intrusion. In the 19th century, large-scale arch dams were constructed around the British Empire, marking advances in dam engineering techniques. The era of large dams began with the construction of the Aswan Low Dam in Egypt in 1902. The Hoover Dam, a massive concrete arch-gravity dam, was built between 1931 and 1936 on the Colorado River. By 1997, there were an estimated 800,000 dams worldwide, with some 40,000 of them over 15 meters high.\nHistory.\nAncient dams.\nEarly dam building took place in Mesopotamia and the Middle East. Dams were used to control water levels, for Mesopotamia's weather affected the Tigris and Euphrates Rivers.\nThe earliest known dam is the Jawa Dam in Jordan, northeast of the capital Amman. This gravity dam featured an originally and stone wall, supported by a earthen rampart. The structure is dated to 3000 BC. However, the oldest continuously operational dam is Lake Homs Dam, built in Syria between 1319-1304 BC.\nThe Ancient Egyptian Sadd-el-Kafara Dam at Wadi Al-Garawi, about south of Cairo, was long at its base and wide. The structure was built around 2800 or 2600 BC as a diversion dam for flood control, but was destroyed by heavy rain during construction or shortly afterwards. During the Twelfth Dynasty in the 19th century BC, the Pharaohs Senosert III, Amenemhat III, and Amenemhat IV dug a canal long linking the Fayum Depression to the Nile in Middle Egypt. Two dams called Ha-Uar running east\u2013west were built to retain water during the annual flood and then release it to surrounding lands. The lake called \"Mer-wer\" or Lake Moeris covered and is known today as Birket Qarun.\nBy the mid-late third millennium BC, an intricate water-management system in Dholavira in modern-day India was built. The system included 16 reservoirs, dams and various channels for collecting water and storing it.\nOne of the engineering wonders of the ancient world was the Great Dam of Marib in Yemen. Initiated sometime between 1750 and 1700 BC, it was made of packed earth \u2013 triangular in cross-section, in length and originally high \u2013 running between two groups of rocks on either side, to which it was linked by substantial stonework. Repairs were carried out during various periods, most importantly around 750 BC, and 250 years later the dam height was increased to . After the end of the Kingdom of Saba, the dam fell under the control of the \u1e24imyarites (c. 115 BC) who undertook further improvements, creating a structure high, with five spillways, two masonry-reinforced sluices, a settling pond, and a canal to a distribution tank. These works were not finished until 325 AD when the dam permitted the irrigation of .\nEflatun P\u0131nar is a Hittite dam and spring temple near Konya, Turkey. It is thought to date from the Hittite empire between the 15th and 13th centuries BC.\nThe Kallanai is constructed of unhewn stone, over long, high and wide, across the main stream of the Kaveri River in Tamil Nadu, South India. The basic structure dates to the 2nd century AD and is considered one of the oldest water diversion or water regulating structures still in use. The purpose of the dam was to divert the waters of the Kaveri across the fertile delta region for irrigation via canals.\nDu Jiang Yan is the oldest surviving irrigation system in China that included a dam that directed waterflow. It was finished in 251 BC. A large earthen dam, made by Sunshu Ao, the prime minister of Chu (state), flooded a valley in modern-day northern Anhui Province that created an enormous irrigation reservoir ( in circumference), a reservoir that is still present today.\nRoman engineering.\nRoman dam construction was characterized by \"the Romans' ability to plan and organize engineering construction on a grand scale.\" Roman planners introduced the then-novel concept of large reservoir dams which could secure a permanent water supply for urban settlements over the dry season. Their pioneering use of water-proof hydraulic mortar and particularly Roman concrete allowed for much larger dam structures than previously built, such as the Lake Homs Dam, possibly the largest water barrier to that date, and the Harbaqa Dam, both in Roman Syria. The highest Roman dam was the Subiaco Dam near Rome; its record height of remained unsurpassed until its accidental destruction in 1305.\nRoman engineers made routine use of ancient standard designs like embankment dams and masonry gravity dams. Apart from that, they displayed a high degree of inventiveness, introducing most of the other basic dam designs which had been unknown until then. These include arch-gravity dams, arch dams, buttress dams and multiple arch buttress dams, all of which were known and employed by the 2nd century AD (see List of Roman dams). Roman workforces also were the first to build dam bridges, such as the Bridge of Valerian in Iran.\nIn Iran, bridge dams such as the Band-e Kaisar were used to provide hydropower through water wheels, which often powered water-raising mechanisms. One of the first was the Roman-built dam bridge in Dezful, which could raise water 50 cubits (c. 23 m) to supply the town. Also diversion dams were known. Milling dams were introduced which the Muslim engineers called the \"Pul-i-Bulaiti\". The first was built at Shustar on the River Karun, Iran, and many of these were later built in other parts of the Islamic world. Water was conducted from the back of the dam through a large pipe to drive a water wheel and watermill. In the 10th century, Al-Muqaddasi described several dams in Persia. He reported that one in Ahwaz was more than long, and that it had many water-wheels raising the water into aqueducts through which it flowed into reservoirs of the city. Another one, the Band-i-Amir Dam, provided irrigation for 300 villages.\nMiddle Ages.\nSh\u0101h Abb\u0101s Arch (Persian: \u0637\u0627\u0642 \u0634\u0627\u0647 \u0639\u0628\u0627\u0633), also known as Kurit Dam, is the thinnest arch dam in the world and one of the oldest arch dams in Asia. It was constructed some 700 years ago in Tabas county, South Khorasan Province, Iran. It stands 60 meters tall, and in crest is a one meter width. Some historians believe the dam was built by Sh\u0101h Abb\u0101s I, whereas others believe that he repaired it.\nIn the Netherlands, a low-lying country, dams were often built to block rivers to regulate the water level and to prevent the sea from entering the marshlands. Such dams often marked the beginning of a town or city because it was easy to cross the river at such a place, and often influenced Dutch place names. The present Dutch capital, Amsterdam (old name \"Amstelredam\"), started with a dam on the river Amstel in the late 12th century, and Rotterdam began with a dam on the river Rotte, a minor tributary of the Nieuwe Maas. The central square of Amsterdam, covering the original site of the 800-year-old dam, still carries the name \"Dam Square\".\nIndustrial Revolution.\nThe Romans were the first to build arch dams, where the reaction forces from the abutment stabilizes the structure from the external hydrostatic pressure, but it was only in the 19th century that the engineering skills and construction materials available were capable of building the first large-scale arch dams.\nThree pioneering arch dams were built around the British Empire in the early 19th century. Henry Russel of the Royal Engineers oversaw the construction of the Mir Alam dam in 1804 to supply water to the city of Hyderabad (it is still in use today). It had a height of and consisted of 21 arches of variable span.\nIn the 1820s and 30s, Lieutenant-Colonel John By supervised the construction of the Rideau Canal in Canada near modern-day Ottawa and built a series of curved masonry dams as part of the waterway system. In particular, the Jones Falls Dam, built by John Redpath, was completed in 1832 as the largest dam in North America and an engineering marvel. In order to keep the water in control during construction, two sluices, artificial channels for conducting water, were kept open in the dam. The first was near the base of the dam on its east side. A second sluice was put in on the west side of the dam, about above the base. To make the switch from the lower to upper sluice, the outlet of Sand Lake was blocked off.\nHunts Creek near the city of Parramatta, Australia, was dammed in the 1850s, to cater to the demand for water from the growing population of the city. The masonry arch dam wall was designed by Lieutenant Percy Simpson who was influenced by the advances in dam engineering techniques made by the Royal Engineers in India. The dam cost \u00a317,000 and was completed in 1856 as the first engineered dam built in Australia, and the second arch dam in the world built to mathematical specifications.\nThe first such dam was opened two years earlier in France. It was the first French arch dam of the industrial era, and it was built by Fran\u00e7ois Zola in the municipality of Aix-en-Provence to improve the supply of water after the 1832 cholera outbreak devastated the area. After royal approval was granted in 1844, the dam was constructed over the following decade. Its construction was carried out on the basis of the mathematical results of scientific stress analysis.\nThe 75-miles dam near Warwick, Australia, was possibly the world's first concrete arch dam. Designed by Henry Charles Stanley in 1880 with an overflow spillway and a special water outlet, it was eventually heightened to .\nIn the latter half of the nineteenth century, significant advances in the scientific theory of masonry dam design were made. This transformed dam design from an art based on empirical methodology to a profession based on a rigorously applied scientific theoretical framework. This new emphasis was centered around the engineering faculties of universities in France and in the United Kingdom. William John Macquorn Rankine at the University of Glasgow pioneered the theoretical understanding of dam structures in his 1857 paper \"On the Stability of Loose Earth\". Rankine theory provided a good understanding of the principles behind dam design. In France, J. Augustin Tortene de Sazilly explained the mechanics of vertically faced masonry gravity dams, and Zola's dam was the first to be built on the basis of these principles.\nModern era.\nThe era of large dams was initiated with the construction of the Aswan Low Dam in Egypt in 1902, a gravity masonry buttress dam on the Nile River. Following their 1882 invasion and occupation of Egypt, the British began construction in 1898. The project was designed by Sir William Willcocks and involved several eminent engineers of the time, including Sir Benjamin Baker and Sir John Aird, whose firm, John Aird &amp; Co., was the main contractor. Capital and financing were furnished by Ernest Cassel. When initially constructed between 1899 and 1902, nothing of its scale had ever before been attempted; on completion, it was the largest masonry dam in the world.\nThe Hoover Dam is a massive concrete arch-gravity dam, constructed in the Black Canyon of the Colorado River, on the border between the US states of Arizona and Nevada between 1931 and 1936 during the Great Depression. In 1928, Congress authorized the project to build a dam that would control floods, provide irrigation water and produce hydroelectric power. The winning bid to build the dam was submitted by a consortium called Six Companies, Inc. Such a large concrete structure had never been built before, and some of the techniques were unproven. The torrid summer weather and the lack of facilities near the site also presented difficulties. Nevertheless, Six Companies turned over the dam to the federal government on 1 March 1936, more than two years ahead of schedule.\nBy 1997, there were an estimated 800,000 dams worldwide, some 40,000 of them over high. In 2014, scholars from the University of Oxford published a study of the cost of large dams \u2013 based on the largest existing dataset \u2013 documenting significant cost overruns for a majority of dams and questioning whether benefits typically offset costs for such dams.\nTypes.\nDams can be formed by human agency, natural causes such as landslides, or even by the intervention of wildlife such as beavers. Man-made dams are typically classified according to their size (height), intended purpose or structure.\nBy structure.\nBased on structure and material used, dams are classified as easily created without materials, arch-gravity dams, embankment dams or masonry dams, with several subtypes.\nArch dams.\nIn the arch dam, stability is obtained by a combination of arch and gravity action. If the upstream face is vertical the entire weight of the dam must be carried to the foundation by gravity, while the distribution of the normal hydrostatic pressure between vertical cantilever and arch action will depend upon the stiffness of the dam in a vertical and horizontal direction. When the upstream face is sloped the distribution is more complicated. The normal component of the weight of the arch ring may be taken by the arch action, while the normal hydrostatic pressure will be distributed as described above. For this type of dam, firm reliable supports at the abutments (either buttress or canyon side wall) are more important. The most desirable place for an arch dam is a narrow canyon with steep side walls composed of sound rock. The safety of an arch dam is dependent on the strength of the side wall abutments, hence not only should the arch be well seated on the side walls but also the character of the rock should be carefully inspected.\nTwo types of single-arch dams are in use, namely the constant-angle and the constant-radius dam. The constant-radius type employs the same face radius at all elevations of the dam, which means that as the channel grows narrower towards the bottom of the dam the central angle subtended by the face of the dam becomes smaller. Jones Falls Dam, in Canada, is a constant radius dam. In a constant-angle dam, also known as a variable radius dam, this subtended angle is kept constant and the variation in distance between the abutments at various levels is taken care of by varying the radii. Constant-radius dams are much less common than constant-angle dams. Parker Dam on the Colorado River is a constant-angle arch dam.\nA similar type is the double-curvature or thin-shell dam. Wildhorse Dam near Mountain City, Nevada, in the United States is an example of the type. This method of construction minimizes the amount of concrete necessary for construction but transmits large loads to the foundation and abutments. The appearance is similar to a single-arch dam but with a distinct vertical curvature to it as well lending it the vague appearance of a concave lens as viewed from downstream.\nThe multiple-arch dam consists of a number of single-arch dams with concrete buttresses as the supporting abutments, as for example the Daniel-Johnson Dam, Qu\u00e9bec, Canada. The multiple-arch dam does not require as many buttresses as the hollow gravity type but requires a good rock foundation because the buttress loads are heavy.\nGravity dams.\nIn a gravity dam, the force that holds the dam in place against the push from the water is Earth's gravity pulling down on the mass of the dam. The water presses laterally (downstream) on the dam, tending to overturn the dam by rotating about its toe (a point at the bottom downstream side of the dam). The dam's weight counteracts that force, tending to rotate the dam the other way about its toe. The designer ensures that the dam is heavy enough that the dam's weight wins that contest. In engineering terms, that is true whenever the resultant of the forces of gravity acting on the dam and water pressure on the dam acts in a line that passes upstream of the toe of the dam. The designer tries to shape the dam so if one were to consider the part of the dam above any particular height to be a whole dam itself, that dam also would be held in place by gravity, i.e., there is no tension in the upstream face of the dam holding the top of the dam down. The designer does this because it is usually more practical to make a dam of material essentially just piled up than to make the material stick together against vertical tension. The shape that prevents tension in the upstream face also eliminates a balancing compression stress in the downstream face, providing additional economy.\nFor this type of dam, it is essential to have an impervious foundation with high bearing strength. Permeable foundations have a greater likelihood of generating uplift pressures under the dam. Uplift pressures are hydrostatic pressures caused by the water pressure of the reservoir pushing up against the bottom of the dam. If large enough uplift pressures are generated there is a risk of destabilizing the concrete gravity dam.\nOn a suitable site, a gravity dam can prove to be a better alternative to other types of dams. When built on a solid foundation, the gravity dam probably represents the best-developed example of dam building. Since the fear of flood is a strong motivator in many regions, gravity dams are built in some instances where an arch dam would have been more economical.\nGravity dams are classified as \"solid\" or \"hollow\" and are generally made of either concrete or masonry. The solid form is the more widely used of the two, though the hollow dam is frequently more economical to construct. Grand Coulee Dam is a solid gravity dam and Braddock Locks &amp; Dam is a hollow gravity dam.\nArch-gravity dams.\nA gravity dam can be combined with an arch dam into an arch-gravity dam for areas with massive amounts of water flow but less material available for a pure gravity dam. The inward compression of the dam by the water reduces the lateral (horizontal) force acting on the dam. Thus, the gravitational force required by the dam is lessened, i.e., the dam does not need to be so massive. This enables thinner dams and saves resources.\nBarrages.\nA barrage dam is a special kind of dam that consists of a line of large gates that can be opened or closed to control the amount of water passing the dam. The gates are set between flanking piers which are responsible for supporting the water load, and are often used to control and stabilize water flow for irrigation systems. An example of this type of dam is the now-decommissioned Red Bluff Diversion Dam on the Sacramento River near Red Bluff, California.\nBarrages that are built at the mouths of rivers or lagoons to prevent tidal incursions or use the tidal flow for tidal power are known as tidal barrages.\nEmbankment dams.\nEmbankment dams are made of compacted earth, and are of two main types: rock-fill and earth-fill. Like concrete gravity dams, embankment dams rely on their weight to hold back the force of water.\nFixed-crest dams.\nA fixed-crest dam is a concrete barrier across a river. Fixed-crest dams are designed to maintain depth in the channel for navigation. They pose risks to boaters who may travel over them, as they are hard to spot from the water and create induced currents that are difficult to escape.\nBy size.\nThere is variability, both worldwide and within individual countries, such as in the United States, in how dams of different sizes are categorized. Dam size influences construction, repair, and removal costs and affects the dams' potential range and magnitude of environmental disturbances.\nLarge dams.\nThe International Commission on Large Dams (ICOLD) defines a \"large dam\" as \"A dam with a height of or greater from lowest foundation to crest or a dam between metres and 15 metres impounding more than \". \"Major dams\" are over in height. The \"Report of the World Commission on Dams\" also includes in the \"large\" category, dams which are between high with a reservoir capacity of more than . Hydropower dams can be classified as either \"high-head\" (greater than 30 m in height) or \"low-head\" (less than 30 m in height).\nAs of 2021[ [update]], ICOLD's World Register of Dams contains 58,700 large dam records. The tallest dam in the world is the Jinping-I Dam in China.\nSmall dams.\nAs with large dams, small dams have multiple uses, such as, but not limited to, hydropower production, flood protection, and water storage. Small dams can be particularly useful on farms to capture runoff for later use, for example, during the dry season. Small scale dams have the potential to generate benefits without displacing people as well, and small, decentralised hydroelectric dams can aid rural development in developing countries. In the United States alone, there are approximately 2,000,000 or more \"small\" dams that are not included in the Army Corps of Engineers National Inventory of dams. Records of small dams are kept by state regulatory agencies and therefore information about small dams is dispersed and uneven in geographic coverage.\nCountries worldwide consider small hydropower plants (SHPs) important for their energy strategies, and there has been a notable increase in interest in SHPs. Couto and Olden (2018) conducted a global study and found 82,891 small hydropower plants (SHPs) operating or under construction. Technical definitions of SHPs, such as their maximum generation capacity, dam height, reservoir area, etc., vary by country.\nNon-jurisdictional dams.\nA dam is non-jurisdictional when its size (usually \"small\") excludes it from being subject to certain legal regulations. The technical criteria for categorising a dam as \"jurisdictional\" or \"non-jurisdictional\" varies by location. In the United States, each state defines what constitutes a non-jurisdictional dam. In the state of Colorado a non-jurisdictional dam is defined as a dam creating a reservoir with a capacity of 100 acre-feet or less and a surface area of 20 acres or less and with a height measured as defined in Rules 4.2.5.1. and 4.2.19 of 10 feet or less. In contrast, the state of New Mexico defines a jurisdictional dam as 25 feet or greater in height and storing more than 15 acre-feet or a dam that stores 50 acre-feet or greater and is six feet or more in height (section 72-5-32 NMSA), suggesting that dams that do not meet these requirements are non-jurisdictional. Most US dams, 2.41\u00a0million of a total of 2.5\u00a0million dams, are not under the jurisdiction of any public agency (i.e., they are non-jurisdictional), nor are they listed on the National Inventory of Dams (NID).\nSmall dams incur risks similar to large dams. However, the absence of regulation (unlike more regulated large dams) and of an inventory of small dams (i.e., those that are non-jurisdictional) can lead to significant risks for both humans and ecosystems. For example, according to the US National Park Service (NPS), \"Non-jurisdictional\u2014means a structure which does not meet the minimum criteria, as listed in the Federal Guidelines for Dam Safety, to be included in dam safety programs. The non-jurisdictional structure does not receive a hazard classification and is not considered for any further requirements or activities under the NPS dam safety program.\" Small dams can be dangerous individually (i.e., they can fail), but also collectively, as an aggregation of small dams along a river or within a geographic area can multiply risks. Graham's 1999 study of US dam failures resulting in fatalities from 1960 to 1998 concluded that the failure of dams between 6.1 and 15 m high (typical height range of smaller dams) caused 86% of the deaths, and the failure of dams less than 6.1 m high caused 2% of the deaths. Non-jurisdictional dams may pose hazards because their design, construction, maintenance, and surveillance is unregulated. Scholars have noted that more research is needed to better understand the environmental impact of small dams (e.g., their potential to alter the flow, temperature, sediment and plant and animal diversity of a river).\nBy use.\nSaddle dam.\nA saddle dam is an auxiliary dam constructed to confine the reservoir created by a primary dam either to permit a higher water elevation and storage or to limit the extent of a reservoir for increased efficiency. An auxiliary dam is constructed in a low spot or \"saddle\" through which the reservoir would otherwise escape. On occasion, a reservoir is contained by a similar structure called a dike to prevent inundation of nearby land. Dikes are commonly used for reclamation of arable land from a shallow lake, similar to a levee, which is a wall or embankment built along a river or stream to protect adjacent land from flooding.\nWeir.\nA weir (sometimes called an \"overflow dam\") is a small dam that is often used in a river channel to create an impoundment lake for water abstraction purposes. It can also be used for flow measurement or retardation.\nCheck dam.\nA check dam is a small dam designed to reduce flow velocity and control soil erosion. Conversely, a wing dam is a structure that only partly restricts a waterway, creating a faster channel that resists the accumulation of sediment.\nDry dam.\nA dry dam, also known as a flood retarding structure, is designed to control flooding. It normally holds back no water and allows the channel to flow freely, except during periods of intense flow that would otherwise cause flooding downstream.\nDiversionary dam.\nA diversionary dam is designed to divert all or a portion of the flow of a river from its natural course. The water may be redirected into a canal or tunnel for irrigation and/or hydroelectric power production.\nUnderground dam.\nUnderground dams are used to trap groundwater and store all or most of it below the surface for extended use in a localized area. In some cases, they are also built to prevent saltwater from intruding into a freshwater aquifer. Underground dams are typically constructed in areas where water resources are minimal and need to be efficiently stored, such as in deserts and on islands like the Fukuzato Dam in Okinawa, Japan. They are most common in northeastern Africa and the arid areas of Brazil while also being used in the southwestern United States, Mexico, India, Germany, Italy, Greece, France and Japan.\nThere are two types of underground dams: \"sub-surface\" and a \"sand-storage\". A sub-surface dam is built across an aquifer or drainage route from an impervious layer (such as solid bedrock) up to just below the surface. They can be constructed of a variety of materials to include bricks, stones, concrete, steel or PVC. Once built, the water stored behind the dam raises the water table and is then extracted with wells. A sand-storage dam is a weir built in stages across a stream or wadi. It must be strong, as floods will wash over its crest. Over time, sand accumulates in layers behind the dam, which helps store water and, most importantly, prevent evaporation. The stored water can be extracted with a well, through the dam body, or by means of a drain pipe.\nTailings dam.\nA tailings dam is typically an earth-fill embankment dam used to store tailings, which are produced during mining operations after separating the valuable fraction from the uneconomic fraction of an ore. Conventional water retention dams can serve this purpose, but due to cost, a tailings dam is more viable. Unlike water retention dams, a tailings dam is raised in succession throughout the life of the particular mine. Typically, a base or starter dam is constructed, and as it fills with a mixture of tailings and water, it is raised. Material used to raise the dam can include the tailings (depending on their size) along with soil.\nThere are three raised tailings dam designs, the \"upstream\", \"downstream\", and \"centerline\", named according to the movement of the crest during raising. The specific design used is dependent upon topography, geology, climate, the type of tailings, and cost. An upstream tailings dam consists of trapezoidal embankments being constructed on top but toe to crest of another, moving the crest further upstream. This creates a relatively flat downstream side and a jagged upstream side which is supported by tailings slurry in the impoundment. The downstream design refers to the successive raising of the embankment that positions the fill and crest further downstream. A centerlined dam has sequential embankment dams constructed directly on top of another while fill is placed on the downstream side for support and slurry supports the upstream side.\nBecause tailings dams often store toxic chemicals from the mining process, modern designs incorporate an impervious geomembrane liner to prevent seepage. Water/slurry levels in the tailings pond must be managed for stability and environmental purposes as well.\nBy material.\nSteel dams.\nA steel dam is a type of dam briefly experimented with around the start of the 20th century which uses steel plating (at an angle) and load-bearing beams as the structure. Intended as permanent structures, steel dams were an (failed) experiment to determine if a construction technique could be devised that was cheaper than masonry, concrete or earthworks, but sturdier than timber crib dams.\nTimber dams.\nTimber dams were widely used in the early part of the industrial revolution and in frontier areas due to ease and speed of construction. Rarely built in modern times because of their relatively short lifespan and the limited height to which they can be built, timber dams must be kept constantly wet in order to maintain their water retention properties and limit deterioration by rot, similar to a barrel. The locations where timber dams are most economical to build are those where timber is plentiful, cement is costly or difficult to transport, and either a low head diversion dam is required or longevity is not an issue. Timber dams were once numerous, especially in the North American West, but most have failed, been hidden under earth embankments, or been replaced with entirely new structures. Two common variations of timber dams were the \"crib\" and the \"plank\".\nTimber crib dams were erected of heavy timbers or dressed logs in the manner of a log house and the interior filled with earth or rubble. The heavy crib structure supported the dam's face and the weight of the water. Splash dams were timber crib dams used to help float logs downstream in the late 19th and early 20th centuries.\n\"Timber plank dams\" were more elegant structures that employed a variety of construction methods using heavy timbers to support a water retaining arrangement of planks.\nOther types.\nCofferdams.\nA cofferdam is a barrier, usually temporary, constructed to exclude water from an area that is normally submerged. Made commonly of wood, concrete, or steel sheet piling, cofferdams are used to allow construction on the foundation of permanent dams, bridges, and similar structures. When the project is completed, the cofferdam will usually be demolished or removed unless the area requires continuous maintenance. (See also causeway and retaining wall.)\nCommon uses for cofferdams include the construction and repair of offshore oil platforms. In such cases, the cofferdam is fabricated from sheet steel and welded into place under water. Air is pumped into the space, displacing the water and allowing a dry work environment below the surface.\nNatural dams.\nDams can also be created by natural geological forces. Lava dams are formed when lava flows, often basaltic, intercept the path of a stream or lake outlet, resulting in the creation of a natural impoundment. An example would be the eruptions of the Uinkaret volcanic field about 1.8\u00a0million\u201310,000 years ago, which created lava dams on the Colorado River in northern Arizona in the United States. The largest such lake grew to about in length before the failure of its dam. Glacial activity can also form natural dams, such as the damming of the Clark Fork in Montana by the Cordilleran Ice Sheet, which formed the Glacial Lake Missoula near the end of the last Ice Age. Moraine deposits left behind by glaciers can also dam rivers to form lakes, such as at Flathead Lake, also in Montana (see Moraine-dammed lake).\nNatural disasters such as earthquakes and landslides frequently create landslide dams in mountainous regions with unstable local geology. Historical examples include the Usoi Dam in Tajikistan, which blocks the Murghab River to create Sarez Lake. At high, it is the tallest dam in the world, including both natural and man-made dams. A more recent example would be the creation of Attabad Lake by a landslide on Pakistan's Hunza River.\nNatural dams often pose significant hazards to human settlements and infrastructure. The resulting lakes often flood inhabited areas, while a catastrophic failure of the dam could cause even greater damage, such as the failure of western Wyoming's Gros Ventre landslide in 1927, which wiped out the town of Kelly resulting in the deaths of six people.\nBeaver dams.\nBeavers create dams primarily out of mud and sticks to flood a particular habitable area. By flooding a parcel of land, beavers can navigate below or near the surface and remain relatively well hidden or protected from predators. The flooded region also allows beavers access to food, especially during the winter.\nConstruction elements.\nPower generation plant.\nAs of 2005[ [update]], hydroelectric power, mostly from dams, supplies some 19% of the world's electricity, and over 63% of renewable energy. Much of this is generated by large dams, although China uses small-scale hydro generation on a wide scale and is responsible for about 50% of world use of this type of power.\nMost hydroelectric power comes from the potential energy of dammed water driving a water turbine and generator; to boost the power generation capabilities of a dam, the water may be run through a large pipe called a penstock before the turbine. A variant on this simple model uses pumped-storage hydroelectricity to produce electricity to match periods of high and low demand, by moving water between reservoirs at different elevations. At times of low electrical demand, excess generation capacity is used to pump water into the higher reservoir. When there is higher demand, water is released back into the lower reservoir through a turbine. (For example, see Dinorwig Power Station.)\nSpillways.\nA spillway is a section of a dam designed to pass water from the upstream side of a dam to the downstream side. Many spillways have floodgates designed to control the flow through the spillway. There are several types of spillway. A \"service spillway\" or \"primary spillway\" passes normal flow. An \"auxiliary spillway\" releases flow in excess of the capacity of the service spillway. An \"emergency spillway\" is designed for extreme conditions, such as a serious malfunction of the service spillway. A \"fuse plug spillway\" is a low embankment designed to be overtopped and washed away in the event of a large flood. The elements of a fuse plug are independent free-standing blocks, set side by side which work without any remote control. They allow increasing the normal pool of the dam without compromising the security of the dam because they are designed to be gradually evacuated for exceptional events. They work as fixed weirs at times by allowing overflow in common floods.\nA spillway can be gradually eroded by water flow, including cavitation or turbulence of the water flowing over the spillway, leading to its failure. It was the inadequate design of the spillway and installation of fish screens that led to the 1889 over-topping of the South Fork Dam in Johnstown, Pennsylvania, resulting in the Johnstown Flood (the \"great flood of 1889\").\nErosion rates are often monitored, and the risk is ordinarily minimized, by shaping the downstream face of the spillway into a curve that minimizes turbulent flow, such as an ogee curve.\nCreation.\nCommon purposes.\nSome of these purposes are conflicting, and the dam operator needs to make dynamic tradeoffs. For example, power generation and water supply would keep the reservoir high, whereas flood prevention would keep it low. Many dams in areas where precipitation fluctuates in an annual cycle will also see the reservoir fluctuate annually in an attempt to balance these different purposes. Dam management becomes a complex exercise amongst competing stakeholders.\nLocation.\nOne of the best places for building a dam is a narrow part of a deep river valley; the valley sides can then act as natural walls. The primary function of the dam's structure is to fill the gap in the natural reservoir line left by the stream channel. The sites are usually those where the gap becomes a minimum for the required storage capacity. The most economical arrangement is often a composite structure such as a masonry dam flanked by earth embankments. The current use of the land to be flooded should be dispensable.\nSignificant other engineering and engineering geology considerations when building a dam include:\nImpact assessment.\nImpact is assessed in several ways: the benefits to human society arising from the dam (agriculture, water, damage prevention and power), harm or benefit to nature and wildlife, impact on the geology of an area (whether the change to water flow and levels will increase or decrease stability), and the disruption to human lives (relocation, loss of archeological or cultural matters underwater).\nEnvironmental impact.\nReservoirs held behind dams affect many ecological aspects of a river. Rivers topography and dynamics depend on a wide range of flows, whilst rivers below dams often experience long periods of very stable flow conditions or sawtooth flow patterns caused by releases followed by no releases. Water releases from a reservoir including that exiting a turbine usually contain very little suspended sediment, and this, in turn, can lead to scouring of river beds and loss of riverbanks; for example, the daily cyclic flow variation caused by the Glen Canyon Dam was a contributor to sand bar erosion.\nOlder dams often lack a fish ladder, which keeps many fish from moving upstream to their natural breeding grounds, causing failure of breeding cycles or blocking of migration paths. Even fish ladders do not prevent a reduction in fish reaching the spawning grounds upstream. In some areas, young fish (\"smolt\") are transported downstream by barge during parts of the year. Turbine and power-plant designs that have a lower impact upon aquatic life are an active area of research.\nAt the same time, however, some particular dams may contribute to the establishment of better conditions for some kinds of fish and other aquatic organisms. Studies have demonstrated the key role played by tributaries in the downstream direction from the main river impoundment, which influenced local environmental conditions and beta diversity patterns of each biological group. Both replacement and richness differences contributed to high values of total beta diversity for fish (average\u202f=\u202f0.77) and phytoplankton (average\u202f=\u202f0.79), but their relative importance was more associated with the replacement component for both biological groups (average\u202f=\u202f0.45 and 0.52, respectively). A study conducted by de Almeida, R. A., Steiner, M.T.A and others found that, while some species declined in population by more than 30% after the building of the dam, others increased their population by 28%. Such changes may be explained by the fact that the fish obtained \"different feeding habits, with almost all species being found in more than one group.\nA large dam can cause the loss of entire ecospheres, including endangered and undiscovered species in the area, and the replacement of the original environment by a new inland lake. As a result, the construction of dams have been opposed in various countries with some, such as Tasmania's Franklin Dam project, being cancelled following environmentalist campaigns.\nLarge reservoirs formed behind dams have been indicated in the contribution of seismic activity, due to changes in water load and/or the height of the water table. However, this is a mistaken assumption, because the relatively marginal stress attributed to the water load is orders of magnitude lesser than the force of an earthquake. The increased stress from the water load is insufficient to fracture the Earth's crust, and thus does not increase the severity of an earthquake.\nDams are also found to influence global warming. The changing water levels in reservoirs are a source for greenhouse gases like methane. While dams and the water behind them cover only a small portion of earth's surface, they harbour biological activity that can produce large quantities of greenhouse gases.\nHuman social impact.\nDams' impact on human society is significant. Nick Cullather argues in \"Hungry World: America's Cold War Battle Against Poverty in Asia\" that dam construction requires the state to displace people in the name of the common good, and that it often leads to abuses of the masses by planners. He cites Morarji Desai, Interior Minister of India, in 1960 speaking to villagers upset about the Pong Dam, who threatened to \"release the waters\" and drown the villagers if they did not cooperate.\nThe Three Gorges Dam on the Yangtze River in China is more than five times the size of the Hoover Dam (U.S.). It creates a reservoir long to be used for flood control and hydropower generation. Its construction required the loss of over a million people's homes and their mass relocation, the loss of many valuable archaeological and cultural sites, and significant ecological change. During the 2010 China floods, the dam held back a what would have been a disastrous flood and the huge reservoir rose by 4\u00a0m (13\u00a0ft) overnight.\nIn 2008, it was estimated that 40\u201380\u00a0million people worldwide have been displaced from their homes as a result of dam construction.\nEconomics.\nConstruction of a hydroelectric plant requires a long lead time for site studies, hydrological studies, and environmental impact assessments, and are large-scale projects in comparison to carbon-based power generation. The number of sites that can be economically developed for hydroelectric production is limited; new sites tend to be far from population centers and usually require extensive power transmission lines. Hydroelectric generation can be vulnerable to major changes in the climate, including variations in rainfall, ground and surface water levels, and glacial melt, causing additional expenditure for the extra capacity to ensure sufficient power is available in low-water years.\nOnce completed, if it is well designed and maintained, a hydroelectric power source is usually comparatively cheap and reliable. It has no fuel and low escape risk, and as a clean energy source it is cheaper than both nuclear and wind power. It is more easily regulated to store water as needed and generate high power levels on demand compared to wind power.\nReservoir and dam improvements.\nDespite some positive effects, the construction of dams severely affects river ecosystems leading to degraded riverine ecosystems as part of the hydrological alteration. One of the main ways to reduce the negative impacts of reservoirs and dams is to implement the newest nature-based reservoir optimization model for resolving the conflict in human water demand and riverine ecosystem protection.\nDam removal.\nWater and sediment flows can be re-established by removing dams from a river. Dam removal is considered appropriate when the dam is old and maintenance costs exceed the expense of its removal. Some effects of dam removal include erosion of sediment in the reservoir, increased sediment supply downstream, increased river width and braiding, re-establishment of natural water temperatures and recolonisation of habitats that were previously unavailable due to dams.\nThe world's largest dam removal occurred on the Elwha river in the U.S. state of Washington (see Restoration of the Elwha River). Two dams, the Elwha and Glynes Canyon dams, were removed between 2011 and 2014 that together stored approximately 30 Mt of sediment. As a result, the delivery of sediment and wood to the downstream river and delta were re-established. Approximately 65% of the sediment stored in the reservoirs eroded, of which ~10% was deposited in the riverbed. The remaining ~90% was transported to the coast. In total, renewed sediment delivery caused approximately 60 ha of delta growth, and also resulted in increased river braiding.\nFailure.\nDam failures are generally catastrophic if the structure is breached or significantly damaged. Routine deformation monitoring and monitoring of seepage from drains in and around larger dams is useful to anticipate any problems and permit remedial action to be taken before structural failure occurs. Most dams incorporate mechanisms to permit the reservoir to be lowered or even drained in the event of such problems. Another solution can be rock grouting\u00a0\u2013 pressure pumping Portland cement slurry into weak fractured rock.\nDuring an armed conflict, a dam is to be considered as an \"installation containing dangerous forces\" due to the massive impact of possible destruction on the civilian population and the environment. As such, it is protected by the rules of international humanitarian law (IHL) and shall not be made the object of attack if that may cause severe losses among the civilian population. To facilitate the identification, a protective sign consisting of three bright orange circles placed on the same axis is defined by the rules of IHL.\nThe main causes of dam failure include inadequate spillway capacity, piping through the embankment, foundation or abutments, spillway design error (South Fork Dam), geological instability caused by changes to water levels during filling or poor surveying (Vajont, Malpasset, Testalinden Creek dams), poor maintenance, especially of outlet pipes (Lawn Lake Dam, Val di Stava Dam collapse), extreme rainfall (Shakidor Dam), earthquakes, and human, computer or design error (Buffalo Creek Flood, Dale Dike Reservoir, Taum Sauk pumped storage plant).\nA notable case of deliberate dam failure (prior to the above ruling) was the Royal Air Force 'Dambusters' raid on Germany in World War II (codenamed \"Operation Chastise\"), in which three German dams were selected to be breached in order to damage German infrastructure and manufacturing and power capabilities deriving from the Ruhr and Eder rivers. This raid later became the basis for several films.\nSince 2007, the Dutch IJkdijk foundation is developing, with an open innovation model and early warning system for levee/dike failures. As a part of the development effort, full-scale dikes are destroyed in the IJkdijk fieldlab. The destruction process is monitored by sensor networks from an international group of companies and scientific institutions.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51519", "revid": "50213014", "url": "https://en.wikipedia.org/wiki?curid=51519", "title": "Pratt &amp; Whitney", "text": "Aircraft engine manufacturer\nPratt &amp; Whitney is an American aerospace manufacturer with global service operations. It is a subsidiary of RTX Corporation (formerly Raytheon Technologies). Pratt &amp; Whitney's aircraft engines are widely used in both civil aviation (especially airliners) and military aviation. Its headquarters are in East Hartford, Connecticut. The company is the world's second largest commercial aircraft engine manufacturer, with a 35% market share as of 2020[ [update]]. In addition to aircraft engines, Pratt &amp; Whitney manufactures gas turbine engines for industrial use, marine propulsion, and power generation. In 2017, the company reported that it supported more than 11,000 customers in 180 countries around the world.\nHistory.\nEarly history.\nIn April 1925, Frederick Rentschler, an Ohio native and former executive at Wright Aeronautical, was determined to start an aviation-related business of his own. His social network included Edward Deeds, another prominent Ohioan of the early aviation industry, and Frederick's brother Gordon Rentschler, both of whom were on the board of Niles Bement Pond, then one of the largest machine tool corporations in the world. Frederick Rentschler approached these men as he sought capital and assets for his new venture. Deeds and G. Rentschler persuaded the board of Niles Bement Pond that their Pratt &amp; Whitney Machine Tool (P&amp;WMT) subsidiary of Hartford, Connecticut, should provide the funding and location to build a new aircraft engine being developed by Rentschler, George J. Mead, and colleagues, all formerly of Wright Aeronautical. Conceived and designed by Mead, the new engine would be a large, air-cooled, radial design. Pratt &amp; Whitney Machine Tool was going through a period of self-revision at the time to prepare itself for the post-World War I era, discontinuing old product lines and incubating new ones. World War I had been profitable to P&amp;WMT, but the peace brought a predictable glut to the machine tool market, as contracts with governments were canceled and the market in used, recently built tools competed against new ones. P&amp;WMT's future growth would depend on innovation. Having idle factory space and capital available at this historical moment, to be invested wherever good return seemed available, P&amp;WMT saw the post-war aviation industry, both military and civil (commercial, private), as one with some of the greatest growth and development potential available anywhere for the next few decades. It lent Rentschler US$250,000, the use of the Pratt &amp; Whitney name, and space in their building. This was the beginning of the Pratt &amp; Whitney Aircraft Company. Pratt &amp; Whitney Aircraft's first engine, the 425-horsepower (317\u00a0kW) R-1340 Wasp, was completed on Christmas Eve 1925. On its third test run it easily passed the U.S. Navy qualification test in March 1926; by October 1926, the U.S. Navy had ordered 200. The Wasp exhibited performance and reliability that revolutionized American aviation. The R-1340 powered the aircraft of Wiley Post, Amelia Earhart, and many other record flights.\nThe R-1340 was followed by another very successful engine, the R-985 Wasp Junior. Eventually a whole Wasp series was developed. Both engines are still in use in agricultural aircraft around the world and produce more power than their original design criteria.\nGeorge Mead soon led the next step in the field of large, state-of-the-art, air-cooled, radial aircraft engines (which the Wasp dominated) when Pratt &amp; Whitney released its R-1690 Hornet. It was basically \"a bigger Wasp\".\nIn 1929, Rentschler ended his association with Pratt &amp; Whitney Machine Tool and merged Pratt &amp; Whitney Aircraft with Boeing and other companies to form the United Aircraft and Transport Corporation (UATC). His agreement allowed him to carry the Pratt &amp; Whitney name with him to his new corporation. Only five years later, in 1934, the federal government of U.S. banned common ownership of airplane manufacturers and airlines. Pratt &amp; Whitney was merged with UATC's other manufacturing interests east of the Mississippi River as United Aircraft Corporation, with Rentschler as president. In 1975, United Aircraft Corporation became United Technologies.\n21st century.\nIn October 2014, Pratt &amp; Whitney was awarded a $592 million contract with the Department of Defense (DoD) to supply 36 F135 engines for the F-35 fighter.\nIn January 2017, ten employees, including the head of the F135 engine program, reportedly left the company after expenses incurred to transport South Korean officials to the company's West Palm Beach, Florida facility in 2012 were deemed unethical.\nIn 2020, United Technologies merged with Raytheon Company to form Raytheon Technologies, with Pratt &amp; Whitney becoming one of the new corporation's four main subsidiaries.\nIn November 2022, Pratt &amp; Whitney was awarded a contract for nearly $4.4 billion by the US DoD to build 100 jet engines for the U.S. military's Air Force, Navy, and Marine Corps branches.\nAs of May 2023, Pratt &amp; Whitney was \"struggling to support its fleet of passenger jets with enough spare parts and engines\" which had consequences for airlines worldwide who had to ground their Airbus A320 Neo and Airbus A220. The durability of the Pratt &amp; Whitney PW1000G geared turbofan engine since its inception in 2016 has been the central issue.\nIn July 2023, Pratt &amp; Whitney issued a product recall that would affect hundreds of jet engines. The recall was issued due to a concern of metal parts being contaminated that could lead to cracking over time. In August 2023, airlines in the US, Europe and Asia announced that they would be temporarily reducing some flights so they could inspect aircraft affected by the recall. As of September 2023[ [update]], it was estimated that around 3,000 engines might have been manufactured with flawed components.\nHeadquarters.\nPratt &amp; Whitney is headquartered in East Hartford, Connecticut, and also has plants in Londonderry, New Hampshire; Springdale, Arkansas; Columbus, Georgia; Middletown, Connecticut; Middletown, Pennsylvania; Dallas, Texas; Palm Beach County, Florida; North Berwick, Maine; Aguadilla, Puerto Rico; Asheville, North Carolina; and Bridgeport, West Virginia.\nPratt &amp; Whitney holds the naming rights for the home stadium for the University of Connecticut Huskies football team, Rentschler Field, which is located adjacent to Pratt &amp; Whitney's East Hartford, Connecticut, campus, on Pratt's company-owned former airfield of the same name. In 2015, the stadium was renamed to Pratt &amp; Whitney Stadium at Rentschler Field in time for the 2015\u20132016 University of Connecticut football season.\nDivisions.\nPratt &amp; Whitney is a business unit of aerospace conglomerate RTX Corporation, making it a sister company to Collins Aerospace and Raytheon. It is also involved in two major joint ventures, the Engine Alliance with GE which manufactures engines for the Airbus A380, and International Aero Engines company with Rolls-Royce, MTU Aero Engines, and the Japanese Aero Engines Corporation which manufactures engines for the Airbus A320 and the McDonnell Douglas MD-90 aircraft.\nCommercial engines.\nPratt &amp; Whitney's large commercial engines power more than 25 percent of the world's passenger aircraft fleet and serve more than 800 customers in 160 countries. With over 16,000 large commercial engines installed today, Pratt &amp; Whitney provides power to hundreds of airlines and operators, from narrow-bodied airplanes to wide-bodied jumbo jetliners. In June 2007, Pratt &amp; Whitney's fleet of large commercial engines surpassed 1\u00a0billion flight hours of service.\nGlobal Material Solutions.\nPratt &amp; Whitney's Global Material Solutions (GMS) makes parts for the CFM56 engine thus giving customers an alternative in new CFM56 engine materials. In addition to engine parts, GMS provides customers with fleet management and customized maintenance service programs. United Airlines was the GMS launch customer.\nGMS received its first part certification in July 2007, when the Federal Aviation Administration (FAA) granted Parts Manufacturing Approval (PMA) certification for the GMS high-pressure turbine (HPT) shroud for the CFM56-3 engine. In March 2008, the FAA certified the GMS fan and booster with a Supplemental Type Certificate (STC) with FAA Chapter 5 life limits equal to the original type certificate holder. The STC was the first FAA certification ever granted for alternative life-limited engine parts. In May 2008, Global Material Solutions received FAA STCs for its remaining life limited parts for CFM56-3 engines.\nGlobal Service Partners.\nPratt &amp; Whitney Global Service Partners (GSP) offers overhaul, maintenance and repair services for Pratt &amp; Whitney, International Aero Engines, General Electric, Rolls-Royce, and CFMI engines. In addition to engine overhaul and repair services, GSP provides services including line maintenance, engine monitoring and diagnostics, environmentally friendly on-wing water washes, leased engines, custom engine service programs and new and repaired parts.\nPratt &amp; Whitney maintains one of the largest service center networks in the world, with more than 40 engine overhaul and maintenance centers located around the globe.\nThe Global Service Partners includes Japan Turbine Technologies (JTT). JTT started in 2000 as a joint venture between Pratt and Japan Airlines, with Japan Airlines owning 33.4 percent of the venture, and Pratt &amp; Whitney owning the rest. In July 2011 Pratt bought out Japan Airlines' share in the venture. The facility is located in the town of Taiei near the city of Narita in the Chiba Prefecture and it primarily repairs V2500, JT8D engine parts.\nMilitary Engines.\nPratt &amp; Whitney's Military Engines power 27 air forces around the globe, with nearly 11,000 military engines in service with 23 customers in 22 nations. Pratt &amp; Whitney military engines include the F135 for the F-35 Lightning II, the F119 for the F-22 Raptor, the F100 family that powers the F-15 Eagle and F-16 Falcon, the F117 for the C-17 Globemaster III, the J52 for the EA-6B Prowler, the TF33 powering E-3 AWACS, E-8 Joint STARS, B-52, and KC-135 aircraft, and the TF30 for the F-111 and F-14A. In addition, Pratt &amp; Whitney offers a global network of maintenance, repair, and overhaul facilities and military aviation service centers focused on maintaining engine readiness for their customers.\nPratt &amp; Whitney Canada.\nPratt &amp; Whitney Canada (PWC), originally Canadian Pratt &amp; Whitney Aircraft Company, and later United Aircraft of Canada, provides a large range of products, including turbofan, turboprop and turboshaft engines targeted for the regional, business, utility and military aircraft and helicopter markets. The company also designs and manufactures engines for auxiliary power units and industrial applications. Its headquarters are located in Longueuil, Quebec (just outside Montreal).\nSpeaking to Reuters June 16, 2013, ahead of the Paris Airshow 2013, Pratt &amp; Whitney President David Hess said he was confident that Canada would decide to stick with the F-35 program despite its recent discussions about having a new competition. If the orders did shift to another company, Pratt &amp; Whitney could decide to move some of the industrial base work it is currently doing in Canada, Hess said. \"We might reallocate the work elsewhere\", he said, adding that reduced order volumes would likely trigger changes in Canada.\nThe division admitted in July 2012 to providing engines and engine software for China's first attack helicopter, the Z-10. This violated U.S. export laws and resulted in a multimillion-dollar fine.\nPratt &amp; Whitney Space Propulsion.\nPratt &amp; Whitney Space Propulsion consisted of liquid space propulsion at the Liquid Space Propulsion Division (West Palm Beach, Florida) and solid rocket propulsion at the Chemical Systems Division (San Jose, California), as well as refurbishment and integration of the non-motor elements of the Space Shuttle's solid rocket boosters at the USBI Co. Division (NASA Kennedy Space Center, Florida). Pratt &amp; Whitney Space Propulsion provided advanced technology solutions to commercial, government and military customers for over four decades. Products included the RL10, the upper stage rocket engine used on the Boeing Delta and Lockheed Martin Atlas rockets, high-pressure turbopumps for the Space Shuttle Main Engines (SSME) and the RD-180 booster engine, offered by RD Amross, a partnership between Pratt &amp; Whitney and NPO Energomash of Russia, for the Atlas III and V programs. The West Palm Beach site consisted of an engineering division and manufacturing division which designed and manufactured the high-pressure turbopumps (fuel and LOX) for the Space Shuttle's Main Engines (SSME) which were manufactured by the former Rocketdyne Corporation.\nPratt &amp; Whitney Rocketdyne.\nPratt &amp; Whitney Rocketdyne (PWR) was formed in 2005 when Pratt &amp; Whitney Space Propulsion and Rocketdyne Propulsion &amp; Power were merged following the latter's acquisition from Boeing.\nP&amp;W Rocketdyne engines powered the Space Shuttle, and the company also supplies booster engines for Delta II rockets and boosters and upper stage engines for Atlas III and V and Delta IV rockets.\nIn 2013, PWR was sold to GenCorp, which merged it with Aerojet to become Aerojet Rocketdyne.\nPratt &amp; Whitney Power Systems.\nPratt &amp; Whitney Power Systems (PWPS) designs, builds, furnishes and supports aero-derivative gas turbine and geothermal power systems for customers worldwide. These industrial gas turbines power everything from small businesses to small cities. PWPS\u2019 industrial turbines not only generate electrical power, but provide variable speed mechanical drive for marine propulsion, gas compression, and liquid pumping. PWPS has over 2,000 industrial gas turbines installed in more than 40 countries worldwide. PWPS also provides parts and repairs for heavy-duty frame gas turbines as an OEM alternative.\nIn May 2013, United Technologies Corporation (UTC) sold its Pratt &amp; Whitney Power Systems unit to Mitsubishi Heavy Industries (MHI).\nInternational Aero Engines.\nInternational Aero Engines is a joint venture that develops, builds and services the V2500 aero engine family, which powers the Airbus A320 family (current engine option) and McDonnell Douglas MD-90 aircraft. The four founding engine manufacturers that make up IAE each contribute an individual module to the V2500 engine. Pratt &amp; Whitney produces the combustor and high-pressure turbine, Rolls-Royce the high-pressure compressor, JAEC the fan and low-pressure compressor and MTU the low-pressure turbine.\nEngine Alliance.\nEngine Alliance, a 50/50 joint venture between General Electric and Pratt &amp; Whitney, was formed in August 1996 to develop, manufacture and support a family of modern technology engines for new high-capacity, long-range aircraft. The main application is the GP7200, designed for use on the Airbus A380. It competes with the Rolls-Royce Trent 900, the launch engine for the aircraft.\nThe first GP7200-powered Airbus A380 entered service with Emirates on August 1, 2008, on a non-stop flight from Dubai to New York City.\nMotorsports.\nBetween 1967 and 1971, Pratt &amp; Whitney turbine engines were used in Formula One and American Championship Car Racing. The STP-Paxton Turbocar dominated the 1967 Indianapolis 500 until a transmission bearing failed four laps from the finish. STP entered four Lotus 56s in the 1968 Indianapolis 500. One car crashed during a qualifying session. Two of the remaining cars qualified fastest and second fastest, but all three retired from the race. Turbine cars were deemed illegal before the following year's race, so Lotus chief Colin Chapman developed the car for use in Formula One and an updated 56B competed in a half dozen Formula One races in 1971.\nProducts.\nEngine maintenance systems.\nPratt &amp; Whitney now markets its \"Ecopower\" pressure-washing service, which uses a high-pressure water spray run through several nozzles to clean grime and contaminants from jet engine parts, most notably turbine blades, to prevent overheating, improve engine operating efficiency and reduce fuel burn. The system collects the runoff from the washing process for appropriate disposal. The washing is accomplished at the airport tarmac in about one hour. Customers include United Airlines, Air India, Martinair, Thai Airways International, Virgin Atlantic, and JetBlue.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51520", "revid": "279219", "url": "https://en.wikipedia.org/wiki?curid=51520", "title": "Hdl cholesterol", "text": ""}
{"id": "51521", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=51521", "title": "Low-density lipoprotein", "text": "One of the five major groups of lipoprotein\nLow-density lipoprotein (LDL) is one of the five major groups of lipoprotein that transport all fat molecules around the body in extracellular water. These groups, from least dense to most dense, are chylomicrons (aka ULDL by the overall density naming convention), very low-density lipoprotein (VLDL), intermediate-density lipoprotein (IDL), low-density lipoprotein (LDL) and high-density lipoprotein (HDL). LDL delivers fat molecules to cells.\nLipoproteins transfer lipids (fats) around the body in the extracellular fluid, making fats available to body cells for receptor-mediated endocytosis. Lipoproteins are complex particles composed of multiple proteins, typically 80\u2013100 proteins per particle (organized by a single apolipoprotein B for LDL and the larger particles). A single LDL particle is about 22\u201327.5 nanometers in diameter, typically transporting 3,000 to 6,000 fat molecules per particle and varying in size according to the number and mix of fat molecules contained within. The lipids carried include all fat molecules with cholesterol, phospholipids, and triglycerides dominant; amounts of each vary considerably.\nElevated LDL is an established causal factor for the development of atherosclerotic cardiovascular disease. A normal non-atherogenic LDL-C level is 20\u201340 mg/dl. Guidelines recommend maintaining LDL-C under 2.6 mmol/L (100 mg/dl) and under 1.8 mmol/L (70 mg/dL) for those at high risk.\nBiochemistry.\nStructure.\nEach native LDL particle enables emulsification, i.e. surrounding the fatty acids being carried, enabling these fats to move around the body within the water outside cells. Each particle contains a single apolipoprotein B-100 molecule (Apo B-100, a protein that has 4536 amino acid residues and a mass of ), along with 80 to 100 additional ancillary proteins. Each LDL has a highly hydrophobic core consisting of polyunsaturated fatty acid known as linoleate and hundreds to thousands (about 1500 commonly cited as an average) of esterified and unesterified cholesterol molecules. This core also carries varying numbers of triglycerides and other fats and is surrounded by a shell of phospholipids and unesterified cholesterol, as well as the single copy of Apo B-100. LDL particles are approximately 22\u00a0nm (0.00000087\u00a0in.) to 27.5\u00a0nm in diameter and have a mass of about 3 million daltons. Since LDL particles contain a variable and changing number of fatty acid molecules, there is a distribution of LDL particle mass and size. Determining the structure of LDL has been difficult for biochemists because of its heterogeneous structure. However, the structure of LDL at human body temperature in native condition, with a resolution of about 16 Angstroms using cryogenic electron microscopy, has been described in 2011.\nPhysiology.\nLDL particles are formed when triglycerides are removed from VLDL by the lipoprotein lipase enzyme (LPL), and they become smaller and denser (i.e., fewer fat molecules with the same protein transport shell), containing a higher proportion of cholesterol esters.\nTransport into the cell.\nWhen a cell requires more cholesterol than its HMG-CoA pathway can produce, it synthesizes the necessary LDL receptors as well as PCSK9, a proprotein convertase that marks the LDL receptor for degradation. LDL receptors are inserted into the plasma membrane and diffuse freely until they associate with clathrin-coated pits. When LDL receptors bind LDL particles in the bloodstream, the clathrin-coated pits are endocytosed into the cell.\nVesicles containing LDL receptors bound to LDL are delivered to the endosomes. In the presence of low pH, such as that found in the endosome, LDL receptors undergo a conformation change, releasing LDL. LDL is then shipped to the lysosomes, where cholesterol esters in the LDL are hydrolysed. LDL receptors are typically returned to the plasma membrane, where they repeat this cycle. If LDL receptors bind to PCSK9, however, transport of LDL receptors is redirected to the lysosome, where they are degraded.\nInnate immune system.\nLDL interferes with the quorum sensing system that upregulates genes required for invasive \"Staphylococcus aureus\" infection. The mechanism of antagonism entails binding apolipoprotein B to a \"S. aureus\" autoinducer pheromone, preventing signaling through its receptor. Mice deficient in apolipoprotein B are more susceptible to invasive bacterial infection.\nSize patterns.\nLDL can be grouped based on its size: large low-density LDL particles are described as \"pattern A\", and small high-density (\"small dense\") LDL particles are \"pattern B\". \"Pattern B\" has been associated by some with a higher risk for coronary artery disease. This is thought to be because the smaller particles are more easily able to penetrate the endothelium of arterial walls. \"Pattern I\", or \"intermediate\", indicates that most LDL particles are very close in size to the normal gaps in the endothelium (26\u00a0nm). According to one study, sizes 19.0\u201320.5\u00a0nm were designated as pattern B and LDL sizes 20.6\u201322\u00a0nm were designated as pattern A.\nSome evidence suggests the correlation between pattern B and coronary artery disease is stronger than the correspondence between the LDL number measured in the standard lipid profile test. Tests to measure these LDL subtype patterns have been more expensive and not widely available, so the standard lipid profile test is used more often.\nThere has also been noted a correspondence between higher triglyceride levels and higher levels of smaller, denser LDL particles and alternately lower triglyceride levels and higher levels of the larger, less dense (\"buoyant\") LDL.\nWith continued research, decreasing cost, greater availability, and wider acceptance of other \"lipoprotein subclass analysis\" assay methods, including NMR spectroscopy, research studies have shown a stronger correlation between clinically evident human cardiovascular events and quantitatively measured particle concentrations.\nOxidized.\nOxidized LDL (oxLDL) is a general term for LDL particles with oxidatively modified structural components. As a result, from free radical attack, both lipid and protein parts of LDL can be oxidized in the vascular wall. Besides the oxidative reactions in the vascular wall, oxidized lipids in LDL can also be derived from oxidized dietary lipids. Oxidized LDL is known to associate with the development of atherosclerosis, and it is therefore widely studied as a potential risk factor of cardiovascular diseases. Atherogenicity of oxidized LDL has been explained by lack of recognition of oxidation-modified LDL structures by the LDL receptors, preventing the normal metabolism of LDL particles and leading eventually to the development of atherosclerotic plaques. Of the lipid material contained in LDL, various lipid oxidation products are known as the ultimate atherogenic species. Acting as a transporter of these injurious molecules is another mechanism by which LDL can increase the risk of atherosclerosis.\nThe LOX-1 scavenge receptor does take up oxLDL, but the liver does not naturally express it. It is instead expressed by endothelial cells, platelets, macrophages, smooth muscle cells, and cardiomyocytes as an innate immune scavenge receptor. When activated, pro-inflammatory signals are generated in the cell, and damaging compounds are released as well. As a result, these cells are most sensitive to the effects of oxLDL. SR-BI and CD36, two class B scavenge receptors, also take up oxLDL into the macrophage.\nDespite lower recognition efficacy by the LDL receptor, the liver does remove oxLDLs from the circulation. This is achieved by Kupffer cells and liver sinusoidal endothelial cells (LSECs). In LSECs, stabilin-1 and stabilin-2 mediate most of the uptake. Uptake of oxLDLs causes visible disruption to the structure of the LSEC in rats. Doing the same also damages human LSEC cultures.\nAcetyl.\nAcetyl LDL (acLDL) is a construct generated \"in vitro\". When scientists produced such a modified version of LDL, they found that a class of scavenge receptors, now called SR-A, can recognize them and take them up. Because scavenge receptors work much faster than the downregulated native LDL receptor of a macrophage, oxLDL and acLDL can both fill up a macrophage quickly, turning it into a foam cell.\nTesting.\nBlood tests commonly report LDL-C: the amount of cholesterol that is estimated to be contained with LDL particles, on average, using a formula, the Friedewald equation. In a clinical context, mathematically calculated estimates of LDL-C are commonly used to estimate how much low-density lipoproteins drive the progression of atherosclerosis. The problem with this approach is that LDL-C values are commonly discordant with both direct measurements of LDL particles and actual rates of atherosclerosis progression.\nDirect LDL measurements are also available and better reveal individual issues but are less often promoted or done due to slightly higher costs and are available from only a couple of laboratories in the United States. In 2008, the American Diabetes Association (ADA) and American College of Cardiology (ACC) recognized direct LDL particle measurement by NMR as superior for assessing individual risk of cardiovascular events.\nEstimation of LDL particles via cholesterol content.\nChemical measures of lipid concentration have long been the most-used clinical measurement, not because they have the best correlation with individual outcomes but because these lab methods are less expensive and more widely available.\nThe lipid profile does not measure LDL particles. It only estimates them using the Friedewald equation by subtracting the amount of cholesterol associated with other particles, such as HDL and VLDL, assuming a prolonged fasting state, etc.:\n formula_1\nwhere \"H\" is HDL cholesterol, \"L\" is LDL cholesterol, \"C\" is total cholesterol, \"T\" is triglycerides, and \"k\" is 0.20 if the quantities are measured in mg/dL and 0.45 in mmol/L.\n\"L\" is often reported as CLDL-C (calculated low-density lipoprotein cholesterol). This is the most common way of estimating the amount of cholesterol carried by low-density lipoprotein.\nThere are limitations to this method, most notably that samples must be obtained after a 12 to 14 h fast and that LDL-C cannot be calculated if plasma triglyceride is &gt;\u00a04.52\u00a0mmol/L (400\u00a0mg/dL). Even at triglyceride levels of 2.5 to 4.5 mmol/L, this formula is considered inaccurate. If both total cholesterol and triglyceride levels are elevated then a modified formula, with quantities in mg/dL, may be used\nformula_2\nThis formula provides an approximation with fair accuracy for most people, assuming the blood was drawn after fasting for about 14 hours or longer, but does not reveal the actual LDL particle concentration because the percentage of fat molecules within the LDL particles, which are cholesterol, varies as much as 8:1 variation. There are several formulas published addressing the inaccuracy in LDL-C estimation. The inaccuracy is based on the assumption that VLDL-C (very low density lipoprotein cholesterol) is always one-fifth of the triglyceride concentration. Other formulae address this issue by using an adjustable factor or using a regression equation. There are few studies which have compared the LDL-C values derived from this formula and values obtained by direct enzymatic method. Direct enzymatic methods are found to be accurate and must be the test of choice in clinical situations. In resource-poor settings, the option to use the formula has to be considered.\nHowever, the concentration of LDL particles, and to a lesser extent, their size, has a stronger and consistent correlation with individual clinical outcomes than the amount of cholesterol within LDL particles, even if the LDL-C estimation is approximately correct. There is increasing evidence and recognition of the value of more targeted and accurate measurements of LDL particles. Specifically, LDL particle number (concentration) and, to a lesser extent, size have shown slightly stronger correlations with atherosclerotic progression and cardiovascular events than obtained using chemical measures of the amount of cholesterol carried by the LDL particles. It is possible that the LDL cholesterol concentration can be low, yet LDL particle number high and cardiovascular events rates are high. Correspondingly, it is possible that LDL cholesterol concentration can be relatively high, yet LDL particle number is low, and cardiovascular events are also low.\nNormal ranges.\nIn the US, the American Heart Association, National Institutes of Health (NIH), and National Cholesterol Education Program (NCEP) provide a set of guidelines for fasting LDL-cholesterol levels, estimated or measured, and risk for heart disease. As of about 2005, these guidelines were:\nOver time, with more clinical research, these recommended levels keep being reduced because LDL reduction, including to abnormally low levels, was the most effective strategy for reducing cardiovascular death rates in one large double blind, randomized clinical trial of men with hypercholesterolemia; far more effective than coronary angioplasty/stenting or bypass surgery.\nThe 2004 updated American Heart Association, NIH, and NCEP recommendations for people with known atherosclerosis diseases are for lowering LDL levels to less than 70\u00a0mg/dL. This low level of less than 70\u00a0mg/dL was recommended for primary prevention of 'very-high risk patients' and secondary prevention as a 'reasonable further reduction'. This position was disputed. Statin drugs involved in such clinical trials have numerous physiological effects beyond simply the reduction of LDL levels.\nFrom longitudinal population studies following the progression of atherosclerosis-related behaviors from early childhood into adulthood, the usual LDL in childhood, before the development of fatty streaks, is about 35\u00a0mg/dL. However, all the above values refer to chemical measures of lipid/cholesterol concentration within LDL, not measured low-density lipoprotein concentrations, which is the accurate approach.\nA study was conducted measuring the effects of guideline changes on LDL cholesterol reporting and control for diabetes visits in the US from 1995 to 2004. It was found that although LDL cholesterol reporting and control for diabetes and coronary heart disease visits improved continuously between 1995 and 2004, neither the 1998 ADA guidelines nor the 2001 ATP III guidelines increased LDL cholesterol control for diabetes relative to coronary heart disease.\nDirect measurement of LDL particle concentrations.\nThere are several competing methods for measuring lipoprotein particle concentrations and size. The evidence is that the NMR methodology (developed, automated and significantly reduced in costs while improving accuracy as pioneered by Jim Otvos and associates) results in a 22-25% reduction in cardiovascular events within one year, contrary to the longstanding claims by many in the medical industry that the superiority over existing methods was weak, even by statements of some proponents.\nSince the later 1990s, because of the development of NMR measurements, it has been possible to clinically measure lipoprotein particles at lower cost [under $80 US (including shipping) and is decreasing versus the previous costs of &gt;$400 to &gt;$5,000] and higher accuracy. There are two other assays for LDL particles; however, most estimate only LDL particle concentrations like LDL-C.\nThe ADA and ACC mentioned direct LDL particle measurement by NMR in a 28 March 2008 joint consensus statement, as having advantages for predicting individual risk of atherosclerosis disease events, but the statement noted that the test is less widely available, is more expensive [about $13.00 US (2015 without insurance coverage) from some labs which use the Vantera Analyzer]. Debate continues that it is \"...unclear whether LDL particle size measurements add value to the measurement of LDL-particle concentration\", though outcomes have continuously tracked LDL particle, not LDL-C, concentrations.\nUsing NMR, the total LDL particle concentrations in nmol/L plasma are typically subdivided by percentiles referenced to the 5,382 men and women participating in the MESA trial who are not on any lipid medications.\nLDL particle concentration can also be measured by measuring the concentration of the protein ApoB, based on the generally accepted principle that each LDL or VLDL particle carries one ApoB molecule.\nOptimal ranges.\nThe LDL particle concentrations are typically categorized by percentiles, &lt;20%, 20\u201350%, 50th\u201380th%, 80th\u201395%, and &gt;95% groups of the people participating and being tracked in the MESA trial, a medical research study sponsored by the United States National Heart, Lung, and Blood Institute.\nOver time, the lowest incidence of atherosclerotic events occurs within the &lt;20% group, with increased rates for the higher groups. Multiple other measures, including particle sizes, small LDL particle concentrations, large total and HDL particle concentrations, along with estimations of insulin resistance pattern and standard cholesterol lipid measurements (for comparison of the plasma data with the estimation methods discussed above) are also routinely provided.\nLowering LDL-cholesterol.\nThe mevalonate pathway serves as the basis for the biosynthesis of many molecules, including cholesterol. The enzyme 3-hydroxy-3-methylglutaryl coenzyme A reductase (HMG CoA reductase) is an essential component and performs the first of 37 steps within the cholesterol production pathway, and is present in every animal cell. Statins block this first step.\nLDL-C is not a count of actual LDL particles. LDL-C represents how much cholesterol is being transported by all LDL particles, which is either a smaller concentration of large particles or a high concentration of small particles. LDL-C itself can be estimated by subtraction (Friedewald's method) or directly measured; see the section Testing above to see how it's measured. LDL particles carry many lipid molecules (typically 3,000 to 6,000 lipid molecules per LDL particle); this includes cholesterol, triglycerides, phospholipids and others. An LDL-C measurement cannot account for differences in size and composition between types of LDL.\nLifestyle.\nLDL cholesterol can be lowered through dietary intervention by limiting foods with saturated fat and avoiding foods with trans fat. Saturated fats are found in meat products (including poultry), full-fat dairy, eggs, and refined tropical oils like coconut and palm. Added trans fat (in the form of partially hydrogenated oils) has been banned in 53 countries. However, trans fat can still be found in red meat and dairy products as it is produced in small amounts by ruminants such as sheep and cows. LDL cholesterol can also be lowered by increasing consumption of soluble fiber and plant-based foods. Canned baked beans, whole chickpeas, navy beans, pinto beans and peas decrease total and LDL cholesterol.\nAnother lifestyle approach to reduce LDL cholesterol has been minimizing total body fat, in particular fat stored inside the abdominal cavity (visceral body fat). Visceral fat, which is more metabolically active than subcutaneous fat, has been found to produce many enzymatic signals, e.g. resistin, which increase insulin resistance and circulating VLDL particle concentrations, thus both increasing LDL particle concentrations and accelerating the development of diabetes mellitus.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51522", "revid": "49247131", "url": "https://en.wikipedia.org/wiki?curid=51522", "title": "Fokker", "text": "1912\u20131996 Dutch aircraft manufacturer\nFokker (; lit.\u2009'Royal Dutch Aircraft Factory Fokker') was a Dutch aircraft manufacturer that operated from 1912 to 1996. The company was founded by the Dutch aviator Anthony Fokker and became famous during World War I for its fighter aircraft. During its most successful period in the 1920s and 1930s, Fokker dominated the civil aviation market. The company's fortunes declined over the course of the late 20th century; it declared bankruptcy in 1996, and its operations were sold to competitors.\nHistory.\nFokker in Germany.\nAt age 20, while studying in Germany, Anthony Fokker built his initial aircraft, the \"Spin\" (Spider)\u2014the first Dutch-built plane to fly in his home country. Taking advantage of better opportunities in Germany, he moved to Berlin, where in 1912, he founded his first company, Fokker Aeroplanbau, later moving to the G\u00f6rries suburb just southwest of Schwerin (at ), where the current company was founded, as Fokker Aviatik GmbH, on 12 February 1912.\nWorld War I.\nFokker capitalized on having sold several Fokker Spin monoplanes to the German government and set up a factory in Germany to supply the German Army in World War I. His first new design for the Germans to be produced in any numbers was the Fokker M.5, which was little more than a copy of the Morane-Saulnier G, built with steel tube instead of wood for the fuselage, and with minor alterations to the outline of the rudder and undercarriage and a new aerofoil section. When it was realized that arming these scouts with a machine gun firing through the arc of the propeller was desirable, Fokker developed a synchronization gear similar to that patented by Franz Schneider.\nFitted with a developed version of this gear, the M.5 became the Fokker Eindecker, which due to its revolutionary armament, became one of the most feared aircraft over the western front, its introduction leading to a period of German air superiority known as the Fokker Scourge which only ended with the introduction of new aircraft such as the Nieuport 11 and Airco DH.2.\nDuring World War I, Fokker engineers worked on the Fokker-Leimberger, an externally powered 12-barrel Gatling gun in the 7.92\u00d757mm round claimed to be capable of firing over 7200 rounds per minute.\nLater in the war, after the Fokker D.V (the last design by earlier chief designer Martin Kreutzer), had failed to gain acceptance with the \"Luftstreitkr\u00e4fte\" the German government forced Fokker (for their aircraft production expertise) and Junkers (for their pioneering all-metal airframe construction techniques, and advanced design concepts) to cooperate more closely, which resulted in the foundation of the Junkers-Fokker Aktiengesellschaft, or Jfa, on 20 October 1917. As this partnership proved to be troublesome, it was eventually dissolved. By then, former Fokker welder and new designer Reinhold Platz, who had taken the late Martin Kreutzer's place with the firm, had adapted some of Prof. Junkers' design concepts, that resulted in a visual similarity between the aircraft of those two manufacturers during the next decade.\nSome of the noteworthy types produced by Fokker during the second half of the war, all designed primarily by Platz, included the Fokker D.VI biplane, Fokker Dr.I triplane or \"Dreidecker\" (remembered as a mount of the Red Baron), Fokker D.VII biplane (the only aircraft ever referred to directly in a treaty: all D.VII's were singled out for handover to the allies in their terms of the armistice agreement) and the Fokker D.VIII parasol monoplane.\nReturn to the Netherlands.\nIn 1919, Fokker, owing large sums in back taxes (including 14,250,000 marks of income tax), returned to the Netherlands and founded a new company near Amsterdam with the support of the \"Steenkolen Handels Vereniging\", now known as SHV Holdings. He chose the name \"Nederlandse Vliegtuigenfabriek\" (Dutch Aircraft Factory) to conceal the Fokker brand because of his involvement in World War I. Because of the strict disarmament conditions of the Treaty of Versailles, Fokker did not return home empty-handed. In 1919, he arranged an export permit and brought six entire trains of parts, and 180 types of aircraft across the Dutch-German border, among them 117 Fokker C.Is, D.VIIs, and D.VIIIs. This initial stock enabled him to set up shop quickly.\nAfter his company's relocation, many Fokker C.I and C.IV military airplanes were delivered to Russia, Romania, and the still-clandestine German air force. Success came on the commercial market, too, with the development of the Fokker F.VII, a high-winged aircraft capable of taking on various types of engines. Fokker continued to design and build military aircraft, delivering planes to the Royal Netherlands Air Force. Foreign military customers eventually included Finland, Sweden, Denmark, Norway, Switzerland, Hungary, and Italy. These countries bought substantial numbers of the Fokker C.V reconnaissance aircraft, which became Fokker's main success in the late 1920s and early 1930s.\n1920s and 1930s.\nIn the 1920s, Fokker entered its glory years, becoming the world's largest aircraft manufacturer by the late 1920s. Its greatest success was the 1925 F.VIIa/3m trimotor passenger aircraft, which was used by 54 airline companies worldwide and captured 40% of the American market in 1936. It shared the European market with the Junkers all-metal aircraft, but dominated the American market until the arrival of the Ford Trimotor which copied the aerodynamic features of the Fokker F.VII, and Junkers structural concepts.\nIn 1923, Anthony Fokker moved to the United States, where in 1927, he established an American branch of his company, the Atlantic Aircraft Corporation, which was renamed the Fokker Aircraft Corporation of America. In 1930, this company merged with General Motors Corporation and the company's name became General Aviation Manufacturing Corporation, which in turn merged with North American Aviation and was divested by GM in 1948. In 1931, discontented at being totally subordinate to GM management, Fokker resigned.\nA serious blow to Fokker's reputation came after the 1931 crash of a Transcontinental &amp; Western Air Fokker F-10 in Kansas, when it became known that the crash was caused by a structural failure caused by wood rot. Notre Dame legendary football coach Knute Rockne was among the fatalities, prompting extensive media coverage and technical investigation. As a result, all Fokkers were grounded in the US, along with many other types that had copied Fokker's wings.\nIn 1934 Nevil Shute of Airspeed Ltd (England) negotiated with Fokker himself for a manufacturing licensing agreement. In January 1935 Airspeed signed an agreement \nfor the Douglas DC-2 and a number of Fokker types, with Fokker to be a consultant for seven years. Shute found him \"genial, shrewd and helpful\" but \"already a sick man\"; and he was difficult to deal with as \"his domestic life was irregular\". Airspeed considered making the Fokker D.XVII for Greece, as Greece wanted to buy from Britain for currency reasons, but the proposal did not \"come off\"; Shute recommended reading his novel \"Ruined City\" on Balkan methods of business. And after a year the drift to war meant that Dutchmen could not go to the Airspeed factory or to board meetings.\nOn December 23, 1939, Fokker died in New York City after a three-week illness.\nWorld War II.\nAt the outset of World War II, the few G.Is and D.XXIs of the Dutch Air Force were able to score a respectable number of victories against the \"Luftwaffe\", but many were destroyed on the ground before they could be used.\nThe Fokker factories were confiscated by the Germans and were used to build B\u00fccker B\u00fc 181 Bestmann trainers and parts for the Junkers Ju 52 transport. At the end of the war, the factories were completely stripped by the Germans and destroyed by Allied bombing.\nPost\u2013World War II rebuilding.\nRebuilding after the war proved difficult. The market was flooded with cheap surplus planes from the war. The company cautiously started building gliders and autobuses and converting Dakota transport planes to civilian versions. A few F25s were built. Nevertheless, the S-11 trainer was a success, being purchased by several air forces. The S-14 Machtrainer became one of the first jet trainers, and although not an export success, it served for over a decade with the Royal Netherlands Air Force.\nA new factory was built next to Schiphol Airport near Amsterdam in 1951. A number of military planes were built there under license, among them the Gloster Meteor twin-jet fighter and Lockheed's F-104 Starfighter. A second production and maintenance facility was established at Woensdrecht.\nIn 1958, the F-27 Friendship was introduced, Fokker's most successful postwar airliner. The Dutch government contributed 27 million guilders to its development. Powered by the Rolls-Royce Dart, it became the world's best-selling turboprop airliner, reaching almost 800 units sold by 1986, including 206 under licence by Fairchild. Also, a military version of the F-27, the F-27 Troopship, was built.\nIn 1962, the F-27 was followed by the jet-powered F-28 Fellowship. Until production stopped in 1987, a total of 241 were built in various versions. Both an F-27 and later an F-28 served with the Dutch Royal Flight, Prince Bernhard himself being a pilot.\nIn 1969, Fokker agreed to an alliance with Bremen-based Vereinigte Flugtechnische Werke under control of a transnational holding company. They collaborated on an unsuccessful regional jetliner, the VFW-614, of which only 19 were sold. This collaboration ended in early 1980.\nFokker was one of the main partners in the F-16 Fighting Falcon consortium (European Participating Air Forces), which was responsible for the production of these fighters for the Belgian, Danish, Dutch and Norwegian Air Forces. It consisted of companies and government agencies from these four countries and the United States. F-16s were assembled at Fokker and at SABCA in Belgium with parts from the five countries involved.\nAerospace.\nIn 1967, Fokker started a modest space division building parts for European satellites. A major advance came in 1968 when Fokker developed the first Dutch satellite (the Astronomical Netherlands Satellite) together with Philips and Dutch universities. This was followed by a second major satellite project, IRAS, successfully launched in 1983. The European Space Agency in June 1974 named a consortium headed by ERNO-VFW-Fokker GmbH to build pressurized modules for Spacelab.\nSubsequently, Fokker contributed to many European satellite projects, as well as to the Ariane rocket in its various models. Together with a Russian contractor, they developed the huge parachute system for the Ariane 5 rocket boosters which would allow the boosters to return to Earth safely and be reused.\nThe space division became more and more independent, until just before Fokker's bankruptcy in 1996, it became a fully stand-alone corporation, known successively as Fokker Space and Systems, Fokker Space, and Dutch Space. On 1 January 2006, it was taken over by EADS-Space Transportation.\nFokker 50, Fokker 100, and Fokker 70.\nAfter a brief and unsuccessful collaboration effort with McDonnell Douglas in 1981, Fokker began an ambitious project to develop two new aircraft concurrently. The Fokker 50 was to be a completely modernised version of the F-27, and the Fokker 100 a new airliner based on the F-28. Development costs were allowed to spiral out of control, almost forcing Fokker out of business in 1987. The Dutch government bailed the company out with 212 million guilders, but demanded Fokker look for a \"strategic partner\", British Aerospace and DASA being named most likely candidates.\nInitial sales of the Fokker 100 were good, leading Fokker to begin development of the Fokker 70, a smaller version of the F100, in 1991, but sales of the F70 were below expectations and the F100 had strong competition from Boeing and Airbus by then. The Dutch government aircraft between 1996 and 2017 was a Fokker 70.\nIn 1992, after a long and arduous negotiation process, Fokker signed an agreement with DASA. This did not solve Fokker's problems, though, mostly because DASA's parent company Daimler-Benz also had to deal with its own organisational problems.\nBankruptcy.\nOn 22 January 1996, the board of directors of Daimler-Benz decided to focus on its core automobile business and cut ties with Fokker. The next day, an Amsterdam court extended temporary creditor protection.\nDiscussions were initiated with Bombardier on 5 February 1996. After having reviewed and evaluated the opportunities and challenges Fokker represented at the time, Bombardier renounced its acquisition on 27 February. On 15 March, the Fokker company was declared bankrupt.\nDifferences in national culture could have played a role in the failed takeover of Fokker by Deutsche Aerospace (DASA).\nThose divisions of the company that manufactured parts and carried out maintenance and repair work were taken over by Stork N.V.; it is now known as Stork Aerospace Group. Stork Fokker exists to sustain remarketing of the company's existing aircraft: it refurbishes and resells F 50s and F 100s, and has converted a few F 50s to transport aircraft. Special projects included the development of an F50 maritime patrol variant and an F100 executive jet. For this project, Stork received the 2005 \"Aerospace Industry Award\" in the Air Transport category from \"Flight International\" magazine.\nOther divisions of the company that were profitable continued as separate companies: Fokker Space (later Dutch Space) and Fokker Control Systems.\nIn November 2009, Stork Aerospace changed its name to Fokker Aerospace Group. As of 2011, the Fokker Aerospace Group changed its name to Fokker Technologies. The five individual business units within Fokker Technologies all carry the Fokker name:\nThe former Fokker aircraft facilities at Schiphol were redeveloped into the Fokker Logistics Park. One of the former Fokker tenants is Fokker Services.\nIn 1996, a new company named Rekkof Aircraft (\"Fokker\" backwards) attempted to restart production of the Fokker F70 and F100, supported by suppliers and airlines. The company was later renamed to Netherlands Aircraft Company, and started to focus on developing a new aircraft named F130NG. This attempt was not successful either, and the project came to a standstill in 2010. In 2023, after years of no updates, the company was renamed Fokker Next Gen. Along with the new name reveal, the company announced that they are developing a new hydrogen-powered regional airliner. Visualisations have been provided, but the name of the aircraft is yet not publicly disclosed.\nIn 2015, GKN considered acquiring Fokker Technologies to supply for the hybrid car market. The British automotive and aerospace supplier plans to buy the Netherlands-based Fokker for \u20ac706 million.\nIn 2021, Fokker Services and Fokker Techniek were acquired by Panta Holdings, a Dutch investment fund. The acquisition sought to strengthen Panta Holdings\u2019 aerospace footprint. Panta Holdings also owns Fokker Next Gen.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51524", "revid": "50796328", "url": "https://en.wikipedia.org/wiki?curid=51524", "title": "The Pink Panther", "text": "Comedy films and cartoons franchise\nThe Pink Panther is an American media franchise primarily focusing on a series of comedy-mystery films featuring an inept French police detective, Inspector Jacques Clouseau. The franchise began with the release of the film \"The Pink Panther\" in 1963. The role of Clouseau was originated by and is most closely associated with Peter Sellers. Most of the films were written and directed by Blake Edwards, with theme music composed by Henry Mancini. Elements and characters inspired by the films were adapted into other media, including books, comic books, video games and animated series.\nThe first film in the series derives its title from a pink diamond that has enormous size and value. The diamond is called the \"Pink Panther\" because the flaw at its center, when viewed closely, is said to resemble a leaping pink panther. The phrase reappears in the title of the fourth film, \"The Return of the Pink Panther\" (1975), in which the theft of the diamond is again the center of the plot. The phrase was used for all the subsequent films in the series, even when the jewel did not figure in the plot. The jewel ultimately appeared in six of the eleven films.\nThe first film in the series had an animated opening sequence, created by DePatie\u2013Freleng Enterprises, featuring \"The Pink Panther Theme\" by Mancini, as well as the Pink Panther character. Designed by Hawley Pratt and Friz Freleng, the animated Pink Panther character was subsequently featured in a series of theatrical cartoons, starting with \"The Pink Phink\" in 1964. The cartoon series gained its highest profile on television, aired on Saturday mornings as \"The Pink Panther Show\". The character returned to the film series opening sequences in 1975.\nFilms.\nOriginal series.\n\"The Pink Panther\" (1963).\n\"The Pink Panther\" (1963), the original film of the series, centered on the Phantom/Sir Charles Lytton, portrayed by David Niven. It is set in the ski resort of Cortina d'Ampezzo. Peter Sellers's performance was so popular that the resulting series was built on the Clouseau character rather than the Phantom character. Niven and Sellers's co-stars included Capucine, Robert Wagner, and Claudia Cardinale.\n\"A Shot in the Dark\" (1964).\n\"A Shot in the Dark\" (1964) was released less than a year after \"The Pink Panther\", and was the first to feature the Clouseau character as the protagonist of the film, investigating a murder set in a mansion in Paris. This film marked the first appearance of many of the tropes and supporting characters long associated with the series, including Commissioner Dreyfus (Herbert Lom), his assistant Fran\u00e7ois (Andr\u00e9 Maranne), and Clouseau's manservant, Cato (Burt Kwouk). Elke Sommer, George Sanders, Graham Stark, Tracy Reed and Douglas Wilmer also appeared in the film.\n\"Inspector Clouseau\" (1968).\nThe 1968 film \"Inspector Clouseau\" stars Alan Arkin as Clouseau, and does not feature any other recurring characters from the rest of the series. Although it was produced by the Mirisch Corporation (who owned the rights to the Pink Panther and Clouseau characters), key people associated with the earlier films, such as Peter Sellers, Blake Edwards, and Henry Mancini, were not involved in the making of this film.\n\"The Return of the Pink Panther\" (1975).\nMore than a decade after his previous portrayal, Peter Sellers returned as Clouseau in 1975's \"The Return of the Pink Panther\". The film marked the return of the famous \"Pink Panther\" diamond as well as most of the creative team associated with the prior films, including director Blake Edwards, composer Henry Mancini, Herbert Lom as Dreyfus, Burt Kwouk as Cato and Andr\u00e9 Maranne as Fran\u00e7ois. Christopher Plummer appears as Sir Charles Lytton, after David Niven declined to reprise the role. The film also co-starred Catherine Schell, Peter Arne, and Graham Stark.\n\"The Pink Panther Strikes Again\" (1976).\nIn \"The Pink Panther Strikes Again\" (1976), Dreyfus' insanity reached its zenith, as he tried to blackmail the rest of the world into killing Clouseau. It co-starred Leonard Rossiter, Lesley-Anne Down, Michael Robbins, Colin Blakely, and featured an uncredited cameo by Omar Sharif.\n\"Revenge of the Pink Panther\" (1978).\n\"Revenge of the Pink Panther\" (1978) pitted Clouseau against the French Connection. It is the last in which Sellers played Clouseau. It co-starred Dyan Cannon, Robert Webber, Robert Loggia and Graham Stark.\n\"Trail of the Pink Panther\" (1982).\n\"Trail of the Pink Panther\" (1982) was the first Pink Panther film made after Peter Sellers' death in 1980. Sellers' role is created by using scenes cut from \"Strikes Again\", as well as flashbacks from the previous \"Pink Panther\" films. This movie was intended as a tribute to Sellers, but after its release, Sellers' widow Lynne Frederick successfully sued Edwards and Metro-Goldwyn-Mayer for tarnishing her late husband's memory. David Niven and Capucine reprise their original roles from the first Pink Panther film. \"Trail\" was a critical and commercial failure.\n\"Curse of the Pink Panther\" (1983).\n1983's \"Curse of the Pink Panther\" is the first to feature a different lead character, blundering American detective Sgt. Clifton Sleigh, portrayed by Ted Wass. Inspector Clouseau and the Pink Panther diamond, both of which had disappeared in \"Trail\", are pursued by Sleigh. Clouseau returns, after having plastic surgery to disguise his identity, in a cameo appearance by Roger Moore (who is credited as \"Turk Thrust II\"). Although intended to spawn a new series of misadventures for the inept Sergeant Sleigh, the film's dismal box-office performance and critical drubbing, along with a complicated series of lawsuits between Edwards and Metro-Goldwyn-Mayer, led to a decade-long hiatus of the series. The lawsuit was eventually settled out of court in 1988, around the time Edwards came up with one final film idea that would ultimately become the unofficial series finale.\n\"Son of the Pink Panther\" (1993).\nIn \"Son of the Pink Panther\" (1993), Blake Edwards made one final attempt to revive the \"Pink Panther\" series, this time by casting Italian actor Roberto Benigni as Gendarme Jacques Gambrelli, Inspector Clouseau's illegitimate son by Maria Gambrelli, the murder suspect from \"A Shot in the Dark\" (1964). Once again, regular \"Panther\" co-stars return \u2013 Herbert Lom, Burt Kwouk, and Graham Stark, and a star of the original 1963 film, Claudia Cardinale. Although intended to relaunch the series with the blundering Jacques as a lead, \"Son\" failed both critically and commercially and became the final installment in the original Pink Panther series. It was also the final film for both retiring director Blake Edwards and composer Henry Mancini, who died in 1994.\nReboot series.\n\"The Pink Panther\" (2006).\nThis reboot launches a new \"Pink Panther\" film series starring Steve Martin as Inspector Clouseau and Kevin Kline as Chief Inspector Dreyfus. Not a remake of the original film, it forms a new starting point for a contemporary series, introducing the Clouseau and Dreyfus characters along with the famous diamond to a new generation. The film was panned by most critics, and grossed $164.1 million against an $80 million budget.\n\"The Pink Panther 2\" (2009).\nThe sequel to Steve Martin's 2006 film. Martin reprises his role, but John Cleese replaces Kevin Kline as Chief Inspector Dreyfus. This film received negative reviews and meager box office, grossing a worldwide total of $76 million against a budget of $70 million.\nFuture.\nIn March 2014, Metro-Goldwyn-Mayer announced plans to develop a new live-action/CGI hybrid feature film starring the Pink Panther, which was set to be directed by David Silverman, with Walter Mirisch and Julie Andrews serving as producers. Andrews, who is the widow of Blake Edwards, would be creatively involved in the process of developing the new project, which unlike previous installments would focus on the titular character instead of the franchise's main character, Inspector Jacques Clouseau. By November 2020, Jeff Fowler had joined the production replacing Silverman as director. Chris Bremner was hired to write the script, while Lawrence Mirisch will serve as an additional producer. The plot will center around the Pink Panther character and Inspector Clouseau. \nBy April 2023, it was announced that after acquiring MGM, Amazon is developing new additions to the franchise in the form of a movie and television series through their subsidiary Amazon Studios (now called Amazon MGM Studios). It was later reported that Eddie Murphy was in talks to star in the film as Clouseau. Murphy confirmed in July 2025 that he would play Clouseau.\nMain cast and characters.\n&lt;templatestyles src=\"Cast indicator/styles.css\"/&gt;\nList indicators\nThis section includes .* An empty grey cell indicates the character was not in the film, or that the character's official presence has not yet been confirmed.* \u00a0A indicates an appearance through archival footage or audio.* \u00a0C indicates a cameo role.* \u00a0E indicates an appearance not included in the theatrical cut.* \u00a0P indicates an appearance in onscreen photographs.* \u00a0U indicates an uncredited appearance.* \u00a0V indicates a voice-only role.* \u00a0Y indicates a younger version of the character.\nProduction.\nDevelopment.\n20th-century film series.\nMost of the films in the series starred Peter Sellers as Inspector Clouseau and were directed and co-written by Blake Edwards. As detailed in the director's commentary for the first film, the Inspector Clouseau character was originally conceived as a vehicle for David Niven, but once written it was decided he should play the raconteur/thief. Then the role was offered to Peter Ustinov, with Ava Gardner to play his wife. When Gardner dropped out, so did Ustinov, so the role of Clouseau went to Sellers. Apparently, the tone of the film changed after Edwards picked up Sellers from the airport, and during the ride to the hotel, they bonded over their mutual love of old film comedians like Harold Lloyd, Buster Keaton and Laurel &amp; Hardy. The role was then modified to include elements of slapstick. The jazz-based Pink Panther Theme was composed by Henry Mancini. In addition to the credits sequences, the theme often accompanies any suspenseful sequence in the first film and in most of the subsequent films featuring the character of Clouseau.\nThe \"Pink Panther\" of the title is a diamond supposedly containing a flaw that forms the image of a \"leaping panther\" which can be seen if held up to the light in a certain way. This is explained at the beginning of the first film, and the camera zooms in on the diamond to reveal the blurry flaw, which focuses on the cartoon Panther (though not actually leaping) to begin the opening credits sequence. (This is also done in \"The Return of the Pink Panther\" [1975].) The plot of the first film is based on the theft of this diamond. The diamond reappears in several later films in the series, \"The Return of the Pink Panther\" (1975), \"Trail of the Pink Panther\" (1982) and \"Curse of the Pink Panther\" (1983). It also appears in the revival of the Inspector Clouseau character in the Steve Martin reboot films \"The Pink Panther\" (2006), and its sequel \"The Pink Panther 2\" (2009). The name \"the Pink Panther\" became attached to Inspector Clouseau in much the same way that \"Frankenstein\" has been used in film titles to refer to Dr. Frankenstein's creation, or \"The Thin Man\" was used in a series of detective films.\n\"A Shot in the Dark\", the second film in the series, was not originally intended to feature Clouseau and is the first of two films in the series (the other being \"Inspector Clouseau\") that features neither the diamond nor the distinctive animated Pink Panther character in the opening credits and ending. Many critics, including Leonard Maltin, regard \"A Shot in the Dark\" as the best film in the series.\nIn the original film, released in 1963, the main focus was on David Niven's role as Sir Charles Litton, the infamous jewel thief nicknamed \"the Phantom\", and his plan to steal the Pink Panther diamond. Inspector Clouseau was only a secondary character as Litton's incompetent antagonist and provided slapstick to an otherwise subtle, lighthearted caper film, a somewhat jarring contrast of styles which is typical of Edwards's films. The popularity of Clouseau caused him to become the main character in subsequent Pink Panther films, which were more straightforward slapstick comedies.\nMancini's theme, with variations in arrangement, is used at the start of all but the first two of the subsequent films. Mancini's other themes for the first film include an Italian-language set-piece called \"Meglio stasera\", whose purpose seems primarily to introduce young actress Fran Jeffries. Portions of an instrumental version also appear in the film's musical score several times. Other segments include \"Shades of Sennett\", a \"honky-tonk\" piano number introducing the film's climactic chase scene through the streets of Rome. Most of the remaining tracks on the soundtrack album are the early 1960s orchestral jazz pieces, matching the style of the era. Although variations of the main theme would reprise for many of the \"Pink Panther\" series entries, as well as the cartoon series, Mancini composed different theme music for \"A Shot in the Dark\"; this theme was later adopted by the animated spin-off series \"The Inspector\".\nAlthough official, the live-action film \"Inspector Clouseau\" (1968) starring Alan Arkin as Clouseau, is generally not considered by fans to be part of the series canon, since it involved neither Sellers nor Edwards. However, some elements of Arkin's performance and costuming of Clouseau were retained when Sellers resumed the role in \"Return\" in 1975. Despite speculation, Arkin does not appear in \"Trail of the Pink Panther\".\n2000s film series.\nThe film that launched the second Pink Panther series, \"The Pink Panther\", starring Steve Martin as Clouseau, directed by Shawn Levy and produced by Robert Simonds, was released in February 2006 by Metro-Goldwyn-Mayer and was co-produced with Columbia Pictures. It is set in the present day and introduces different main characters, therefore belonging to a different continuity. Martin also stars in the sequel, \"The Pink Panther 2\", released in 2009.\nCartoons.\nThe opening title sequence in the original 1963 \"The Pink Panther\" film was such a success with the United Artists executives that they decided to adapt the title sequence into a series of theatrical animated shorts. DePatie\u2013Freleng Enterprises, run by former Warner Bros. Cartoons personnel David H. DePatie and Isadore \"Friz\" Freleng, produced the opening sequences, with Freleng as director. United Artists commissioned a long series of \"The Pink Panther\" shorts, the first of which, 1964's \"The Pink Phink,\" won the 1964 Academy Award for Animated Short Film. This was the first (and to date only) time a studio's first work won an Oscar.\nBy the autumn of 1969, the shorts were being broadcast on NBC during Saturday mornings on \"The Pink Panther Show;\" after 1969, new shorts were produced for both television broadcast and theatrical release. A number of sister series also joined the Pink Panther character on movie screens and on the airwaves, including \"The Inspector,\" featuring a comical French police officer based on the Jacques Clouseau character. Traditionally mute, the Pink Panther was given the voice of actor Matt Frewer for a 1993-1995 animated TV series and the voice of Michael Sinterniklaas for two computer games based on this series: ' (1996) and ' (1997).\nIn addition to computer and console games, the Pink Panther character has appeared advertising campaigns for several companies, most notably for Owens Corning Fiberglass insulation. He also starred in a short-lived animated series entitled \"Pink Panther and Pals\" (2010), which is aimed at younger children. In 2014, MGM announced (see above) that it was planning an animation / live-action hybrid film reboot of the franchise, to be directed by David Silverman and produced by Walter Mirisch and Julie Andrews. But in November 2020, it was later announced that Jeff Fowler will direct the movie instead with Mirisch and Andrews still producing.\nThe animated Pink Panther character also appeared in a short animated segment on the educational TV series \"Sesame Street\", demonstrating his karate skills to carve the letter K out of a block of stone, only for it to crumble quickly afterward.\nCancelled projects.\n\"Romance of the Pink Panther\".\n\"Romance of the Pink Panther\" would have been the seventh film in the franchise, and written by Peter Sellers and Jim Moloney. Due to a rift between Blake Edwards and Sellers, Edwards would not have directed the film. The basic plot was to involve Inspector Clouseau becoming smitten with a cat burglar called \"the Frog\", played by Pamela Stephenson. Shortly after Sellers' death in July 1980, it was reported that Dudley Moore might play Clouseau, but Blake Edwards instead chose to introduce a new character in the series, rather than recast the role of Clouseau. Both Clive Donner and Sidney Poitier were reported at various times to be directing the movie, with Donner's name in that role on the cover sheet of the July 1980 'final draft' script.\n\"Pink Panther\" television series.\nIn the late 1980s, MGM/UA had been developing a live action/animation hybrid \"Pink Panther\" TV series, focusing on a young reporter to be portrayed by Charlie Schlatter who is helped in his investigations by the Pink Panther. The series was encouraged by the success of \"Who Framed Roger Rabbit\", but for unknown reasons, it was not greenlit.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51525", "revid": "17954829", "url": "https://en.wikipedia.org/wiki?curid=51525", "title": "Haganah", "text": "Zionist paramilitary organization (1920\u20131948)\nHaganah ( , lit.\u2009'The Defense') was the main Zionist paramilitary organization that operated for the Yishuv in the British Mandate for Palestine. It was founded in 1920 to defend the Yishuv's presence in the region, and was formally disbanded in 1948, when it became the core force integrated into the Israel Defense Forces shortly after the Israeli Declaration of Independence.\nFormed out of previous existing militias, Haganah's original purpose was to defend Jewish settlements against Arab attacks; this was the case during the 1921 Jaffa riots, the 1929 Palestine riots, the 1936 Jaffa riots, and the 1936\u20131939 Arab revolt in Palestine, among others. The paramilitary was under the control of the Jewish Agency, the official governmental body in charge of Palestine's Jewish community during the British era. Until the end of World War II, Haganah's activities were moderate, in accordance with the strategic policy of \"havlagah\" (lit.\u2009'self-restraint'), which caused the breakaway of the more radical paramilitaries: Irgun and Lehi. Haganah militants received clandestine military support from Poland and sought cooperation with the United Kingdom in the event of an Axis-led invasion of Palestine through North Africa, prompting the creation of the Palmach, their elite fighting force, in 1941.\nFollowing the end of World War II, the British refused to lift the restrictions on Jewish immigration that they had imposed with the 1939 White Paper. This resulted in Haganah leading a Jewish insurgency against the British authorities in Palestine; the campaign included the paramilitaries' bombing of bridges, railways, and ships used to deport illegal Jewish immigrants, as well as assisting in bringing more diaspora Jews to Palestine in defiance of British policies. After the adoption of the United Nations Partition Plan for Palestine in 1947, Haganah came into the open as the biggest fighting force among the Palestinian Jews, successfully overcoming Arab militias during the Palestinian civil war. After the Declaration of Independence of the State of Israel, the Haganah became the new nation's military force. Shortly after Israeli independence and the beginning of the 1948 Arab\u2013Israeli War, the Haganah was merged with other paramilitary groups and reorganized into the Israel Defense Forces (IDF).\nHistory.\nOverview.\nThe evolution of Jewish defense organizations in Palestine and later Israel went from small self-defense groups active during Ottoman rule, to ever-larger and more sophisticated ones during the British Mandate, leading through the Haganah to the national army of Israel, the IDF. The evolution went step by step from Bar-Giora, to Hashomer, to Haganah, to IDF.\nThe Jewish paramilitary organizations in the New Yishuv (the Zionist enterprise in Palestine) started with the Second Aliyah (1904 to 1914). The first such organization was Bar-Giora, founded in September 1907. It consisted of a small group of Jewish immigrants who guarded settlements for an annual fee. It was converted to Hashomer (; \"The Watchman\") in April 1909, which operated until the British Mandate of Palestine came into being in 1920. Hashomer was an elitist organization which never had more than 100 members. During World War I, the forerunners of the Haganah/IDF were the Zion Mule Corps and the Jewish Legion, both of which were part of the British Army. After the Arab riots against Jews in April 1920, the Yishuv's leadership saw the need to create a nationwide underground defense organization, and the Haganah was founded in June of the same year. The Haganah became a full-scale defense force after the 1936\u20131939 Arab revolt in Palestine with an organized structure, consisting of three main units\u2014the Field Corps, Guard Corps, and the Palmach strike force. During World War II the successor to the Jewish Legion of World War I was the Jewish Brigade, which was joined by many Haganah fighters. During the 1947\u201348 civil war between the Arab and Jewish communities in what was still Mandatory Palestine, a reorganized Haganah managed to defend or wrestle most of the territory it was ordered to hold or capture. At the beginning of the ensuing 1948\u201349 full-scale conventional war against regular Arab armies, the Haganah was reorganized to become the core of the new Israel Defense Forces.\n1920 Palestine riots and 1921 Jaffa riots.\nAfter the 1920 Nebi Musa riots, the Jewish leadership in Palestine believed that the British, to whom the League of Nations had given a mandate over Palestine in 1920, had no desire to confront local Arab groups that frequently attacked Palestinian Jews. Believing that they could not rely on the British administration for protection from these gangs, the Jewish leadership created the Haganah to protect Jewish farms and kibbutzim. The first head of the Haganah was a 28-year-old named Yosef Hecht, a veteran of the Jewish Legion.\nIn addition to guarding Jewish communities, the role of the Haganah was to warn the residents of and repel attacks by Palestinians. In the period between 1920 and 1929, the Haganah lacked a strong central authority or coordination. Haganah \"units\" were very localized and poorly armed: they consisted mainly of Jewish farmers who took turns guarding their farms or their kibbutzim.\nFollowing the 1929 Palestine riots, the Haganah's role changed dramatically. It became a much larger organization encompassing nearly all the youth and adults in the Jewish settlements, as well as thousands of members from the cities. It also acquired foreign arms and began to develop workshops to create grenades and simple military equipment, transforming from an untrained militia to a capable underground army.\n1931 Irgun split.\nMany Haganah fighters objected to the official policy of \"havlagah\" (restraint) that Jewish political leaders (who had become increasingly controlling of the Haganah) had imposed on the militia. Fighters had been instructed to only defend communities and not initiate counterattacks against Arab gangs or their communities. This policy appeared defeatist to many who believed that the best defense is a good offense. In 1931, the more militant elements of the Haganah splintered off and formed the \"Irgun Tsva'i-Leumi\" (National Military Organization), better known as \"Irgun\" (or by its Hebrew acronym, pronounced \"Etzel\").\n1936\u20131939 Arab revolt in Palestine.\nDuring the 1936\u20131939 Arab revolt in Palestine, the Haganah worked to protect British interests and to quell the rebellion using the Posh and then the Hish units. At that time, the Haganah fielded 10,000 mobilized men along with 40,000 reservists. Although the British administration did not officially recognize the Haganah, the British security forces cooperated with it by forming the Jewish Settlement Police, Jewish Supernumerary Police and Special Night Squads, which were trained and led by Colonel Orde Wingate. The battle experience gained during the training was useful in the 1948 Arab\u2013Israeli War.\nSupport from the Second Polish Republic.\nDuring the interwar period, as part of its policy of supporting a Jewish state in Palestine in order to facilitate mass Jewish emigration from its territory, the Second Polish Republic provided military training and weapons to Zionist paramilitary groups, including Haganah. Envoys from Haganah headed by Yehuda Arazi received dozens of shipments with military supplies, including 2,750 Mauser rifles, 225 RKM machine guns, 10,000 hand grenades, two million bullets for rifles and machine guns, and a large number of pistols with ammunition. The British exerted heavy pressure on the Polish government to stop these deliveries. One of the last purchases of Arazi were two airplanes and two gliders. When he fled Poland to France, around 500 rifles were abandoned in a Warsaw warehouse. Members of the Haganah were also trained in a military camp in Rembertow along with Betar members between 1931 and 1937; it is estimated that training courses at the camp were attended by around 8,000 to 10,000 participants during their existence.\n1939 White Paper.\nBy 1939, the British had issued the White Paper, which severely restricted Jewish immigration to Palestine, deeply angering the Zionist leadership. David Ben-Gurion, then chairman of the Jewish Agency, set the policy for the Zionist relationship with the British: \"We shall fight the war against Hitler as if there were no White Paper, and we shall fight the White Paper as if there were no war.\"\nIn reaction to the White Paper, the Haganah built up the Palmach as the Haganah's elite strike force and organized illegal Jewish immigration to Palestine. Approximately 100,000 Jews were brought to Palestine in over one hundred ships during the final decade of what became known as Aliyah Bet. The Haganah also organized demonstrations against British immigration quotas.\nBombing of the SS \"Patria\" in Haifa.\nIn 1940 the Haganah sabotaged the \"Patria\", an ocean liner being used by the British to deport 1,800 Jews to Mauritius, with a bomb intended to cripple the ship. However, the ship sank, killing 267 people and injuring 172.\nWorld War II.\nIn the first years of World War II, the British authorities asked Haganah for cooperation again, due to the fear of an Axis breakthrough in North Africa. After Rommel was defeated at El Alamein in 1942, the British stepped back from their all-out support for Haganah. In 1943, after a long series of requests and negotiations, the British Army announced the creation of the Jewish Brigade Group. While Palestinian Jews had been permitted to enlist in the British army since 1940, this was the first time an exclusively Jewish military unit served in the war under a Jewish flag. The Jewish Brigade Group consisted of 5,000 soldiers and was initially deployed with the 8th Army in North Africa and later in Italy in September 1944. The brigade was disbanded in 1946. All in all, some 30,000 Palestinian Jews served in the British army during the war.\nOn May 14, 1941, the Haganah created the Palmach (an acronym for \"Plugot Mahatz\"\u2014strike companies), an elite commando section, in preparation against the possibility of a British withdrawal and Axis invasion of Palestine. Its members, young men and women, received specialist training in guerrilla tactics and sabotage. During 1942 the British gave assistance in the training of Palmach volunteers but in early 1943 they withdrew their support and attempted to disarm them. The Palmach, then numbering over 1,000, continued as an underground organization with its members working half of each month as kibbutz volunteers, the rest of the month spent training. It was never large\u2014by 1947 it amounted to merely five battalions (about 2,000 men)\u2014but its members had not only received physical and military training, but also acquired leadership skills that would subsequently enable them to take up command positions in Israel's army.\n\"The Saison\" post-assassination of Lord Moyne.\nIn 1944, after the assassination of Lord Moyne (the British Minister of State for the Middle East) by members of the Lehi, the Haganah worked with the British to kidnap, interrogate, and, in some cases, deport Irgun members. This action, which lasted from November 1944 to February 1945, was called the \"Saison\", or the Hunting Season, and was directed against the Irgun and not the Lehi. Future Jerusalem mayor Teddy Kollek was later revealed to be a Jewish Agency liaison officer working with the British authorities who had passed on information that led to the arrest of many Irgun activists.\nMany Jewish youth, who had joined the Haganah in order to defend the Jewish people, were greatly demoralized by operations against their own people. The Irgun, paralyzed by the Saison, were ordered by their commander, Menachem Begin, not to retaliate in an effort to avoid a full blown civil war. Although many Irgunists objected to these orders, they obeyed Begin and refrained from fighting back. The Saison eventually ended due to perceived British betrayal of the Yishuv becoming more obvious to the public and increased opposition from Haganah members.\nPost\u2013World War II.\nThe Saison officially ended when the Haganah, Irgun and the Lehi formed the Jewish Resistance Movement, in 1945. Within this new framework, the three groups agreed to operate under a joint command. They had different functions, which served to drive the British out of Palestine and create a Jewish state.\nThe Haganah was less active in the Jewish Rebellion than the other two groups, but the Palmach did carry out anti-British operations, including a raid on the Atlit detainee camp that released 208 illegal immigrants, the Night of the Trains, the Night of the Bridges, and attacks on Palestine Police bases. The Haganah withdrew on 1 July 1946, but \"remained permanently unco-operative\" with the British authorities. It continued to organize illegal Jewish immigration as part of the Aliyah Bet program, in which ships carrying illegal immigrants attempted to breach the British blockade of Palestine and land illegal immigrants on the shore (most were intercepted by the Royal Navy), and the Palmach performed operations against the British to support the illegal immigration program. The Palmach repeatedly bombed British radar stations being used to track illegal immigrant ships, and sabotaged British ships being used to deport illegal immigrants, as well as two British landing and patrol craft. The Palmach performed a single assassination operation in which a British official who had been judged to be excessively cruel to Jewish prisoners was shot dead. The Haganah also organized the Birya affair. Following the expulsion of the residents of the Jewish settlement of Birya for illegal weapons possession, thousands of Jewish youth organized by the Haganah marched to the site and rebuilt the settlement. They were expelled by British shortly afterward while showing passive resistance, but after they returned a third time, the British backed off and allowed them to remain.\nIn addition to its operations, the Haganah continued to secretly prepare for a war with the Arabs once the British left by building up its arms and munitions stocks. It maintained a secret arms industry, with the most significant facility being an underground bullet factory underneath Ayalon, a kibbutz that had been established specifically to cover it up.\nBritish estimates of the Haganah's strength at this time were a \"paper strength of 75,000 men and women\" with an effective strength of 30,000. After the British army, the Haganah was considered the most powerful military force in the Middle East.\nResearch by Amos Perlmutter estimated that the Haganah budget in 1946 was \u00a3400,000, and by October 1947 its budget had reached \u00a33.3 million. The same source estimated that the membership of the Haganah was 7% of the Jewish population in Palestine.\nIn July 1947, eager to maintain order with the visit of UNSCOP to Palestine and under heavy pressure from the British authorities to resume collaboration, the Jewish Agency reluctantly came into brief conflict with the Irgun and Lehi, and ordered the Haganah to put a stop to the operations of the other two groups for the time being. As Palmach members refused to participate, a unit of about 200 men from regular Haganah units was mobilized, and foiled several operations against the British, including a potentially devastating attack on the British military headquarters at Citrus House in Tel Aviv, in which a Haganah member was killed by an Irgun bomb. The Haganah also joined the search for two British sergeants abducted by the Irgun as hostages against the death sentences of three Irgun members in what became known as the Sergeants affair. The Jewish Agency leadership feared the damage this act would do to the Jewish cause, and also believed that holding the hostages would only jeopardize the fates of the three condemned Irgun members. The attempts to free the sergeants failed, and following the executions of the three Irgun members, the two sergeants were killed and hanged in a eucalyptus grove. However, the campaign soon disintegrated into a series of retaliatory abductions and beatings of each other's members by the Haganah and Irgun, and eventually petered out. The campaign was dubbed the \"Little Season\" by the Irgun.\nReorganization.\nAfter \"having gotten the Jews of Palestine and of elsewhere to do everything that they could, personally and financially, to help Yishuv,\" Ben-Gurion's second-greatest achievement was his having successfully transformed Haganah from being a clandestine paramilitary organization into a true army. Ben-Gurion appointed Israel Galili to the position of head of the High Command counsel of Haganah and divided Haganah into six infantry brigades, numbered 1 to 6, allotting a precise theatre of operation to each one. Yaakov Dori was named Chief of Staff, but it was Yigael Yadin who assumed the responsibility on the ground as chief of Operations. Palmach, commanded by Yigal Allon, was divided into three elite brigades, numbered 10\u201312, and constituted the mobile force of Haganah. Ben-Gurion's attempts to retain personal control over the newly formed IDF culminated with The Generals' Revolt.\nOn 19 November 1947, obligatory conscription was instituted for all men and women aged between 17 and 25. By end of March 21,000 people had been conscripted. On 30 March the call-up was extended to men and single women aged between 26 and 35. Five days later a General Mobilization order was issued for all men under 40.\n\"From November 1947, the Haganah, (...) began to change from a territorial militia into a regular army. (...) Few of the units had been well trained by December. (...) By March\u2013April, it fielded still under-equipped battalion and brigades. By April\u2013May, the Haganah was conducting brigade size offensive.\nThe brigades of the Haganah which merged into the IDF once this was created on 26 May 1948:\nThe northern Levanoni Brigade, located in the Galilee, was split on February 22, 1948 into the 1st and 2nd Brigades.\nTo the initial six brigades, three were added later during the war:\nThe Palmach brigades which merged into the IDF:\n1948 Arab-Israeli War.\nAfter the British announced they would withdraw from Palestine, and the United Nations approved the partition of Palestine, the 1947\u20131948 civil war in Mandatory Palestine broke out. The Haganah played the leading role in the Yishuv's war with the Palestinian Arabs. Initially, it concentrated on defending Jewish areas from Arab raids, but after the danger of British intervention subsided as the British withdrew, the Haganah went on the offensive and seized more territory. Following the Israeli Declaration of Independence and the start of the 1948 Arab\u2013Israeli War on May 15, 1948, the Haganah, now the army of the new state, engaged the invading armies of the surrounding Arab states.\nOn May 28, 1948, less than two weeks after the creation of the state of Israel on May 15, the provisional government created the Israel Defense Forces, merging the Haganah, Irgun, and Lehi, although the other two groups continued to operate independently in Jerusalem and abroad for some time after. The reorganization led to several conflicts between Ben-Gurion and the Haganah leadership, including what was known as The Generals' Revolt and the dismantling of the Palmach.\nFamous members of the Haganah included Yitzhak Rabin, Ariel Sharon, Rehavam Ze'evi, Dov Hoz, Moshe Dayan, Yigal Allon and Dr. Ruth Westheimer.\nThe Museum of Underground Prisoners in Jerusalem commemorates the activity of the underground groups in the pre-state period, recreating the everyday life of those imprisoned there.\nPal-Heib Unit for Bedouins.\nSome Bedouins had longstanding ties with nearby Jewish communities. They helped defend these communities in the 1936\u20131939 Arab revolt in Palestine. During the 1948 Arab\u2013Israeli War, some Bedouins of Tuba formed an alliance with the Haganah defending Jewish communities in the Upper Galilee against Syria. Some were part of a Pal-Heib unit of the Haganah. Sheik Hussein Mohammed Ali Abu Yussef of Tuba was quoted in 1948 as saying, \"Is it not written in the Koran that the ties of neighbors are as dear as those of relations? Our friendship with the Jews goes back for many years. We felt we could trust them and they learned from us too\".\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51526", "revid": "30552921", "url": "https://en.wikipedia.org/wiki?curid=51526", "title": "Heinrich Anton de Bary", "text": "German surgeon, botanist, microbiologist and mycologist (1831\u20131888)\nHeinrich Anton de Bary (26 January 1831\u00a0\u2013 19 January 1888) was a German surgeon, botanist, microbiologist, and mycologist (fungal systematics and physiology).\nHe is considered a founding father of plant pathology (phytopathology) as well as the founder of modern mycology. His extensive and careful studies of the life history of fungi and contribution to the understanding of algae and higher plants established landmarks in biology.\nEarly life and education.\nBorn in Frankfurt to physician August Theodor de Bary (1802\u20131873) and Emilie Meyer de Bary, Anton de Bary was one of ten children. He joined excursions of naturalists who collected local specimens. De Bary\u2019s interest was further inspired by George Fresenius, a physician, who also taught botany at Senckenberg Institute. Fresenius was an expert on thallophytes. In 1848, de Bary graduated from a gymnasium at Frankfurt, and began to study medicine at Heidelberg, continuing at Marburg. In 1850, he went to Berlin to continue pursuing his study of medicine, and also continued to explore and develop his interest in plant science. Although he received his degree in medicine, his dissertation at Berlin in 1853 was titled \"De plantarum generatione sexuali\", a botanical subject. He also published a book on fungi and the causes of rusts and smuts.\nEarly career.\nAfter graduation, de Bary briefly practiced medicine in Frankfurt, but he was drawn back to botany and became Privatdozent in botany at the University of T\u00fcbingen, where he worked for a while as an assistant to Hugo von Mohl (1805\u20131872). In 1855, he succeeded the botanist Karl Wilhelm von N\u00e4geli (1818\u20131891) at Freiburg, where he established the most advanced botanical laboratory at the time and directed many students.\nLater career and research.\nIn 1867, de Bary moved to the University of Halle as successor to Professor Diederich Franz Leonhard von Schlechtendal, who, with Hugo von Mohl, co-founded the pioneer botanical journal \"Botanische Zeitung\". De Bary became its coeditor and later sole editor. As an editor of and contributor to the journal, he exercised great influence upon the development of botany. Following the Franco-Prussian War (1870\u20131871), de Bary took the position of professor of botany at the University of Strasbourg, where he was the director of the Jardin botanique de l'Universit\u00e9 de Strasbourg, and founder of its New Garden. He was also elected as the inaugural rector of the reorganized university. He conducted much research in the university botanical institute, attracted many international students, and made a large contribution to the development of botany.\nHis 1884 book \"Vergleichende Morphologie und Biologie der Pilze, Mycetozoen und Bakterien\" was translated into English as \"Comparative Morphology and Biology of the Fungi, Mycetozoa, and Bacteria\" (Clarendon Press, 1887).\nFungi and plant diseases.\nDe Bary was devoted to the study of the life history of fungi. At that time, various fungi were still considered to arise via spontaneous generation. He proved that pathogenic fungi were like other plants, and not the products of secretions from sick cells.\nIn de Bary\u2019s time, potato late blight had caused sweeping crop devastation and economic loss. The origin of such plant diseases was not known at that time. de Bary studied the pathogen \"Phytophthora infestans\" (formerly \"Peronospora infestans\") and elucidated its life cycle. Miles Joseph Berkeley (1803\u20131889) had insisted in 1841 that the oomycete found in potato blight caused the disease. Similarly, de Bary asserted that rust and smut fungi caused the pathological changes that affected diseased plants. He concluded that Uredinales and Ustilaginales were parasites.\nDe Bary spent much time studying the morphology of fungi and noticed that certain forms that were classed as separate species were actually successive stages of development of the same organism. De Bary studied the developmental history of Myxomycetes (slime molds), and thought it was necessary to reclassify the lower animals. He first coined the term Mycetozoa to include lower animals and slime molds. In his work on Myxomycetes (1858), he pointed out that at one stage of their life cycle (the plasmodial stage), they were nearly-formless, motile masses of a substance that F\u00e9lix Dujardin (1801\u20131860) had called sarcode (protoplasm). This is the fundamental basis of the protoplasmic theory of life.\nDe Bary was the first to demonstrate sexuality in fungi. In 1858, he had observed conjugation in the alga \"Spirogyra\", and in 1861, he described sexual reproduction in the fungus \"Peronospora\" sp. He saw the importance of observing pathogens throughout their whole life cycle and attempted to follow that practice in his studies of living host plants.\nPeronosporeae.\nDe Bary published his first work on potato blight fungi in 1861, and then spent more than 15 years studying Peronosporeae, particularly \"Phytophthora infestans\" (formerly \"Peronospora infestans\") and \"Cystopus\" (\"Albugo\"), parasites of potato. In his published work in 1863 entitled \"Recherches sur le developpement de quelques champignons parasites\", he reported inoculating healthy potato leaves with spores of \"P. infestans\". He observed that mycelium penetrated the leaf and affected the tissue, forming conidia and the black spots characteristic of potato blight. He did similar experiments on tubers and potato stalks. He watched conidia in the soil and their infection of the tubers, observing that mycelium could survive the cold winter in the tubers. Based on these studies, he concluded that organisms were not being generated spontaneously.\n\"Puccinia graminis\".\nHe did a thorough investigation on \"Puccinia graminis\", the pathogen that produces rust in wheat, rye and other grains. He noticed that \"P. graminis\" produced reddish summer spores or \"urediospores\", and darker winter spores or \"teleutospores\". He inoculated the leaves of barberry (\"Berberis vulgaris\") with sporidia from winter spores of wheat rust. The sporidia germinated, leading to the forming of aecia with yellow spores, the familiar symptoms of infection on the barberry. De Bary then inoculated aecidiospores on moisture-retaining slides and then transferred them to the leaves of seedling of rye plants. In time, he observed the reddish summer spores appearing in the leaves. Sporidia from winter spores germinated only on barberry. De Bary clearly demonstrated that \"P. graminis\" lived upon different hosts at different stages of its development. He called this phenomenon \"heteroecism\" in contrast to \"autoecism\", in which development takes place only in one host. De Bary\u2019s discovery explained why the practice of eradicating barberry plants was important as a control for rust.\nLichen.\nDe Bary also studied the formation of lichens which are the result of an association between a fungus and an alga. He traced their stages of growth and reproduction and showed how adaptations helped them to survive conditions of drought and winter. In 1879 he coined the word \"symbiosis\", meaning \"the living together of unlike organisms\", in the publication \"Die Erscheinung der Symbiose\" (Strasbourg, 1879). He carefully studied the morphology of molds, yeasts, and fungi and basically established mycology as an independent science.\nInfluence.\nDe Bary's concept and methods had a great impact on the fields of bacteriology and botany, making him one of the most influential bioscientists of the 19th century. He published more than 100 research papers. Many of his students later became distinguished botanists and microbiologists including Sergei Winogradsky (1856\u20131953), William Gilson Farlow (1844\u20131919), and Pierre-Marie-Alexis Millardet (1838\u20131902).\nPersonal life and death.\nDe Bary came from a noble family of Huguenots from Wallonia, which was driven out from there by the Spanish Habsburgs under Emperor Charles V and can be found in Frankfurt since 1555. Anton's father and his brother Johann Jakob de Bary were respected doctors in Frankfurt. His mother was Caroline Emilie von Meyer (1805\u20131887), whose family produced two renowned scientists.\nDe Bary married Antonie Einert (21 January 1831, Leipzig \u2013 22 May 1892, Thann, Alsace\u2013Lorraine) in 1861; they raised four children: Wilhelm, August, Marie and Hermann. Antonie was a talented artist and painter, particularly of plants, who contributed to her husband's scientific work. He was elected to Honorary membership of the Manchester Literary and Philosophical Society on 9 February 1886 .\nHe died on 19 January 1888 in Strasbourg, of a tumor of the jaw, after undergoing extensive surgery.\n&lt;templatestyles src=\"Botanist/styles.css\"/&gt;The standard author abbreviation de Bary is used to indicate this person as the author when citing a botanical name.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51527", "revid": "1461430", "url": "https://en.wikipedia.org/wiki?curid=51527", "title": "Elias Magnus Fries", "text": "Swedish mycologist (1794\u20131878)\nElias Magnus Fries (15 August 1794 \u2013 8 February 1878) was a Swedish mycologist and botanist. He is sometimes called the \"Linnaeus of Mycology\". In his works he described and assigned botanical names to hundreds of fungus and lichen species, many of which remain authoritative today.\nCareer.\nFries was born at Femsj\u00f6 (Hylte Municipality), Sm\u00e5land, the son of the pastor there. He attended school in V\u00e4xj\u00f6.\nHe acquired an extensive knowledge of flowering plants from his father. In 1811 Fries entered Lund University where he studied under Carl Adolph Agardh and Anders Jahan Retzius. He obtained his doctorate in 1814. In the same year he was appointed an associate professorship in botany. Fries edited several exsiccata series, the first starting in 1818 under the title \"Lichenes Sveciae exsiccati, curante Elia Fries\" and the last together with Franz Joseph Lagger under the title \"Hieracia europaea exsiccata\". He was elected a member of the Royal Swedish Academy of Sciences, and in 1824, became a full professor. In 1834 he became Borgstr\u00f6m professor (Swed. \"Borgstr\u00f6mianska professuren\", a chair endowed by Erik Eriksson Borgstr\u00f6m, 1708\u20131770) in applied economics at Uppsala University. The position was changed to \"professor of botany and applied economics\" in 1851. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1849. That year he was also appointed director of the Uppsala University Botanical Garden. In 1853, he became rector of the University.\nFries most important works were the three-volume \"Systema mycologicum\" (1821\u20131832), \"Elenchus fungorum\" (1828), the two-volume \"Monographia hymenomycetum Sueciae\" (1857 and 1863) and \"Hymenomycetes Europaei\" (1874).\nFries is considered to be, after Christian Hendrik Persoon, a founding father of the modern taxonomy of mushrooms. His taxonomy of mushrooms was influenced by Goethe and the German romantics. He utilized spore color and arrangement of the hymenophore (pores, gills, teeth etc.) as major taxonomic characteristics. He was one of the most prolific authors of new fungal species, having formally described 3210 in his career.\nFries died in Uppsala on 8 February 1878. When he died, \"The Times\" commented: \"His very numerous works, especially on fungi and lichens, give him a position as regards those groups of plants comparable only to that of Linnaeus.\" Fries was succeeded in the Borgstr\u00f6m professorship (from 1859 to 1876) by Johan Erhard Areschoug, after whom Theodor Magnus Fries, the son of Elias, held the chair (from 1877 to 1899).\nBotanical reference.\n&lt;templatestyles src=\"Botanist/styles.css\"/&gt;The standard author abbreviation Fr. is used to indicate this person as the author when citing a botanical name.\nFamily.\nHis wife was Christina Wieslander (1808\u20131862), with whom he raised nine children. His son Theodor Magnus Fries became a botanist and lichenologist, eventually holding the Borgstr\u00f6m professorship himself, and another son, Oscar Robert Fries, became a physician in Gothenburg while maintaining a keen interest in mycology. Theodor \"Thore\" Magnus's sons Thore Christian Elias Fries and Robert Elias Fries also became botanists.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51528", "revid": "25865034", "url": "https://en.wikipedia.org/wiki?curid=51528", "title": "Christiaan Hendrik Persoon", "text": "German mycologist (1761\u20131836)\nChristiaan Hendrik Persoon (31 December 1761 \u2013 16 November 1836) was a Cape Colony mycologist who is recognized as one of the founders of mycological taxonomy.\nEarly life.\nPersoon was born in Cape Colony at the Cape of Good Hope, the third child of an immigrant Pomeranian father, Christiaan Daniel Persoon, and Dutch mother, Wilhelmina Elizabeth Groenwald. His mother died soon after he was born. In 1775, at the age of thirteen, he was sent to Europe for his education. His father died a year later in 1776.\nEducation.\nInitially a student of theology at Halle, Persoon switched his studies to medicine, which he pursued in Leiden and then G\u00f6ttingen. He received a doctorate from the Deutsche Akademie der Naturforscher in Erlangen 1799.\nLater years.\nHe moved to Paris by 1803, where he spent the rest of his life, renting the upper floor of a house in a poor part of town. He was apparently unemployed, unmarried, poverty-stricken and a recluse, although he corresponded with botanists throughout Europe. Because of his financial difficulties, Persoon agreed to donate his herbarium to the House of Orange, in return for an adequate pension for life.\nAcademic career.\nThe origin of Persoon's botanical interest is unknown. The earliest of his works was \"Abbildungen der Schw\u00e4mme\" (Illustrations of the fungi), published in three parts, in 1790, 1791, and 1793. In 1794, Persoon introduced the term &lt;dfn id=\"\"&gt;lirella&lt;/dfn&gt; for the furrowed ascomata of the lichen genus \"Graphis\". From 1805 to 1807, he published two volumes of his http:// (), a popular work describing 20,000 species of all types of plants. But his pioneering work was in the fungi, for which he published several works, beginning with the http:// (1801); it is the starting point for nomenclature of the Uredinales, Ustilaginales, and the Gasteromycetes. Persoon described many polypore species; most were from his own collections in central Europe, while several other tropical species were sent to him from collections made by French botanist Charles Gaudichaud-Beaupr\u00e9 during his circumglobal expedition. These latter fungi are among the first tropical polypores ever described. In 1815, Persoon was elected a corresponding member of the Royal Swedish Academy of Sciences.\nPersoon was a prolific author of new fungal species, having formally described 2269 in his career.\nRecognition.\nThe genus \"Persoonia\", a variety of small Australian trees and shrubs, was named after him. The title \"Persoonia\" is also given to a biannual scientific journal of molecular phylogeny and evolution of fungi, published jointly by the National Herbarium of the Netherlands and the CBS Fungal Biodiversity Centre.\n&lt;templatestyles src=\"Botanist/styles.css\"/&gt;The standard author abbreviation Pers. is used to indicate this person as the author when citing a botanical name.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51531", "revid": "48643156", "url": "https://en.wikipedia.org/wiki?curid=51531", "title": "Geostatistics", "text": "Branch of statistics focusing on spatial data sets\nGeostatistics is a branch of statistics focusing on spatial or spatiotemporal datasets. Developed originally to predict probability distributions of ore grades for mining operations, it is currently applied in diverse disciplines including petroleum geology, hydrogeology, hydrology, meteorology, oceanography, geochemistry, geometallurgy, geography, forestry, environmental control, landscape ecology, soil science, and agriculture (esp. in precision farming). Geostatistics is applied in varied branches of geography, particularly those involving the spread of diseases (epidemiology), the practice of commerce and military planning (logistics), and the development of efficient spatial networks. Geostatistical algorithms are incorporated in many places, including geographic information systems (GIS).\nBackground.\nGeostatistics is intimately related to interpolation methods but extends far beyond simple interpolation problems. Geostatistical techniques rely on statistical models based on random function (or random variable) theory to model the uncertainty associated with spatial estimation and simulation.\nA number of simpler interpolation methods/algorithms, such as inverse distance weighting, bilinear interpolation and nearest-neighbor interpolation, were already well known before geostatistics. Geostatistics goes beyond the interpolation problem by considering the studied phenomenon at unknown locations as a set of correlated random variables.\nLet \"Z\"(x) be the value of the variable of interest at a certain location x. This value is unknown (e.g., temperature, rainfall, piezometric level, geological facies, etc.). Although there exists a value at location x that could be measured, geostatistics considers this value as random since it was not measured or has not been measured yet. However, the randomness of \"Z\"(x) is not complete. Still, it is defined by a cumulative distribution function (CDF) that depends on certain information that is known about the value \"Z\"(x):\nformula_1\nTypically, if the value of \"Z\" is known at locations close to x (or in the neighborhood of x) one can constrain the CDF of \"Z\"(x) by this neighborhood: if a high spatial continuity is assumed, \"Z\"(x) can only have values similar to the ones found in the neighborhood. Conversely, in the absence of spatial continuity \"Z\"(x) can take any value. The spatial continuity of the random variables is described by a model of spatial continuity that can be either a parametric function in the case of variogram-based geostatistics, or have a non-parametric form when using other methods such as multiple-point simulation or pseudo-genetic techniques.\nBy applying a single spatial model on an entire domain, one makes the assumption that \"Z\" is a stationary process. It means that the same statistical properties are applicable on the entire domain. Several geostatistical methods provide ways of relaxing this stationarity assumption.\nIn this framework, one can distinguish two modeling goals:\n formula_2\n In this approach, the presence of multiple solutions to the interpolation problem is acknowledged. Each realization is considered as a possible scenario of what the real variable could be. All associated workflows are then considering ensemble of realizations, and consequently ensemble of predictions that allow for probabilistic forecasting. Therefore, geostatistics is often used to generate or update spatial models when solving inverse problems.\nA number of methods exist for both geostatistical estimation and multiple realizations approaches. Several reference books provide a comprehensive overview of the discipline.\nMethods.\nEstimation.\nKriging.\nKriging is a group of geostatistical techniques to interpolate the value of a random field (e.g., the elevation, z, of the landscape as a function of the geographic location) at an unobserved location from observations of its value at nearby locations.\nBayesian estimation.\nBayesian inference is a method of statistical inference in which Bayes' theorem is used to update a probability model as more evidence or information becomes available. Bayesian inference is playing an increasingly important role in geostatistics. Bayesian estimation implements kriging through a spatial process, most commonly a Gaussian process, and updates the process using Bayes' Theorem to calculate its posterior. High-dimensional Bayesian geostatistics refers to Bayesian modeling and analysis for geostatistical data when the number of spatial locations is massive. Probabilistic machine learning methods, specifically predictive stacking, are also available for Bayesian geostatistics.\nFinite difference method.\nConsidering the principle of conservation of probability, recurrent difference equations (finite difference equations) were used in conjunction with lattices to compute probabilities quantifying uncertainty about the geological structures. This procedure is a numerical alternative method to Markov chains and Bayesian models.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51534", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=51534", "title": "Brenner Pass", "text": "Mountain pass through the Alps; border between Italy and Austria\nThe Brenner Pass ( , in short ; ) is a mountain pass over the Alps which forms the border between Italy and Austria. It is one of the major passes of the Eastern Alpine range and has the lowest altitude among Alpine passes of the area.\nDairy cattle graze in alpine pastures throughout the summer in valleys beneath the pass and on the mountain slopes. At lower altitudes, farmers log pine trees, plant crops and harvest hay for winter fodder. Many of the high pastures are at an altitude of over ; a small number stand high in the mountains at around .\nThe central section of the Brenner Pass covers a four-lane motorway and railway tracks connecting Bozen/Bolzano in the south and Innsbruck to the north. The village of Brenner consists of an outlet shopping centre (supermarkets and stores), fruit stores, restaurants, caf\u00e9s, hotels and a gas station. It has a population of 400 to 600 (as of 2011[ [update]]).\nEtymology.\nOlder, obsolete theories suggested a connection of the name \"Brenner\" with the ancient tribe of the Breuni or the Gaulish chieftain Brennus, but since the pass name appears for the first time only in the 14th century, a more recent etymology is far more likely.\n\"Prenner\" was originally the name of a nearby farm, which was named after its former owner. The farm of a certain \"Prennerius\" is mentioned in documents in 1288; a certain \"Chunradus Prenner de Mittenwalde\" is mentioned in 1299. The German word \"Prenner\" probably refers to somebody who uses slash-and-burn techniques for land clearing. A name for the pass itself appears for the first time in 1328 as \"ob dem Prenner\" (German for \"above the Prenner\").\nHistory.\nRoman Empire.\nThe Romans regularised the mountain pass at Brenner, which had already been under frequent use during the prehistoric eras since the most recent Ice Age. The Brenner Pass, however, was not the first trans-Alpine Roman road to become regularised under the Roman Empire.\nThe first Roman road to cross the Alpine range, Via Claudia Augusta, connected Verona in northern Italy with Augusta Vindelicorum (modern-day Augsburg) in the Roman province of Raetia. Via Augusta was completed in 46\u201347 AD; the route took its course along the Adige valley to the neighbouring Reschen Pass (west of the Brenner Pass), then descended into the Inn valley before rising to Fern Pass towards Augsburg.\nThe Roman road that physically crossed over the Brenner Pass did not exist until the 2nd century AD. It took part of the \"eastern\" route up the Eisack Valley and descended into Veldidena (modern-day Wilten), where it crossed the Inn and into Zirl and arrived at Augsburg via F\u00fcssen.\nThe Alamanni (Germanic tribe) crossed the Brenner Pass southward into modern-day Italy in 268 AD, but they were stopped in November of that year at the Battle of Lake Benacus. The Romans kept control over the mountain pass until the end of their empire in the 5th century.\nHoly Roman Empire.\nDuring the High Middle Ages, Brenner Pass was a part of the important \"Via Imperii\", an imperial road linking the Kingdom of Germany north of the Alps with the Italian March of Verona. In the Carolingian \"Divisio Regnorum\" of 806, the Brenner region was called \"per alpes Noricas\", the transit through the Noric Alps. From the 12th century, the Brenner Pass was controlled by the Counts of Tyrol within the Holy Roman Empire. Emperor Frederick Barbarossa made frequent use of the Brenner Pass to cross the Alps during his imperial expeditions into Italy. The 12th-century Brenner Pass accommodated mule trains and carts.\nModernisation of the Brenner Pass started in 1777, when a carriage road was laid out at the behest of Empress Maria Theresa.\nAustrian Empire.\nModernisation further took place under the Austrian Empire and the Brenner Railway, which was completed in stages from 1853 to 1867. It became the first trans-Alpine railway without a major tunnel and at high altitude (crossing the Brenner Pass at 1,371 m). Completion of the railway enabled the Austrians to move their troops more efficiently; they had hoped to secure their territories of Venetia and Lombardy (south of the Alps), but lost them to Italy following the Second Italian War of Independence in 1859 and Austro-Prussian War in 1866.\nRecent history.\nAt the end of World War I in 1918, the control of the Brenner Pass became shared between Italy and Austria under the Treaty of Saint-Germain-en-Laye (1919). The Treaty of London (1915) secretly awarded Italy the territories south of the Brenner Pass for supporting the Entente Powers. Welschtirol/Trentino, along with the southern part of the County of Tyrol (now South Tyrol), was transferred to Italy, and Italian troops occupied Tyrol and arrived at the Brenner Pass in 1919 to 20.\nOn 12 March 1938, Austria was annexed by Germany. Two years and six days later during World War II, Adolf Hitler and Benito Mussolini met at the Brenner Pass to celebrate their Pact of Steel on 18 March 1940. Later, in 1943, following the Italian armistice with the Allies, the Brenner Pass was annexed by Nazi Germany, shifting the border with the Italian Social Republic, the Nazi puppet state headed by Mussolini, much further south. In 1945, the area was occupied by the US Army and returned to Italy after the end of the war. The Brenner Pass was part of the ratlines that were used by senior Nazis fleeing the allies after the German surrender in 1945.\nFollowing World War II, the pass once again formed the border between Italy and the newly independent Republic of Austria, and maintained its importance as a key trade route. On 1 January 1995 the Schengen Agreement entered into force in Austria, a treaty Italy ratified on 26 October. As a consequence, border checks were abolished in the Brenner Pass for goods and people between the two countries. On 19 November 1995 the border barrier between Italy and Austria at Brenner was officially abolished, with a commemoration attended by Austrian Minister of the Interior Karl Schl\u00f6gl, Italian Minister of the Interior Giorgio Napolitano, and the governors of Innsbruck and Bolzano.\nMotorway.\nThe motorway E45 (European designation; in Italy A22, in Austria the A13), Brenner Autobahn/Autostrada del Brennero, begins in Innsbruck, runs through the Brenner Pass, Bozen/Bolzano, Verona and finishes outside Modena. It is one of the most important routes of north\u2013south connections in Europe.\nAfter the signing of the Schengen Agreement in 1992 and Austria's subsequent entry into the European Union in 1995, customs and immigration posts at the Brenner Pass were removed in 1997. However, Austria reinstituted border checks in 2015 as a response to the European migrant crisis. In April 2016, Austria announced it would build a 370-meter long fence at the Pass but clarify that \"it would be used only to \"channel\" people and was not a barrier.\"\nThe Europabr\u00fccke (\"Europe Bridge\"), located roughly halfway between Innsbruck and the Brenner Pass, is a large concrete bridge carrying the six-lane Brenner Autobahn over the valley of Sill River (Wipptal). At a height of and span of , the bridge was celebrated as a masterpiece of engineering upon its completion in 1963. It is a site where bungee-jumping from the bridge has become a popular tourist attraction.\nThe ever-increasing freight and leisure traffic, however, has been causing long traffic jams at busy times even without border enforcements. The Brenner Pass is the only major mountain pass within the area; other nearby alternatives are footpaths across higher mountains at an altitude of above . As a result, air and noise pollution have generated heavy debate in regional and European politics. As of 2004[ [update]], about 1.8 million trucks crossed the Europa Bridge per year.\nRailway.\nIn order to ease the road traffic, there are plans to upgrade the Brenner Railway from Verona to Innsbruck with a series of tunnels, including the Brenner Base Tunnel underneath Brenner. The official groundbreaking of the tunnel took place in 2006 (with survey tunnels drilled in the same year), but substantial work did not begin until 2011. Funding issues have delayed the tunnel's scheduled date of completion from 2022 to no earlier than 2032.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51536", "revid": "36767729", "url": "https://en.wikipedia.org/wiki?curid=51536", "title": "IBM Token ring", "text": ""}
{"id": "51537", "revid": "7767740", "url": "https://en.wikipedia.org/wiki?curid=51537", "title": "Hydraulic fill", "text": "Method of selectively emplacing soil or other materials using a stream of water\nHydraulic fill is a means of selectively emplacing soil or other materials using a stream of water. It is also a term used to describe the materials thus emplaced. Gravity, coupled with velocity control, is used to effect the selected deposition of the material.\nBorrow pits containing suitable material are accessible at an elevation such that the earth can be sluiced to the fill after being washed from the bank by high-pressure nozzles. Hydraulic fill is likely to be the most economic method of construction. Even when the source material lacks sufficient elevation, it can be elevated to the sluice by a dredge pump. \nIn the construction of a hydraulic fill dam, the edges of the dam are defined by low embankments or dykes which are built upward as the fill progresses. The sluices are carried parallel to, and just inside of, these dykes. The sluices discharge their water-earth mixture at intervals, the water fanning out and flowing towards the central pool which is maintained at the desired level by discharge control. While flowing from the sluices, coarse material is deposited first and then finer material is deposited (fine material has a slower terminal velocity thus takes longer to settle, see Stokes' Law) as the flow velocity is reduced towards the center of the dam. This fine material forms an impervious core to the dam. The water flow must be well controlled at all times, otherwise the central section may be bridged by tongues of coarse material which would facilitate seepage through the dam later.\nHydraulic fill dams can be dangerous in areas of seismic activity due to the high susceptibility of the uncompacted, cohesion-less soils in them to liquefaction. The Lower San Fernando Dam is an example of a hydraulic fill dam that failed during an earthquake. In these situations, a dam built of compacted soil may be a better choice. \nPoorly built hydraulic fill dams pose a risk of catastrophic failure. The Fort Peck Dam is an example of a hydraulic fill dam that failed during construction where the hydraulic filling process may have contributed to the failure.\nHydraulic fill is also a term used in hard rock mining and describes the placement of finely ground mining wastes into underground stopes in a slurry by boreholes and pipes to stabilize the voids.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51538", "revid": "1126265", "url": "https://en.wikipedia.org/wiki?curid=51538", "title": "Category 5 cable", "text": "Unshielded twisted pair communications cable\nCategory\u00a05 cable (Cat\u00a05) is a twisted pair cable for computer networks. Since 2001, the variant commonly in use is the Category\u00a05e specification (Cat\u00a05e). The cable standard provides performance of up to 100\u00a0MHz and is suitable for most varieties of Ethernet over twisted pair up to 2.5GBASE-T but more commonly runs at 1000BASE-T (Gigabit Ethernet) speeds. Cat\u00a05 is also used to carry other signals such as telephone and video.\nThis cable is commonly connected using punch-down blocks and modular connectors. Most Category\u00a05 cables are unshielded, relying on the balanced line twisted pair design and differential signaling for noise suppression.\nStandards.\nCategory\u00a05 is currently defined in ISO/IEC 11801, IEC 61156 and EN 50173, though it was originally defined in ANSI/TIA/EIA-568-A (with clarification in TSB-95). These documents specify performance characteristics and test requirements for frequencies up to 100\u00a0MHz.\nThe cable is available in both stranded and solid conductor forms. The stranded form is more flexible and withstands more bending without breaking. Patch cables are stranded. Permanent wiring used in structured cabling is solid. The category and type of cable can be identified by the printing on the jacket.\nThe Category\u00a05 specification requires conductors to be pure copper. There has been a rise in counterfeit cables, especially of the copper-clad aluminum (CCA) variety. This has exposed the manufacturers and installers of such fake cable to legal liabilities.\nVariants and comparisons.\nThe Category\u00a05e specification improves upon the Category\u00a05 specification by further mitigating crosstalk. The bandwidth (100\u00a0MHz) and physical construction are the same between the two, and most Cat 5 cables actually happen to meet Cat 5e specifications even though they are not certified as such. Category\u00a05 was deprecated in 2001 and superseded by the Category\u00a05e specification.\nThe Category\u00a06 specification improves upon the Category\u00a05e specification by extending frequency response and further reducing crosstalk. The improved performance of Cat\u00a06 provides 250\u00a0MHz bandwidth. Category\u00a06A cable provides 500\u00a0MHz bandwidth. Both variants are backward compatible with Category\u00a05 and 5e cables.\nTermination.\nCable types, connector types and cabling topologies are defined by ANSI/TIA-568. Category\u00a05 cable is nearly always terminated with 8P8C modular connectors (often referred to incorrectly as RJ45 connectors). The cable is terminated in either the T568A scheme or the T568B scheme. The two schemes work equally well and may be mixed in an installation so long as the same scheme is used on both ends of each cable.\nApplications.\nCategory 5 cable is used in structured cabling for computer networks such as Ethernet over twisted pair. The cable standard prescribes performance parameters for frequencies up to 100 MHz and is suitable for 10BASE-T, 100BASE-TX (Fast Ethernet), 1000BASE-T (Gigabit Ethernet), and 2.5GBASE-T. 10BASE-T and 100BASE-TX Ethernet connections require two wire pairs. 1000BASE-T and faster Ethernet connections require four wire pairs. Through the use of power over Ethernet (PoE), power can be carried over the cable in addition to Ethernet data. \nCat\u00a05 is also used to carry other signals such as telephony and video. In some cases, multiple signals can be carried on a single cable; Cat\u00a05 can carry two conventional telephone lines as well as 100BASE-TX in a single cable. The USOC/RJ-61 wiring standard may be used in multi-line telephone connections. Various schemes exist for transporting both analog and digital video over the cable. HDBaseT (10.2 Gbit/s) is one such scheme.\nCharacteristics.\nThe use of balanced lines helps preserve a high signal-to-noise ratio despite interference from both external sources and crosstalk from other pairs.\nInsulation.\nOuter insulation is typically polyvinyl chloride (PVC) or low smoke zero halogen (LS0H).\nBending radius.\nMost Category\u00a05 cables can be bent at any radius exceeding approximately four times the outside diameter of the cable.\nMaximum cable segment length.\nThe maximum length for a cable segment is per TIA/EIA 568-5-A. If longer runs are required, the use of active hardware such as a repeater or switch is necessary. The specifications for 10BASE-T networking specify a 100-meter length between active devices. This allows for 90 meters of solid-core permanent wiring, two connectors and two stranded patch cables of 5 meters, one at each end.\nConductors.\nSince 1995, solid-conductor unshielded twisted pair (UTP) cables for backbone cabling is required to be no thicker than 22 American Wire Gauge (AWG) and no thinner than 24 AWG, or 26 AWG for shorter-distance cabling. This standard has been retained with the 2009 revision of ANSI TIA/EIA 568.&lt;ref name=\"ANSI/TIA/EIA-568-B.2-2001, Commercial Building Telecommunications Cabling Standard\"&gt;&lt;/ref&gt;\nAlthough cable assemblies containing four pairs are common, Category\u00a05 is not limited to four pairs. Backbone applications involve using up to 100 pairs.\nIndividual twist lengths.\nThe distance per twist is commonly referred to as pitch. Each of the four pairs in a Cat\u00a05 cable has a differing pitch to minimize crosstalk between the pairs. The pitch of the twisted pairs is not specified in the standard.\nEnvironmental ratings.\nSome cables are \"UV-rated\" or \"UV-stable\" meaning they can be exposed to outdoor UV radiation without significant degradation.\nPlenum-rated cables are slower to burn and produce less smoke than cables using a mantle of materials like PVC. Plenum-rated cables may be installed in plenum spaces where PVC is not allowed.\nShielded cables (FTP or STP) are useful for environments where proximity to RF equipment may introduce electromagnetic interference, and can also be used where eavesdropping likelihood should be minimized.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51539", "revid": "15946136", "url": "https://en.wikipedia.org/wiki?curid=51539", "title": "Cat-5", "text": ""}
{"id": "51540", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=51540", "title": "Fast ethernet", "text": ""}
{"id": "51541", "revid": "19244234", "url": "https://en.wikipedia.org/wiki?curid=51541", "title": "Washington County, New York", "text": "Washington County is a county in the U.S. state of New York. As of the 2020 census, the population was 61,302. The county seat is Fort Edward. The county was named for U.S. President George Washington. The county is part of the Capital District region of the state.\nHistory.\nWhen counties were established in the colony of New York in 1683, the present Washington County was part of Albany County. This was an enormous county, including the northern part of New York State as well as all of the present state of Vermont and, in theory, extending westward to the Pacific Ocean. This county was reduced in size on July 3, 1766, by the creation of Cumberland County, and further on March 16, 1770, by the creation of Gloucester County, both containing territory now in Vermont.\nOn March 12, 1772, what was left of Albany County was split into three parts, one remaining under the name Albany County. The other two were called Tryon County (later renamed Montgomery County) and Charlotte County.\nOn April 2, 1784, Charlotte County was renamed Washington County in honor of George Washington, the American Revolutionary War general and later President of the United States of America.\nIn 1788, Clinton County was split off from Washington County. This was a much larger area than the present Clinton County, including several other counties or county parts of the present New York State.\nIn 1791, the Town of Cambridge was transferred from Albany County to Washington County.\nIn 1813, Warren County was split off from Washington County.\nIn 1994, with the completion of the new municipal center, the county seat was moved from Hudson Falls to Fort Edward.\nIn 2006, Cambridge Town Supervisor Jo Ann Trinkle made history by being elected as the first chairwoman of the Board of Supervisors.\nHistoric sites.\nWashington County has four historic covered bridges, each listed on the National Register of Historic Places:\nIncluding those, it has a total of 35 sites listed on the National Register. The Lemuel Haynes House is designated as a National Historic Landmark, the highest level of significance.\nGeography.\nAccording to the U.S. Census Bureau, the county has a total area of , of which is land and (1.7%) is water.\nWashington County is a long narrow county located in the northeastern section of the State. It is known for its rich valley farm land and is part of the Great Appalachian Valley (also known simply as the 'Great Valley') which is a long narrow valley strip often between tall mountain ranges. The county transitions from the Taconic Mountains to the Adirondack Mountains, and from the Lake Champlain Valley to Hudson River Valley.\nMuch of the county is part of the slate valley of the Upper Taconic Mountains (Taghkanic, meaning 'in the trees'). The eastern boundary of Washington County is the New York\u2013Vermont border, part of which is Lake Champlain. This is also the border with New England proper. The northern end of the county is within the 6.1 million acre Adirondack Park. Western boundaries include primarily the Hudson River and Lake George.\nWashington County belongs to the following valleys and watersheds: Champlain Valley / Lake George Watershed\u201402010001 Hudson River Valley / Hudson-Hoosic Watershed\u201402020003 Waters in the northern part drain into Lake Champlain via Lake George (Horican) or the Mettawee River, and then flow into the Saint Lawrence River (Kaniatarowanenneh). These waters mingle in the Saint Lawrence with waters of all the Great Lakes as they flow northeast into the Gulf of Saint Lawrence, and ultimately join the Atlantic Ocean. Meanwhile, the remainder of waters drain south via the Hudson River (Muh-he-kun-ne-tuk or Muhheakantuck), and ultimately flow south into the Atlantic Ocean below New York City. See the approximation of the watershed divide mapped in context of mountains and valleys.\nNearly half of its borders are by long bodies of water. Winding across the bottom of the county is the legendary Batten Kill (Dionondehowa), famous for its worldclass flyfishing, and its marvelous falls (near the Washington County fairgrounds).\nBlack Mountain, in the Adirondacks, is the tallest peak in Washington County at approximately , and has beautiful views of Lake George, Lake Champlain, the surrounding countryside, and the Adirondacks, Taconic Mountains and Green Mountains. Willard Mountain is a ski center in the southern part of the county.\nDemographics.\n&lt;templatestyles src=\"US Census population/styles.css\"/&gt;\n2000 census.\nAs of the census of 2000, there were 61,042 people, 22,458 households, and 15,787 families residing in the county. The population density was . There were 26,794 housing units at an average density of . The racial makeup of the county was 94.97% White, 2.92% Black or African American, 0.20% Native American, 0.28% Asian, 0.01% Pacific Islander, 0.84% from other races, and 0.77% from two or more races. 2.02% of the population were Hispanic or Latino of any race. 17.5% were of Irish, 14.1% French, 12.1% English, 11.1% American, 9.0% Italian and 7.7% German ancestry according to Census 2000. 96.9% spoke English and 1.4% Spanish as their first language.\nThere were 22,458 households, out of which 33.20% had children under the age of 18 living with them, 55.20% were married couples living together, 10.40% had a female householder with no husband present, and 29.70% were non-families. 24.00% of all households were made up of individuals, and 10.80% had someone living alone who was 65 years of age or older. The average household size was 2.55 and the average family size was 3.01.\nIn the county, the population was spread out, with 24.60% under the age of 18, 8.30% from 18 to 24, 29.40% from 25 to 44, 23.70% from 45 to 64, and 14.00% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 105.20 males. For every 100 females age 18 and over, there were 104.50 males.\nThe median income for a household in the county was $37,668, and the median income for a family was $43,500. Males had a median income of $31,537 versus $22,160 for females. The per capita income for the county was $17,958. About 6.80% of families and 9.40% of the population were below the poverty line, including 12.30% of those under age 18 and 7.30% of those age 65 or over.\nGovernment.\nThe county government consists of a board of supervisors with weighted votes. Each town supervisor holds a seat on the county government, and their votes are based on the population of their town, with Kingsbury and Fort Edward supervisors having the largest number of votes, and Putnam having the fewest votes. The 2017 weighted vote totals are available on the http://.\nPolitics.\n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;Gubernatorial elections results\nPrior to 1996, Washington County was a Republican stronghold, with the only time between 1884 and 1992 that a Republican presidential candidate failed to win the county being 1964 when Barry Goldwater lost every county in New York in his statewide and national landslide loss. Since 1996, it has become a bellwether county, but Republican candidate margins of victory have been greater than those by Democratic candidates and broke its bellwether streak in 2020 when Donald Trump won the county. In his 2020 performance, Trump received the highest percentage of the vote for a Republican since 1988 when George H. W. Bush received 62 percent, proceeding to expand his success further four years later. No Democrat aside from Lyndon B. Johnson in the aforementioned 1964 election has managed to win majority of the county's votes.\nTransportation.\nAirports.\nThe following public use airports are located in the county:\nRail.\nAmtrak's \"Adirondack\" and \"Ethan Allen Express\" services each travel through Washington County once a day in each direction on their routes between New York, New York and Montreal, Qu\u00e9bec or Burlington, Vermont, respectively. Both routes stop in Fort Edward and the \"Adirondack\" additionally serves Whitehall. The \"Adirondack\" was temporarily suspended from March 2020 through early April 2023 due to the closure of the Canadian/American border in response to the COVID-19 pandemic and related logistical challenges.\nCommunities.\nTowns.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nVillages.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51542", "revid": "25938316", "url": "https://en.wikipedia.org/wiki?curid=51542", "title": "Cat5", "text": "Cat5 or CAT5 may refer to:\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n Topics referred to by the same termThis page lists articles associated with the same title formed as a letter\u2013number combination."}
{"id": "51543", "revid": "8087765", "url": "https://en.wikipedia.org/wiki?curid=51543", "title": "Phonetic complement", "text": "Pronunciation guide accompanying logographic writing\nA phonetic complement is a phonetic symbol used to disambiguate word characters (logograms) that have multiple readings, in mixed logographic-phonetic scripts such as Egyptian hieroglyphs, Akkadian cuneiform, Linear B, Japanese, and Mayan. Often they disambiguate an ideogram by spelling out the first or last syllable of the word; occasionally (as in Linear B) they may instead abbreviate an adjective that modifies the logogram.\nWritten English has few logograms, primarily numerals, and therefore few phonetic complements. An example is the \"nd\" of \"2nd\" 'second', which avoids ambiguity with 2 standing for the word 'two'. In addition to numerals, other examples include Xmas, Xianity, and Xing for Christmas, Christianity, and Crossing \u2013 note the separate readings \"Christ\" and \"Cross\".\nIn cuneiform.\nIn Sumerian, the single word \"kur\" (\ud808\uddb3) had two main meanings: 'hill' and 'country'. Akkadian, however, had separate words for these two meanings: \"\u0161ad\u00fa\" 'hill' and \"m\u0101tu\" 'country'. When Sumerian cuneiform was adapted (known as \"orthographic borrowing\") for writing Akkadian, this was ambiguous because both words were written with the same character (\ud808\uddb3, conventionally transcribed KUR, after its Sumerian pronunciation). To alert the reader as to which Akkadian word was intended, the phonetic complement \"-\u00fa\" was written after KUR if 'hill' was intended, so that the characters KUR-\u00fa were pronounced \"\u0161ad\u00fa,\" whereas KUR without a phonetic complement was understood to mean \"m\u0101tu\" 'country'.\nPhonetic complements also indicated the Akkadian nominative and genitive cases. Similarly, Hittite cuneiform occasionally uses phonetic complements to attach Hittite case endings to Sumerograms and Akkadograms.\nPhonetic complements should not be confused with determinatives (which were also used to disambiguate) since determinatives were used specifically to indicate the category of the word they preceded or followed. For example, the sign DINGIR (\ud808\udc2d) often precedes names of gods, as LUGAL (\ud808\ude17) does for kings. It is believed that determinatives were not pronounced. \nIn Japanese.\nAs in Akkadian, Japanese borrowed a logographic script, Chinese, designed for a very different language. The Chinese phonetic components built into these \"kanji\" () do not work when they are pronounced in Japanese, and there is not a one-to-one relationship between them and the Japanese words they represent.\nFor example, the kanji \u751f, pronounced \"sh\u014d\" or \"sei\" in borrowed Chinese vocabulary, stands for several native Japanese words as well. When these words have inflectional endings (verbs/adjectives and adverbs), the end of the stem is written phonetically:\nas well as the hybrid Chinese-Japanese word\nNote that some of these verbs share a kanji reading (\"i,\" \"u,\" and \"ha\"), and okurigana are conventionally picked to maximize these sharings.\nThese phonetic characters are called \"okurigana.\" They are used even when the inflection of the stem can be determined by a following inflectional suffix, so the primary function of \"okurigana\" for many kanji is that of a phonetic complement.\nGenerally it is the final syllable containing the inflectional ending is written phonetically. However, in adjectival verbs ending in \"-shii\" (-\u3057\u3044), and in those verbs ending in \"-ru\" (-\u308b) in which this syllable drops in derived nouns, the final two syllables are written phonetically. There are also irregularities. For example, the word \"umareru\" 'be born' is derived from \"umu\" 'to bear, to produce'. As such, it may be written \u751f\u307e\u308c\u308b [\u751fmareru], reflecting its derivation, or \u751f\u308c\u308b [\u751freru], as with other verbs ending in elidable \"-ru\".\nIn Phono-Semantic Characters.\nIn Chinese.\nChinese never developed a system of purely phonetic characters. Instead, about 90% of Chinese characters are compounds of a determinative (called a 'radical'), which may not exist independently, and a phonetic complement indicates the approximate pronunciation of the morpheme. However, the phonetic element is basic, and these might be better thought of as characters used for multiple near homonyms, the identity of which is constrained by the determiner. Due to sound changes over the last several millennia, the phonetic complements are not a reliable guide to pronunciation. Also, sometimes it is not obvious at all where the phonetic complements reside, for instance, the phonetic complement in \u807d is \ud844\ude3c, in \u985e is \u982a, and in \u52dd is \u6715, etc. \nIn Vietnamese.\n\"Ch\u1eef N\u00f4m\" of Vietnamese is almost all constructed as phono-semantic characters, whose phonetic component and semantic component are usually individual unabridged Chinese characters (like the \"Ch\u1eef N\u00f4m\" \ud84c\udf8f and \ud84e\ude42), instead of often radicals as in Sinographs.\nIn Korean.\nA handful of Korean \"gukja\" are also constructed as phono-semantic characters, such as \u4e6d (pronounced as \ub3cc, dol) whose phonetic complement is the bottom \u4e59. \nIn Japanese.\nSome of Japanese \"Kokuji\" are phono-semantic characters, like \u50cd, \u817a, \u9453, whose phonetic complement is \u52d5, \u6cc9, \u9063 respectively.\nIn the Maya Script.\nThe Maya Script, the logosyllabic orthography of the Maya Civilization, used phonetic complements extensively and phonetic complements could be used synharmonically or disharmonically. The former is exemplified by the placement of the syllabogram for \"ma\" underneath the logogram for \"jaguar\" (in Classic Maya, \"BALAM\"): thus, though pronounced \"BALAM\", the word for \"jaguar\" was spelled \"BALAM-m(a)\". Disharmonic spellings also existed in the Maya Script.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51544", "revid": "50034649", "url": "https://en.wikipedia.org/wiki?curid=51544", "title": "Enhanced Interior Gateway Routing Protocol", "text": "Internet protocol\nEnhanced Interior Gateway Routing Protocol (EIGRP) is an advanced distance-vector routing protocol that is used on a computer network for automating routing decisions and configuration. The protocol was designed by Cisco Systems as a proprietary protocol, available only on Cisco routers. In 2013, Cisco permitted other vendors to freely implement a limited version of EIGRP with some of its associated features such as High Availability (HA), while withholding other EIGRP features such as EIGRP stub, needed for DMVPN and large-scale campus deployment. Information needed for implementation was published with informational status as in 2016, which did not advance to Internet Standards Track level, and allowed Cisco to retain control of the EIGRP protocol.\nEIGRP is used on a router to share routes with other routers within the same autonomous system. Unlike other well known routing protocols, such as RIP, EIGRP only sends incremental updates, reducing the workload on the router and the amount of data that needs to be transmitted.\nEIGRP replaced the Interior Gateway Routing Protocol (IGRP) in 1993. One of the major reasons for this was the change to classless IPv4 addresses in the Internet Protocol, which IGRP could not support.\nOverview.\nAlmost all routers contain a routing table that contains rules by which traffic is forwarded in a network. If the router does not contain a valid path to the destination, the traffic is discarded. EIGRP is a dynamic routing protocol by which routers automatically share route information. This eases the workload on a network administrator who does not have to configure changes to the routing table manually.\nIn addition to the routing table, EIGRP uses the following tables to store information:\nInformation in the topology table may be inserted into the router's routing table and can then be used to forward traffic. If the network changes (for example, a physical link fails or is disconnected), the path will become unavailable. EIGRP is designed to detect these changes and will attempt to find a new path to the destination. The old path that is no longer available is removed from the routing table. Unlike most distance vector routing protocols, EIGRP does not transmit all the data in the router's routing table when a change is made, but will only transmit the changes that have been made since the routing table was last updated. EIGRP does not send its routing table periodically, but will only send routing table data when an actual change has occurred. This behavior is more inline with link-state routing protocols, thus EIGRP is mostly considered a hybrid protocol.\nWhen a router running EIGRP is connected to another router also running EIGRP, information is exchanged between the two routers. They form a relationship, known as an \"adjacency\". The entire routing table is exchanged between both routers at this time. After the exchange has completed, only differential changes are sent.\nEIGRP is often considered a hybrid protocol because it also sends link state updates when link states change.\nFeatures.\nEIGRP supports the following features:\nConfiguration.\nCisco IOS example.\nExample of setting up EIGRP on a Cisco IOS router for a private network. The 0.0.15.255 wildcard in this example indicates a subnetwork with a maximum of 4094 hosts\u2014it is the bitwise complement of the subnet mask 255.255.240.0. The no auto-summary command prevents automatic route summarization on classful boundaries, which would otherwise result in routing loops in discontiguous networks.\nTechnical details.\nEIGRP is a distance vector &amp; Link State routing protocol that uses the diffusing update algorithm (DUAL) (based on work from SRI International) to improve the efficiency of the protocol and to help prevent calculation errors when attempting to determine the best path to a remote network. EIGRP determines the value of the path using five metrics: bandwidth, load, delay, reliability and MTU. EIGRP uses five different messages to communicate with its neighbor routers \u2013 Hello, Update, Query, Reply, and Acknowledgement.\nEIGRP routing information, exchanged to a router from another router within the same autonomous system, has a default administrative distance of 90. EIGRP routing information, that has come from an EIGRP-enabled router outside the autonomous system, has a default administrative distance of 170.\nEIGRP does not operate using the Transmission Control Protocol (TCP) or the User Datagram Protocol (UDP). This means that EIGRP does not use a port number to identify traffic. Rather, EIGRP is designed to work on top of Layer 3 (i.e. the IP protocol). Since EIGRP does not use TCP for communication, it implements Cisco's Reliable Transport Protocol (RTP) to ensure that EIGRP router updates are delivered to all neighbors completely. The Reliable Transport Protocol also contains other mechanisms to maximize efficiency and support multicasting. EIGRP uses 224.0.0.10 as its multicast address and protocol number 88.\nDistance vector routing protocol.\nCisco Systems now classifies EIGRP as a distance vector routing protocol, but it is normally said to be a hybrid routing protocol. While EIGRP is an advanced routing protocol that combines many of the features of both link-state and distance-vector routing protocols, EIGRP's DUAL algorithm contains many features which make it more of a distance vector routing protocol than a link-state routing protocol. Despite this, EIGRP contains many differences from most other distance-vector routing protocols, including:\nEIGRP composite and vector metrics.\nEIGRP associates six different vector metrics with each route and considers only four of the vector metrics in computing the Composite metric:\n Router1# show ip eigrp topology 10.0.0.1 255.255.255.255\n IP-EIGRP topology entry for 10.0.0.1/32\n State is Passive, Query origin flag is 1, 1 Successor(s), FD is 40640000\n Routing Descriptor Blocks:\n 10.0.0.1 (Serial0/0/0), from 10.0.0.1, Send flag is 0x0\n Composite metric is (40640000/128256), Route is Internal\n Vector metric:\n Minimum bandwidth is 64\u00a0Kbit\n Total delay is 25000 microseconds\n Reliability is 255/255\n Load is 197/255\n Minimum MTU is 576\n Hop count is 2\nRouting metric.\nThe composite routing metric calculation uses five parameters, so-called K values, K1 through K5. These act as multipliers or modifiers in the composite metric calculation. K1 is not equal to Bandwidth, etc.\nBy default, only total delay and minimum bandwidth are considered when EIGRP is started on a router, but an administrator can enable or disable all the K values as needed to consider the other Vector metrics.\nFor the purposes of comparing routes, these are combined together in a weighted formula to produce a single overall metric:\n formula_1\nwhere the various constants (formula_2 through formula_3) can be set by the user to produce varying behaviors. An important and unintuitive fact is that if formula_3 is set to zero, the term formula_5 is not used (i.e. taken as 1).\nThe default is for formula_2 and formula_7 to be set to 1, and the rest to zero, effectively reducing the above formula to formula_8.\nObviously, these constants must be set to the same value on all routers in an EIGRP system, or permanent routing loops may result. Cisco routers running EIGRP will not form an EIGRP adjacency and will complain about K-values mismatch until these values are identical on these routers.\nEIGRP scales the interface \"Bandwidth\" and \"Delay\" configuration values with following calculations:\n formula_9 = 107 / Value of the \"bandwidth\" interface command\n formula_10 = Value of the \"delay\" interface command\nOn Cisco routers, the interface bandwidth is a configurable static parameter expressed in kilobits per second (setting this only affects metric calculation and not actual line bandwidth). Dividing a value of 107 kbit/s (i.e. 10\u00a0Gbit/s) by the interface bandwidth statement value yields a result that is used in the weighted formula. The interface delay is a configurable static parameter expressed in tens of microseconds. EIGRP takes this value directly without scaling into the weighted formula. However, various \"show\" commands display the interface delay in microseconds. Therefore, if given a delay value in microseconds, it must first be divided by 10 before using it in the weighted formula.\nIGRP uses the same basic formula for computing the overall metric, the only difference is that in IGRP, the formula does not contain the scaling factor of 256. In fact, this scaling factor was introduced as a simple means to facilitate backward compatility between EIGRP and IGRP: In IGRP, the overall metric is a 24-bit value while EIGRP uses a 32-bit value to express this metric. By multiplying a 24-bit value with the factor of 256 (effectively bit-shifting it 8 bits to the left), the value is extended into 32 bits, and vice versa. This way, redistributing information between EIGRP and IGRP involves simply dividing or multiplying the metric value by a factor of 256, which is done automatically.\nFeasible successor.\nA feasible successor for a particular destination is a next hop router that is guaranteed not to be a part of a routing loop. This condition is verified by testing the feasibility condition.\nThus, every successor is also a feasible successor. However, in most references about EIGRP the term \"feasible successor\" is used to denote only those routes which provide a loop-free path but which are not successors (i.e. they do not provide the least distance). From this point of view, for a reachable destination, there is always at least one successor, however, there might not be any feasible successors.\nA feasible successor provides a working route to the same destination, although with a higher distance. At any time, a router can send a packet to a destination marked \"Passive\" through any of its successors or feasible successors without alerting them in the first place, and this packet will be delivered properly. Feasible successors are also recorded in the topology table.\nThe feasible successor effectively provides a backup route in the case that existing successors become unavailable. Also, when performing unequal-cost load-balancing (balancing the network traffic in inverse proportion to the cost of the routes), the feasible successors are used as next hops in the routing table for the load-balanced destination.\nBy default, the total count of successors and feasible successors for a destination stored in the routing table is limited to four. This limit can be changed in the range from 1 to 6. In more recent versions of Cisco IOS (e.g. 12.4), this range is between 1 and 16.\nActive and passive state.\nA destination in the topology table can be marked either as \"passive\" or \"active\". A passive state is a state when the router has identified the successor(s) for the destination. The destination changes to \"active\" state when the current successor no longer satisfies the feasibility condition and there are no feasible successors identified for that destination (i.e. no backup routes are available). The destination changes back from \"active\" to \"passive\" when the router received replies to all queries it has sent to its neighbors. Notice that if a successor stops satisfying the feasibility condition but there is at least one feasible successor available, the router will promote a feasible successor with the lowest total distance (the distance as reported by the feasible successor plus the cost of the link to this neighbor) to a new successor and the destination will remain in the \"passive\" state.\nFeasibility condition.\nThe feasibility condition is a sufficient condition for loop freedom in EIGRP-routed network. It is used to select the successors and feasible successors that are guaranteed to be on a loop-free route to a destination. Its simplified formulation is strikingly simple:\n\"If, for a destination, a neighbor router advertises a distance that is strictly lower than our feasible distance, then this neighbor lies on a loop-free route to this destination.\"\nor in other words,\n\"If, for a destination, a neighbor router tells us that it is closer to the destination than we have ever been, then this neighbor lies on a loop-free route to this destination.\"\nIt is important to realize that this condition is a sufficient, not a necessary, condition. That means that neighbors which satisfy this condition are guaranteed to be on a loop-free path to some destination, however, there may be also other neighbors on a loop-free path which do not satisfy this condition. However, such neighbors do not provide the shortest path to a destination, therefore, not using them does not present any significant impairment of the network functionality. These neighbors will be re-evaluated for possible usage if the router transitions to Active state for that destination.\nUnequal Path Cost Load Balancing.\nEIGRP features load balancing on paths with different costs. A multiplier, called variance, is used to determine which paths to include into load balancing. The variance is set to 1 by default, which means load balancing on equal cost paths. The maximum variance is 128. The minimum metric of a route is multiplied by the variance value. Each path with a metric that is smaller than the result is used in load balancing.\nWith the functionality of the Unequal Path Cost Load Balancing on EIGRP, OSPF protocol is unable to design the network by Unequal Path Cost Load Balancing. Regarding the Unequal Path Cost Load Balancing function on industry usage, the network design can be flexible with the traffic management.\nEIGRP and compatibility to other vendors.\nCisco released details of the proprietary EIGRP routing protocol in an RFC in an effort to assist companies whose networks operate in a multi-vendor environment. The protocol is described in . EIGRP was developed 20 years ago, yet it is still one of the primary Cisco routing protocols due to its purported usability and scalability in comparison to other protocols.\nCisco has stated that EIGRP is an open standard but they leave out several core details in the RFC definition which makes interoperability hard to set up between different vendors' routers when the protocol is used. Even Cisco NX-OS for example does not support unequal cost load balancing.\nAs of 2022 EIGRP has alpha support in FRRouting and it seems to be generally unsupported by other routing software.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51545", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=51545", "title": "EIGRP", "text": ""}
{"id": "51546", "revid": "1287970770", "url": "https://en.wikipedia.org/wiki?curid=51546", "title": "Elam, son of Shem", "text": "Biblical character\nElam (; &lt;templatestyles src=\"Script/styles_hebrew.css\" /&gt;\u05e2\u05b5\u05d9\u05dc\u05b8\u05dd\u200e \"\u2018Elam\") in the Hebrew Bible (Genesis 10:22, Ezra 4:9) is said to be one of the sons of Shem, the son of Noah. The name is also used (as in Akkadian) for the ancient country of Elam in what is now southwestern Iran, whose people the Hebrews believed to be the offspring of Elam, son of Shem (Genesis 10:22). This implies that the Elamites were considered Semites by the Hebrews. Their language was not one of the Semitic languages, but is considered a linguistic isolate. \nElam (the nation) is also mentioned in Genesis 14, describing an ancient war in the time of Abraham, involving Chedorlaomer, the king of Elam at that time.\nThe prophecies of the Book of Isaiah (11:11, 21:2, 22:6) and the Book of Jeremiah (25:25) also mention Elam. The last part of Jeremiah 49 is an apocalyptic oracle against Elam which states that Elam will be scattered to the four winds of the earth, but \"will be, in the end of days, that I will return their captivity,\" a prophecy self-dated to the first year of Zedekiah (597 BC).\nThe Book of Jubilees may reflect ancient tradition when it mentions a son (or daughter, in some versions) of Elam named \"Susan\", whose daughter Rasuaya married Arpachshad, progenitor of another branch of Shemites. Shushan (or Susa) was the ancient capital of the Elamite Empire. (Dan. 8:2)\nElam as a personal name also refers to other figures appearing in the Hebrew Bible:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51547", "revid": "67144", "url": "https://en.wikipedia.org/wiki?curid=51547", "title": "United States Post Office Department", "text": "U.S. federal department (1872\u20131971)\nThe United States Post Office Department (USPOD; also known as the Post Office or U.S. Mail) was the predecessor of the United States Postal Service, established in 1792. From 1872 to 1971, it was officially in the form of a Cabinet department. It was headed by the postmaster general.\nThe Postal Service Act, signed by U.S. president George Washington on February 20, 1792, established the department. Postmaster General John McLean, in office from 1823 to 1829, was the first to call it the Post Office \"Department\" rather than just the \"Post Office.\" The organization received a boost in prestige when President Andrew Jackson invited his postmaster general, William T. Barry, to sit as a member of the Cabinet in 1829. The Post Office Act of 1872 (17\u00a0Stat.\u00a0https://) elevated the Post Office Department to Cabinet status.\nDuring the American Civil War (1861\u20131865), postal services in the Confederate States of America were provided by the Confederate States of America Post-office Department, headed by Postmaster General John Henninger Reagan. It faced insurmountable obstacles, especially the requirement that it not run a deficit.\nThe Postal Reorganization Act was signed by President Richard Nixon on August 12, 1970. It replaced the cabinet-level Post Office Department with the independent United States Postal Service on July 1, 1971. The regulatory role of the postal services was then transferred to the Postal Regulatory Commission.\nFoundations.\nIn the early years of the North American colonies, many attempts were made to initiate a postal service. These early attempts were of small scale and usually involved a colony, Massachusetts Bay Colony for example, setting up a location in Boston where one could post a letter back home to England. Other attempts focused on a dedicated postal service between two of the larger colonies, such as Massachusetts and Virginia, but the available services remained limited in scope and disjointed for many years. For example, informal independently run postal routes operated in Boston as early as 1639, with a Boston to New York City service starting in 1672.\nA central postal organization came to the colonies in 1691, when Thomas Neale received a 21-year grant from the British Crown for a North American Postal Service. On February 17, 1691, a grant of \"letters patent\" from the joint sovereigns, William III and Mary II, empowered him:\nto erect, settle, and establish within the chief parts of their majesties' colonies and plantations in America, an office or offices for receiving and dispatching letters and pacquets, and to receive, send, and deliver the same under such rates and sums of money as the planters shall agree to give, and to hold and enjoy the same for the term of twenty-one years.\nThe patent included the exclusive right to establish and collect a formal postal tax on official documents of all kinds. The tax was repealed a year later. Neale appointed Andrew Hamilton, Governor of New Jersey, as his deputy postmaster. The first postal service in America commenced in February 1692. Rates of postage were fixed and authorized, and measures were taken to establish a post office in each town in Virginia. Massachusetts and the other colonies soon passed postal laws, and a very imperfect post office system was established. Neale's patent expired in 1710, when Parliament extended the English postal system to the colonies. The chief office was established in New York City, where letters were conveyed by regular packets across the Atlantic.\nThe American Revolution.\nBefore the Revolution, there was only a trickle of business or governmental correspondence between the colonies. Most of the mail went back and forth to counting houses and government offices in London. The revolution made Philadelphia, the seat of the Continental Congress, the information hub of the new nation. News, new laws, political intelligence, and military orders circulated with a new urgency, and a postal system was necessary. Journalists took the lead, securing post office legislation that allowed them to reach their subscribers at very low cost, and to exchange news from newspapers between the thirteen states. Overthrowing the London-oriented imperial postal service in 1774\u20131775, printers enlisted merchants and the new political leadership, and created a new postal system. The \"United States Post Office\" (USPO) was created on July 26, 1775, by decree of the Second Continental Congress. Benjamin Franklin headed it briefly.\nBefore the Revolution, individuals like Benjamin Franklin and William Goddard were the colonial postmasters who managed the mails then and were the general architects of a postal system that started out as an alternative to the Crown Post.\nAfter the Revolution.\n The official post office was created in 1792 as the Post Office Department (USPOD). It was based on the Constitutional authority empowering Congress \"To establish post offices and post roads\". The 1792 law provided for a greatly expanded postal network, and served editors by charging newspapers an extremely low rate. The law guaranteed the sanctity of personal correspondence, and provided the entire country with low-cost access to information on public affairs, while establishing a right to personal privacy.\nRufus Easton was appointed by Thomas Jefferson first postmaster of St. Louis under the recommendation of Postmaster General Gideon Granger. Rufus Easton was the first postmaster and built the first post office west of the Mississippi. At the same time Easton was appointed by Thomas Jefferson, judge of Louisiana Territory, the largest territory in North America. Bruce Adamson wrote that: \"Next to Benjamin Franklin, Rufus Easton was one of the most colorful people in United States Postal History.\" It was Easton who educated Abraham Lincoln's attorney general, Edward Bates. In 1815 Edward Bates moved into the Easton home and lived there for years at Third and Elm. Today this is the site of the Jefferson Memorial Park. In 1806 Postmaster General Gideon Granger wrote a three-page letter to Easton, begging him not to partake in a duel with vice-president Aaron Burr. Two years earlier it was Burr who had shot and killed Alexander Hamilton. Many years later in 1852, Easton's son, Brevet Major-General Langdon Cheves Easton, was commissioned by William T. Sherman, at Fort Union to deliver a letter to Independence, Missouri. Sherman wrote: \"In the Spring of 1852, General Sherman mentioned that the quartermaster, Major L.C. Easton, at Fort Union, New Mexico, had occasion to send some message east by a certain date, and contracted with Aubrey to carry it to the nearest post office (then Independence, Missouri), making his compensation conditional on the time consumed. He was supplied with a good horse, and an order on the outgoing trains for exchange. Though the whole route was infested with hostile Indians, and not a house on it, Aubrey started alone with his rifle. He was fortunate in meeting several outward-bound trains, and thereby made frequent changes of horses, some four or five, and reached Independence in six days, having hardly rested or slept the whole way.\"\nTo cover long distances, the Post Office used a hub-and-spoke system, with Washington as the hub and chief sorting center. By 1869, with 27,000 local post offices to deal with, it had changed to sorting mail en route in specialized railroad mail cars, called railway post offices, or RPOs. The system of postal money orders began in 1864. Free mail delivery began in the larger cities in 1863.\n19th century.\nThe postal system played a crucial role in national expansion. It facilitated expansion of the western American frontier by creating an inexpensive, fast, convenient communication system. Letters from early settlers provided information and boosterism to encourage increased migration to the West, helped scattered families stay in touch and provide assistance, assisted entrepreneurs in finding business opportunities, and made possible regular commercial relationships between merchants in the west and wholesalers and factories back east. The postal service likewise assisted the army in expanding control over the vast western territories. The widespread circulation of important newspapers by mail, such as the \"New York Weekly Tribune,\" facilitated coordination among politicians in different states. The postal service helped integrate established areas with the frontier, creating a spirit of nationalism and providing a necessary infrastructure.\nThe Post Office in the 19th century was a major source of federal patronage. Local postmasterships were rewards for local politicians\u2014often the editors of party newspapers. About three quarters of all federal civilian employees worked for the Post Office. In 1816 it employed 3,341 men, and in 1841, 14,290. The volume of mail expanded much faster than the population, as it carried annually 100 letters and 200 newspapers per 1,000 white population in 1790, and 2,900 letters and 2,700 newspapers per thousand in 1840.\nThe Post Office Department was enlarged during the tenure of President Andrew Jackson. As the Post Office expanded, difficulties were experienced due to a lack of employees and transportation. The Post Office's employees at that time were still subject to the so-called \"spoils\" system, where faithful political supporters of the executive branch were appointed to positions in the post office and other government corporations as a reward for their patronage. These appointees rarely had prior experience in postal service and mail delivery. This system of political patronage was replaced in 1883, after passage of the Pendleton Civil Service Reform Act.\nIn 1823, ten years after the Post Office had first begun to use steamboats to carry mail between post towns where no roads existed, waterways were declared post roads. Once it became clear that the postal system in the United States needed to expand across the entire country, the use of the railroad to transport the mail was instituted in 1832, on one line in Pennsylvania. All railroads in the United States were designated as post routes, after passage of the Act of July 7, 1838. Mail service by railroad increased rapidly thereafter.\nAn Act of Congress provided for the issuance of postage stamps on March 3, 1847, and the postmaster general immediately let a contract to the New York City engraving firm of Rawdon, Wright, Hatch, and Edson. The first stamp issue of the U.S. was offered for sale on July 1, 1847, in New York City, with Boston receiving stamps the following day and other cities thereafter. The 5-cent stamp paid for a letter weighing less than and traveling less than 300 miles, the 10-cent stamp for deliveries to locations greater than 300 miles, or twice the weight deliverable for the 5-cent stamp.\nIn 1847, the U.S. Mail Steamship Company acquired the contract which allowed it to carry the U.S. mails from New York, with stops in New Orleans and Havana, to the Isthmus of Panama for delivery in California. The same year, the Pacific Mail Steamship Company had acquired the right to transport mail under contract from the United States Government from the Isthmus of Panama to California. In 1855, William Henry Aspinwall completed the Panama Railway, providing rail service across the Isthmus and cutting to three weeks the transport time for the mails, passengers and goods to California. This remained an important route until the completion of the transcontinental railroad in 1869. Railroad companies greatly expanded mail transport service after 1862, and the Railway Mail Service was inaugurated in 1869.\nRail cars designed to sort and distribute mail while rolling were soon introduced. RMS employees sorted mail \"on-the-fly\" during the journey, and became some of the most skilled workers in the postal service. An RMS sorter had to be able to separate the mail quickly into compartments based on its final destination, before the first destination arrived, and work at the rate of 600 pieces of mail an hour. They were tested regularly for speed and accuracy.\nParcel Post service began with the introduction of International Parcel Post between the U.S. and foreign countries in 1887. That same year, the U.S. Post Office and the postmaster general of Canada established parcel-post service between the two nations. A bilateral parcel-post treaty between the independent (at the time) Kingdom of Hawaii and the USA was signed on December 19, 1888 and put into effect early in 1889. Parcel-post service between the U.S. and other countries grew with the signing of successive postal conventions and treaties. While the Post Office agreed to deliver parcels sent into the country under the UPU treaty, it did not institute a domestic parcel-post service for another twenty-five years.\n20th century.\nThe advent of Rural Free Delivery (RFD) in the U.S. in 1896, and the inauguration of a domestic parcel post service by Postmaster General Frank H. Hitchcock in 1913, greatly increased the volume of mail shipped nationwide, and motivated the development of more efficient postal transportation systems. Many rural customers took advantage of inexpensive Parcel Post rates to order goods and products from businesses located hundreds of miles away in distant cities for delivery by mail. From the 1910s to the 1960s, many college students and others used parcel post to mail home dirty laundry, as doing so was less expensive than washing the clothes themselves.\nAfter four-year-old Charlotte May Pierstorff was mailed from her parents to her grandparents in Idaho in 1914, mailing of people was prohibited. In 1917, the Post Office imposed a maximum daily mailable limit of two hundred pounds per customer per day after a business entrepreneur, W. H. Coltharp, used inexpensive parcel-post rates to ship more than eighty thousand masonry bricks some four hundred seven miles via horse-drawn wagon and train for the construction of a bank building in Vernal, Utah.\nThe advent of parcel post also led to the growth of mail order businesses that substantially increased rural access to modern goods over what was typically stocked in local general stores.\nOne of the largest organizations of the early 20th century, the Post Office Department is reported to have had nearly 350,000 employees in 1924.\nUnited States Postal Savings System.\nIn 1912, carrier service was announced for establishment in towns of second and third class with $100,000 appropriated by Congress. From January 1, 1911, until July 1, 1967, the United States Post Office Department operated the United States Postal Savings System. An Act of Congress of June 25, 1910, established the Postal Savings System in designated post offices, effective January 1, 1911. The legislation aimed to get money out of hiding, attract the savings of immigrants accustomed to the postal savings system in their native countries, provide safe depositories for people who had lost confidence in banks, and furnish more convenient depositories for working people. The law establishing the system directed the Post Office Department to redeposit most of the money in the system in local banks, where it earned 2.5 percent interest.\nThe system paid 2% interest per year on deposits. The half-percent difference in interest was intended to pay for the operation of the system. Certificates were issued to depositors as proof of their deposit. Depositors in the system were initially limited to hold a balance of $500, but this was raised to $1,000 in 1916 and to $2,500 in 1918. The initial minimum deposit was $1. In order to save smaller amounts for deposit, customers could purchase a 10-cent postal savings card and 10-cent postal savings stamps to fill it. The card could be used to open or add to an account when its value, together with any attached stamps, amounted to one or more dollars, or it could be redeemed for cash. At its peak in 1947, the system held almost $3.4 billion in deposits, with more than four million depositors using 8,141 postal units.\nAirmail.\nOn August 12, 1918, the Post Office Department took over airmail service from the United States Army Air Service (USAAS). Assistant Postmaster General Otto Praeger, appointed Benjamin B. Lipsner to head the civilian-operated Air Mail Service. One of Lipsner's first acts was to hire four pilots, each with at least 1,000 hours' flying experience, paying them an average of $4,000 per year ($ today). The Post Office Department used new Standard JR-1B biplanes specially modified to carry the mail while the war was still in progress, but following the war operated mostly World War I surplus military de Havilland DH-4 aircraft.\nDuring 1918, the Post Office hired an additional 36 pilots. In its first year of operation, the Post Office completed 1,208 airmail flights with 90 forced landings. Of those, 53 were due to weather and 37 to engine failure. By 1920, the Air Mail service had delivered 49 million letters. Domestic air mail became obsolete in 1975, and international air mail in 1995, when the USPS began transporting First-Class mail by air on a routine basis.\nThe Post Office was the first federal government departments to regulate obscene materials on a national basis. When the U.S. Congress passed the Comstock laws of 1873, it became illegal to send through the U.S. mail any material considered obscene or indecent, or which promoted abortion issues or birth control. Following the 2022 Dobbs v. Jackson Women's Health Organization ruling, the Comstock Act became a renewed matter of contention. The Biden administration stated that the Comstock Act does not prohibit mailing abortifacients intended for lawful use, and the law is the subject of an ongoing federal court case.\nIn 1937 to 1941 The Post Office handled the shipment of gold from the New York City Assay office and Philadelphia Mint to the newly constructed bullion depository at Fort Knox.\nU.S. postal strike of 1970.\nOn March 18, 1970, postal workers in New York City\u2014upset over low wages and poor working conditions, and emboldened by the Civil Rights Movement\u2014organized a strike against the United States government. The strike initially involved postal workers in only New York City, but it eventually gained support of over 210,000 United States Post Office Department workers across the nation. While the strike ended without any concessions from the Federal government, it did ultimately allow for postal worker unions and the government to negotiate a contract which gave the unions most of what they wanted, as well as the signing of the Postal Reorganization Act by President Richard Nixon on August 12, 1970. The act replaced the cabinet-level Post Office Department with a new federal agency, the United States Postal Service, effective July 1, 1971.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51548", "revid": "38716918", "url": "https://en.wikipedia.org/wiki?curid=51548", "title": "Montreux", "text": "Montreux (, ; ; ) is a Swiss municipality and town on the shoreline of Lake Geneva at the foot of the Alps. It belongs to the Riviera-Pays-d'Enhaut district in the canton of Vaud, having a population of nearly 27,000 with about 85,000 in the Vevey-Montreux agglomeration as of 2019.\nLocated in the centre of a region named the Vaud or Swiss Riviera (), Montreux has been an important tourist destination since the 19th century due to its mild climate. The region includes numerous Belle \u00c9poque palaces and hotels near the shores of Lake Geneva. Montreux railway station is a stop on the Simplon Railway and is a mountain railway hub.\nHistory.\nThe earliest settlement was a Late Bronze Age village at Baugy. Montreux lies on the north east shore of Lake Geneva at the fork in the Roman road from Italy over the Simplon Pass, where the roads to the Roman capital of Aventicum and the road into Gaul through Besan\u00e7on separated. This made it an important settlement in the Roman era. A Roman villa from the 2nd-4th centuries and a 6th\u20137th century cemetery have been discovered.\nThe name derives from a small monastery () on the site by the 11th century, which subsequently developed as , (1215), (1228), , , and . Viniculture was introduced in the 12th century, and the sunny slopes of the lake from Lavaux to Montreux became an important winegrowing region. In 1295, the Bishop of Sion sold the parish of Montreux to Girard of Oron. In 1317, it was split between the Lords of Oron (Le Ch\u00e2telard) and the Counts of Savoy (Les Planches). A Brotherhood of the Holy Spirit administered estates and a hospital in Montreux starting in about 1309.\nThe region was subject to various princes, most notably the princes of Savoy from the south side of the lake. They unified the territory which comprises the present canton of Vaud and were generally popular sovereigns.\nAfter the Burgundian Wars in the 15th century, the Swiss in Bern occupied the region without resistance, an indication of the weakness of the princes of Savoy. Under Bernese rule (1536\u20131798) it belonged to the Bailiwick of Chillon (renamed in 1735 into the Bailiwick of Vevey).\nThe Reformation made the region around Montreux and Vevey an attractive haven for Huguenots from Italy, who brought their artisanal skills and set up workshops and businesses.\nThe abbey of Les Echarpes blanches was founded in 1626.\nIn 1798, the French captured the region from the Bernese. In the 19th century, the tourist industry became a major commercial outlet, with the grand hotels of Montreux attracting the rich and cultured from Europe and America.\nStarting in the 19th century there were three independent municipalities that shared a central authority. This county council was made up of four deputies from Le Ch\u00e2telard, two from Les Planches and one from Veytaux. The church, the market hall of La Rouvenaz, the secondary school (the building was from 1872 and 1897) and the slaughter-house (1912) were all owned by the county council. Each municipality had its own taxes and a mayor. In 1962, the municipalities of Le Ch\u00e2telard and Les Planches merged, while Veytaux remained independent.\nGeography.\nMontreux has an area, as of 2009[ [update]], of . Of this area, or 25.0% is used for agricultural purposes, while or 50.3% is forested. Of the rest of the land, or 20.0% is settled (buildings or roads), or 0.3% is either rivers or lakes and or 4.6% is unproductive land.\nOf the built up area, housing and buildings make up 11.8% and transportation infrastructure made up 6.4%. Out of the forested land, 46.8% of the total land area is heavily forested and 2.9% is covered with orchards or small clusters of trees. Of the agricultural land, 1.5% is used for growing crops and 9.9% is pastures, while 1.2% is used for orchards or vine crops and 13.7% is used for alpine pastures. All the water in the municipality is flowing water.\nThe municipality was part of the Vevey District until it was dissolved on 31 August 2006, and Montreux became part of the new district of Riviera-Pays-d'Enhaut.\nThe municipality stretches from Lake Geneva to the foothills of the Swiss Alps (Rochers-de-Naye). It includes the former municipalities of Montreux-Les Planches (until 1952 Les Planches) and Montreux-Le Ch\u00e2telard (until 1952 Le Ch\u00e2telard). It was formed in 1962 with the merger of the two former municipalities.\nClimate.\nThe K\u00f6ppen Climate Classification subtype for Montreux's climate is \"\" (Marine West Coast Climate/Oceanic climate).\nDemographics.\nMontreux has a population (as of December 2020[ [update]]) of . As of 2008[ [update]], 44.2% of the population are resident foreign nationals. Over the last 10 years (1999\u20132009) the population has changed at a rate of 14.7%. It has changed at a rate of 22.3% due to migration and at a rate of -0.8% due to births and deaths.\nMost of the population (as of 2000[ [update]]) speaks French (16,695 or 74.4%) as their first language, with German being second most common (1,398 or 6.2%) and Italian being third (897 or 4.0%). There are 9 people who speak Romansh.\nThe age distribution, as of 2009[ [update]], in Montreux is; 2,050 children or 8.3% of the population are between 0 and 9 years old and 3,021 teenagers or 12.2% are between 10 and 19. Of the adult population, 4,216 people or 17.0% of the population are between 20 and 29 years old. 3,016 people or 12.2% are between 30 and 39, 3,552 people or 14.4% are between 40 and 49, and 3,048 people or 12.3% are between 50 and 59. The senior population distribution is 2,565 people or 10.4% of the population are between 60 and 69 years old, 1,795 people or 7.3% are between 70 and 79, there are 1,206 people or 4.9% who are between 80 and 89, and there are 263 people or 1.1% who are 90 and older.\nAs of 2000[ [update]], there were 9,380 people who were single and never married in the municipality. There were 9,758 married individuals, 1,631 widows or widowers and 1,685 individuals who are divorced.\nAs of 2000[ [update]], there were 9,823 private households in the municipality, and an average of 2 persons per household. There were 4,198 households that consist of only one person and 402 households with five or more people. Out of a total of 10,236 households that answered this question, 41.0% were households made up of just one person and there were 53 adults who lived with their parents. Of the rest of the households, there are 2,563 married couples without children, 2,245 married couples with children. There were 605 single parents with a child or children. There were 159 households that were made up of unrelated people and 413 households that were made up of some sort of institution or another collective housing.\nIn 2000[ [update]] there were 1,375 single family homes (or 43.2% of the total) out of a total of 3,183 inhabited buildings. There were 1,024 multi-family buildings (32.2%), along with 530 multi-purpose buildings that were mostly used for housing (16.7%) and 254 other use buildings (commercial or industrial) that also had some housing (8.0%).\nIn 2000[ [update]], a total of 9,553 apartments (70.7% of the total) were permanently occupied, while 3,043 apartments (22.5%) were seasonally occupied and 916 apartments (6.8%) were empty. As of 2009[ [update]], the construction rate of new housing units was 1.6 new units per 1000 residents.\nAs of 2003[ [update]] the average price to rent an average apartment in Montreux was 1067.93 Swiss francs (CHF) per month (US$850, \u00a3480, \u20ac680 approx. exchange rate from 2003). The average rate for a one-room apartment was 567.76 CHF (US$450, \u00a3260, \u20ac360), a two-room apartment was about 787.77 CHF (US$630, \u00a3350, \u20ac500), a three-room apartment was about 1014.16 CHF (US$810, \u00a3460, \u20ac650) and a six or more room apartment cost an average of 1817.64 CHF (US$1450, \u00a3820, \u20ac1160). The average apartment price in Montreux was 95.7% of the national average of 1116 CHF. The vacancy rate for the municipality, in 2010[ [update]], was 0.55%.\nThe historical population is given in the following chart:\nHeritage sites of national significance.\nThe Swiss heritage site of national significance in Montreux includes: The Audiorama (also known as the Swiss National Audiovisual Museum), Cr\u00eates Castle, Ch\u00e2telard Castle, the Train Station, the H\u00f4tel Montreux Palace, the Caux Palace Hotel, the \u00cele and Villa Salagnon, the March\u00e9 couvert, the Grand-H\u00f4tel/the H\u00f4tel des Alpes (which served as the recording studio for Deep Purple's \"Machine Head\" album), and the Villa Karma.\nThe entire urban village of Territet / Veytaux as well as the Caux, Montreux and Villas Dubochet areas are all part of the Inventory of Swiss Heritage Sites.\nTwin towns.\nMontreux is twinned with the towns of\nPolitics.\nIn the 2007 federal election the most popular party was the SP which received 22.11% of the vote. The next three most popular parties were the SVP (21.97%), the FDP (16.06%) and the Green Party (13.49%). In the federal election, a total of 4,473 votes were cast, and the voter turnout was 39.7%.\nEconomy.\nAs of \u00a02010[ [update]], Montreux had an unemployment rate of 6.9%. As of 2008[ [update]], there were 70 people employed in the primary economic sector and about 27 businesses involved in this sector. 1,165 people were employed in the secondary sector and there were 174 businesses in this sector. 9,290 people were employed in the tertiary sector, with 999 businesses in this sector. There were 10,202 residents of the municipality who were employed in some capacity, of which females made up 46.1% of the workforce.\nIn 2008[ [update]] the total number of full-time equivalent jobs was 8,991. The number of jobs in the primary sector was 55, of which 31 were in agriculture, 17 were in forestry or lumber production and 6 were in fishing or fisheries. The number of jobs in the secondary sector was 1,118 of which 403 or (36.0%) were in manufacturing and 708 (63.3%) were in construction. The number of jobs in the tertiary sector was 7,818. In the tertiary sector; 1,296 or 16.6% were in wholesale or retail sales or the repair of motor vehicles, 439 or 5.6% were in the movement and storage of goods, 1,311 or 16.8% were in a hotel or restaurant, 70 or 0.9% were in the information industry, 564 or 7.2% were the insurance or financial industry, 458 or 5.9% were technical professionals or scientists, 943 or 12.1% were in education and 1,591 or 20.4% were in health care.\nIn 2000[ [update]], there were 4,949 workers who commuted into the municipality and 4,964 workers who commuted away. The municipality is a net exporter of workers, with about 1.0 workers leaving the municipality for every one entering. About 2.3% of the workforce coming into Montreux are coming from outside Switzerland, while 0.0% of the locals commute out of Switzerland for work. Of the working population, 22.5% used public transportation to get to work, and 50.9% used a private car.\nReligion.\nFrom the 2000 census[ [update]], 8,557 or 38.1% were Roman Catholic, while 6,438 or 28.7% belonged to the Swiss Reformed Church. Of the rest of the population, there were 745 members of an Orthodox church (or about 3.32% of the population), there were 18 individuals (or about 0.08% of the population) who belonged to the Christian Catholic Church, and there were 925 individuals (or about 4.12% of the population) who belonged to another Christian church. There were 73 individuals (or about 0.33% of the population) who were Jewish, and 1,031 (or about 4.59% of the population) who were Muslim. There were 80 individuals who were Buddhist, 171 individuals who were Hindu and 90 individuals who belonged to another church. 2,796 (or about 12.45% of the population) belonged to no church, are agnostic or atheist, and 1,941 individuals (or about 8.64% of the population) did not answer the question.\nEducation.\nIn Montreux about 7,464 (33.2%) of the population have completed non-mandatory upper secondary education, and 3,171 or (14.1%) have completed additional higher education (either university or a ). Of the 3,171 who completed tertiary schooling, 39.8% were Swiss men, 25.3% were Swiss women, 19.8% were non-Swiss men and 15.1% were non-Swiss women.\nIn the 2009/2010 school year there were a total of 2,106 students in the Montreux school district. In the Vaud cantonal school system, two years of non-obligatory pre-school are provided by the political districts. During the school year, the political district provided pre-school care for a total of 817 children of which 456 children (55.8%) received subsidized pre-school care. The canton's primary school program requires students to attend for four years. There were 1,056 students in the municipal primary school program. The obligatory lower secondary school program lasts for six years and there were 931 students in those schools. There were also 119 students who were home schooled or attended another non-traditional school.\nAs of 2000[ [update]], there were 490 students in Montreux who came from another municipality, while 790 residents attended schools outside the municipality.\nPublic libraries.\nMontreux is home to the \"Biblioth\u00e8que municipale de Montreux et Veytaux\" library. The library has (as of 2008[ [update]]) 48,948 books or other media, and loaned out 99,490 items in the same year. It was open a total of 274 days with average of 28 hours per week during that year.\nPrivate schools.\nThe Riviera School or \u00c9cole Riviera, an international school, is in Montreux.\nOther local schools include Surval Montreux (an international all girls boarding school) and St George's School in Switzerland (British International School, in Clarens).\nInstitut Monte Rosa, an international co-educational boarding school, is in Territet.\nSwiss Montreux Business School\nPrivate hospitality schools in the area include Swiss Hotel Management School (in Caux), Hotel Institute Montreux (Montreux), and Glion Institute of Higher Education (Glion).\nTransportation.\nMontreux has three railway stations on the Simplon line, Clarens, Territet, and Montreux. The latter is also the western terminus of the Montreux\u2013Glion\u2013Rochers-de-Naye and Montreux\u2013Lenk im Simmental lines, both of which climb into the hills away from Lake Geneva and have several dozen stations within Montreux.\nCulture.\nMontreux was a haven for Catherine Barkley and Lt. Frederic Henry in Ernest Hemingway's classic novel \"A Farewell to Arms\".\nMontreux hosts several festivals:\nMontreux has a walking trail along the lake, stretching from Villeneuve to Vevey. The main square of the town, Place du March\u00e9, features a statue of Freddie Mercury facing Lake Geneva. Some of the numerous small cities around Montreux include La Tour-de-Peilz, and Villeneuve. The Ch\u00e2teau of Chillon has views over Lake of Geneva and can be accessed via bus, train, walk or boat.\nDeep Purple traveled to Montreux in December 1971 to record \"Machine Head\". The band's song \"Smoke on the Water\" tells of the events of December 1971, when a Frank Zappa fan with a flare gun set the Montreux Casino on fire, destroying the casino where they had originally planned to record the album. Thanks to Claude Nobs, who eventually arranged alternate locations, the Grand H\u00f4tel de Territet was where almost the entirety of the album was created and recorded, except for \"Smoke on the Water\" which had already been partly recorded at the \"Le Petit Palais\", formerly called \"Le Pavilion\". Deep Purple again returned in 1973 to record \"Burn\". The Montreux Casino was reopened in 1975, and later a monument commemorating Deep Purple and their song \"Smoke on the Water\" was built along the lake shore, with the band's name, the song title, and the riff in musical notes. However, the only other memorial dedicated to the band's song that can be found nowadays in Montreux, is a small plaque placed outside the back entrance of the former Grand H\u00f4tel de Territet.\nThe Dubliners's song \"Montreux Monto\" on their album \"Live at Montreux\" was recorded live at the Montreux Jazz Festival in 1976.\nMontreux is the home of Mountain Studios, the recording studio used by several artists. \"Bonzo's Montreux\" by Led Zeppelin is named after the city where the drums session of John Bonham was recorded in 1976. In 1978, the band Queen bought the studio. It was then sold to Queen producer David Richards. In 2002, the Mountain Studios was converted into a bar as part of a complete renovation of the studio. David Richards has left Montreux to settle down somewhere else. Queen also appeared in 1984 and in 1986 at the Golden Rose Festival and Queen guitarist Brian May appeared in 2001 at the Jazz Festival. Montreux was also the subject of the 1995 Queen single \"A Winter's Tale\" on the album \"Made in Heaven\", one of Freddie's last songs before his death on 24 November 1991. The album cover features the statue of Mercury beside the lake.\nIn 1990, the Wakker Prize for the development and preservation of its architectural heritage was awarded to Montreux.\nThe F\u00e9d\u00e9ration Internationale de Roller Sports was founded in Montreux in 1924. Montreux HC, the oldest roller hockey club in Switzerland (founded in 1911) is based in Montreux.\nThe symphonic metal band Ad Infinitum was founded in Montreux.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51549", "revid": "196446", "url": "https://en.wikipedia.org/wiki?curid=51549", "title": "Postal code", "text": "Series of letters and digits for sorting mail\nA postal code (also known locally in various English-speaking countries throughout the world as a postcode, post code, PIN or ZIP Code) is a series of letters or digits or both, sometimes including spaces or punctuation, included in a postal address for the purpose of sorting mail.\nAs of \u00a02021,https:// the Universal Postal Union lists 160 countries which require the use of a postal code.\nAlthough postal codes are usually assigned to geographical areas, special codes are sometimes assigned to individual addresses or to institutions that receive large volumes of mail, such as government agencies and large commercial companies. One example is the French CEDEX system.\nTerms.\nThere are a number of synonyms for postal code; some are country-specific:\nHistory.\nThe development of postal codes happened first in large cities. Postal codes began with postal district numbers (or postal zone numbers) within large cities. London was first subdivided into 10 districts in 1857 (EC (East Central), WC (West Central), N, NE, E, SE, S, SW, W, and NW), four were created to cover Liverpool in 1864; and Manchester/Salford was split into eight numbered districts in 1867/68. By World War I, such postal district or zone numbers also existed in various large European cities. They existed in the United States at least as early as the 1920s, possibly implemented at the local post office level only (for example, instances of \"Boston 9, Mass\" in 1920 are attested) although they were evidently not used throughout all major US cities (implemented USPOD-wide) until World War II.\nBy 1930 or earlier, the idea of extending the postal district or zone numbering plans beyond large cities to cover even small towns and rural locales had started. These developed into postal codes as they are defined today. The name of US postal codes, \"ZIP Codes\", reflects this evolutionary growth from a zone plan to a zone improvement plan, \"ZIP\". Modern postal codes were first introduced in the Ukrainian Soviet Socialist Republic in December 1932, but the system was abandoned in 1939. The next country to introduce postal codes was Germany in 1941, followed by Singapore in 1950, Argentina in 1958, the United States in 1963 and Switzerland in 1964. The United Kingdom began introducing its current system in Norwich in 1959, but it was not used nationwide until 1974.\nPresentation.\nCharacter sets.\nThe characters used in postal codes are:\nReserved and Excluded characters.\nPostal codes in the Netherlands originally did not use the letters 'F', 'I', 'O', 'Q', 'U' and 'Y' for technical reasons. But as almost all existing combinations are now used, these letters were allowed for new locations starting 2005. The letter combinations \"SS\" (), \"SD\" (), and \"SA\" () are not used, due to links with the Nazi occupation in World War II.\nPostal codes in Canada do not include the letters D, F, I, O, Q, or U, as the optical character recognition (OCR) equipment used in automated sorting could easily confuse them with other letters and digits. The letters W and Z are used, but are not currently used as the first letter. The Canadian Postal Codes use alternate letters and numbers (with a space after the third character), formatted ANA NAN.\nIn Ireland, the eircode system uses the following letters only: A, C, D, E, F, H, K, N, P, R, T, V, W, X, Y. This serves to avoid confusion in OCR, and to avoid accidental double-entendres by avoiding the creation of word lookalikes, as Eircode's last four characters are random.\nAlphanumeric postal codes.\nMost of the postal code systems are numeric; only a few are alphanumeric (i.e., use both letters and digits). Alphanumeric systems can, given the same number of characters, encode many more locations. For example, while a two digit numeric code can represent 100 locations, a two character alphanumeric code using ten digits and twenty letters can represent 900 locations.\nThe independent nations using alphanumeric postal code systems are:\nCountries which prefix their postal codes with a fixed group of letters, indicating a country code, include Andorra, Azerbaijan, Barbados, Ecuador and Saint Vincent and the Grenadines.\nCountry code prefixes.\nISO 3166-1 alpha-2 country codes were recommended by the European Committee for Standardization as well as the Universal Postal Union to be used in conjunction with postal codes starting in 1994, but they have not become widely used. Andorra, Azerbaijan, Barbados, Ecuador, Latvia and Saint Vincent and the Grenadines use the ISO 3166-1 alpha-2 as a prefix in their postal codes.\nIn some countries (such as in continental Europe, where a numeric postcode format of four or five digits is commonly used) the numeric postal code is sometimes prefixed with a country code when sending international mail to that country.\nPlacement of the code.\nPostal services have their own formats and placement rules for postal codes. In most English-speaking countries, the postal code forms the last item of the address, following the city or town name, whereas in most continental European countries it precedes the name of the city or town. When it follows the city, it may be on the same line or on a new line.\nIn Japan, it is written at the start of the address when written in Japanese, but at the end when the address is written in the Latin alphabet.\nGeographic coverage.\nPostal codes are usually assigned to geographical areas. Sometimes codes are assigned to individual addresses or to institutions that receive large volumes of mail, e.g. government agencies or large commercial companies. One example is the French Cedex system.\nPostal zone numbers.\nBefore postal codes as described here were used, large cities were often divided into postal zones or postal districts, usually numbered from 1 upwards within each city. The newer postal code systems often incorporate the old zone numbers, as with London postal district numbers, for example. Ireland still uses postal district numbers in Dublin. In New Zealand, Auckland, Wellington and Christchurch were divided into postal zones, but these fell into disuse, and have now become redundant as a result of a new postcode system being introduced.\nCodes defined along administrative borders.\nSome postal code systems, like those of Ecuador and Costa Rica, show an exact agreement with the hierarchy of administrative divisions.\nFormat of six digit numeric (eight digit alphanumeric) postal codes in Ecuador, introduced in December 2007: ECAABBCC\n EC \u2013 ISO 3166-1 alpha-2 country code\n AA \u2013 one of the 24 provinces of Ecuador\n BB \u2013 one of the 226 cantons of Ecuador\n CC \u2013 one of the parishes of Ecuador.\nFormat of five digit numeric Postal codes in Costa Rica, introduced in 2007: ABBCC\n A \u2013 one of the seven provinces of Costa Rica\n BB \u2013 one of the 81 cantons of Costa Rica\n CC \u2013 one of the districts of Costa Rica.\nIn Costa Rica these codes were originally used as district identifiers by the National Institute of Statistics and Census of Costa Rica and the Administrative Territorial Division, and continue to be equivalent.\nThe first two digits of the postal codes in Turkey correspond to the provinces and each province has assigned only one number. They are the same for them as in .\nThe first two digits of the postal codes in Vietnam indicate a province. Some provinces have one, other have several two digit numbers assigned. The numbers differ from the number used in .\nCodes defined close to administrative boundaries.\nIn France the numeric code for the departments is used as the first two digits of the postal code, except for the two departments in Corsica that have codes 2A and 2B and use 20 as postal code. Furthermore, the codes are only the codes for the department in charge of delivery of the post, so it can be that a location in one department has a postal code starting with the number of a neighbouring department.\nCodes defined indirectly to administrative borders.\nThe first digit of the postal codes in the United States comprises discrete states. From the first three digits one can infer the state, with a few exceptions where an area is served by a central office in an adjacent state.\nSimilarly, in Canada, the first letter indicates the province or territory, although the provinces of Quebec and Ontario are divided into several lettered sub-regions (e.g. H for Montreal and Laval), and the Northwest Territories and Nunavut share the letter X.\nCodes defined independently from administrative areas.\nThe first two digits of the postal codes in Germany define areas independently of administrative regions. The coding space of the first digit is fully used (0\u20139); that of the first two combined is utilized to 89%, i.e. there are 89 postal zones defined. Zone 11 is non-geographic.\nRoyal Mail designed the postal codes in the United Kingdom mostly for efficient distribution. Nevertheless, people associated codes with certain areas, leading to some people wanting or not wanting to have a certain code. See also postcode lottery.\nIn Brazil the are an evolution of the five-digit area postal codes. In the 1990s the Brazilian five-digit postal code (illustrated), codice_1, received a three-digit suffix codice_2, but this suffix is not directly related to the administrative district hierarchy. The suffix was created only for logistic reasons.\nThe postal code assignment can be assigned to individual land lots in some special cases \u2013 in Brazil, they are named \"large receivers\" and receive suffixes 900\u2013959. It is an error to associate the postal code with the whole land lot area (illustrated). A postal code is often related to a land lot, but postal codes are usually related to access points on streets. Small or middle-sized houses, in general, only have a single main gate, which is the delivery point. Parks, large businesses such as shopping centers and big houses, may have more than one entrance and more than one delivery point.\nPrecision.\nCzechoslovakia.\nCzechoslovakia introduced Postal Routing Numbers (PS\u010c \u2013 po\u0161tovn\u00ed sm\u011brovac\u00ed \u010d\u00edsla) in 1973. The code consists of 5 digits formatted into two groups: NNN NN. Originally, the first group marked a district transport centre, the second group represented the order of post offices on the collection route. In the first group, the first digit corresponds partly with the region, the second digit meant a collection transport node (sb\u011brn\u00fd p\u0159epravn\u00ed uzel, SPU) and the third digit a \"district transport node\" (okresn\u00ed p\u0159epravn\u00ed uzel). However, processing was later centralized and mechanized while codes remained the same. After separation, Slovakia and the Czech Republic kept the system. Codes with an initial digit of 1, 2, 3, 4, 5, 6, or 7 are used in the Czech Republic, while codes with an initial digit of 8, 9, or 0 are used in Slovakia.\nA code corresponds to a local postal office. However, some larger companies or organizations have their own post codes. In 2004\u20132006, there were some efforts in Slovakia to reform the system, to get separate post codes for every district of single postmen, but the change was not realized.\nIndia.\nPostal codes are known as Postal Index Numbers (PINs; sometimes as PIN codes) in India. The PIN system was introduced on 15 August 1972 by India Post. India uses a unique six-digit code as a geographical number to identify locations in India. The format of the PIN is ZSDPPP defined as follows:\n Z \u2013 Zone\n S \u2013 Sub-zone\n D \u2013 Sorting District\n P \u2013 Service Route\n PP \u2013 Post Office\nThe first digit represents nine total zones: eight regional and one functional.\nIreland.\nIn Ireland, the new postal code system launched in 2015, known as Eircode provides a unique 7-character alphanumerical code for each individual address. The first three digits are the routing key, which is a postal district and the last four characters are a unique identifier that relates to an individual address (business, house or apartment). A fully developed API is also available for integrating the Eircode database into business databases and logistics systems.\nWith a single exception, these codes are in the format:\nANN XXXX\nThe single exception is the Dublin D6W postal district. It is the only routing key area in the country that takes the format ANA instead of ANN:\nD6W XXXX\nWhile it is not intended to replace addresses, in theory simply providing a seven-character Eircode would locate any Irish delivery address. For example, the Irish Parliament D\u00e1il \u00c9ireann is: D02 A272\nNetherlands.\nPostal codes in the Netherlands, known as postcodes, are alphanumeric, consisting of four digits followed by a space and two letters (NNNN AA). Adding the house number to the postcode will identify the address, making the street name and town name redundant. For example: 2597 GV 75 will direct a postal delivery to Theo Mann-Bouwmeesterlaan 75, 's-Gravenhage (the International School of The Hague).\nSingapore.\nSince 1 September 1995, every building in Singapore has been given a unique, six-digit postal code.\nUnited Kingdom.\nFor domestic properties, an individual postcode may cover up to 100 properties in contiguous proximity (e.g. a short section of a populous road, or a group of less populous neighbouring roads). The postcode together with the number or name of a property is not always unique, particularly in rural areas. For example, GL20 8NX/1 might refer to either 1 Frampton Cottages or 1 Frampton Farm Cottages, roughly a quarter of a mile (400 metres) apart.\nThe structure is alphanumeric, with the following six valid formats, as defined by BS 7666:\n AN NAA\n ANA NAA\n ANN NAA\n AAN NAA\n AANA NAA\n AANN NAA\nThere are always two halves: the separation between outward and inward postcodes is indicated by one space.\nThe outward postcode covers a unique area and has two parts which may in total be two, three or four characters in length. A postcode area of one or two letters, followed by one or two digits, followed in some parts of London by a letter.\nThe outward postcode and the leading numeric of the inward postcode in combination forms a postal sector, and this usually corresponds to a couple of thousand properties.\nLarger businesses and isolated properties such as farms may have a unique postcode. Extremely large organisations such as larger government offices or bank headquarters may have multiple postcodes for different departments.\nThere are 121 postcode areas in the UK, ranging widely in size from BT which covers the whole of Northern Ireland to WC for a small part of Central London. Postcode areas occasionally cross national boundaries, such as SY which covers a large, predominantly rural area from Shrewsbury and Ludlow in Shropshire, England, through to the seaside town of Aberystwyth, Ceredigion on Wales' west coast. There are a number of special purpose postcode areas that are \"non-geographic\" and which provide special routing instructions (such as parcel returns to online retailers). The three Crown dependencies and Gibraltar also use UK formatted postcodes. Some British Overseas Territories have adopted a single postcode for their territory that is very similar to the UK format.\nUnited States.\nIn the United States, the basic ZIP Code is composed of five digits. The first three digits identify a specific sectional center facility\u2014or central sorting facility\u2014that serves a geographic region (typically a large part of a state). The next two digits identify a specific post office either serving an area of a city (if in an urban area or large suburban area) or an entire village, town, or small city and its surrounding area (if in a small suburban or rural area).\nThere is an extended format of the ZIP Code known as the ZIP+4, which contains the basic five-digit ZIP Code, followed by a hyphen and four additional digits. These digits identify a specific delivery route, such as one side of a building, a group of apartments, or several floors of a large office building. Although using the ZIP+4 offers higher accuracy, addressing redundancy, and sorting efficiency within the USPS, it is optional and not widely used by the general public. It is primarily only used by business mailers.\nFor high volume business mailers using automated mailing machines, the USPS has promulgated the Intelligent Mail barcode standard, which is a barcode containing the ZIP+4 code plus a two digit delivery point. This 11-digit number is theoretically a unique identifier for every address in the country.\nStates and overseas territories sharing a postal code system.\nFrench overseas departments and territories use the five-digit French postal code system, each code starting with the three-digit department identifier. Monaco is also integrated in the French system and has no system of its own.\nThe British Crown Dependencies of Guernsey, Jersey and the Isle of Man are part of the UK postcode system. They use the schemes AAN NAA and AANN NAA, in which the first two letters are a unique code (GY, JE and IM respectively). Most of the Overseas Territories have UK-style postcodes, with a single postcode for each territory or dependency, although they are still treated as international destinations by Royal Mail in the UK, and charged at international rather than UK inland rates. The four other Overseas Territories Anguilla, Bermuda, British Virgin Islands and Cayman Islands have their own separate systems and formats.\nThe Pacific island states of Palau, Marshall Islands and the Federated States of Micronesia remain part of the US ZIP code system, despite having become independent states.\nSan Marino and the Vatican City are part of the Italian postcode system, while Liechtenstein similarly uses the Swiss system, as do the Italian exclave of Campione d'Italia and the German exclave of B\u00fcsingen am Hochrhein, although they also form part of their respective countries' postal code systems.\nThe Czech Republic and Slovakia still use the codes of the former Czechoslovakia, their ranges not overlapping. In 2004\u20132006, Slovakia prepared a reform of the system but the plan was postponed and may have been abandoned. In the Czech Republic, there was no significant effort to modify the system.\nNon-geographic codes.\nIn the United Kingdom, the non-conforming postal code GIR 0AA was used for the National Girobank until its closure in 2003. A non-geographic series of postcodes, starting with BX, is used by some banks and government departments.\nHM Revenue and Customs \u2013 VAT Controller\nVAT Central Unit\nBX5 5AT\nThe XX postcode is used for parcel returns. The BF postcode is used for British Forces Post Office (BFPO) addresses.\nA fictional address is also used by Royal Mail for letters to Santa Claus, more commonly known as Santa or Father Christmas:\nSanta's Grotto\nReindeerland XM4 5HQ\nPreviously, the postcode SAN TA1 was used.\nIn Finland, the special postal code 96930 is for Korvatunturi, the place where Santa Claus ( in Finnish) is said to live, although mail is delivered to the Santa Claus Village in Rovaniemi. The special postal code 99999 was formerly used.\nIn Canada, the amount of mail sent to Santa Claus increased every Christmas, up to the point that Canada Post decided to start an official Santa Claus letter-response program in 1983. Approximately one million letters come in to Santa Claus each Christmas, including from outside of Canada, and all of them are answered in the same languages in which they are written. Canada Post introduced a special address for mail to Santa Claus, complete with its own postal code:\nSANTA CLAUS\nNORTH POLE \u00a0H0H 0H0\nIn Belgium bpost sends a small present to children who have written a letter to Sinterklaas. They can use the non-geographic postal code 0612, which refers to the date Sinterklaas is celebrated (6 December), although a fictional town, street and house number are also used. In Dutch, the address is\nSinterklaas\nSpanjestraat 1\n0612 Hemel\nThis translates as \"1 Spain Street, 0612 Heaven\". In French, the street is called \"Paradise Street\":\nSaint-Nicolas\nRue du Paradis 1\n0612 Ciel\nNon-postal uses.\nWhile postal codes were introduced to expedite the delivery of mail, they can be used for:\nAvailability.\nIn some countries, the postal authorities charge for access to the code database. As of \u00a02010[ [update]], the United Kingdom Government is consulting on whether to waive licensing fees for some geographical data sets (to be determined) related to UK postcodes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51550", "revid": "2766075", "url": "https://en.wikipedia.org/wiki?curid=51550", "title": "ZIP Code", "text": "Numeric postal code used in the US and its territories\nThe ZIP Code system (an acronym for Zone Improvement Plan) is the system of postal codes used by the United States Postal Service (USPS). The term \"ZIP\" was chosen to suggest that the mail travels more efficiently and quickly (zipping along) when senders include the code in the postal address. \"ZIP+4\" is a registered trademark of the United States Postal Service, which also registered \"ZIP Code\" as a service mark until 1997, and which claims \"ZIP Code\" as a trademark though it is not registered.\nIntroduced on July 1, 1963, the basic format was five digits, the first designating a region of the country and subsequent digits localizing the destination further. In 1983, an extended code was introduced named \"ZIP+4\"; it included the five digits of the ZIP Code, followed by a hyphen and four digits that designated a location even more specific than the original five. \nPrivate carriers and the USPS use ZIP Codes to route deliveries. In addition, ZIP Codes have become a basis for breaking down demographic, marketing, and sales data for analytical purposes.\nHistory.\nEarly postal zones.\nThe early history and context of postal codes began with postal district/zone numbers. The United States Post Office Department (USPOD) implemented postal zones for 124 large cities in May 1943. Postmaster General Frank C. Walker explained that many experienced postal clerks were going into the army, and the zone system would enable inexperienced clerks to sort mail without having to learn the delivery area of each city carrier.\nFor example:\n&lt;poem&gt;\nMr. John Smith\n3256 Epiphenomenal Avenue\nMinneapolis \"16\", Minnesota\n&lt;/poem&gt;\nThe \"16\" is the number of the postal zone in a specific city.\nEstablishment.\nBy the early 1960s, a more organized system was needed. Non-mandatory five-digit ZIP Codes were introduced nationwide on July 1, 1963. The USPOD issued its \"Publication 59: Abbreviations for Use with ZIP Code\" on October 1, 1963, with the list of two-letter state abbreviations which are generally written with both letters capitalized. An earlier list, publicized in June 1963, had proposed capitalized abbreviations ranging from two to five letters. According to \"Publication 59\", the two-letter standard was \"based on a maximum 23-position line, because this has\nbeen found to be the most universally acceptable line capacity basis for major addressing systems\", which would be exceeded by a long city name combined with a multi-letter state abbreviation, such as \"Sacramento, Calif.\" along with the ZIP Code. The abbreviations have remained unchanged, except for Nebraska, which was changed from NB to NE in 1969 at the request of the Canada Post Corporation, to avoid confusion with New Brunswick.\nRobert Moon is considered the father of the ZIP Code; he submitted his proposal in 1944 while working as a postal inspector. \nThe phrase \"zone improvement plan\" is credited to D. Jamison Cain, a Postal Service executive. The post office credits Moon with only the first three digits of the ZIP Code, which describe the sectional center facility (SCF) or \"sec center\". An SCF is a central mail processing facility with those three digits. The fourth and fifth digits, which give a more precise locale within the SCF, were proposed by Henry Bentley Hahn Sr.\nThe SCF sorts mail to all post offices with those first three digits in their ZIP Codes. The mail is sorted according to the final two digits of the ZIP Code and sent to the corresponding post offices in the early morning. Sectional centers do not deliver mail and are not open to the public, although the building may include a post office that is open to the public, and most of their employees work the night shift. Items of mail picked up at post offices are sent to their SCFs in the afternoon, where the mail is sorted overnight. In the case of large cities, the last two digits as assigned generally coincided with the older postal zone number.\nFor example:\n&lt;poem&gt;\nMr. John Smith\n3256 Epiphenomenal Avenue\nMinneapolis, MN554\"16\"\n&lt;/poem&gt;\nIn 1967, these became mandatory for second- and third-class bulk mailers, and the system was soon adopted generally. The United States Post Office used a cartoon character, which it called Mr. ZIP, to promote the use of the ZIP Code. The name \"Mr. ZIP\" was coined by D. Jamison Cain. Mr. ZIP was often depicted with a legend such as \"USE ZIP CODE\" in the selvage of panes of postage stamps or on the covers of booklet panes of stamps. Mr. ZIP was featured prominently alongside musical group \"The Swingin' Six\" in a variety show that the post office used to explain the importance of using ZIP Codes.\nZIP+4.\nIn 1983, the U.S. Postal Service introduced an expanded ZIP Code system that it named \"ZIP+4\", often known as \"plus-four codes\", \"add-on codes\", or \"add-ons\". A ZIP+4 Code uses the basic five-digit code plus four additional digits to identify a geographic segment within the five-digit delivery area, such as a city block, a group of apartments, an individual high-volume receiver of mail, a post office box, or any other unit that could use an extra identifier to aid in efficient mail sorting and delivery. However, the new format was not adopted universally by the public.\nCommercial customers generally apply a ZIP+4 or a delivery point code (i.e., ZIP+6) to mail as part of address normalization. They may need to do so to receive discounted postage rates. The public does not need to write the ZIP+4 code, as mail is read by a multiline optical character reader (MLOCR) that almost instantly determines the correct ZIP+4 Code from the address\u2014along with the even more specific delivery point\u2014and sprays an Intelligent Mail barcode (IMb) on the face of the mail piece that corresponds to 11 digits\u2014nine for the ZIP+4 Code and two for the delivery point.\nFor post office boxes, the general but not invariable rule is that each box has its own ZIP+4 Code. The add-on code is often one of the following: the last four digits of the box number (e.g. PO Box 107050, Albany, NY 12201-7050), zero plus the last three digits of the box number (e.g., PO Box 17727, Eagle River, AK 99577-0727), or, if the box number consists of fewer than four digits, enough zeros are attached to the front of the box number to produce a four-digit number (e.g., PO Box 77, Juneau, AK 99750-0077). However, there is no uniform rule, so the ZIP+4 Code must be looked up individually for each box (e.g. using the USPS's official ZIP Code Lookup tool, and being sure to enter just city and state, not the 5-digit ZIP).\nPostal barcode.\nThe ZIP Code is often translated into an Intelligent Mail barcode printed on the mailpiece to make it easier for automated machines to sort. A barcode can be printed by the sender (some word-processing programs such as WordPerfect include the feature), but this is not recommended, as the address-to-ZIP lookup tables can be significantly out of date.\nCustomers who send bulk mail can get a discount on postage if they have printed the barcode and have presorted the mail. This requires more than just a simple font; mailing lists must be standardized with up-to-date Coding Accuracy Support System (CASS)-certified software that adds and verifies a full, correct ZIP+4 Code and an additional two digits representing the exact delivery point. Furthermore, mail must be sorted in a specific manner to an 11-digit code with at least 150 mailpieces for each qualifying ZIP Code. It must be accompanied by documentation confirming this. These steps are usually done with PAVE-certified software that prints the barcoded address labels and the barcoded sack or tray tags.\nThe assignment of delivery point digits (the 10th and 11th digits) ensures that every mailable point in the country has an 11-digit number. The delivery-point digits are calculated based on the primary or secondary number of the address. The USPS publishes the rules for calculating the delivery point in a document called the CASS Technical Guide.\nStructure and allocation.\nZIP Codes designate delivery points within the United States (and its territories). \nTypes.\nThere are four types of ZIP Codes:\nUnique ZIP Codes are used for governmental agencies, universities, businesses, prisons, or buildings receiving sufficiently high volumes of mail to justify the assignment to them of exclusive ZIP Codes. Government examples include 20505 for the Central Intelligence Agency in Washington, D.C., and 81009 for the Federal Citizen Information Center of the U.S. General Services Administration (GSA) in Pueblo, Colorado. An example of a university-specific ZIP Code is 21252, which serves Towson University. An example of a unique ZIP Code assigned to a prison is 81290 for the Federal Correctional Complex near Florence, Colorado. An example of a private address with a unique ZIP Code is that assigned to the headquarters of Walmart (72716). They may also be assigned to a single individual, such as Smokey Bear \"20252\", or a program, such as the Postal Service's Operation Santa Claus program, under which children are invited to write to Santa Claus at \"North Pole 88888\".\nAn example of a PO box-only ZIP Code is 22313, used for boxes at the main post office in Alexandria, Virginia, including those used by the United States Patent and Trademark Office. In the area surrounding that post office, home and business mail delivery addresses use ZIP Code 22314, a standard ZIP Code.\nGeographic hierarchy.\nPrimary state prefixes.\nZIP Codes are numbered with the first digit representing a certain group of U.S. states, the second and third digits together representing a region in that group (or perhaps a large city), and the fourth and fifth digits representing a group of delivery addresses within that region. The main town in a region (if applicable) often gets the first ZIP Codes for that region; afterward, the numerical order often follows the alphabetical order. Because ZIP Codes are intended for efficient postal delivery, there are unusual cases where a ZIP Code crosses state boundaries, such as a military facility spanning multiple states or remote areas of one state most easily serviced from a bordering state. For example, ZIP Code 42223 serves Fort Campbell, which spans Christian County, Kentucky, and Montgomery County, Tennessee, and ZIP Code 97635 includes portions of Lake County, Oregon, and Modoc County, California.\nThe first three digits generally designate a sectional center facility, the area's mail sorting and distribution center. A sectional center facility may have more than one three-digit code assigned to it. For example, the Northern Virginia sectional center facility in Merrifield is assigned codes 220, 221, 222, and 223. In some cases, a sectional center facility may serve an area in an adjacent state, usually due to the lack of a proper location for a center in that region. For example, 739 in Oklahoma is assigned to Amarillo, Texas; 297 in South Carolina is assigned to Charlotte, North Carolina; 865 in Arizona is assigned to Albuquerque, New Mexico; and 961 in California to Reno, Nevada.\nMany of the lowest ZIP Codes, which begin with\u00a0'0', are in the New England region. In the '0' region are New Jersey (non-contiguous with the remainder of the '0' area), Puerto Rico, the U.S. Virgin Islands, and APO/FPO military addresses for personnel stationed in Europe, Africa, Southwest Asia, and onboard vessels based in the waters adjoining those lands. The lowest ZIP Code is in Holtsville, New York (00501, a ZIP Code exclusively for the U.S. Internal Revenue Service center there). Other low ZIP Codes are 00601 for Adjuntas, Puerto Rico; 01001 for Agawam, Massachusetts, and the ZIP Codes 01002 and 01003 for Amherst, Massachusetts; 01002 is used for mail in town, while the University of Massachusetts Amherst primarily uses 01003. Until 2001, there were six ZIP Codes lower than 00501 that were numbered from 00210 to 00215 (located in Portsmouth, New Hampshire) and were used by the Diversity Immigrant Visa program to receive applications from non-U.S. citizens.\nThe numbers increase southward along the East Coast, such as 02115 (Boston), 10001 (New York City), 19103 (Philadelphia), 21201 (Baltimore), 20008 (Washington, D.C.), 30303 (Atlanta), and 33130 (Miami) (these are only examples, as each of these cities contains several ZIP Codes in the same range). From there, the numbers increase heading westward and northward east of the Mississippi River, southward west of the Mississippi River, and northward on the West Coast. For example, 40202 is in Louisville, 50309 in Des Moines, 60601 in Chicago, 63101 in St. Louis, 77036 in Houston, 80202 in Denver, 94111 in San Francisco, 98101 in Seattle, and 99950 in Ketchikan, Alaska (the highest ZIP Code).\nThe first digit of the ZIP Code is allocated as follows:\nSecondary regional prefixes (123xx) and local ZIP Codes (12345).\nThe second and third digits represent the sectional center facility (SCF) (e.g., 477xx = Vanderburgh County, Indiana). The fourth and fifth digits represent the area of the city (if in a metropolitan area), or a village/town (outside metro areas), e.g., 47722 (4=Indiana, 77=Vanderburgh County, 22=University of Evansville area). When a sectional center facility's area crosses state lines, it is assigned separate three-digit prefixes for the states it serves.\nIn some urban areas, like 462 for Marion County, Indiana, the three-digit prefix will often exist in one county, while, in rural and most suburban areas, the prefix will exist in multiple counties; for example, the neighboring 476 prefix is found in part or entirely in six counties: Gibson, Pike, Posey, Spencer, Vanderburgh, and Warrick. In some cases, an urban county may have more than one prefix. This is the case with Allen (467, 468), Lake (464, 463), St. Joseph (465, 466), and Vanderburgh (476, 477) counties. Cities like Chicago, Houston, Los Angeles, and New York City have multiple prefixes within their city limits. In some cases, these may be served from the same SCF, such as in San Diego County, California, where the three-digit prefixes 919 and 920 are used for suburban and rural communities, and 921 for the city of San Diego itself, although all three are processed through the same SCF.\nDespite the geographic derivation of most ZIP Codes, the codes do not represent geographic regions; generally, they correspond to address groups or delivery routes. Consequently, ZIP Code \"areas\" can overlap, be subsets of each other, or be artificial constructs with no geographic area (such as 095 for mail to the Navy, which is not geographically fixed). Similarly, in areas without regular postal routes (rural route areas) or no mail delivery (undeveloped areas), ZIP Codes are not assigned or are based on sparse delivery routes, and hence the boundary between ZIP Code areas is undefined. For example, some residents in or near Haubstadt, Indiana, which has the ZIP Code 47639, have mailing addresses with 47648, the ZIP Code for neighboring Fort Branch, Indiana, while others living in or near Fort Branch have addresses with 47639. Many rural counties have similar logistical inconsistencies caused by the sparse delivery routes, often called Rural Routes or other similar designations.\nInternational mail.\nThere are generally no ZIP Codes for deliveries to other countries, except for the independent countries of the Federated States of Micronesia, the Republic of the Marshall Islands, and the Republic of Palau, each of which is integrated into the U.S. postal system under a Compact of Free Association. Another exception is ZIP Codes used for overseas stations of U.S. armed forces.\nMail to U.S. diplomatic missions overseas is addressed as if it were addressed to a street address in Washington, D.C. The four-digit diplomatic pouch number is used as a building number, while the city in which the embassy or consulate is located is combined with the word \"Place\" to form a fictional street name. Each mission uses a ZIP+4 Code consisting of 20521 and the diplomatic pouch number.\nFor example, the U.S. Embassy in India has this address in India's postal system:\n&lt;poem&gt;\nEmbassy of the United States of America\nShantipath, Chanakyapuri,\nNew Delhi,\nNational Capital Territory of Delhi, 110021\n&lt;/poem&gt;\nas well as this U.S. address:\n&lt;poem&gt;\nEmbassy of the United States of America\n9000 New Delhi Place\nWashington, DC20521-9000\n&lt;/poem&gt;\nIndividuals posted at diplomatic missions overseas are now assigned a Diplomatic Post Office address and a unique box number. The ZIP Code identifies the diplomatic mission destination and differs from the diplomatic pouch number in the example above. While delivered through the pouch system, mail to such addresses is not considered \"Diplomatic Pouch\" material and must adhere to the mailing regulations of the host country. An example address is:\n&lt;poem&gt;\nJOHN ADAMS\nUNIT 8400 BOX 0000\nDPO AE09498-0048\n&lt;/poem&gt;\nDivision and reallocation.\nLike area codes, ZIP Codes are sometimes divided and changed, especially when a rural area becomes suburban. Typically, the new codes become effective once announced, and a grace period (e.g., six months) is provided in which the new and old codes are used concurrently so that postal patrons in the affected area can notify correspondents, order new stationery, etc.\nOpening a new sectional center facility is sometimes necessary in rapidly growing communities, which must then be allocated three-digit ZIP-code prefixes. Such allocation can be done in various ways. For example, when a new sectional center facility was opened at Dulles Airport in Virginia, the prefix 201 was allocated to that facility; therefore, for all post offices to be served by that sectional center facility the ZIP Code changed from an old code beginning with 220 or 221 to a new code or codes starting with 201. However, no new prefix was assigned when a new sectional center facility was opened to serve Montgomery County, Maryland. Instead, ZIP Codes in the 207 and 208 ranges, which had previously been assigned alphabetically, were reshuffled so that 207xx ZIP Codes in the county were changed to 208xx codes, while 208xx codes outside that county were changed to 207xx codes. Because Silver Spring (whose postal area includes Wheaton) has its own prefix, 209, there was no need to apply the reshuffling to Silver Spring; instead, all mail going to 209xx ZIP Codes was simply rerouted to the new sectional center facility.\nOn the other hand, depopulation may cause a post office to close and its associated ZIP Code to be deallocated. For example, Centralia, Pennsylvania's ZIP Code, 17927, was retired in 2002, and ZIP Codes for Onoville (14764), Quaker Bridge (14771) and Red House (14773) in New York were prevented from going into use in 1964 in preparation for the Kinzua Dam's completion.\nRelationship with local government boundaries.\nEach ZIP Code has one or more \"postal city\" names assigned to it. Since a ZIP Code is a collection of delivery points served by a specific physical post office, ZIP Codes often do not coincide with the boundaries of local government units. For example, suburban and unincorporated areas may share a postal city name with a neighboring municipality, even if no part of its ZIP Code is actually within that city.\nOther uses.\nDelivery services.\nDelivery services other than the USPS, such as FedEx, United Parcel Service, and DHL, require a ZIP Code for optimal internal routing of a package.\nStatistics.\nAs of \u00a014, 2025[ [update]], there are 41,557 ZIP Codes in the United States. Due to convenience, ZIP Codes are used not only for tracking of mail, but also commonly for gathering geographical statistics in the United States by some researchers. ZIP Codes are not created for statistical analysis, and thus their use for statistical analysis is heavily criticized for numerous reasons and advised against as a cartographic practice. As ZIP Codes are not polygons, but collections of mail routes and points, they are unsuitable for publication or distribution of most data. Polygons for ZIP Codes are not released by the USPS and instead interpolated by 3rd party vendors. These interpolations introduce topological errors and are not standard between vendors. The USPS often discontinues, splits, or otherwise modifies ZIP Codes, making continuous space-time analysis challenging, leading to issues with both the modifiable areal unit problem (MAUP) and modifiable temporal unit problem (MTUP). As the ZIP Codes are postal routing numbers, individuals and organizations without concrete spatial locations may be given a number, making it impossible to associate demographic data with them. Demographic data is inconsistent between ZIP Codes, and no effort is made to ensure they are proper enumeration units for analysis. As ZIP Codes are not made with the same considerations as other enumeration units, and is not possible without committing the ecological fallacy. This again becomes an issue with the MAUP. They have been found not to have significant correlations with health indicators, which can lead to poor conclusions. Despite these issues, ZIP Codes remain popular among researchers in fields such as public health due to their convenience, public familiarity with them, ability to anonymize subject addresses through aggregation, and possible ignorance of more appropriate enumeration units on the part of researchers.\nIn an attempt to satisfy demand \"by data users for statistical data by ZIP Code area\", the U.S. Census Bureau calculates approximate boundaries of ZIP Code areas, which it calls ZIP Code Tabulation Areas (ZCTAs). Statistical census data is then provided for these approximate areas. The geographic data provided for these areas includes the latitude and longitude of the center-point of the ZCTAs. ZIP Codes are inherently discrete or \"point-based\" data, as they are assigned only at the point of delivery, not for the spaces between the delivery points. The United States Census Bureau then interpolates this discrete data set to create polygons by attempting to match ZIP Code extents with Census blocks. The resulting aereal units represent the approximate extent of the ZIP Code, which are combined to use for mapping and data presentation. The process of creating ZCTAs and their use for statistical analysis is heavily criticized in the literature. First, the creation of ZCTAs from Census blocks encounters issues when a Census block straddles multiple ZIP Codes. Addressing this is another instance of the MAUP, and the solution of dividing aggregate units between ZIP Codes causes some individuals to fall into ZCTAs that do not match their ZIP Code. The creation of these units is therefore committing the ecological fallacy by attempting to disaggregate aggregate data. As ZIP Codes are not continuous, not everyone in the United States has one; there are ZIP Codes for non-populated or geographic areas, resulting in there not being one ZCTA for every ZIP Code. ZCTAs are not updated as frequently as the USPS updates ZIP Codes, resulting in further temporal analysis issues when ZIP Codes change during a study period. Datasets providing a similar approximate geographic extent to ZCTA are commercially available. Despite these issues, ZCTAs are still very popular with researchers in fields like epidemiology, and among government agencies, with some states employing them to publish and distribute public health data during the COVID-19 pandemic.\nMarketing.\nThe data is often used in direct mail marketing campaigns in ZIP-code marketing. Point-of-sale cashiers sometimes ask consumers for their home ZIP Codes. Besides providing purchasing-pattern data useful in determining the location of new business establishments, retailers can use directories to correlate this ZIP Code with the name on a credit card to obtain a consumer's full address and telephone number. ZIP-Coded data are also used in analyzing geographic risk factors, an insurance and banking industry practice pejoratively known as redlining. This can cause problems, \"e.g.\", expensive insurance, for people living near a town with a high crime rate and sharing its ZIP Code, while they live in a relatively crime-free town.\nCalifornia outlawed this practice in 2011.\nLegislative districts.\nZIP Codes may not currently be used to identify existing legislative districts. Although the website of the United States House of Representatives has a \"Find Your Representative\" feature that looks up congressional districts based on ZIP Codes alone, it often returns multiple districts corresponding to a single ZIP Code. This is because different parts of one ZIP Code can be in different districts. One proposal to eliminate the possibility of extreme partisan gerrymandering calls on using ZIP Codes as the basic units for redistricting.\nInternet.\nA 1978 proposal for a nationwide system of community networks suggested using ZIP Codes for routing.\nZIP Code data is an integral part of dealer / store locator software on many websites, especially brick-and-click websites. This software processes a user-input ZIP Code and returns a list of store or business locations, usually in the order of increasing distance from the center of the input ZIP Code. As the ZIP Code system is confined to the U.S. Postal network, websites that require ZIP Codes cannot register customers outside the United States. Many sites will purchase postal code data of other countries or make allowances in cases where the ZIP Code is not recognized.\nZIP Codes are regularly used on the Internet to provide a location where an exact address is not necessary (or desirable) but the user's municipality or general location is needed. Examples (in addition to the store locator example listed above) include weather forecasts, television listings, local news, and online dating (most general-purpose sites, by default, search within a specified radius of a given ZIP Code, based on other users' entered ZIP Codes).\nCredit card security.\nZIP Codes are used in credit card authorization, specifically Address Verification System (AVS). When a merchant collects the entire address, the ZIP Code is an important part of AVS. In some cases, the ZIP Code is the only thing used for AVS, specifically where collecting a signature or other information is infeasible, such as pay at the pump or vending machines.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51552", "revid": "46657093", "url": "https://en.wikipedia.org/wiki?curid=51552", "title": "Ollolai", "text": "A municipality and town in Sardinia, Italy\nOllolai is a \"comune\" at the centre of Barbagia, in the province of Nuoro, Sardinia, Italy. Its territory covers an area of . It is the main town of the Barbagia di Ollolai.\nPlaces of interest.\nArchitecture.\nThe main square of the village was created in the early 20th century by diverting the stream that crossed it. Inside it lies a church dedicated to St. Michael the Archangel, which contains paintings by Carmelo Floris in the apse as well as a crucifix painted by Franco Bussu, an inhabitant of Ollolai. The oldest part of the church is a chapel dedicated to St. Bartholomew.\nNear the town centre, there is another church dedicated to Anthony of Padua, where the Feast of Saint Anthony is traditionally held along with a lighting of a bonfire.\nA few kilometres (miles) from the village towards the valley, there lies a church dedicated to Saint Peter, rebuilt in the 1970s after the demolition of a Romanesque church.\nIn a valley surrounded by granite peaks lies a church dedicated to Basil of Caesarea, built by the Basilian monks and used, after the expulsion of the Basilians, by the Franciscans. The adjoining convent was built later.\nWater features.\nThe town is abundant in water features. The most famous is \"Gupunnio,\" the source of which is in the centre of the village, and was recently renamed \"Regina Fontium\". There is also the fountain of Su Sapunadorju, and a few hundred metres (feet) from the town, Su Puthu, which was once used as a watering hole.\nPopulation decline and 2024 U.S. election.\nSince 1925, Ollolai's population declined from 2,250 to 1,150 residents as families left for economic reasons. Following the 2024 United States presidential election, the town announced it would offer homes in need of renovation for one euro to Americans wishing to relocate to Ollolai. The town also offers free temporary homes to digital nomads and ready-to-occupy houses for up to (). Ollolai's mayor said the town received 38,000 requests in the weeks following the election.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51553", "revid": "271068", "url": "https://en.wikipedia.org/wiki?curid=51553", "title": "Emilio Lussu", "text": "Italian writer and politician (1890\u20131975)\nEmilio Lussu (4 December 1890 \u2013 5 March 1975) was a Sardinian and Italian writer, anti-fascist intellectual, military officer, partisan, and politician. He is also the author of the novel \"One Year on the High Plateau\". Lussu was elected multiple times to Parliament, serving as a member of the Constituent Assembly of Italy for the constituency of Cagliari and twice as a minister. He founded the Sardinian Action Party and co-founded the Justice and Freedom movement. As an anti-fascist, he was assaulted, wounded, and to Lipari in the Aeolian Islands by the Italian fascist regime as a direct decision of Benito Mussolini. After escaping, with Carlo Rosselli and Fausto Nitti, he spent about fourteen years as a refugee abroad. He served as an officer in World War I, where he received multiple decorations, and participated in the Spanish Civil War as a political leader and in the Italian Resistance.\nBiography.\nThe soldier.\nLussu was born in Armungia, province of Cagliari (Sardinia) and graduated with a degree in law in 1914. Lussu married Joyce Salvadori, a notable poet, and member of the noble Paleotti family of the Marche, who were counts of Fermo. Prior to the entry of Italy into World War I, Lussu joined the army and was involved in several skirmishes. As a complementary officer of the Brigade \"Sassari\" in 1916 he was stationed on the Asiago Plateau. The brigade had arrived on the plateau in May 1916 to help in the Italian effort to stop the Austrian Spring offensive. In the month of June 1916 the brigade conquered Monte Fior, Monte Castelgomberto, Monte Spil, Monte Miela and Monte Zebio. After the war Lussu published the novel \"One Year on the High Plateau\" (\"Un anno sull'altipiano\") about his experiences of trench warfare on the Asiago Plateau. The 1970 movie \"Many Wars Ago\" (\"Uomini contro\") by Francesco Rosi is based on this book.\nPolitics and exile.\nAfter the war Lussu, together with Camillo Bellieni, founded the \"Partidu Sardu\"-\"Partito Sardo d'Azione\" (Sardinian Action Party), that blended social-democratic values and Sardinian nationalism. The party took a formal position in 1921, opposing the increasing power of the Fascist movement. Lussu was elected to the Italian parliament in 1921 and, in 1924 was among the Aventine secessionists who withdrew from the Italian Parliament after the murder of Giacomo Matteotti.\nLussu's anti-Fascist position was, at the time, one of the most radical in Italy. Lussu was physically attacked and injured by unknown aggressors several times. In 1926, during one of these attacks (notably, the same day that Benito Mussolini suffered an attack in Bologna), Lussu shot one of the \"squadristi (italian blackshirts)\", in self-defense. He was arrested and tried, being found non-guilty due to the right to self defense, only to be later sentenced (due to fascist political interference) to 5 years of confinement on the island of Lipari, within the Aeolian Islands near the northern coast of Sicily.\nIn 1929 Lussu escaped from his confinement and reached Paris. There, together with Gaetano Salvemini, Carlo Rosselli, Riccardo Bauer, Ernesto Rossi and other anti-fascist refugees he founded \"Giustizia e Libert\u00e0\" (Justice and Freedom), an anti-Fascist movement that proposed revolutionary methods to upset the Italian Fascist Regime. While in exile, he came to be known as \"Mister Mills\".\nIn 1938, Lussu's novel \"One Year on the High Plateau\" (\"Un anno sull'altipiano\") was published in Paris. This thinly fictional account tells of the lives of soldiers during World War I and the trench warfare they encountered. One Year on the High Plateau underlines, with chill rationalism, how the irrationalities of warfare affected the common man. Gifted with a keen sense of observation and sharp logic, Lussu demonstrates how distant the real life of soldiers is from everyday activities. In a notable passage, he describes the silent terror in the moments preceding an attack, as he is forced to abandon the \"safe\" protective trench for an external unknown, risky, undefined world: \"All the machine-guns are waiting for us\".\nReturn to Italy.\nLussu took part in the civil war in Spain. Between 1941 and 1942 he was the protagonist of the most important \"episode\" of the collaboration between British Special Operations Executive and Italian antifascism in exile. He tried to get the clearance for an antifascist uprising in his home island of Sardinia, which the SOE supported at some stage but did not receive approval from the Foreign Office. He returned to Italy after the armistice of 1943 when joined the Italian Resistance and became the secretary of the Sardinian Action Party for southern Italy. He became the leader of the left wing of the party and later joined forces with the Italian Socialist Party (PSI). After World War II he served as the Minister for Post-War Assistance in the government of Ferruccio Parri and later as a minister without portfolio in Alcide De Gasperi's government.\nIn 1964 Lussu separated from the PSI, creating the Italian Socialist Party of Proletarian Unity (PSIUP). Ideological differences with the political line of \"Partito Sardo d'Azione\" deepened and Lussu left Sardinia.\nEmilio Lussu died in Rome in 1975.\nWorks.\nMany political meanings have been drawn from Lussu's works, but his works are perhaps more important at a personal level. Morally and philosophically, Lussu's books reflect his need to repent, having been previously an \"interventista\" (favourable to entering the war) and a revolutionary (in \"Giustizia e Libert\u00e0\"); his works soberly describe what war, in its cruellest moments, was like for him.\nThe alteration of Lussu's opinion of war is quite apparent in the range of his works: first an \"interventista\", then the author of a manual for revolution, soon afterwards the author of a pacifist book, then again a revolutionary and a volunteer in the Spanish civil war. Anyway, One Year on the High Plateau combines well the repulse of the war with the bravery of the fighter. Lussu's consistency has been questioned and politics often invades evaluations of his works.\nBibliography.\nIn the Florestano Vancini's film The Assassination of Matteotti (1973), Lussu is played by Giovanni Brusatori.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51554", "revid": "48898971", "url": "https://en.wikipedia.org/wiki?curid=51554", "title": "Riyadh", "text": "Capital and largest city of Saudi Arabia\nRiyadh is the capital and largest city of Saudi Arabia. It is also the capital of the Riyadh Province and the centre of the Riyadh Governorate. Located on the eastern bank of Wadi Hanifa, the current form of the metropolis largely emerged in the 1950s as an offshoot of the 18th century walled town following the dismantling of its defensive fortifications.\nIt is the largest city on the Arabian Peninsula, and is situated in the center of the Nafud desert, on the eastern part of the Najd plateau. The city sits at an average of above sea level, and receives around 5 million tourists each year, making it the forty-ninth most visited city in the world and the 6th in the Middle East. Riyadh had a population of 7.0 million people in 2022, making it the most-populous city in Saudi Arabia, 3rd most populous in the Middle East, and the 38th most populous in Asia.\nThe first mention of the city by the name \"Riyadh\" was in 1590, by an Arab chronicler. In 1745, Dahham ibn Dawwas, who was from the neighboring Manfuhah, seized control of the town. Dahham built a mudbrick palace and a wall around the town, and the best-known source of the name \"Riyadh\" is from this period, thought to be referring to the earlier oasis towns that predated the wall built by Ibn Dawwas. In 1744, Muhammad ibn Abd al-Wahhab formed an alliance with the Emir of Diriyah, Muhammad bin Saud, and they took Riyadh from Dahham. However their state, now known as the \"First Saudi state\", collapsed in 1818. Turki ibn Abdullah founded the \"Second Saudi state\" in the early 19th century and made Riyadh his capital in 1825. However, his reign over the city was disrupted by a joint Ottoman\u2013Rashidi alliance. Finally, in the early 20th century, Ibn Saud retrieved his ancestral rule in 1902 with the Emirate of Riyadh and consolidated his rule by 1926 with the final Saudi conquest of Hejaz, subsequently naming his kingdom 'Saudi Arabia' in September 1932 with Riyadh as the capital. The town was the administrative center of the government until 1938, when Ibn Saud moved to the Murabba Palace. In the 1950s, the walls were dismantled and Riyadh metropolis outgrew as an offshoot of the walled town.\nRiyadh is the political and administrative center of Saudi Arabia. The Consultative Assembly, the Council of Ministers, the king and the Supreme Judicial Council are all situated in the city. Alongside these four bodies that form the core of the legal system of Saudi Arabia, the headquarters of other major and minor governmental bodies are also located in Riyadh. Out of the 24 ministries of the Saudi government, 23 are headquartered in Riyadh, further reinforcing its status as the nation's administrative capital. The city hosts 114 foreign embassies, most of which are located in the Diplomatic Quarter in the western reaches of the city.\nRiyadh also holds economic significance, as it contains the headquarters of many banks and major companies, such as the Saudi National Bank, Alrajhi Bank, SABIC, Almarai, STC Group, and MBC Group, In addition to its strong local presence, Riyadh has also attracted major international investment. Global companies such as Lenovo, Google, Amazon, Samsung, and Philips have moved their regional headquarters to the city. In total, over 500 foreign companies have relocated their regional bases to Riyadh, reinforcing its growing status as a regional business hub, and Highway 65, known locally as the King Fahd Road, runs through some of these important centers in the city, including the King Abdullah Financial District, one of the world's largest financial districts, the Al-Faisaliah Tower and the Kingdom Center. Riyadh is one of the world's fastest-growing cities in population and is home to many expatriates.\nThe city is divided into fifteen municipal districts, which are overseen by the Municipality of Riyadh headed by the mayor; and the Royal Commission for Riyadh City, which is chaired by the governor of the province, Faisal bin Bandar. As of July 2020, the mayor is Faisal bin Abdulaziz. Riyadh will host Expo 2030, becoming the second Arab city to host after Dubai in 2020.\nOn the outskirts of Riyadh is Diriyah, the original home of the ruling House of Saud and site of At-Turaif Palace, a UNESCO heritage site.\nHistory.\nEarly history.\nDuring the Pre-Islamic era, the city at the site of modern Riyadh was called Hajr (), and was reportedly founded by the tribe of Banu Hanifa. Hajr served as the capital of the province of Al-Yamama, whose governors were responsible for most of central and eastern Arabia during the Umayyad Caliphate and Abbasid Caliphate eras. Al-Yamama broke away from the Abbasid in 866 and the area fell under the rule of the Banu Ukhaidhir, who moved the capital from Hajr to nearby Al-Kharj. The city then went into a long period of decline. In the 14th century, North African traveler Ibn Battuta wrote of his visit to Hajr, describing it as \"the main city of Al-Yamama, and its name is Hajr\". Ibn Battuta goes on to describe it as a city of canals and trees with most of its inhabitants belonging to the Banu Hanifa, and reports that he continued on with their leader to Mecca to perform the Hajj.\nLater on, Hajr broke up into several separate settlements and estates. The most notable of these were Migrin (or Muqrin) and Mi'kal, though the name \"Hajr\" continued to appear in local folk poetry. The earliest known reference to the area by the name \"Riyadh\" comes from a 17th-century chronicler reporting on an event from the year 1590. In 1737, Dahham ibn Dawwas, a refugee from neighboring Manfuhah, took control of Riyadh. Ibn Dawwas built a single wall to encircle the various oasis towns in the area, making them effectively a single fortress city. The name \"Riyadh\", meaning \"gardens\" refers to these earlier oasis towns.\nEconomy.\nThe capital of Saudi Arabia, Riyadh, was initially known for its availability of water and fertile land which made it ideal for farming dates and other crops. Wheat was also widely grown until the crops were infested with insects and mites. After Riyadh was designated as the capital in the mid-1900s, Riyadh became a manufacturing hub. Almost one-third of Saudi Arabia's factories are located in Riyadh, producing a range of products including machinery, equipment, metallurgical goods, chemicals, construction materials, food, textiles, furniture, and numerous publications.\nFirst Saudi State.\nIn 1744, Muhammad ibn Abd al-Wahhab formed an alliance with Muhammad bin Saud, the ruler of the nearby town of Diriyah. Ibn Saud then set out to conquer the surrounding region with the goal of bringing it under the rule of a single Islamic state. Ibn Dawwas of Riyadh led the most determined resistance, allied with forces from Al-Kharj, Al-Ahsa, and the Banu Yam clan of Najran. However, Ibn Dawwas fled and Riyadh capitulated to the Saudis in 1774, ending long years of wars, and leading to the declaration of the First Saudi state, with Diriyah as its capital.\nThe First Saudi State was ended by forces sent by Muhammad Ali of Egypt, acting on behalf of the Ottoman Empire. Ottoman forces razed the Saudi capital Diriyah in 1818. They had maintained a garrison at Najd. This marked the decline of the House of Saud for a short time. Turki bin Abdullah became the first ruler of the Second Saudi state; the cousin of Saud bin Saud, he ruled for 19 years till 1834, leading to the consolidation of the area though they were notionally under the control of Muhammad Ali, the Viceroy of Egypt. In 1823, Turki ibn Abdallah chose Riyadh as the new capital. Following the assassination of Turki in 1834, his eldest son Faisal killed the assassin, took control of the capital, and refused to be controlled by the Viceroy of Egypt. Najd was then invaded, and Faisal was taken captive and held in Cairo. However, as Egypt became independent of the Ottoman Empire, Faisal escaped after five years of incarceration, returned to Najd, and resumed his reign, ruling until 1865 and consolidating the reign of the House of Saud.\nFollowing the death of Faisal, there was rivalry among his sons which situation was exploited by Muhammad bin Rashid who took most of Najd, signed a treaty with the Ottomans, and also captured Al-Ahsa in 1871. In 1889, Abdul Rahman bin Faisal, the third son of Faisal again regained control over Najd and ruled till 1891, whereafter the control was regained by Muhammad bin Raschid.\nInternecine struggles between Turki's grandsons led to the fall of the Second Saudi State in 1891 at the hand of the rival House of Rashid, which ruled from the northern city of Ha'il. The Al-Masmak Palace dates from that period.\nAbdul Rahman bin Faisal al-Saud had sought refuge among a tribal community on the outskirts of Najd and then went to Kuwait with his family and stayed in exile. However, his son Ibn Saud retrieved his ancestral kingdom of Najd in 1902 and consolidated his rule by 1926, and further expanded his kingdom to cover \"most of the Arabian Peninsula.\" He named his kingdom as Saudi Arabia in September 1932 with Riyadh as the capital. King Ibn Saud died in 1953 and his son Saud took control as per the established succession rule of father to son from the time Muhammad bin Saud had established the Saud rule in 1727. However, this established line of succession was broken when King Saud was succeeded by his brother King Faisal in 1964. In 1975, Faisal was succeeded by his brother King Khalid. In 1982, King Fahd took the reins from his brother. This new line of succession is among the sons of King Abdul Aziz who has 35 sons; this large family of Ibn Saud hold all key positions in the large kingdom.\nModern history.\nFrom the 1940s, Riyadh mushroomed from a relatively narrow, spatially isolated town into a spacious metropolis. When King Saud came to power, he made it his objective to modernize Riyadh, and began developing Annasriyyah, the royal residential district, in 1950. Following the example of American cities, new settlements and entire neighborhoods were created on grid plans, and connected by high-capacity main roads to the inner areas. The grid pattern in the city was introduced in 1953. The population growth of the town from 1974 to 1992 averaged 8.2 percent per year.\nOn 16 November 1983, King Khalid International Airport was officially opened by King Fahd, in memory to the late King Khalid. It remains the biggest airport in the world at nearly 300 sq miles to date.\nAl-Qaeda under Osama bin Laden launched coordinated attacks on compounds in Riyadh on 12 May 2003, resulting in the deaths of 39 people. The bombings were considered to be a terrorism campaign against Western influence in Saudi Arabia.\nIn 2010, the first Saudi capital Diriyah, on the northwestern outskirts of Riyadh was inscribed as a World Heritage Site by UNESCO.\nThe mayor is Prince Faisal bin Abdulaziz al-Muqrin. Al-Muqrin was appointed in 2019 by royal decree and succeeds Tariq bin Abdul Aziz Al-Faris. Riyadh is now the administrative and to a great extent the commercial hub of the Kingdom. According to the Saudi Real Estate Companion, most large companies in the country established either sole headquarters or a large office in the city. For this reason, there has been significant growth in high-rise developments in all areas of the city. Most notable among these is King Abdullah Financial District which is fast becoming the key business hub in the city. Riyadh also has the largest all-female university in the world, the Princess Nourah Bint Abdul Rahman University.\nAccording to the Global Financial Centres Index, Riyadh ranked at 77 in 2016\u20132017. Though the rank moved up to 69 in 2018, diversification in the economy of the capital is required in order to avoid what the World Bank called a \"looming poverty crisis\" brought on by lingering low oil prices and rich state benefits.\nSince 2017, Riyadh has been the target of missiles from Yemen. In March 2018, one person died as a result of a missile attack. The number of missiles which targeted Riyadh are a small portion of the dozens of missiles fired from Yemen at Saudi Arabia due to the Saudi-led intervention in the Yemeni civil war. In April 2018, heavy gunfire was heard in Khozama; this led to rumors of a coup attempt.\nA restoration of heritage buildings of historical significance was launched in Riyadh by Crown Prince Mohammed bin Salman on 13 September 2020.\nUrban development history.\nUp to 1930s.\nHistorical Riyadh was enclosed by walls. At its center was a town square and a market (souq), surrounded by residential quarters of mosques and adobe homes, each with an interior courtyard. Outside its walls were orchards of date trees, hence the name 'Riyadh' or 'gardens'. During the 1930s, there was an initial outward expansion because new administrative buildings were needed for the country and because the population was growing. According to Dr. Saleh Al Hathloul, former deputy minister of town planning, this era coincided with the period of sedentarization as nomads settled in and around towns and cities such as Riyadh.\n1940s\u20131950s.\nWhen commercial oil production began, there was a rapid rise in the rate of urbanization and the city transitioned from traditional to newer houses and buildings. This included the railway station and the (now-defunct) first airport of Riyadh. Government departments were relocated from Jeddah to Riyadh and new ministry buildings were built. To accommodate the government employees who had moved in from Jeddah, the government developed the Malaz housing block. This block's layout was influenced by the layouts of Dammam and Khobar, which in turn were influenced by the Aramco-built Dhahran. Malaz, with its street grid and detached house type, was instrumental in shaping the master plans for Riyadh that followed, as per Dr. Saleh Al Hathloul.\n1960s\u20131970s.\nThe Department of Municipal Affairs (later Ministry of Municipalities and Housing) selected Doxiadis Associates (DA) in 1968 to prepare a masterplan for Riyadh. After preliminary studies, they submitted a plan that was approved in 1972. They proposed that Riyadh will expand in the north-south axis along a commercial spine with and most importantly, that it will be divided into neighborhoods of 2 \u00d7 2 km blocks, thus solidifying the grid pattern to be the defining feature of Riyadh's layout. It also maintains the style of housing that was prominent in Malaz, detached houses with setbacks, designed in what Dr. Saleh Al Hathloul identifies as an 'international Mediterranean' style i.e. crimson colors. However, DA's shortcomings lay in their inability to accurately predict the extent of Riyadh's future growth.\nAt the start of the 70s, Riyadh did not go much beyond what is today the Khurais road. But nearing the 80s, Riyadh's expansion had already reached the Northern Ring Road in the north and had made considerable progress in the eastern part of the city.\nIn 1974, the government founded the High Commission for the Development of Arriyadh (later Royal Commission for Riyadh City) which was headed by the then governor of Riyadh Province, King Salman Bin Abdulaziz, who oversaw Riyadh's development. With the economic growth and national development plans of the 70s, the national infrastructure consisting of electricity grids, telecommunications networks, water pipelines, and highways was laid down that made further urban growth possible. The old and new industrial cities of Riyadh were both founded in this period.\n1980s\u20131990s.\nThe city grew at a much faster rate than Doxiadis Associates had projected and very soon, their plan became obsolete. DA predicted that Riyadh's urban area would be 304 km2 in 30 years when it reached 400 km2 just four years after the plan was authorized. Therefore, SCET International was assigned to revise and update the original plan to reflect the drastic growth and offer adaptive measures, which were approved in 1982. While keeping the 2km x 2km block, they expanded it in all directions unlike DA's linear expansion. They also added the radial ring roads and altered the DA conception of how commercial and other zones should be distributed.\nIt was in the 80s and 90s that most of the buildings that defined Riyadh's urban identity were constructed. Built in styles contemporary of that time, marble with a hint of desert beige, these included the King Khalid International Airport, King Fahd Sports City, Television tower, King Saud University new campus, the King Faisal Foundation, the Ministry of Interior, Ministry of Foreign Affairs, MOMRA, and Imam Mohammad Ibn Saud Islamic University. And the historical district was rebuilt with the National Museum, Qasr Al-Hokm District, and the Imam Turki bin Abdullah Grand Mosque. Numerous health facilities were founded as well.\u00a0Other developments in this period included the opening of the first shopping centers and supermarkets.\nApproaching the 2000s, Riyadh had expanded well beyond the Northern Ring Road in the north and had reached the Second Ring Road in the east.\n2000s\u20132010s.\nThe MEDSTAR (metropolitan development strategy for Arriyadh) was the strategy that directed urban development in this era. Since the SCET plan also turned out to underestimate the rate of growth, a continuous approach instead of a one-off plan was adopted. The MEDSTAR was not a long term plan but an ongoing strategy on managing urban growth and economic development in the city. It was initiated after comprehensive studies by the Arriyadh Development Authority (the high commission's research wing) on demographics, land use, transportation, security, environment, and traffic safety. In 2007, MEDSTAR won second place in the international award for liveable communities. One of the MEDSTAR strategies was balanced development by turning Riyadh into a polycentric city rather than having one single downtown. [Riyadh: The Metamorphosis of a City From Centerless to Polycentric Fernando Perez,] This has resulted in there being multiple hubs scattered around the city such Al-Olaya, King Abdullah Financial District, Sahafa, Granada, Business Gate, Digital City, and Hittin.\nRiyadh's skyline arose along the King Fahd Road starting in the 2000s. Significant construction projects like the Riyadh metro and the Princess Noura University, the world's largest women's university, were undertaken. Most malls and hypermarkets opened in this era and became a feature of city life. The municipality added wide sidewalks to a number of streets which became popular spots for walking, and parks were built in many neighborhoods. Major roads were redesigned, such as the King Fahd road, King Abdullah Road, Abu Bakr Al Siddiq road, and Oruba road, transforming the look of the city. In addition, the Royal Commission rehabilitated the Wadi Hanifa wetlands.\nAt the onset of the 2020s, Riyadh's expansion had gone further ahead of the King Salman Road in the north and had reached the Janadriyah road in the east.\n2020\u2013present.\nVision 2030 has stated its objective for Saudi cities to reach the list of top 100 cities of the world in quality of life and the city is working towards this goal through\u00a0new development investments. Every year, the number of tourists visiting Saudi Arabia and Riyadh increases. In the large empty area where the old airport once was, the world's largest urban park, King Salman Park is being constructed, with leisure, residential, office, hospitality, and retail spaces. The historical city of Diriyah, now encompassed by Riyadh, has been restored and developed into a cultural and tourist destination. Many roads and streets, such as the Olaya street and the Imam Saud road, are being refurbished.\nFewer malls are opening and squares (or plazas) are taking over in popularity, the most popular having been the Riyadh Boulevard on the Prince Turki Al Awwal Road. A new downtown called 'New Murabba' at the intersection of the King Salman and King Khaled roads is planned.\nNew fully residential suburbs, unlike regular neighborhoods that have storefront-lined main streets, are under construction in the far north and far east of the city.\nGeography.\nClimate.\nRiyadh has a hot desert climate (K\u00f6ppen Climate Classification \"BWh\"), with long, extremely hot summers and short, very mild winters. The average high temperature in July is . If not for its elevation Riyadh would experience an even hotter climate. The city experiences very little precipitation, especially during the summer, but receives a fair amount of rain in March and April. It is also known to have dust storms during which the dust can be so thick that visibility is under . On 1 and 2 April 2015, a massive dust storm hit Riyadh, causing the suspension of classes in many schools in the area and the cancellation of hundreds of flights, both domestic and international.\nCity districts.\nRiyadh is divided into fourteen branch municipalities, in addition to the Diplomatic Quarter. Each branch municipality in turn contains several districts, amounting to over 130 in total, though some districts are divided between more than one branch municipality. The branch municipalities are Al-Shemaysi, Irqah, Al-Ma'athar, Al-Olaya, Al-Aziziyya, Al-Malaz, Al-Selayy, Nemar, Al-Neseem, Al-Shifa, Al-'Urayja, Al-Bat'ha, Al-Ha'ir, Al-Rawdha, and Al-Shamal (\"the North\"). Al-Olaya District is the commercial heart of the city, with accommodation, entertainment, dining and shopping options. The Kingdom Centre, Al-Faisaliah, and Al-Tahlya Street are the area's most prominent landmarks. The center of the city, Al-Bateha and Al-Deerah, is also its oldest part.\nSome of the main districts of Riyadh are:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nDemographics.\n&lt;templatestyles src=\"Module:Historical populations/styles.css\"/&gt;\nIn 2022, the city had over 7 million people. The city had a population of 40,000 inhabitants in 1935 and 83,000 in 1949. The city has experienced very high rates of population growth, from 150,000 inhabitants in the 1960s to over seven million, according to the most recent sources. As of 2017, the population of Riyadh is composed of 64.19% Saudis, while non-Saudis account for 35.81% of the population. Indians are the largest minority population at 13.7%, followed by Pakistanis at 12.4%.\nThe population is so high due to the doubled birth rates and the high economic growth. There was also an influx of immigrants.\nLandmarks and architecture.\nVernacular architecture of Old Riyadh.\nThe old town of Riyadh within the city walls did not exceed an area of 1\u00a0km2, and therefore very few significant architectural remnants of the original walled oasis town of Riyadh exist today. The most prominent is the Al-Masmak Palace and some parts of the original wall structure with its gate which have been restored and reconstructed. There are also a number of traditional mud-brick houses within these old limits, but they are for the most part dilapidated.\nExpansion outside the city walls was slow to begin with, although there were some smaller oases and settlements surrounding Riyadh. The first major construction beyond the walls was King Abdulaziz's Murabba Palace. It was constructed in 1936, completed in 1938, and a household of 800 people moved into it in 1938. The palace is now part of a bigger complex called King Abdulaziz Historical Center.\nThere are other traditional villages and towns in the area around traditional Riyadh which the urban sprawl reached and encompasses. These include Diriyah, Manfuha and Wadi Laban. Unlike in the early days of development in Riyadh during which vernacular structures were razed to the ground without consideration, there is a new-found appreciation for traditional architecture. The Ministry of Tourism is making efforts to revitalize the historic architecture in Riyadh and other parts of the kingdom.\nAin Heet cave has an underground lake (150 meters deep) situated at the face of Mount Al Jubayl in Wadi As Sulay in a small village called Heet in Riyadh. Between Riyadh and Al Kharj road, it is one of the easily accessible caves in the area of Riyadh.\nArcheological sites.\nThe archeological sites at Riyadh which are of historical importance, in which the Municipality of Riyadh is involved, are the five old gates on the old walls of Riyadh. These are the eastern gate of Thumaira, the northern gate of Al-Suwailen, the southern gate of Dukhna, the western gate of Al-Madhbah, and the south-western gate of Shumaisi. There are also four historic palaces: Al-Masmak Palace, Murabba Palace (palace of King Abdulaziz), Atiqah Palace (belongs to Prince Muhammad bin Abdul Rahman) and Al-Shamsiah Palace (belongs to Saud Al-Kabeer).\nTuraif district.\nThe Turaif district, is another important archeological site inscribed in UNESCO World Heritage List on 31 July 2010. It was founded in the 15th century bearing an architectural style of Najdi. There are some Historic palaces and monuments in Al-Turaif district include: Salwa Palace, Saad bin Saud Palace, The Guest House and At-Turaif Bath House, and Imam Mohammad bin Saud Mosque.\nAl-Musmak Palace.\nThis fortress was built around 1865 under the rule of the House of Rashid, the rulers of Ha'il to the north, who had wrested control of the city from the rival house, House of Saud. In January 1902 Ibn Saud, who was at the time living in exile in Kuwait, succeeded in capturing the Musmak Palace from its Rashid garrison. The event, which restored Saudi control over Riyadh, has acquired an almost mythical status in the history of Saudi Arabia. The story of the event is often retold and has as its central theme the heroism and bravery of King Ibn Saud. The Musmak Palace is now a museum and is in close proximity to the Deera Square.\nContemporary architecture.\nKingdom Centre.\nDesigned by the team of Ellerbe Becket and Omrania, the tower is built on 94,230\u00a0square meters of land. The Kingdom Centre is owned by a group of companies including Kingdom Holding Company, headed by Al-Waleed bin Talal, a prince of the Saudi royal family, and is the headquarters of the holding company. The project cost 2\u00a0billion Saudi Arabian Riyals and the contract was undertaken by El-Seif. The Kingdom Centre is the winner of the 2002 Emporis Skyscraper Award, selected as the \"best new skyscraper of the year for design and functionality\". A three-level shopping center, which also won a major design award, fills the east wing. The large opening is illuminated at night in continuously changing colors. The shopping center has a separate floor for women only to shop where men are not allowed to enter.\nThe Kingdom Centre has 99 stories and is the fifth tallest structure in the country, rising to 300m. A special aspect of the tower is that it is divided into two parts in the last one-third of its height and is linked by a sky-bridge walkway, which provides extensive views of Riyadh.\nBurj Rafal.\nBurj Rafal, located on King Fahd Road, is the tallest skyscraper in Riyadh at 307.9 meters (1,010 feet) tall. The tower was designed and engineered by P &amp; T Group. Construction began in 2010 and was completed in 2014. The project was considered a success, with 70% of the residential units already sold by the time the skyscraper was topped out. The tower contained 474 residential condominium units and a 349-room 5-star Kempinski hotel. Since then the hotel has been operated under the JW Mariott brand.\nAl-Faisaliah Tower.\nAl Faisaliah Tower (Arabic: \u0628\u0631\u062c \u0627\u0644\u0641\u064a\u0635\u0644\u064a\u0629) is the first skyscraper constructed in Saudi Arabia and is the third tallest building in Riyadh after Burj Rafal and the Kingdom Centre. The golden ball that lies atop the tower is said to be inspired by a ballpoint pen, and contains a restaurant; immediately below this is an outside viewing deck. There is a shopping center with major world brands at ground level. Al-Faisaliah Tower also has a hotel on both sides of the tower while the main building is occupied by office. The Al-Faisaliah Tower has 44 stories. It was designed by Foster and Partners.\nRiyadh TV Tower.\nThe Riyadh TV Tower is a 170-meter-high television tower located inside the premises of the Saudi Ministry of Information. It is a vertical cantilever structure which was built between 1978 and 1981. The first movie made in 1983 by the TV tower group and named \"1,000 Nights and Night\" had Mohammed Abdu and Talal Mmdah as the main characters. At that time, there were no women on TV because of religious restrictions. Three years later, Abdul Khaliq Al-Ghanim produced a TV series called \"Tash Ma Tash,\" which earned a good reaction from audiences in Eastern Arabia. This series created a media revolution back in the 1980s.\nMuseums and collections.\nIn 1999, a new central museum was built in Riyadh, at the eastern side of the King Abdulaziz Historical Center. The National Museum of Saudi Arabia combined several collections and pieces that had up until then been scattered over several institutions and other places in Riyadh and the Kingdom. For example, the meteorite fragment is known as the \"Camel's Hump\", recovered in 1966 from the Wabar site, that was on display at the King Saud University in Riyadh became the new entry piece of the National Museum of Saudi Arabia.\nThe Royal Saudi Air Force Museum, or Saqr Al-Jazira, is located on the East Ring Road of Riyadh between exits 10 and 11. It contains a collection of aircraft and aviation-related items used by the Royal Saudi Air Force and Saudia Airlines.\nSports.\nFootball is the most popular sport in Saudi Arabia. The city hosts four major football clubs, Al-Hilal was established in 1957 and has won 19 championships in the Saudi Pro League. Al-Nassr club is another team in the top league that has many supporters around the kingdom. It was established in 1955, and has been named champion of the Saudi professional League 9 times. Another well-known club, Al-Shabab, was established in 1947 and holds 6 championships. There is also Al-Riyadh Club, which was established in 1954, as well as many other minor clubs.\nThe city also has several large stadiums such as King Fahd Sports City Stadium with a seating capacity of 70,200. The stadium hosted the FIFA Confederations Cup three times, in the years 1992, 1995 and 1997. It also hosted the FIFA U-20 World Cup in 1989, and Prince Faisal bin Fahd Sports City Stadium that is used mostly for Football matches. The stadium has a capacity of 22,500 people.\nThe city's GPYW Indoor Stadium served as host arena for the 1997 ABC Championship, where Saudi Arabia men's national basketball team reached the \"Final Four\".\nOn 29 February 2020, the world's richest thoroughbred horse race took place at the King Abdulaziz Racetrack in Riyadh. The Saudi Cup is a new race for thoroughbreds aged four and up, to be run at weight-for-age terms over 1800m (9f). The prize money is US$20m with a prize of US$10m to the winner and prize money down to tenth place. The Saudi Cup is perfectly positioned between the Pegasus World Cup and the Dubai World Cup to attract the best horses from around the world to compete for horse racing's richest prize. Putting the Kingdom of Saudi Arabia on the international horseracing map, the Saudi Cup will also hold an undercard of international races on both dirt and the new turf course.\nOn 26 April 2020, Saudi Arabia entered the bidding process for the 2030 Asian Games; their main rival for this event was Doha, Qatar. On 16 December 2020, it was announced that Riyadh will host the 2034 Asian Games.\nEsports in Riyadh began its journey with the city's first major tournament, the GSA E-Sports Cup, in 2018. The Saudi Esports Federation further boosted this growth by organizing the Gamers8 festival in 2022 and 2023. The 2023 edition of the festival offered the largest prize pool in the history of global competitive esports at the time, totaling $45 million. As part of the Gamers8 festival, the Riyadh Masters, a Dota 2 tournament, boasted a significant prize pool of $15 million, surpassed only by the Fortnite World Cup Finals and The International in the history of esports tournaments. Gamers8 would be replaced by the Esports World Cup in 2024, which boasts a total prize pool of over $60 million, which will be the largest prize pool in the history of global competitive esports, split among at least 20 different tournaments and a Club Championship for esports organizations.\nTransportation.\nAir.\nRiyadh's King Khalid International Airport (KKIA) is located 35\u00a0kilometers north of the city center. It is the city's main airport, and served over 20 million passengers in 2013. The airport will be expanded, with six parallel runways and three or four large passenger terminals by 2030. It will be able to serve 120 million passengers per year after 2030, and 185 million passengers per year by 2050.\nBuses.\nAs part King Abdulaziz Public Transport Project, Riyadh Bus network consists of 3 main bus lines, covering a distance of 1,905 km. Riyadh\u2019s bus network consists of 87 routes across the city using 842 vehicles with approximately 3,000 service stations. The bus network transported 50 million passengers in 2024.\nThe main charter bus company in the kingdom, known as the Saudi Public Transport Company (SAPTCO), offers trips both within the kingdom and to its neighboring countries, including Egypt (via ferries from Safaga or Nuweiba) and Arab states of the Gulf Cooperation Council.\nMetro.\nThe Riyadh Metro, part of the King Abdulaziz Public Transport Project, is the world's longest driverless metro system.\nRailways.\nSaudi Arabia Railways operates two separate passenger and cargo lines between Riyadh and Dammam, passing through Hofuf and Haradh. Two future railway projects, connecting Riyadh with Jeddah and Mecca in the western region, and connecting Riyadh with Buraidah, Ha'il and Northern Saudi Arabia are underway.\nRoads.\nThe city is served by a major highway system. The main Eastern Ring Road connects the city's south and north, while the Northern Ring Road connects the city's east and west. King Fahd Road runs through the center of the city from north to south, in parallel with the East Ring Road. Makkah Road, which runs east\u2013west across the city's center, connects eastern parts of the city with the city's main business district and the diplomatic quarters.\nMedia.\nThe Riyadh TV Tower, operated by the Ministry of Information, was built between 1978 and 1981. National Saudi television channels Saudi TV1, Saudi TV2, Al-Riyadiya, Al-Ekhbariya, Arab Radio and Television Network operate from here. Television broadcasts are mainly in Arabic, although some radio broadcasts are in English or French. Arabic is the main language used in television and radio but radio broadcasts are also made in different languages such as Urdu, French, or English. Riyadh has four Arabic newspapers; \"Asharq Al-Awsat\" (which is owned by the city governor), \"Al-Riyadh\", \"Al-Jazirah\" and \"Al-Watan\", two English language newspapers; \"Saudi Gazette\" and \"Arab News\", and one Malayalam language newspaper, \"Gulf Madhyamam\". The Saudi government monitors and filters internet content. Political dissent is not tolerated in Saudi Arabia. Saudi Arabia has had strict regulations on cinema and the arts.\nDevelopment projects.\nIn 2019, King Salman launched a plan to implement 1281 development projects in Riyadh. The project is planned to cost around US$22 billion. The main goal of the plan is to improve the infrastructure, transportation, environment and other facilities in Riyadh and the surrounding area. In the framework of Saudi Vision 2030, the plan will take care of constructing 15 housing projects, building a huge museum, establishing an environmental project, sports areas, medical cities, educational facilities, etc. This includes the establishment of 14 electricity projects, 20 sewage projects, 10 housing areas, 66 trading and industrial areas, a number of lakes covering 315,000 square meters, and advanced sports cities. Since the announcement of the Vision, Riyadh has implemented various reforms to lay the foundation for the next steps of the Vision. Vision 2030's stated goals are to promote tourism, and to help push Saudi Arabia to the global front.\nAlongside the development project and with the aim of enhancing the artistic landscape of the city, 1000 pieces of art are planned to be publicly displayed in the city by the end of 2030. In the framework of Riyadh's development projects, an amount of SR 604 million has been awarded to develop and construct roads of Riyadh. On 3 July 2020, \"Bloomberg\" reported that Saudi Arabia has allocated $20 billion on the mega-project of tourism and culture in Riyadh, branded as Diriyah, while facing a double economic crisis after rise in coronavirus cases.\nThe Ministry of Investment and the Royal Commission for Riyadh City (RCRC) announced on 13 July 2021 that they have partnered with SEK Education Group to open SEK International School Riyadh, its first campus in Saudi Arabia. The new international school will welcome students from Pre-K (age 3 years) to Grade 12 (age 17/18 years), and will become one of the few schools in Riyadh accredited to offer the International Baccalaureate (IB) Primary Years Programme (PYP), Middle Years Programme (MYP), and Diploma Programme (DP).\nIn July 2024 the plan to create the Sports Boulevard which will include the world's tallest sports tower was approved. This is part of a $23 billion project meant to enlarge green spaces within the city.\nArts.\nIn March 2019, the Royal Commission for Riyadh City launched Riyadh Art, a public art project aimed at transforming Riyadh into an art hub by giving artists the chance to display and implement their talent in public spaces.\nLiteracy rate.\nThe literacy rate in 2020 was 99.36% and in 2021 it was 99.38%. The literacy rate in Saudi Arabia has improved from 2010 when it was 98.10%.\nEvents and festivals.\nJenadriyah.\nJenadriyah is an annual festival that has been held in Riyadh. It includes a number of cultural and traditional events, such as camel race, poetry reading and others.\nRiyadh International Book Fair.\nRiyadh International Book Fair is one of the largest book fairs in the middle east. It is usually held between March and April and it hosts a wide range of Saudi, Arab and international publishers.\nRiyadh Season.\nRiyadh Season was held as part of an initiative to promote tourism. The season took place from October to December 2019. It included a wide range of sports, musical, theatrical, fashion shows, circus, and various other entertainment activities.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51555", "revid": "404282", "url": "https://en.wikipedia.org/wiki?curid=51555", "title": "Rightism", "text": ""}
{"id": "51556", "revid": "125972", "url": "https://en.wikipedia.org/wiki?curid=51556", "title": "Rhineland", "text": "Historic region of Germany\nThe Rhineland ( ; ; ; ) is a loosely defined area of western Germany along the Rhine, chiefly its middle section. It is the main industrial heartland of Germany because of its many factories, and it has historic ties to the Holy Roman Empire, Prussia, and the German Empire.\nTerm.\nHistorically, the term \"Rhinelands\" refers to a loosely defined region encompassing the land on the banks of the Rhine, which were settled by Ripuarian and Salian Franks and became part of Frankish Austrasia. In the High Middle Ages, numerous Imperial States along the river emerged from the former stem duchy of Lotharingia, without developing any common political or cultural identity.\nA \"Rhineland\" conceptualization can be traced to the period of the Holy Roman Empire from the sixteenth until the eighteenth centuries when the Empire's Imperial Estates (territories) were grouped into regional districts in charge of defense and judicial execution, known as Imperial Circles. Three of the ten circles through which the Rhine flowed referred to the river in their names: the Upper Rhenish Circle, the Electoral Rhenish Circle, and the Lower Rhenish-Westphalian Circle (very roughly equivalent to the present-day German federal state of North Rhine Westphalia). In the twilight period of the Empire, after the War of the First Coalition, a short-lived Cisrhenian Republic was established (1797\u20131802). The term covered the whole French annexed territory west of the Rhine (German: \"\"), but also included a small portion of the bridgeheads on the eastern banks. After the defeat of the French empire, the regions of J\u00fclich-Cleves-Berg and Lower Rhine were annexed to the Kingdom of Prussia. In 1822 the Prussian administration reorganized the territory as the Rhine Province (\"Rheinprovinz\", also known as Rhenish Prussia), a tradition that continued in the naming of the current German states of Rhineland-Palatinate and North Rhine-Westphalia. \nIn the early 1800s, Rhinelanders settled the Missouri Rhineland, a German cultural region and wine-producing area in the U.S. State of Missouri, and named it after noticing similarities in soil and topography to the Rhineland in Europe. By 1860, nearly half of all settlers in Missouri Rhineland came from Koblenz, capital of the Rhine Province.\nThe western part of the Rhineland was occupied by Entente forces from the end of the First World War until 1930. Under the 1919 Treaty of Versailles, German military presence in the region was banned, a restriction which the government of Weimar Germany pledged to honor in the 1925 Locarno Treaties. Nazi Germany remilitarized the territory in 1936 as part of a diplomatic test of will three years before the outbreak of the Second World War.\nGeography.\nTo the west the area stretches to the borders with Luxembourg, Belgium, and the Netherlands; on the eastern side, it encompasses the towns and cities along the river and the Bergisches Land area up to the Westphalian (Siegerland) and Hessian regions. Stretching down to the North Palatine Uplands in the south, this area, except for the Saarland, more or less corresponds with the modern use of the term. \nThe southern and eastern parts are mainly hill country (Westerwald, Hunsr\u00fcck, Siebengebirge, Taunus and Eifel), cut by river valleys, principally the Middle Rhine up to Bingen (or very rarely between the confluence with the Neckar and Cologne) and its Ahr, Moselle and Nahe tributaries. The border of the North German plain is marked by the lower Ruhr. In the south, the river cuts the Rhenish Massif.\nThe area encompasses the western part of the Ruhr industrial region and the Cologne Lowland. Some of the larger cities in the Rhineland are Aachen, Bonn, Cologne, Duisburg, D\u00fcsseldorf, Essen, Koblenz, Krefeld, Leverkusen, Mainz, M\u00f6nchengladbach, M\u00fclheim an der Ruhr, Oberhausen, Remscheid, Solingen, Trier and Wuppertal.\nToponyms as well as local family names often trace back to the Frankish heritage.The lands on the western shore of the Rhine are strongly characterized by Roman influence, including viticulture. In the core territories, large parts of the population are members of the Catholic Church.\nHistory.\nPre-Roman.\nAt the earliest historical period, the territories between the Ardennes and the Rhine were occupied by the Treveri, the Eburones, and other Celtic tribes, who, however, were all more or less modified and influenced by their Germanic neighbors. On the East bank of the Rhine, between the Main and the Lahn, were the settlements of the Mattiaci, a branch of the Germanic Chatti, while farther to the north were the Usipetes and Tencteri.\nRomans and Franks.\nJulius Caesar conquered the Celtic tribes on the West bank, and Augustus established numerous fortified posts on the Rhine, but the Romans never succeeded in gaining a firm footing on the East bank. As the power of the Roman empire declined the Franks pushed forward along both banks of the Rhine, and by the end of the 5th century had conquered all the lands that had formerly been under Roman influence. By the 8th century, the Frankish dominion was firmly established in western Germania and northern Gaul.\nOn the division of the Carolingian Empire at the Treaty of Verdun the part of the province to the east of the river fell to East Francia, while that to the west remained with the kingdom of Lotharingia.\nHoly Roman Empire.\nBy the time of Emperor Otto I (d. 973) both banks of the Rhine had become part of the Holy Roman Empire, and in 959 the Rhenish territory was divided between the duchies of Upper Lorraine, on the Mosel, and Lower Lorraine on the Meuse.\nAs the central power of the Holy Roman Emperor weakened, the Rhineland disintegrated into numerous small independent principalities, each with its separate vicissitudes and special chronicles. The old Lotharingian divisions became obsolete, and while the Lower Lorraine lands were referred to as the Low Countries, the name of Lorraine became restricted to the region on the upper Moselle that still bears it. After the Imperial Reform of 1500/12, the territory was part of the Lower Rhenish\u2013Westphalian, Upper Rhenish, and Electoral Rhenish Circles. Notable Rhenish Imperial States included:\nDespite its dismembered condition and the sufferings it underwent at the hands of its French neighbors in various periods of warfare, the Rhenish territory prospered greatly and stood in the foremost rank of German culture and progress. Aachen was the place of coronation of the German emperors, and the ecclesiastical principalities of the Rhine played a large role in German history.\nFrench Revolution.\nAt the Peace of Basel in 1795, the whole of the left bank of the Rhine was taken by France. The population was about 1.6 million in numerous small states. In 1806, the Rhenish princes all joined the Confederation of the Rhine, a puppet of Napoleon. France took direct control of the Rhineland until 1814 and radically and permanently liberalized the government, society, and economy. The Coalition of France's enemies made repeated efforts to retake the region, but France repelled all the attempts. \nThe French swept away centuries worth of outmoded restrictions and introduced unprecedented levels of efficiency. The chaos and barriers in a land divided and subdivided among many different petty principalities gave way to a rational, simplified, centralized system controlled by Paris and run by Napoleon's relatives. The most important impact came from the abolition of all feudal privileges and historic taxes, the introduction of legal reforms of the Napoleonic Code, and the reorganization of the judicial and local administrative systems. The economic integration of the Rhineland with France increased prosperity, especially in industrial production, while business accelerated with the new efficiency and lowered trade barriers. The Jews were liberated from the ghetto. There was limited resistance; most Germans welcomed the new regime, especially the urban elites, but one sour point was the hostility of the French officials toward the Roman Catholic Church, the choice of most of the residents. The reforms were permanent. Decades later workers and peasants in the Rhineland often appealed to Jacobinism to oppose unpopular government programs, while the intelligentsia demanded the maintenance of the Napoleonic Code (which stayed in effect for a century).\nPrussian influence.\nA Prussian influence began on a small scale in 1609 by the occupation of the Duchy of Cleves. A century later, Upper Guelders and Moers also became Prussian. The Congress of Vienna expelled the French and assigned the whole of the lower Rhenish districts to Prussia, who left them in undisturbed possession of the liberal institutions to which they had become accustomed under the French. The Rhine Province remained part of Prussia after Germany was unified in 1871.\n1918\u20131945.\nThe occupation of the Rhineland took place following the Armistice with Germany of 11 November 1918. The occupying armies consisted of American, Belgian, British and French forces. Under the Treaty of Versailles, German troops were banned from all territory west of the Rhine and within 50 kilometers east of the Rhine.\nIn 1920, under massive French pressure, the Saar was separated from the Rhine Province and administered by the League of Nations until a plebiscite in 1935, when the region was returned to Germany. At the same time, in 1920, the districts of Eupen and Malmedy were transferred to Belgium (see German-Speaking Community of Belgium).\nIn January 1923, in response to Germany's failure to meet its reparations obligations, French and Belgian troops occupied the Ruhr district, strictly controlling all important industrial areas. The Germans responded with passive resistance, which led to hyperinflation, and the French gained very little of the reparations they wanted. French troops left the Ruhr in August 1925.\nThe occupation of the remainder of the Rhineland ended on 30 June 1930.\nOn 7 March 1936, in violation of the Treaty of Versailles, German troops marched into the Rhineland and other regions along the Rhine. After 1918, territory west of the Rhine had been off-limits to the German military. However, there was no opposition to its re-occupation from other powers such as France and the UK, despite the fact that in 1936 the German forces were not particularly strong and could have been pushed back. This lack of action by other powers gave Hitler confidence and Germany increased its programme of re-armament which led to war in 1939. \nTowards the end of the war the Rhineland was the scene of major fighting as the Allied forces overwhelmed the German defenders in 1945.\nPost-1946.\nIn 1946, the Rhineland was divided into the newly founded states of Hesse, North Rhine-Westphalia, and Rhineland-Palatinate. North Rhine-Westphalia is one of the prime German industrial areas, containing significant mineral deposits (coal, lead, lignite, magnesium, oil, and uranium) and water transport. In the Rhineland-Palatinate agriculture is more important, including the vineyards in the Ahr, Mittelrhein, and Mosel regions.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51557", "revid": "279219", "url": "https://en.wikipedia.org/wiki?curid=51557", "title": "Read only memory", "text": ""}
{"id": "51559", "revid": "47707289", "url": "https://en.wikipedia.org/wiki?curid=51559", "title": "Mendel University in Brno", "text": "Agricultural university in Brno, Czech Republic\nMendel University in Brno () is located in Brno, Czech Republic. It was founded on 24 July 1919 on the basis of the former T\u00e1bor Academy. It now consists of five faculties and one institute - the Faculty of AgriSciences, Faculty of Forestry and Wood Technology, Faculty of Business and Economics, Faculty of Horticulture, Faculty of Regional Development and International Studies and Institute of Lifelong Education. It is named since 1994 after Gregor Johann Mendel, the botanist and \"father of genetics\", who was active in this city during his lifetime.\nIn June 2020, the university was included in the QS World University Rankings top 1,000 for the first time, placed #701-750.\nHistory.\nEstablished on July 24, 1919, the university initially known as the University of Agriculture is the oldest agricultural school in Czechoslovakia. Renamed in the mid-1990s to honor Gregor Johann Mendel, reflecting its strong heritage in the agricultural sciences. It began with programs in forestry and economics and has since expanded to include several specialized faculties and research institutes.\nIn 2010, it adopted the name Mendel University in Brno. The establishment of CEITEC MENDELU in 2011 integrated the university into the Central European Institute of Technology, enhancing its research capabilities.\nScientific research tasks.\nThe resolution of scientific research tasks is based on the long-term forecast and scientific orientation of the university and its faculties, covering the fields of agricultural, forestry, biological, economic, and technical sciences. It involves an open range of research activities, implemented in institutional research, competitive grant agency procedures, research centers, and international scientific research programs. The research program at MENDELU in Brno generally follows current trends in the development of basic scientific disciplines, especially biology and their applications in agricultural, forestry, horticultural, and economic sciences. \nIn practical implementation, it involves targeted management of biological processes, efficient utilization of non-renewable and creation of renewable natural resources in the development of sustainable, multifunctional agriculture and agribusiness. Emphasis is placed on the quality and safety of agricultural products in general, and food in particular. In the context of the conclusions of the EU Common Agricultural Policy, the university prioritizes topics related to multifunctional agriculture and forestry, the significance of their production and non-production functions in shaping the landscape and rural development.\"\nDepartment of Plant Biology\nDepartment of Applied and Landscape Ecology\nDepartment of Agrosystems and Bioclimatology\nDepartment of Crop Science, Plant Breeding and Plant Medicine\nDepartment of Agrochemistry, Soil Science, Microbiology and Plant Nutrition\nDepartment of Animal Nutrition and Forage Production\nDepartment of Animal Morphology, Physiology and Genetics\nDepartment of Zoology, Fisheries, Hydrobiology and Apiculture\nDepartment of Molecular Biology and Radiobiology\nDepartment of Agriculture, Food and Environmental Engineering\nDepartment of Engineering and Automobile Transport\nDepartment of Food Technology\nDepartment of Animal Breeding\nDepartment of Chemistry and Biochemistry\nDepartment of Physical Training\nDepartment of Forest Management and Applied Geoinformatics\nDepartment of Geology and Pedology\nDepartment of Mathematics\nDepartment of Forest Botany, Dendrology and Geobiocoenology\nDepartment of Forest and Wood Products Economics and Policy\nDepartment of Landscape Management\nDepartment of Forest and Forest Products Technology\nDepartment of Forest Protection and Wildlife Management\nDepartment of Silviculture\nDepartment of Wood Processing\nDepartment of Furniture, Design and Habitat \nDepartment of Wood Science\nInstitute of Forest Ecology\nDepartment of Economics\nDepartment of Business Economics\nDepartment of Management\nDepartment of Statistics and Operational Analysis\nDepartment of Accounting and Taxation\nDepartment of Marketing and Trade\nDepartment of Informatics\nDepartment of Law\nDepartment of Finance\nDepartment of Social Sciences\nDepartment of Fruit Growing\nDepartment of Horticultural Machinery\nDepartment of Vegetable Growing and Floriculture\nDepartment of Breeding and Propagation of Horticultural Plants\nDepartment of Post-harvest Technology of Horticultural Products\nDepartment of Viticulture and Viniculture\nDepartment of Garden and Landscape Architecture\nDepartment of Planting Design and Maintenance\nDepartment of Landscape Planning\nThe Mendeleum - Institute of Genetics and Plant Breeding\nFaculties.\nBachelor programs of the Faculty of Regional Development and International Studies.\nSource:\nMaster programs of the Faculty of Regional Development and International Studies.\nSource:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51560", "revid": "15881234", "url": "https://en.wikipedia.org/wiki?curid=51560", "title": "Latvian Gambit", "text": "Chess opening\nChess opening\nThe Latvian Gambit (or Greco Countergambit) is a chess opening characterised by the moves:\n1. \n2. ?!\nIt is one of the oldest chess openings, having been analysed in the 16th century by Giulio Cesare Polerio and the 17th century by Gioachino Greco, after whom it is sometimes named. The opening has the appearance of a King's Gambit with &lt;dfn id=\"\"&gt;colours reversed&lt;/dfn&gt;. It is an aggressive but objectively dubious opening for Black which often leads to wild and tricky positions. \nThe \"ECO\" code for the Latvian Gambit is C40, along with several other uncommon responses to 2.Nf3.\n&lt;templatestyles src=\"Template:TOC_left/styles.css\" /&gt;\nHistory.\nAnalysed by Giulio Cesare Polerio, in the late 16th century, the opening came to be known as the Greco Countergambit, and some writers still refer to it as such. That name recognised the Italian player Gioachino Greco (1600\u20131634), who contributed to the early theory of the opening.\nThe name \"Latvian Gambit\" is a tribute to several Latvian players who analysed it, K\u0101rlis B\u0113ti\u0146\u0161 being the most prominent among them. The Austrian master Albert Becker once published an article that B\u0113ti\u0146\u0161 judged to be dismissive about the Latvian Gambit. In response, B\u0113ti\u0146\u0161 published and analysed one of his own games in order to defend the gambit: Ilyin-Zhenevsky vs. K B\u0113ti\u0146\u0161, 1921.\nThe opening has a dubious reputation; FIDE Master Dennis Monokroussos even goes so far as to describe it as \"possibly the worst opening in chess\". Paul van der Sterren writes \"What is required to play the Latvian Gambit with any degree of success is a sharp eye for tactics and a mental attitude of total contempt for whatever theory has to say about it.\" The Latvian has always been uncommon in top-level &lt;dfn id=\"\"&gt;over-the-board&lt;/dfn&gt; play, but some correspondence players are devotees.\nThe opening is used mainly for its surprise value, as the chances of an opponent being familiar with this opening are low. It has been used, however, by Boris Spassky and Mikhail Chigorin, amongst many others; albeit, usually in casual play. Most notably, even Bobby Fischer and Jos\u00e9 Ra\u00fal Capablanca have lost to it. Sweden's Jonny Hector is one of the few grandmasters to play it in serious competition; he has argued that it is not as bad as its reputation and that even with best play, White's advantage is not large.\nMain line: 3.Nxe5 Qf6.\nWhite's 3.Nxe5 is considered the main line against the Latvian. After the usual 3...Qf6, the traditional main line has been 4.d4 d6 5.Nc4 fxe4, often continuing 6.Nc3 Qg6. The immediate 4.Nc4 (the Leonhardt Variation) has the advantage of preserving the option of d2\u2013d3, and has increased in popularity in recent years.\n4.d4.\nA possible continuation after 4.d4 is 4...d6 5.Nc4 fxe4 6.Nc3 Qg6 7.f3 exf3 8.Qxf3 Nf6 9.Bd3 Qg4 10.Qe3+ Qe6 11.0-0 Qxe3+ 12.Bxe3 Be7 13.Rae1 0-0. White is better here, but Black has chances due to White's misplaced king and weak light squares.\n4.Nc4.\n4.Nc4 has the advantage of allowing White to open the &lt;dfn id=\"\"&gt;centre&lt;/dfn&gt; with d3, for example 4...fxe4 5.Nc3 Qg6?! 6.d3 exd3? 7.Bxd3 Qxg2? and now White is winning after 8.Qh5+ Kd8 (or 8...g6 9.Qe5+ and 10.Be4) 9.Be4. If 6... Bb4, however, White must be careful following the same line, e.g. 7.Bd2 exd3 8.Bxd3 Qxg2 9.Qh5+ Kd8 10.Be4 Nf6! because now if White plays Bg5, which would be necessary to win the queen in the earlier line, then ...Bxc3+ wins for Black.\nThe main line continues 5...Qf7 6.Ne3! Black usually responds with 6...c6!?, when White can either accept the pawn sacrifice with 7.Nxe4 d5 8.Ng5 Qf6 9.Nf3, or decline it with the more popular 7.d3 exd3 8.Bxd3 d5 9.0-0. The latter variation has been deeply analysed; the British grandmaster Anthony Kosten analyses one line to move 32. One line discussed by IM Jeremy Silman is 9...Bc5 10.Na4 Bd6 11.c4 d4 12.Nc2 c5 13.b4 Ne7 14.Nxc5 Bxc5 15.bxc5 Nbc6 16.Bb2 0\u20130 17.Nxd4 Nxd4 18.Bxd4 Bf5 19.Bxf5 Nxf5 20.Be3 Qxc4 21.Qb3 Nxe3!? 22.fxe3 Rxf1+ 23.Rxf1 Qxb3 24.axb3 Rc8 25.Rf5 and now 25...Rd8 or 25...Rc6 gives Black an excellent chance to draw the pawn-down endgame. Silman later argued that 10.b4!! and now 10...Bxb4 11.Ncxd5 cxd5 12.Nxd5 or 10...Bd6 11.Re1! Ne7 12.Nexd5 cxd5 13.Nb5 is close to winning for White, and that the \"old, discredited\" 9...Bd6 (rather than 9...Bc5) might be Black's best try, though still insufficient for &lt;dfn id=\"\"&gt;equality&lt;/dfn&gt;.\n3.Nxe5, other lines.\nAlso possible is the eccentric 3...Nc6?!, against which John Nunn recommends 4.d4, preferring principled opening play to the unclear tactics resulting from 4.Qh5+. After 4.d4, if 4...Qh4? (Kosten's original recommendation) 5.Nf3! Qxe4+ 6.Be2 leaves Black with a lost position. After 4.d4, Kosten analyses 4...Qf6!? 5.Nc3 Bb4 6.exf5! Nxe5 7.Qe2.\nInstead of 4.d4, Kosten says that White can accept the proffered rook with 4.Qh5+ g6 5.Nxg6 Nf6 6.Qh3 hxg6 7.Qxh8 Qe7 (7...fxe4? 8.d4! is strong) 8.d3! (Stefan B\u00fccker gives an alternative 8.Nc3! Nb4 9.d3 as also winning for White) 8...fxe4 9.Be3 d5 10.Bc5! Qxc5 11.Qxf6 Bf5 12.dxe4 Nd4 13.exf5! Nxc2+ 14.Kd1 Nxa1 15.Bd3 Qd6 16.Re1+ Kd7 17.Qf7+ Be7 18.Re6 winning.\n3...Nf6 is occasionally seen. A common line is 4.exf5 Qe7 5.Qe2 d6 6.Nc4 Bxf5 7.Qxe7+ Bxe7.\n3.Bc4.\nWhite's 3.Bc4 may lead to perhaps the most notorious and heavily analysed line of the Latvian, which begins 3...fxe4 4.Nxe5 Qg5 5.d4 Qxg2 6.Qh5+ g6 7.Bf7+ Kd8 8.Bxg6! Qxh1+ 9.Ke2 Qxc1 (9...c6 is a major alternative) 10.Nf7+ Ke8 11.Nxh8+ hxg6 12.Qxg6+ Kd8 13.Nf7+ Ke7 14.Nc3! (\"diagram\").\nInstead of 4...Qg5, however, \"nowadays players often give preference to 4...d5\", the Svedenborg or Polerio Variation. According to Latvian Gambit experts Kon Grivainis and John Elburg, Black wins more often than White in this line. After 4...d5 5.Qh5+ g6 6.Nxg6, Black chooses between 6...Nf6 and 6...hxg6. 6...Nf6 usually leads, after 7.Qe5+ Be7 8.Bb5+! (a small zwischenzug to deprive Black's knight of the c6-square) 8...c6 9.Nxe7 Qxe7 10.Qxe7+ Kxe7 11.Be2 (or 11.Bf1), to an endgame where Black is a pawn down but has positional compensation. Sharper is 6...hxg6, when 7.Qxh8 Kf7 9.Qd4 Be6 gives White a large &lt;dfn id=\"\"&gt;material&lt;/dfn&gt; advantage, but his \"position is constantly on the edge of a precipice\", and the line has accordingly fallen out of favour. More often, White plays 7.Qxg6+ Kd7 8.Bxd5 Nf6, leading to sharp and &lt;dfn id=\"\"&gt;unclear&lt;/dfn&gt; play.\nBlack's best response is 3...fxe4. Some sample continuations are\nAssessment: Black is usually down material, but has excellent compensation. Most of White's pieces are still on the back rank. IM Mio argues Black is better.\nOther lines.\nSeveral other responses for White have been analysed.\n3.Nc3.\nWhite's 3.Nc3 was originally analysed by the American master Stasch Mlotkowski (1881\u20131943) in the 1916 \"British Chess Magazine\". Kosten gives as Black's two main responses 3...Nf6 4.Bc4 (4.exf5 is also possible) fxe4 5.Nxe5 d5 6.Nxd5! Nxd5 7.Qh5+ g6 8.Nxg6! hxg6! 9.Qxg6+ Kd7 10.Bxd5 Qe7 11.Qxe4 Rh4 12.Qxe7+ Bxe7, reaching an endgame where White has four pawns for a &lt;dfn id=\"\"&gt;minor piece&lt;/dfn&gt;, and 4...fxe4 5.Nxe5 Qf6, when White can choose from 6.Nc4! (transposing to the main line 3.Nxe5 Qf6 4.Nc4 fxe4 6.Nc3), 6.d4, and 6.f4!? Black can also play 3...d6, when 4.d4 transposes to the Philidor Countergambit (1.e4 e5 2.Nf3 d6 3.d4 f5!?), which was favoured by Paul Morphy in the mid 19th century and is still seen occasionally today.\nToday, however, Black's response is considered to be 3...fxe4.\nAssessment: One of the best lines for Black. Black has better bishops and a strong centre.\n3.exf5.\nWhite's 3.exf5 followed by 3...e4 4.Ne5 Nf6 5.Be2 is recommended by John L. Watson and Eric Schiller. 4.Qe2, 4.Nd4, and even 4.Ng1!? are also possible.\nAfter 3...e4, White has three possible moves:\nSample continuation No. 1 (Ian Defence): 4.Ne5 Nf6 5.Be2 Be7 6.Bh5+ Kf8! 7.Nf7 Qe8 8.Nxh8 Nxh5 9.Nc3 Kg8\nBlack lost castle and exchanged its rook with white knight and bishop, but kingside is solid, ready to expand queenside, which is considered to be equal.\nSample continuation No. 2: 4.Nd4 Nf6 5.d3 c5 6.Nb3 exd3 7.Bxd3 d5 8.Bb5+ Nc6\nCommon mistake: 4.Qe2? Qe7 5.Nd4 Nc6 6.Qh5+ Kd8 7.Nxc6+ dxc6 8.Be2 Nf6 9.Qg5 h6 10.Qe3 Bxf5 11.0-0 Nd5 12.Qd4 Qd6 13.d3 Nb4\nAssessment: Black is not lost here, and often allows Nf7 and sacrifice Black's kingside rook.\n3.d4.\nWhite's 3.d4 followed by 3...fxe4 4.Nxe5 Nf6 5.Bg5 d6 leads, as usual, to sharp play. White often offers a piece sacrifice with either 6.Nc3!? or 6.Nd2!?, but Black seems to have adequate resources against both.\nBlack's best response is considered to be 3...fxe4.\nSample continuation No. 1: 4.Nxe5 Nf6 5.Be2 d6 6.Nc4 Be6 7.Ne3 d5 8.c4 c6 9.Nc3 Be7 10.0-0 0-0\nCommon mistake: 8.0-0? c5!\nSample continuation No. 2: 4.Nxe5 Nf6 5.Be2 d6 6.Ng4 Be7 7.Nc3 d5 8.Ne5 0-0 9.Bg5 c6 10.0-0 Bf5\nAssessment: Black has a better pawn structure, and better bishops.\n3.d3.\nThis passive move does not promise White any advantage. After 3...Nc6 4.Nc3 Nf6 5.exf5 d5, Black is okay. Alternatively, 3...d6 4.Nc3 Nf6 5.g3 Be7 6.Bg2 is also considered a prudent defence by Black.\nBlack's best response is 3...Nc6.\nSample continuation No. 1: 4.Nc3 Nc6 5.exf5 d5\nSample continuation No. 2: 4.Nc3 Nc6 5.Bg5 Bb4 6.exf5 d5 7.a3 Bxc3+ 8.bxc3 Bxf5\nAssessment: Normal position that is comparable to several other openings. White has a weak pawn structure but the &lt;dfn id=\"\"&gt;bishop pair&lt;/dfn&gt;. This is a tough advantage to prove, however, since White's light-squared bishop is restricted.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51561", "revid": "1521335", "url": "https://en.wikipedia.org/wiki?curid=51561", "title": "Lee DeForest", "text": ""}
{"id": "51563", "revid": "48270863", "url": "https://en.wikipedia.org/wiki?curid=51563", "title": "The Luzhin Defence", "text": "The Luzhin Defence is a 2000 romantic drama film directed by Marleen Gorris, starring John Turturro and Emily Watson. The film centres on a mentally tormented chess grandmaster and the young woman he meets while competing at a world-class tournament in Italy. The screenplay was based on the novel \"The Defense\" (or \"The Luzhin Defence\") by Vladimir Nabokov.\nEmily Watson received best actress nominations at the British Independent Film Awards and the London Film Critics Circle Awards.\nPlot.\nIt is the early 1920s and Aleksandr Ivanovich 'Sascha' Luzhin (Turturro), a gifted but tormented chess player, arrives in a Northern Italian city to compete in an international chess competition. Prior to the tournament he meets Natalia Katkov (Watson) and he falls in love with her almost immediately. She in turn finds his manner to be appealing and they begin to see each other in spite of her mother's disapproval.\nCompeting alongside Luzhin in the championship is Dottore Salvatore Turati (Fabio Sartor), who is approached by Leo Valentinov (Stuart Wilson), a Russian, who is Luzhin's former chess tutor from pre-revolutionary Russia. Valentinov tells the Italian that Luzhin cannot handle pressure and he intimates he will make sure that his former prodigy will be unsettled off-table, giving Turati a winning chance.\nThe competition starts badly for Luzhin, who is unsettled by the presence of his former friend and coach. He struggles through the early rounds but he soon begins to win again as his relationship with Katkov becomes closer and intimate. She then informs her parents that she is going to marry him. Meanwhile, Luzhin goes on to reach the final and face Turati.\nBut in the finals the Russian \u00c9migr\u00e9 loses out to the time clock, forcing the game to adjourn. However, outside the venue, he is whisked away by an accomplice of Valentinov who abandons him in the countryside. His former teacher knows that this will completely unhinge him because of the memory of his parents' abandonment many years ago. Luzhin wanders aimlessly until he collapses and is found by a group of Blackshirts.\nLuzhin is taken to the hospital suffering from complete mental exhaustion. The doctor informs Katkov that he will die if he keeps playing chess as he is addicted to the game and it is consuming his very being. Nevertheless, even while recuperating Valentinov comes around with a chess board encouraging Luzhin to finish the match with the Italian, Turati. Natalia defends her beloved but urges him to break off with the game. Luzhin seems to agree.\nEventually Luzhin leaves the hospital. He and Natalia then agree to marry at the earliest opportunity. However, on the morning of the wedding, Luzhin is put into a car with Valentinov, who tells him that there is the small matter of finishing the competition. In terror, Luzhin leaps from the car. Dazed, cut and mentally confused, he stumbles back to the hotel where he tries to dig up the rest of the glass chess pieces he buried on the grounds years ago, (1:36:39 \"I've got the King but I need the whole army...\") but he does not find them.\nLuzhin, who is in his muddied wedding suit, sits in his room as Natalia and the hotel staff try to open the door. But before they can get in, the troubled chess grandmaster throws himself out of his bedroom window and dies. The tragic death is witnessed by Valentinov who has just arrived by car.\nThe film then concludes in the competition hall where Natalia completes the competition using her fianc\u00e9's notes. She discovers the papers in his pocket and an experienced chess player explains to her the matter of the notes. In an arranged meeting without public she plays against Turati who does exactly what Luzhin expected and loses. Katkov and Turati then leave acknowledging the Pyrrhic victory and the genius of Luzhin.\nProduction.\nNabokov based \"The Defense\" on the life of German chess master Curt von Bardeleben, who seemingly committed suicide by leaping from a window in 1924.\nThe film was shot entirely in Europe. Budapest, Hungary was used for outdoor scenes as they were set in St Petersburg, these included the Sz\u00e9chenyi Chain Bridge, Hungarian National Museum and Heroes' Square. The chess tournament (although in Italy) was shot inside the main hall of the Museum of Ethnography, Budapest. In Italy, the hotel scenes were filmed at Villa Erba, Cernobbio, on the Lake Como. The scene at the railway station is in Brenna-Alzate, near Como.\nIn the novel, Valentinov's first name is never mentioned; on the contrary, Luzhin's first name is revealed only in the closing sentences. Another dissimilarity is that the novel ends up by Luzhin's suicide, thereby his game would be never finished.\nThe finale.\nThe chess position they play for the final between Turati and Luzhin is already a winning position for Black (Luzhin), even though Black is down on material. By playing 1. Kg4 (as opposed to 1. Kf2) White walks into a forced checkmate with a rook sacrifice:\nIf White plays 1. Kf2 instead of 1. Kg4 this leads to a heavy material loss for White and an easy game for Black:\nand Black is up by a rook.\nIn the film Luzhin's final moves were made by his fianc\u00e9. The tournament had been paused after Luzhin had a nervous breakdown which had been caused by extreme strain. On the same day he was going to get married he committed suicide by jumping from the hotel balcony. His fianc\u00e9 later found a piece of paper inside his jacket where he had written down his intended final moves against Turati.\nCinematic error.\nIn Luzhin's previous game, on his way to the final, the film shows an inaccurate checkmating move. The scene shows White (Luzhin) play an apparently brilliant combination culminating in a queen sacrifice followed by Rd1-d8#.\nHowever, White's rook on the \"d1\" square is pinned against its king in the corner at \"h1\" by Black's rook on \"c1\", making the checkmate unplayable.\nNevertheless, Luzhin (White) is shown playing the illegal winning move to wild applause from the audience.\nThe sequence is as follows, Luzhin has just played his rook to e8 (check) although it is not clear if this was a capture or not. Play then continues:\nWhite's last move is illegal (see rules of chess). Moving the rook would put White in check, which is prohibited.\nIn the film, the Black rook was erroneously placed on \"c1\" instead of \"c2\", where it was in the actual game this winning combination was played, making it legal, a famous win of Milan Vidmar over future world champion Max Euwe.\nRelease.\nThe film opened in 59 cinemas in the United Kingdom on 8 September 2000 and grossed \u00a359,779 in its opening weekend, placing at 15th at the UK box office.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51567", "revid": "50570567", "url": "https://en.wikipedia.org/wiki?curid=51567", "title": "Cnidocyte", "text": "Stinging cell used by cnidarians\nA cnidocyte (also known as a cnidoblast) is a type of cell containing a large secretory organelle called a \"cnidocyst\", that can deliver a sting to other organisms as a way to subdue prey and defend against predators. A cnidocyte explosively ejects the toxin-containing cnidocyst which is responsible for the stings delivered by a cnidarian. The presence of this cell defines the phylum Cnidaria, which also includes the corals, sea anemones, hydrae, and jellyfish. Cnidocytes are single-use cells that need to be continuously replaced.\nStructure and function.\nEach cnidocyte contains an organelle called a cnidocyst, which consists of a bulb-shaped capsule and a hollow, coiled tubule that is contained within. Immature cnidocytes are referred to as cnidoblasts or nematoblasts. The externally oriented side of the cell has a hair-like trigger called a cnidocil, a mechano-chemical receptor. When the trigger is activated, the tubule shaft of the cnidocyst is ejected and, in the case of the penetrant nematocyst, the forcefully ejected tubule penetrates the target organism. This discharge takes a few microseconds, and is able to reach accelerations of about 40,000\u00a0\"g\". Research from 2006 suggests the process occurs in as little as 700\u00a0nanoseconds, thus reaching an acceleration of up to 5,410,000\u00a0\"g\". After penetration, the toxic content of the nematocyst is injected into the target organism, allowing the sessile cnidarian to capture the immobilized prey. Recently, in two sea anemone species (\"Nematostella vectensis\" and \"Anthopleura elegantissima\"), the type I neurotoxin protein Nv1 was shown to be localized in ectodermal gland cells in the tentacles, next to but not in nematocysts. Upon encounter with a crustacean prey, nematocysts discharge and pierce the prey, and Nv1 is massively secreted into the extracellular medium by the nearby gland cells, thus suggesting another mode of entry for toxins.\nCnidocyte capsule composition.\nThe cnidocyte capsule is made of novel Cnidaria-specific gene products which combine known protein domains. Minicollagen gene products (proteins) are one of the major structural components of the capsule. They are very short genes containing the characteristic collagen-triple helix sequence, as well as polyproline domains and cysteine-rich domains. Trimeres of mini collagen proteins assemble through their terminal cysteine-rich domain, forming highly organized and rigid supra-structures. Minicollagen 1 Ncol-1 polymers assemble on the inner shell while the outer capsule is composed of polymerized NOWA (Nematocyst Outer Wall Antigen) proteins. Nemato Galectin, minicollagen Ncol-15 and chondroitin are novel proteins used to build the tubule shaft. In piercing cnidocytes, the novel protein spinalin is used to make the spines present at the base of the shaft.\nDischarge mechanism.\nThe cnidocyst capsule stores a large concentration of calcium ions, which are released from the capsule into the cytoplasm of the \"cnidocyte\" when the trigger is activated. This causes a large concentration gradient of calcium across the cnidocyte plasma membrane. The resulting osmotic pressure causes a rapid influx of water into the cell. This increase in water volume in the cytoplasm forces the coiled cnidae tubule to eject rapidly. Prior to discharge the coiled cnidae tubule exists inside the cell in an \"inside out\" condition. The back pressure resulting from the influx of water into the cnidocyte together with the opening of the capsule tip structure or operculum, triggers the forceful eversion of the cnidae tubule causing it to right itself as it comes rushing out of the cell with enough force to impale a prey organism.\nThat force is to be calculated as the mass of the mechanism's stylet multiplied by its acceleration. The pressure that is generated by this impact into its prey is to be calculated by the stylet's force divided by its area. Researchers have calculated an ejected mass of 1\u00a0nanogram, an acceleration of 5,410,000\u00a0g and a stylet tip radius of 15\u00a0\u00b1\u00a08\u00a0nm. Therefore, a pressure of more than 7\u00a0GPa was estimated at the stylet tip which they write is in the range of technical bullets.\nFluid dynamics in nematocyst discharge.\nFew papers have modeled the discharge aside from direct observation. Observational studies typically used a tentacle solution assay with a chemical stimulant to create discharge and cameras to record them. One in 1984 and another in 2006 as imaging technology improved. One study involved computational fluid dynamics where variables such as barb plate size, prey cylindrical diameter and fluid medium Reynolds number were manipulated.\nObservational studies indicate that velocities of the barb/stylet decrease throughout the discharge. As such, the incredible maximum acceleration is achieved at the beginning. Dynamic traits such as maximum discharge velocities and trajectory patterns may not correspond to static traits such as tubule lengths and capsule volumes. Therefore, caution is appropriate when using medusan nematocyst assemblages as indicators of prey selection and trophic role. This is possibly the case for other jelly species and hence one cannot generally infer nematocyst static traits to prey size.\nPrey detection.\nCnidae are \"single use\" cells, and thus represent a large expenditure of energy to produce. In Hydrozoans, in order to regulate discharge, cnidocytes are connected as \"batteries\", containing several types of cnidocytes connected to supporting cells and neurons. The supporting cells contain chemosensors, which, together with the mechanoreceptor on the cnidocyte (cnidocil), allow only the right combination of stimuli to cause discharge, such as prey swimming, and chemicals found in prey cuticle or cutaneous tissue. This prevents the cnidarian from stinging itself although sloughed off cnidae can be induced to fire independently.\nTypes of cnidae.\nOver 30 types of cnidae are found in different cnidarians. They can be divided into the following groups:\nCnidocyte subtypes can be differentially localized in the animal. In the sea anemone \"Nematostella vectensis\", the majority of its non-penetrant sticky cnidocytes, the spherocytes, are found in the tentacles, and are thought to help with prey capture by sticking to the prey. By contrast, the two penetrant types of cnidocytes present in this species display a much broader localization, on the outer epithelial layer of the tentacles and body column, as well as on the pharynx epithelium and within mesenteries.\nThe diversity of cnidocytes types correlates with the expansion and diversification of structural cnidocyst genes like mini collagen genes. Minicollagen genes form compact gene clusters in Cnidarian genomes, suggesting a diversification through gene duplication and subfunctionalization. Anthozoans display less capsule diversity and a reduced number of mini collagen genes, and medusozoans have more capsule diversity (about 25 types) and a vastly expanded minicollagen genes repertoire. In the sea anemone \"Nematostella vectensis\", some minicollagens display a differential expression pattern in different cnidocytes subtypes.\nCnidocyte development.\nCnidocytes are single-use cells that need to be continuously replaced throughout the life of the animal with different mode of renewal across species.\nModes of renewal.\nIn Hydra polyps, cnidocytes differentiate from a specific population of stem cells, the interstitial cells (I-cells) located within the body column. Developing nematocysts first undergo multiple rounds of mitosis without cytokinesis, giving rise to nematoblast nests with 8, 16, 32 or 64 cells. After this expansion phase, nematoblasts develop their capsules. Nests separate into single nematocysts when the formation of the capsule is complete. Most of them migrate to the tentacles where they are incorporated into battery cells, which hold several nematocysts, and neurons. Battery cells coordinate firing of nematocysts.\nIn the hydrozoan jellyfish \"Clytia hemisphaerica\", nematogenesis takes place at the base of the tentacles, as well as in the manubrium. At the base of the tentacles, nematoblasts proliferate then differentiate along a proximal-distal gradient, giving rise to mature nematocytes in the tentacles through a conveyor belt system.\nIn the Anthozoan sea anemone \"Nematostella vectensis\", nematocytes are thought to develop throughout the animal from epithelial progenitors. Furthermore, a single regulatory gene that codes for the transcription factor ZNF845 also called CnZNF1 promotes the development of a cnidocyte and inhibits the development of a RFamide producing neuron cell. This gene evolved in the stem cnidarian through domain shuffling.\nCnidocyst maturation.\nThe nematocyst forms through a multi-step assembly process from a giant post-Golgi vacuole. Vesicles from the Golgi apparatus first fuse onto a primary vesicle: the capsule primordium. Subsequent vesicle fusion enables the formation of a tubule outside of the capsule, which then invaginates into the capsule. Then, an early maturation phase enables the formation of long arrays of barbed spines onto the invaginated tubule through the condensation of spinalin proteins. Finally, a late maturation stage gives rise to undischarged capsules under high osmotic pressure through the synthesis of poly-\u03b3-glutamate into the matrix of the capsule. This trapped osmotic pressure enables rapid thread discharge upon triggering through a massive osmotic shock.\nNematocyst toxicity.\nNematocysts are very efficient weapons. A single nematocyst has been shown to suffice in paralyzing a small arthropod (\"Drosophila\" larva). The most deadly cnidocytes (to humans, at least) are found on the body of a box jellyfish. One member of this family, the sea wasp, \"Chironex fleckeri\", is \"claimed to be the most venomous marine animal known,\" according to the Australian Institute of Marine Science. It can cause excruciating pain to humans, sometimes followed by death. Other cnidarians, such as the jellyfish \"Cyanea capillata\" (the \"Lion's Mane\" made famous by Sherlock Holmes) or the siphonophore \"Physalia physalis\" (Portuguese man o' war, \"Bluebottle\") can cause extremely painful and sometimes fatal stings. On the other hand, aggregating sea anemones may have the lowest sting intensity, perhaps due to the inability of the nematocysts to penetrate the skin, creating a feeling similar to touching sticky candies. Besides feeding and defense, sea anemone and coral colonies use cnidocytes to sting one another in order to defend or win space. Despite their effectiveness in prey-predator interactions, there is an evolutionary tradeoff as cnidarian venom systems are known to reduce the cnidarian's reproductive fitness and overall growth.\nVenom from animals such as cnidarians, scorpions and spiders may be species-specific. A substance that is weakly toxic for humans or other mammals may be strongly toxic to the natural prey or predators of the venomous animal. Such specificity has been used to create new medicines and bioinsecticides, and biopesticides.\nAnimals in the phylum Ctenophora (\"sea-gooseberries\" or \"comb jellies\") are transparent and jelly-like but have no nematocysts, and are harmless to humans.\nCertain types of sea slugs, such as the nudibranch aeolids, are known to undergo kleptocnidy (in addition to kleptoplasty), whereby the organisms store nematocysts of digested prey at the tips of their cerata.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "51569", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=51569", "title": "Diacylglycerol", "text": ""}
{"id": "51571", "revid": "3113", "url": "https://en.wikipedia.org/wiki?curid=51571", "title": "Multivariate gaussian distribution", "text": ""}
